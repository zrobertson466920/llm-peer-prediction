{
  "example_idx": 6,
  "reference": "Under review as a conference paper at ICLR 2023\n\nREWARD LEARNING WITH TREES: METHODS AND EVALUATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecent efforts to learn reward functions from human feedback have tended to use deep neural networks, whose lack of transparency hampers our ability to explain agent behaviour or verify alignment. We explore the merits of learning intrinsically interpretable tree models instead. We develop a recently proposed method for learning reward trees from preference labels, and show it to be broadly competitive with neural networks on challenging high-dimensional tasks, with good robustness to limited or corrupted data. Having found that reward tree learning can be done effectively in complex settings, we then consider why it should be used, demonstrating that the interpretable reward structure gives significant scope for traceability, verification and explanation.\n\n1\n\nINTRODUCTION\n\nFor a reinforcement learning (RL) agent to reliably achieve a goal or desired behaviour, this objective must be encoded as a reward function. However, manual reward design is widely understood to be challenging, with risks of under-, over-, and mis-specification leading to undesirable, unsafe and variable outcomes (Pan et al., 2022). For this reason, there has been growing interest in enabling RL agents to learn reward functions from normative feedback provided by humans (Leike et al., 2018). These efforts have proven successful from a technical perspective, but an oft-unquestioned aspect of the approach creates a roadblock to practical applications: reward learning typically uses black-box neural networks (NNs), which resist human scrutiny and interpretation. For advocates of explainable AI (XAI), this is a problematic state of affairs. The XAI community is vocal about the safety and accountability risks of opaque learning algorithms (Rudin, 2019), but an inability to interpret even the objective that an agent is optimising places us in yet murkier epistemic territory, in which an understanding of the causal origins of learnt behaviour, and their alignment with human preferences, becomes virtually unattainable. Black-box reward learning could also be seen as a missed scientific opportunity. A learnt reward function is a tantalising object of study from an XAI perspective, due to its triple status as (1) an explanatory model of revealed human preferences, (2) a normative model of agent behaviour, and (3) a causal link between the two. The approach proposed by Bewley & Lecue (2022) provides a promising way forward. Here, human preference labels over pairs of agent behaviours are used to learn tree-structured reward functions (reward trees), which are hierarchies of local rules that admit visual and textual representation and can be leveraged to monitor and debug agent learning. In this paper, we adapt and extend the method (including by integrating it with model-based RL agents), and compare it to NN-based reward learning in a challenging aircraft handling domain. We find it to be broadly competitive on both quantitative metrics and qualitative assessments, with our new modification to tree growth yielding significant improvements. The resultant trees are small enough to be globally interpretable (≈ 20 leaves), and we demonstrate how they can be analysed, verified, and used to generate explanations. The primary contribution of this paper is positive empirical evidence that reward learning can be done effectively using interpretable models such as trees, even in complex, high-dimensional continuous environments. We also make secondary methodological contributions: improvements to the originally-proposed learning algorithm, as well as metrics and methods for reward evaluation and interpretability that may be useful to others working in what remains a somewhat preparadigmatic field. After reviewing the necessary background and related work in Sections 2 and 3, we present our refinement of reward tree learning in Section 4, and describe how we deploy it online with a model-based agent in Section 5. Section 6 contains our experiments and results, which consider both quantitative and qualitative aspects of learning performance, and an illustrative analysis of learnt tree structures. Finally, Section 7 concludes and discusses avenues for future work.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2 BACKGROUND AND RELATED WORK\n\nMarkov Decision Processes (MDPs) In this formulation of sequential decision making, the state of a system at time t, st ∈ S, and the action of an agent, at ∈ A, condition the successor state st+1 according to dynamics D : S × A → ∆(S) (∆(·) denotes the set of all probability distributions over a set). A reward function R : S ×A×S → R then outputs a scalar reward rt+1 given st, at and st+1. RL uses exploratory data collection to learn action-selection policies π : S → ∆(A), with the goal of maximising the expected discounted sum of future reward, ED,π Reward Learning In the usual MDP framing, R is an immutable property of the environment, which belies the practical fact that AI objectives originate in the uncertain goals and preferences of fallible humans (Russell, 2019). Reward learning (or modelling) (Leike et al., 2018) replaces handspecified reward functions with models learnt from humans via revealed preference cues such as demonstrations (Ng et al., 2000), scalar evaluations (Knox & Stone, 2008), approval labels (Griffith et al., 2013), corrections (Bajcsy et al., 2017), and rankings (Christiano et al., 2017). The default use of NNs for reward learning severely limits interpretability; reward trees provide a possible solution.\n\nh=0 γhrt+h+1, γ ∈ [0, 1].\n\n(cid:80)∞\n\nXAI for RL (XRL) Surveys of XAI for RL (Puiutta & Veith, 2020; Heuillet et al., 2021) divide between intrinsic approaches, which imbue agents with structure such as object-oriented representations (Zhu et al., 2018) or symbolic policy primitives (Verma et al., 2018), and post hoc analyses of learnt representations (Zahavy et al., 2016), including computing feature importance/saliency (Huber et al., 2019). Spatiotemporal scope varies from the local explanation of single actions (van der Waa et al., 2018) to the summary of entire policies via representative trajectories (Amir & Amir, 2018) or critical states (Huang et al., 2018). While most post hoc methods focus on fixed policies, some provide insight into the dynamics of agent learning (Dao et al., 2018; Bewley et al., 2022).\n\nExplainable Reward Functions At the intersection of reward learning and XRL lie efforts to improve human understanding of reward functions and their effects on action selection. While this area is “less developed” than other XRL sub-fields (Glanois et al., 2021), a distinction has again emerged between intrinsic approaches which create rewards that decompose into semantic components (Juozapaitis et al., 2019) or optimise for sparsity (Devidze et al., 2021), and post hoc approaches which apply feature importance analysis (Russell & Santos, 2019), counterfactual probing (Michaud et al., 2020), or simplifying transformations (Jenner & Gleave, 2022). Sanneman & Shah (2022) use human-oriented metrics to compare the efficacy of reward explanation techniques. In this taxonomy, reward tree learning is an intrinsic approach, as the rule structure is inherently readable.\n\nTrees in RL Tree models have a long history in RL (Chapman & Kaelbling, 1991; Dˇzeroski et al., 1998; Pyeatt, 2003). Their use is increasingly given an XRL motivation. Applications again divide into intrinsic methods, where an agent’s policy (Silva et al., 2020), value function (Liu et al., 2018; Roth et al., 2019) or dynamics model (Jiang et al., 2019) is a tree, and post hoc tree approximations of an existing agent’s policy (Bastani et al., 2018; Coppens et al., 2019) or transition statistics (Bewley et al., 2022). Related to our focus on human-centric learning, Cobo et al. (2012) learn tree-structured MDP abstractions from demonstrations and Tambwekar et al. (2021) distill a differentiable tree policy from natural language. While Sheikh et al. (2022) use tree evolution to learn dense intrinsic rewards from sparse environment ones, Bewley & Lecue (2022) are the first to learn and use reward trees in the absence of any ground-truth reward signal, and the first to do so from human feedback.\n\n3 PREFERENCE-BASED REWARD LEARNING\n\nWe adopt the preference-based approach to reward learning, in which a human is presented with pairs of agent trajectories (sequences of state, action, next state transitions) and expresses which of each pair they prefer as a solution to a given task of interest. A reward function is then learnt to explain the pattern of preferences. This approach is popular in the existing literature (Wirth et al., 2016; Christiano et al., 2017; Lee et al., 2021b) and has a firm psychological basis. Experimental results indicate that humans find it cognitively easier to make relative (vs. absolute) quality judgements (Kendall, 1975; Wilde et al., 2020) and exhibit lower variance when doing so (Guo et al., 2018). This is due in part to the lack of requirement for an absolute scale to be maintained in working memory, which is liable to induce bias as it shifts over time (Eric et al., 2007). t) ∈ RF repre1,..., xi We formalise a trajectory ξi as a sequence (xi sents a single transition as an F -dimensional feature vector. Given N trajectories, Ξ = {ξi}N i=1, the human provides K ≤ N (N − 1)/2 pairwise preference labels, L = {(i, j)}K k=1, each of which indicates that the jth trajectory is preferred to the ith (denoted by ξj ≻ ξi). Figure 1 (left) shows how a preference dataset D = (Ξ, L) can be viewed as a directed graph.\n\nT i), where xi\n\nt = φ(si\n\nt−1, ai\n\nt−1, si\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Left: The input to preference-based reward learning is a directed graph over a trajectory i=1, where each edge (i, j) represents a preference ξj ≻ ξi. Each member of Ξ is a set Ξ = {ξi}N sequence of points in RF (blue connectors show mapping). Right: Application of the four model induction stages from Sections 4.1-4.4 to this example. See Appendix A.3 for an annotated version.\n\nTo learn a reward function from D, we must assume a generative model for the preference labels. Typically, it is assumed that the human produces labels in Boltzmann-rational accordance with the sum of rewards (or return) output by a latent reward function over the feature space, R : RF → R. This is formalised by adapting the classic preference model of Bradley & Terry (1952):\n\nP (ξj ≻ ξi|R) =\n\n1\n\n1 + exp( 1\n\nβ (G(ξi|R) − G(ξj|R)))\n\n, where G(ξi|R) =\n\n(cid:88)T i\n\nt=1\n\nR(xi\n\nt),\n\n(1)\n\nand β > 0 is a temperature coefficient. The objective of reward learning is to approximate R within some learnable function class R. This is often formalised as minimising the negative log-likelihood (NLL) loss over L. Wirth et al. (2016) also use the discrete 0-1 loss, which considers only the directions of predicted preferences rather than their strengths. These two losses are defined as: I[P (ξj ≻ ξi|R) ≤ 0.5].\n\n− log P (ξj ≻ ξi|R);\n\nlNLL(D, R) =\n\nl0-1(D, R) =\n\n(cid:88)\n\n(cid:88)\n\n(2)\n\n(i,j)∈L\n\n(i,j)∈L\n\n4 REWARD TREE INDUCTION\n\nIn prior work, R is the class of linear models R(x) = w⊤x (Sadigh et al., 2017), which have limited expressivity, or deep NNs (Christiano et al., 2017), which resist human interpretation. As an intermediate option, Bewley & Lecue (2022) (BL) propose the reward tree model. Here, the parameter space consists of node-level splitting rules and reward predictions for an axis-aligned decision tree, whose leaves induce a hyperrectangular partition of RF . While differentiable trees exist (Su ́arez & Lutsko, 1999), these are of the oblique (c.f. axis-aligned) kind, whose multi-feature rules are far harder to interpret in high dimensions. Therefore, instead of optimising the losses in Equation 2 end-to-end, we use a multi-stage induction method with a proxy objective at each stage. The four stages outlined below, and depicted in Figure 1 (right), depart from BL’s original method in several respects. A list of changes, and their performance implications, is given in Appendix A.1.\n\n4.1 TRAJECTORY-LEVEL RETURN ESTIMATION\n\nThis stage considers the N trajectories as atomic units, and uses the preference graph to construct a vector of return estimates g ∈ RN , which should be higher for more preferred trajectories (blue in Figure 1 (4.1), c.f. red). This is a vanilla preference-based ranking problem, and admits a standard solution. BL use a least squares matrix method to solve for g under Thurstone’s Case V preference model (Gulliksen, 1956). For consistency with prior work, and to avoid an awkward clipping step which biases preference probabilities to enable matrix inversion, we instead use a gradient method to minimise the NLL loss under the Bradley-Terry model. Concretely, the objective for this stage is\n\nargmin g∈RN\n\n(cid:104) (cid:88)\n\n(i,j)∈L\n\n− log\n\n1 1 + exp(gi − gj)\n\n(cid:105) ,\n\nsubject to\n\n(cid:26)\n\nmin(g) = 0 or max(g) = 0\n\nand std(g) = β,\n\n(3)\n\nwhere β is the mean trajectory length in Ξ, (cid:80)N i=1 T i/N . The min-or-max constraint ensures that all return estimates have the same sign (positive or negative), which aids both policy learning and interpretability (see Appendix A.2). We first optimise the NLL loss by gradient descent with the Adam optimiser (Kingma & Ba, 2014), then apply shift and scale factors to meet the two constraints.\n\n4.2 LEAF-LEVEL REWARD PREDICTION\n\nThe vector g estimates trajectory-level returns, but the aim of reward learning is to decompose these into sums of rewards for the constituent transitions, then generalise this to make reward predictions for unseen data (e.g. novel trajectories executed by a learning agent). BL’s contribution is to do this using a tree model T , consisting of a hierarchy of rules that partition the transition-level feature space RF into LT hyperrectangular subsets called leaves. Each leaf l ∈ {1..LT } is associated with\n\n3\n\n4.14.24.34.4Under review as a conference paper at ICLR 2023\n\na reward prediction rl as follows. Let the function leafT : RF → {1..LT } map a feature vector x ∈ RF to the leaf in which it resides by propagating it through the rule hierarchy. rl is defined as an average over g, weighted by the proportion of timesteps that each trajectory in Ξ spends in l: (cid:80)T i\n\nrl =\n\nN (cid:88)\n\ni=1\n\ngi T i\n\n(cid:80)N\n\nt=1 (cid:80)T j\n\nj=1\n\nt=1\n\nI[leafT (xi\n\nt) = l]\n\nI[leafT (xj\n\nt ) = l]\n\n.\n\n(4)\n\nThe effect of Equation 4 is to assign higher reward to leaves that contain more timesteps from trajectories with high g values. Predicting the reward for an arbitrary unseen feature vector x ∈ RF then involves simply looking up the reward of the leaf in which it resides: RT (x) = rleafT (x). While ostensibly na ̈ıve, BL find that this time-weighted credit assignment is more robust than several more sophisticated alternatives. It reduces the number of free parameters in subsequent induction stages, permits fast implementation, and provides an intuitive interpretation of predicted reward that is traceable back to a g value and timestep count for each ξi ∈ Ξ. Figure 1 (4.2) shows how 4, 2 and 3 timesteps from ξ1, ξ3 and ξ4 are averaged over to yield the reward prediction for one leaf (indicated by the orange shading). For more intuition on this stage, see the annotated figure in Appendix A.3.\n\n4.3 TREE GROWTH\n\nRecall that the objective of preference-based reward learning is to find a reward model that optimises a measure of fidelity to D, such as the losses in Equation 2. When the model is a tree, this is achieved by the discrete operations of growth (adding partitioning rules) and pruning (removing rules). Given a tree T , a new rule has the effect of splitting the lth leaf with a hyperplane at a location c ∈ Cf along the f th feature dimension (where Cf ⊂ R is a set of candidate split thresholds, e.g. all midpoints between unique values in Ξ). Let T + [lf c] denote the newly-enlarged tree. Splitting recursively creates an increasingly fine partition of RF . Figure 1 (4.3) shows an example with 23 leaves. A central issue is the criterion for selecting the next rule to add. BL use the proxy objective of minimising the local variance of reward predictions, which is exactly the CART algorithm (Breiman et al., 2017). While very fast, this criterion is only loosely aligned with fidelity to the preferences in D. We propose the more direct criterion of greedily maximising the immediate reduction in l0-1:\n\nargmax1≤l≤LT , 1≤f ≤F, c∈Cf\n\n(cid:2) l0-1(D, RT ) − l0-1(D, RT +[lf c]) (cid:3) .\n\n(5)\n\nIn Section 6, we show that switching to this bespoke criterion consistently improves performance. Its implementation involves a major reformulation of the tree growth algorithm; we provide vectorised, just-in-time compiled code for this in the Supplementary Material. Recursive splitting stops when no reduction in l0-1 can be achieved by any single split, or a tree size limit LT = Lmax is reached.\n\n4.4 TREE PRUNING\n\nGrowth is followed by a pruning sweep which reduces the size of the tree by rule removal. Such reduction is beneficial for both performance (Tien et al. (2022) find that limiting model capacity lowers the risk of causal confusion in preference-based reward learning) and human comprehension (in the language of Jenner & Gleave (2022), it is a form of “processing for interpretability”). Given a tree T , one pruning operation has the effect of merging two leaves into one by removing the rule at the common parent node. Let T denote the sequence of nested subtrees induced by pruning the tree recursively back to its root, at each step removing the rule that minimises the next subtree’s l0-1. We select the T ∈ T that minimises l0-1, additionally regularised by a term that encourages small trees: argminT ∈T[l0-1(D, RT ) + αLT ], where α ≥ 0. Note that even with α = 0 pruning may still yield a reduced tree, as unlike in traditional decision tree induction, the effect of individual rules on l0-1 depends on the order in which they are added or removed. In the example in Figure 1 (4.4), pruning yields a final tree with 3 leaves, for which illustrative leaf-level reward predictions are shown.\n\n5 ONLINE LEARNING SETUP\n\n5.1\n\nITERATED POLICY AND REWARD LEARNING\n\nSections 3 and 4 do not discuss the origins of the trajectories Ξ, or how reward learning relates to the downstream objective of learning a policy for the underlying task. Following most recent work since Christiano et al. (2017), we resolve both questions with an online bootstrapped approach. Assuming an episodic MDP, the ith episode of policy learning produces a new trajectory ξi to add to Ξ. We immediately connect ξi to the preference graph by asking the human to compare it to Kbatch random trajectories from the existing set (while Sadigh et al. (2017) and others have proposed active querying schemes, that is not our focus here, and this simple strategy performs satisfactorily). We then update the reward tree on the full preference graph via the four stages given in Section 4. We\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nfind that BL’s original method of starting growth from the current state of the tree causes lock-in to poor initial solutions, so instead re-grow from scratch on each update. The rule structure nonetheless tends to stabilise, as the enlarging preference graph becomes increasingly similar for later updates. For the (i + 1)th episode, the policy learning agent then attempts to optimise for the newly-updated reward. By iterating this process up to a total preference budget Kmax and/or episode budget Nmax, we hope to converge to both a reward tree that reflects the human’s preferences, and an agent policy that satisfies those preferences. Appendix A.4 contains pseudocode for the online algorithm.\n\n5.2\n\nINTEGRATION WITH MODEL-BASED RL\n\nReward learning methods are generally agnostic to the structure of the policy learning agent; this modularity is hailed as an advantage over other human-agent teaching paradigms (Leike et al., 2018). In line with most recent works, BL use a model-free RL agent, specifically soft actor-critic (SAC) (Haarnoja et al., 2018). However, other works (Reddy et al., 2020; Rahtz et al., 2022) use modelbased RL (MBRL) agents that leverage learnt dynamics models and planning. MBRL is attractive in the reward learning context because it disentangles the predictive and normative aspects of decisionmaking. Since (assuming no changes to the environment) dynamics remain stationary during online reward learning, the amount of re-learning required is reduced and along with it, the risk of pitfalls such as manipulation (Armstrong et al., 2020) and premature convergence. Additionally, MBRL can be very data-efficient; we find that switching from SAC to a model-based algorithm called PETS (Chua et al., 2018) reduces environment interaction during reward learning by orders of magnitude, and cuts wall-clock runtime (see Appendix B). PETS selects actions by decision-time planning through a learnt dynamics model D′ : S × A → ∆(S) up to a horizon H. In state s, planning searches for a sequence of H future actions that maximise return under the current reward model:\n\nargmax (a0,...,aH−1)∈AH\n\nED′\n\n(cid:104) (cid:88)H−1 h=0\n\nγhRT (φ(sh, ah, sh+1))\n\n(cid:105)\n\n, where s0 = s, sh+1 ∼ D′(sh, ah).\n\n(6)\n\nThe first action a = a0 is executed, and then the agent re-plans on the next timestep. In practice, D′ is an ensemble of probabilistic NNs, the expectation over D′ is replaced by a Monte Carlo estimate, and the optimisation is approximated by the iterative cross-entropy method.\n\n6 EXPERIMENTS AND RESULTS\n\nIn this section, we combine quantitative and qualitative evaluations to assess the performance of reward tree learning, specifically in comparison to the standard approach of using NNs. We also illustrate how the intrinsic interpretability of reward trees allows us to analyse what they have learnt. Our experiments focus on an aircraft handling domain (Figure 2), in which an agent must manoeuvre an aircraft (the ego jet, EJ) in a desired manner relative to a second reference jet (RJ) whose motion, if any, is part of the environment dynamics. We consider three tasks: Follow (turn to fly in formation with RJ on a linear path); Chase (maintain distance/line of sight to RJ as it turns randomly); and Land (approach a runway using RJ as a reference). These tasks are useful test cases for reward learning, as each has a large space of plausible reward functions, which may reflect the divergent priorities and stylistic preferences of aeronautical experts. Such expert knowledge is often tacit and difficult to codify (Sternberg & Horvath, 1999), motivating a learning-based approach. Appendix C contains a broader justification of this experimental domain alongside implementation details.\n\nFigure 2: State-action space of aircraft handling domain, and diagrams of Follow/Chase/Land tasks.\n\nIn place of costly human-in-the-loop evaluation, our experiments use synthetic oracle preferences with respect to nominal reward functions of varying complexity, which are given in Appendix C.3. This approach is popular (Griffith et al., 2013; Christiano et al., 2017; Reddy et al., 2020; Lindner et al., 2021) as it enables scalable systematic comparison, with the ability to quantify performance (and in our case, appraise learnt trees) in terms of reconstruction of a known ground truth. However, emulating a human with an oracle that responds with perfect rationality is unrealistic (Lee et al., 2021a). For this reason, Section 6.3 examines the performance impacts of noisy and myopic oracles, and a restricted data budget. Experimental details and hyperparameters are given in Appendix D.\n\n5\n\nEgo JetAction: pitch,roll, yaw, thrustdemands for EJRJEJFollowRJEJChaseRJEJLandReference JetState: poseinformationfor bothEJ and RJProceedsalong linearflight pathTask: turnonto sameflight pathand matchspeedRepresentsa targettouchdownposeTask: execute astable approachpath, ending at target poseFliessubject torandomcontrolinputTask: stayclose to RJwith lineof sight;keep safealtitudeUnder review as a conference paper at ICLR 2023\n\n6.1 QUANTITATIVE PERFORMANCE\n\nWe evaluate online reward learning with PETS using trees with the l0-1 split criterion, baselined against BL’s original variance criterion, as well as the de facto standard of NN reward learning (see Appendix D.4 for details). We use Kmax = 1000 preferences over Nmax = 200 online trajectories, and run 10 repeats. As a headline statistic, we report the oracle regret ratio (ORR): the median drop in oracle return of PETS agents deployed using each trained reward model compared with directly using the oracle reward, as a fraction of the drop to a random policy (lower is better). Below are the median (top) and minimum (bottom) ORR values across the 10 repeats for each task-model pairing: Chase Tree (0-1) .040 −.011\n\nLand Tree (0-1) .050 .011\n\nFollow Tree(0-1) .120 .057\n\nTree (var) .126 .065\n\nTree (var) .062 .010\n\nTree (var) .284 .158\n\nNN −.030 −.051\n\nNN .014 −.030\n\nNN .000 −.010\n\nWe observe that: 1) NN reward learning is strong on all tasks; 2) switching to a reward tree induces a small but variable performance hit; 3) l0-1 splitting outperforms the variance-based method; and 4) both NN and tree models sometimes exceed the direct use of the oracle (negative ORR). This has been observed before (Cao et al., 2021) and may be due to improved shaping in the learnt reward. Figure 3 expands these results with more metrics, revealing subtler trends not captured by headline ORR values. Metrics are plotted as time series over the 200 learning episodes (sliding-window medians and interquartile ranges across repeats). In the left column (a), the ORR of online trajectories shows how agent performance converges. For Follow, there is a gap between the models, with l0-1 splitting clearly aiding performance but still lagging behind the NNs. The learning curves for Chase and Land are more homogeneous, and the NNs reach only slightly lower asymptotes. For the reward tree models, (b) shows how the number of leaves changes over time. The variance-based trees tend to grow rapidly initially before stabilising or shrinking, while the l0-1 trees enlarge more conservatively, suggesting this method is less liable to overfit to small preference graphs. Trees of a readily-interpretable size (≈ 20 leaves) are produced for all tasks; it is possible that performance could be improved by independently tuning the size regulariser α per task. (c) shows l0-1 over time, which tends to increase as the growing preference graph presents a harder reconstruction problem, though the shape of all curves suggests convergence (note that random prediction gives l0-1 = 0.5). For Follow and Land, the trees that directly split on l0-1 actually perform better than the NNs; they more accurately predict the direction of preferences in the graph. The fact that this does not translate into lower ORR indicates that the problems of learning a good policy and replicating the preference dataset are not identical, a point made by Lindner et al. (2021). In the final two columns, we follow Gleave et al. (2021) in performing an unbiased, policy-invariant comparison of the models by correlating their outputs with the oracle reward functions on common evaluation datasets (see Appendix D.5 for dataset creation). We compute online correlations with the oracles in terms of both transition-level rewards (d) and the ordinal ranking of trajectories by return (e), the latter via the Kendall (1938) τ coefficient. The curves subtly differ, indicating that it is possible to reconstruct trajectory rankings (and by extension, any pairwise preferences) to a given accuracy with varying fidelity at the individual reward level. However, the common overall trend is that l0-1-based trees outperform variance-based ones, with NNs sometimes improving again by a smaller margin, and sometimes bringing no added benefit. Moving top-to-bottom down the tasks, the gap between models reduces from both sides; NN performance worsens while variance-based trees improve.\n\nFigure 3: Time series of metrics for online NN- and tree-based reward learning on all three tasks.\n\n6\n\n00.51FollowLandChaseOnline ORRaRank correlationeNNTree (0-1)Tree (var)00.5100.20.40-1 lossc00.050.10.1500.050.10.1500.050.10.150.250.50.75100.250.50.751Tree sizeb10201020102000.5100.510.20.6100.51Reward correlationdUnder review as a conference paper at ICLR 2023\n\nA potentially important factor in these experiments is that the oracle reward for Follow is a simple linear function, while the other two contain progressively more terms and discontinuities (see Appendix C.3). A trend suggested by these results is thus that the performance gap between NNs and reward trees (on both ORR and correlation metrics) reduces as the ground truth reward becomes more complex and nonlinear. Further experiments would be needed to test this hypothesis.\n\n6.2 VISUAL TRAJECTORY INSPECTION\n\nWhile useful for benchmarking, quantitative metrics provide little insight into the structure of the learnt solutions. They would also mostly be undefined when learning from humans since the ground truth reward is unknown. We therefore complement them with a visual analysis of induced agent behaviour. Figure 4 plots 500 trajectories of PETS agents using the best repeat by ORR for each task-model combination, across a range of features as well as time (see Appendix C.2 for feature definitions). Dashed curves show the trajectory with the highest predicted return according to each model. We also show trajectories for PETS agents with direct oracle access, and for random policies. The high-level trend is that all models are far closer to the oracle than random, with few examples of obviously incorrect behaviour (highlighted in red, due to colouring by ORR). While the NNs induce trajectories that are almost indistinguishable from the oracle, the l0-1-based reward trees lag not far behind. The variance-based trees produce more anomalies. Successes of the l0-1 trees include the execution of Follow with a single banked turn before straightening up, as shown by the up error time series (a). Indeed, the trajectories for this model are almost imperceptibly different from those of the NN, a result which is belied by the mediocre ORR of 0.158. This underlines the importance of joint quantitative-qualitative evaluation. For Chase (b), the l0-1 tree has learnt to keep the agent narrowly above the altitude threshold alt < 50, below which the oracle reward is strongly negative (see Appendix C.3). The threshold is violated in only eight of 500 trajectories (1.6%). For Land, the l0-1 tree replicates the oracle in producing a gradual reduction in alt (c) while usually keeping pitch close to 0 (d), although the distribution of roll values is less narrow. In contrast, the agent using the variance-based tree for Follow sometimes fails to reach the target position (e; red trajectories), and also does not reliably straighten up to reduce up error (f). For Chase, the altitude threshold does not appear to have been learnt precisely, and lower-altitude trajectories often fail to close the distance to RJ (g and h; red trajectories). For Land, the variancebased tree gives a later and less smooth descent (i), and less consistent pitch control (j), than the NN or l0-1-based tree, although all models produce a somewhat higher altitude profile than the oracle.\n\nFigure 4: Agent trajectories using the best models by ORR, with oracle and random for comparison.\n\n6.3 SENSITIVITY ANALYSIS\n\nIt is important to consider how learning performance degrades with reduced or corrupted data. In Figure 5, we evaluate the effect of varying the number of preferences Kmax (with fixed Nmax = 200) and trajectories Nmax (with fixed Kmax = 1000) on reward learning with NNs and l0-1-splitting trees. Following Lee et al. (2021a), we also create more human-like preference data via two modes of oracle irrationality: preference noise (by using a nonzero Boltzmann temperature β to give a desired error rate on the coverage datasets) and a myopic recency bias (by exponentially discounting earlier timesteps when evaluating trajectory returns). We run five repeats for all cases, and report the medians and interquartile ranges of ORR (lower is better) and rank correlation (higher is better). Both NN and tree models exhibit good robustness with respect to all four parameters. Although NNs remain superior in most cases, the gap varies, and is often reduced compared to the base\n\n7\n\nOracleTree (var)Follow0353-3131distclosing speed1200up errorTimestepChaseLand50350altlos error-1.401.55rollpitch0200030altdist horORR010-12020150Timestepdist0,00,0RandomegjhfTree (0-1)abdcHighest returnaccordingto oracleNNHighest returnaccordingto modeliUnder review as a conference paper at ICLR 2023\n\nFigure 5: Comparative sensitivity analysis of reward learning with NNs and trees.\n\ncases (bold labels). The budget sensitivity is low, with little improvement for Kmax > 1000 and Nmax > 200, and no major drop even with 25% of the data as the base case. For all tasks, the oracle error probability can increase to around 20% before significant drops in performance are observed. This is a promising indicator of the transferability of reward tree learning to imperfect human data. Another general observation is that the trends for trees are somewhat smoother than for NNs, with fewer sharp jumps and fewer instances of very high spread across the five repeats. In the right column (a), we summarise these results by taking the difference between the NN and tree metrics, and averaging across the three tasks. In all cases aside from rank correlation with β > 0, the NN-tree gap tends to become more favourable to the tree models as the varied parameter becomes more challenging (top-to-bottom). This sensitivity analysis thus indicates that reward trees are at least as robust to difficult learning scenarios as NNs, and may even be slightly more so.\n\n6.4 TREE STRUCTURE ANALYSIS\n\nThus far we have shown that reward learning with l0-1-based trees can be competitive with NNs, but not quite as performant overall. We now turn to a concrete advantage which may tip practical tradeoffs in its favour: the ability to interpret the learnt model, and analyse how its structure arises from the underlying preference graph. In this section we favour depth over breadth, so focus on the single best tree by ORR on the Chase task. The analysis in Figure 6 is divided into sections (a – d): (a) This reward tree has 17 leaves. The oracle reward, printed below, uses four features, all of which are used in the tree in ways that are broadly aligned (e.g. lower los error leads to leaves with higher reward). The model has learnt the crucial threshold alt < 50, correctly assigning low reward when it is crossed. This explains why we observe rare violations of the altitude threshold in Figure 4. However, it has not learnt the ideal distance to RJ, dist = 20, with 43.3 being the lowest value used in a rule. This could be because the underlying preference graph lacks sufficient preferences to make this distinction; adopting an active querying scheme may help to discover such subtleties efficiently. Other features besides those used by the oracle are present in the tree, indicating some causal confusion (Tien et al., 2022). This may not necessarily harm agent performance, as it could provide beneficial shaping (e.g. penalising positive closing speed, which indicates increasing distance to RJ). That may indeed be the case for this model since ORR is actually negative. (b) We plot the tree’s predicted reward against the oracle reward for all timesteps in the online trajectories (correlation = 0.903). The predictions for each leaf lie along a horizontal line. Most leaves, including 1 and 2, are well-aligned on this data because their oracle reward distributions are tightly concentrated around low/high averages respectively (note that the absolute scale is irrelevant here). Leaf 16 has a wider oracle reward distribution, with several negative outliers. An optimal tree would likely split this leaf further, perhaps using the alt < 50 threshold. The one anomaly is leaf 13, which contains just a single timestep from ξ77. This trajectory is the eighth best in the dataset by oracle return, but this leaf assigns that credit to a state that seemingly does not merit it, as the distance to RJ is so high (dist > 73). This may be an example of suboptimal reward learning, but the fact that its origin can be pinpointed precisely is a testament to the value of interpretability. (c) We leverage the tree structure to produce a human-readable explanation of reward predictions for a single trajectory, which may be of value to an end user (e.g. a pilot). We consider ξ191, a rare\n\n8\n\nOraclemyopia( )ChaseFollowPreferencebudget( )Trajectorybudget( )NNTree (0-1)Land100020004000500250200400800100500.980.20.30.400.950.90.80.10.45Oracle errorprobability(via )1ORR00.51Rank correlation00.5100.5100.5100.5100.51Average NN-Tree GapNNbetter-0.2500.25-0.2500.25ORRORRORRTreebetterTreebetterNNbetteraRank correlationRank correlationRank correlationUnder review as a conference paper at ICLR 2023\n\nFigure 6: Analysis of a reward tree learnt for the Chase task.\n\ncase that violates the altitude threshold. The time series of reward shows that the 20 timesteps are spent in leaves 16, 15, 11 and 7. Rescaled oracle rewards are overlaid in teal, and show that the model’s predictions are well-aligned. To the right, we translate this visualisation into a textual form, similar to a nested program. Read top-to-bottom, the text indicates which rules of the tree are active at each timestep, and the effect this has on predicted reward. This trajectory starts fairly positively, with reward gradually increasing over the first 16 timesteps as dist is reduced to between 43.3 and 73, but then falls dramatically when the alt < 50 threshold is crossed. We are unaware of any method that could extract such a compact explanation of sequential predictions from an NN. (d) We isolate a subtree, starting at the root node, that splits only on dist and alt. We give a spatial representation of the subtree, and how it is populated by the 200 online trajectories, using a 2D partition plot analogous to those in Figure 1. Zooming into leaf 1, which covers cases where the altitude threshold is violated, we see that it contains a total of 30 timesteps across four trajectories. By Equation 4, the low reward for this leaf results from a weighted average of the return estimates for these four trajectories, which in turn (by Equation 3) are derived from the preference graph. We can use this inverse reasoning to ask why this leaf has much lower reward than its sibling (leaf 2 of the subtree). A proximal explanation comes by filtering the graph for preferences that specifically compare trajectories that visit those two leaves. 49 such preferences exist, and in all cases, the oracle prefers the trajectory that does not visit leaf 1. Some of these preferences may be more practically salient than others. For example, we might highlight trajectories that feature more than once (e.g. ξ28 is preferred to both ξ18 and ξ48), or cases where trajectories with low overall return estimates are nonetheless preferred to those in leaf 1 (e.g. ξ43 ≻ ξ21 and ξ56 ≻ ξ47). We believe that much more could be done to extend this framework for traceable explanation of preference-based reward.\n\n7 CONCLUSION AND FUTURE WORK\n\nReward learning with trees provides a promising alternative to black-box NNs, and could enable more trustworthy and verifiable agent alignment. Through oracle experiments on high-dimensional tasks, we show that reward trees with around 20 leaves can achieve quantitative and qualitative performance close to that of NNs, with a more direct split criterion bringing consistent improvements. We find evidence that the NN-tree gap reduces as the ground truth reward becomes more nonlinear, and remains stable or reduces further in the presence of limited or corrupted data. While practical applications may accept some loss in performance for a gain in interpretability, further algorithmic improvements should be sought, including to move beyond locally-greedy split criteria. However, our immediate aim is to develop an end-to-end framework for explainable model-based agents with preference-based reward trees (roughly: planning can be reframed as comparing alternative paths through the discrete leaves of the tree). Having established this framework, we then intend to evaluate reward learning and explanation with real human preferences in the aircraft handling domain.\n\n9\n\nr=2.48-----dist≥43.3?r=3.12-----alt≥50.2?Nor=2.15-----dist≥95.4?Yes(1) r=0.937Nor=3.17-----los error≥1.10?Yesr=3.22-----roll error≥0.847?Nor=3.04-----abs roll≥1.56?Yes(2) r=3.35No(3) r=3.18Yesr=3.14-----closing speed≥7.63?No(6) r=2.92Yes(4) r=3.16No(5) r=2.47Yesr=2.31-----alt≥50.0?Nor=1.72-----closing speed≥8.34?Yes(7) r=0.763Nor=2.34-----dist≥73.0?Yesr=2.47-----abs roll≥0.744?Nor=2.22-----alt error≥2.14?Yesr=2.72-----dist≥68.7?Nor=2.37-----delta hdg error≥-3.68?Yes(8) r=2.78No(9) r=2.45Yes(10) r=3.20No(11) r=2.37Yes(12) r=2.76Nor=2.21-----los error≥0.0126?Yes(13) r=3.92Nor=2.21-----thrust≥3.54?Yes(14) r=1.70No(15) r=2.21Yes(16) r=1.76No(17) r=0.78YesIntermediate reward fornon-leaf node by timestep-weighted averageOracle reward:adr=2.48-----dist≥43.3?r=3.12-----Nor=2.15-----dist≥95.4?Yes(1) r=0.937No(2) r=3.17Yesr=2.31-----alt≥50.0?No(6) r=1.72Yes(3) r=0.763Nor=2.34-----dist≥73.0?Yes(4) r=2.47No(5) r=2.22Yes043.395.4150dist0350alt5073alt≥50.2?Total: 30 timesteps1245631andPredicted reward04.45b(13) r = 3.92(2) r = 3.35(16) r = 1.76(1) r = 0.937, t = 2(16)(15)(11)(7)14Predicted rewardTimestep120cOracle reward(rescaled)-300-200-100014Oracle rewardPredicted rewardTimestepTrajectory returnestimate089Under review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nReward learning from feedback is a technique for improving the alignment of learning agents with human preferences, by replacing the rigidity of explicit reward design with a dynamic interaction with a human-in-the-loop. As such, its successful use can benefit the performance, reliability and safety of these learning systems, which has the potential to deliver immensely positive ethical value. The contribution of reward trees is to render the process of reward learning more humaninterpretable, and thus easier to explain, debug and verify. We believe this can deliver a further reduction in ethical risk through the identification and mitigation of unforeseen consequences.\n\nPreference-based reward tree learning has many diverse applications. The aircraft handling domain used in our evaluation was selected to provide a good balance of technical complexity, task diversity, industrial relevance, intuitiveness for our intended readership, and transferability to other domains such as land and sea transportation. In addition to this wide range of related use cases, as well as civilian uses of aviation itself (e.g. aerobatics), we acknowledge that the ability to interactively learn aircraft control policies may have applications in the defence sector. The three concrete tasks of Follow, Chase and Land are largely application-neutral, with no implication of harm, and are concerned solely with the safe and human-like control of aircraft in environments with other aircraft. Any general learning technique such as ours is, however, fundamentally dual-use, and transparency about this fact seems to us the best mitigation of the ethical risk. It is vital that anyone intending to use or develop our method continues to do so in an ethically responsible manner.\n\nREFERENCES\n\nDan Amir and Ofra Amir. Highlights: Summarizing agent behavior to people. In Proceedings of\n\nthe 17th International Conference on Autonomous Agents and MultiAgent Systems, 2018.\n\nStuart Armstrong, Jan Leike, Laurent Orseau, and Shane Legg. Pitfalls of learning a reward function online. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, 2020.\n\nAndrea Bajcsy, Dylan P Losey, Marcia K O’Malley, and Anca D Dragan. Learning robot objectives from physical human interaction. In Conference on Robot Learning, pp. 217–226. PMLR, 2017.\n\nOsbert Bastani, Yewen Pu, and Armando Solar-Lezama. Verifiable reinforcement learning via policy\n\nextraction. Advances in Neural Information Processing Systems, 31, 2018.\n\nTom Bewley and Freddy Lecue. Interpretable preference-based reinforcement learning with treestructured reward functions. In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, pp. 118–126, 2022.\n\nTom Bewley, Jonathan Lawry, and Arthur Richards. Summarising and comparing agent dynamics with contrastive spatiotemporal abstraction. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2022.\n\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\n\nof paired comparisons. Biometrika, 39(3/4):324–345, 1952.\n\nLeo Breiman, Jerome H Friedman, Richard A Olshen, and Charles J Stone. Classification and\n\nregression trees. Routledge, 2017.\n\nZehong Cao, KaiChiu Wong, and Chin-Teng Lin. Weak human preference supervision for deep reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems, 32(12): 5369–5378, 2021.\n\nDavid Chapman and Leslie Pack Kaelbling. Input generalization in delayed reinforcement learning:\n\nAn algorithm and performance comparisons. In Ijcai, volume 91, pp. 726–731, 1991.\n\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Advances in Neural Information Processing Systems, 31, 2018.\n\nLuis C Cobo, Charles L Isbell Jr, and Andrea L Thomaz. Automatic task decomposition and state\n\nabstraction from demonstration. Georgia Institute of Technology, 2012.\n\nYouri Coppens, Kyriakos Efthymiadis, Tom Lenaerts, and Ann Now ́e. Distilling deep reinforcement learning policies in soft decision trees. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2019.\n\nGiang Dao, Indrajeet Mishra, and Minwoo Lee. Deep reinforcement learning monitor for snapshot recording. In 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 591–598. IEEE, 2018.\n\nJan De Leeuw and Patrick Mair. Multidimensional scaling using majorization: Smacof in r. Journal\n\nof statistical software, 31:1–30, 2009.\n\nRati Devidze, Goran Radanovic, Parameswaran Kamalaruban, and Adish Singla. Explicable reward design for reinforcement learning agents. Advances in Neural Information Processing Systems, 34:20118–20131, 2021.\n\nSaˇso Dˇzeroski, Luc De Raedt, and Hendrik Blockeel. Relational reinforcement learning. In Inter-\n\nnational Conference on Inductive Logic Programming, pp. 11–22. Springer, 1998.\n\nBrochu Eric, Nando Freitas, and Abhijeet Ghosh. Active Preference Learning with Discrete Choice\n\nData. Advances in Neural Information Processing Systems, 20, 2007.\n\nClaire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu.\n\nA survey on interpretable reinforcement learning. arXiv preprint arXiv:2112.13112, 2021.\n\nAdam Gleave, Michael D Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying differ-\n\nences in reward functions. In International Conference on Learning Representations, 2021.\n\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy shaping: Integrating human feedback with reinforcement learning. Advances in neural information processing systems, 26, 2013.\n\nHarold Gulliksen. A least squares solution for paired comparisons with incomplete data. Psychome-\n\ntrika, 21(2):125–134, 1956.\n\nYuan Guo, Peng Tian, Jayashree Kalpathy-Cramer, Susan Ostmo, J Peter Campbell, Michael F Chiang, Deniz Erdogmus, Jennifer G Dy, and Stratis Ioannidis. Experimental design under the bradley-terry model. In IJCAI, pp. 2198–2204, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pp. 1861–1870. PMLR, 2018.\n\nAlexandre Heuillet, Fabien Couthouis, and Natalia D ́ıaz-Rodr ́ıguez. Explainability in deep rein-\n\nforcement learning. Knowledge-Based Systems, 214:106685, 2021.\n\nSandy H Huang, Kush Bhatia, Pieter Abbeel, and Anca D Dragan. Establishing appropriate trust via critical states. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3929–3936. IEEE, 2018.\n\nTobias Huber, Dominik Schiller, and Elisabeth Andr ́e. Enhancing explainability of deep reinforcement learning through selective layer-wise relevance propagation. In Joint German/Austrian Conference on Artificial Intelligence (K ̈unstliche Intelligenz), pp. 188–202. Springer, 2019.\n\nErik Jenner and Adam Gleave. Preprocessing reward functions for interpretability. arXiv preprint\n\narXiv:2203.13553, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nWei-Cheng Jiang, Kao-Shing Hwang, and Jin-Ling Lin. An experience replay method based on tree structure for reinforcement learning. IEEE Transactions on Emerging Topics in Computing, 9(2): 972–982, 2019.\n\nZoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and Finale Doshi-Velez. Explainable reinforcement learning via reward decomposition. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2019.\n\nMaurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.\n\nMG Kendall. Rank Correlation Methods; Griffin, C., Ed, 1975.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nW Bradley Knox and Peter Stone. Tamer: Training an agent manually via evaluative reinforcement. In 2008 7th IEEE international conference on development and learning, pp. 292–297. IEEE, 2008.\n\nKimin Lee, Laura Smith, Anca Dragan, and Pieter Abbeel. B-pref: Benchmarking preference-based\n\nreinforcement learning. Advances in Neural Information Processing Systems, 35, 2021a.\n\nKimin Lee, Laura M Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. In International Conference on Machine Learning, pp. 6152–6163. PMLR, 2021b.\n\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\n\nDavid Lindner, Matteo Turchetta, Sebastian Tschiatschek, Kamil Ciosek, and Andreas Krause. Information directed reward learning for reinforcement learning. Advances in Neural Information Processing Systems, 34:3850–3862, 2021.\n\nGuiliang Liu, Oliver Schulte, Wang Zhu, and Qingcan Li. Toward interpretable deep reinforcement In Joint European Conference on Machine Learning and\n\nlearning with linear model u-trees. Knowledge Discovery in Databases, pp. 414–429. Springer, 2018.\n\nEric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv\n\npreprint arXiv:2012.05862, 2020.\n\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, vol-\n\nume 1, pp. 2, 2000.\n\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping In International Conference on Learning Representations,\n\nand mitigating misaligned models. 2022.\n\nErika Puiutta and Eric Veith. Explainable reinforcement learning: A survey. In International crossdomain conference for machine learning and knowledge extraction, pp. 77–95. Springer, 2020.\n\nLarry D Pyeatt. Reinforcement learning with decision trees. In 21 st IASTED International Multi-\n\nConference on Applied Informatics, pp. 26–31, 2003.\n\nMatthew Rahtz, Vikrant Varma, Ramana Kumar, Zachary Kenton, Shane Legg, and Jan Leike. Safe\n\ndeep rl in 3d environments using human feedback. arXiv preprint arXiv:2201.08102, 2022.\n\nSiddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human objectives by evaluating hypothetical behavior. In International Conference on Machine Learning, pp. 8020–8029. PMLR, 2020.\n\nAaron M Roth, Nicholay Topin, Pooyan Jamshidi, and Manuela Veloso.\n\nConservative qimprovement: Reinforcement learning for an interpretable decision-tree policy. arXiv preprint arXiv:1907.01180, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and\n\nuse interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.\n\nJacob Russell and Eugene Santos. Explaining reward functions in Markov decision processes. In\n\nThe Thirty-Second International Flairs Conference, 2019.\n\nStuart Russell. Human compatible: Artificial intelligence and the problem of control. Penguin,\n\n2019.\n\nDorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learn-\n\ning of reward functions. In Proceedings of Robotics: Science and Systems (RSS), 2017.\n\nLindsay Sanneman and Julie A Shah. An empirical study of reward explanations with human-robot\n\ninteraction applications. IEEE Robotics and Automation Letters, 2022.\n\nHassam Ullah Sheikh, Shauharda Khadka, Santiago Miret, Somdeb Majumdar, and Mariano Phielipp. Learning intrinsic symbolic rewards in reinforcement learning. In 2022 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2022.\n\nAndrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun Son. Optimization methods for interpretable differentiable decision trees applied to reinforcement learning. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1855–1865. PMLR, 26–28 Aug 2020.\n\nRobert J Sternberg and Joseph A Horvath. Tacit knowledge in professional practice: Researcher\n\nand practitioner perspectives. Psychology Press, 1999.\n\nAlberto Su ́arez and James F Lutsko. Globally optimal fuzzy decision trees for classification and regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(12):1297–1311, 1999.\n\nPradyumna Tambwekar, Andrew Silva, Nakul Gopalan, and Matthew Gombolay. Specifying and interpreting reinforcement learning policies through simulatable machine learning. arXiv preprint arXiv:2101.07140, 2021.\n\nJeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D Dragan, and Daniel Brown. A study of causal confusion in preference-based reward learning. arXiv preprint arXiv:2204.06601, 2022.\n\nJasper van der Waa, Jurriaan van Diggelen, Karel van den Bosch, and Mark Neerincx. Contrastive explanations for reinforcement learning in terms of expected consequences. In IJCAI/ECAI Workshop on Explainable Artificial Intelligence, 2018.\n\nAbhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. In International Conference on Machine Learning, pp. 5045–5054. PMLR, 2018.\n\nNils Wilde, Alexandru Blidaru, Stephen L Smith, and Dana Kuli ́c. Improving user specifications for robot behavior through active preference learning: Framework and evaluation. The International Journal of Robotics Research, 39(6):651–667, 2020.\n\nChristian Wirth, Johannes F ̈urnkranz, and Gerhard Neumann. Model-free preference-based rein-\n\nforcement learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nTom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding dqns. In\n\nInternational conference on machine learning, pp. 1899–1908. PMLR, 2016.\n\nGuangxiang Zhu, Zhiao Huang, and Chongjie Zhang. Object-oriented dynamics predictor. Advances\n\nin Neural Information Processing Systems, 31, 2018.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA METHODOLOGICAL DETAILS\n\nA.1 LIST OF CHANGES TO BEWLEY & LECUE’S ORIGINAL METHOD\n\n• Change from Thurstone to Bradley-Terry preference model and least squares matrix\n\nmethod to gradient descent on lNLL;\n\n– Motivation: Consistency with prior work. – Performance implications: Thurstone → Bradley-Terry minimally affects results. Ma-\n\ntrix method → gradient descent eliminates a bias caused by preference clipping.\n\n– Computational implications: Thurstone → Bradley-Terry improves computational efficiency (bypasses inverse normal CDF computation). Matrix method → gradient descent tends to slightly increase runtime, but this depends on preference dataset size.\n\n• Add scale and sign constraints to return estimates;\n\n– Motivation / implications: Scaling results in leaf-level reward predictions having a consistent scale with a standard deviation close to 1. This is intended to aid human readability but has no effect on performance. For sign constraints, see Appendix A.2.\n\n• Change from variance-based to l0-1-based split criterion during tree growth;\n\n– Motivation: Hypothesised performance gain; new split criterion is more directly\n\naligned with the objective of preference reconstruction.\n\n– Performance implications: Significantly improves preference construction and quan-\n\ntitative/qualitative agent performance on evaluation tasks; see Section 6.\n\n– Computational implications: More costly as variance computation has an extremely efficient iterative implementation. However, we have developed optimised code for the new splitting method using just-in-time compilation; see Supplementary Material.\n\n• Deploy online with a model-based (PETS) RL agent instead of model-free (SAC);\n\n– Motivation / implications: See Section 5.2 and Appendix B.\n\n• Always use latest online trajectory in all pairs during each preference batch;\n\n– Motivation: Acts as a simple form of active sampling to correct reward overestimation; model-based planning is liable to exploit any behaviours with inappropriately high current reward, which can then be immediately corrected by a negative preference. – Performance implications: Performs similarly to BL’s active sampling method, which\n\nup-weights trajectories in Ξ with high predicted return.\n\n– Computational implications: Less expensive than BL’s method; no need to recompute\n\nreturn predictions for all trajectories in Ξ on each batch.\n\n* Note: New method would be less effective with a model-free agent where policy updates are gradual; can rely less on agent immediately exploiting current reward.\n\n• Regrow tree from scratch on each update.\n\n– Motivation: Prevents rule structure from prematurely converging to local minima. – Performance implications: Early experiments indicated that premature convergence problem is mitigated by this change, resulting in more sustained improvements in reward fidelity and agent performance.\n\n– Computational implications: Since splits are evaluated and made per update step, computation time is increased. However, when typical post-pruning tree size LT (≈ 20 in our experiments) is small compared with Lmax (= 100), this increase is fractional, and contributes only a few percentage points to overall runtime.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nA.2 SIGN CONSTRAINT FOR RETURN ESTIMATES\n\nApplying a sign constraint to the trajectory-level return estimates means that rewards output by a reward tree (via Equation 4) are also all either positive or negative. This has no effect on any measure of preference reconstruction since preferences are invariant to affine transformations of an underlying utility function. However, we find it brings two distinct benefits:\n\n• Enabling the prevention of perverse incentives for agents to terminate or elongate episodes in tasks with termination conditions (negative rewards on non-terminal transitions incentivise termination, while positive rewards incentivise elongation).\n\n• Simplifying the manual interpretation of tradeoffs between rewards from different leaves of a tree (understanding the relative impacts of “more of a negative reward” and “less of a positive reward” requires the awkward mental juggling of negatives).\n\nFor the task with a termination condition in this paper (Land), we use negative rewards (max = 0 constraint) to disincentivise episode elongation, because termination is generally indicative of success. For the two fixed-length tasks (Follow and Chase) we default to using positive rewards (min = 0 constraint). Although this is arbitrary, our own experience is that positive rewards make for somewhat more intuitive interpretation of the tree structure, and its effect on agent actions. We stress that this is purely anecdotal; the relative human interpretability of positive, negative and mixed-sign rewards would be a worthy subject for deeper empirical investigation.\n\nA.3 ANNOTATED VERSION OF FIGURE 1\n\nFigure A1: Annotated version of Figure 1.\n\n15\n\nA dataset of pairwise preferences over trajectories can be visualised as a directed graph, where nodes are trajectories and edges are preferences. Each trajectory is a sequence of vectors in feature space. In turn, each vector represents the (state, action, next state) transition that occurs at a particular timestep t.Feature vector for timestep t:mappingGiven an existing tree structure and trajectories with estimated returns, we define the reward for each leaf as a timestep-weighted average over the trajectories that visit that leaf.4.2Naive assumption: all timesteps in trajectorycontribute equally to return (i.e. uniform colour)Reward calculation for highlighted leaf:Growing a tree by adding rules recursively splits the feature space into an increasing number of leaves. In our improved growth method, we use the 0-1 loss as the criterion for selecting rules to add. 4.3Growth tends to separate high- and low-return trajectories into different leaves\"Pure\" leafcontaining asingletrajectoryAfter growing to a maximum size, a final pruning stage recursively removes rules to minimise the 0-1 loss, with an additional regularisation term to penalise large trees. This yields a reduced tree for use as a reward function.4.4Pruning merges leaves back together, while retainingthe most important splits for preference predictionReward values shown are illustrative4.1Before doing anything in the feature space, we first use the standard Bradley-Terry preference model to estimate an overall return value for each trajectory in the dataset based on its preferences.Two favourable preferences;highest estimated returnThree unfavourablepreferences; lowestestimated returnPreferred to but not to ; intermediate estimated returnEach edge points topreferred trajectoryUnder review as a conference paper at ICLR 2023\n\nA.4 PSEUDOCODE FOR ONLINE ALGORITHM\n\nAlgorithm 1 Online preference-based reward tree learning\n\nInputs: Possibly pre-trained dynamics model D′, feature function φ, trajectory budget Nmax, preference budget Kmax, tree size limit Lmax, tree size regularisation α\n\nInitialise time t ← 0 and environment state si 0\n▷ Model-based trajectory generation (Sec.5.2) while episode not yet terminated do\n\n1: Initialise empty preference graph Ξ ← ∅, L ← ∅ 2: Initialise one-leaf tree T with r ← [0] 3: for i ∈ {1, ..., Nmax} do 4: 5: 6:\n\nt using PETS algorithm\n\nCompute action ai with D′ and rewards via Equation 4 Send ai Update D′ on recent transitions xi t+1 ← φ(si t ← t + 1\n\nt, ai\n\nt+1)\n\nt, si\n\nt to environment and get next state si\n\nt+1\n\n▷ May not be required; see Appendix D.3\n\n1, ..., xi\n\nend while ξi ← (xi Kbatch ← min((Kmax − |L|)/(Nmax + 1 − i), |Ξ|) for k ∈ {1, ..., Kbatch} do\n\nT i)\n\nSample ξj from Ξ uniformly without replacement Query human for preference ξi ≻ ξj or ξj ≻ ξi\n\nL ← L ∪\n\n(cid:26) {(i, j)} {(j, i)}\n\nif ξj ≻ ξi otherwise\n\n▷ Preference batch collection (Sec 3)\n\nend for Ξ ← Ξ ∪ {ξi} if |L| > 0 then\n\nCompute g via Equation 3 Initialise one-leaf tree T C ← midpoints between per-feature unique values in Ξ while LT < Lmax do\n\nfor l ∈ {1, ..., LT } do\n\nfor f ∈ {1, ..., F } do for c ∈ Cf do\n\nCompute l0-1 reduction for T + [lf c] via Equation 5\n\n▷ Trajectory-level return estimation (Sec 4.1)\n\n▷ Tree growth (Section 4.3)\n\nend for\n\nend for\n\nend for if max(l0-1 reduction) ≤ 0 then\n\nbreak\n\nend if l, f, c ← argmax(l0-1 reduction) T ← T + [lf c]\n\nend while T = () while LT > 1 do\n\n▷ Stop tree growth early\n\n▷ Tree pruning (Section 4.4)\n\nfor l ∈ {1, ..., LT } do\n\nCompute l0-1 reduction for T − [l]\n\n▷ T − [l] denotes pruning lth leaf\n\nend for l ← argmax(l0-1 reduction) T ← T − [l] Append T to T\n\nend while T ← argminT ∈T(l0-1 plus α-scaled tree size)\n\nend if\n\n7: 8: 9: 10: 11: 12: 13: 14: 15: 16:\n\n17:\n\n18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: 45: 46: 47: 48: 49: end for\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nB COMPARISON TO MODEL-FREE REINFORCEMENT LEARNING\n\nOne of the most consistently observed benefits of model-based RL is its sample efficiency, and this trend holds in our context. Running Algorithm 1 unchanged except for the use of a soft actor-critic (SAC) agent for policy learning, we find that approximately two orders of magnitude more environment interaction is required to achieve equivalent performance in terms of regret at convergence. In turn, this increases wall-clock runtime by 10-20 times, thereby outweighing the higher per-timestep computational cost of PETS over SAC. The caption of Figure A2 gives further details.\n\nFigure A2: Comparing the use of PETS (model-based) and SAC (model-free) agents on the Follow task (see Appendix C.3 for task details). The PETS results are taken directly from Figure 3. For SAC we retain the total preference budget of Kmax = 1000, but for longer runs add episode trajectories to Ξ and L at a reduced frequency so that Nmax = 200 (e.g. for 50000 episodes, only 1 in 250 episodes are added to the graph; the rest are skipped). All SAC agents use policy and value networks with two 256-unit hidden layers each, learning rates of 1e−4 and 1e−3 for the policy and value updates with the Adam optimiser, a discount factor of γ = 0.99, and an interpolation factor of 0.99 for Polyak averaging of the target networks. Updates use mini-batches of 32 transitions sampled uniformly from a rolling replay buffer of capacity 5e4. We find that initially running SAC for a total of 200 episodes, matching our PETS experiments, gives the model-free learning algorithm insufficient time to achieve good performance in terms of regret on the oracle reward function (a higher learning rate leads SAC to become unstable). We then progressively increase the length of SAC runs until regret performance matches the use of PETS, and find that this requires around 50000 episodes, an increase of 250 times. It is noteworthy that the variance-based tree model seems to perform best in the short-runtime regime, but worst in the long-runtime regime. Time constraints prevented us from investigating whether this holds in other tasks, but such an investigation would be worthwhile. In terms of wall-clock time (on a single NVIDIA Tesla P100 GPU), running reward learning with SAC for 1000 episodes is roughly equivalent to 200 episodes using PETS (25-60 minutes, depending on the reward model architecture). For 50000 episodes, this time increases to 9 hours. This brings a very practical disadvantage: if reward learning were done using human preferences instead of an oracle, that person would have to dedicate more than a full working day to the exercise, most of which would be spent waiting for several minutes between each successive preference batch.\n\nNote: The PETS wall-clock times quoted here exclude the time to pre-train the dynamics models. Although this is not how MBRL sample complexity is typically measured, we argue that it is appropriate for the reward learning context, where the key factor is the period for which a human would be required to be in-the-loop. Regardless, pre-training in the aircraft handling domain takes around 30 minutes, which remains low compared with the 9 hours for high-performing SAC agents.\n\n17\n\nPETS (200 episodes)Online ORRNNTree (var)Tree (0-1)SAC (200 episodes)SAC (1k episodes)SAC (10k episodes)SAC (50k episodes)Wall-clock time(minutes)Episode1200200400120040010001200400100001200400500001200200400EpisodeEpisodeEpisodeEpisode00.5100.5100.5100.5100.51Under review as a conference paper at ICLR 2023\n\nC AIRCRAFT HANDLING ENVIRONMENT\n\nC.1 MOTIVATION\n\nPilots of fast jet aircraft require exceptional handling abilities, acquired over years of advanced training. There would be immense practical value in developing a method for distilling the knowledge and preferences of pilots and other domain experts into a software model that captures realistic handling behaviour. The scalability of such a model would make it useful for strategic planning exercises, training of a range of operational roles, and development and testing of other software systems. However, as in many contexts where intuitive decision-making and rapid motor control are paramount, the preferences of experts (over the space of fast jet handling trajectories) are in large part tacit, and thus defy direct scrutiny or verbal description. Put simply: experts know good handling when they see it, but cannot directly express why.1 This makes it practically challenging to accurately elicit this knowledge for codification into an automated system.\n\nThe methods presented in this paper form the basis of a possible solution to this dilemma. Given a dataset of trajectories executed by an artificial learning agent and labelled with pairwise expert preferences (which require only tacit knowledge to produce), we use statistical learning algorithms to construct an interpretable explanatory model of those preferences. The result is two distinct outputs that could form valuable components of future planning, training and development software:\n\n1. A tree-structured reward function, which may be used for automated scoring of flight trajectories executed by human or artificial pilots. We aim for this to produce an evaluation that is consistent, unbiased and aligned with the judgement that the original expert would have made, alongside an explanatory rationale that can be leveraged to justify, verify and improve handling behaviour.\n\n2. A model-based RL agent capable of executing high-quality handling behaviour with respect\n\nto the reward function, for use in simulation.\n\nIt should be noted that any realistic handling scenario would involve multiple experts somewhatdiffering knowledge and expertise. A natural extension of our approach, which we see as valuable future work, is to learn individual reward functions for each expert, then leverage the intrinsic interpretability to identify biases, inconsistencies and tradeoffs. This suggests a third application of reward tree learning: providing a basis for evaluating and training the experts themselves.\n\nC.2\n\nIMPLEMENTATION\n\nWe consider a simple set-piece formulation of the aircraft handling problem, in which the piloting agent is given a short time window to manoeuvre their aircraft (the ego jet, EJ) in a particular manner relative to a second reference jet (RJ). Special cases of this formulation create a wide variety of tasks for the pilot to solve. Options include:\n\n• RJ is a friendly aircraft which EJ should accompany in formation flight.\n\n• RJ is adversarial and EJ must outmanoeuvre it to gain a tactical advantage.\n\n• Rather than being a distinct physical entity, RJ defines a goal pose (position and attitude)\n\nfor EJ to reach. The goal pose may be fixed or moving over time.\n\nWe developed this formulation to strike a balance between simplicity and generality; many realistic scenarios faced by a fast jet pilot involve interaction with a single other airborne entity. On a practical level, it provides scope for the definition of many alternative tasks given the same state and action spaces, and largely unchanged dynamics.\n\nThe state space contains the positions, attitudes, velocities and accelerations of both EJ and RJ (state dimensionality = 37) and the action space consists of pitch, roll, yaw and thrust demands for\n\n1This statement certainly underestimates the rich complexity of human expertise; in reality, an expert’s mental model is likely to be partly tacit and partly explicit. The general strategy of preference-based reward learning is to operate as if the mental model were 100% tacit, and explore what can be achieved under such a strong restriction. Real-world applications would likely benefit from combining this approach with some amount of hand-coded expert knowledge.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nEJ only (action dimensionality = 4). The EJ dynamics function integrates these demands with a simplified physics engine, including gravity and air resistance (we make no claim of realism here; the simulator is merely a proof of concept). A new action is accepted every 25 steps of the physics engine, reducing an agent’s decision frequency to approximately 1Hz. RJ dynamics, as well as the conditions of state initialisation and termination, vary between tasks (see Appendix C.3).\n\nThe final generic aspect of the implementation is the feature function φ, which maps the transition space S × A × S (total dimensionality = 37 + 4 + 37 = 78) into an F -dimensional space of taskrelevant features. In consultation with engineers with experience of aerospace simulation and control algorithms, we devised the following set of F = 30 features that is sufficiently expressive to capture the important information for all three of our target tasks, without being overly specialised to one or providing too much explicit guidance to the reward learning process. Apart from those containing “delta” or “rate”, all features are computed over the successor state for each transition, st+1.\n\ndist closing speed alt alt error delta alt error dist hor delta dist hor pitch error delta pitch error abs roll roll error delta roll error hdg error delta hdg error fwd error delta fwd error up error delta up error right error delta right error los error\n\ndelta los error abs lr offset\n\nspeed g force pitch rate roll rate yaw rate thrust delta thrust\n\nEuclidean distance between EJ and RJ Closing speed between EJ and RJ (negative = moving closer) Altitude of EJ Difference in altitude between EJ and RJ (negative = EJ is lower) Change in alt error between st and st+1 Euclidean distance between EJ and RJ in horizontal plane Change in dist hor between st and st+1 (negative = moving closer) Absolute difference in pitch angle between EJ and RJ Change in pitch error between st and st+1 Absolute roll angle of EJ Absolute difference in roll angle between EJ and RJ Change in roll error between st and st+1 Absolute difference in heading angle between EJ and RJ Change in hdg error between st and st+1 Angle between 3D vectors indicating forward axes of EJ and RJ Change in fwd error between st and st+1 Angle between 3D vectors indicating upward axes of EJ and RJ Change in up error between st and st+1 Angle between 3D vectors indicating rightward axes of EJ and RJ Change in right error between st and st+1 Angle between forward axis of EJ and vector from EJ to RJ (measures whether RJ is in EJ’s line of sight) Change in los error between st and st+1 Magnitude of projection of vector from EJ to RJ onto RJ’s rightward axis (measures left-right offset between the two aircraft in RJ’s reference frame) Airspeed of EJ Instantaneous g-force experienced by EJ Absolute change of EJ pitch between st and st+1 Absolute change of EJ roll between st and st+1 Absolute change of EJ yaw between st and st+1 Instantaneous thrust output by EJ engines Absolute change in thrust between st and st+1\n\nC.3 TASKS AND ORACLES\n\nIn this paper, we consider three concrete tasks that instantiate the general EJ-RJ framework. For each, we construct a plausible oracle reward function from a subset of the 30 features, meaning that reward learning is in part a feature selection problem (tree models perform feature selection explicitly whenever they add a new splitting rule). Although the precise nature of the oracle reward functions is secondary, and those given below are among many equally reasonable alternatives, we dedicated several hours of development time to ensuring they yield reasonable behaviour upon visual inspection. The difficulty and seeming arbitrariness of this manual reward design process is precisely why reward learning (ultimately from real human preferences) is an enticing proposition. Descriptions of the three tasks, along with their respective oracles, are given below:\n\n• Follow: Here RJ follows a linear horizontal flight path at a constant velocity, which is oriented opposite to the initial velocity of EJ. The goal is to turn onto and then maintain the path up to the episode time limit of 20 timesteps. This constitutes a very simple form of\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nformation flight. The oracle reward function incentivises closing the distance to the moving target, and matching the upward axes of EJ and RJ:\n\nr = −(dist + 0.05 × closing speed + 10 × up error).\n\n• Chase: Here RJ follows an erratic trajectory generated by random control inputs, and the goal is to chase it without taking EJ below a safe altitude of 50. Episodes terminate after 20 timesteps. The oracle reward function incentivises keeping RJ at a distance of 20 and within EJ’s line of sight, while keeping EJ oriented upright. It also has a large penalty for dropping below the safe altitude:\n\nr = −(abs(dist − 20) + 10 × los error + 5 × abs roll +\n\n(cid:26) 100 0\n\nif alt < 50 otherwise\n\n).\n\n• Land: Here the goal is to execute a safe approach towards landing on a runway, where RJ represents the ideal landing position (central, zero altitude, slight upward pitch). EJ is initialised at a random altitude, pitch, roll and offset, such that landing may be challenging but always physically possible. An episode terminates if EJ passes RJ along the axis of the runway, or after 25 timesteps otherwise. The oracle reward function for this task is by far the most complex of the three, including terms that incentivise continual descent, penalise g-force and engine thrust, and punish the agent for making contact with the ground (alt < 0.6) before the start of the runway:\n\nr = −(0.05 × abs lr offset + 0.05 × alt + hdg error + abs roll +0.5 × pitch error + 0.25 × (yaw rate + roll rate + pitch rate) +0.1 × g force + 0.025 × thrust + 0.05 × delta thrust (cid:26) 1 0\n(cid:26) 1 0\n\n(cid:26) 2 0\n(cid:26) 10 if alt < 0.6\n\nif delta dist hor > 0 otherwise\n\nif abs lr offset > 10 otherwise\n\nif delta alt > 0 otherwise\n\notherwise\n\n+\n\n+\n\n+\n\n+\n\n).\n\n0\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nD IMPLEMENTATION AND EXPERIMENT DETAILS\n\nD.1 ORACLE PREFERENCES\n\nOracle preferences are generated in accordance with the Bradley-Terry model given in Equation 1, i.e. by computing the returns for the two trajectories ξi and ξj, and sampling from a Boltzmann distribution parameterised by those returns. In our main experiments, we set the temperature coefficient β = 0, which results in the oracle deterministically selecting the trajectory with higher return (ties broken uniform-randomly). In Section 6.3 we study cases with β > 0, which provide a more realistic emulation of real human preference data.\n\nD.2 HYPERPARAMETERS FOR TREE INDUCTION\n\nIn all experiments, we use the following hyperparameters during tree induction. These were identified through informal search, and we make no claim of optimality, but they do lead to reasonable performance on the three tasks of varying complexity. This indicates a general insensitivity of the method to precise hyperparameter values, which is often practically advantageous.\n\n• Trajectory return estimation using the Adam optimiser with a learning rate of 0.1. Optimisation stops when the mean lNLL changes by < 1e−5 between successive gradient steps.\n\n• Per-feature candidate split thresholds C defined as all midpoints between adjacent unique\n\nvalues in the trajectory set Ξ. These are recomputed on each update.\n\n• Tree size limit Lmax = 100.\n\n• Tree size regularisation coefficient α = 5e−3.\n\nAs mentioned in Appendix A.2, we enforce negative rewards (max = 0 constraint) for the Land task, and positive rewards (min = 0 constraint) for Follow and Chase.\n\nD.3 MODEL-BASED RL IMPLEMENTATION\n\nFor conceptual details on the PETS algorithm, we refer readers to the original paper by Chua et al. (2018). In our implementation, the dynamics model is an ensemble of five NNs, each with four hidden layers of 200 hidden units and ReLU activations. State vectors are pre-normalised by applying a hand-specifed scale factor to each dimension. Decision-time planning operates over a time horizon of H = 10 and consists of 10 iterations of the cross-entropy method. Each iteration samples 20 candidate action sequences from an independent Gaussian, of which the top 5 in terms of return are identified as elites, then updates the sampling Gaussian towards the elites with a learning rate of 0.5. In all experiments we use γ = 1, meaning no temporal discounting is applied during planning.\n\nIn our experiments, we find that the particular dynamics of the aircraft handling environment permit us to pre-train D′ on random offline data, and accurately generalise to states encountered during online reward learning. This means we perform no further updates to the model while reward learning is ongoing. As well as improving wall-clock speed, this avoids complexity and convergence issues arising from having two interacting learning processes (note that simultaneous learning is completely unavoidable with model-free RL). To pre-train, we collect 1e5 transitions by rolling out a uniform random policy, then update each of the five networks on 1e5 independently sampled mini-batches of 256 transitions, using the mean squared error loss over normalised next-state predictions.\n\nD.4 NEURAL NETWORK REWARD LEARNING BASELINE\n\nWe baseline our reward tree models against the de facto standard approach of reward learning using a NN. In constructing this baseline, we aimed to retain as much of Algorithm 1 as possible, so that only the model architecture varies. The result is that we replace lines 21-47 with the following:\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nLmini-batch ← a mini-batch of B preference labels sampled from L Compute lNLL over Lmini-batch via Equations 1 and 2 Backpropagate loss and update network parameters\n\n21: for m ∈ {1, ..., M } do 22: 23: 24: 25: end for 26: rall ← reward predictions for all feature vectors in Ξ 27: Scale network outputs by 1/std(rall) 28: Shift network outputs by −min(rall) or −max(rall), depending on desired reward sign\n\nThe new lines 26-28 replicate the two constraints applied in Equation 3.\n\nIn all experiments, we follow Lee et al. (2021b) in implementing the reward model as a three-layer network with 256 hidden units each and leaky ReLU activations, and performing the update on line 24 using the Adam optimiser (Kingma & Ba, 2014) with a learning rate of 3e−4. On each update, we sample M = 100 mini-batches of size B = 32 and take one gradient step per mini-batch.\n\nD.5 COVERAGE DATASETS FOR POLICY-INVARIANT EVALUATION\n\nGleave et al. (2021) recently highlighted the importance of comparing and evaluating learnt reward functions in a policy-invariant manner, by using a common evaluation distribution rather than onpolicy data generated by agents optimising for each reward. Ideally, the offline evaluation data should have high coverage (i.e. high-entropy state distribution, both high- and low-quality trajectories), in order to characterise the reward functions’ outputs across a spectrum of plausible policies.\n\nIn our context, we can generate data that satisfies these requirements by leveraging the known oracle reward functions and the PETS algorithm. We deploy PETS using the oracle reward, but randomise the planning parameters (number of planning iterations ∈ {1, ..., 50}, number of action sequence samples ∈ {4, ..., 50}) on every episode. In all cases, we take the top 25% of action sequences as elites. This randomisation results in trajectories that are sometimes near-optimal with respect to the oracle, sometimes moderate in quality, and sometimes barely better than random. For all three tasks, we generate a dataset of 200 evaluation trajectories in this manner.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nE VISUALING REWARD FUNCTIONS WITH SIMILARITY EMBEDDINGS\n\nHere we briefly discuss a novel form of reward visualisation which we developed and found valuable during our own analysis. Given some measure of similarity between reward functions, such as the EPIC metric proposed by Gleave et al. (2021), we can compute a matrix of pairwise similarities between any number of such functions (computational cost permitting). We can then produce a 2D embedding of the functions by applying multidimensional scaling (MDS). Visualising this embedding as a scatter plot enables the discovery of salient patterns and trends in the set of functions.\n\nIn Figure A3, we use the metric of rank correlation on the coverage datasets, and the SMACOF MDS algorithm (De Leeuw & Mair, 2009), to embed all 30 model repeats and the oracle for each task. This gives an impression of the models’ similarity not just to the oracle, but to each other. Aside from the Follow NNs, which form a tight cluster near the oracle, the distribution for each model indicates roughly equal consistency between repeats. The overlap of convex hulls suggests that the rankings produced by all models are broadly similar for Land, but more distinct for Chase. Shading points by ORR reveals that while models further from the oracle tend to induce worse-performing policies, the trend is not monotonic. This reinforces the point made elsewhere that the problems of learning a good policy and exactly replicating the ground truth reward are not identical.\n\nFigure A3: Rank correlation embeddings for all model repeats from the main experiments, with the scatter point for each repeat shaded by ORR.\n\nPopulating such embedding plots more densely, perhaps by varying model hyperparameters, could provide a means of mapping the space of learnable reward functions and its relationship to policy It would also be straightforward to compute similarity values for the same model performance. repeat at multiple checkpoints during learning. This would yield a trajectory in the rank embedding space, which could aid the assessment of the stability and convergence properties of online learning with different models and hyperparameter values.\n\n23\n\nOracleORR10FollowChaseLandNNTree (0-1)Tree (var)",
  "translations": [
    "# Summary Of The Paper\n\nIn this paper, the authors work on preference-based RL setups, where they develop reward trees from the preference labels. They use four stages for the learning, first estimating the returns for trajectories, then, predicting the reward at the leaf level, and finally, they apply tree growth and tree pruning operations for the reward trees. Even though they observe performance hit when switching from neural network-estimated rewards to tree-based rewards, the authors argue that the tree-based methods can be competitive. The advantages of the tree-based method are shown in Figure 6 by easy interpretation of the reward meanings and semantic understanding.\n\n# Strength And Weaknesses\n\nStrength\n+ A complete pipeline for tree-based reward learning from preference labels where the authors show effective learning and competitive performance against commonly used neural network approaches;\n+ Detailed description of the four-stage pipeline;\n+ Competitive performance is shown in evaluation against NN approaches;\n+ Example reward trees are shown to understand the advantages of the tree-based reward methods;\n\nWeaknesses\n- The method is tested upon a not commonly used domain, i.e., aircraft handling environment. It's hard to understand the properties of the domain and the difficulty of learning just from the text description. The authors could use additional relatively standard benchmarks for their experiments;\n- The generality of the proposed method could be shown by using more diverse domains for the experiments.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper has decent clarity and quality.  The novelty seems a bit limited but it's a complete pipeline from preference labels to agent training. The reproducibility should be good since they provided source code.\n\n# Summary Of The Review\n\nI'm currently leaning towards rejection of the paper since it is showing a certain level of ethical concerns. The domains tested in the paper could be used for wars, weapons, etc, which I don't think it's the only application. The authors could demonstrate the effectiveness of their method in a less sensitive domain.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nThe experiments are conducted with an aircraft-controlling domain, where the agent maneuvers an aircraft to follow/chase/land according to a reference aircraft. Though it's not explicitly stated in the paper, it's quite likely the use case is for wars/weapons/protection of homeland/etc. For evaluating the technical aspects of the proposed method, I don't think it really must use such a domain which could cause huge ethics concerns.",
    "# Summary Of The Paper\nThe paper presents a novel approach to learning reward functions from human feedback utilizing interpretable tree models, diverging from the prevalent use of black-box neural networks. It introduces a framework for learning reward trees from pairwise preference labels, demonstrating competitive performance in complex tasks while offering enhanced interpretability. Key contributions include empirical validation of the proposed method against traditional neural network approaches and methodological improvements in reward tree induction and online learning integration.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to reward learning, emphasizing interpretability and explainability, which are critical in RL applications. The use of tree-structured models allows for better understanding and verification of agent behavior. Additionally, the empirical results show that while neural networks may outperform tree models slightly, the latter maintains a robust performance with fewer anomalies and a clear advantage in interpretability. However, the paper could benefit from a more extensive examination of the limitations of tree models, especially in cases where complex decision boundaries are needed. Additionally, the reliance on synthetic oracle preferences for evaluation may not fully capture the intricacies of real-world feedback scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The writing is concise, making complex concepts accessible to readers. The thorough explanation of the experimental setup and detailed analysis of results contribute to the paper's overall quality. The novelty of integrating tree models within the context of reward learning from human feedback is a significant advancement, particularly in the realm of explainable AI. Reproducibility is supported by the inclusion of methodological details in the appendices, although providing access to the code or datasets used would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper provides a compelling alternative to traditional neural network approaches in reward learning by leveraging tree-based models for improved interpretability and alignment with human preferences. While the empirical performance is promising, the reliance on synthetic data and the need for further exploration of limitations are notable points for improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Reward Learning with Trees: Methods and Evaluation\" investigates the learning of reward functions from human feedback using interpretable tree models, termed \"reward trees.\" Unlike conventional opaque deep neural networks (DNNs), these reward trees enhance interpretability and traceability through hierarchies of local rules. The methodology builds upon the work of Bewley & Lecue (2022) by refining a method that learns reward trees from preference labels and integrating it with model-based reinforcement learning (RL) agents. The authors conduct experiments in an aircraft handling domain, demonstrating that reward trees show competitive performance against DNNs, exhibit robustness to noisy data, and provide valuable insights into decision-making through qualitative analyses.\n\n# Strength And Weaknesses\nThe paper's primary strengths lie in its focus on interpretability and robustness, addressing a significant gap in reward learning where human feedback can be noisy or limited. The methodological advancements, particularly the introduction of a new l0-1 split criterion for tree growth and pruning, contribute to enhancing the quality and utility of learned reward functions. However, a notable limitation is that reward trees did not consistently outperform DNNs, especially in simpler reward landscapes, suggesting scenarios where traditional methods remain preferable. Additionally, the reliance on synthetic preferences for evaluation raises concerns about the generalizability of results to real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to a broad audience. The novelty of using tree models for reward learning is significant, yet it builds upon existing frameworks, which may limit its perceived groundbreaking nature. The reproducibility of the experiments is supported by comprehensive descriptions of the methodology and evaluation metrics, although the use of synthetic preferences could hinder practical application and real-world validation.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of reward learning by emphasizing interpretability and robustness through the use of reward trees. While it shows competitive performance against DNNs and offers methodological advancements, its reliance on synthetic preferences and mixed performance results suggest areas for further exploration and improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the learning of reward functions from human feedback using interpretable tree models, termed reward trees, instead of traditional deep neural networks (NNs). It builds upon previous work by Bewley & Lecue (2022) to enhance the performance and interpretability of reward trees in complex environments. The methodology involves a four-stage induction process for constructing reward trees based on human preferences, and the findings demonstrate that these trees can be competitive with NNs in high-dimensional tasks while maintaining robustness against limited or corrupted data, thereby enhancing interpretability and accountability in reinforcement learning (RL).\n\n# Strength And Weaknesses\nThe paper presents a strong contribution by addressing the interpretability issues associated with reward learning in RL, which is a pressing concern in AI safety. The proposed reward trees are backed by a solid theoretical foundation and demonstrate competitive performance compared to NNs across various tasks. However, the paper could benefit from additional empirical validation in more diverse and real-world scenarios to further substantiate the robustness of the reward trees. The experimental setup, although thorough, relies on oracle preferences, which may not fully represent the complexities of human preferences in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly outlines the methodology, making it accessible to readers familiar with RL and preference-based learning. The novelty of the approach lies in its focus on tree-based models for reward learning, which contrasts with the prevalent use of NNs. The reproducibility of the results is facilitated by detailed descriptions of experimental setups and hyperparameters provided in the appendices, although further clarity on the implementation details could enhance the reproducibility for practitioners.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of reward learning by introducing interpretable reward trees that compete with traditional NNs in performance while addressing crucial interpretability concerns. While the findings are promising, additional empirical validation in real-world contexts would strengthen the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Reward Learning with Trees: Methods and Evaluation\" presents a novel approach to reward learning through the use of interpretable tree-based models. The authors propose methodologies that leverage tree structures to enhance the transparency and interpretability of reward learning systems, which are often obscured by the complexities of neural networks. Experimental results demonstrate that the proposed method achieves competitive performance in high-dimensional tasks and exhibits robustness against noisy human preference data, particularly in an aircraft handling domain. The paper also outlines methodological improvements, including advancements in the reward tree learning algorithm and metrics for evaluating reward interpretability.\n\n# Strength And Weaknesses\nThe paper has several strengths, including the emphasis on interpretable models, competitive performance against neural networks in certain tasks, and robust handling of noisy data. However, it also has limitations, such as the complexity of tree structures that may hinder performance in highly non-linear tasks, a slight performance trade-off when compared to neural networks, and a focus on a limited domain that raises questions about generalizability. Additionally, while the authors provide a strong empirical foundation, the lack of comprehensive benchmarking against a wider array of methods may leave uncertainty regarding the overall efficacy of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodologies, and findings. The use of interpretable tree models is a novel approach in the context of reward learning, aligning well with contemporary interests in explainable AI. The authors provide sufficient empirical evidence to support their claims; however, the reproducibility of results could be questioned due to the reliance on synthetic oracle preferences and the limited diversity of tasks evaluated.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of reward learning by introducing interpretable tree models that balance performance with transparency. While it demonstrates competitive results and robustness, the limitations in generalizability and benchmarking suggest that further validation is necessary to establish the broader applicability of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel approach to reward learning from human feedback using decision tree models, referred to as \"reward trees.\" The authors argue that this methodology enhances interpretability while still achieving competitive performance compared to traditional neural networks (NNs). Their experimental validation demonstrates that the reward trees are effective in high-dimensional tasks, particularly in the context of aircraft handling, achieving notable performance with limited or noisy data. Key contributions include a preference-based learning framework, an innovative tree induction process, and integration with model-based reinforcement learning.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its methodological advancements and strong empirical results. The introduction of reward trees enhances interpretability—a crucial aspect of AI systems. The rigorous experiments conducted in relevant domains provide convincing evidence of the approach's efficacy. However, the paper's weaknesses include a reliance on synthetic or oracle preferences, which may limit the generalizability of the results. Further validation using real human preferences is necessary to assess the practical applicability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the complex ideas accessible to readers. The quality of the research is high, with thorough empirical evaluations and detailed explanations of the methodological innovations. The novelty of the approach is significant, as it addresses the growing demand for interpretable AI solutions. However, reproducibility could be a concern given the reliance on synthetic preferences; additional details on the experimental setup and data sources would enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a well-researched and innovative approach to reward learning that balances interpretability and performance. While the contributions are substantial, further validation with real-world human preferences is needed to ensure broad applicability. The work aligns with the increasing focus on transparency in AI systems.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper explores the integration of tree-structured models into the domain of adversarial training, which has predominantly been the realm of deep neural networks (DNNs). The authors propose a novel methodology for adversarial training tailored for tree models, emphasizing their interpretability and robustness against adversarial attacks. The findings indicate that tree-based adversarial models not only perform competitively against standard DNN approaches in various challenging tasks but also offer significant interpretability advantages, making them appealing for applications where model transparency is critical.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to applying tree models for adversarial training and its thorough comparative analysis with DNNs. The introduction of specific evaluation metrics for both robustness and interpretability is commendable, as it enhances the rigor of the evaluation. However, the paper acknowledges limitations, particularly concerning the scalability of tree models in high-dimensional spaces, which could restrict their applicability in more complex real-world scenarios. Additionally, while the experimental results are promising, further validation across a wider range of datasets and tasks would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making complex concepts accessible to readers. The methodology is detailed, allowing for reproducibility, although the scalability issues mentioned may pose challenges for practitioners looking to implement this approach at scale. The novelty of applying tree models to adversarial training is significant, as it opens new avenues for research in explainable AI and interpretable machine learning. However, the depth of the empirical evaluation could be expanded to reinforce the conclusions drawn.\n\n# Summary Of The Review\nOverall, this paper presents a compelling case for the use of tree-structured models in adversarial training, highlighting their interpretability and robustness. While the methodology and findings are strong, addressing scalability concerns and conducting further empirical validation would enhance the paper's impact. The work is timely and relevant, contributing to the ongoing discourse in the field of explainable AI.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward learning using tree models, advocated by the authors as a transformative shift from traditional deep neural networks (DNNs). It claims to offer enhanced transparency and interpretability in agent behavior while demonstrating improvements in learning effectiveness, particularly in scenarios with limited or corrupted data. The methodology includes a unique online learning setup aimed at integrating human feedback in real-time, supported by experimental results that suggest tree-based approaches outperform DNNs across various tasks, although the actual performance gains appear modest.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ambitious claim to innovate within the reward learning domain by introducing tree models, which the authors argue provide better interpretability and computational efficiency than DNNs. However, the weaknesses are notable: many claims of superiority are not sufficiently substantiated by the empirical results presented. While the authors propose a suite of new metrics and methods, their categorical dismissal of prior work feels unjustified and may mislead the community regarding the state of the field. Additionally, the exaggeration of performance improvements and robustness claims detracts from the paper's credibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly structured, with well-defined sections that present the authors’ ideas, methodology, and results. However, the claims of novelty and significance are overstated, as they downplay existing literature on reward learning and explainable AI (XAI). The reproducibility of the findings could be called into question due to the lack of detailed descriptions of experimental setups and datasets, making it difficult for other researchers to validate the claims independently.\n\n# Summary Of The Review\nOverall, while the paper proposes an interesting approach to reward learning with tree models, its claims of groundbreaking advancements are not fully supported by the experimental data. The contributions are pitched as revolutionary, yet they seem to exaggerate both the novelty and performance improvements relative to existing methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"REWARD LEARNING WITH TREES: METHODS AND EVALUATION\" introduces a novel approach to learning reward functions from human feedback using interpretable tree models, as opposed to traditional deep neural networks (NNs). The authors demonstrate that their tree-based method is competitive in performance on complex high-dimensional tasks while offering enhanced interpretability. Through a series of experiments in an aircraft handling domain, the paper provides empirical evidence of the effectiveness of their approach, including improved median oracle regret ratios and robustness to error in human feedback.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to reward learning that emphasizes interpretability without sacrificing performance. The comparative analysis between tree-based models and NNs highlights the potential advantages of using trees in scenarios where understanding model decisions is crucial. However, the paper does have weaknesses, including the reliance on synthetic oracle preferences for evaluation, which may not fully capture the complexities of real-world human feedback. Additionally, while the performance metrics are promising, further validation in diverse environments would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of methodology, results, and conclusions. The authors articulate their contributions clearly, making it easy for readers to follow their arguments. The quality of the empirical evaluations is high, though the reproducibility could be a concern due to the specific nature of the experimental setups and the need for detailed descriptions of the tree growth algorithm modifications. The novelty of the approach is significant, as it combines the interpretability of decision trees with the complexities of reward learning, setting a new direction for future research.\n\n# Summary Of The Review\nOverall, this paper presents a compelling case for the use of tree-based models in reward learning, demonstrating that they can perform comparably to NNs while providing greater interpretability. The methodology is sound, although the reliance on synthetic data raises questions about real-world applicability. The findings lay important groundwork for future work in preference-based learning.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to preference-based reward learning using tree structures, claiming that this method enhances interpretability compared to traditional neural network models. The authors focus on generating preferences based on a Boltzmann-rational model and emphasize the minimization of negative log-likelihood and l0-1 loss as objectives for reward learning. Their findings suggest that the proposed method is competitive with neural network approaches in high-dimensional tasks, particularly within the context of aircraft handling.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its contribution to the interpretability of reward learning through tree-based models, which could potentially enhance human understanding of agent behavior. However, the paper has notable weaknesses, including assumptions around human feedback consistency and rationality, the oversimplification of tree structures for capturing complex human preferences, and a lack of engagement with the ethical implications of learned rewards. The robustness claims regarding limited or corrupted data also require further validation. Additionally, the findings may not generalize well beyond the specific tasks explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear exposition of methods and findings. However, the assumptions made regarding human preferences and the interpretation of results could benefit from more rigorous exploration. The novelty of using tree structures for interpretability in reward learning is noteworthy, yet the paper does not sufficiently address the broader implications of its findings, particularly concerning ethical considerations. Reproducibility is somewhat compromised by the reliance on specific metrics that may not capture all aspects of performance.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to preference-based reward learning through tree models, highlighting potential benefits in interpretability. However, the reliance on certain assumptions and the limited exploration of ethical implications detract from its overall impact. Further validation of robustness and generalizability is necessary.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel approach to learning reward functions from human feedback using interpretable tree models, as opposed to the opaque deep neural networks (NNs) typically employed. The methodology revolves around preference-based reward learning, where human preferences over trajectory pairs are used to infer reward functions. The authors introduce reward tree induction with a multi-stage approach, utilizing advanced splitting criteria to enhance the accuracy of preference reconstruction. Experimental results demonstrate that while NNs exhibit strong performance, the proposed tree models maintain competitive efficacy while offering superior interpretability, especially in challenging scenarios with limited or noisy data.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its focus on interpretability, a critical aspect often overlooked in reinforcement learning applications utilizing NNs. The proposed methodology for learning reward trees effectively addresses the limitations of conventional reward learning approaches, particularly in terms of human alignment and understanding of agent behavior. However, a potential weakness includes the reliance on preference data, which may not always be readily available or accurately represent human intent. Additionally, while the paper demonstrates robust performance in a specific domain, further empirical validation across diverse tasks would strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The writing is concise, and the technical details are sufficiently explained, making it accessible to a broad audience. The novelty of using tree models for reward learning is significant, particularly in enhancing interpretability. However, reproducibility could be a concern if the experimental setup and implementation details are not thoroughly documented, which should be ensured in future work.\n\n# Summary Of The Review\nOverall, this paper presents a compelling approach to interpretable reward learning through tree models, offering a promising alternative to traditional deep learning methods. The findings suggest that reward trees can effectively align AI systems with human preferences while maintaining a level of transparency that is crucial for ethical AI deployment.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework for enhancing the interpretability and performance of machine learning models within the context of healthcare predictions. The authors propose a hybrid approach that combines feature extraction techniques with interpretable model architectures. Their methodology emphasizes the integration of domain knowledge into model design, leading to improved prediction accuracy and transparency. The empirical findings demonstrate that the proposed method outperforms several existing state-of-the-art approaches on benchmark datasets, particularly in terms of interpretability and user trust.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Relevance**: The focus on healthcare predictions addresses a critical need for interpretable models in a field where decisions can have significant real-world consequences.\n2. **Innovation**: The hybrid approach is a novel contribution that effectively combines established techniques with new ideas, showcasing originality and potential for future exploration.\n3. **Empirical Validation**: The paper includes comprehensive experiments with clear results that validate the effectiveness of the proposed method across multiple datasets.\n4. **Clarity**: The writing is generally clear, with a well-structured format that logically presents the problem, methodology, results, and implications.\n5. **Broader Impact**: The implications for practical applications in healthcare suggest the research could lead to improved decision-making processes in clinical settings.\n\n**Weaknesses:**\n1. **Limited Comparison**: The paper could benefit from a more robust comparative analysis with a wider array of contemporary methods to strengthen claims of superiority.\n2. **Methodological Depth**: Some details regarding the implementation of the proposed framework are lacking, which may hinder reproducibility.\n3. **Scalability Concerns**: There is insufficient discussion regarding how the proposed method scales with larger datasets or more complex tasks.\n4. **Assumptions**: The model relies on specific assumptions about data distributions that may not hold in all real-world scenarios, potentially limiting its applicability.\n5. **Future Work**: While the authors hint at future research directions, a more detailed exploration of these avenues could enhance the paper's contribution to the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and organized, making it accessible to both experts and non-experts in the field. The novelty of the proposed framework is significant, as it introduces a new hybrid approach that combines feature extraction with interpretable modeling. However, the reproducibility of the results could be improved by providing more detailed methodological information and addressing the limitations of the proposed approach.\n\n# Summary Of The Review\nThis paper presents an innovative and relevant approach to enhancing interpretability in healthcare predictions using a hybrid model framework. While the empirical results are promising, further comparisons and methodological clarifications are needed to strengthen the paper's impact and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper proposes a novel approach to reward learning in reinforcement learning (RL) by utilizing interpretable tree models instead of traditional deep neural networks (DNNs). The authors develop a method for learning reward trees based on human preference labels, demonstrating that these trees can achieve competitive performance compared to neural networks, especially in complex tasks and scenarios with limited data. The key contributions include the introduction of reward trees as a more transparent alternative to black-box models, which enhances agent behavior interpretability and facilitates safety and accountability in RL applications.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its focus on interpretability, which addresses a crucial gap in existing reward learning methods that rely heavily on DNNs. The empirical results show that reward trees not only perform comparably to neural networks but also excel in nonlinear tasks and exhibit robustness to data corruption. However, the paper could benefit from a more extensive evaluation across a wider range of tasks to further validate the proposed method's effectiveness. Additionally, there may be limitations in the scalability of tree-based models for very high-dimensional action spaces compared to DNNs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The writing is accessible, making it easy to follow the proposed approach and results. The novel contribution of using interpretable tree models in reward learning is a significant advancement, particularly in the context of explainable AI (XAI). However, the reproducibility of results would benefit from providing more detailed descriptions of the experimental setups and parameters used in the evaluations.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of reward learning by introducing interpretable reward trees, which offer advantages in transparency and robustness. While the findings are promising, further validation across diverse scenarios is necessary to fully establish the method's effectiveness and scalability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to learning reward functions from human feedback through the use of tree-structured reward models, termed \"reward trees.\" The authors critique the opacity of deep neural networks in this context, proposing instead a method that enhances interpretability while maintaining competitive performance. The methodology includes a multi-stage tree induction process that integrates preference-based learning with online policy learning. Experimental results demonstrate that the tree models not only perform comparably to neural networks across various tasks but also provide insights into learned behaviors, showcasing robustness and interpretability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its contribution to the ongoing discourse on explainability in reinforcement learning (RL). The introduction of reward trees addresses significant challenges associated with black-box models, particularly in terms of transparency and safety. The preference-based approach is well-motivated and provides a clear advantage over traditional absolute reward learning. However, a potential weakness is the limited exploration of the scalability of the proposed models, particularly in highly complex environments where traditional neural networks excel. Additionally, while the qualitative insights into agent behavior are valuable, more quantitative metrics could strengthen the evaluation of interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections that logically lead the reader through the motivations, methodology, and results. The writing quality is high, and the figures effectively illustrate key concepts and findings. The novelty of the approach is significant; reward trees represent a fresh perspective in reward learning that could influence future research directions. The reproducibility of the results appears strong, as the authors provide detailed descriptions of their experimental setup and methodologies, although access to code or data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to reward learning that emphasizes interpretability without compromising performance. The introduction of tree-structured models is both innovative and timely, addressing critical gaps in the current understanding of reward functions in RL. While there are areas for improvement in the scalability assessment and quantitative evaluation of interpretability, the contributions are noteworthy and relevant.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Reward Learning with Trees: Methods and Evaluation\" presents a novel approach to learning reward functions from human feedback using interpretable tree models, as opposed to traditional deep neural networks (NNs). The authors propose a four-stage method for preference-based reward learning, which includes trajectory-level return estimation and tree growth strategies. The findings indicate that tree-based models not only achieve competitive performance with NNs on high-dimensional tasks but also offer robustness to data issues, enhanced interpretability, and better alignment with human preferences.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to reward learning using tree structures, which addresses the opacity often associated with neural networks. The methodology is well-structured and detailed, making the process of reward tree induction clear. The empirical results demonstrate the advantages of tree models, particularly in interpretability and robustness. However, a potential weakness is the reliance on human feedback, which may introduce biases and variability in the reward signals. Additionally, while the experiments are comprehensive, they could benefit from further exploration of the limitations of the proposed methods in more diverse or complex environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, methodology, and results. The quality of the methodology is high, with thorough explanations of the processes involved in learning reward trees. The novelty of using tree structures for reward learning is significant, particularly in the context of explainable AI. The reproducibility of the results is supported by detailed descriptions of the experimental setup, hyperparameters, and evaluation methodologies, although sharing code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a compelling approach to reward learning that emphasizes interpretability and robustness, with strong empirical results supporting its claims. The methodology is well-defined, and the contributions to the field of explainable AI in reinforcement learning are noteworthy. However, the reliance on human feedback may pose challenges in certain applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward learning in Reinforcement Learning (RL) through the use of interpretable reward trees, which address the challenges of manual reward function design and the opacity of black-box neural networks. The authors propose a preference-based reward learning framework that utilizes human feedback to generate pairwise preference labels, which are then used to construct reward trees through a series of optimization techniques, including trajectory-level return estimation, leaf-level reward prediction, tree growth, and pruning. The findings demonstrate that reward trees achieve comparable performance to neural networks while providing enhanced interpretability, thereby facilitating better alignment of agent behavior with human preferences.\n\n# Strength And Weaknesses\nStrengths of the paper include its methodological innovation in leveraging interpretable reward trees, which address critical issues related to transparency and verification in RL. The application of a probabilistic model that adheres to the Bradley-Terry framework is well-grounded and effectively connects human feedback to reward learning. However, the paper could benefit from more detailed empirical comparisons against existing state-of-the-art methods, particularly in diverse environments with varying complexity. Additionally, while the theoretical foundations are robust, practical implementation challenges in real-world applications are not extensively addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodologies and mathematical formulations. The organization of the content allows for a logical flow from problem definition to the proposed solution and experimental results. However, some sections could benefit from additional context or examples to enhance understanding, particularly the intricacies of the tree growth and pruning processes. The reproducibility of the work is supported by comprehensive appendices detailing methodological changes, hyperparameter settings, and implementation specifics.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of reward learning by introducing interpretable reward trees that enhance agent transparency and alignment with human preferences. While the theoretical approach is solid and the results are promising, further empirical evaluation against existing methods could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the use of interpretable tree models for reward learning, contrasting with traditional deep neural networks (DNNs). The authors propose a methodology that involves multiple stages of estimation for inducing reward trees, aiming to enhance interpretability and robustness compared to DNNs. However, the findings suggest that while reward trees may perform competitively in some contexts, they often yield only marginal improvements or even declines in performance relative to established neural network approaches.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its focus on interpretability and the potential advantages of tree models in reward learning. However, these strengths are undermined by several weaknesses. The motivation for departing from DNNs is inadequately justified, and the supposed interpretability benefits of tree models lack empirical support. The complexity of the proposed methodology raises questions about its necessity and effectiveness compared to existing simpler approaches. Moreover, the empirical validation is limited, primarily relying on synthetic oracle preferences that may not accurately reflect real-world scenarios, casting doubt on the practical applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper exhibits moderate clarity, but the convoluted methodology could hinder comprehension. The quality of the experiments is questionable, given their reliance on idealized conditions that do not mimic the complexities of real-world human feedback. The novelty of the approach is not convincingly demonstrated, as the paper does not clearly delineate how the proposed tree models improve upon or differ from existing literature. Reproducibility is also a concern due to the limited scope of experiments and the lack of rigorous validation under realistic conditions.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing exploration of reward learning with tree models but fails to provide compelling evidence for their superiority over DNNs. The contributions are muddied by a lack of clear differentiation, insufficient empirical validation, and an over-reliance on theoretical claims lacking rigorous support.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward learning by utilizing tree-structured models instead of traditional black-box neural networks. The authors demonstrate that these reward trees can effectively learn robust reward functions from human feedback in complex environments, offering increased transparency and interpretability. Key findings indicate that reward trees are competitive with neural networks, robust against limited or corrupted data, and can be applied across various industries, including robotics and autonomous vehicles.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative use of tree models, which enhance interpretability and provide clearer insights into agent behavior. The effective learning of reward functions through preference-based methods is a significant advancement over existing approaches. However, a potential weakness is the limited exploration of the framework's performance in real-world scenarios, as the current evaluation primarily focuses on simulated environments. Future work should address this gap to validate the practical impact of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, contributing to its overall clarity. The quality of the work is high, with a robust evaluation of the proposed approach against existing methods. The novelty of using tree-structured models for reward learning is significant, and the authors provide sufficient detail for reproducibility, although additional real-world experiments would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper introduces a transformative approach to reward learning that emphasizes interpretability and robustness, making it a valuable contribution to the field of AI. The potential applications across various domains highlight its significance, although further validation in real-world contexts is necessary to fully assess its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper investigates the use of interpretable tree models for reward learning in reinforcement learning (RL), aiming to address the opacity of deep neural networks (DNNs) that complicates understanding agent behavior and alignment with human values. The authors propose a structured framework for reward tree induction that incorporates human feedback to construct reward functions that accurately reflect human preferences. Key findings suggest that reward trees enhance interpretability and traceability in RL, provide a systematic method for reward learning, and hold promise for aligning agents more closely with human normative behavior.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contribution to the field of reward learning, particularly the introduction of reward tree models that facilitate interpretability—an essential quality in RL systems. The methodology is robust, grounded in well-established psychological theories regarding human preference judgments, and incorporates a systematic approach to reward tree induction. However, the paper could benefit from empirical validation of the proposed methods, as the theoretical framework is not accompanied by concrete experimental results demonstrating the efficacy of the reward trees in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of concepts and methodologies. The quality of the theoretical framework is high, drawing from relevant literature and establishing a solid foundation for the proposed approach. In terms of novelty, the use of interpretable tree models for reward learning is a fresh perspective that contributes significantly to the field. However, the reproducibility of the proposed work may be hindered due to the lack of detailed experimental procedures or datasets that would allow other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a valuable theoretical contribution to reward learning by advocating for interpretable tree models, which enhance understanding and alignment with human preferences. While the clarity and theoretical rigor are commendable, the absence of empirical validation limits the practical applicability of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward learning in reinforcement learning (RL) by introducing a reward tree induction method. This method utilizes a multi-stage process involving trajectory-level return estimation, leaf-level reward prediction, tree growth based on loss minimization, and tree pruning for enhanced interpretability. The authors integrate this approach within an online learning framework that allows for dynamic updates using new trajectories and preferences, significantly improving efficiency by using the PETS algorithm for model-based RL. Experimental results demonstrate the effectiveness of the proposed method across three aircraft handling tasks, highlighting its potential for reducing environment interactions compared to traditional model-free methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear focus on interpretability in reward learning and the comprehensive methodology that combines aspects of online learning and model-based RL. The detailed experimental setup and metrics, such as the oracle regret ratio, provide robust evidence of the method’s effectiveness. However, the paper may lack a deeper exploration of the implications of interpretability in practical applications, as the focus is heavily weighted towards algorithmic improvements and technical details. Additionally, the reliance on synthetic oracle preferences in experiments may limit the generalizability of the results to real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections detailing the background, methodology, experiments, and results. The implementation details are thoroughly documented, including hyperparameters and supplementary materials that enhance reproducibility. The novelty of the reward tree induction method is significant, offering a fresh perspective on reward learning. However, the depth of explanation regarding the interpretability aspects could be improved to further clarify its importance in the context of the proposed methodology.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of reinforcement learning by introducing an innovative method for reward learning that emphasizes interpretability and efficiency. While the methodology and experimental results are well-presented, the potential real-world applicability and broader implications of the findings could be explored more thoroughly.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel method for learning reward functions using tree-based models, positing that this approach offers greater interpretability compared to the prevalent use of deep neural networks (NNs). The authors emphasize their method's competitive performance in certain tasks, particularly in the aircraft handling domain, while proposing a new split criterion for tree growth. However, the paper's findings are limited by a lack of comprehensive comparative analysis with NNs, which have shown superior performance in complex reinforcement learning environments.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its focus on interpretability, addressing a critical area of concern in machine learning. The introduction of a new split criterion for tree growth is an interesting contribution, potentially improving the methodological framework for reward learning. However, the paper exhibits significant weaknesses, particularly in its inadequate comparison with NNs. The selective domain choice and lack of acknowledgment of prior work's limitations detract from the overall credibility of the claims. Additionally, the paper fails to sufficiently contextualize its contributions within the broader landscape of advancements in NNs and emerging explainable AI (XAI) techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally adequate, but the lack of balanced comparisons with existing methodologies undermines the quality of the argumentation. While the novelty of using tree-based models for reward learning is noteworthy, the paper does not convincingly articulate how these contributions advance the field relative to established NNs. Reproducibility may be challenging due to the limited details provided regarding experimental setups and the selective nature of the chosen tasks.\n\n# Summary Of The Review\nOverall, the paper presents intriguing insights into tree-based reward learning, but it falls short in providing a balanced evaluation of its proposed method against the well-documented capabilities of NNs. A more comprehensive comparative analysis and acknowledgment of existing advancements in the field would significantly enhance the paper's contributions and credibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled **\"REWARD LEARNING WITH TREES: METHODS AND EVALUATION\"** presents an innovative approach to reward learning through the use of reward trees, which enhances the interpretability of learned policies. The methodology involves constructing compact reward trees that facilitate both training and decision-making processes in reinforcement learning scenarios. The authors conduct extensive empirical evaluations demonstrating that their method not only maintains competitive performance compared to existing techniques but also significantly improves transparency, allowing for better understanding and trust in AI systems.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear articulation of the reward tree framework and the comprehensive experimental validation that supports the proposed methods. The authors effectively demonstrate the advantages of reward trees in terms of interpretability and performance. However, one weakness is the limited discussion on the scalability of their approach to more complex environments, which may restrict its applicability in real-world scenarios. Additionally, while the paper presents promising results, further exploration of the limitations and potential biases of the reward tree construction could strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to a wide audience. The quality of the figures and tables is high, contributing to the overall clarity of the findings. The novelty of the approach is evident, as it introduces a unique framework for reward learning that has not been previously explored in the literature. The reproducibility of the experiments is supported by detailed descriptions of the methodologies used and the provision of supplementary materials, although sharing code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of reward learning by introducing an interpretable approach through reward trees. It is well-executed, with strong empirical support, though it would benefit from a deeper analysis of scalability and potential limitations. The findings have significant implications for the development of transparent AI systems.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward learning using tree models, focusing on aircraft handling as the primary application. The authors propose improvements to the reward tree learning algorithm with an emphasis on interpretability and understanding learned preferences. However, the methodology is primarily validated through synthetic oracle preferences, limiting insights into real-world applicability. The findings suggest that the tree structure enhances interpretability, but the authors do not extensively discuss practical implementations or the broader implications of their work.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to reward learning and its emphasis on interpretability, which could potentially aid in the understanding of agent behavior. However, several weaknesses are evident: the limited scope of experiments to synthetic preferences, the lack of discussion on ethical implications and practical applications of the model, and an absence of comparative analysis with existing methods. Furthermore, the paper does not address long-term implications of preference-based reward learning, nor does it explore how the method can adapt to dynamic environments or multi-agent systems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but clarity could be improved by elaborating on the practical implications of the proposed tree structures. The novelty is notable in the context of reward learning, yet the lack of a comparative framework diminishes its significance. Reproducibility is also a concern, as the experiments rely solely on synthetic data, which may not translate effectively to real-world scenarios.\n\n# Summary Of The Review\nWhile the paper introduces an interesting approach to reward learning through tree structures, it falls short in terms of generalizability and practical relevance. The lack of empirical validation with real-world data and insufficient exploration of the broader implications limit its overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a comparative analysis of interpretable tree models and black-box neural networks (NNs) for learning reward functions from human feedback in reinforcement learning (RL) systems. It proposes a novel multi-stage method for inducing a tree structure to represent reward functions, emphasizing the importance of interpretability for safety and accountability. The findings demonstrate that while NNs generally outperform tree models, the latter remain competitive across various high-dimensional tasks, and their robustness against preference noise is highlighted through sensitivity analyses.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating interpretable tree structures into reward learning, which addresses critical issues of transparency in RL. The methodology is well-defined, with a focus on minimizing negative log-likelihood loss and utilizing a robust statistical framework for evaluation. However, the paper's weaknesses include the performance disparity between tree models and NNs, which may undermine the perceived advantages of interpretability. Additionally, while the statistical analyses are thorough, there could be greater emphasis on practical implications and real-world validation of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, with clear explanations of the proposed methodologies and findings. The quality of the empirical evaluations is high, employing robust statistical tests to substantiate claims. The novelty of the approach in applying tree structures to reward learning is commendable, although the extent of its significance may be moderated by the performance of competing methods. Reproducibility is supported by detailed methodological descriptions, though further elaboration on dataset specifications and experimental setups would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper provides a compelling exploration of interpretable models in reward learning, with significant contributions to the field. While the findings suggest that tree models offer competitive performance, the inherent limitations compared to NNs may pose challenges in advocating for their broader adoption. The paper is well-structured, but further empirical validation in real-world contexts is necessary to bolster its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward learning using a tree-structured model that aims to improve the interpretability of learned reward functions. The methodology involves constructing reward trees that can capture complex preference structures and provide insights into the decision-making process. The findings indicate that the proposed method demonstrates robustness to limited or corrupted data, although empirical validation primarily relies on synthetic oracle preferences rather than real human feedback.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its contribution to the interpretability of reward learning, offering a structured way to represent preferences. However, the paper has several weaknesses. It does not explore alternative tree structures, limiting its versatility. The evaluation lacks a comprehensive comparison with other reward learning approaches, which could enhance the generalizability of the results. Additionally, the reliance on synthetic data raises concerns about the applicability of the findings in real-world scenarios. The paper also fails to address biases in preference elicitation and does not provide sufficient analysis of its computational costs or scalability in complex environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a well-structured presentation of ideas. However, the quality of the evaluation is undermined by the limited scope of empirical testing and the lack of real-world validation. The novelty is evident in the proposed tree structure, but the overall contribution is somewhat diminished by the absence of a broader context and alternative methods. Reproducibility is hindered by the lack of detailed methodologies for implementation and the absence of a comprehensive analysis of practical deployment challenges.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to reward learning with a focus on interpretability, but its contributions are limited by a narrow evaluation scope and insufficient exploration of practical implications. The findings, while promising, require further validation in real-world contexts to assess their effectiveness and applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Reward Learning with Trees\" proposes a novel approach to reward learning using decision trees to enhance interpretability over traditional neural network models. The methodology builds upon the framework established by Bewley & Lecue, introducing a four-stage induction method aimed at refining reward design in reinforcement learning contexts. The findings suggest that while the proposed tree-based model achieves broadly competitive performance against neural networks, it emphasizes the advantages of interpretability, albeit with limited empirical novelty.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its focus on interpretability, a crucial aspect in the context of AI safety and human-centered design. The adaptation of existing methods and the systematic approach to reward learning demonstrate a solid understanding of the challenges in the field. However, the weaknesses are significant: the contributions appear incremental and lack substantial innovation. The reliance on synthetic oracles for experimentation and the comparison to neural networks feels somewhat conventional, reducing the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly; however, the novelty of the contributions is questionable. Much of the content revisits established concepts in reward learning and interpretability without offering groundbreaking insights. While the methodology is reproducible, the incremental nature of the work may lead to limited interest from researchers seeking novel approaches in reward design.\n\n# Summary Of The Review\nOverall, this paper presents a familiar narrative regarding reward learning and interpretability, offering little in terms of groundbreaking contributions. While it provides a clear framework and systematic methodology, the work largely reiterates existing knowledge without pushing the boundaries of the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces reward trees as an alternative to traditional neural networks (NNs) for learning reward functions from human feedback, focusing on the interpretability of the resulting models. The authors highlight the robustness of reward trees against limited or corrupted data, suggesting their potential for data-efficient learning methods. By emphasizing preference-based reward learning, the paper lays the groundwork for future research into advanced preference elicitation techniques and the integration of tree structures with model-based reinforcement learning (MBRL) agents. Empirical results indicate that while reward trees exhibit minor performance drops compared to NNs, their interpretability and robustness present significant advantages, particularly for trustworthy agent alignment.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to combining interpretability with reward learning, which is crucial for building trustworthy AI systems. The authors' emphasis on robustness in the face of limited data is also commendable, presenting an opportunity for further exploration in data-efficient learning. However, a notable weakness is the lack of exploration into hybrid models that could leverage the strengths of both neural networks and reward trees. Additionally, while the empirical results are promising, the performance comparison with NNs warrants deeper investigation, particularly regarding ensemble methods that could enhance performance without sacrificing interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodologies. The quality of the writing is high, making complex ideas accessible. The novelty of the approach is significant, especially in the context of interpretability and data robustness. However, the reproducibility of the findings could be improved by providing more detailed descriptions of the experimental setup and the datasets used, which would facilitate replication of the results by other researchers.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for reward trees as a viable alternative to neural networks in reward function learning, emphasizing their interpretability and robustness. While the contributions are valuable, there are opportunities for further research into hybrid models and ensemble methods that could enhance performance. The foundation laid for future work in this domain is promising and could lead to significant advancements in the field of AI safety and alignment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel method for reward tree learning, which aims to provide interpretable models for decision-making in reinforcement learning tasks. The authors conduct experiments comparing their proposed method against neural networks (NNs) on three aircraft handling tasks: Follow, Chase, and Land. The findings indicate that while reward trees yield interpretable results, they generally fall short of NNs in terms of performance, with higher Oracle Regret Ratio (ORR) values suggesting inferior alignment with the oracle reward. The paper also discusses the performance variability of the tree models, their robustness to noise, and the potential advantages in data-scarce environments.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its focus on interpretability, a critical aspect often overlooked in deep learning approaches. The use of reward trees provides a transparent model that can help understand agent behavior, which is an essential contribution to the field. However, the paper also highlights a significant weakness: the performance of the proposed method is consistently outperformed by NNs, particularly in terms of convergence speed and overall effectiveness. The variability in tree performance raises concerns about their reliability, making it difficult to advocate for their use in scenarios where consistent performance is required.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making it accessible to readers. The quality of the experiments is satisfactory, with comprehensive comparisons drawn between reward trees and NNs across multiple tasks. However, while the novelty of the reward tree approach is acknowledged, the empirical evidence suggests that it does not significantly advance the state-of-the-art in reinforcement learning. Reproducibility is supported by the clear presentation of results and methodology, although further detail on implementation specifics would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to reward tree learning with an emphasis on interpretability, but it ultimately falls short in performance compared to neural networks. The findings suggest that while reward trees have potential applications, especially in contexts requiring transparency, their current utility may be limited due to inconsistent performance. Further research is needed to improve their effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to reward design in reinforcement learning through the introduction of \"reward trees,\" a structured method that facilitates the specification of complex reward functions. The authors propose a methodology that leverages tree-structured representations to simplify the design process and enhance the learning efficiency of agents. Empirical results demonstrate that agents trained with reward trees achieve superior performance in various benchmarks compared to those using traditional reward mechanisms.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to reward design, which addresses a significant challenge in reinforcement learning—manual reward specification. The use of reward trees allows for a more intuitive and flexible representation of rewards, potentially broadening the applicability of reinforcement learning in complex environments. However, there are notable weaknesses, including a lengthy introduction that lacks concise articulation of the challenges addressed, and the presence of excessive jargon that may alienate readers unfamiliar with the field. Furthermore, some figures require clearer explanations to properly convey their relevance.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the overall structure of the paper is sound, clarity is hindered by complex sentence structures and repetitive phrasing. The methodology is described in detail, yet it could benefit from summarization in the main text, guiding readers more effectively. The novelty is evident in the introduction of reward trees, but articulating their advantages over existing methods requires further emphasis. The reproducibility of the results is supported by comprehensive methodological details, but the presentation could be improved with clearer visual aids and consistent terminology.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of reinforcement learning by proposing a novel approach to reward design. While the methodology and findings are promising, the clarity and presentation of the paper could be improved to enhance accessibility and understanding.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.618144185434157,
    -1.8259948640113604,
    -1.9172799929747872,
    -1.8186134762289778,
    -1.8061924138506973,
    -1.8272687972370372,
    -1.4536720955001692,
    -2.042803955543093,
    -1.7157976482994894,
    -1.9396355068380093,
    -1.848690374310328,
    -1.3995785933903242,
    -1.6276672603970561,
    -1.743753056779029,
    -1.709417128723867,
    -1.8263128868031597,
    -1.8564387467964785,
    -1.6158133345679264,
    -1.7139504750110395,
    -1.8788993680536188,
    -1.8770442634894209,
    -1.508017714909503,
    -1.8426559413354429,
    -1.8801933321192958,
    -1.6799475123560108,
    -2.183631911398181,
    -1.6337431475967976,
    -1.800463533218698,
    -1.706953011622758
  ],
  "logp_cond": [
    [
      0.0,
      -2.4015247608833947,
      -2.369757877908004,
      -2.409081026913979,
      -2.3872578727899834,
      -2.4003458647341454,
      -2.471868968797237,
      -2.4483175171776184,
      -2.3901866370828846,
      -2.3985130026447834,
      -2.3946709908029877,
      -2.5079820261216073,
      -2.420394178246951,
      -2.4254147521179448,
      -2.3668745776156364,
      -2.3391135027084164,
      -2.413241928682942,
      -2.4204483368134144,
      -2.45452131260071,
      -2.364586098853715,
      -2.3933485198054116,
      -2.4274226005747517,
      -2.415183777673899,
      -2.443521739903768,
      -2.4562887747234656,
      -2.410438071117628,
      -2.419020370762157,
      -2.36547679617845,
      -2.4457270742544326
    ],
    [
      -1.4598719016295345,
      0.0,
      -1.3902265963792826,
      -1.3773277241933828,
      -1.3685912695380906,
      -1.4077787052234194,
      -1.480946049616859,
      -1.4145477708434668,
      -1.3756962021832457,
      -1.4156251788825973,
      -1.400890157923409,
      -1.528039539270649,
      -1.3927786755835032,
      -1.391040519345753,
      -1.34859979742311,
      -1.3233308966956128,
      -1.4374373302095953,
      -1.3954294488252579,
      -1.4372832202151036,
      -1.3952523936487353,
      -1.4579396357544612,
      -1.4813728411507467,
      -1.4721238003302521,
      -1.4085409368212898,
      -1.4213539119692533,
      -1.414125472679252,
      -1.4247396462274282,
      -1.4091953250855889,
      -1.4741288254648857
    ],
    [
      -1.5447380270908628,
      -1.5267305053535265,
      0.0,
      -1.4082652441166743,
      -1.4363716299147877,
      -1.4990379431827685,
      -1.5780808510581015,
      -1.5477621897172191,
      -1.4615771160769246,
      -1.4890967404898803,
      -1.5044288920875748,
      -1.648914923766505,
      -1.558349978395841,
      -1.5171879427554391,
      -1.4667520047524458,
      -1.5374102515338943,
      -1.523365071878424,
      -1.5439394626827736,
      -1.554561429651398,
      -1.5532558304174215,
      -1.5194629845545835,
      -1.554995449550492,
      -1.5291430731507356,
      -1.5428797349433299,
      -1.5255431944272868,
      -1.4380243222769111,
      -1.5243695141335687,
      -1.5660864983396476,
      -1.5822739720992351
    ],
    [
      -1.486880995826345,
      -1.3612213927087882,
      -1.2772948508646431,
      0.0,
      -1.3951362332963437,
      -1.3954989549907528,
      -1.4828951730605375,
      -1.4143094601207553,
      -1.3827970945510828,
      -1.431971472760421,
      -1.38564937855879,
      -1.558048486933464,
      -1.4179547617830226,
      -1.4039031447862051,
      -1.3469870909331012,
      -1.384247524578794,
      -1.3739417760115915,
      -1.423598445413538,
      -1.4312668115329232,
      -1.4201532419676202,
      -1.4525535819709068,
      -1.5110190201878089,
      -1.477547526694774,
      -1.3607276580693517,
      -1.3908213471847433,
      -1.2605655974522503,
      -1.3868612389621364,
      -1.4619323047777895,
      -1.5312447279634047
    ],
    [
      -1.4793064437628654,
      -1.4044572798592667,
      -1.2746870379486495,
      -1.4084304555143503,
      0.0,
      -1.3634722109823503,
      -1.4750294738423646,
      -1.4392735942868935,
      -1.2874187681684095,
      -1.3257787335839464,
      -1.4241278861868532,
      -1.498960820178824,
      -1.4240855812917348,
      -1.5038253889487743,
      -1.4243090310881459,
      -1.4739820432027404,
      -1.4137942142260318,
      -1.4648206699014579,
      -1.461918118025694,
      -1.457851760992422,
      -1.4101663382999805,
      -1.4956856794944806,
      -1.3454590639688022,
      -1.4149604380993908,
      -1.4045933801889885,
      -1.3752221334959978,
      -1.4778452520569354,
      -1.4079557440722417,
      -1.4996659745306504
    ],
    [
      -1.4497778110917872,
      -1.418368693555662,
      -1.3274380469514382,
      -1.328850843741476,
      -1.3561206936251196,
      0.0,
      -1.5139591191288948,
      -1.4344359020109154,
      -1.2909781392425173,
      -1.3611637914627839,
      -1.318289791041329,
      -1.5916191449151587,
      -1.426676328787417,
      -1.4387167356587502,
      -1.420692959182693,
      -1.4742232672656381,
      -1.3798146817630392,
      -1.3889849861113763,
      -1.4861083999744753,
      -1.4093000512153295,
      -1.4084988088407273,
      -1.459410334558262,
      -1.339976866897545,
      -1.4719531748362307,
      -1.4206993296220354,
      -1.4375939982421995,
      -1.4420710737600395,
      -1.4621417711479965,
      -1.510871921214719
    ],
    [
      -1.1900528210741042,
      -1.123766483996101,
      -1.1127357758539302,
      -1.1141024792250793,
      -1.1092650291971655,
      -1.1635723229994441,
      0.0,
      -1.0999455800932172,
      -1.1625524790246615,
      -1.1081175323221943,
      -1.1685396879293124,
      -1.184699569457668,
      -1.1718743825149316,
      -1.1243089797940022,
      -1.116048269862045,
      -1.1335110041594898,
      -1.1534561543723234,
      -1.1706359865288132,
      -1.1320991514489365,
      -1.1267153654049311,
      -1.0920903580055983,
      -1.1728291832169155,
      -1.1543086723931058,
      -1.0959099205397094,
      -1.1182653950447055,
      -1.1218506357325237,
      -1.1638894762704852,
      -1.1442131282338432,
      -1.1832569632267533
    ],
    [
      -1.743041765917538,
      -1.6283087442980275,
      -1.640382241067134,
      -1.6420946491344137,
      -1.633963679583139,
      -1.6850012219829995,
      -1.6885565850617992,
      0.0,
      -1.6787278157334904,
      -1.630673593134581,
      -1.6879218147974504,
      -1.7858522233683471,
      -1.6607407814526611,
      -1.6500644498006136,
      -1.6396493185767755,
      -1.684920160749122,
      -1.665952776964576,
      -1.6762139068598743,
      -1.655906697459848,
      -1.6714040083430097,
      -1.6546188318457846,
      -1.7313080874471434,
      -1.6800558049781347,
      -1.679893694742504,
      -1.6878805790351126,
      -1.6440947138489028,
      -1.6548871558394584,
      -1.674500546580744,
      -1.7545610426359306
    ],
    [
      -1.410991594790793,
      -1.304312705751274,
      -1.2383123583036284,
      -1.2876221468567934,
      -1.2034526062217894,
      -1.2457655737394457,
      -1.4353827884703922,
      -1.373159260851528,
      0.0,
      -1.306172901607093,
      -1.2907830319456095,
      -1.4799243960172372,
      -1.3375112120576413,
      -1.3729205376186335,
      -1.2790518231964907,
      -1.3602147957277606,
      -1.2883540879122923,
      -1.3408080915621212,
      -1.3935044228584241,
      -1.3286137317052142,
      -1.2997971191235502,
      -1.3242668577124177,
      -1.2724966305816146,
      -1.3417519948085013,
      -1.3855172697667524,
      -1.3517529858275685,
      -1.3402581745217244,
      -1.3147602511455716,
      -1.4068787741487063
    ],
    [
      -1.5733657349885393,
      -1.4857102522200452,
      -1.4560311312260954,
      -1.467940533074832,
      -1.432397844967014,
      -1.4845253679215538,
      -1.5903759611598218,
      -1.4547851411800832,
      -1.4938367999975777,
      0.0,
      -1.5057777359964821,
      -1.6507912673677148,
      -1.5122410408369094,
      -1.5403475750587314,
      -1.4588770394537138,
      -1.5209306032987602,
      -1.5344044971999211,
      -1.4667919663626012,
      -1.5621815296257722,
      -1.5189796348819766,
      -1.5218660464270801,
      -1.5884094805504831,
      -1.499008080319837,
      -1.5257481772642822,
      -1.476759935584307,
      -1.530709754799506,
      -1.4676291410007682,
      -1.5367458729396697,
      -1.6073178424527752
    ],
    [
      -1.4830584510796807,
      -1.3646575735403166,
      -1.3589252250108304,
      -1.377423277141344,
      -1.3690053036185914,
      -1.323272279980198,
      -1.5136093457079334,
      -1.4743060222060604,
      -1.3535277973294393,
      -1.4299642757221953,
      0.0,
      -1.5768432614597,
      -1.367792251942298,
      -1.4290180560381138,
      -1.3687310814770899,
      -1.3929757766171391,
      -1.3746287224941747,
      -1.3777520768533593,
      -1.4132236477771667,
      -1.4657662973190646,
      -1.4403573944071815,
      -1.4797754855533864,
      -1.3985083860259286,
      -1.5067322847991775,
      -1.4664242012527444,
      -1.4482515332847599,
      -1.3901893508946146,
      -1.4584456426725545,
      -1.4713061091363677
    ],
    [
      -1.2356291347294208,
      -1.2012510620535635,
      -1.2084901971214246,
      -1.1918594131472329,
      -1.1693408756060883,
      -1.2040961671773776,
      -1.1740894918444023,
      -1.1732152502134572,
      -1.2005425932934641,
      -1.1813993280178803,
      -1.2001721571508555,
      0.0,
      -1.2007294267802728,
      -1.192527529770169,
      -1.2051073696272314,
      -1.1792608304950216,
      -1.2127760424475393,
      -1.2293003258270716,
      -1.2072251619423016,
      -1.2086993565796182,
      -1.187884135907932,
      -1.1917345108123656,
      -1.1980406685848943,
      -1.1976060721279014,
      -1.1912089977736264,
      -1.1778248881189715,
      -1.1954770130949044,
      -1.2000509474040102,
      -1.1829897826029625
    ],
    [
      -1.2909277786785522,
      -1.133510987308256,
      -1.1188926804046793,
      -1.1290382331793931,
      -1.1415973531814705,
      -1.197673835823902,
      -1.2963944058879155,
      -1.1599782342030973,
      -1.1567609678569433,
      -1.202193175560864,
      -1.1417109264379077,
      -1.3439246203635673,
      0.0,
      -1.2124927832689036,
      -1.1570237837989559,
      -1.1835896383005133,
      -1.1436351698115093,
      -1.1254433825629222,
      -1.2475720096648473,
      -1.2738050153662224,
      -1.218057120065905,
      -1.2491397005869889,
      -1.2063008793561067,
      -1.2193192580754781,
      -1.253185365702321,
      -1.2081241448969195,
      -1.207610987697445,
      -1.2078795429661398,
      -1.2544474668151036
    ],
    [
      -1.4210422827330478,
      -1.273760242370712,
      -1.2876713250522922,
      -1.2846852426215498,
      -1.3249185022664978,
      -1.3034154645803664,
      -1.3807162540903681,
      -1.324746429928039,
      -1.3685269224877747,
      -1.3576666816954917,
      -1.3299111651423217,
      -1.4598201103356567,
      -1.331522930245056,
      0.0,
      -1.2594670305314768,
      -1.309348157480828,
      -1.3766913487523968,
      -1.3315823492067338,
      -1.3506042016516504,
      -1.3576057255408525,
      -1.3751679016082259,
      -1.3377995839142076,
      -1.3779957661686313,
      -1.308387672560495,
      -1.3670298441802726,
      -1.3215106258926912,
      -1.3102310019284704,
      -1.3743178124873106,
      -1.3759500717005304
    ],
    [
      -1.303472938938481,
      -1.2201292194819904,
      -1.2422638798680874,
      -1.2240812269976968,
      -1.252683570725007,
      -1.3333461961781556,
      -1.3746590964295275,
      -1.2941292607187298,
      -1.2394704978578073,
      -1.2941704273549728,
      -1.2433575529302747,
      -1.4225099819790388,
      -1.2823027264491462,
      -1.2625875436164264,
      0.0,
      -1.2139739487613121,
      -1.283412365751201,
      -1.3273928035214129,
      -1.3013468450541337,
      -1.2870652301151777,
      -1.3275024185392579,
      -1.31056721518579,
      -1.366093764898326,
      -1.2828712663383703,
      -1.3538737597633614,
      -1.2587734440207898,
      -1.2900131510490906,
      -1.2969350641596566,
      -1.3464645653765166
    ],
    [
      -1.4285424174665957,
      -1.3987503889687867,
      -1.433291342867568,
      -1.4440410069253984,
      -1.5074015999590629,
      -1.500256697585988,
      -1.537426168588335,
      -1.4834963251598041,
      -1.5252193488151407,
      -1.466217835429588,
      -1.4700622032733817,
      -1.5844775773341564,
      -1.4829693313292036,
      -1.4675366513687367,
      -1.4123439095756016,
      0.0,
      -1.5138536194781238,
      -1.4742930724451757,
      -1.443473191780068,
      -1.3573674626971195,
      -1.5338729887594102,
      -1.5176441472232123,
      -1.5129493740992497,
      -1.490956627718195,
      -1.5093753958490863,
      -1.4589547835114158,
      -1.4697537321931162,
      -1.515655832767101,
      -1.515917455749063
    ],
    [
      -1.5017129712993214,
      -1.4465659742568016,
      -1.425308707864689,
      -1.3858098253852826,
      -1.4622296904154735,
      -1.4404447262592048,
      -1.578796931943918,
      -1.4952783256289992,
      -1.4051435742957823,
      -1.4856609428781211,
      -1.4076796730988028,
      -1.6507446985521672,
      -1.4520031734397565,
      -1.5018188559429948,
      -1.4399836757576987,
      -1.5049163751544599,
      0.0,
      -1.4652904070610073,
      -1.4897599252140643,
      -1.494912987196698,
      -1.5297534173550473,
      -1.5343076127301385,
      -1.43911395282787,
      -1.5043755911381946,
      -1.495257204903448,
      -1.4954352869216936,
      -1.5304457660888329,
      -1.5322891405211139,
      -1.5169035867748437
    ],
    [
      -1.2452852219357735,
      -1.134202524921122,
      -1.1385631324548755,
      -1.0976644826826099,
      -1.20395768957257,
      -1.1208945689482703,
      -1.2824477656646038,
      -1.2312659232895515,
      -1.1555184574996926,
      -1.1664401276577225,
      -1.1032054104336504,
      -1.333481879552923,
      -1.0968451646427484,
      -1.1629341532753954,
      -1.1839264892321857,
      -1.1650991434543188,
      -1.172920302937145,
      0.0,
      -1.188124935090091,
      -1.234328131074516,
      -1.2359354802313642,
      -1.2052959253652502,
      -1.165189744180596,
      -1.2044072805433266,
      -1.135289360832135,
      -1.2229997013627945,
      -1.0998745052681163,
      -1.2343111578785488,
      -1.2432294873783807
    ],
    [
      -1.4010551293459008,
      -1.3170731007509417,
      -1.2971175230828096,
      -1.3074220862011376,
      -1.3206278941571334,
      -1.361962637149069,
      -1.38513913901888,
      -1.3241168937254,
      -1.360766027305279,
      -1.3644963598528912,
      -1.348781211755573,
      -1.4367126110168127,
      -1.3346520814451897,
      -1.3850195718448683,
      -1.292491240393741,
      -1.3297707822363163,
      -1.2991721432134367,
      -1.3668839559870456,
      0.0,
      -1.3661123859529245,
      -1.3556981322317385,
      -1.3600535983788613,
      -1.3330653001761281,
      -1.348403342678009,
      -1.3360782135253697,
      -1.3073568438728387,
      -1.339207652356602,
      -1.33764811340476,
      -1.383004649426037
    ],
    [
      -1.4448683281950128,
      -1.4721672296537494,
      -1.472966411678379,
      -1.4902655273117669,
      -1.4637180720414498,
      -1.4885198937857684,
      -1.5664019895972139,
      -1.5024834239913503,
      -1.4498240393611652,
      -1.4817863308905965,
      -1.5246916034305122,
      -1.6228037301075642,
      -1.5471880382698782,
      -1.5209363840407795,
      -1.4602685274294915,
      -1.4101916750851688,
      -1.5106077440670427,
      -1.5524511444500315,
      -1.5198275627697584,
      0.0,
      -1.5103141606335722,
      -1.568143426763864,
      -1.475410828092718,
      -1.525441193468505,
      -1.4927558968290373,
      -1.4763039981176376,
      -1.5456647155440542,
      -1.4536861265642422,
      -1.5858375380364298
    ],
    [
      -1.4771557157545163,
      -1.5334249144877037,
      -1.4531336363161471,
      -1.4829741239625838,
      -1.4833615830372648,
      -1.4975158358675789,
      -1.5663270341266613,
      -1.5123378085414598,
      -1.479801893180815,
      -1.4659589986935992,
      -1.5210819323996563,
      -1.6093599614336351,
      -1.532768278703578,
      -1.5414378192878284,
      -1.4933489083920453,
      -1.5395613303794526,
      -1.541980185205262,
      -1.5354024206058396,
      -1.5189952921933056,
      -1.5059011025823579,
      0.0,
      -1.5645863059252723,
      -1.509295283759915,
      -1.5127956337757285,
      -1.546998194989231,
      -1.5561768074468625,
      -1.5232520715042528,
      -1.4904865246854884,
      -1.584194438661644
    ],
    [
      -1.2586837583551391,
      -1.1750338863225234,
      -1.1732519800817034,
      -1.1907575123824479,
      -1.1662686406257168,
      -1.1621565425934632,
      -1.2048224600041917,
      -1.1709180244407305,
      -1.106586403628977,
      -1.161159809694865,
      -1.1686368461794105,
      -1.2500482720336714,
      -1.1847777275248,
      -1.1448994755253932,
      -1.1553484046976297,
      -1.1406473549144325,
      -1.1334080968496276,
      -1.195114027712243,
      -1.1783547092894022,
      -1.184340466354705,
      -1.2010089397189347,
      0.0,
      -1.1595289137999965,
      -1.1900858881580954,
      -1.1787298468349294,
      -1.1695964634406033,
      -1.1943636906881634,
      -1.2031625784080977,
      -1.1668776134729946
    ],
    [
      -1.4641598267975555,
      -1.467691590660309,
      -1.4269922133617763,
      -1.4754098238508353,
      -1.3818944028326479,
      -1.3982014767985087,
      -1.5396561622330527,
      -1.4704749623786872,
      -1.4120330987826908,
      -1.3933531196262487,
      -1.4520650409825175,
      -1.559250672909756,
      -1.4642917369012176,
      -1.4900365810765004,
      -1.4550269680066998,
      -1.505354426150564,
      -1.3678057362074159,
      -1.4498311681167113,
      -1.49679773471055,
      -1.4157885135227197,
      -1.4369832499132378,
      -1.4623981827677939,
      0.0,
      -1.5002947773525632,
      -1.4171931968962619,
      -1.455433668923683,
      -1.4561378935613098,
      -1.4594919003323743,
      -1.4845676993601513
    ],
    [
      -1.51215489923056,
      -1.4111804120341567,
      -1.3687307841753276,
      -1.385216292865724,
      -1.4081338196583928,
      -1.4754766258401921,
      -1.4893102161011293,
      -1.4570473128324382,
      -1.445265479019589,
      -1.4188565206683688,
      -1.4619426293805935,
      -1.5894383697982064,
      -1.465814732949572,
      -1.4683141281417302,
      -1.3753671552344566,
      -1.4462054024995674,
      -1.4432371423006607,
      -1.4670177027668168,
      -1.4261355494078085,
      -1.4626389441948662,
      -1.451746407467777,
      -1.5237341931400887,
      -1.5041457546068802,
      0.0,
      -1.478162209935624,
      -1.4359795157568236,
      -1.42766434866683,
      -1.425884239370678,
      -1.5605017981812093
    ],
    [
      -1.3626560749915941,
      -1.2790578503638077,
      -1.3079146205664325,
      -1.2373105269678835,
      -1.291220607298912,
      -1.3279759146335257,
      -1.3549758768710587,
      -1.2691823602590842,
      -1.324943070165623,
      -1.2573739761337435,
      -1.3591479315514552,
      -1.3902266393749376,
      -1.3690373329652716,
      -1.3013834244045963,
      -1.2930422220492557,
      -1.3133318628412987,
      -1.2550927875037974,
      -1.300499047289434,
      -1.3339657270914214,
      -1.3089836936632708,
      -1.319793008807561,
      -1.3370455398620789,
      -1.242227262261737,
      -1.3589053132600935,
      0.0,
      -1.2710445461313111,
      -1.2414441550292776,
      -1.3022685340522777,
      -1.3602510993911308
    ],
    [
      -1.8455353965302586,
      -1.7147920523838724,
      -1.6736658457259972,
      -1.6213333906171064,
      -1.7226040405518368,
      -1.790607495725547,
      -1.8108878498595307,
      -1.7389042323456416,
      -1.7409891369434387,
      -1.773846336707939,
      -1.7975189157949027,
      -1.8866995498593813,
      -1.805072456873296,
      -1.7760378298627189,
      -1.7277734139101555,
      -1.7972387114390513,
      -1.7121629679682577,
      -1.8223031844405448,
      -1.8237902583559251,
      -1.7388903303195011,
      -1.7781099787336876,
      -1.8221992029825593,
      -1.7954168217386817,
      -1.7768814694229502,
      -1.78236050764468,
      0.0,
      -1.780943217279795,
      -1.787044358708654,
      -1.824161581646092
    ],
    [
      -1.3346327120969819,
      -1.2631597969740311,
      -1.255342336066409,
      -1.2533278654723075,
      -1.2895721434322585,
      -1.2648185385394548,
      -1.3304490879656812,
      -1.2647599395998534,
      -1.3151707265730632,
      -1.2612279225261631,
      -1.2605552431824911,
      -1.3925874632418729,
      -1.3351183608801334,
      -1.294847376110889,
      -1.2356219525002348,
      -1.2739363245686024,
      -1.3186528120027696,
      -1.263253372059585,
      -1.271186100353754,
      -1.3325375605895713,
      -1.3033294114631258,
      -1.3286960001189838,
      -1.3099127431383082,
      -1.2898630646396378,
      -1.2695673342974687,
      -1.2654930458056364,
      0.0,
      -1.2947791752248017,
      -1.347602527756294
    ],
    [
      -1.471164358006507,
      -1.452836318112999,
      -1.404898592803519,
      -1.4317718413958254,
      -1.424988711249675,
      -1.4272220347603932,
      -1.5262755556627368,
      -1.4443183489585392,
      -1.425758145745455,
      -1.426528856998585,
      -1.4324823398472954,
      -1.5553362696764879,
      -1.4551474223622152,
      -1.4676716845773783,
      -1.416853741636201,
      -1.448575651262427,
      -1.440043311556833,
      -1.4523719028080637,
      -1.4524435656469412,
      -1.4180462886214915,
      -1.4115009233257394,
      -1.497086225881525,
      -1.444952013242982,
      -1.4434304813928416,
      -1.4545135111140808,
      -1.454805112246103,
      -1.4341607233772733,
      0.0,
      -1.5016245160694222
    ],
    [
      -1.4535849776833112,
      -1.392132733176647,
      -1.398031041497103,
      -1.4077799426880124,
      -1.3977012853808792,
      -1.3836121805499773,
      -1.3809626460641868,
      -1.3769717905717966,
      -1.3745688165381444,
      -1.4089274413643151,
      -1.3727102544607015,
      -1.4166254078971037,
      -1.3732560138747414,
      -1.3661385309949303,
      -1.3771513128789845,
      -1.3752379998937898,
      -1.3258921974077003,
      -1.3496537694608426,
      -1.406289456037171,
      -1.4044799827910595,
      -1.4001588971474153,
      -1.3320912012837434,
      -1.362406256112294,
      -1.4385899450254187,
      -1.3923033334799364,
      -1.3642797051978224,
      -1.415990801310173,
      -1.4007343116673479,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.21661942455076222,
      0.24838630752615298,
      0.20906315852017787,
      0.23088631264417359,
      0.2177983207000116,
      0.1462752166369201,
      0.16982666825653858,
      0.22795754835127235,
      0.21963118278937355,
      0.22347319463116921,
      0.11016215931254969,
      0.19775000718720603,
      0.19272943331621217,
      0.2512696078185206,
      0.2790306827257405,
      0.2049022567512151,
      0.19769584862074252,
      0.163622872833447,
      0.2535580865804419,
      0.2247956656287453,
      0.19072158485940527,
      0.20296040776025803,
      0.17462244553038886,
      0.16185541071069132,
      0.20770611431652908,
      0.1991238146719998,
      0.252667389255707,
      0.17241711117972436
    ],
    [
      0.36612296238182584,
      0.0,
      0.43576826763207777,
      0.44866713981797757,
      0.4574035944732697,
      0.41821615878794094,
      0.3450488143945014,
      0.4114470931678935,
      0.4502986618281146,
      0.41036968512876304,
      0.42510470608795137,
      0.2979553247407114,
      0.43321618842785714,
      0.43495434466560745,
      0.47739506658825026,
      0.5026639673157476,
      0.38855753380176505,
      0.4305654151861025,
      0.3887116437962568,
      0.43074247036262503,
      0.3680552282568992,
      0.3446220228606136,
      0.3538710636811082,
      0.4174539271900706,
      0.40464095204210704,
      0.4118693913321083,
      0.40125521778393214,
      0.4167995389257715,
      0.3518660385464747
    ],
    [
      0.3725419658839244,
      0.39054948762126074,
      0.0,
      0.5090147488581129,
      0.4809083630599995,
      0.41824204979201873,
      0.33919914191668576,
      0.3695178032575681,
      0.4557028768978626,
      0.42818325248490696,
      0.41285110088721244,
      0.2683650692082822,
      0.35893001457894624,
      0.4000920502193481,
      0.4505279882223414,
      0.3798697414408929,
      0.3939149210963633,
      0.37334053029201364,
      0.36271856332338914,
      0.3640241625573657,
      0.39781700842020373,
      0.36228454342429517,
      0.38813691982405163,
      0.3744002580314574,
      0.39173679854750043,
      0.4792556706978761,
      0.39291047884121855,
      0.3511934946351396,
      0.3350060208755521
    ],
    [
      0.3317324804026329,
      0.4573920835201897,
      0.5413186253643347,
      0.0,
      0.4234772429326341,
      0.423114521238225,
      0.3357183031684403,
      0.40430401610822253,
      0.43581638167789505,
      0.3866420034685569,
      0.43296409767018784,
      0.26056498929551375,
      0.40065871444595524,
      0.4147103314427727,
      0.47162638529587664,
      0.4343659516501839,
      0.44467170021738633,
      0.39501503081543987,
      0.3873466646960546,
      0.3984602342613577,
      0.36605989425807106,
      0.307594456041169,
      0.3410659495342039,
      0.45788581815962615,
      0.4277921290442346,
      0.5580478787767276,
      0.4317522372668414,
      0.3566811714511884,
      0.2873687482655731
    ],
    [
      0.32688597008783193,
      0.40173513399143057,
      0.5315053759020478,
      0.397761958336347,
      0.0,
      0.442720202868347,
      0.3311629400083327,
      0.3669188195638038,
      0.5187736456822878,
      0.4804136802667509,
      0.38206452766384413,
      0.30723159367187325,
      0.3821068325589625,
      0.302367024901923,
      0.3818833827625514,
      0.3322103706479569,
      0.3923981996246655,
      0.3413717439492394,
      0.34427429582500335,
      0.3483406528582753,
      0.3960260755507168,
      0.3105067343562167,
      0.46073334988189507,
      0.3912319757513065,
      0.4015990336617088,
      0.43097028035469953,
      0.3283471617937619,
      0.3982366697784556,
      0.30652643932004686
    ],
    [
      0.37749098614525,
      0.4089001036813751,
      0.499830750285599,
      0.49841795349556106,
      0.4711481036119176,
      0.0,
      0.31330967810814236,
      0.39283289522612175,
      0.5362906579945199,
      0.4661050057742533,
      0.5089790061957082,
      0.2356496523218785,
      0.40059246844962026,
      0.3885520615782869,
      0.4065758380543443,
      0.35304552997139904,
      0.44745411547399794,
      0.43828381112566084,
      0.34116039726256187,
      0.41796874602170764,
      0.41876998839630986,
      0.3678584626787751,
      0.48729193033949225,
      0.3553156224008065,
      0.4065694676150018,
      0.3896747989948377,
      0.38519772347699766,
      0.3651270260890407,
      0.31639687602231814
    ],
    [
      0.26361927442606503,
      0.32990561150406816,
      0.34093631964623894,
      0.33956961627508986,
      0.34440706630300366,
      0.29009977250072505,
      0.0,
      0.353726515406952,
      0.29111961647550766,
      0.34555456317797484,
      0.2851324075708568,
      0.2689725260425011,
      0.2817977129852376,
      0.32936311570616694,
      0.3376238256381241,
      0.3201610913406794,
      0.3002159411278458,
      0.283036108971356,
      0.3215729440512327,
      0.32695673009523807,
      0.3615817374945709,
      0.28084291228325364,
      0.29936342310706343,
      0.3577621749604598,
      0.3354067004554637,
      0.33182145976764543,
      0.28978261922968396,
      0.30945896726632593,
      0.27041513227341585
    ],
    [
      0.2997621896255549,
      0.4144952112450653,
      0.40242171447595876,
      0.4007093064086791,
      0.40884027595995387,
      0.3578027335600933,
      0.3542473704812936,
      0.0,
      0.3640761398096024,
      0.41213036240851175,
      0.3548821407456424,
      0.25695173217474565,
      0.38206317409043167,
      0.3927395057424792,
      0.40315463696631726,
      0.3578837947939708,
      0.3768511785785167,
      0.36659004868321854,
      0.3868972580832448,
      0.37139994720008307,
      0.3881851236973082,
      0.3114958680959494,
      0.3627481505649581,
      0.36291026080058875,
      0.3549233765079802,
      0.39870924169419,
      0.38791679970363435,
      0.3683034089623487,
      0.2882429129071622
    ],
    [
      0.3048060535086965,
      0.41148494254821544,
      0.47748528999586104,
      0.428175501442696,
      0.5123450420777,
      0.4700320745600437,
      0.2804148598290972,
      0.3426383874479615,
      0.0,
      0.40962474669239635,
      0.42501461635387994,
      0.2358732522822522,
      0.3782864362418481,
      0.3428771106808559,
      0.43674582510299875,
      0.35558285257172884,
      0.4274435603871971,
      0.3749895567373682,
      0.3222932254410653,
      0.3871839165942752,
      0.4160005291759392,
      0.39153079058707174,
      0.4433010177178749,
      0.3740456534909882,
      0.33028037853273706,
      0.3640446624719209,
      0.3755394737777651,
      0.40103739715391784,
      0.3089188741507831
    ],
    [
      0.36626977184947007,
      0.4539252546179642,
      0.48360437561191394,
      0.47169497376317726,
      0.5072376618709953,
      0.4551101389164556,
      0.3492595456781875,
      0.48485036565792616,
      0.44579870684043166,
      0.0,
      0.4338577708415272,
      0.2888442394702946,
      0.4273944660011,
      0.3992879317792779,
      0.4807584673842955,
      0.41870490353924916,
      0.4052310096380882,
      0.4728435404754081,
      0.37745397721223717,
      0.4206558719560327,
      0.4177694604109292,
      0.3512260262875262,
      0.44062742651817244,
      0.4138873295737271,
      0.46287557125370227,
      0.40892575203850323,
      0.4720063658372411,
      0.40288963389833965,
      0.33231766438523413
    ],
    [
      0.3656319232306473,
      0.48403280077001143,
      0.48976514929949766,
      0.4712670971689841,
      0.4796850706917366,
      0.5254180943301301,
      0.33508102860239464,
      0.37438435210426757,
      0.4951625769808887,
      0.4187260985881327,
      0.0,
      0.2718471128506279,
      0.48089812236803,
      0.4196723182722142,
      0.4799592928332381,
      0.4557145976931889,
      0.47406165181615334,
      0.4709382974569687,
      0.4354667265331613,
      0.3829240769912634,
      0.40833297990314654,
      0.36891488875694156,
      0.45018198828439937,
      0.3419580895111505,
      0.3822661730575836,
      0.40043884102556815,
      0.4585010234157134,
      0.3902447316377735,
      0.3773842651739603
    ],
    [
      0.16394945866090338,
      0.19832753133676073,
      0.19108839626889962,
      0.20771918024309133,
      0.23023771778423585,
      0.19548242621294665,
      0.22548910154592194,
      0.22636334317686702,
      0.19903600009686007,
      0.21817926537244392,
      0.19940643623946874,
      0.0,
      0.19884916661005136,
      0.20705106362015524,
      0.19447122376309278,
      0.22031776289530258,
      0.18680255094278486,
      0.1702782675632526,
      0.19235343144802264,
      0.19087923681070595,
      0.21169445748239224,
      0.2078440825779586,
      0.20153792480542987,
      0.20197252126242282,
      0.20836959561669777,
      0.22175370527135274,
      0.2041015802954198,
      0.199527645986314,
      0.2165888107873617
    ],
    [
      0.3367394817185039,
      0.49415627308880006,
      0.5087745799923769,
      0.498629027217663,
      0.48606990721558563,
      0.42999342457315404,
      0.33127285450914057,
      0.4676890261939588,
      0.47090629254011285,
      0.4254740848361922,
      0.4859563339591484,
      0.2837426400334888,
      0.0,
      0.4151744771281525,
      0.47064347659810024,
      0.4440776220965428,
      0.48403209058554686,
      0.5022238778341339,
      0.38009525073220884,
      0.35386224503083374,
      0.4096101403311512,
      0.37852755981006725,
      0.42136638104094937,
      0.408348002321578,
      0.37448189469473503,
      0.41954311550013657,
      0.4200562726996111,
      0.41978771743091636,
      0.37321979358195256
    ],
    [
      0.32271077404598114,
      0.469992814408317,
      0.4560817317267367,
      0.4590678141574791,
      0.41883455451253115,
      0.44033759219866253,
      0.3630368026886608,
      0.41900662685099,
      0.3752261342912542,
      0.38608637508353727,
      0.41384189163670726,
      0.2839329464433722,
      0.41223012653397295,
      0.0,
      0.4842860262475521,
      0.43440489929820103,
      0.36706170802663207,
      0.4121707075722951,
      0.3931488551273785,
      0.38614733123817646,
      0.36858515517080304,
      0.4059534728648213,
      0.36575729061039763,
      0.4353653842185339,
      0.3767232125987563,
      0.4222424308863377,
      0.4335220548505585,
      0.36943524429171837,
      0.3678029850784985
    ],
    [
      0.40594418978538616,
      0.4892879092418767,
      0.4671532488557797,
      0.48533590172617025,
      0.45673355799886006,
      0.37607093254571144,
      0.33475803229433954,
      0.4152878680051373,
      0.4699466308660598,
      0.4152467013688943,
      0.46605957579359236,
      0.2869071467448283,
      0.42711440227472086,
      0.44682958510744064,
      0.0,
      0.49544317996255494,
      0.426004762972666,
      0.3820243252024542,
      0.4080702836697334,
      0.4223518986086894,
      0.3819147101846092,
      0.3988499135380772,
      0.3433233638255411,
      0.42654586238549674,
      0.35554336896050565,
      0.45064368470307725,
      0.41940397767477644,
      0.41248206456421044,
      0.3629525633473505
    ],
    [
      0.397770469336564,
      0.42756249783437306,
      0.39302154393559174,
      0.38227187987776134,
      0.31891128684409686,
      0.32605618921717183,
      0.2888867182148247,
      0.3428165616433556,
      0.3010935379880191,
      0.3600950513735717,
      0.35625068352977807,
      0.24183530946900333,
      0.3433435554739561,
      0.35877623543442305,
      0.4139689772275581,
      0.0,
      0.3124592673250359,
      0.352019814357984,
      0.3828396950230917,
      0.46894542410604023,
      0.2924398980437495,
      0.30866873957994745,
      0.31336351270391005,
      0.3353562590849648,
      0.3169374909540734,
      0.3673581032917439,
      0.3565591546100435,
      0.3106570540360587,
      0.31039543105409684
    ],
    [
      0.3547257754971571,
      0.4098727725396769,
      0.43113003893178936,
      0.47062892141119583,
      0.394209056381005,
      0.41599402053727363,
      0.27764181485256056,
      0.36116042116747926,
      0.4512951725006962,
      0.37077780391835735,
      0.44875907369767565,
      0.20569404824431126,
      0.404435573356722,
      0.35461989085348367,
      0.41645507103877977,
      0.3515223716420186,
      0.0,
      0.39114833973547114,
      0.36667882158241416,
      0.3615257595997805,
      0.32668532944143114,
      0.32213113406634,
      0.41732479396860844,
      0.35206315565828383,
      0.3611815418930304,
      0.3610034598747849,
      0.3259929807076456,
      0.3241496062753646,
      0.33953516002163475
    ],
    [
      0.3705281126321529,
      0.48161080964680436,
      0.4772502021130509,
      0.5181488518853166,
      0.4118556449953563,
      0.4949187656196561,
      0.3333655689033226,
      0.3845474112783749,
      0.4602948770682338,
      0.4493732069102039,
      0.512607924134276,
      0.28233145501500334,
      0.518968169925178,
      0.452879181292531,
      0.43188684533574073,
      0.45071419111360767,
      0.4428930316307813,
      0.0,
      0.4276883994778353,
      0.3814852034934104,
      0.3798778543365622,
      0.4105174092026762,
      0.45062359038733035,
      0.41140605402459984,
      0.48052397373579137,
      0.39281363320513196,
      0.5159388292998102,
      0.3815021766893776,
      0.37258384718954574
    ],
    [
      0.31289534566513866,
      0.3968773742600977,
      0.4168329519282299,
      0.40652838880990183,
      0.39332258085390603,
      0.3519878378619705,
      0.3288113359921594,
      0.3898335812856395,
      0.3531844477057604,
      0.3494541151581483,
      0.3651692632554664,
      0.2772378639942268,
      0.3792983935658498,
      0.32893090316617113,
      0.4214592346172985,
      0.3841796927747232,
      0.4147783317976028,
      0.3470665190239939,
      0.0,
      0.347838089058115,
      0.35825234277930096,
      0.35389687663217817,
      0.38088517483491136,
      0.3655471323330304,
      0.37787226148566977,
      0.4065936311382008,
      0.37474282265443737,
      0.3763023616062795,
      0.3309458255850024
    ],
    [
      0.434031039858606,
      0.40673213839986944,
      0.40593295637523985,
      0.3886338407418519,
      0.415181296012169,
      0.39037947426785036,
      0.3124973784564049,
      0.3764159440622685,
      0.4290753286924536,
      0.39711303716302226,
      0.35420776462310655,
      0.2560956379460546,
      0.3317113297837406,
      0.3579629840128393,
      0.41863084062412725,
      0.46870769296844994,
      0.3682916239865761,
      0.3264482236035873,
      0.3590718052838604,
      0.0,
      0.36858520742004663,
      0.3107559412897547,
      0.4034885399609007,
      0.35345817458511375,
      0.3861434712245815,
      0.4025953699359812,
      0.33323465250956463,
      0.42521324148937656,
      0.293061830017189
    ],
    [
      0.39988854773490456,
      0.3436193490017172,
      0.42391062717327377,
      0.3940701395268371,
      0.39368268045215604,
      0.379528427621842,
      0.3107172293627596,
      0.36470645494796106,
      0.3972423703086059,
      0.41108526479582164,
      0.3559623310897646,
      0.26768430205578575,
      0.344275984785843,
      0.33560644420159247,
      0.3836953550973756,
      0.33748293310996824,
      0.33506407828415896,
      0.34164184288358124,
      0.35804897129611524,
      0.371143160907063,
      0.0,
      0.3124579575641486,
      0.36774897972950593,
      0.36424862971369243,
      0.3300460685001898,
      0.3208674560425584,
      0.3537921919851681,
      0.3865577388039325,
      0.2928498248277769
    ],
    [
      0.24933395655436397,
      0.3329838285869797,
      0.3347657348277997,
      0.3172602025270552,
      0.3417490742837863,
      0.34586117231603986,
      0.3031952549053114,
      0.33709969046877264,
      0.4014313112805261,
      0.3468579052146381,
      0.33938086873009254,
      0.2579694428758317,
      0.32323998738470316,
      0.3631182393841099,
      0.3526693102118734,
      0.3673703599950706,
      0.37460961805987547,
      0.31290368719726014,
      0.32966300562010087,
      0.32367724855479807,
      0.30700877519056835,
      0.0,
      0.34848880110950664,
      0.31793182675140774,
      0.32928786807457366,
      0.3384212514688998,
      0.3136540242213397,
      0.3048551365014054,
      0.3411401014365085
    ],
    [
      0.37849611453788734,
      0.3749643506751339,
      0.4156637279736666,
      0.36724611748460756,
      0.460761538502795,
      0.44445446453693416,
      0.3029997791023902,
      0.3721809789567556,
      0.43062284255275207,
      0.4493028217091941,
      0.3905909003529253,
      0.2834052684256869,
      0.37836420443422525,
      0.35261936025894247,
      0.387628973328743,
      0.33730151518487883,
      0.474850205128027,
      0.39282477321873155,
      0.34585820662489275,
      0.42686742781272313,
      0.4056726914222051,
      0.380257758567649,
      0.0,
      0.34236116398287963,
      0.425462744439181,
      0.38722227241175977,
      0.386518047774133,
      0.38316404100306856,
      0.35808824197529154
    ],
    [
      0.36803843288873583,
      0.4690129200851392,
      0.5114625479439683,
      0.49497703925357195,
      0.47205951246090305,
      0.4047167062791037,
      0.3908831160181665,
      0.4231460192868577,
      0.43492785309970694,
      0.4613368114509271,
      0.41825070273870235,
      0.29075496232108944,
      0.41437859916972375,
      0.4118792039775656,
      0.5048261768848392,
      0.43398792961972843,
      0.43695618981863515,
      0.4131756293524791,
      0.45405778271148733,
      0.41755438792442967,
      0.4284469246515188,
      0.3564591389792071,
      0.3760475775124157,
      0.0,
      0.4020311221836719,
      0.44421381636247226,
      0.45252898345246595,
      0.45430909274861775,
      0.3196915339380866
    ],
    [
      0.3172914373644167,
      0.4008896619922031,
      0.3720328917895783,
      0.44263698538812735,
      0.3887269050570987,
      0.3519715977224851,
      0.3249716354849521,
      0.41076515209692666,
      0.35500444219038774,
      0.42257353622226734,
      0.3207995808045556,
      0.2897208729810732,
      0.31091017939073917,
      0.3785640879514145,
      0.38690529030675513,
      0.36661564951471215,
      0.42485472485221343,
      0.3794484650665768,
      0.34598178526458945,
      0.37096381869274,
      0.3601545035484499,
      0.34290197249393195,
      0.43772025009427384,
      0.32104219909591736,
      0.0,
      0.4089029662246997,
      0.4385033573267332,
      0.3776789783037331,
      0.31969641296488005
    ],
    [
      0.33809651486792247,
      0.4688398590143086,
      0.5099660656721838,
      0.5622985207810747,
      0.4610278708463442,
      0.39302441567263413,
      0.37274406153865036,
      0.4447276790525394,
      0.4426427744547423,
      0.4097855746902421,
      0.3861129956032783,
      0.29693236153879976,
      0.37855945452488493,
      0.40759408153546217,
      0.45585849748802554,
      0.3863931999591297,
      0.4714689434299233,
      0.3613287269576362,
      0.3598416530422559,
      0.4447415810786799,
      0.40552193266449343,
      0.3614327084156217,
      0.38821508965949936,
      0.4067504419752308,
      0.401271403753501,
      0.0,
      0.40268869411838604,
      0.39658755268952706,
      0.3594703297520889
    ],
    [
      0.29911043549981575,
      0.3705833506227665,
      0.3784008115303885,
      0.38041528212449016,
      0.34417100416453916,
      0.3689246090573428,
      0.30329405963111644,
      0.3689832079969442,
      0.3185724210237344,
      0.3725152250706345,
      0.3731879044143065,
      0.24115568435492474,
      0.29862478671666426,
      0.33889577148590866,
      0.39812119509656285,
      0.3598068230281952,
      0.315090335594028,
      0.3704897755372125,
      0.3625570472430437,
      0.3012055870072263,
      0.33041373613367186,
      0.3050471474778138,
      0.32383040445848943,
      0.34388008295715977,
      0.3641758132993289,
      0.36825010179116124,
      0.0,
      0.33896397237199594,
      0.28614061984050365
    ],
    [
      0.329299175212191,
      0.34762721510569894,
      0.3955649404151791,
      0.36869169182287265,
      0.37547482196902293,
      0.3732414984583048,
      0.27418797755596125,
      0.35614518426015884,
      0.3747053874732431,
      0.373934676220113,
      0.36798119337140256,
      0.24512726354221015,
      0.34531611085648284,
      0.3327918486413197,
      0.383609791582497,
      0.35188788195627096,
      0.36042022166186505,
      0.3480916304106343,
      0.3480199675717568,
      0.3824172445972065,
      0.3889626098929586,
      0.30337730733717305,
      0.35551151997571595,
      0.35703305182585643,
      0.3459500221046172,
      0.345658420972595,
      0.3663028098414247,
      0.0,
      0.29883901714927585
    ],
    [
      0.25336803393944685,
      0.31482027844611093,
      0.3089219701256549,
      0.2991730689347456,
      0.3092517262418788,
      0.3233408310727808,
      0.3259903655585712,
      0.3299812210509614,
      0.3323841950846136,
      0.2980255702584429,
      0.33424275716205654,
      0.29032760372565436,
      0.33369699774801664,
      0.3408144806278277,
      0.3298016987437735,
      0.3317150117289682,
      0.3810608142150578,
      0.35729924216191544,
      0.300663555585587,
      0.3024730288316986,
      0.30679411447534277,
      0.37486181033901467,
      0.344546755510464,
      0.26836306659733933,
      0.31464967814282163,
      0.34267330642493565,
      0.29096221031258507,
      0.3062186999554102,
      0.0
    ]
  ],
  "row_avgs": [
    0.2052681512023599,
    0.4079872292572973,
    0.39290125088913536,
    0.4040767157310534,
    0.3834394311292958,
    0.4069567734568388,
    0.31393592450295527,
    0.36740478085598155,
    0.3831427152698263,
    0.4230467215466932,
    0.4246021203338491,
    0.203202567309897,
    0.4248019229748122,
    0.4015354622378165,
    0.41529391579316216,
    0.3457378693418139,
    0.37029792533553474,
    0.43139768644791643,
    0.3675258814222646,
    0.3740591701890924,
    0.35634376220728914,
    0.3305688458476142,
    0.38699109044207364,
    0.42357538261122196,
    0.37029390500665826,
    0.40978296374203804,
    0.34017168555464183,
    0.3498632314922861,
    0.3195150747500599
  ],
  "col_avgs": [
    0.33596717404792775,
    0.39881432101203496,
    0.4231777551185382,
    0.41493122528573634,
    0.4067356953643449,
    0.38660137332235195,
    0.31551642805140034,
    0.3736179745706858,
    0.4006638868484337,
    0.38909277169986684,
    0.38475327677801013,
    0.2636883021815205,
    0.3666789699953811,
    0.36433760953516015,
    0.4075870821735555,
    0.3825415071621094,
    0.3868714488151645,
    0.3680447778568827,
    0.355219895941501,
    0.3698676274582494,
    0.36407193122721765,
    0.3364835436059857,
    0.373930770621476,
    0.35832666300621707,
    0.36466419725322996,
    0.38507931489201547,
    0.3753869842904604,
    0.3635725626357612,
    0.31749508613026034
  ],
  "combined_avgs": [
    0.2706176626251438,
    0.4034007751346661,
    0.4080395030038368,
    0.4095039705083949,
    0.3950875632468204,
    0.3967790733895954,
    0.3147261762771778,
    0.37051137771333365,
    0.39190330105912996,
    0.40606974662328,
    0.4046776985559296,
    0.23344543474570875,
    0.39574044648509665,
    0.38293653588648835,
    0.4114404989833588,
    0.36413968825196164,
    0.37858468707534965,
    0.3997212321523996,
    0.3613728886818828,
    0.3719633988236709,
    0.36020784671725337,
    0.33352619472679995,
    0.3804609305317748,
    0.3909510228087195,
    0.3674790511299441,
    0.3974311393170268,
    0.3577793349225511,
    0.3567178970640237,
    0.3185050804401601
  ],
  "gppm": [
    616.2450757686013,
    614.2341168616297,
    600.7467853104254,
    607.9865038742519,
    610.9457187631072,
    620.7813113723317,
    655.6264101694667,
    622.8051760721055,
    614.6905497603833,
    618.2797988950894,
    620.3667211453442,
    672.5238159465368,
    631.8795192299484,
    629.69265972913,
    612.1219856440081,
    620.6744079285774,
    619.98731377607,
    633.7766457546543,
    635.3650116931548,
    625.7404923949329,
    628.9588417484053,
    646.3296961104735,
    627.4544070058133,
    630.6419688728454,
    632.7915485887809,
    619.0254355638365,
    625.958812598603,
    627.9518513290343,
    652.6039793758508
  ],
  "gppm_normalized": [
    1.4215917691347346,
    1.4141777771896364,
    1.3849267149391908,
    1.4025614987282249,
    1.4025211345494024,
    1.425321973967603,
    1.5127739613995417,
    1.4318775792738656,
    1.4130261386498935,
    1.4221457259177246,
    1.4251520430605298,
    1.5619343906606586,
    1.456762581609538,
    1.4582753778866313,
    1.4084347272538764,
    1.42837385605339,
    1.424686372062213,
    1.45478918527291,
    1.4567575228466938,
    1.43750245463226,
    1.4503727253502035,
    1.483437423292477,
    1.444218306064445,
    1.4508137674549955,
    1.4501920081823088,
    1.4255402858052975,
    1.438699016464798,
    1.439225873223741,
    1.49226316397586
  ],
  "token_counts": [
    449,
    442,
    457,
    469,
    405,
    411,
    478,
    424,
    432,
    434,
    418,
    576,
    471,
    536,
    433,
    435,
    431,
    402,
    393,
    417,
    463,
    400,
    456,
    424,
    387,
    443,
    422,
    396,
    359,
    580,
    438,
    467,
    431,
    435,
    409,
    435,
    418,
    448,
    403,
    426,
    581,
    423,
    449,
    432,
    432,
    425,
    368,
    421,
    434,
    418,
    424,
    385,
    435,
    408,
    359,
    468,
    462,
    400
  ],
  "response_lengths": [
    3064,
    2598,
    2643,
    2466,
    2535,
    2344,
    2477,
    2353,
    2525,
    2358,
    2513,
    3296,
    2346,
    2698,
    2468,
    2608,
    2355,
    2121,
    2447,
    2582,
    2383,
    2449,
    2174,
    2530,
    2368,
    2084,
    2728,
    2607,
    2316
  ]
}