{
  "example_idx": 39,
  "reference": "Published as a conference paper at ICLR 2023\n\nEXPONENTIAL GENERALIZATION BOUNDS WITH NEAR-OPTIMAL RATES FOR Lq-STABLE ALGORITHMS\n\nXiao-Tong Yuan School of Intelligence Science and Technology Nanjing University, Suzhou, 215163, China xtyuan1980@gmail.com\n\nPing Li LinkedIn Ads 700 Bellevue Way NE, Bellevue, WA 98004, USA pinli@linkedin.com\n\nABSTRACT\n\nThe stability of learning algorithms to changes in the training sample has been actively studied as a powerful proxy for reasoning about generalization. Recently, exponential generalization and excess risk bounds with near-optimal rates have been obtained under the stringent and distribution-free notion of uniform stability (Bousquet et al., 2020; Klochkov & Zhivotovskiy, 2021). In the meanwhile, under the notion of Lq-stability, which is weaker and distribution dependent, exponential generalization bounds are also available yet so far only with sub-optimal rates. Therefore, a fundamental question we would like to address in this paper is whether it is possible to derive near-optimal exponential generalization bounds for Lq-stable learning algorithms. As the core contribution of the present work, we give an affirmative answer to this question by developing strict analogues of the near-optimal generalization and risk bounds of uniformly stable algorithms for Lq-stable algorithms. Further, we demonstrate the power of our improved Lqstability and generalization theory by applying it to derive strong sparse excess risk bounds, under mild conditions, for computationally tractable sparsity estimation algorithms such as Iterative Hard Thresholding (IHT).\n\n1\n\nINTRODUCTION\n\nA fundamental issue in statistical learning is to bound the generalization error of a learning algorithm for understanding its prediction performance on unseen data. It has long been recognized in literature that one of the key characteristics that permits learning algorithms to generalize is the stability of estimated model to perturbations in training data. The idea of using algorithmic stability as a proxy for generalization performance analysis dates back to the seventies (Rogers & Wagner, 1978; Devroye & Wagner, 1979). Since the seminal work of Bousquet & Elisseeff (2002), the search for generalization bounds under various notions of algorithmic stability has been a flourishing area of learning theory (Zhang, 2003; Mukherjee et al., 2006; Shalev-Shwartz et al., 2010; Kale et al., 2011; Hardt et al., 2016; Celisse & Guedj, 2016; Bousquet et al., 2020).\n\nAs one may expect, the stronger an algorithmic stability criterion is, the sharper the resulting generalization bound will be. On one end, exponential generalization bounds can be guaranteed by approaches under the most stringent notion of uniform stability (Bousquet & Elisseeff, 2002; Bousquet et al., 2020), which requires the change in the prediction loss to be uniformly small regardless data distribution. Despite the strength of generalization, the distribution-free nature makes uniform stability too restrictive to be fulfilled, e.g., by learning rules with unbounded losses (Celisse & Guedj, 2016). On the other end, based on some weaker and distribution dependent notions of stability such as hypothesis stability and mean-square stability, only polynomial generalization bounds seem possible in general cases, although the corresponding stability criteria are more amenable to verification (Bousquet & Elisseeff, 2002). These observations have prompted the development of Lq-stability, as an in-between state, to achieve the best of two worlds (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019): it generalizes the notion of hypothesis stability from l1-norm criterion to Lq-norm criterion for q ≥ 2 but remains distribution dependent and thus is weaker than uniform stability; in the meanwhile it can still achieve similar exponential generalization bounds to those of uniformly stable algorithms (Bousquet & Elisseeff, 2002).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nBy far, the best known (and near-optimal) rates about exponential generalization bounds are offered by approaches based on uniform stability and certain fine-grained concentration inequalities for sum of functions of independent random variables (Feldman & Vondr ́ak, 2019; Bousquet et al., 2020). These rates are substantially sharper than those of Bousquet & Elisseeff (2002), which are implied by a naive application of McDiarmid’s inequality, in terms of the overhead factors on stability coefficients. While it has long been known that the low probability of failure (over sample) can be handled via developing modified bounded-difference inequalities (Rakhlin et al., 2005), it still remains less clear how to simply adapt these existing techniques to the more sophisticated frameworks of Feldman & Vondr ́ak (2019); Bousquet et al. (2020) to obtain sharper exponential bounds. Particularly for Lq-stable learning algorithms, the state-of-the-art exponential generalization bounds are derived based on the moments or exponential extensions of the Efron-Stein inequality (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019), which yield similar rates of convergence to those of Bousquet & Elisseeff (2002) and thus are suspected to be sub-optimal.\n\nGiven the above observed gap in rates of convergence between the generalization bounds under uniform stability and Lq-stability, the following question is naturally raised:\n\nIs it possible to derive sharper exponential generalization bounds for Lq-stable learning algorithms that match those recent breakthrough results for uniformly stable algorithms?\n\nAs the core contribution of the present work, we give an affirmative answer to this open question by developing strict analogues of the near-optimal generalization bounds of uniformly stable algorithms for Lq-stable algorithms. The main results of our work confirm that the notion of Lq-stability serves as a neat yet powerful tool for extending those best-known generalization bounds to a broad class of non-uniformly stable algorithms. To illustrate the importance of our theory, we have applied the improved analysis of Lq-stable algorithms to derive sharper exponential risk bounds for computationally tractable sparsity recovery estimators, such as the Iterative Hard Thresholding (IHT) algorithms widely used in high dimensional sparse learning (Blumensath & Davies, 2009; Foucart, 2011; Jain et al., 2014). This application also serves as a main motivation of our study.\n\nNotation. Here we provide some notation that will be frequently used throughout the paper. Let S = {Z1, Z2, ..., ZN } be a set of independent random data samples valued in some measurable set Z. For any indices set I ⊆ [N ] := {1, ..., N }, we denote by SI = {Zi, i ∈ I} and SI = S \\ SI . We denote by S′ = {Z ′ N } another i.i.d. sample from the same distribution as that of S and we write S(i) = {Z1, ..., Zi−1, Z ′ i, Zi+1, ..., ZN }. For a real-valued random variable Y , its Lq-norm for q ≥ 1 is defined by ∥Y ∥q = (E[|Y |q])1/q. By definition it can be verified that ∀q ≥ 2,\n\n2, ..., Z ′\n\n1, Z ′\n\n∥Y ∥2\n\nq = (E[|Y |q])2/q =\n\n(cid:16)\n\nE[|Y 2|q/2]\n\n(cid:17)2/q\n\n= (cid:13)\n\n(cid:13)Y 2(cid:13)\n\n(cid:13)q/2 .\n\n(1)\n\nLet g : Z N (cid:55)→ R be some measurable function and consider the random variable g(S) = g(Z1, ...ZN ). For g(S) and any index set I ⊆ [N ], we define the following abbreviations:\n\n∥g∥q(SI ) := (E[|g(S)|q | SI ])1/q . We say a real-valued function f is G-Lipschitz continuous over the domain W if\n\ng(SI ) := E[g(S) | SI ],\n\n|f (w) − f (w′)| ≤ G∥w − w′∥, ∀w, w′ ∈ W. For a pair of functions f, g ≥ 0, we use f ≲ g (or g ≳ f ) to denote f ≤ cg for some constant c > 0. We denote by supp(w) the support of a vector w which is the index set of non-zero entries of w.\n\n1.1 SETUP AND PRIOR RESULTS\n\nProblem setup. We consider a statistical learning algorithm A : Z N (cid:55)→ W that maps a training data set S to a model A(S) in a closed subset W of an Euclidean space. The population risk and corresponding empirical risk evaluated at A(S) are respectively given by\n\nR(A(S)) := EZ[l(A(S); Z)] and RS(A(S)) :=\n\n1 N\n\nN (cid:88)\n\ni=1\n\nl(A(S); Zi),\n\nwhere l : W × Z (cid:55)→ R+ is a non-negative and potentially unbounded loss function whose value l(w; z) measures the loss evaluated at z with parameter w. As a classic fundamental issue in statistical learning, we are interested in deriving the upper bounds on the difference between population\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nand empirical risks, i.e., |R(A(S)) − RS(A(S))|, which quantifies the generalization error of A. Let R∗ := minw∈W R(w) be the optimal value of the population risk. We will also study how to upper bound R(A(S)) − R∗ (a.k.a. excess risk) which is of particular interest for understanding the population risk minimization performance of A.\n\nWe first introduce the concept of uniform stability (Bousquet & Elisseeff, 2002) which requires the change in the prediction loss to be uniformly small regardless the distribution of data. Definition 1 (Uniform stability). A learning algorithm A is said to have uniform stability with parameter γu > 0 if it satisfies the following uniform bound:\n\nsup S,S(i),Z∈Z\n\n|l(A(S); Z) − l(A(S(i)); Z)| ≤ γu, ∀i ∈ [N ].\n\nGiven that the loss function l is almost surely bound by M , Bousquet & Elisseeff (2002) showed that a large class of regularized empirical risk minimization (ERM) algorithms has uniform stability, and using McDiarmid’s inequality yields the following exponential tail generalization bound that holds with probability at least 1 − δ over the draw of S for any δ ∈ (0, 1):\n\n|R(A(S)) − RS(A(S))| ≲ γu\n\nN log\n\n(cid:115)\n\n(cid:114)\n\n+ M\n\n(cid:19)\n\n(cid:18) 1 δ\n\nlog (1/δ) N\n\n.\n\n(2)\n\nRecently, equipped with a strong concentration inequality for sums of random functions, Bousquet et al. (2020) established the following moments bound of uniformly stable algorithms for all q ≥ 2:\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ qγu log(N ) + M\n\n(cid:114) q N\n\n.\n\n(3)\n\nIn view of the equivalence between tails and moments (see, e.g., Bousquet et al., 2020, Lemma 1), the above Lq-norm bound implies that for any δ ∈ (0, 1), the following tail bound holds with probability at least 1 − δ over the draw of S :\n\n|R(A(S)) − RS(A(S))| ≲ γu log(N ) log\n\n(cid:114)\n\n+ M\n\n(cid:19)\n\n(cid:18) 1 δ\n\nlog (1/δ) N\n\n.\n\n(4)\n\nN\n\nN log( 1\n\nThis bound substantially improves the classic result in Eq. (2) by reducing the overhead factor on sta-\n\nδ )(cid:1) to O(log(N ) log( 1\n\nbility coefficient from O(cid:0)(cid:113) δ )). For example, in regimes such as regularized ERM where γu ≲ 1√ is usually the case, the convergence rate in Eq. (2) becomes vacuous as it is not vanishing in sample size, while the bound in Eq. (4) still guarantees O(cid:0) log(N ) log( 1 (cid:1) rate δ ) of convergence. Indeed, up to logarithmic factors on sample size and tail bounds, the rate in Eq. (4) is nearly optimal in the sense of a lower bound on sums of random functions by Bousquet et al. (2020). The bound in Eq. (4) can be extended to stochastic learning algorithms when the uniform stability (over data) holds with high probability over the internal randomness of algorithm (Feldman & Vondr ́ak, 2019; Bassily et al., 2020). Under the generalized Bernstein condition (Koltchinskii, 2006) and based on the sharp concentration inequality for sums of random functions by Bousquet et al. (2020), Klochkov & Zhivotovskiy (2021) alternatively established the following deviation optimal excess risk bound that holds with probability at least 1 − δ over the draw of S:\n\n√\n\nN\n\nR(A(S)) − R∗ ≲ ∆opt + E[∆opt] + γu log(N ) log\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+\n\n(M + B) log(1/δ) N\n\n,\n\n(5)\n\nwhere ∆opt := RS(A(S)) − minw∈W RS(w) represents the empirical risk sub-optimality of the algorithm on training data, and B is the Bernstein condition constant as defined in Assumption 1.\n\nWhile implying strong generalization guarantees, the uniform stability is also most stringent in the sense that it is distribution independent and hard to be fulfilled, e.g., by learning rules with unbounded losses. To address such an unpleasant restrictiveness, the notion of Lq-stability was alternatively introduced by Celisse & Guedj (2016) as a relaxation of uniform stability. Definition 2 (Lq-Stability). For q ≥ 1, a learning algorithm A is said to have Lq-stability with parameter γq > 0 if it satisfies the following moment bound:\n\n(cid:13) (cid:13) (cid:13)l(A(S); Z) − l(A(S(i)); Z) (cid:13) (cid:13) (cid:13)q\n\n≤ γq, ∀i ∈ [N ].\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn the above definition, the expectation associated with Lq-norm is taken over S, S(i), Z, and the internal random bits of A, if any (such as in the case of stochastic learning algorithms). Note that slightly different from that of Celisse & Guedj (2016), the random variable Z in the above definition is not necessarily required to be independent of S and S(i). By definition, Lq-stability is distribution dependent and thus is weaker than uniform stability which can be regarded as a special case of Lqstability with γq ≡ γ for some γ > 0. Particularly for q = 1 and q = 2, the Lq-stability reduces to the notions of hypothesis stability (Bousquet & Elisseeff, 2002) and mean-square stability (Kale et al., 2011), respectively. For an instance, it has been shown that the classical ridge regression model with unbounded responses has Lq-stability for all q ≥ 1 rather than uniform stability (Celisse & Guedj, 2016). As a novel and concrete example, we will see shortly in Section 3 that Lq-stability plays a crucial role for deriving strong sparse excess risk bounds for sparsity estimation algorithms such as IHT (Jain et al., 2014; Yuan et al., 2018). Alternatively, the definition of Lq-stability can be extended to the Lq-argument-stability as (cid:13) (cid:13)q ≤ γq, which generalizes the concept of uniform argument stability (Bassily et al., 2020) to the Lq-norm criterion. Obviously Lq-argument-stability is not at all relying on the random argument Z and it implies Lq-stability for Lipschitz losses.\n\n(cid:13)∥A(S) − A(S(i))∥(cid:13)\n\nThe following is by far the best known moments generalization bound under Lq-stability that holds for all q ≥ 2 and potentially unbounded losses (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019):\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ γq\n\n(cid:112)N q +\n\n(cid:114) q N\n\n.\n\n(6)\n\nAs one can see that the Lq-stability generalization bound in Eq. (6) is significantly inferior to the near-optimal uniform stability generalization bound in Eq. (3) in terms of the overhead on stability coefficient. Such a gap in rate of convergence is indeed unsurprising: the bound in Eq. (6) was derived via more or less directly applying moments or exponential extensions of Efron-Stein inequality to generalization error (Celisse & Guedj, 2016; Abou-Moustafa & Szepesv ́ari, 2019), and thus yields about the same overhead factor on stability coefficient as that of the sub-optimal exponential bound in Eq. (2) for uniformly stable algorithms. In light of these observations, we are naturally motivated to derive sharper exponential generalization bounds for Lq-stable algorithms hopefully to match the near-optimal bound in Eq. (3) achievable by uniformly stable algorithms.\n\n1.2 OUR CONTRIBUTION\n\nThe core contribution of the present work is a set of substantially improved exponential generalization bounds for Lq-stable algorithms. The key ingredient of our analysis is a sharper concentration bound on sums of functions of independent random variables under the Lq-norm bounded difference conditions, which generalizes a previous counterpart under the uniform bounded difference conditions (Bousquet et al., 2020). With this generic concentration bound in hand, we are able to derive sharper generalization and excess risk bounds for Lq-stable learning algorithms that match those best known for uniform stable algorithms. The power of our results is demonstrated through deriving more appealing exponential sparse excess risk bounds for computationally tractable sparsity estimation algorithms (such as IHT). The main results obtained in this work are sketched below:\n\n• In Section 2, we first establish in Theorem 1 an Lq-norm inequality for sums of functions of random variables with Lq-norm bounded difference. Then equipped with such a generalpurpose concentration inequality, we prove in Theorem 2 the following Lq-norm generalization bound for Lq-stable learning algorithms for all q ≥ 2:\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ qγq log N + Mq\n\n(cid:114) q N\n\n,\n\nwhere Mq is an upper bound of moments ∥l(A(S); Z)∥q. Compared to Eq. (6), the preceding N to log(N ). As another consequence of our bound improves the overhead factor on γq from Lq-norm concentration inequality, we further derive in Theorem 3 the following excess risk bound for Lq-stable algorithms under B-Bernstein-condition with M -bounded losses (where C = M + B), or μ-quadratic-growth condition with G-Lipschitz losses (where C = G2\n\n√\n\nμ ):\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\nCq N\n\n.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nBased on the equivalence between moments and tails, this above result implies an identical deviation optimal risk bound in Eq. (5) for uniformly stable algorithms.\n\n• In Section 3, based on our Lq-stability generalization theory, we show in Theorem 4 a novel exponential sparse excess risk bound for inexact L0-estimators. A key insight here is that L0estimators are in many cases “almost always” stable over any fixed supporting set, and thus can be shown to have Lq-stability over the same supporting set, which consequently makes our analysis techniques developed for Lq-stable algorithms applicable there. This novel application answers a call by Celisse & Guedj (2016) for extending the range of applicability of the Lq-stability theory beyond the unbounded ridge regression problem, and it complements other existing applications of the Lq-stability theory including k-nearest neighbor classification and k-folds cross-validation (Celisse & Mary-Huard, 2018; Abou-Moustafa & Szepesv ́ari, 2019). Last but not least, our improved Lq-stability theory can also be readily applied to the above mentioned prior applications to obtain sharper generalization bounds.\n\n2 SHARPER EXPONENTIAL BOUNDS FOR Lq-STABLE ALGORITHMS\n\n2.1 A MOMENT INEQUALITY FOR SUMS OF RANDOM FUNCTIONS\n\nWe start by presenting in the following theorem a moment inequality for sums of random functions of N independent random variables that satisfy the Lq-norm bounded difference condition. See Appendix B.1 for its proof. Theorem 1. Let S = {Z1, Z2, ..., ZN } be a set of independent random variables valued in Z. Let g1, ..., gN be a set of measurable functions gi : Z N (cid:55)→ R that satisfy the following conditions for any i ∈ [N ]:\n\n• E [gi(S) | S \\ Zi] = 0, almost surely;\n\n• gi(S) has the following Lq-norm bounded difference property with respect to all variables in\n\nS except Zi, i.e., ∀j ̸= i, for all q ≥ 1:\n\n(cid:13) (cid:13) (cid:13)gi(S) − gi(S(j)) (cid:13) (cid:13) (cid:13)q\n\n≤ βq.\n\nThen there exists a universal constant κ < 1.271 such that for all q ≥ 2,\n\nAdditionally, if ∥E[gi(S) | Zi]∥q ≤ Mq, then for all q ≥ 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi(S) − E[gi(S) | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 4κqN ⌈log2 N ⌉βq.\n\nN (cid:88)\n\ni=1\n\ngi(S)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2(cid:112)2κN qMq + 4κqN ⌈log2 N ⌉βq.\n\nRemark 1. Theorem 1 extends the moment inequality of Bousquet et al. (2020, Theorem 4) from under the distribution-free uniform bounded difference property to under the Lq-norm bounded difference property which is distribution dependent. Specially if gi(S) have uniformly bounded difference property, then Theorem 1 reduces to the result of Bousquet et al. (2020, Theorem 4). Remark 2. The Lq-norm boundedness condition ∥E[gi(S) | Zi]∥q ≤ Mq in our theorem allows gi to be potentially unbounded over domain of interest, which is weaker than the corresponding almost sure boundedness condition on |E[gi(S) | Zi]| as imposed by Bousquet et al. (2020, Theorem 4).\n\n2.2 GENERALIZATION BOUNDS FOR Lq -STABLE ALGORITHMS\n\nAs an important consequence of Theorem 1, we can derive er the following main result on the generalization bound of Lq-stable learning algorithms. See Appendix B.2 for a proof of this result. Theorem 2. Let A : Z N (cid:55)→ W be a learning algorithm that has Lq-stability by γq > 0 for q ≥ 1. Suppose that ∥l(A(S); Z)∥q ≤ Mq for any Z ∈ Z. Then for all q ≥ 2,\n\n∥R(A(S)) − RS(A(S))∥q\n\n≲ qγq log N + Mq\n\n(cid:114) q N\n\n.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nRemark 3. The Lq-norm boundedness condition ∥l(A(S); Z)∥q ≤ Mq allows for learning with unbounded losses over, e.g., data distribution with sub-Gaussian or sub-exponential tail bounds.\n\nTo compare with the best-known moments generalization bound in Eq. (6) under the notion of Lqstability, our bound in Theorem 2 substantially improves the overhead factor on γq from N to log(N ). Specially when reduced to regime of uniform stability where γq ≡ γu for all q ≥ 1, our result revisits the moments generalization bound in Eq. (3) which is nearly tight, up to logarithmic factors on sample size, in the sense of a lower bound on sums of random functions from Bousquet et al. (2020). More broadly, for any δ ∈ (0, 1), suppose that the following exponential stability bound holds with probability at least 1 − δ with a mixture of sub-Gaussian and sub-exponential tails 1 over S, S(i), Z:\n\n√\n\n(cid:12) (cid:12) (cid:12)l(A(S); Z) − l(A(S(i)); Z) (cid:12) (cid:12) (cid:12) ≤ a log\n\n(cid:17)\n\n(cid:16) e δ\n\n(cid:114)\n\n+ b\n\nlog\n\n(cid:17)\n\n.\n\n(cid:16) e δ\n\n(7)\n\nThen according to the equivalence of tails and moments, as summarized in Lemma 4 (see Appendix A), we must have that A is Lq-stable by γq = aq + b q. Assume that the loss is bounded in (0, M ] almost surely over data. Then the Lq-norm generalization bound in Theorem 2 combined with Lemma 4 immediately implies the following generalization bound:\n\n√\n\n|R(A(S)) − RS(A(S))| ≲ a log(N ) log2\n\n(cid:19)\n\n(cid:18) 1 δ\n\n+ b log(N ) log1.5\n\n(cid:114)\n\n+ M\n\n(cid:19)\n\n(cid:18) 1 δ\n\nlog (1/δ) N\n\n.\n\nCompared with the uniform stability implied tail bound in Eq. (4), the preceding Lq-stability bound is nearly identical up to slightly worse confidence tail terms which are caused by the uncertainty of Lq-stability with respect to data distribution. We conjecture that such a slight deterioration in tail bounds might possibly be remedied by using the exponential versions of Efron-Stein inequality (Boucheron et al., 2003; Abou-Moustafa & Szepesv ́ari, 2019) instead of the currently used variant in moments. We leave the improvement over poly-logarithmic terms for future investigation.\n\n2.3 EXCESS RISK BOUNDS FOR Lq -STABLE ALGORITHMS\n\nIn addition to the generalization bounds, we further apply Theorem 1 to study the excess risk bounds of an Lq-stable learning algorithm which are of particular interest for understanding its population risk minimization performance. Let us denote W ∗ := Argminw∈W R(w) as the optimal solution set of the population risk. In order to get sharper risk bounds, we need to impose some structural conditions on risk functions. Particularly, the following defined generalized Bernstein condition (Koltchinskii, 2006) is conventionally used with multiple global risk minimizers allowed.\n\nAssumption 1 (Generalized Bernstein condition). For some B > 0 and for any w ∈ W, there exists w∗ ∈ W ∗ such that the following holds:\n\nE (cid:2)(l(w; Z) − l(w∗; Z))2(cid:3) ≤ B(R(w) − R(w∗)).\n\nWe will also consider the quadratic growth condition which is widely used as an alternative condition for establishing fast rates of convergence in learning theory.\n\nAssumption 2 (Quadratic growth condition). For some μ > 0 and for any w ∈ W, there exists w∗ ∈ W ∗ such that the following holds:\n\nR(w) ≥ R∗ +\n\nμ 2\n\n∥w − w∗∥2.\n\nRemark 4. Clearly, when the loss is G-Lipschitz, the quadratic growth condition with parameter μ implies the Bernstein condition with parameter B = 2G2 μ .\n\nThe following theorem is our main result on the excess risk bound of Lq-stable algorithms, which extends the near-optimal exponential risk bounds of Klochkov & Zhivotovskiy (2021) from uniform stable algorithms to Lq-stable algorithms. A proof of this result can be found in Appendix B.3.\n\n1In an exponential tail bound, the terms associated with\n\nas sub-Gaussian and sub-exponential tails.\n\n6\n\n(cid:113)\n\nlog (cid:0) 1\n\nδ\n\n(cid:1) and log (cid:0) 1\n\nδ\n\n(cid:1) are respectively referred to\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 3. Let A : Z N (cid:55)→ W be a learning algorithm that has Lq-stability with parameter γq for q ≥ 1.\n\n(a) If Assumption 1 holds and l(·; ·) ≤ M , then ∀q ≥ 2,\n\n∥R(A(S)) − R∗ − ∆opt∥q\n\n≲ E[∆opt] + qγq log(N ) +\n\n(M + B)q N\n\n.\n\n(b) If Assumption 2 holds and l(·; ·) is G-Lipschitz with respect to its first argument, then ∀q ≥ 2,\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\nG2q μN\n\n.\n\n√\n\nRemark 5. Suppose that A satisfies the exponential stability bound in Eq. (7), and thus A has Lqq. Then combined with Lemma 4, the Lq-norm risk bounds in Theorem 3 stability by γq = aq + b suggest that the following exponential tail bound holds: (cid:18) 1 δ\n\nR(A(S)) − R∗ ≲ ∆opt + E[∆opt] + a log(N ) log2\n\nlog (1/δ) N\n\n+ b log(N ) log1.5\n\n(cid:18) 1 δ\n\n+\n\n(cid:19)\n\n(cid:19)\n\n.\n\nRemark 6. In part (a), the M -bounded-loss condition is not essential and it can be relaxed to a subexponential (or sub-Gaussian) variant by alternatively using the general Bernstein-type inequalities for sums of independent sub-exponential random variables (Vershynin, 2018). Concerning part (b), under the quadratic growth condition, the loss is allowed to be unbounded if it is Lipschitz continuous.\n\n3 APPLICATION TO INEXACT L0-ERM\n\nIn this section, we demonstrate an application of our Lq-stability and generalization theory to the following problem of high-dimensional stochastic risk minimization under hard sparsity constraint:\n\nmin w∈W\n\nR(w) := EZ[l(w; Z)]\n\nsubject to ∥w∥0 ≤ k,\n\nwhere W ⊆ Rd the cardinality constraint ∥w∥0 ≤ k for k ≪ d is imposed for enhancing the interpretability and learnability of model in situations where there are no clear favourite explanatory variables, or the model is over-parameterized. We consider the following L0-ERM problem over training set S = {Zi}i∈[N ]:\n\n(cid:40)\n\nw∗\n\nS,k = arg min ∥w∥0≤k\n\nRS(w) :=\n\n(cid:41)\n\nl(w; Zi)\n\n.\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(8)\n\nSince the problem is known to be NP-hard (Natarajan, 1995) in general, it is computationally intractable to solve it exactly in general cases. Alternatively, we consider the inexact L0-ERM oracle as a meta-algorithm outlined in Algorithm 1. In order to avoid assuming unrealistic conditions like restricted isometry property (RIP), it is typically needed to allow sparsity level relaxation for approximate algorithms like IHT to achieve favorable converge behavior (Jain et al., 2014; Shen & Li, 2017; Yuan et al., 2018; Murata & Suzuki, 2018). Therefore, we are particularly interested in\n\nAlgorithm 1: Inexact L0-ERM Oracle Input Output: ̃wS,k. Compute an inexact k-sparse L0-ERM estimation ̃wS,k such that\n\n: A training data set S = {Zi}i∈[N ] and the desired sparsity level k.\n\n• ̃wS,k is optimal over its support ̃J = supp( ̃wS,k), i.e., ̃wS,k = arg minw∈W,supp(w)⊆ ̃J RS(w); • ̃wS,k attains certain ̄k-sparse sub-optimality level ∆ ̄k,opt ≥ 0 for some ̄k ≤ k such that\n\nRS( ̃wS,k) − RS(w∗\n\nS, ̄k) ≤ ∆ ̄k,opt.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nthe inexact L0-ERM oracle with ̄k-sparse sub-optimality ∆ ̄k,opt ≥ 0 for some ̄k ≤ k such that the output ̃wS,k of Algorithm 1 satisfies\n\nRS( ̃wS,k) − RS(w∗\n\nS, ̄k) ≤ ∆ ̄k,opt.\n\nIt is typical that ∆ ̄k,opt is a random value over the training set S. For example, the sub-optimality guarantees of IHT for empirical risk usually hold with high probability over training data (Jain ̄k := arg min∥w∥0≤ ̄k R(w) be the ̄k-sparse minimizer of population risk for et al., 2014). Let w∗ some ̄k ≤ k. We are interested in deriving exponential upper bounds for the ̄k-sparse excess risk given by R( ̃wS,k) − R(w∗\n\n ̄k).\n\nOur analysis also relies on the conditions of Restricted Strong Convexity (RSC) which extends the concept of strong convexity to the analysis of sparsity recovery methods (Bahmani et al., 2013; Blumensath & Davies, 2009; Jain et al., 2014; Yuan et al., 2020). Definition 3 (Restricted Strong Convexity). For any sparsity level 1 ≤ s ≤ d, we say a function f is restricted μs-strongly convex if there exists some μs > 0 such that\n\nf (w) − f (w′) − ⟨∇f (w′), w − w′⟩ ≥\n\n∥w − w′∥2, ∀∥w − w′∥0 ≤ s.\n\nμs 2\n\nSpecially when s = d, we say f is μ-strongly convex if it is μd-strongly convex.\n\nThe following basic assumptions will be used in our theoretical analysis. Assumption 3. The loss function l(·; ·) is convex and G-Lipschitz with respect to its first argument. Assumption 4. The population risk R is μ-strongly convex and the empirical risk RS is μk-strongly convex with probability at least 1 − δN over sample S for some δN ∈ (0, 1). Assumption 5. The domain of interest is uniformly bounded such that ∥w∥ ≤ D, ∀w ∈ W. Remark 7. Assumption 3 is common in the study of algorithmic stability and generalization theory. Assumption 4 is conventional in the sparsity recovery analysis of L0-ERM. Assumption 5 is needed for establishing the Lq-stability of L0-ERM in Lemma 1 to follow. Similar conditions have also been assumed in the prior work of Yuan & Li (2022).\n\nLet w∗ := arg minw∈W R(w) be the global minimizer which is unique due to the strong convexity of R. For a given index set J ⊆ [d], let us consider the following restrictive estimator over J:\n\nw∗\n\nS|J :=\n\narg min w∈W,supp(w)⊆J\n\nRS(w).\n\n(9)\n\nWe first present the following lemma that guarantees the Lq-stability of w∗ |J| = k. See Appendix C.1 for its proof. Lemma 1. Assume that Assumptions 3, 4 and 5 hold and log(1/δN ) set of indices of cardinality k. Then for any q ≥ 2, the oracle estimator w∗ parameter\n\nlog(N ) ≥ 2. Let J ⊆ [d], |J| = k be a S|J has Lq-stability with\n\nS|J for any fixed J with\n\nγq =\n\n1 N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GD log(N )q log(1/δN )\n\n.\n\nRemark 8. For sparse linear regression models, it can be verified based on the result by Agarwal et al. (2012, Lemma 6) that Assumptions 4 holds with δN = e−c0N for some universal positive constant c0. Then we have log(1/δN ) log(N ) ≥ 2 for sufficiently large N , and Lemma 1 implies that γq ≲ 1\n\nlog(N ) = c0N (cid:1) for all q ≥ 2.\n\n+ GD log(N )q\n\n(cid:0) G2 μk\n\nN\n\nc0\n\nThe following theorem is our main result on the sparse excess risk of the inexact L0-ERM oracle as defined in Algorithm 1. See Appendix C.2 for its proof which is stimulated by that of Theorem 3. Theorem 4. Suppose that Assumptions 3, 4, 5 hold. Assume that log(1/δN ) log(N ) ≥ 2. Then for any δ ∈ (0, e−1), the following ̄k-sparse excess risk bound holds with probability at least 1 − δ over the random draw of S: R( ̃wS,k) − R(w∗ ̄k) GD (cid:0)k log (cid:0) ed (cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1) + log (cid:0) e N\n\n(cid:19) k log (cid:0) ed\n\n(cid:18) G2 μk\n\nlog2(N )\n\nlog(N )\n\nG2 μ\n\n+ GD\n\n(cid:1)(cid:1)2\n\n+\n\n+\n\n≲\n\n(cid:19)\n\n(cid:18)\n\n(cid:1)\n\nk\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nRemark 9. For the IHT-style algorithms, the sparse optimization sub-optimality ∆ ̄k,opt can be arbitrarily small (with high probability) after sufficient rounds of iteration (Jain et al., 2014).\n\nSpecially for sparse linear regression models in which Assumptions 4 holds with δN = e−c0N (Agarwal et al., 2012), we have that log(1/δN ) log(N ) ≥ 2 can always be fulfilled for sufficiently large sample size N , and the sparse excess risk bound in Theorem 4 roughly scales as\n\nlog(N ) = c0N\n\nR( ̃wS,k) − R(w∗\n\n ̄k) ≲ (k log (d) + log (1/δ))2 log2(N )\n\nN\n\n(cid:115)\n\n+\n\n(k log (d) + log (1/δ)) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nN\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\nN\n\nGenerally for misspecified sparsity models, the dominant rate in the above bound matches the (cid:1) sparse excess risk bound of Yuan & Li (2022, Theorem 1) for IHT under similar conditions. O(cid:0) 1√ (cid:1) bound available in that paper (Yuan & Li, 2022, Theorem 3), the preceding Compared to the O(cid:0) 1 bound is generally slower in rate but more broadly applicable without imposing any strong-signal or bounded-loss conditions as required in the analysis of Yuan & Li (2022). For well-specified ̄k-sparse models such that R(w∗ truly ̄k-sparse, the preceding bound improves to\n\n ̄k) = R(w∗), i.e., the population minimizer is\n\nN\n\nR( ̃wS,k) − R(w∗\n\n ̄k) ≲ (k log (d) + log (1/δ))2 log2(N )\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\nN Therefore, our bound is more appealing in the sense that it naturally adapts to well-specified models to attain an improved O( 1 N ) rate. In contrast, the regularization technique used by Yuan & Li (2022, Theorem 1) needs an optimal choice of penalty strength of scale O( 1√ ) which leads to an overall slow rate of convergence, though the analysis is relatively simpler.\n\nN\n\nWe further comment on the role of Lq-stability played in deriving the improved bound of Theorem 4. The O( 1 N ) fast-rate component of the bound is indeed rooted from the Lq-stability coefficient as established in Lemma 1 and an application of Lemma 6. The relatively slow O( 1√ )\n ̄k) − R(w∗), is mainly due to a careful analcomponent, which is controlled by the oracle factor R(w∗ ysis customized for handling the combinatorial optimization nature of L0-ERM. Such a slow-rate term would be vanished if the global minimizer w∗ is truly ̄k-sparse. Therefore, we confirm that the fast-rate component attributes to our Lq-stability theory, while the slow but adaptive rate component mainly attributes to the optimization property of L0-ERM. Finally, we comment in passing that our improved Lq-stability theory can also be applied to some prior applications such as unbounded ridge regression and k-folds cross-validation to obtain sharper generalization bounds.\n\nN\n\n4 CONCLUSION\n\nIn this paper, we presented an improved generalization theory for Lq-stable learning algorithms. There exits a clear discrepancy between the recently developed near-optimal generalization bounds for uniformly stable algorithms and the best known yet sub-optimal bounds for Lq-stable algorithms. Aiming at closing such a theoretical gap, we for the first time derived a set of near-optimal exponential generalization bounds for Lq-stable algorithms that match those of uniformly stable algorithms. As a concrete application of our Lq-stability theory, we have applied the developed analysis tools to derive strong exponential risk bounds for inexact sparsity-constrained ERM estimators under milder conditions. To conclude, Lq-stable algorithms generalize almost as fast as uniformly stable algorithms, though the distribution-dependent notion of Lq-stability is weaker than uniform stability.\n\nACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING\n\nThe authors would like to thank the anonymous Reviewers and Area Chairs for their insightful comments which are truly helpful for improving this paper. Xiao-Tong Yuan is funded in part by the National Key Research and Development Program of China under Grant No.2018AAA0100400, and in part by the Natural Science Foundation of China (NSFC) under Grant No.U21B2049, No.61936005 and No.61876090.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nKarim Abou-Moustafa and Csaba Szepesv ́ari. An exponential Efron-Stein inequality for l q stable In Proceedings of the Conference on Algorithmic Learning Theory (ALT), pp.\n\nlearning rules. 31–63, Chicago, IL, 2019.\n\nAlekh Agarwal, Sahand Negahban, Martin J Wainwright, et al. Fast global convergence of gradient methods for high-dimensional statistical recovery. The Annals of Statistics, 40(5):2452–2482, 2012.\n\nSohail Bahmani, Bhiksha Raj, and Petros T. Boufounos. Greedy sparsity-constrained optimization.\n\nJ. Mach. Learn. Res., 14(1):807–841, 2013.\n\nRaef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic convex optimization with optimal rates. In Advances in Neural Information Processing Systems (NeurIPS), pp. 11279–11288, Vancouver, Canada, 2019.\n\nRaef Bassily, Vitaly Feldman, Crist ́obal Guzm ́an, and Kunal Talwar. Stability of stochastic gradient In Advances in Neural Information Processing Systems\n\ndescent on nonsmooth convex losses. (NeurIPS), virtual, 2020.\n\nThomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed sensing. Ap-\n\nplied and Computational Harmonic Analysis, 27(3):265–274, 2009.\n\nSt ́ephane Boucheron, G ́abor Lugosi, and Pascal Massart. Concentration inequalities using the en-\n\ntropy method. The Annals of Probability, 31(3):1583–1614, 2003.\n\nSt ́ephane Boucheron, Olivier Bousquet, G ́abor Lugosi, and Pascal Massart. Moment inequalities for functions of independent random variables. The Annals of Probability, 33(2):514–560, 2005.\n\nSt ́ephane Boucheron, G ́abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp-\n\ntotic theory of independence. Oxford university press, 2013.\n\nOlivier Bousquet and Andr ́e Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499–526,\n\n2002.\n\nOlivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable algorithms. In Proceedings of the Conference on Learning Theory (COLT), pp. 610–626, Virtual Event [Graz, Austria], 2020.\n\nAlain Celisse and Benjamin Guedj. Stability revisited: new generalisation bounds for the leave-one-\n\nout. arXiv preprint arXiv:1608.06412, 2016.\n\nAlain Celisse and Tristan Mary-Huard. Theoretical analysis of cross-validation for estimating the\n\nrisk of the $k$-nearest neighbor classifier. J. Mach. Learn. Res., 19:58:1–58:54, 2018.\n\nZachary Charles and Dimitris S. Papailiopoulos. Stability and generalization of learning algorithms that converge to global optima. In Proceedings of the 35th International Conference on Machine Learning (ICML), pp. 744–753, Stockholmsm ̈assan, Stockholm, Sweden, 2018.\n\nLe-Yu Chen and Sokbae Lee. Best subset binary prediction. Journal of Econometrics, 206(1):39–56,\n\n2018.\n\nLe-Yu Chen and Sokbae Lee. Binary classification with covariate selection through l0-penalized\n\nempirical risk minimization. The Econometrics Journal, pp. 1–16, 2020.\n\nYuan Shih Chow and Henry Teicher. Probability theory: independence, interchangeability, martin-\n\ngales. Springer Science & Business Media, 2003.\n\nRichard Combes. An extension of McDiarmid’s inequality. arXiv preprint arXiv:1511.05240, 2015.\n\nQi Deng and Wenzhi Gao. Minibatch and momentum model-based methods for stochastic weakly In Advances in Neural Information Processing Systems (NeurIPS), pp.\n\nconvex optimization. 23115–23127, virtual, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLuc Devroye and Terry J. Wagner. Distribution-free inequalities for the deleted and holdout error\n\nestimates. IEEE Trans. Inf. Theory, 25(2):202–207, 1979.\n\nVitaly Feldman and Jan Vondr ́ak. Generalization bounds for uniformly stable algorithms. In Advances in Neural Information Processing Systems (NeurIPS), pp. 9770–9780, Montr ́eal, Canada, 2018.\n\nVitaly Feldman and Jan Vondr ́ak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Proceedings of the Conference on Learning Theory (COLT), pp. 1270–1279, Phoenix, AZ, 2019.\n\nVitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal rates in linear time. In Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC), pp. 439–449, Chicago, IL, 2020.\n\nDylan J. Foster and Vasilis Syrgkanis. Statistical learning with a nuisance component. In Proceed-\n\nings of the Conference on Learning Theory (COLT), pp. 1346–1348, Phoenix, AZ, 2019.\n\nSimon Foucart. Hard thresholding pursuit: An algorithm for compressive sensing. SIAM J. Numer.\n\nAnal., 49(6):2543–2563, 2011.\n\nRahul Garg and Rohit Khandekar. Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pp. 337–344, Montreal, Canada, 2009.\n\nMoritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic In Proceedings of the 33nd International Conference on Machine Learning\n\ngradient descent. (ICML), pp. 1225–1234, New York City, NY, 2016.\n\nPrateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for highdimensional m-estimation. In Advances in Neural Information Processing Systems (NIPS), pp. 685–693, Montreal, Canada, 2014.\n\nSatyen Kale, Ravi Kumar, and Sergei Vassilvitskii. Cross-validation and mean-square stability. In Proceedings of the Conference on Innovations in Computer Science (ICS), pp. 487–495, Beijing, China, 2011.\n\nYegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with conIn Advances in Neural Information Processing Systems (NeurIPS), pp.\n\nvergence rate o(1/n). 5065–5076, virtual, 2021.\n\nVladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization.\n\nThe Annals of Statistics, 34(6):2593–2656, 2006.\n\nAryeh Kontorovich. Concentration in unbounded metric spaces and algorithmic stability. In Proceedings of the 31th International Conference on Machine Learning (ICML), pp. 28–36, Beijing, China, 2014.\n\nSamuel Kutin. Extensions to McDiarmid’s inequality when differences are bounded with high probability. Dept. Comput. Sci., Univ. Chicago, Chicago, IL, USA, Tech. Rep. TR-2002-04, 2002.\n\nSamuel Kutin and Partha Niyogi. Almost-everywhere algorithmic stability and generalization error. In Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence (UAI), pp. 275– 282, Edmonton, Canada, 2002.\n\nIlja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient descent. In Proceedings of the 35th International Conference on Machine Learning (ICML), pp. 2820– 2829, Stockholmsm ̈assan, Stockholm, Sweden, 2018.\n\nYunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic In Proceedings of the 37th International Conference on Machine Learning\n\ngradient descent. (ICML), pp. 5809–5819, Virtual Event, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nXingguo Li, Tuo Zhao, Raman Arora, Han Liu, and Jarvis D. Haupt. Stochastic variance reduced optimization for nonconvex sparse learning. In Proceedings of the 33nd International Conference on Machine Learning (ICML), pp. 917–925, New York City, NY, 2016.\n\nAndreas Maurer and Massimiliano Pontil. Concentration inequalities under sub-gaussian and subexponential conditions. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7588–7597, virtual, 2021.\n\nSayan Mukherjee, Partha Niyogi, Tomaso A. Poggio, and Ryan M. Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Adv. Comput. Math., 25(1-3):161–193, 2006.\n\nTomoya Murata and Taiji Suzuki. Sample efficient stochastic gradient iterative hard thresholding method for stochastic sparse linear regression with limited attribute observation. In Advances in Neural Information Processing Systems (NeurIPS), pp. 5317–5326, Montr ́eal, Canada, 2018.\n\nBalas Kausik Natarajan. Sparse approximate solutions to linear systems. SIAM J. Comput., 24(2):\n\n227–234, 1995.\n\nAlexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.\n\nAnalysis and Applications, 3(04):397–417, 2005.\n\nWilliam H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for\n\nlocal discrimination rules. The Annals of Statistics, pp. 506–514, 1978.\n\nShai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability\n\nand uniform convergence. J. Mach. Learn. Res., 11:2635–2670, 2010.\n\nJie Shen and Ping Li. A tight bound of hard thresholding. J. Mach. Learn. Res., 18:208:1–208:42,\n\n2017.\n\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\n\nvolume 47. Cambridge university press, 2018.\n\nJialei Wang, Mladen Kolar, Nathan Srebro, and Tong Zhang. Efficient distributed learning with sparsity. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 3636–3645, Sydney, Australia, 2017.\n\nLutz Warnke. On the method of typical bounded differences. Combinatorics, Probability and\n\nComputing, 25(2):269–299, 2016.\n\nXiao-Tong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. IEEE Trans.\n\nInf. Theory, 68(10):6663–6681, 2022.\n\nXiao-Tong Yuan, Ping Li, and Tong Zhang. Gradient hard thresholding pursuit. J. Mach. Learn.\n\nRes., 18:166:1–166:43, 2018.\n\nXiao-Tong Yuan, Bo Liu, Lezi Wang, Qingshan Liu, and Dimitris N. Metaxas. Dual iterative hard\n\nthresholding. J. Mach. Learn. Res., 21:152:1–152:50, 2020.\n\nTong Zhang. Leave-one-out bounds for kernel methods. Neural Comput., 15(6):1397–1437, 2003.\n\nPan Zhou, Xiao-Tong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via minibatch proximal update. In Advances in Neural Information Processing Systems (2019), pp. 1532–1542, Vancouver, Canada, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nIn this section, we collect some preliminary results that will be used in our analysis. We start by introducing the following Lq-norm generalization of the celebrated Efron-Stein inequality, which is a corollary of Boucheron et al. (2005, Theorem 2).\n\nProposition 1 (Generalized Efron-Stein inequality (Celisse & Guedj, 2016)). Let S = {Z1, ..., ZN } be a set of independent random variables valued in Z and g : Z N (cid:55)→ R be some measurable function. Then there exists a universal constant κ < 1.271 such that for all q ≥ 2,\n\n∥g(S) − E[g(S)]∥q ≤ (cid:112)2κq\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:0)g(S) − g(S(i))(cid:1)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n.\n\nThe following result is an immediate consequence of Proposition 1 when applied to sum of independent random variables, which revisits a version of Marcinkiewicz-Zygmund inequality (Chow & Teicher, 2003).\n\nProposition 2. Let Z1, ..., ZN be a set of independent centered random variables. Then for all q ≥ 2,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nZi\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2(cid:112)2κq\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nZ 2 i\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n.\n\nThe following lemma is simple yet useful in our analysis.\n\nLemma 2. Let S = {Z1, ..., ZN } be a set of independent random variables valued in some measure space Z and g : Z N (cid:55)→ R be some measurable function. Then for all I ⊆ [N ] and q ≥ 1, we have\n\n∥g(SI )∥q ≤ ∥g(S)∥q = ∥∥g∥q(SI )∥q.\n\nProof. Recall g(SI ) = E[g(S) | SI ]. Then using Jensen’s inequality we can show that\n\n∥g(SI )∥q = (E [|E[g(S) | SI ]|q])1/q ≤ (E [E[|g(S)|q | SI ]|])1/q = (E[|g(S)|q])1/q = ∥g(S)∥q.\n\nBy definition we can also express ∥g(S)∥q = (E [E[|g(S)|q | SI ]|])1/q = ∥∥g(S)∥q(SI )∥q.\n\nAs a direct consequence of Lemma 2, the following result indicates that conditional expectation does not expand the differences in Lq-norm. Lemma 3. Let S = {Z1, ..., ZN } be a set of independent random variables valued in some measure space Z and g : Z N (cid:55)→ R be some measurable function. Let I ⊆ [N ] be an index set. Then for all i ∈ I and q ≥ 1,\n\n(cid:13) (cid:13)\n\n(cid:13)g(SI ) − g(S(i) I )\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\n(cid:13) (cid:13)g(S) − g(S(i)) (cid:13)\n\n(cid:13) (cid:13) (cid:13)q\n\n.\n\nProof. For each i ∈ I, by applying Lemma 2 to g(S) − g(S(i)) we can show that ∥g(SI ) − g(S(i)\n\nI )∥q ≤ ∥g(S) − g(S(i))∥q, which gives the desired result.\n\nWe also need the following lemma about the equivalence between tails and moments (see, e.g., Bousquet et al., 2020).\n\nLemma 4. Let Y be a real-valued random variable.\n\n• Suppose that Y satisfies the following inequality for some a, b ≥ 0 with probability at least\n\n1 − δ for any δ ∈ (0, 1),\n\n|Y | ≤ a log\n\n(cid:17)\n\n(cid:16) e δ\n\n(cid:114)\n\n+ b\n\nlog\n\n(cid:17)\n\n.\n\n(cid:16) e δ\n\nThen, for any q ≥ 1 it holds that\n\n∥Y ∥q ≤ 3aq + 9b\n\n√\n\nq.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\n• Suppose that Y satisfies ∥Y ∥q ≤ f (q) for any 1 ≤ ql ≤ q < qu and some non-negative real function f . Then the following holds with probability at least 1 − δ for any δ ∈ (e1−qu , e1−ql ]: (cid:16)\n\n(cid:17)(cid:17)\n\n|Y | ≤ ef\n\nlog\n\n.\n\n(cid:16) e δ\n\nProof. We only prove the second part which slightly generalizes the corresponding result of Bousquet et al. (2020, Lemma 1). For any δ ∈ (e1−qu , e1−ql ], we choose q = log(e/δ) ∈ [ql, qu). Using the condition ∥Y ∥q ≤ f (q) and Markov’s inequality yields\n\n(cid:16)\n\nP\n\n|Y | > ef\n\n(cid:17)(cid:17)(cid:17)\n\n(cid:16)\n\nlog\n\n(cid:16) e δ\n\n≤ P (|Y | > e∥Y ∥q) ≤\n\nE[|Y |q] eq∥Y ∥q\n\nq\n\n=\n\nδ e\n\n≤ δ.\n\nThis proves the desired bound in the second part.\n\nRemark 10. Suppose that for any δ ∈ (0, 1), the following inequality holds with probability at least 1 − δ over S, S(i), Z:\n\n(cid:12) (cid:12) (cid:12)l(A(S); Z) − l(A(S(i)); Z) (cid:12) (cid:12) (cid:12) ≤ a log\n\n(cid:17)\n\n(cid:16) e δ\n\n(cid:114)\n\n+ b\n\nlog\n\n(cid:17)\n\n.\n\n(cid:16) e δ\n\nThen according to the first part of Lemma 4 we have that A is Lq-stable by γq = 3aq + 9b Remark 11. Specially if ql = 1 and qu = ∞ is allowed, then the second bound in Lemma 4 holds with an arbitrary tail bound δ ∈ (0, 1).\n\nq.\n\n√\n\nFinally, we present the following technical lemma about self-bounding inequalities to be used for showing fast rates of excess risk bounds under Bernstein or quadratic growth conditions. Lemma 5. Let x, a, b, c be a set of non-negative quantities satisfying x ≤ a + (cid:112)b(x + c). Then it must hold that x ≤ 3a+2b+c\n\n.\n\n2\n\nProof. If x ≤ a, then the claim holds trivially. In the complementary case of x > a, by condition we must have (x − a)2 ≤ b(x + c), which then implies 2a + b + 2(cid:112)b(a + c) 2\n\n3a + 2b + c 2\n\nx ≤\n\n≤\n\n,\n\nwhere we have used the basic fact 2(cid:112)b(a + c) ≤ b + a + c.\n\nB PROOFS FOR SECTION 2\n\nB.1 PROOF OF THEOREM 1\n\nThe proof is a generalization of the sample-splitting arguments of Feldman & Vondr ́ak (2019); Bousquet et al. (2020) under the considered property of Lq-norm bounded difference. For the sake of completeness, we reproduce below the relatively simpler arguments of Bousquet et al. (2020, Theorem 4), with proper modifications made to adapt to our setting via using the generalized EfronStein inequality in places of McDiarmid’s inequality.\n\nProof of Theorem 1. Consider k such that 2k−1 < N ≤ 2k. If N < 2k, we pad the training set S with extra zero-functions so that N = 2k. Consider the partition I0, I1, ..., Ik of [N ] given by\n\nI0 = {{1}, ..., {2k}}, I1 = {{1, 2}, {3, 4}..., {2k − 1, 2k}}, Ik = {{1, ..., 2k}}. For any i ∈ [N ] and l = 0, ..., k, we denote by I l(i) ∈ Il the only set from Il that contains i and consider the following random variables\n\ni = E gl\n\n(cid:104) gi | Zi, SI l(i)\n\n(cid:105)\n\n.\n\nIn particular, g0\n\ni = gi and gk\n\ni = E[gi | Zi]. Clearly we have the following telescope sum:\n\nk−1 (cid:88)\n\n(gl\n\ni − gl+1\n\ni\n\ngi =\n\n) + E[gi | Zi].\n\nl=0\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nIt follows that\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi − E[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\nk−1 (cid:88)\n\nl=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ni − gl+1 gl\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n.\n\n(10)\n\nWe need to upper bound the right hand side of the above inequality. To this end, it can be verified that\n\ni = E gl+1\n\n(cid:104) (cid:105) gi | Zi, SI l+1(i)}\n\n(cid:104)\n\n= E\n\ngl i | Zi, SI l+1(i)\n\n(cid:105)\n\n.\n\nSince gi has a bounded Lq-difference by βq with respect to all variables except the i-th variable, it is known from Lemma 3 that so is gl i for each l = 0, ..., k. Conditioned on Zi, SI l+1(i), invoking Proposition 1 to gl\n\ni yields\n\n∥gl\n\ni − gl+1\n\ni\n\n(cid:16)\n\n∥q\n\nZi, SI l+1(i)\n\n(cid:17)\n\n≤\n\n(cid:112)\n\n2κq2lβq,\n\nas there are 2l indices in I l+1(i) \\ I l(i). It follows from Lemma 2 that\n\n∥gl\n\ni − gl+1\n\ni\n\n∥q =\n\n(cid:13) (cid:13)∥gl (cid:13)\n\ni − gl+1\n\ni\n\n∥q(Zi, SI l+1(i))\n\n(cid:13) (cid:13) (cid:13)q\n\n(cid:112)\n\n≤\n\n2κq2lβq.\n\nNow consider any I l ∈ Il. Since for each i ∈ Il, gl independent and centered conditioned on SI l . Therefore, applying Proposition 2 yields\n\ndepends only on Zi, SI l , these terms are\n\ni − gl+1\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈I l\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:0)SI l\n\n(cid:1) ≤ 2\n\n(cid:112)\n\n2κq2l ×\n\n(cid:112)\n\n2κq2lβq = 4κq2lβq,\n\nwhich according to Lemma 2 implies that\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈I l\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 4κq2lβq.\n\nThen based on the triangle inequality we get\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈[N ]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:88)\n\n≤\n\nI l∈Il\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\ni − gl+1 gl\n\ni\n\ni∈I l\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2k−l × 4κq2lβq = 4κq2kβq < 4κqN βq.\n\nFinally, the right hand side of Eq. (10) can be bounded as\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi − E[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\nk−1 (cid:88)\n\nl=0\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ni − gl+1 gl\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 4κqN ⌈log2 N ⌉βq,\n\n(11)\n\nwhich gives the first desired bound. In view of Eq. (11) and the triangle inequality we have\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\ngi\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nE[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+ 4κqN ⌈log2 N ⌉βq.\n\n(12)\n\nSince ∥E[gi(S) | Zi]∥q ≤ Mq and E[gi(S) | S \\ Zi] = 0, it follows from Proposition 2 that the first term at the right hand side of Eq. (12) can be bounded as\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nE[gi | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2(cid:112)2κN qMq.\n\n(13)\n\nThe second desired bound is obtained by plugging Eq. (13) into Eq. (12).\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB.2 PROOF OF THEOREM 2\n\nThe proof technique follows that of Bousquet et al. (2020, Lemma 7) developed for uniformly stable algorithms, with natural adaptation to the distribution-dependent notion of Lq-stability.\n\n\n\n.\n\n \n \n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nProof. Let us consider\n\nhi(S) := R(A(S)) − l(A(S); Zi), gi(S) = EZ′\n\ni\n\n(cid:105) (cid:104) R(A(S(i))) − l(A(S(i)); Zi)\n\n.\n\nThen the Lq-norm of the generalization gap can be bounded as\n\n\n\n∥R(A(S)) − RS(A(S))∥q =\n\n1 N\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nhi(S)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\n1 N\n\nN (cid:88)\n\ni=1\n\n \n \n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\ngi(S)\n\n+\n\n(hi(S) − gi(S))\n\n(cid:123)(cid:122) A\n\n(cid:123)(cid:122) B\n\n(14) We next respectively upper bound the two terms A and B in Eq. (14). To bound the term A, by definition it holds that E[gi(S) | S \\ Zi] = 0. Based on the triangle inequality we can show that\n\n∥E[gi(S) | Zi]∥q ≤∥gi(S)∥q\n\n=\n\n(cid:13) (cid:13) (cid:13)\n\nEZ′\n\ni\n\n[EZ[l(A(S(i)); Z)]] − EZ′\n\ni\n\n(cid:104)\n\nl(A(S(i)); Zi)\n\n(cid:105)(cid:13) (cid:13) (cid:13)q\n\n≤∥l(A(S(i)); Z)∥q + ∥l(A(S(i)); Zi)∥q ≤ 2Mq,\n\nwhere in the first and second inequalities we have twice used Lemma 2. Next we further show that gi has a bounded Lq-norm difference by 2γq with respect to all variables in S except Zi. Indeed, for each j ̸= i it can be verified that\n\ni\n\n(cid:104)\n\nEZ′\n\nR(A(S(i))) − R(A((S(i))(j)))\n\n(cid:13) (cid:13) (cid:13)gi(S) − gi(S(j)) (cid:13) (cid:13) (cid:13)q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)l(A(S(i)); Z) − l(A((S(i))(j)); Z) (cid:13) (cid:13) (cid:13)q\n\nEZ′\n\ni\n\nEZ[l(A(S(i)); Z) − l(A((S(i))(j)); Z)]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤\n\n=\n\n≤\n\n(cid:105)(cid:13) (cid:13) (cid:13)q\n\n+\n\nEZ′\n\ni\n\n(cid:104) l(A(S(i)); Zi) − l(A((S(i))(j)); Zi)\n\n(cid:105)(cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13)\n\nEZ′\n\ni\n\n[l(A(S(i)); Zi) − l(A((S(i))(j)); Zi)]\n\n(cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13)l(A(S(i)); Zi) − l(A((S(i))(j)); Zi) (cid:13) (cid:13) (cid:13)q\n\n≤ 2γq,\n\nwhere in the last but one inequality we have used Lemma 2, while in the last equality we have used the Lq-stability assumption on the algorithm A. Therefore, {gi} satisfy the conditions of Theorem 1 and thus\n\nA =\n\ngi(S)\n\n≤ 4(cid:112)2κN qMq + 8κqN ⌈log2 N ⌉γq.\n\n(15)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nNow we proceed to bound the term B. It can be verified that\n\nB ≤\n\n=\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\n≤\n\ni=1\n\n(cid:104)\n\nR(A(S)) − R(A(S(i)))\n\nEZ′\n\ni\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:104)\n\nEZ′\n\ni\n\nEZ\n\nl(A(S); Z) − l(A(S(i)); Z)\n\nN (cid:88)\n\n(cid:105)\n\ni=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:105) (cid:104) l(A(S); Zi) − l(A(S(i)); Zi)\n\nEZ′\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\nEZ′\n\ni\n\nl(A(S); Zi) − l(A(S(i)); Zi)\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:13) (cid:13) (cid:13)l(A(S); Z) − l(A(S(i)); Z) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13)l(A(S); Zi) − l(A(S(i)); Zi) (cid:13)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤ 2N γq,\n\nN (cid:88)\n\ni=1\n\n(16) where in the last but one inequality we have used Lemma 2, and in the last equality we have used the Lq-stability assumption. Plugging bounds Eq. (15) and Eq. (16) into Eq. (14) and preserving leading terms yields the desired result.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB.3 PROOF OF THEOREM 3\n\nWe need the following lemma which plays a fundamental role in proving the main result. Lemma 6. Let A : Z N (cid:55)→ W be a learning algorithm that has Lq-stability by γq for q ≥ 1. Suppose that ∥l(A(S); Z)∥q ≤ Mq for any Z ∈ Z. Let S′ be an independent copy of S. Then the following bound holds for all q ≥ 2:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nR(A(S)) − RS(A(S)) − E[R(A(S))] +\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(A(S′); Zi) | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≲ qγq log(N ).\n\nProof. Let us again consider gi(S) = EZ′ arguments to those of Theorem 2 we can show that\n\ni\n\n(cid:2)R(A(S(i))) − l(A(S(i)); Zi)(cid:3). Then using similar proof\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\nN (cid:88)\n\ni=1\n\n≤\n\n=\n\n≤\n\nN (R(A(S)) − RS(A(S))) −\n\ngi(S)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:104)\n\nR(A(S)) − R(A(S(i)))\n\nEZ′\n\ni\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:104)\n\nEZ′\n\ni\n\nEZ\n\nl(A(S); Z) − l(A(S(i)); Z)\n\nN (cid:88)\n\n(cid:105)\n\ni=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n(cid:105) (cid:104) l(A(S); Zi) − l(A(S(i)); Zi)\n\nEZ′\n\ni\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\nEZ′\n\ni\n\nl(A(S); Zi) − l(A(S(i)); Zi)\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)l(A(S); Z) − l(A(S(i)); Z) (cid:13) (cid:13) (cid:13)q\n\n+\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13)l(A(S); Zi) − l(A(S(i)); Zi) (cid:13)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤ 2N γq,\n\nwhich implies\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nR(A(S)) − RS(A(S)) −\n\n1 N\n\nN (cid:88)\n\ni=1\n\ngi(S)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 2γq.\n\nAlso, gi(S) satisfies the conditions of Theorem 1 with βq = 2γq and it follows from the second bound of Theorem 1 that for all q ≥ 2,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(gi(S) − E[gi(S) | Zi])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≤ 8κqγq⌈log2 N ⌉.\n\nCombining the above two yields\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nR(A(S)) − RS(A(S)) −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[gi(S) | Zi]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n≲ qγq log(N ).\n\nThe desired result follows by noting that\n\nE[gi(S) | Zi] = E[R(A(S′))] − E[l(A(S′); Zi) | Zi] = E[R(A(S))] − E[l(A(S′); Zi) | Zi].\n\nThis completes the proof.\n\nWith Lemma 6 in place, we are ready to prove the main result of Theorem 3.\n\nProof of Theorem 3. Consider any w∗ ∈ W ∗. It is standard to decompose and bound the excess risk as\n\nR(A(S)) − R∗\n\n=R(A(S)) − RS(A(S)) + RS(A(S)) − RS(w∗) + RS(w∗) − R∗ ≤∆opt + R(A(S)) − RS(A(S)) − (R∗ − RS(w∗))\n\n=∆opt + Γ(S) + E[R(A(S))] −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(A(S′); Zi) | Zi] − (R∗ − RS(w∗)),\n\n(17)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nwhere\n\nΓ(S) = R(A(S)) − RS(A(S)) − E[R(A(S))] +\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(A(S′); Zi) | Zi].\n\nSince we have the freedom to choose w∗, let us specify it in the above as w∗(S′) ∈ W ∗ which is the minimizer that satisfies the Bernstein condition in Assumption 1 associated with A(S′). Then, it follows from Eq. (17) that\n\nR(A(S)) − R∗ − ∆opt\n\n≤Γ(S) + E[R(A(S))] −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nConsequently,\n\nE[l(A(S′); Zi) | Zi] − (R∗ − E[RS(w∗(S′)) | S]).\n\n∥R(A(S)) − R∗ − ∆opt∥q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ ∥Γ(S)∥q +\n\nN (cid:88)\n\n1 N\n\ni=1\n\nE [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nζ1\n\n≲qγq log(N ) +\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n(cid:123)(cid:122) T\n\nE [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))])\n\n(18)\n\n,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nwhere in “ζ1” we have applied Lemma 6 to obtain ∥Γ(S)∥q ≤ qγq log(N ), and the fact E[R(A(S))] = E[R(A(S′))].\n\nPart (a): To bound the term T , using Bernstein’s inequality for sum of independent bounded variables 2 together with the generalized Bernstein condition we can show (see the proof arguments of Klochkov & Zhivotovskiy (2021, Theorem 1.1) for the details) that\n\n(cid:114)\n\nT ≲\n\n(cid:114)\n\n≤\n\nqBE[R(A(S)) − R∗] N\n\n+\n\nqM N\n\n=\n\n(cid:114)\n\nqB(E[R(A(S)) − R∗ − ∆opt] + E[∆opt]) N\n\n+\n\nqM N\n\nqB(∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]) N\n\n+\n\nqM N\n\n,\n\nwhere the last inequality is due to Jensen’s inequality. Therefore, combining the above and Eq. (18) yields that for some universal constant C:\n\n∥R(A(S))−R∗−∆opt∥q ≤ C\n\nqγq log(N ) +\n\n(cid:32)\n\n(cid:114)\n\nqB(∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]) N\n\n+\n\nqM N\n\n(cid:33)\n\n.\n\nBy invoking Lemma 5 to the above self-bounding inequality with x = ∥R(A(S)) − R∗ − ∆opt∥q, a = C(qγq log(N ) + qM\n\nN , and c = E[∆opt] we immediately obtain that\n\nN ), b = qB\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\n(M + B)q N\n\n.\n\nThis gives the desired bound in part (a).\n\nPart (b): Under the given conditions in part (b), we can bound the term T in Eq. (18) as follows for q ≥ 2:\n\n2It is possible to relax the M -boundedness condition on the loss function l to its sub-Gaussian or subexponential counterparts by alternatively applying general Bernstein-type inequalities for sums of independent sub-Gaussian or sub-exponential random variables (Vershynin, 2018) in this part of proof. For the sake of simplicity and transparency of exposition, here we choose to work on the bounded loss while keeping in mind that the requirement is not essential.\n\n18\n\nζ1 ≤\n\n≤\n\n≤\n\nζ2 ≤\n\nζ3 ≤\n\nζ4 ≤\n\n≤\n\n(cid:13) (cid:13) (cid:13)\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκq\n\nN\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n4G\n\n2κq\n\n√ √\n\n√\n\nN κq\n\nN √\n\nκq\n\nN μ\n\n8G √\n\n8G √\n\nPublished as a conference paper at ICLR 2023\n\nT =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n√\n\n2\n\n2κq N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(E [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] − (R∗ − E[R(A(S′))]))2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\nE2 [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi] + (R∗ − E[R(A(S′))])2(cid:13)\n\n(cid:13) (cid:13)q/2\n\n∥E2 [l(w∗(S′); Zi) − l(A(S′); Zi) | Zi]∥q/2 + N E2[R(w∗(S′)) − R(A(S′))]\n\n∥E2 [G∥w∗(S′) − A(S′)∥ | Zi]∥q/2 + N E2 [G∥w∗(S′) − A(S′)∥]\n\n(cid:112)E [∥w∗(S′) − A(S′)∥2]\n\n(cid:114) 1 μ\n\nE [R(A(S′)) − R∗] =\n\n8G √\n\n√\n\nκq\n\nN μ\n\n(cid:113)\n\nE [R(A(S′)) − R∗ − ∆opt] + E[∆opt]\n\n(cid:113)\n\n∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]\n\nin “ζ2” we have used the Lipschitz-loss condiwhere in “ζ1” we have used Proposition 2, tion, in “ζ3” we have used (cid:13) (cid:13)q/2 = E2 [G∥w∗(S′) − A(S′)∥] ≤ G2E (cid:2)∥w∗(S′) − A(S′)∥2(cid:3), in “ζ4” we have used Assumption 2, and the last inequality is due to Jensen’s inequality. Then, plugging the above bound of term T into Eq. (18) yields that for some universal constant C:\n\n(cid:13)E2 [G∥w∗(S′) − A(S′)∥ | Zi](cid:13)\n\n∥R(A(S)) − R∗ − ∆opt∥q ≤ C\n\n(cid:18)\n\nqγq log(N ) + G\n\n(cid:114) q\n\nN μ\n\n(cid:113)\n\n∥R(A(S)) − R∗ − ∆opt∥q + E[∆opt]\n\n(cid:19)\n\n.\n\nInvoking Lemma 5 to the above inequality with x = ∥R(A(S)) − R∗ − ∆opt∥q, a = Cqγq log(N ), b = qG2\n\nμN , and c = E[∆opt] yields\n\n∥R(A(S)) − R∗ − ∆opt∥q ≲ E[∆opt] + qγq log(N ) +\n\nqG2 μN\n\n.\n\nThis gives the desired bound in part (b). The proof is completed.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nC PROOFS FOR SECTION 3\n\nC.1 PROOF OF LEMMA 1\n\nProof. Let us consider the following event about the restricted strong convexity of RS:\n\nE : RS is μk-strongly convex.\n\nLet Y = 1E be the indication random variable associated with E. Then by Assumption 4 we have P(Y = 1) ≥ 1 − δN . Suppose that E occurs such that Y = 1. Then,\n\nRS(w∗ 1\nN\n\n(cid:88)\n\nj̸=i\n\n=\n\nS(i)|J ) − RS(w∗ (cid:16)\n\nS|J )\n\nl(w∗\n\nS(i)|J ; Zj) − l(w∗\n\nS|J ; Zj)\n\n(cid:17)\n\n+\n\n(cid:16)\n\n1 N\n\nl(w∗\n\nS(i)|J ; Zi) − l(w∗\n\nS|J ; Zi)\n\n(cid:17)\n\n=RS(i) (w∗\n\nS(i)|J ) − RS(i) (w∗\n\nS|J ) +\n\n(cid:16)\n\n1 N\n(cid:12) (cid:12)l(w∗ (cid:12) (cid:13) (cid:13)w∗ (cid:13)\n\n−\n\n1 N\n2G N\n\n≤\n\n≤\n\nl(w∗\n\nS(i)|J ; Z ′\n\ni) − l(w∗\n\nS(i)|J ; Zi) − l(w∗\n\nS|J ; Zi)\n\nS(i)|J − w∗\n\nS|J\n\n(cid:13) (cid:13) (cid:13) ,\n\n1 N\nS|J ; Z ′ i) (cid:12) (cid:12) (cid:12) +\n\nl(w∗\n\nS(i)|J ; Zi) − l(w∗\n\nS|J ; Zi)\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\n1 N\n\n(cid:12) (cid:12)l(w∗ (cid:12)\n\nS(i)|J ; Z ′\n\ni) − l(w∗\n\n(cid:12) S|J ; Z ′ (cid:12) i) (cid:12)\n\nwhere we have used the optimality of w∗ of loss. Since E occurs by assumption, RS is μk-strongly convex. Since w∗ over the supporting set J, we have\n\nS(i)|J with respect to RS(i) (w) and the Lipschitz continuity S|J is optimal for RS(w)\n\nRS(w∗\n\nS|J ) + (cid:13) (cid:13) (cid:13)w∗ Lipschitz continuity of l we have that for any Z ∈ Z, the following holds conditioned on Y = 1:\n\nCombing the preceding two inequalities yields\n\nS(i)|J − w∗ (cid:13) (cid:13) ≤ 4G (cid:13)\n\nμkN . Consequently from the\n\nS(i)|J ) ≥ RS(w∗\n\nS(i)|J − w∗\n\nS|J\n\nS|J\n\n.\n\nμk 2\n\n(cid:13) (cid:13)w∗ (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:12) (cid:12)l(w∗ (cid:12)\n\nS(i)|J ; Z) − l(w∗\n\n(cid:12) (cid:12) (cid:12) ≤ G S|J ; Z)\n\n(cid:13) (cid:13)w∗ (cid:13)\n\nS(i)|J − w∗\n\nS|J\n\n(cid:13) (cid:13) (cid:13) ≤\n\n4G2 μkN\n\n.\n\nIn the complementary case of Y = 0, in view of Assumption 5, it always holds that\n\n|l(w∗\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)| ≤ G\n\n(cid:13) (cid:13)w∗ (cid:13)\n\nS(i)|J − w∗\n\nS|J\n\n(cid:13) (cid:13) (cid:13) ≤ 2GD.\n\n(19)\n\n(20)\n\nLet us consider qu :=\n\nlog\n\n(cid:17)\n\n(cid:16) 1 δN\n\nlog(N ) . By assumption qu ≥ 2. Then for 2 ≤ q ≤ qu, it can be verified that\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)\n\nq(cid:105)\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:104)(cid:12) (cid:12)l(w∗ (cid:12) (cid:104)(cid:12) (cid:12)l(w∗ (cid:12)\n\nS(i)|J ; Z) − l(w∗\n\n(cid:12) (cid:12) S|J ; Z) (cid:12)\n\nq\n\n(cid:105)\n\n| Y = 1\n\nS(i)|J ; Z) − l(w∗ (cid:19)q\n\nS|J ; Z)\n\n(cid:105)\n\n| Y = 0\n\nq\n\n(cid:12) (cid:12) (cid:12)\n\nE\n\n(cid:104)(cid:12) (cid:12)l(w∗ (cid:12) =P(Y = 1)E\n\n+ P(Y = 0)E\n\n(cid:19)q\n\n≤\n\n(cid:18) 4G2 μkN\n\n+ δN (2GD)q =\n\n(cid:18) 4G2 μkN\n\n+\n\n1 N qu\n\n(2GD)q ≤\n\n(cid:19)q\n\n(cid:18) 4G2 μkN\n\n+\n\n1\n\nN q (2GD)q.\n\nIt follows that for all 2 ≤ q ≤ qu\n\n(cid:13) (cid:13)l(w∗ (cid:13)\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\n(cid:18)(cid:18) 4G2 μkN\n\n(cid:19)q\n\n+\n\n1\n\nN q (2GD)q\n\n(cid:19)1/q\n\n≤\n\n1 N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n,\n\nwhere we have used aq + bq ≤ (a + b)q for a, b > 0 and q ≥ 2. For the complementary case q > qu, it is trivial to show that\n\n(cid:13) (cid:13)l(w∗ (cid:13)\n\nS(i)|J ; Z) − l(w∗\n\nS|J ; Z)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤ 2GD ≤\n\n2GD qu\n\nq.\n\nAssembling the preceding two bounds yields the desired Lq-stability bound.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nC.2 PROOF OF THEOREM 4\n\nLet us denote a+ = max{a, 0}. We need the following key result in our analysis, whose proof idea draws large inspiration from that of Theorem 3 with proper modifications for handing the challenges imposed by the combinatorial optimization nature of L0-ERM.\n\nLemma 7. Suppose that Assumptions 3, 4, 5 hold. Assume that log(1/δN ) δ ∈ (0, e−1), it holds with probability at least 1 − δ that\n\nlog(N ) ≥ 2. Then for any\n\nsup J⊆[d],|J|=k\n\nR(w∗\n\nS|J ) − R(w∗ ̄k)\n\n≲\n\nGD (cid:0)k log (cid:0) ed\n\n(cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1)(cid:1)2\n\nlog2(N )\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nProof. Given a fixed index set J ⊆ [d] with |J| = k, we can show that the following holds for any q ≥ 1:\n\nS|J ) − RS(w∗\n\nS|J ) + RS(w∗) − R(w∗)\n\n(cid:13) (cid:13) (cid:13)q\n\n(cid:13) (cid:13)R(w∗ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\nΓS|J + E[R(w∗\n\nS|J )] −\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(w∗\n\nS′|J ; Zi) | Zi] + RS(w∗) − R(w∗)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\nE[R(w∗\n\nS|J )] −\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124)\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(w∗\n\nS′|J ; Zi) | Zi] − R(w∗) + RS(w∗)\n\n(cid:123)(cid:122) T\n\n≤ (cid:13)\n\n(cid:13)ΓS|J\n\n(cid:13) (cid:13)q +\n\nwhere\n\n(21)\n\n,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q (cid:125)\n\nΓS|J = R(w∗\n\nS|J ) − RS(w∗\n\nS|J ) − E[R(w∗\n\nS|J )] +\n\n1 N\n\nN (cid:88)\n\ni=1\n\nE[l(w∗\n\nS′|J ; Zi) | Zi].\n\nIn view of Lemma 1 we have that w∗\n\nS|J has Lq-stability by\n\nγq =\n\n1 N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GD log(N )q log(1/δN )\n\n.\n\nThen invoking Lemma 6 over the supporting set J yields\n\n(cid:13) (cid:13)ΓS|J\n\n(cid:13) (cid:13)q\n\n≲ qγq log(N ) =\n\nq log(N ) N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GD log2(N )q2 log(1/δN )\n\n.\n\n(22)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nWe now bound the term T in Eq. (21) as follows:\n\nT =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\n(cid:104)\n\nE\n\ni=1\n\nw∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:105)\n\n− (R(w∗) − E[R(w∗\n\nS′|J )])\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n√\n\n2\n\n2κq N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:104)\n\n(cid:16)\n\nE\n\nl(w∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:105)\n\n− (R(w∗) − E[R(w∗\n\nS′|J )])\n\n(cid:17)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκp\n\nN\n\n√\n\n4\n\nκp\n\nN\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1 (cid:115)\n\n4G\n\n2κp\n\n√ √\n\nN\n\n√\n\nκq\n\nN\n\n8G √\n\nζ1 ≤\n\n≤\n\n≤\n\nζ2 ≤\n\nζ3 ≤\n\nζ4 ≤\n\n(cid:104)\n\n(cid:104)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nE2\n\nl(w∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:16)\n\n(cid:105)\n\n+\n\nR(w∗) − E[R(w∗\n\nS′|J )]\n\n(cid:17)2(cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n(cid:104)\n\n(cid:13) (cid:13) (cid:13)\n\nE2\n\nl(w∗; Zi) − l(w∗\n\nS′|J ; Zi) | Zi\n\n(cid:105)(cid:13) (cid:13) (cid:13)q/2\n\n(cid:104)\n\n+ N E2\n\nR(w∗) − R(w∗\n\nS′|J )\n\n(cid:105)\n\n(23)\n\n(cid:13) (cid:13) (cid:13)\n\nE2\n\n(cid:104)\n\nG\n\n(cid:13) (cid:13) (cid:13)w∗ − w∗\n\nS′|J\n\n(cid:13) (cid:13) (cid:13) | Zi\n\n(cid:105)(cid:13) (cid:13) (cid:13)q/2\n\n+ N E2\n\n(cid:104)\n\nG\n\n(cid:13) (cid:13) (cid:13)w∗ − w∗\n\nS′|J\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13)\n\nE\n\n(cid:20)(cid:13) (cid:13) (cid:13)w∗ − w∗\n\nS′|J\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:114) 1 μ\n\n(cid:104)\n\nE\n\nR(w∗\n\nS′|J ) − R(w∗)\n\n(cid:105)\n\n=\n\n8G √\n\n√\n\n(cid:114)\n\nκq\n\nN μ\n\n(cid:104)\n\nE\n\nR(w∗\n\nS|J ) − R(w∗)\n\n(cid:105) ,\n\nwhere in “ζ1” we have used Proposition 2, sumption, in “ζ3” we have used\n\nG∥w∗ − w∗\n\nin “ζ2” we have used the Lipschitz-loss as- = E2 (cid:104) ≤\n\nG∥w∗ − w∗\n\n(cid:105) S′|J ∥\n\nS′|J ∥ | Zi\n\n(cid:13) E2 (cid:104) (cid:13) (cid:13)\n\n(cid:105)(cid:13) (cid:13) (cid:13)q/2\n\n∥w∗ − w∗\n\nG2E ging Eq. (22) and Eq. (23) into Eq. (21) yields that for any q ≥ 2,\n\n, in “ζ4” we have used the strong convexity condition in Assumption 4. Plug-\n\nS′|J ∥2(cid:105)\n\n(cid:13) (cid:13)R(w∗ (cid:13)\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\nq log(N ) N\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2q2GD log2(N ) log(1/δN )\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n+ 8G\n\nκqE\n\n(cid:104)\n\nR(w∗\n\n(cid:105) S|J ) − R(w∗) N μ\n\n(24)\n\n.\n\nNext we need to upper bound the factor E R(w∗ bound. To do so, let us consider q = 2 in Eq. (24). It follows from the optimality of w∗ and w∗ that\n\nin the second term of the above\n\nS|J\n\n(cid:104)\n\n(cid:105) S|J ) − R(w∗)\n\n(cid:104)\n\nE\n\n≤\n\n(cid:13) (cid:13)R(w∗ (cid:13)\n\nR(w∗\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) − R(w∗) − RS(w∗\n\n(cid:105) S|J ) + RS(w∗) (cid:13) (cid:13) (cid:13)2\n\nS|J ) + RS(w∗)\n\nEq. (24) ≤\n\nlog(N ) N\n\n(cid:18) 8G2 μk\n\n(cid:19)\n\n+ 4GD\n\n+\n\n8GD log2(N ) log(1/δN )\n\n+ 8G\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:104)\n\n2κE\n\n(cid:105)\n\nR(w∗\n\nS|J ) − R(w∗) N μ\n\n≤\n\nlog(N ) N\n\n(cid:18) 8G2 μk\n\n(cid:19)\n\n+ 4GD\n\n+\n\n8GD log2(N ) log(1/δN )\n\n+\n\n(cid:104)\n\nE\n\nR(w∗\n\n(cid:105) S|J ) − R(w∗)\n\n2\n\n+\n\n64κG2 N μ\n\n,\n\nwhere in the first inequality we have used Cauchy-Schwarz inequality, and in the last inequality we 2 for any a, b, t > 0. Rearranging both sides of the above inequality have used the fact\n\n2t + bt\n\nab ≤ a\n\n√\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nwith simple algebra leads to\n\n(cid:104)\n\nE\n\nR(w∗\n\n(cid:105) S|J ) − R(w∗)\n\n≤2E\n\n(cid:104) RS(w∗\n\nS|J ) − RS(w∗)\n\n(cid:105)\n\n+\n\nlog(N ) N\n\n(cid:18) 16G2 μk\n\n(cid:19)\n\n+ 8GD\n\n+\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n+ 2(R(w∗\n\n ̄k) − R(w∗))\n\n(cid:19)\n\n+\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n(25)\n\nS|J ) − RS(w∗ (cid:18) 16G2 μk\n\n ̄k) + RS(w∗ (cid:19)\n\n(cid:105) ̄k) − RS(w∗) 16GD log2(N ) log(1/δN )\n\n+ 8GD\n\n+\n\n+\n\n128κG2 N μ\n\n(cid:104)\n\n=2E\n\nRS(w∗\n\n+\n\n=2E\n\n+\n\n≤2E\n\n+\n\nlog(N ) N\n\n(cid:104) RS(w∗\n\nlog(N ) N\n\n(cid:104) RS(w∗\n\nlog(N ) N\n\n(cid:105) S|J ) − RS(w∗ ̄k) (cid:18) 16G2 μk\n\n+ 8GD\n\nS|J ) − RS(w∗ (cid:18) 16G2 μk\n\n+ 8GD\n\n(cid:20)(cid:16)\n\n≤ 2E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:124)\n\n+\n\nlog(N ) N\n\n(cid:124)\n\n(cid:123)(cid:122) T1\n\n(cid:18) 16G2 μk\n\n(cid:105)\n\n+ 2(R(w∗\n\n ̄k) − R(w∗))\n\nS, ̄k)\n\n(cid:19)\n\n+\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n(cid:21)\n\n(cid:125)\n\n+\n\n+ 2 (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:124)\n\n(cid:123)(cid:122) T2\n\n(cid:125)\n\n(cid:19)\n\n+ 8GD\n\n+\n\n(cid:123)(cid:122) T3\n\n16GD log2(N ) log(1/δN )\n\n+\n\n128κG2 N μ\n\n.\n\n(cid:125)\n\nPlugging Eq. (25) into Eq. (24) yields that for any q ≥ 2\n\n(cid:13) (cid:13)R(w∗ (cid:13)\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗)\n\n(cid:13) (cid:13) (cid:13)q\n\n≤\n\nq log(N ) N\n\n≤\n\nq log(N ) N\n\n(cid:18) 4G2 μk\n\n(cid:18) 4G2 μk\n\n(cid:19)\n\n+ 2GD\n\n+\n\n(cid:19)\n\n+ 2GD\n\n+\n\n2GDq2 log2(N ) log(1/δN )\n\n2GDq2 log2(N ) log(1/δN )\n\nκq(T1 + T2 + T3) N μ\n\nκq(T1 + T3) N μ\n\n(cid:115)\n\n+ 8G\n\nκqT2 N μ\n\n(cid:115)\n\n+ 8G\n\n(cid:115)\n\n+ 8G\n\n(cid:115)\n\n+ 8G\n\n(cid:19)\n\n+ 2GD\n\n(cid:19)\n\n+\n\n+ GD\n\n+\n\n2GDq2 log2(N ) log(1/δN ) GDq2 log2(N ) log(1/δN )\n\n+\n\nqG2 N μ\n\nκqT2 N μ (cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n+\n\n+ T1 + T3 +\n\n16κqG2 N μ\n\n(26)\n\nζ1 ≤\n\nq log(N ) N\n\n≲ q log(N ) N\n\n(cid:115)\n\n+ G\n\n(cid:18) 4G2 μk (cid:18) G2 μk q (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\n,\n\nN μ\n\nwhere in “ζ1” we have again used the fact to finally upper bound the desired sparse excess risk with respect to w∗\n\n2 for a, b, t > 0. Now we are in the position ̄k, which can be decomposed\n\nab ≤ a\n\n2t + bt\n\n√\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nin the following way:\n\nR(w∗\n\nS|J ) − R(w∗\n\n ̄k) =R(w∗\n\n+ RS(w∗\n\n≤R(w∗\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗) + RS(w∗ ̄k) − RS(w∗) + R(w∗) − R(w∗ ̄k) S|J ) + RS(w∗) + RS(w∗ ̄k) − RS(w∗) + R(w∗) − R(w∗ ̄k)\n\nS|J ) − R(w∗) − RS(w∗\n\n+ RS(w∗\n\nS|J ) − RS(w∗ ̄k)\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\nS|J ) − R(w∗) − RS(w∗\n\n≤ R(w∗ (cid:124)\n\nS|J ) + RS(w∗) (cid:125)\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n+ RS(w∗\n\n ̄k) − RS(w∗) + R(w∗) − R(w∗ ̄k) ,\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) T ′ 1\n\n(cid:123)(cid:122) T ′ 2\n\n(27) where in the first inequality we have used the fact RS(w∗ ̄k). We are going to bound the above two terms T ′ 1, since Eq. (26) holds for all q ≥ 2, in view of the second part of Lemma 4 (with ql = 2 and qu = ∞) we can show that the following holds with probability at least 1 − δ\n\n2 respectively with high probability. Concerning T ′\n\nS, ̄k) ≤ RS(w∗\n\n1 and T ′\n\n2 for any δ ∈ (0, e−1):\n\nS|J ) − R(w∗) − RS(w∗\n\nS|J ) + RS(w∗)\n\nT ′\n\n1 =R(w∗ (cid:12) (cid:12)R(w∗ (cid:12) GD log2 (cid:0) e\n\n≤\n\n≲\n\nS|J ) − R(w∗) − RS(w∗ (cid:1) log2(N )\n\n(cid:18)\n\n+\n\n(cid:12) S|J ) + RS(w∗) (cid:12) (cid:12) (cid:18) G2 μk\n\nlog(N )\n\n+ GD\n\nδ log(1/δN ) (cid:115) log (cid:0) e\n\nδ\n\n+ G\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:1) (cid:0)R(w∗ N μ\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\n(cid:19)\n\n+\n\nG2 μ\n\n(cid:19) log (cid:0) e\n\nδ\n\n(cid:1)\n\nN\n\n(28)\n\nRegarding the term T ′ as follows:\n\n2, similar to the argument of Eq. (23), we can bound its Lq-norm for any q ≥ 2\n\n∥T ′\n\n2∥q =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:0)l(w∗\n\n ̄k; Zi) − l(w∗; Zi)(cid:1) − (R(w∗\n\n ̄k) − R(w∗))\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q\n\n√\n\n2\n\n2κq N\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\ni=1\n\n(cid:0)l(w∗\n\n ̄k; Zi) − l(w∗; Zi) − (R(w∗\n\n ̄k) − R(w∗))(cid:1)2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)q/2\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nN (cid:88)\n\ni=1\n\n(cid:13) (cid:0)l(w∗ (cid:13) (cid:13)\n\n ̄k; Zi) − l(w∗; Zi)(cid:1)2\n\n+ (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)2(cid:13)\n\n(cid:13) (cid:13)q/2\n\n(cid:13) (cid:0)l(w∗ (cid:13) (cid:13)\n\n ̄k; Zi) − l(w∗; Zi)(cid:1)2(cid:13)\n\n(cid:13) (cid:13)q/2\n\n+ N (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)2\n\n≤\n\n≤\n\n≤\n\n≤\n\n≤\n\n√\n\n4\n\nκq\n\nN\n\n√\n\n4\n\nκq\n\nN √\n\n4\n\n2κq N\n√\n\nκq\n\nN μ\n\n8G √\n\n(cid:113)\n\nN G2 (cid:13)\n\n(cid:13)w∗\n\n ̄k − w∗(cid:13)\n\n2 (cid:13)\n\n(cid:113)\n\nR(w∗\n\n ̄k) − R(w∗).\n\nBy invoking the second part of Lemma 4 with ql = 2 and qu = ∞ we can translate the above moment bound into the following exponential tail bound that holds with probability at least 1 − δ for any δ ∈ (0, e−1):\n\n2\n\n(cid:115)\n\nT ′\n\n2\n\n≲ G\n\nlog (cid:0) e\n\nδ\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:1) (cid:0)R(w∗ N μ\n\n.\n\n(29)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nBy plugging Eq. (28) and Eq. (29) into Eq. (27) and applying union probability argument we obtain that the following holds with probability at least 1 − δ for any δ ∈ (0, e−1):\n\nR(w∗\n\nS|J ) − R(w∗ ̄k) (cid:1) log2(N )\n\nGD log2 (cid:0) e\n\nδ log(1/δN )\n\n≲\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n+ E\n\n(cid:18) G2 μk (cid:20)(cid:16)\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) log (cid:0) e\n\nδ\n\n(cid:1)\n\nN\n\n(cid:115)\n\n+ G\n\nlog (cid:0) e\n\nδ\n\n ̄k) − R(w∗)(cid:1)\n\n(cid:1) (cid:0)R(w∗ N μ\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nThis proves the desired sparse excess risk bound over the supporting set J.\n\nAs the final step, since there are at most (cid:0)d k\nwe can show that the following holds with probability 1 − δ for any δ ∈ (0, e−1):\n\n(cid:1) ≤ (cid:0) ed\n\n(cid:1)k\n\nk\n\ndifferent J with |J| = k, by union probability\n\nsup J⊆[d],|J|=k\n\nR(w∗\n\nS|J ) − R(w∗ ̄k)\n\n≲\n\nGD (cid:0)k log (cid:0) ed\n\n(cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1)(cid:1)2\n\nlog2(N )\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS|J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nThis completes the proof.\n\nNow we are in the position to prove the main result in Theorem 4.\n\nProof of Theorem 4. Let us consider ̃J = supp( ̃wS,k). By definition we have ̃wS,k = w∗ . Then invoking Lemma 7 yields that for any δ ∈ (0, e−1), the following holds with probability at least 1 − δ:\n\nS| ̃J\n\nR( ̃wS,k) − R(w∗ GD (cid:0)k log (cid:0) ed\n\n ̄k) = R(w∗ S| ̃J ) − R(w∗ ̄k) (cid:1)(cid:1)2 (cid:1) + log (cid:0) e log2(N ) k\nδ log(1/δN )\n\n+\n\n(cid:18)\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n(cid:16)\n\n+\n\nRS(w∗\n\nS| ̃J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n+\n\n(cid:20)(cid:16)\n\n+ E\n\nRS(w∗\n\nS| ̃J ) − RS(w∗\n\nS, ̄k)\n\n(cid:17)\n\n(cid:21)\n\n.\n\n+\n\nGD (cid:0)k log (cid:0) ed\n\n(cid:1) + log (cid:0) e k\nδ log(1/δN )\n\n(cid:1)(cid:1)2\n\nlog2(N )\n\n(cid:18)\n\n+\n\nlog(N )\n\n(cid:18) G2 μk\n\n(cid:19)\n\n+ GD\n\n+\n\nG2 μ\n\n(cid:19) k log (cid:0) ed\n\nk\n\n(cid:1)\n\n(cid:1) + log (cid:0) e N\n\nδ\n\n≲\n\n≲\n\n(cid:115) (cid:0)k log (cid:0) ed\n\nk\n\n+ G\n\n(cid:1) + log (cid:0) e\n\n(cid:1)(cid:1) (cid:0)R(w∗\n\n ̄k) − R(w∗)(cid:1)\n\nδ N μ\n\n+ ∆ ̄k,opt + E (cid:2)∆ ̄k,opt\n\n(cid:3) .\n\nThis proves the desired sparse excess risk bound.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nD OTHER RELATED WORK\n\nUniform stability and exponential generalization. Stimulated by a recent landmark work of Hardt et al. (2016), there is a renewed interest in the use of uniform stability for deriving generalization bounds for various learning algorithms and paradigms including stochastic gradient descent (SGD) (Kuzborskij & Lampert, 2018; Charles & Papailiopoulos, 2018; Lei & Ying, 2020), stochastic model based optimization (Wang et al., 2017; Deng & Gao, 2021), optimization based meta learning (Zhou et al., 2019), and differential privacy stochastic optimization (Bassily et al., 2019; Feldman et al., 2020). Compared to other stability arguments, uniform stability is notorious for implying high-probability generalization bounds in addition to the traditional in-expectation bounds. Until very recently, the basic result of Bousquet & Elisseeff (2002), as expressed in Eq. (2), remains the best known exponential generalization bound for uniformly stable algorithms. Using some elegant proof techniques from adaptive data analysis, Feldman & Vondr ́ak (2018) managed to replace\n\nγu log( 1\n\nδ ), which leads to an improvement whenever γu ≳ 1\n\nthe first term in Eq. (2) by N . Soon after, a series of breakthrough results were obtained (Feldman & Vondr ́ak, 2019; Bousquet et al., 2020) using tighter concentration bounds for sum of random functions, which eventually improve the stability dependent rate to a near-optimal one γu log(N ) log (cid:0) 1 (cid:1) as shown in Eq. (4). Additionally under the generalized Bernstein condition, these state-of-the-art results lead to O( 1 N ) excess risk bounds for uniformly stable algorithms (Klochkov & Zhivotovskiy, 2021).\n\nδ\n\n(cid:113)\n\nNon-uniform stability and exponential generalization. More broadly for non-uniformly stable algorithms, exponential generalization bounds have also been shown to be possible under various weaker and distribution-dependent notions of stability. As an early work in this line, Kutin & Niyogi (2002) showed that under the so called “almost-everywhere” stability, which is a high-probability counterpart of uniform stability, generalization bounds that hold with overwhelming probability are still possible in view of certain modified McDiarmid’s inequality (Kutin, 2002). Later, Rakhlin et al. (2005) revisited the bounded-difference results of Kutin & Niyogi (2002) in a more straightforward manner by using a powerful moment extension of Efron-Stein inequality (Boucheron et al., 2005). Recently for general Lq-stable algorithms, the exponential leave-one-out generalization bounds were derived using moment or exponential extensions of Efron-Stein inequality, with applications found in ridge regression, k-nearest neighbor classification and k-folds cross-validation (Celisse & Guedj, 2016; Celisse & Mary-Huard, 2018; Abou-Moustafa & Szepesv ́ari, 2019). However, when it comes to the recent break-through bounds of Feldman & Vondr ́ak (2019); Bousquet et al. (2020), it is much less obvious how to easily extend these near-optimal bounds under the almost-everywhere stability via simply incorporating the low probability failure events into concentration inequality. This is actually in sharp contrast to what have been done by Feldman & Vondr ́ak (2019, Theorem 4.5) and Bassily et al. (2020, Theorem 2.1) for stochastic learning algorithms with uniform stability (over the randomness of data) holding with high probability over the internal randomness of algorithm. We refer the interested readers to Boucheron et al. (2013); Kontorovich (2014); Combes (2015); Warnke (2016); Maurer & Pontil (2021) for more results on concentration inequalities beyond boundeddifference conditions, which are fundamental for deriving exponential bounds for non-uniformly stable algorithms.\n\nGeneralization analysis of l0-estimators. We further briefly review some prior results on the generalization guarantees for statistical learning under cardinality constraints, which is the theme of the application part of our work. For the l0-ERM estimator as expressed in Eq. (8), provided that the solution is exactly known, a series of uniform excess risk bounds were derived over binary prediction classes (Chen & Lee, 2018; 2020) and bounded liner prediction classes (Foster & Syrgkanis, 2019). The exact solutions of l0-ERM, however, is computationally intractable in general high-dimensional cases due to the NP-hardness of problem. Therefore, it is more realistic and desirable to establish generalization bounds for approximate l0-estimators such as the IHT-style algorithms (Yuan et al., 2018; Garg & Khandekar, 2009; Li et al., 2016). Particularly for misspecified sparsity models, a set of sparse excess risk bounds with slow and fast rates were established through the lens of uniform stability theory under proper regularity conditions (Yuan & Li, 2022).\n\n26",
  "translations": [
    "# Summary Of The Paper\n\nThis paper continues the recent line of work on high-probability generalization and excess risk bounds for stable algorithms. In this regard the paper:\n\n* Extends the nearly-optimal generalization bounds for uniformly stable algorithms to $L_q$-stable algorithms.\n* Similarly, it extends the excess risk bounds for uniformly stable algorithms under the Bernstein condition to $L_q$-stable algorithms under either the Bernstein or quadratic growth conditions.\n* It considers the inexact empirical risk minimization (ERM) with sparsity constraints problem. Under Lipschitzness, (high probability) strong convexity, and bounded domain conditions, it shows it is $L_q$-stable and it proves a new excess risk bound for that problem. The bounds are comparable to others in the literature with different assumptions.\n\n# Strength And Weaknesses\n\n**Strengths**\n\n* The generalization error bounds under $L_q$-stability are near-optimal, improving upon the current bounds, and matching the rate of their uniform-stability counterparts.\n* The excess risk bounds under $L_q$-stability match the rate of the uniform-stability counterparts under the Bernstein condition and add a new form under the quadratic growth condition.\n* The bounds on inexact ERM with sparsity constraints include algorithms like iterative hard thresholding (IHT) and match the rate of current bounds under potentially milder conditions. \n\n**Weaknesses**\n\n* There are some parts in the text and the proofs where the contributions and influence of previous work are not clear. It would be good to clarify these parts.\n\n  * The bound in Proposition 1 is *[Celisse and Guedj 2016, Corollary D1]*, which is a Corollary of *[Boucheron et al 2005, Theorem 2]*. Although the text preceding the proposition mentions the references, the wording is slightly confusing.\n  * The proof of Theorem 1 essentially follows *[Bousquet et al 2020, proof of Theorem 4]* with little variation to adapt it to $L_q$ stability. The same happens with the proof of Theorem 2 which follows essentially the combination of Lemma 7 and Theorem 4 of *[Bousquet et al 2020]*. Also with the proof of Lemma 5 which follows *[Klochkov and Zhivotovskiy 2021, proof of Lemma 3.1]* and the subsequent beginning and part (a) of the proof of Theorem 3 which follows *[Klochkov and Zhivotovskiy 2021, proof of Theorem 1.1]*.\n\n* In the exposition of the results, sometimes the assumptions are unclear, usually due to the notation $\\lesssim$ (which needs to be introduced in the notation section).\n\n  * The standard results from the uniform-stability literature usually require the loss is bounded by some constant, say $L$. This usually appears in (2), (3), (4), and (5), even under the use of $\\lesssim$ to make this assumption explicit. Having these constants would help understand better the improvement of using $L_q$-stability. If they are not there, it would be good to explicitly mention these bounds hold under this assumption prior to their introduction.\n  * Similarly, in (5) the Bernstein condition constant is usually present to make sure this dependence is clear.\n  * The same happens when presenting the contributions in Section 1.2. \n\n    * The generalization bound should have the constant $M_q$ demanding a finite moment $\\lVert \\ell(A(S);Z) \\rVert_q \\leq M_q$ or at least an explicit mention that this is required. \n\n    * The excess risk bound should either have the Bernstein constant $B$ and a bounded constant, or an explicit mention that these assumptions are required. Moreover, it should include the Lipschitz constant $G$ and the strong-convexity constant $\\mu$  or an explicit mention that these assumptions are required. \n\n* Algorithm 1 seems self-referential. Namely, note that $\\tilde{w}_{S,k} := \\argmin_{w \\in \\mathcal{W}, \\textnormal{supp}(w) \\subseteq \\tilde{J}} R_S(w)$ and $\\tilde{J} = \\textnormal{supp}(\\tilde{w}_{S,k})$. This made understanding the section a little more difficult.\n\n* Remark 10 seems a little unfair. It is comparing a bound for misspecified models *[Yuan and Li 2022, Theorem 3]* with a rate $\\mathcal{O}(1/N)$ with further assumptions with their $\\mathcal{O}(1/\\sqrt{N})$ bound on Theorem 4 with weaker assumptions. However, it does not consider the $\\mathcal{O}(1/\\sqrt{N})$ bound of the same paper *[Yuan and Li 2022, Theorem 1]* which has very similar assumptions (i.e. does not need a bounded parameter space nor a strongly convex population risk as this paper does but requires smoothness of the empirical risk) that can be similarly applicable.\n\n* The paper claims that it uses the developed theory to bound the excess risk of inexact ERM with sparsity constraints. This is not the case, the excess risk bounds are based on $L_q$-stability, yes, but don't use the theorems in Section 2. First, the assumptions are larger in that setting, and second, only some ideas of the proof of Theorem 3 are employed to prove Lemma 6 in the Appendix. \\\nI believe this should be clarified in the Abstract and the Introduction. Section 3 serves as a motivation for the importance of studying $L_q$-stability, but not for the usage of their generic bounds for $L_q$-stable algorithms, since they are not used there.\n\n* Could you please clarify or write explicitly in the text the final step in the proof of Theorem 3, part (a) and part (b)? That is, the step that follows the \"which implies that\" and \"which then implies\".\n\n* The exposition would be clearer if the excess risk bounds and concepts were also introduced in the problem setup instead of that later in Section 2.3. \n\n**References**\n\n*[Boucheron et al 2005]* Moment inequalities for functions of independent random variables. \\\n*[Celisse and Guedj 2016]* Stability revisited: new generalisation bounds for the leave-one-out. \\\n*[Bousquet et al 2020]* Sharper bounds for uniformly stable\nalgorithms. \\\n*[Klochkov and Zhivotovskiy 2021]* Stability and deviation optimal risk bounds with convergence rate o(1/n). \\\n*[Yuan and Li 2022]* Stability and risk bounds of iterative hard thresholding.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n* **Clarity**: The paper is clear and well-written.\n\n* **Quality**: The quality of the paper is good. \n\n* **Novelty**: The gross of the proofs to extend the uniform-stability generalization and excess risk bounds to $L_q$ stability is not novel. However, the results are, as well as the results and proofs of Section 3.\n\n* **Reproducibility**: \\\n*Theory*: I reproduced all the proofs except for the questions that I placed the authors in the weaknesses. \\\n*Experiments*: There are no experiments.\n\n# Summary Of The Review\n\nThis paper extends and builds upon the current best rate bounds on the generalization and excess risk bounds under the uniform-stability assumption to the milder $L_q$-stability assumption. \n\n* For the generalization bounds, instead of further needing a bounded loss, it needs bounded moments. \n* For the excess risk bounds, it either maintains the requirement of a bounded loss and the Bernstein condition or requires Lipschitness and the quadratic growth condition. These rates under the latter assumptions are novel as far as I know.\n\nThe paper needs to make clearer, though, the influences of the previous literature in their proofs, since some of them follow these papers closely. Similarly, it needs to further clarify some of the assumptions in the introductory text.\n\nThen, the paper finds bounds for the excess risk of inexact ERM with sparsity constraints (which are applicable to IHT). First, they prove these algorithms are $L_q$-stable and then they find specialized bounds that are comparable to those in the literature.\n\nThe paper needs to make clearer the comparison of their bounds and assumptions to those in the literature as well as the reason for Section 3, which does not use directly their bounds for generic $L_q$-stable algorithms. \n\nOverall, I believe the paper is well written and is a good contribution, so I recommend acceptance. Nonetheless, I still believe the comments in the weaknesses part should be addressed. \n\n**Minor comments and nitpicks that did not impact the score of the review**\n\n* Please, introduce the notation $\\lesssim$.\n\n* In the first paragraph of page 2, \"the Efron-Stein inequality\".\n\n* In the paragraph before Definition 1, \"introduce the ~~following~~ concept of uniform stability*.\n\n* In Remark 1, use either \"uniformly bounded\" or \"uniform boundedness\".\n\n* Before (7), please mention explicitly that this holds with probability $1-\\delta$.\n\n* Before the acronym HTP, give the name \"Hard Threshold Pursuit\".\n\n* For completeness, in page 7, after the displayed equation before Definition 3 mentioned that $\\tilde{w}_{S,k}$ is the output of Algorithm 1.\n\n* In Section 3, describe what you mean by support to disambiguate with the support of distributions since you are also working with random objects in this work.\n\n* In the last paragraph in Section 4, either \"a cardinality constraint\" or \"cardinality constraints\".\n\n* In the Conclusion you don't mention anything about your results on inexact ERM with sparsity constraints. Maybe you want to write a little about that.\n\n* Throughout the text you are using $a$ for the constant on the sub-exponential term and $b$ for the constant on the sub-Gaussian term. However, in the first bullet point of Lemma 4, you reversed them.\n\n* Maybe you want to separate Remark 11 into two remarks. \n\n* At the beginning of Theorem 1 you say \"we pad the set with extra...\", please write explicitly which set you mean.\n\n* In the last paragraph of page 15: \"Based on the triangle and Jensen's inequalities we can show that\"\n\n* In (16) I think it should be $4 \\sqrt{2 \\kappa Nq} M_q$ instead of $2 \\sqrt{2 \\kappa Nq} M_q$.\n\n* The first inequality of part (a) of Theorem 3 on page 18 and the last inequality of part (b) on page 18 are not due to Hölder's inequality since it only holds for exponents $1 < r < s < \\infty$ where the first inequalities are strict. However, it does hold due to Jensen's inequality.\n\n* There is a typo in the first equation in the proof of Lemma 1. In the $\\ell(w^* w_{S|J};Z_j)$ it should be $\\ell(w^*_{S|J};Z_j)$.\n\n* In (23) instead of inequality $\\leq$ it should be $\\lesssim$.\n\n* In the array of equations and inequalities after (29) in the second inequality I believe you forgot a factor of $\\sqrt{2}$ (it should be a 4 instead of $2 \\sqrt{2}$) and it carries over that part.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable",
    "# Summary Of The Paper\nThe paper \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li addresses the generalization capabilities of Lq-stable algorithms by deriving near-optimal exponential generalization and excess risk bounds. The authors highlight that traditional uniform stability leads to overly stringent bounds, while Lq-stability can yield improved performance. Their main contributions include establishing Lq-norm inequalities for independent random variables and presenting new generalization bounds that match the rates of uniformly stable algorithms, albeit with logarithmic factors. Additionally, they apply their theoretical results to demonstrate enhanced bounds for sparse excess risk in Iterative Hard Thresholding (IHT) estimators.\n\n# Strength And Weaknesses\nThe paper makes a significant contribution by bridging the gap in generalization bounds between Lq-stable and uniformly stable algorithms, thus providing a more flexible framework for understanding the performance of modern learning algorithms. The establishment of new concentration inequalities is a key strength, as it leads to improved theoretical results. However, the paper could benefit from more extensive empirical validation of the proposed bounds across various learning scenarios, as the focus is heavily theoretical. Additionally, while the theoretical framework is sound, its practical implications and applicability could be further emphasized.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its methodology and findings. The definitions and theoretical results are presented with sufficient detail for readers familiar with the topic. The novelty of the approach lies in its rigorous treatment of Lq-stability, which has not been thoroughly explored in the context of exponential generalization bounds. The reproducibility of the results appears high, given the detailed derivations and the comprehensive references provided, although more empirical demonstration would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a valuable theoretical advancement in the understanding of generalization bounds for Lq-stable algorithms, offering near-optimal rates that are well-founded in the literature. While the theoretical contributions are solid, additional empirical work would strengthen the paper's impact and applicability in practical scenarios.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a comprehensive analysis of the stability of learning algorithms and their generalization capabilities, focusing particularly on the contrast between uniform stability and Lq-stability. The authors introduce a novel set of exponential generalization bounds for Lq-stable algorithms, effectively bridging a critical gap identified in existing literature. Key contributions include several theorems that establish Lq-norm inequalities and generalization bounds, alongside a practical application to Iterative Hard Thresholding (IHT) algorithms in high-dimensional sparse learning contexts.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its theoretical advancements, offering near-optimal generalization bounds for Lq-stable algorithms, which have previously been less understood compared to uniformly stable ones. The clarity of definitions and the rigor in establishing conditions enhance comprehension and applicability. However, the limitations include a lack of empirical validation, as the focus is predominantly theoretical, potentially restricting the practical applicability of the findings. Additionally, the reliance on specific conditions (e.g., generalized Bernstein conditions) may hinder the universality of the results across all learning scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its theoretical contributions, making it accessible to readers familiar with stability concepts. The definitions and results are articulated with precision, aiding in understanding the implications of Lq-stability. While the theoretical results are novel, the absence of empirical validation raises questions about reproducibility in real-world applications; however, the groundwork laid can spur future experimental investigations.\n\n# Summary Of The Review\nOverall, this paper makes significant theoretical contributions to the understanding of stability in learning algorithms, particularly through the lens of Lq-stability. Its clear definitions and rigorous proofs provide a strong foundation for future research, although the lack of empirical validation limits its immediate practical utility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li addresses the crucial problem of bounding the generalization error of learning algorithms, focusing specifically on Lq-stability as a weaker and distribution-dependent alternative to uniform stability. The authors derive near-optimal exponential generalization bounds for Lq-stable algorithms, significantly improving upon existing bounds by reducing the overhead factor associated with stability. The theoretical contributions are supported by theorems that provide sharper generalization and excess risk bounds, particularly relevant for algorithms like Iterative Hard Thresholding (IHT).\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical rigor and clear exposition of the stability definitions, particularly the introduction of Lq-stability and its implications for generalization bounds. The paper successfully bridges a gap in the literature where previous bounds were sub-optimal, offering new insights that can enhance the understanding and application of Lq-stable algorithms. However, a notable weakness is the lack of empirical validation; while the theoretical results are compelling, the absence of experimental results leaves questions regarding the practical applicability of the theoretical findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of theoretical foundations, definitions, and results. The quality of the writing is high, making complex concepts accessible to the reader. In terms of novelty, the introduction of Lq-stability and the subsequent generalization bounds represent a significant advancement in the field. However, the reproducibility of the results is hindered by the absence of empirical experiments; the theoretical focus, while valuable, does not allow for direct validation of claims in practical settings.\n\n# Summary Of The Review\nOverall, the paper presents a strong theoretical contribution to the understanding of generalization bounds for Lq-stable algorithms, significantly improving upon existing literature. However, the lack of empirical validation limits its practical applicability and leaves room for further exploration in real-world scenarios.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper provides significant advancements in the understanding of generalization bounds for Lq-stable algorithms, offering near-optimal exponential bounds that improve upon previous results reserved for uniformly stable algorithms. The methodology involves deriving strict analogues of generalization and risk bounds, which are then applied to practical scenarios like sparse estimation through algorithms such as Iterative Hard Thresholding (IHT). The findings highlight the potential for these bounds to enhance theoretical and practical frameworks in machine learning stability and generalization.\n\n# Strength And Weaknesses\nThe paper's contributions are robust, particularly in advancing generalization bounds and providing novel theoretical insights. However, the bounds presented are distribution-dependent, which may limit their utility in broader contexts where uniform stability is critical. The reliance on specific assumptions for algorithmic tractability could also restrict the general applicability of the results. While the application to sparse estimation demonstrates practical relevance, the lack of extensive empirical validation raises questions about the real-world applicability of the theoretical results. Furthermore, while the literature review is comprehensive, it may be overwhelming for readers unfamiliar with the foundational concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and logically flows from theory to application, which enhances clarity for those familiar with the subject. However, the depth of technical content might pose challenges for newcomers to the field. The novelty of the theoretical contributions is significant, yet the practical reproducibility of the results is yet to be established through empirical evidence. Overall, the paper maintains a high quality in presentation, though supplementary materials could benefit a wider audience.\n\n# Summary Of The Review\nThis paper makes meaningful theoretical contributions to the generalization bounds for Lq-stable algorithms, providing a solid foundation for further exploration in stability theory. While the findings are promising, the reliance on specific conditions and the need for empirical validation may limit the immediate impact of the results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a comprehensive study on the stability of learning algorithms, specifically focusing on Lq-stable algorithms. It introduces new, near-optimal exponential generalization bounds that enhance the understanding of the relationship between stability and generalization performance. The authors develop a refined concentration inequality pertinent to Lq-norms and demonstrate its applicability through a case study on sparsity estimation algorithms, particularly Iterative Hard Thresholding (IHT), showcasing both theoretical advancements and practical implications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear theoretical contributions, particularly in extending Lq-stability and deriving new generalization bounds that are competitive with established results for uniformly stable algorithms. The methodological advancements, including the new concentration inequalities, provide a solid foundation for future research. However, a notable weakness is the lack of extensive empirical validation across diverse datasets, which could reinforce the practical relevance of the theoretical results. Furthermore, the complexity of the proposed methods may hinder their implementation in real-world applications, suggesting the need for further research on computational efficiency.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to a broad audience within the machine learning community. The quality of writing is high, and the theoretical developments are presented in a logical manner. The novelty of the work is significant, as it provides new insights into Lq-stability and its implications for generalization. However, reproducibility may be a concern due to the intricate nature of the new concentration inequalities and the absence of empirical studies to validate the theoretical claims.\n\n# Summary Of The Review\nOverall, this paper makes substantial contributions to the understanding of algorithmic stability and generalization bounds through the lens of Lq-stability. While the theoretical advancements are commendable, the lack of empirical validation and potential implementation challenges suggest areas for further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li investigates adversarial training methods, focusing on their stability properties. The authors develop a novel framework based on Lq-stability to derive exponential generalization bounds for models trained under adversarial conditions. Their methodology includes the extension of Lq-stability to accommodate adversarial settings and the derivation of sharp risk bounds using concentration inequalities. The findings demonstrate that adversarially trained models can achieve improved generalization performance, supported by empirical evidence across various training algorithms.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical rigor and relevance. The introduction of Lq-stability offers a fresh perspective on understanding adversarial training, while the derived generalization bounds are shown to be competitive with existing uniform stability approaches. Furthermore, the implications for robust learning add practical value to the theoretical contributions. However, a notable weakness is the limited scope of the empirical validation, which only covers a few well-known adversarial training methods. This may not fully reflect the diversity of approaches in the field. Additionally, the complexity of the proposed methods could pose challenges for practitioners lacking advanced mathematical expertise.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with rigorous proofs and detailed analyses enhancing its quality. The novelty of the Lq-stability framework is significant, contributing to ongoing discussions in adversarial training. However, the reproducibility of the results may be limited due to the complexity of the proposed techniques and the narrow focus on specific training algorithms.\n\n# Summary Of The Review\nOverall, the paper provides valuable contributions to the understanding of adversarial training through a new stability framework and competitive generalization bounds. While the theoretical aspects are robust and relevant, the empirical validation could be expanded to support a broader range of adversarial training methods. Future work should aim to address these limitations.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li addresses the issue of generalization error in statistical learning by proposing a new framework for Lq-stable algorithms. The authors claim to provide unprecedented near-optimal exponential generalization bounds that rival those of uniformly stable algorithms. Additionally, the paper discusses applications of these bounds to sparsity estimation methods such as Iterative Hard Thresholding (IHT) and introduces a novel concentration inequality that enhances the understanding of sums of functions of independent random variables. The authors assert that their findings could have broad implications in various fields within statistical learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its proposal of near-optimal generalization bounds for Lq-stable algorithms and the introduction of a new concentration inequality, which may contribute positively to the field. The application of these theoretical advancements to sparsity estimation is also noteworthy, as it could impact computationally feasible methods. However, the claims of revolutionary changes and foundational contributions feel exaggerated given the specificity of the findings. The potential impact of the work appears limited to a niche area, and the assertion that it could influence a broad range of domains may be overly ambitious.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its exposition, presenting complex ideas in a structured manner. The quality of the methodology appears sound, though the novelty of certain contributions may be overstated. The introduction of the new concentration inequality is a positive aspect, but the paper does not sufficiently differentiate its contributions from existing literature. Reproducibility is not thoroughly addressed, and additional details regarding the implementation of the proposed methods and results would enhance the paper's practical value.\n\n# Summary Of The Review\nOverall, while the paper presents some novel contributions to the understanding of Lq-stable algorithms and generalization bounds, the claims of its revolutionary impact are overstated. The work is valuable within a specific context but lacks the broader applicability that the authors suggest.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li introduces a novel framework for understanding generalization performance through the lens of Lq-stability, which is a distribution-dependent notion of stability. The authors derive near-optimal exponential generalization bounds for Lq-stable algorithms, improving upon previous results that focused on uniformly stable algorithms. Key contributions include new generalization bounds, excess risk bounds, and theoretical comparisons that suggest Lq-stable algorithms can achieve comparable performance to uniformly stable ones, but with significantly improved rates, especially in high-dimensional contexts.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of Lq-stability, which broadens the scope of generalization theory and the derived bounds that suggest a substantial improvement in generalization rates. The application of Lq-stability to specific algorithms, such as Iterative Hard Thresholding, provides practical relevance and showcases the theory's utility. However, a potential weakness lies in the clarity of the derivations and theoretical comparisons, which may require additional elaboration to fully convey the implications of the results. Furthermore, while the results are promising, empirical validation through broader experiments beyond sparse estimation could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, presenting the theoretical contributions in a coherent manner. However, some of the mathematical derivations may be dense for readers unfamiliar with stability concepts, which could hinder reproducibility. The novelty is significant as it challenges traditional notions of stability in machine learning, but the quality of empirical results could be enhanced by more extensive testing across various algorithms and datasets. Overall, clarity could be improved to ensure that the implications of the findings are easily accessible to a wider audience.\n\n# Summary Of The Review\nThe paper presents a valuable advancement in the understanding of generalization through Lq-stability, offering near-optimal bounds that enhance the applicability of stability concepts in machine learning. While the theoretical contributions are robust, the clarity and breadth of empirical validation could be improved to bolster the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the relationship between Lq-stability and uniform stability as proxies for generalization performance in machine learning algorithms. It proposes a framework that attempts to derive generalization bounds based on Lq-stability, aiming to relax the strict conditions necessary for uniform stability. The findings suggest that, under certain assumptions about loss functions and data distributions, Lq-stability can yield nearly optimal generalization bounds. However, the applicability of these results in more complex or real-world scenarios remains uncertain.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to bridge the gap between two established notions of stability in machine learning, potentially offering a more flexible approach to understanding generalization. However, several weaknesses undermine this contribution. The reliance on the assumption that stability accurately reflects generalization performance is problematic, especially given the complexity of real-world learning scenarios. Additionally, the paper's dependence on specific distributions and loss function properties raises concerns about the generality of its conclusions. The lack of robust empirical validation further detracts from the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its arguments clearly. However, the assumptions made throughout the analysis could be more thoroughly justified, and some technical details may hinder reproducibility. The novelty is present in the exploration of Lq-stability but lacks a deeper examination of alternative stability notions or empirical validation, which would bolster the contributions. Overall, while the paper is of good quality, its novel insights require further substantiation.\n\n# Summary Of The Review\nOverall, the paper presents an interesting perspective on the relationship between Lq-stability and uniform stability in terms of generalization bounds in machine learning. However, the foundational assumptions and lack of empirical validation raise significant concerns about the applicability and robustness of its claims.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper addresses the gap in exponential generalization bounds between uniformly stable algorithms and Lq-stable algorithms, presenting near-optimal exponential generalization and excess risk bounds for the latter. The authors develop sharper concentration inequalities to enhance the understanding of generalization in Lq-stable algorithms, particularly focusing on applications in high-dimensional sparse learning. The findings suggest that Lq-stability allows for broader generalization bounds than previously recognized, especially in the context of sparsity recovery, and they derive improved bounds for algorithms like Iterative Hard Thresholding.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical contributions, particularly the introduction of sharper exponential bounds for Lq-stable algorithms, which significantly improve upon existing results. The application of these theoretical advancements to derive sparse excess risk bounds for specific algorithms demonstrates practical relevance. However, a weakness is the reliance on specific conditions for their results, which may limit the generalizability of their findings. Additionally, while the theoretical framework is well-articulated, the empirical validation of these bounds through experiments is somewhat lacking.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers with a background in statistical learning theory. The quality of the theoretical analysis is high, with thorough explanations of key concepts. The novelty in proposing sharper bounds for Lq-stability is significant, addressing a pertinent gap in the literature. However, the reproducibility of the results could be improved by including more extensive experimental validation or concrete examples demonstrating the practical implications of the theoretical bounds.\n\n# Summary Of The Review\nOverall, this paper makes a meaningful contribution to the understanding of generalization bounds in Lq-stable algorithms, presenting novel theoretical results with practical implications for sparse learning. While the clarity and quality of the work are commendable, further empirical validation would strengthen the overall impact of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at understanding the relationship between stability properties of learning algorithms and their generalization performance. The authors develop a theoretical foundation that connects these stability properties to generalization bounds, providing insights that can be applied across various algorithms and datasets. The findings suggest that enhancing the stability of algorithms could lead to improved generalization, thus offering new avenues for future research and algorithm design.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its fresh perspective on generalization bounds, which could potentially bridge significant gaps in the current literature. The theoretical results are well-formulated and highlight the importance of stability in achieving better generalization. However, a notable weakness is the lack of empirical validation; while theoretical contributions are robust, the absence of extensive experimental results limits the practical applicability of the proposed framework. Additionally, the discussion on how the work builds upon or differs from existing studies could be more comprehensive.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, with a clear and logical progression from the problem statement through the theoretical development to the conclusions. Notation is mostly consistent, though some terms and definitions could benefit from further clarification to enhance accessibility for a broader audience. The quality of the theoretical work is high, but the reproducibility of the findings is currently uncertain due to the lack of detailed empirical evaluations.\n\n# Summary Of The Review\nThe paper makes a significant contribution to the understanding of generalization in machine learning by linking stability properties to generalization bounds. However, the absence of empirical validation and a more detailed discussion of related work diminishes its overall impact. With appropriate revisions and enhancements, the paper has the potential to make a notable contribution to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents significant advancements in understanding the relationship between algorithmic stability and generalization performance in machine learning. The authors focus on Lq-stability, a relaxed notion of stability, and propose near-optimal exponential generalization bounds for Lq-stable learning algorithms. Through rigorous analysis, they demonstrate that these bounds can achieve performance comparable to those derived from uniformly stable algorithms, particularly in the context of sparse excess risk bounds for algorithms such as Iterative Hard Thresholding (IHT).\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its clear theoretical contributions, which bridge the gap between Lq-stability and uniform stability in terms of generalization bounds. The derivation of near-optimal bounds is particularly noteworthy, as it enhances the practical applicability of Lq-stable algorithms in real-world scenarios. However, a potential weakness is the lack of extensive empirical validation of the theoretical results. While the paper mentions the application of these bounds to algorithms like IHT, further experiments could strengthen the claims and demonstrate the bounds' effectiveness in diverse settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, with a logical flow that facilitates understanding of complex concepts. The methodology is sound, providing a solid foundation for the theoretical results. The novelty of deriving near-optimal bounds for Lq-stable algorithms contributes significantly to the field. However, reproducibility could be improved by providing more details on the implementation aspects of the algorithms discussed, as well as clear benchmarks for comparison.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the understanding of algorithmic stability and generalization performance, providing near-optimal bounds for Lq-stable algorithms. While the theoretical insights are strong, the paper would benefit from additional empirical validation to fully substantiate its claims.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Yuan and Li investigates the relationship between algorithm stability and generalization in machine learning. The authors introduce near-optimal exponential generalization bounds for Lq-stable algorithms, addressing the limitations of existing polynomial bounds. They employ moment inequalities and concentration bounds to derive their results, presenting three significant theorems that improve upon previous work. The paper highlights applications in sparsity estimation and shows that Lq-stable algorithms can achieve generalization rates comparable to those of uniformly stable algorithms, effectively bridging theoretical gaps in the field.\n\n# Strength And Weaknesses\nThe paper makes a substantial contribution by presenting near-optimal bounds for Lq-stable algorithms, which is a notable advancement over the previously established polynomial bounds. The introduction of sharper concentration inequalities is particularly valuable for enhancing generalization bounds. However, the methodology, while rigorous, may be perceived as complex, potentially limiting accessibility for readers not deeply familiar with the underlying statistical concepts. Furthermore, the empirical validation of the theoretical findings could be strengthened with more extensive experiments or real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-defined terms and a logical flow of ideas. However, certain sections, particularly those detailing the methodology, could benefit from additional explanations or illustrative examples to enhance understanding. The quality of the work is high, with rigorous proofs and solid theoretical foundations. In terms of novelty, the paper makes a significant contribution to the field, advancing the understanding of stability and generalization. The reproducibility of the results may be challenging due to the complexity of the theoretical constructs involved, and the paper could improve by providing more detailed insights into the implementation of the proposed methods.\n\n# Summary Of The Review\nOverall, this paper presents a noteworthy advancement in the theoretical understanding of generalization bounds for Lq-stable algorithms. While the contributions are significant, the complexity of the methodology may pose challenges for some readers. Further empirical validation and clarity in exposition would enhance the paper’s impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li presents significant advancements in the understanding of generalization bounds associated with Lq-stable learning algorithms. The authors derive near-optimal exponential generalization bounds by employing a theoretical framework based on concentration inequalities relevant to Lq-norms. Their findings indicate that these new bounds closely align with the optimal results obtained from uniformly stable algorithms, thereby offering enhanced insights into sparsity estimation algorithms and generalization performance in statistical learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical contributions, which fill a notable gap in the literature concerning Lq-stability. The introduction of new concentration bounds is particularly commendable, as it enhances the applicability of Lq-stable algorithms in practical scenarios. However, the paper could benefit from more illustrative examples or simulations to substantiate its theoretical claims. Additionally, while it provides a solid theoretical foundation, the implications of these findings on existing algorithms could be explored in greater depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and exhibits a logical flow, making it accessible to the target audience. The terminology is adequately defined, and the mathematical rigor is solid, with proofs provided for key theorems. The originality of the work is evident in its approach to extending the theory of Lq-stability, which adds significant value to the field of learning theory. However, further empirical validation would enhance the reproducibility of the results.\n\n# Summary Of The Review\nOverall, this paper makes a notable contribution to the literature on Lq-stable algorithms by deriving near-optimal generalization bounds and providing a robust theoretical framework. While the theoretical insights are commendable, the paper would benefit from additional empirical examples to enhance its practical implications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the relationship between the stability of learning algorithms and their generalization performance, specifically focusing on Lq-stable algorithms. It identifies a gap in the literature regarding exponential generalization and excess risk bounds for Lq-stability, which are distribution-dependent and less stringent than uniform stability bounds. The authors derive near-optimal exponential generalization bounds for Lq-stable algorithms, establishing analogues to bounds achieved under uniform stability. They demonstrate the applicability of their framework through the development of sparse excess risk bounds in Iterative Hard Thresholding (IHT).\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its contribution to the understanding of Lq-stability and its implications for generalization bounds. The derived theorems provide valuable insights and extend existing work in stability theory, bridging a notable gap in the literature. Additionally, the application of these theoretical results to practical algorithms like IHT enhances the paper's relevance. However, the paper could benefit from a more extensive empirical evaluation of the proposed bounds across different datasets and learning scenarios, as the current focus is primarily theoretical.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the theoretical developments accessible to readers familiar with stability theory. The quality of the writing is high, with precise definitions and rigorous proofs provided for all theorems. The novelty of the approach is significant, as it introduces new bounds for a less-explored area in the context of Lq-stability. While the theoretical results are compelling, the reproducibility aspect could be strengthened by including more detailed descriptions of the experimental setups and datasets used to validate the findings.\n\n# Summary Of The Review\nOverall, the paper presents a substantial advancement in the understanding of generalization bounds in the context of Lq-stable algorithms. It successfully derives near-optimal bounds and applies them to practical learning scenarios, although it would benefit from more empirical validation. The clarity and rigor of the presentation are commendable, making it a valuable contribution to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents an exploration of generalization bounds for Lq-stable algorithms, claiming to derive \"near-optimal\" exponential bounds that improve upon existing results related to uniformly stable algorithms. The methodology involves using concentration inequalities to establish the theoretical foundations of these bounds. However, the findings raise questions regarding the practical applicability and empirical validation of these claims, especially in the context of specific conditions such as B-Bernstein and Lipschitz continuity.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to bridge the gap between Lq-stability and uniform stability, highlighting a less explored area in learning theory. However, significant weaknesses undermine its contributions. The assertion of \"near-optimal\" bounds remains vague without rigorous empirical validation, and the reliance on Lq-stability is problematic due to its distribution-dependent nature. Furthermore, the authors do not sufficiently address the limitations of their assumptions, which may restrict the applicability of their findings to real-world scenarios. The analysis of computational algorithms like Iterative Hard Thresholding lacks a convincing demonstration of improved predictive performance. Lastly, the discussion of related work is limited, and the paper’s dense structure may hinder reader comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by its convoluted structure, making it difficult for readers to discern the main contributions and implications. The quality of the theoretical claims is mixed; while they are theoretically sound, the lack of empirical support detracts from their robustness. The novelty of the approach is present, but it is diminished by the superficial treatment of existing literature and alternative stability approaches. Reproducibility may be a concern due to the specificity of the conditions under which their results hold, which may not be easily replicated in practice.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing exploration of Lq-stable algorithms and their generalization bounds, but it suffers from several critical weaknesses, including vague claims, limited empirical validation, and a lack of engagement with existing literature. The contributions, while potentially significant, are undermined by the problematic assumptions and lack of clarity throughout the paper.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to deriving near-optimal exponential generalization bounds for Lq-stable algorithms, significantly advancing the understanding of algorithmic stability and generalization performance in machine learning. The authors develop strict analogues to previously established bounds for uniformly stable algorithms, enhancing the theoretical framework of generalization. Key findings include improved Lq-stability and practical applications for sparse excess risk bounds in algorithms like Iterative Hard Thresholding (IHT), which are particularly relevant for high-dimensional settings.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its ability to bridge a crucial gap in the literature by extending generalization bounds to a broader class of algorithms, thereby enhancing their practical utility. The application of the theory to sparsity estimation represents a significant practical contribution, making complex algorithms more accessible. However, the paper may lack empirical validation in certain sections, as it primarily focuses on theoretical advancements without extensive real-world experimentation to support the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its methodology clearly, making it accessible to both theorists and practitioners in machine learning. The quality of the mathematical exposition is high, and the innovative techniques introduced—specifically, sharper concentration bounds under Lq-norm conditions—demonstrate significant methodological advancements. While the theoretical contributions are reproducible, further empirical validation would enhance confidence in the applicability of the results.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the understanding of Lq-stable algorithms and their generalization capabilities. The theoretical advancements are compelling and have significant implications for the field, though the lack of extensive empirical validation slightly limits the practical impact of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a theoretical framework for deriving near-optimal exponential generalization bounds for Lq-stable algorithms, addressing the limitations of existing stability theories. The authors build upon the concept of uniform stability, introducing Lq-stability as a valuable extension that accommodates a broader class of algorithms. The findings indicate that Lq-stability can yield generalization bounds closely aligned with those of uniformly stable algorithms. Additionally, the paper applies these theoretical advancements to derive strong sparse excess risk bounds for algorithms like Iterative Hard Thresholding (IHT), establishing practical implications for the theory.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical analysis and the introduction of enhanced Lq-stability theory, which significantly contributes to the understanding of generalization in statistical learning. The derivation of near-optimal bounds represents a substantial advancement over previous work, highlighting the potential of Lq-stability. However, a weakness is that the paper primarily focuses on theoretical aspects without extensive empirical validation, which could strengthen its claims regarding the practical applicability of the proposed bounds.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers familiar with learning theory. The quality of the theoretical analysis is high, and the novelty of introducing enhanced Lq-stability bounds is significant, as it pushes the boundaries of existing knowledge in this area. However, the reproducibility of the results may be limited due to the lack of empirical experiments showcasing the practical implications of the theoretical findings.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the understanding of Lq-stability and its implications for generalization in statistical learning. While the theoretical advancements are impressive, further empirical validation would enhance the robustness of the claims made.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" by Xiao-Tong Yuan and Ping Li presents significant theoretical advancements in the understanding of generalization bounds for Lq-stable algorithms. The authors establish near-optimal exponential generalization bounds through a series of theorems that derive Lq-norm inequalities and excess risk bounds under specific conditions. Notably, the paper applies its findings to improve risk bounds for Iterative Hard Thresholding (IHT) algorithms. The methodology relies on established properties of Lq-stability and concentration inequalities, with a focus on proving theorems that enhance the understanding of stability coefficients and convergence rates.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical framework and the derivation of new bounds, which contribute to the existing literature on stability in machine learning. The proofs are well-structured and provide clear insights into the assumptions and conditions necessary for the results. However, a notable weakness is the lack of practical implementation details and code availability, which limits the reproducibility of the findings and their application in real-world scenarios. The reliance on theoretical constructs without empirical validation also raises questions about the practical significance of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulates its contributions; however, the complexity of the mathematical derivations may be challenging for readers without a strong background in the subject. The quality of the proofs and the logical flow are commendable, although the absence of code or practical examples diminishes its reproducibility. In terms of novelty, while the theoretical advancements are noteworthy, the lack of empirical experiments or applications to real-world datasets limits the overall impact of the work.\n\n# Summary Of The Review\nOverall, the paper presents a solid theoretical contribution to the understanding of generalization bounds for Lq-stable algorithms, with well-defined theorems and rigorous proofs. However, the lack of practical implementation details and empirical validation detracts from its applicability and reproducibility in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper proposes new exponential generalization bounds for Lq-stable algorithms, claiming to close existing gaps in convergence rates compared to uniformly stable algorithms. The authors utilize a distribution-dependent approach to Lq-stability, suggesting that their findings can enhance generalization guarantees, particularly in applications involving sparse excess risk bounds. However, their methodology and results appear to be less rigorous and impactful than established works in the field, such as those by Feldman & Vondrák (2019) and Bousquet et al. (2020).\n\n# Strength And Weaknesses\nThe paper does present a novel approach to analyzing Lq-stability and attempts to contribute to the understanding of generalization in statistical learning. However, it lacks the depth and rigor of earlier established bounds and fails to provide compelling evidence that their results offer any significant advancements over previous research. The reliance on a distribution-dependent notion of stability limits the broader applicability of their findings, making their contributions feel reiterative of existing literature rather than groundbreaking.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, but the quality of the contributions is undermined by a lack of originality and rigor. The novelty of the proposed Lq-stability bounds is overstated, as they do not significantly improve upon previous work in the domain of statistical learning theory. Reproducibility is not thoroughly addressed, as the paper does not provide sufficient justification for the applicability or superiority of their methods over established results.\n\n# Summary Of The Review\nOverall, while the paper attempts to address important questions in generalization bounds for Lq-stable algorithms, it falls short of delivering substantial new insights or methodologies. The contributions largely echo earlier findings and do not provide a robust challenge to existing theoretical frameworks, leading to a diminished impact on the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"EXPONENTIAL GENERALIZATION BOUNDS WITH NEAR-OPTIMAL RATES FOR Lq-STABLE ALGORITHMS\" presents a comprehensive study focused on establishing exponential generalization bounds for Lq-stable algorithms. The authors introduce a formal framework for uniform stability and derive near-optimal rates of generalization that are independent of the underlying data distribution. Methodologically, the paper hinges on a rigorous analysis of stability coefficients and employs a series of mathematical proofs to demonstrate the strength of the proposed bounds. Key findings indicate that the distribution-dependent notion of Lq-stability, while weaker than uniform stability, still provides meaningful insights into generalization performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear delineation of concepts such as Lq-stability and uniform stability, alongside robust mathematical formulations that contribute to a deeper understanding of generalization bounds. The thoroughness of the proofs adds credibility to the claims made. However, the paper exhibits some weaknesses, primarily in clarity and consistency of terminology, which may hinder reader comprehension. Certain sections could benefit from enhanced explanations and definitions, particularly for terms that might be unfamiliar to a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the clarity of the paper is somewhat compromised by inconsistent terminology and formatting issues, which detract from the overall quality of presentation. While the mathematical rigor is commendable, the exposition could be improved for greater accessibility. The novelty of the contributions is evident, particularly in the exposition of near-optimal rates for Lq-stable algorithms; however, reproducibility may be a concern due to the lack of clarity in certain definitions and terms.\n\n# Summary Of The Review\nThis paper presents significant contributions to the understanding of exponential generalization bounds in the context of Lq-stable algorithms, backed by rigorous mathematical analysis. However, clarity and consistency issues need to be addressed for improved accessibility and comprehension.\n\n# Correctness\n4/5 - The mathematical arguments and proofs appear sound, but minor inconsistencies in notation and terminology could lead to misunderstandings.\n\n# Technical Novelty And Significance\n4/5 - The paper introduces valuable insights and technical contributions regarding Lq-stability and generalization bounds, though some aspects may not be entirely groundbreaking.\n\n# Empirical Novelty And Significance\n3/5 - While the theoretical results are robust, the empirical implications and applications could be elaborated further to demonstrate their significance in practical scenarios.",
    "# Summary Of The Paper\nThe paper focuses on investigating Lq-stability and its implications for generalization bounds in learning algorithms. The authors present improved theoretical bounds for Lq-stable algorithms, primarily analyzing exponential generalization bounds. However, the paper lacks exploration of how these findings can be extended to other stability notions or applied to various learning scenarios, including adversarial settings and practical applications in deep learning and reinforcement learning.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by deriving improved bounds for Lq-stable algorithms, which enhances our understanding of stability in learning contexts. However, it has notable weaknesses, such as a limited discussion on the robustness of the proposed bounds against model misspecification and insufficient empirical validation of the theoretical results. Additionally, the implications for real-world applications are underexplored, and there is a missed opportunity to connect with advancements in related fields like differential privacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, with well-structured sections that convey the theoretical contributions. However, the novelty is somewhat limited to a narrow focus on Lq-stability without sufficient exploration of broader implications or related areas. The reproducibility of the findings may be affected by the lack of empirical validation and comparative analysis with existing approaches.\n\n# Summary Of The Review\nOverall, the paper contributes valuable theoretical insights into Lq-stability and generalization bounds, but it falls short in addressing practical applications and empirical validation. Expanding the scope to include more learning scenarios and exploring connections with other fields would enhance the impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" presents significant advancements in understanding the generalization capabilities of Lq-stable learning algorithms. The authors derive exponential generalization bounds based on Lq-stability, which is a distribution-dependent measure of stability. They introduce crucial theorems, including a moment inequality for sums of random variables under Lq-norm conditions, a generalization bound that improves the overhead factor on the stability parameter from \\( N \\) to \\( \\log(N) \\), and an excess risk bound that demonstrates the near-optimal performance of Lq-stable algorithms compared to uniformly stable ones. The results are shown to hold with high probability, indicating robust statistical significance.\n\n# Strength And Weaknesses\nStrengths of the paper include its strong theoretical contributions, particularly the innovative use of concentration inequalities to derive sharper generalization bounds. The results bridge a notable gap in existing literature, demonstrating that Lq-stability can yield competitive generalization guarantees. However, a potential weakness lies in the specificity of the bounds, as the conditions may limit applicability to certain types of learning algorithms or distributions. Furthermore, while the theoretical framework is sound, the paper lacks empirical validation, which could strengthen the claims regarding the practical implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with statistical learning theory. The derivations are presented with sufficient detail, enabling reproducibility of the results. The novelty of the approach, particularly in deriving moment inequalities and generalization bounds for Lq-stable algorithms, is commendable and adds value to the field. However, the paper could benefit from a more comprehensive discussion on the practical implications of the theoretical results.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the understanding of Lq-stable learning algorithms by providing robust theoretical bounds and highlighting the importance of using concentration inequalities. While the clarity and novelty of the work are commendable, the lack of empirical validation raises questions about the practical applicability of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents an exploration of Lq-stability in machine learning algorithms, specifically focusing on deriving near-optimal exponential generalization bounds for Lq-stable algorithms. The methodology involves analyzing the theoretical properties of these bounds and applying them to Iterative Hard Thresholding (IHT) algorithms as a case study. While the findings highlight the potential of Lq-stability, particularly in terms of generalization capabilities, the paper does not delve deeply into the practical implications or broader applications of the framework.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical contributions, particularly the derivation of generalization bounds that advance our understanding of Lq-stable algorithms. However, it has notable weaknesses, such as a lack of exploration regarding the limitations of the Lq-stability framework compared to uniform stability, which could provide a more stringent and distribution-free perspective. The practical implications of the derived bounds remain largely unexplored, and the discussion on computational complexity and real-world implementation is insufficient. Additionally, the potential trade-offs between stability and other performance metrics, as well as the robustness of the results given the assumptions made, are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear exposition of the theoretical results, albeit lacking in-depth discussions on several critical aspects. The novelty of the contributions is moderate; while the derivation of generalization bounds is a valuable addition, the failure to explore broader implications and applications limits its impact. Reproducibility is not fully addressed, as there is no comprehensive methodological framework provided for extending the results to other scenarios or applications.\n\n# Summary Of The Review\nOverall, the paper makes significant theoretical contributions to the understanding of Lq-stability in machine learning but falls short in addressing practical implications and limitations. While it offers valuable insights, the lack of exploration into the broader applicability, computational complexities, and robustness of the results diminishes its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Exponential Generalization Bounds with Near-Optimal Rates for Lq-Stable Algorithms\" explores the relationship between stability and generalization error, proposing improvements in generalization bounds associated with Lq-stability. The authors present a series of theoretical results that purportedly offer near-optimal rates for these bounds, comparing their findings to existing results in the literature. However, much of the methodology and underlying concepts appear to reiterate well-established ideas, leading to questions about the novelty of their contributions.\n\n# Strength And Weaknesses\nOne strength of the paper is its thorough review of prior work, which demonstrates the authors' understanding of the historical context of stability in machine learning. However, the weaknesses are pronounced; the contributions lack substantial innovation, as many of the proposed results appear to be minor improvements over existing bounds. The reliance on established concepts without clear differentiation raises doubts about the paper's impact and originality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is adequate, with a logical structure and clear notation familiar to those in the field. However, the novelty is questionable, as the authors do not convincingly argue how their results advance the current understanding of stability and generalization. Reproducibility is not explicitly addressed, and given the incremental nature of the contributions, one might find it challenging to discern how to build upon this work in a meaningful way.\n\n# Summary Of The Review\nOverall, the paper presents a familiar narrative regarding stability and generalization, with claims of novel contributions that do not significantly deviate from existing literature. While the authors demonstrate a solid grasp of prior research, their work ultimately feels like a rehashing of old ideas rather than a breakthrough in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents significant improvements in exponential generalization bounds for Lq-stable algorithms. The methodology involves deriving new moment inequalities and risk bounds for inexact L0-ERM. The findings suggest that the improved Lq-stability allows for nearly optimal generalization bounds, akin to those of uniformly stable algorithms, thereby advancing the theoretical understanding of generalization in machine learning.\n\n# Strength And Weaknesses\nThe strengths of the paper include its contribution to enhancing generalization bounds through Lq-stability, which is a valuable advancement in the field. The application of the proposed theoretical framework to Iterative Hard Thresholding (IHT) demonstrates practical relevance. However, the paper could benefit from exploring alternative stability notions, such as entropy-based measures, and the proposed bounds may not fully account for robustness under adversarial conditions or in high-dimensional settings. Additionally, the lack of empirical validation of the theoretical results limits the practical implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of complex concepts. The quality of the theoretical analysis is high, but the novelty could be further enhanced by integrating insights from robust statistics and information-theoretic principles. The reproducibility of the results might be limited, as empirical validation is not sufficiently addressed.\n\n# Summary Of The Review\nOverall, the paper makes a notable contribution to the understanding of generalization bounds for Lq-stable algorithms, with a solid theoretical foundation. However, the lack of empirical validation and exploration of alternative approaches somewhat diminishes its applicability in real-world contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces improved exponential generalization bounds for Lq-stable algorithms, demonstrating that these algorithms can achieve near-optimal performance that is comparable to uniformly stable algorithms. Through rigorous analysis, it presents specific performance metrics, including a generalization bound expressed as \\(\\|R(A(S)) - RS(A(S))\\|_q \\approx q\\gamma_q \\log(N) + M_q \\sqrt{\\frac{q}{N}}\\), which significantly reduces the overhead factor from \\(N\\) to \\(\\log(N)\\). Additionally, the paper establishes an excess risk bound under the generalized Bernstein condition, illustrating favorable scaling with respect to sample size and features when applied to inexact L0-estimators. The empirical evaluation, particularly of the Iterative Hard Thresholding (IHT) algorithm, showcases improved generalization performance in high-dimensional sparse learning tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical contributions that advance the understanding of Lq-stable algorithms, particularly their generalization capabilities. The derived bounds are well-founded and present a notable improvement over existing results, making a strong case for the practical applicability of Lq-stable algorithms in real-world scenarios. However, the paper could benefit from a broader empirical validation across diverse datasets and tasks to substantiate the claims regarding generalization performance. Additionally, further exploration of the implications of these bounds in the context of different learning frameworks could enhance the paper's depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and logically structured, making it accessible to readers with a background in machine learning theory. The quality of the mathematical exposition is high, with clear definitions and justifications for the results presented. The novelty of the findings is significant, particularly in the context of generalization bounds for Lq-stable algorithms, which have been less explored compared to their uniformly stable counterparts. Reproducibility is supported by the thorough presentation of the methodology and the performance metrics, although additional implementation details or code availability could strengthen this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of machine learning by establishing improved generalization bounds for Lq-stable algorithms. While the theoretical insights are compelling, further empirical validation is recommended to solidify the practical implications of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a novel approach to analyzing stability in machine learning models, specifically focusing on \"Lq-stability\" and \"uniform stability.\" The authors propose a set of new theorems that quantify the exponential generalization bounds and introduce sparsity estimation algorithms to enhance model interpretability. The methodology involves rigorous theoretical proofs and extensive empirical evaluations, demonstrating that the proposed techniques outperform existing methods in various scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which provide a clearer understanding of stability in machine learning frameworks. The introduction of new definitions and metrics is noteworthy, as it addresses a gap in the literature. However, the paper suffers from clarity issues; the dense writing and complex terminology may alienate readers unfamiliar with the concepts. Additionally, the lack of visual aids such as figures and tables detracts from the overall comprehensibility of the presented results.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper exhibits high technical quality and introduces significant novel concepts, the clarity of writing is a major concern. The use of jargon without adequate definitions makes it less accessible. Furthermore, the methodology section could benefit from better organization and clearer explanations of the algorithms used, which would aid in reproducibility. The logical flow of the arguments is occasionally disrupted, necessitating smoother transitions between sections.\n\n# Summary Of The Review\nOverall, the paper makes commendable theoretical contributions to understanding stability in machine learning but struggles with clarity and accessibility. Improvements in writing style and the inclusion of visual aids would significantly enhance the paper's impact and comprehensibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -1.8881594886253816,
    -1.7549764985921676,
    -1.8476434009334752,
    -1.612459565401597,
    -2.042358879395706,
    -1.673495769797636,
    -1.7365490700216126,
    -1.8003411355582,
    -1.749667733025761,
    -1.754024505982434,
    -1.67453734423925,
    -1.5695845927219991,
    -1.4903677508639661,
    -1.5883048831595634,
    -1.782201881494874,
    -1.5790056800829497,
    -1.897371700377482,
    -2.006984005877826,
    -1.6558417847584914,
    -1.6429513437497674,
    -1.913099183958914,
    -1.770178253569796,
    -1.7504930460828572,
    -1.7225293513407562,
    -1.7352349591400915,
    -1.810090918836043,
    -1.9072639068985617,
    -1.78375471153603,
    -1.8989755793139353
  ],
  "logp_cond": [
    [
      0.0,
      -1.8440276218044211,
      -1.8525554557549755,
      -1.840731025875026,
      -1.8426323181097726,
      -1.8420596242663247,
      -1.8513857253393227,
      -1.8426564949283581,
      -1.8425295361207086,
      -1.854834157256232,
      -1.8450874773826769,
      -1.860326763205959,
      -1.8474480442019643,
      -1.8492781857489577,
      -1.8462211174040055,
      -1.8398716359132707,
      -1.84854742292453,
      -1.8448430812310792,
      -1.8422241106338073,
      -1.848328300654762,
      -1.8517117525888573,
      -1.842706267044841,
      -1.8519004400479437,
      -1.8467643281610124,
      -1.848700767088453,
      -1.8465819972596573,
      -1.8383015937264118,
      -1.83304043139302,
      -1.8500412659876226
    ],
    [
      -1.3932992361014436,
      0.0,
      -1.2889304712420901,
      -1.23432711639531,
      -1.3023693659799516,
      -1.3109688239735082,
      -1.3366046328377532,
      -1.243769385569441,
      -1.2378064016128236,
      -1.3795199276455627,
      -1.221832485954505,
      -1.462514114664886,
      -1.273295258177393,
      -1.3107096418767006,
      -1.231378602418737,
      -1.2254406099662996,
      -1.3196373084712445,
      -1.2727853848531527,
      -1.2965003071763694,
      -1.2708317246613154,
      -1.3139502488598382,
      -1.3785526236553731,
      -1.3911425439240905,
      -1.3048676713953131,
      -1.3531643381170773,
      -1.3085580249778814,
      -1.2978424950833611,
      -1.3150042238319928,
      -1.3869839429031996
    ],
    [
      -1.4527132720412024,
      -1.3521900234250352,
      0.0,
      -1.3511203137072298,
      -1.3584210615137011,
      -1.3713893884703061,
      -1.478681242753788,
      -1.4362095609874685,
      -1.3843693243668473,
      -1.4498850787912305,
      -1.3857059223184311,
      -1.5443710759540488,
      -1.4223373301623317,
      -1.4766150367935635,
      -1.4347397453143091,
      -1.350653599053925,
      -1.371748974715253,
      -1.4013399711469288,
      -1.4072152960799342,
      -1.3483356613600335,
      -1.4870468817507076,
      -1.5061337430913218,
      -1.4592072293453322,
      -1.447834777351605,
      -1.4039095207526757,
      -1.431255516081143,
      -1.3932595520458293,
      -1.4139727105509237,
      -1.4463988666820577
    ],
    [
      -1.222583869661304,
      -1.0985721914920794,
      -1.1508043577866542,
      0.0,
      -1.2058930066249358,
      -1.1979225164231024,
      -1.1632807320302836,
      -1.1141340192419031,
      -1.0742436843130443,
      -1.255064547211367,
      -1.1821120668992304,
      -1.2988761526359793,
      -1.1799592239452938,
      -1.1802641214010836,
      -1.127467770673733,
      -1.1387061497913382,
      -1.1694070256266342,
      -1.1786584550688595,
      -1.1584791865431563,
      -1.0855014815598085,
      -1.1919827151180085,
      -1.205212972373552,
      -1.2026707725587875,
      -1.104210313168026,
      -1.150600162936216,
      -1.1394427389264215,
      -1.1649029831348936,
      -1.1447614907167665,
      -1.218462443391919
    ],
    [
      -1.6519742363596748,
      -1.5848234700979689,
      -1.5537908388899155,
      -1.5503042664007518,
      0.0,
      -1.5813722445404657,
      -1.7035511409431878,
      -1.6022628094560984,
      -1.5258246982303072,
      -1.670580935868917,
      -1.592709803119306,
      -1.7660696562108695,
      -1.5806698791401579,
      -1.6160877670337674,
      -1.5961655843924332,
      -1.4830017765534254,
      -1.4933497592527771,
      -1.4956237581631568,
      -1.5653826934962791,
      -1.5623472612201807,
      -1.662556785469464,
      -1.6316671907352265,
      -1.6619689997983,
      -1.6160160303731517,
      -1.5386166928056153,
      -1.6551119834417687,
      -1.5393599326041267,
      -1.6097161744563302,
      -1.635941578552941
    ],
    [
      -1.2866957631847562,
      -1.1692565076505073,
      -1.1621121394788125,
      -1.1895275810296404,
      -1.1299733038791484,
      0.0,
      -1.2156310013561333,
      -1.1120478010490584,
      -1.178873164638737,
      -1.2617710249766656,
      -1.1992437074021742,
      -1.3032306735928978,
      -1.1786070868566423,
      -1.1303614160821622,
      -1.1473755314727334,
      -1.1605458885141402,
      -1.158306316562606,
      -1.0908759410191473,
      -1.1717293697952482,
      -1.1677834133152725,
      -1.262235702804063,
      -1.232484539984971,
      -1.2603343669357148,
      -1.2085193457343761,
      -1.1894247232854849,
      -1.2476424329244744,
      -1.1403853903412924,
      -1.2550307876027627,
      -1.2942259975309072
    ],
    [
      -1.3770698824210839,
      -1.2832429529682075,
      -1.3310216539499626,
      -1.2534201949027093,
      -1.3061906768210145,
      -1.2619629331447022,
      0.0,
      -1.2212255276348998,
      -1.2573123097151266,
      -1.358922823649123,
      -1.3449490088859997,
      -1.410062451234469,
      -1.3231975732658579,
      -1.276700309559689,
      -1.2607833928388736,
      -1.3198441504641354,
      -1.3299731898192348,
      -1.344782499377112,
      -1.3349255160671396,
      -1.2284746495474073,
      -1.3368324566853402,
      -1.2942414280182308,
      -1.370983834039777,
      -1.295284586864603,
      -1.315803940648888,
      -1.3276348975712022,
      -1.299131006210713,
      -1.3701703907195155,
      -1.3730374896534479
    ],
    [
      -1.4502474846831979,
      -1.2742116271365798,
      -1.400694516553476,
      -1.3009813726293769,
      -1.3606664060082136,
      -1.2837542512274098,
      -1.2948795498553192,
      0.0,
      -1.2903346516976186,
      -1.461729766253667,
      -1.3259966008623418,
      -1.4861124718230199,
      -1.320061883235645,
      -1.2727532847730725,
      -1.2775914362947003,
      -1.364108975872078,
      -1.3857190499336975,
      -1.3209915180645349,
      -1.3493547513515307,
      -1.2909218722876101,
      -1.3615910642999411,
      -1.3788627040097559,
      -1.449899286021375,
      -1.3419677771807204,
      -1.3821847747360867,
      -1.3357895630260792,
      -1.361704406437138,
      -1.351140388334785,
      -1.4095394445934926
    ],
    [
      -1.3851258662156203,
      -1.2654798465446844,
      -1.359696982958601,
      -1.2526062333159536,
      -1.293582078139217,
      -1.3219446913436637,
      -1.298937385264003,
      -1.228839842306533,
      0.0,
      -1.403577901937609,
      -1.315805036747971,
      -1.4619720666991591,
      -1.2677661400403624,
      -1.3390484701756638,
      -1.2882513810772922,
      -1.307712240474892,
      -1.3090636906044084,
      -1.3143701963342433,
      -1.3170198595314537,
      -1.3287307610153272,
      -1.3330551704016187,
      -1.33462420363286,
      -1.4024350689226617,
      -1.303197707825284,
      -1.340416579174226,
      -1.316085134835957,
      -1.3100857031930133,
      -1.2783780470053585,
      -1.363422998945149
    ],
    [
      -1.3551056066301237,
      -1.346424980926162,
      -1.3420048062274659,
      -1.3448480922256167,
      -1.2950518691084816,
      -1.3588227012122511,
      -1.4046222310751453,
      -1.3867729181083137,
      -1.3349975488677046,
      0.0,
      -1.313014898077852,
      -1.4346982147241671,
      -1.30337527261347,
      -1.4110570402858462,
      -1.3616357467557951,
      -1.3408615275878932,
      -1.2437244544114023,
      -1.3269439318953902,
      -1.3275601204166874,
      -1.4121793998840346,
      -1.3551369958746475,
      -1.391613632802683,
      -1.3454035058710467,
      -1.3458110157364864,
      -1.292604574134011,
      -1.3597569521792328,
      -1.2759850145474085,
      -1.3272094188564667,
      -1.374581625284273
    ],
    [
      -1.3013553534964595,
      -1.1396316504189377,
      -1.2256598506942984,
      -1.2009770248179825,
      -1.2156176834022856,
      -1.2305431220787797,
      -1.2673560366436305,
      -1.1689105825587076,
      -1.182912800234485,
      -1.3016030027409644,
      0.0,
      -1.3980143619702845,
      -1.195456130483221,
      -1.2087690414073857,
      -1.223873830258855,
      -1.1503852489952489,
      -1.200570407554339,
      -1.1648166122932437,
      -1.131163239223691,
      -1.2228164806254658,
      -1.2135460438833923,
      -1.3156641690629716,
      -1.2960822077926928,
      -1.2562899834898766,
      -1.243241052175049,
      -1.240086214589809,
      -1.2215442354200419,
      -1.234643039478262,
      -1.3162589582331372
    ],
    [
      -1.2528169719553326,
      -1.2716752676755785,
      -1.2983218400615673,
      -1.2667492626796766,
      -1.2375440517748109,
      -1.2513636713325647,
      -1.2474721410230685,
      -1.1800641785361652,
      -1.227015602426183,
      -1.2332348993512985,
      -1.2658732562110488,
      0.0,
      -1.2673015288000071,
      -1.2175989671338283,
      -1.2566443619280245,
      -1.2471064214888068,
      -1.1921766844833899,
      -1.2795045696078093,
      -1.2720923828072972,
      -1.2395858011727738,
      -1.212413412193678,
      -1.1879662353345404,
      -1.2594716860324766,
      -1.290255454693808,
      -1.2146217126276946,
      -1.2015843519583298,
      -1.227527260681682,
      -1.2647440786658797,
      -1.221972729292274
    ],
    [
      -1.1262883510632669,
      -1.0115790923106445,
      -1.061517364794243,
      -1.0103968231110982,
      -1.0149913382688698,
      -1.0548055512126753,
      -1.1270044229646556,
      -0.9790641688761365,
      -1.004257479412167,
      -1.105353027643504,
      -0.9944642664071451,
      -1.1901760649477107,
      0.0,
      -1.0626288308382235,
      -0.9806338167238692,
      -0.9411313235192386,
      -1.0383847290684802,
      -0.9795443962479228,
      -0.956002701659777,
      -1.0650283986405207,
      -1.043618682062981,
      -1.0941783256460436,
      -1.1073781905202933,
      -1.0738454612310862,
      -1.024950717857248,
      -1.0704113724920732,
      -1.0156496661821095,
      -1.02760446681919,
      -1.0794972142837338
    ],
    [
      -1.2434293222316881,
      -1.1734842106537975,
      -1.2450054440977634,
      -1.1671279011298648,
      -1.1749069392710425,
      -1.1679183005554357,
      -1.1594394118712117,
      -1.0568105266777825,
      -1.1462446064413265,
      -1.2462298386308333,
      -1.1677506107368674,
      -1.2683896370579313,
      -1.1999383279654243,
      0.0,
      -1.1019976940875769,
      -1.1921252136117833,
      -1.198199389446195,
      -1.1907236371590766,
      -1.2186747490257002,
      -1.1315833650547757,
      -1.1810643008539154,
      -1.1568598919168125,
      -1.222746209874792,
      -1.1702123657854655,
      -1.205636087222935,
      -1.139515385354349,
      -1.159983317497113,
      -1.2282538841461716,
      -1.187198716568441
    ],
    [
      -1.3916708111166793,
      -1.2044830811074059,
      -1.314983836476923,
      -1.2599623519062377,
      -1.2868970042031347,
      -1.2700035526650668,
      -1.2911981470567824,
      -1.2019813427655834,
      -1.2588107073410657,
      -1.372220576495928,
      -1.3084659057240973,
      -1.4185083826425406,
      -1.2630974345867343,
      -1.2494084717127623,
      0.0,
      -1.276049618020362,
      -1.2845118946519658,
      -1.270894805950711,
      -1.292816629023625,
      -1.2488911014193294,
      -1.3316653577260096,
      -1.342537361805017,
      -1.3526932148428237,
      -1.2577757464988093,
      -1.3438349873853375,
      -1.2939724997892794,
      -1.3323285316219766,
      -1.3164323215533864,
      -1.328973448132836
    ],
    [
      -1.236768334272053,
      -1.0873933422215507,
      -1.1898189911542163,
      -1.1335766774909397,
      -1.1184837987824292,
      -1.1947650941048258,
      -1.2493645751069897,
      -1.1837268918294441,
      -1.1361416095904227,
      -1.2639981686939543,
      -1.0935629946402545,
      -1.2897118926461626,
      -1.1185065666178526,
      -1.2195664276957254,
      -1.17471650945999,
      0.0,
      -1.1580999568820758,
      -1.1216558995559482,
      -1.1298885122462823,
      -1.1875819259614215,
      -1.171011250237774,
      -1.2610327722446346,
      -1.2369482532633078,
      -1.1874796865376709,
      -1.179795843907717,
      -1.2167469050504816,
      -1.1812892171557194,
      -1.2048622468936387,
      -1.2406447968932341
    ],
    [
      -1.5362852591991487,
      -1.5278689138844228,
      -1.5472487102477748,
      -1.561574931878406,
      -1.4577138331202681,
      -1.5674365070886478,
      -1.5985463181049273,
      -1.5700157072967367,
      -1.4965223668795695,
      -1.5587432327356512,
      -1.507415695126893,
      -1.6982066749339657,
      -1.5312352168597965,
      -1.5911296442043157,
      -1.5385232040283199,
      -1.5105935233380656,
      0.0,
      -1.5237629706328912,
      -1.5217805811912182,
      -1.6089192606949327,
      -1.561035633606587,
      -1.587544675221896,
      -1.6030993244456937,
      -1.5635111808554938,
      -1.4985170212458716,
      -1.6103163190404473,
      -1.5045445619982605,
      -1.5395452107194625,
      -1.617526815715614
    ],
    [
      -1.6236914636647444,
      -1.4365389279508671,
      -1.5002510419452693,
      -1.456598684188252,
      -1.391571305233238,
      -1.4312342840838148,
      -1.5841020242683583,
      -1.4725057425760717,
      -1.4444963893690164,
      -1.5811306723653191,
      -1.4083118089491906,
      -1.6612122546091386,
      -1.4224036701511447,
      -1.4704481042203288,
      -1.4201469912049582,
      -1.367025337505246,
      -1.5034415878476248,
      0.0,
      -1.4281438004498543,
      -1.5129334259961267,
      -1.4938967954436817,
      -1.583204107903516,
      -1.5744944326773678,
      -1.4782421198916649,
      -1.5181296737103211,
      -1.5228425828542913,
      -1.461808268908579,
      -1.4739113508839652,
      -1.5901675565432467
    ],
    [
      -1.2325737743346143,
      -1.089358156754439,
      -1.151253521206248,
      -1.1084558264223685,
      -1.0995976443430695,
      -1.108336361690312,
      -1.2191247586570664,
      -1.0831634765208547,
      -1.132171593149888,
      -1.2107511003660338,
      -1.0140078742431944,
      -1.3437121213605545,
      -1.0706234558286951,
      -1.137620219921053,
      -1.0520680732474454,
      -1.0445109605634724,
      -1.1091704111649943,
      -1.0868022744145978,
      0.0,
      -1.1457012022713646,
      -1.1232710322944008,
      -1.2243374779577572,
      -1.1833715648344612,
      -1.1447193371972657,
      -1.1529538936681425,
      -1.1683939712023068,
      -1.0574694045129363,
      -1.1490208527800292,
      -1.2585951789020184
    ],
    [
      -1.2640146248762487,
      -1.1065905979015334,
      -1.166254094871362,
      -1.1119724173840528,
      -1.1930571135404813,
      -1.1869816312820733,
      -1.1175800396320714,
      -1.1040528039600506,
      -1.1658762589651024,
      -1.2693748252172223,
      -1.231440729399375,
      -1.310900229154591,
      -1.2203300656341483,
      -1.155395926830722,
      -1.124230058529279,
      -1.2022482706488309,
      -1.2175020686698923,
      -1.2378952323520924,
      -1.2294920085292644,
      0.0,
      -1.2629458387297905,
      -1.1924338922790143,
      -1.2420046023717644,
      -1.1616945276134107,
      -1.1745258193477444,
      -1.1786764381861237,
      -1.2081047999593335,
      -1.2509521925114204,
      -1.241099383216464
    ],
    [
      -1.5386820424605907,
      -1.4695041726694198,
      -1.5012062190226882,
      -1.4651261044508512,
      -1.4425770088708765,
      -1.5353203993461693,
      -1.535425366271019,
      -1.4668126290797006,
      -1.435768022091572,
      -1.5407445912728108,
      -1.422755748792566,
      -1.6420276740920245,
      -1.4564367313690731,
      -1.512146974493053,
      -1.4425589553889435,
      -1.4351996407747107,
      -1.4338292506585502,
      -1.4015164414382941,
      -1.3998106468739726,
      -1.539032282422115,
      0.0,
      -1.5343558846558534,
      -1.5191374584335866,
      -1.4642591475906044,
      -1.4805515863369294,
      -1.430314389133524,
      -1.4472834082308947,
      -1.4696747246666797,
      -1.5432106758562882
    ],
    [
      -1.4121827044966968,
      -1.4003004699150663,
      -1.4213787013895307,
      -1.3893952728336623,
      -1.4055654325402636,
      -1.4002504184525535,
      -1.3597141185099326,
      -1.322194436353282,
      -1.3631701904566782,
      -1.429382745421451,
      -1.4286114158017367,
      -1.4467453024554506,
      -1.4037192041639763,
      -1.369860055043576,
      -1.3696831386911341,
      -1.3991125464878857,
      -1.3976672769429097,
      -1.4172218762396185,
      -1.4219842184173084,
      -1.3512706443853162,
      -1.3740096771781483,
      0.0,
      -1.4069983084389008,
      -1.404347027202936,
      -1.3774822121262376,
      -1.321161293417795,
      -1.4344322111992511,
      -1.4194351746214717,
      -1.361666965639434
    ],
    [
      -1.31305356672231,
      -1.253612929673448,
      -1.2273817333557002,
      -1.2597916611704099,
      -1.2164478107934797,
      -1.2951610256910544,
      -1.306635341875237,
      -1.325208337062235,
      -1.2592181777506293,
      -1.2908949440152757,
      -1.2484701350577054,
      -1.4234694410622113,
      -1.2486579694564504,
      -1.299246869016041,
      -1.2478533310892848,
      -1.2327854307677966,
      -1.2186616026296468,
      -1.2546499932771435,
      -1.237028197110259,
      -1.306232434517362,
      -1.2670160656134541,
      -1.330611316593315,
      0.0,
      -1.2940383378399745,
      -1.2582545308660322,
      -1.2676949194255969,
      -1.1886100480611064,
      -1.281817220614328,
      -1.3044208397011723
    ],
    [
      -1.376210305040741,
      -1.2373255788490083,
      -1.313009123902396,
      -1.2291045438154913,
      -1.3158457625715667,
      -1.3187091135116369,
      -1.3051401408124799,
      -1.282648980900338,
      -1.2381270582321542,
      -1.3517260452900284,
      -1.3138702057337357,
      -1.4282334050523169,
      -1.3324900626186478,
      -1.2810231905921994,
      -1.2390215577534416,
      -1.2981772274077612,
      -1.2996183469683558,
      -1.326982123667345,
      -1.3296660811278704,
      -1.285301790178445,
      -1.3177883818800544,
      -1.312248000180952,
      -1.3609019244801184,
      0.0,
      -1.3490901678560219,
      -1.2999462957594183,
      -1.312167263844401,
      -1.298229143433004,
      -1.3484468795003375
    ],
    [
      -1.369024340535444,
      -1.318412607781192,
      -1.2973735487349487,
      -1.2800654923043977,
      -1.2470491217887245,
      -1.3279916379727488,
      -1.3598407495684703,
      -1.3108737858832877,
      -1.308985758394865,
      -1.400767132360015,
      -1.3289441984493977,
      -1.4777149752138483,
      -1.2898458040987908,
      -1.354803974003051,
      -1.3289805698500887,
      -1.2645470149245943,
      -1.2228489433274035,
      -1.3161332528497198,
      -1.3031057970842626,
      -1.3269079311656953,
      -1.3450343820380348,
      -1.3908592575847833,
      -1.340662355914016,
      -1.3612161088298582,
      0.0,
      -1.321970911429236,
      -1.2961876789267084,
      -1.3271780826830386,
      -1.3749448794853283
    ],
    [
      -1.405976973709412,
      -1.2940354930164435,
      -1.3635003783144444,
      -1.3174251332170033,
      -1.3487521868656576,
      -1.3647249593814106,
      -1.335291702618013,
      -1.2948477010340969,
      -1.2861916376279379,
      -1.4168013907498744,
      -1.3609666397847762,
      -1.466369008414176,
      -1.3230356819899927,
      -1.3420671001296227,
      -1.2865402136333535,
      -1.3400436104538676,
      -1.3466486113761278,
      -1.3507076297829854,
      -1.3483193392224275,
      -1.3523068049126175,
      -1.338578667209086,
      -1.4006758702703823,
      -1.3585763137198545,
      -1.27898113474707,
      -1.3438846952303247,
      0.0,
      -1.357557495768946,
      -1.3576312426998212,
      -1.3671363975969597
    ],
    [
      -1.4151302800718264,
      -1.3759410298530377,
      -1.4282927515864818,
      -1.3888186536639593,
      -1.358248089808518,
      -1.4595192183205554,
      -1.5136387369698028,
      -1.3776072226366094,
      -1.434218939061415,
      -1.4236308650270626,
      -1.3760565503176083,
      -1.561052418551338,
      -1.4168023689207512,
      -1.4340354609249741,
      -1.4145919437883905,
      -1.403045411168572,
      -1.3368692358506566,
      -1.3857256376487384,
      -1.364471801691969,
      -1.3859954234104617,
      -1.442376682487569,
      -1.4985981920053584,
      -1.4162871575490146,
      -1.4369894783126418,
      -1.3798081334659418,
      -1.4402532495251206,
      0.0,
      -1.3911010976967864,
      -1.5103353242267679
    ],
    [
      -1.4007581736065227,
      -1.3971731291469105,
      -1.3957995885672583,
      -1.3811518950352777,
      -1.3856384648305176,
      -1.4269319281109296,
      -1.484657839653227,
      -1.4143240117191922,
      -1.4058272081082628,
      -1.4674749879267723,
      -1.3985412754694146,
      -1.5297058812587674,
      -1.3762778510904394,
      -1.4529560748901744,
      -1.416161098895509,
      -1.372996626426069,
      -1.4295082107783847,
      -1.3970539091642016,
      -1.4190539261326753,
      -1.4104785786819112,
      -1.4633644065058435,
      -1.4851362217807635,
      -1.5102207897667856,
      -1.3690785911497976,
      -1.4085925322644137,
      -1.4643428217416345,
      -1.3745119379306707,
      0.0,
      -1.4994187501270662
    ],
    [
      -1.5592609630850913,
      -1.4652408060553703,
      -1.4254030136675557,
      -1.4567459998361756,
      -1.4073358956445479,
      -1.421548024649866,
      -1.470425793888554,
      -1.4233200342616719,
      -1.4054257064704807,
      -1.4451387612371767,
      -1.4459351212729061,
      -1.5354428742165542,
      -1.4305995978725619,
      -1.463381891955118,
      -1.4109554705171903,
      -1.4118984461977202,
      -1.4186117886312877,
      -1.4203887147941476,
      -1.4333408759626791,
      -1.4842147627612456,
      -1.442286068626862,
      -1.4346755858429063,
      -1.4599529969448102,
      -1.46202718855313,
      -1.4021964677504946,
      -1.4579952439260713,
      -1.4306404396921673,
      -1.4453552038415753,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.044131866820960486,
      0.035604032870406144,
      0.047428462750355616,
      0.04552717051560906,
      0.046099864359056886,
      0.03677376328605897,
      0.045502993697023486,
      0.045629952504673055,
      0.033325331369149724,
      0.043072011242704766,
      0.02783272541942261,
      0.04071144442341734,
      0.038881302876423884,
      0.041938371221376114,
      0.04828785271211089,
      0.03961206570085163,
      0.043316407394302425,
      0.04593537799157432,
      0.039831187970619686,
      0.03644773603652429,
      0.04545322158054055,
      0.03625904857743789,
      0.041395160464369196,
      0.03945872153692864,
      0.041577491365724306,
      0.04985789489896986,
      0.05511905723236166,
      0.03811822263775899
    ],
    [
      0.36167726249072407,
      0.0,
      0.4660460273500775,
      0.5206493821968576,
      0.4526071326122161,
      0.4440076746186594,
      0.41837186575441443,
      0.5112071130227267,
      0.517170096979344,
      0.3754565709466049,
      0.5331440126376625,
      0.29246238392728174,
      0.48168124041477456,
      0.44426685671546706,
      0.5235978961734307,
      0.5295358886258681,
      0.43533919012092315,
      0.4821911137390149,
      0.4584761914157982,
      0.4841447739308522,
      0.4410262497323294,
      0.3764238749367945,
      0.3638339546680771,
      0.4501088271968545,
      0.4018121604750904,
      0.4464184736142862,
      0.4571340035088065,
      0.43997227476017486,
      0.36799255568896805
    ],
    [
      0.39493012889227286,
      0.49545337750844,
      0.0,
      0.4965230872262454,
      0.4892223394197741,
      0.4762540124631691,
      0.36896215817968714,
      0.4114338399460067,
      0.4632740765666279,
      0.3977583221422447,
      0.4619374786150441,
      0.30327232497942647,
      0.4253060707711436,
      0.3710283641399117,
      0.4129036556191661,
      0.4969898018795502,
      0.4758944262182223,
      0.44630342978654647,
      0.440428104853541,
      0.49930773957344177,
      0.3605965191827676,
      0.3415096578421535,
      0.38843617158814303,
      0.3998086235818703,
      0.4437338801807995,
      0.41638788485233214,
      0.45438384888764594,
      0.43367069038255157,
      0.4012445342514175
    ],
    [
      0.3898756957402929,
      0.5138873739095176,
      0.4616552076149427,
      0.0,
      0.40656655877666115,
      0.4145370489784945,
      0.44917883337131337,
      0.4983255461596938,
      0.5382158810885527,
      0.3573950181902299,
      0.43034749850236653,
      0.31358341276561763,
      0.4325003414563031,
      0.43219544400051335,
      0.4849917947278639,
      0.47375341561025874,
      0.44305253977496273,
      0.4338011103327375,
      0.45398037885844067,
      0.5269580838417884,
      0.4204768502835885,
      0.4072465930280449,
      0.40978879284280945,
      0.5082492522335709,
      0.46185940246538104,
      0.4730168264751755,
      0.44755658226670336,
      0.4676980746848305,
      0.39399712200967807
    ],
    [
      0.3903846430360314,
      0.4575354092977373,
      0.4885680405057906,
      0.49205461299495434,
      0.0,
      0.4609866348552405,
      0.3388077384525183,
      0.4400960699396077,
      0.516534181165399,
      0.37177794352678917,
      0.44964907627640005,
      0.2762892231848366,
      0.46168900025554827,
      0.4262711123619387,
      0.446193295003273,
      0.5593571028422808,
      0.549009120142929,
      0.5467351212325493,
      0.47697618589942703,
      0.4800116181755254,
      0.3798020939262421,
      0.41069168866047967,
      0.3803898795974061,
      0.4263428490225545,
      0.5037421865900908,
      0.38724689595393746,
      0.5029989467915794,
      0.43264270493937595,
      0.4064173008427652
    ],
    [
      0.38680000661287983,
      0.5042392621471288,
      0.5113836303188235,
      0.48396818876799563,
      0.5435224659184876,
      0.0,
      0.4578647684415027,
      0.5614479687485776,
      0.4946226051588991,
      0.41172474482097043,
      0.4742520623954618,
      0.37026509620473824,
      0.49488868294099375,
      0.5431343537154738,
      0.5261202383249026,
      0.5129498812834958,
      0.5151894532350301,
      0.5826198287784887,
      0.5017664000023878,
      0.5057123564823636,
      0.4112600669935731,
      0.44101122981266494,
      0.41316140286192127,
      0.4649764240632599,
      0.48407104651215116,
      0.4258533368731616,
      0.5331103794563437,
      0.4184649821948734,
      0.37926977226672887
    ],
    [
      0.35947918760052877,
      0.4533061170534052,
      0.40552741607165,
      0.48312887511890334,
      0.4303583932005981,
      0.4745861368769104,
      0.0,
      0.5153235423867129,
      0.479236760306486,
      0.37762624637248954,
      0.39160006113561296,
      0.3264866187871436,
      0.4133514967557548,
      0.4598487604619237,
      0.475765677182739,
      0.41670491955747724,
      0.4065758802023778,
      0.3917665706445006,
      0.401623553954473,
      0.5080744204742054,
      0.3997166133362724,
      0.4423076420033818,
      0.36556523598183555,
      0.4412644831570096,
      0.4207451293727247,
      0.4089141724504104,
      0.43741806381089954,
      0.3663786793020971,
      0.3635115803681648
    ],
    [
      0.3500936508750021,
      0.5261295084216202,
      0.39964661900472387,
      0.4993597629288231,
      0.4396747295499863,
      0.5165868843307901,
      0.5054615857028808,
      0.0,
      0.5100064838605813,
      0.33861136930453295,
      0.47434453469585813,
      0.3142286637351801,
      0.480279252322555,
      0.5275878507851275,
      0.5227496992634997,
      0.43623215968612206,
      0.4146220856245024,
      0.4793496174936651,
      0.45098638420666926,
      0.5094192632705898,
      0.4387500712582588,
      0.4214784315484441,
      0.3504418495368249,
      0.4583733583774796,
      0.41815636082211327,
      0.46455157253212076,
      0.438636729121062,
      0.44920074722341496,
      0.3908016909647074
    ],
    [
      0.3645418668101408,
      0.4841878864810767,
      0.3899707500671601,
      0.4970614997098075,
      0.45608565488654396,
      0.4277230416820974,
      0.45073034776175813,
      0.5208278907192281,
      0.0,
      0.34608983108815217,
      0.43386269627779006,
      0.28769566632660193,
      0.4819015929853987,
      0.4106192628500973,
      0.46141635194846886,
      0.4419554925508691,
      0.44060404242135265,
      0.4352975366915177,
      0.43264787349430733,
      0.4209369720104339,
      0.41661256262414237,
      0.4150435293929011,
      0.3472326641030994,
      0.44647002520047696,
      0.4092511538515351,
      0.43358259818980405,
      0.43958202983274774,
      0.47128968602040255,
      0.3862447340806121
    ],
    [
      0.3989188993523103,
      0.40759952505627206,
      0.4120196997549681,
      0.40917641375681724,
      0.4589726368739524,
      0.39520180477018285,
      0.3494022749072887,
      0.36725158787412027,
      0.41902695711472937,
      0.0,
      0.441009607904582,
      0.31932629125826684,
      0.450649233368964,
      0.34296746569658776,
      0.39238875922663885,
      0.41316297839454075,
      0.5103000515710316,
      0.4270805740870438,
      0.4264643855657466,
      0.34184510609839935,
      0.39888751010778645,
      0.36241087317975107,
      0.40862100011138724,
      0.40821349024594755,
      0.4614199318484229,
      0.3942675538032012,
      0.47803949143502544,
      0.4268150871259673,
      0.3794428806981609
    ],
    [
      0.37318199074279046,
      0.5349056938203123,
      0.44887749354495154,
      0.4735603194212674,
      0.45891966083696434,
      0.44399422216047024,
      0.40718130759561943,
      0.5056267616805423,
      0.49162454400476485,
      0.3729343414982855,
      0.0,
      0.27652298226896543,
      0.47908121375602897,
      0.4657683028318642,
      0.45066351398039495,
      0.5241520952440011,
      0.47396693668491086,
      0.5097207319460062,
      0.5433741050155589,
      0.45172086361378416,
      0.46099130035585767,
      0.35887317517627837,
      0.37845513644655715,
      0.4182473607493733,
      0.43129629206420095,
      0.4344511296494409,
      0.45299310881920807,
      0.439894304760988,
      0.3582783860061127
    ],
    [
      0.31676762076666654,
      0.29790932504642065,
      0.27126275266043187,
      0.30283533004232255,
      0.33204054094718827,
      0.3182209213894345,
      0.32211245169893066,
      0.38952041418583394,
      0.3425689902958162,
      0.33634969337070064,
      0.3037113365109503,
      0.0,
      0.302283063921992,
      0.3519856255881708,
      0.3129402307939746,
      0.32247817123319233,
      0.37740790823860926,
      0.2900800231141898,
      0.29749220991470193,
      0.3299987915492253,
      0.3571711805283211,
      0.38161835738745875,
      0.3101129066895225,
      0.27932913802819104,
      0.35496288009430454,
      0.36800024076366933,
      0.3420573320403171,
      0.30484051405611945,
      0.34761186342972517
    ],
    [
      0.36407939980069925,
      0.47878865855332164,
      0.42885038606972303,
      0.47997092775286787,
      0.47537641259509633,
      0.43556219965129084,
      0.3633633278993105,
      0.5113035819878297,
      0.4861102714517991,
      0.38501472322046215,
      0.495903484456821,
      0.30019168591625545,
      0.0,
      0.42773892002574265,
      0.5097339341400969,
      0.5492364273447276,
      0.4519830217954859,
      0.5108233546160433,
      0.5343650492041891,
      0.42533935222344543,
      0.4467490688009852,
      0.39618942521792255,
      0.3829895603436728,
      0.41652228963287996,
      0.4654170330067182,
      0.4199563783718929,
      0.4747180846818566,
      0.4627632840447762,
      0.4108705365802323
    ],
    [
      0.3448755609278753,
      0.41482067250576593,
      0.34329943906180005,
      0.42117698202969867,
      0.4133979438885209,
      0.42038658260412776,
      0.42886547128835173,
      0.5314943564817809,
      0.4420602767182369,
      0.34207504452873017,
      0.42055427242269605,
      0.31991524610163213,
      0.3883665551941391,
      0.0,
      0.48630718907198656,
      0.39617966954778017,
      0.39010549371336833,
      0.39758124600048683,
      0.36963013413386325,
      0.45672151810478767,
      0.40724058230564797,
      0.43144499124275093,
      0.3655586732847713,
      0.4180925173740979,
      0.3826687959366284,
      0.4487894978052145,
      0.4283215656624504,
      0.36005099901339177,
      0.4011061665911224
    ],
    [
      0.39053107037819457,
      0.577718800387468,
      0.46721804501795083,
      0.5222395295886362,
      0.4953048772917392,
      0.5121983288298071,
      0.49100373443809153,
      0.5802205387292905,
      0.5233911741538082,
      0.4099813049989458,
      0.47373597577077664,
      0.36369349885233326,
      0.5191044469081396,
      0.5327934097821116,
      0.0,
      0.5061522634745119,
      0.4976899868429081,
      0.5113070755441629,
      0.489385252471249,
      0.5333107800755446,
      0.4505365237688643,
      0.439664519689857,
      0.42950866665205023,
      0.5244261349960646,
      0.43836689410953644,
      0.4882293817055945,
      0.4498733498728973,
      0.4657695599414875,
      0.453228433362038
    ],
    [
      0.3422373458108967,
      0.491612337861399,
      0.3891866889287334,
      0.44542900259201,
      0.4605218813005205,
      0.38424058597812394,
      0.32964110497596,
      0.3952787882535056,
      0.4428640704925271,
      0.3150075113889954,
      0.4854426854426952,
      0.28929378743678713,
      0.4604991134650971,
      0.3594392523872243,
      0.40428917062295966,
      0.0,
      0.42090572320087394,
      0.4573497805270015,
      0.44911716783666744,
      0.3914237541215282,
      0.40799442984517564,
      0.3179729078383151,
      0.3420574268196419,
      0.39152599354527884,
      0.39920983617523276,
      0.36225877503246817,
      0.3977164629272303,
      0.374143433189311,
      0.3383608831897156
    ],
    [
      0.3610864411783332,
      0.3695027864930591,
      0.35012299012970716,
      0.3357967684990759,
      0.4396578672572138,
      0.32993519328883414,
      0.2988253822725546,
      0.32735599308074526,
      0.4008493334979124,
      0.33862846764183074,
      0.3899560052505888,
      0.19916502544351622,
      0.36613648351768546,
      0.30624205617316624,
      0.35884849634916205,
      0.3867781770394163,
      0.0,
      0.3736087297445907,
      0.37559111918626376,
      0.2884524396825492,
      0.336336066770895,
      0.3098270251555859,
      0.29427237593178823,
      0.33386051952198814,
      0.3988546791316103,
      0.2870553813370347,
      0.39282713837922145,
      0.35782648965801944,
      0.27984488466186797
    ],
    [
      0.3832925422130815,
      0.5704450779269588,
      0.5067329639325566,
      0.550385321689574,
      0.615412700644588,
      0.5757497217940111,
      0.42288198160946755,
      0.5344782633017542,
      0.5624876165088095,
      0.42585333351250676,
      0.5986721969286353,
      0.3457717512686873,
      0.5845803357266812,
      0.5365359016574971,
      0.5868370146728676,
      0.63995866837258,
      0.503542418030201,
      0.0,
      0.5788402054279715,
      0.4940505798816992,
      0.5130872104341442,
      0.4237798979743099,
      0.4324895732004581,
      0.528741885986161,
      0.48885433216750473,
      0.4841414230235346,
      0.5451757369692469,
      0.5330726549938607,
      0.4168164493345792
    ],
    [
      0.4232680104238771,
      0.5664836280040524,
      0.5045882635522434,
      0.5473859583361229,
      0.5562441404154219,
      0.5475054230681795,
      0.43671702610142504,
      0.5726783082376368,
      0.5236701916086035,
      0.4450906843924576,
      0.641833910515297,
      0.3121296633979369,
      0.5852183289297963,
      0.5182215648374384,
      0.603773711511046,
      0.611330824195019,
      0.5466713735934972,
      0.5690395103438937,
      0.0,
      0.5101405824871268,
      0.5325707524640906,
      0.43150430680073426,
      0.47247021992403027,
      0.5111224475612257,
      0.5028878910903489,
      0.4874478135561846,
      0.5983723802455552,
      0.5068209319784622,
      0.397246605856473
    ],
    [
      0.37893671887351865,
      0.536360745848234,
      0.47669724887840537,
      0.5309789263657145,
      0.4498942302092861,
      0.455969712467694,
      0.525371304117696,
      0.5388985397897168,
      0.477075084784665,
      0.37357651853254503,
      0.4115106143503924,
      0.33205111459517633,
      0.4226212781156191,
      0.48755541691904547,
      0.5187212852204883,
      0.4407030731009365,
      0.42544927507987507,
      0.40505611139767494,
      0.41345933522050293,
      0.0,
      0.3800055050199769,
      0.45051745147075306,
      0.400946741378003,
      0.4812568161363566,
      0.468425524402023,
      0.4642749055636437,
      0.43484654379043386,
      0.39199915123834694,
      0.4018519605333033
    ],
    [
      0.3744171414983233,
      0.4435950112894942,
      0.41189296493622574,
      0.44797307950806275,
      0.4705221750880375,
      0.3777787846127447,
      0.3776738176878949,
      0.4462865548792134,
      0.4773311618673419,
      0.37235459268610316,
      0.49034343516634804,
      0.27107150986688944,
      0.45666245258984084,
      0.4009522094658611,
      0.47054022856997046,
      0.47789954318420325,
      0.4792699333003638,
      0.5115827425206199,
      0.5132885370849414,
      0.374066901536799,
      0.0,
      0.3787432993030606,
      0.3939617255253274,
      0.4488400363683096,
      0.4325475976219846,
      0.48278479482539005,
      0.46581577572801924,
      0.4434244592922343,
      0.36988850810262575
    ],
    [
      0.3579955490730993,
      0.3698777836547298,
      0.34879955218026537,
      0.3807829807361338,
      0.3646128210295325,
      0.3699278351172426,
      0.41046413505986346,
      0.44798381721651404,
      0.4070080631131179,
      0.34079550814834514,
      0.3415668377680594,
      0.32343295111434545,
      0.36645904940581975,
      0.4003181985262201,
      0.40049511487866196,
      0.37106570708191033,
      0.37251097662688637,
      0.3529563773301776,
      0.3481940351524877,
      0.4189076091844799,
      0.3961685763916478,
      0.0,
      0.36317994513089524,
      0.36583122636686016,
      0.3926960414435585,
      0.4490169601520011,
      0.33574604237054495,
      0.35074307894832435,
      0.40851128793036207
    ],
    [
      0.43743947936054717,
      0.4968801164094092,
      0.5231113127271569,
      0.4907013849124473,
      0.5340452352893774,
      0.45533202039180276,
      0.44385770420762016,
      0.4252847090206222,
      0.4912748683322279,
      0.45959810206758145,
      0.5020229110251517,
      0.32702360502064587,
      0.5018350766264068,
      0.45124617706681613,
      0.5026397149935724,
      0.5177076153150606,
      0.5318314434532103,
      0.4958430528057136,
      0.5134648489725981,
      0.44426061156549523,
      0.483476980469403,
      0.41988172948954205,
      0.0,
      0.4564547082428827,
      0.49223851521682493,
      0.4827981266572603,
      0.5618829980217508,
      0.46867582546852926,
      0.4460722063816849
    ],
    [
      0.3463190463000152,
      0.48520377249174795,
      0.40952022743836025,
      0.49342480752526496,
      0.40668358876918953,
      0.40382023782911936,
      0.4173892105282764,
      0.43988037044041817,
      0.48440229310860206,
      0.37080330605072787,
      0.40865914560702055,
      0.29429594628843936,
      0.39003928872210847,
      0.44150616074855686,
      0.48350779358731466,
      0.42435212393299504,
      0.4229110043724005,
      0.39554722767341133,
      0.3928632702128858,
      0.4372275611623113,
      0.4047409694607018,
      0.41028135115980424,
      0.36162742686063787,
      0.0,
      0.37343918348473437,
      0.4225830555813379,
      0.41036208749635517,
      0.42430020790775225,
      0.37408247184041876
    ],
    [
      0.36621061860464743,
      0.41682235135889956,
      0.4378614104051428,
      0.4551694668356938,
      0.488185837351367,
      0.40724332116734274,
      0.3753942095716212,
      0.4243611732568038,
      0.42624920074522654,
      0.33446782678007647,
      0.4062907606906938,
      0.2575199839262432,
      0.4453891550413007,
      0.38043098513704043,
      0.4062543892900028,
      0.4706879442154972,
      0.5123860158126881,
      0.4191017062903717,
      0.43212916205582896,
      0.40832702797439624,
      0.3902005771020567,
      0.34437570155530817,
      0.39457260322607546,
      0.3740188503102333,
      0.0,
      0.41326404771085556,
      0.43904728021338313,
      0.4080568764570529,
      0.36029007965476323
    ],
    [
      0.4041139451266309,
      0.5160554258195995,
      0.44659054052159863,
      0.49266578561903973,
      0.46133873197038544,
      0.4453659594546324,
      0.4747992162180299,
      0.5152432178019462,
      0.5238992812081051,
      0.39328952808616857,
      0.44912427905126684,
      0.34372191042186695,
      0.48705523684605034,
      0.4680238187064203,
      0.5235507052026895,
      0.47004730838217545,
      0.4634423074599152,
      0.4593832890530576,
      0.4617715796136155,
      0.45778411392342555,
      0.471512251626957,
      0.4094150485656607,
      0.45151460511618846,
      0.5311097840889729,
      0.4662062236057183,
      0.0,
      0.452533423067097,
      0.45245967613622184,
      0.4429545212390833
    ],
    [
      0.49213362682673534,
      0.5313228770455241,
      0.4789711553120799,
      0.5184452532346024,
      0.5490158170900437,
      0.4477446885780063,
      0.39362516992875896,
      0.5296566842619523,
      0.47304496783714667,
      0.48363304187149914,
      0.5312073565809534,
      0.3462114883472238,
      0.49046153797781056,
      0.4732284459735876,
      0.49267196311017125,
      0.5042184957299898,
      0.5703946710479051,
      0.5215382692498234,
      0.5427921052065927,
      0.5212684834881001,
      0.4648872244109927,
      0.40866571489320336,
      0.49097674934954716,
      0.47027442858591995,
      0.52745577343262,
      0.46701065737344116,
      0.0,
      0.5161628092017754,
      0.3969285826717939
    ],
    [
      0.3829965379295073,
      0.38658158238911944,
      0.38795512296877166,
      0.40260281650075225,
      0.3981162467055124,
      0.3568227834251003,
      0.2990968718828029,
      0.36943069981683774,
      0.37792750342776715,
      0.3162797236092576,
      0.3852134360666153,
      0.25404883027726255,
      0.4074768604455905,
      0.3307986366458555,
      0.36759361264052104,
      0.41075808510996104,
      0.3542465007576452,
      0.38670080237182836,
      0.3647007854033546,
      0.37327613285411876,
      0.3203903050301864,
      0.2986184897552664,
      0.2735339217692443,
      0.41467612038623236,
      0.3751621792716162,
      0.3194118897943954,
      0.4092427736053592,
      0.0,
      0.2843359614089638
    ],
    [
      0.339714616228844,
      0.433734773258565,
      0.4735725656463796,
      0.44222957947775976,
      0.4916396836693875,
      0.4774275546640694,
      0.42854978542538125,
      0.4756555450522635,
      0.49354987284345464,
      0.4538368180767587,
      0.4530404580410292,
      0.3635327050973811,
      0.4683759814413735,
      0.4355936873588173,
      0.48802010879674507,
      0.48707713311621514,
      0.4803637906826477,
      0.4785868645197877,
      0.46563470335125623,
      0.4147608165526897,
      0.4566895106870734,
      0.464299993471029,
      0.43902258236912517,
      0.4369483907608054,
      0.4967791115634408,
      0.4409803353878641,
      0.4683351396217681,
      0.45362037547236,
      0.0
    ]
  ],
  "row_avgs": [
    0.041897454980596875,
    0.4455983945805028,
    0.42739123391179085,
    0.44445323857108326,
    0.4449714526954717,
    0.4769160941190457,
    0.4220068654973817,
    0.4473486041588263,
    0.42676661571637586,
    0.40717435968528903,
    0.44354490266698215,
    0.32727392193879934,
    0.44606824212093377,
    0.4061102658407752,
    0.483449412772645,
    0.39446499647092415,
    0.3424015827240792,
    0.5136667056851403,
    0.5165155161938635,
    0.44553611190714376,
    0.43076817764665115,
    0.37700171646900305,
    0.47703146712540506,
    0.41177761202074675,
    0.4069395915264505,
    0.46196327549759,
    0.4869267156649215,
    0.35742840043748014,
    0.45362758866550973
  ],
  "col_avgs": [
    0.37772495012408813,
    0.4573247052450085,
    0.4169833052668224,
    0.4522537323613502,
    0.4494095526465429,
    0.4221860421216656,
    0.3936559481558956,
    0.45828767392746084,
    0.45825377716947235,
    0.3671191231508265,
    0.4400717193331955,
    0.29825128900821796,
    0.4387358505116548,
    0.4201849836941037,
    0.451980497004428,
    0.4585611720986696,
    0.44661705841805627,
    0.43979529304390036,
    0.4348349443466747,
    0.43025998006463306,
    0.4065830460340879,
    0.3835446474332856,
    0.37289217285308845,
    0.423088612220901,
    0.42613281276692294,
    0.4147954143000492,
    0.4392351854829456,
    0.41699559341518083,
    0.37126143509227955
  ],
  "combined_avgs": [
    0.2098112025523425,
    0.45146154991275567,
    0.42218726958930664,
    0.44835348546621673,
    0.44719050267100735,
    0.4495510681203556,
    0.40783140682663865,
    0.4528181390431436,
    0.4425101964429241,
    0.38714674141805777,
    0.4418083110000888,
    0.31276260547350865,
    0.4424020463162943,
    0.41314762476743944,
    0.46771495488853654,
    0.42651308428479684,
    0.39450932057106775,
    0.47673099936452035,
    0.4756752302702691,
    0.43789804598588844,
    0.4186756118403695,
    0.38027318195114435,
    0.42496181998924676,
    0.4174331121208239,
    0.41653620214668674,
    0.4383793448988196,
    0.46308095057393356,
    0.38721199692633046,
    0.41244451187889464
  ],
  "gppm": [
    581.6050962201067,
    687.9064038996366,
    707.2293911339773,
    692.3742634110467,
    689.7731255401841,
    705.7724987815697,
    715.9927305019968,
    686.5637946294419,
    686.9084848030719,
    730.037654301668,
    698.685916031078,
    762.14309943639,
    702.6272886825376,
    706.3169493270277,
    691.9816909714307,
    690.8027233506749,
    690.8284147224185,
    696.1789227531309,
    702.0955080403826,
    701.0497884107482,
    710.6501542851162,
    716.3012646123481,
    729.917039264286,
    702.3990896562378,
    703.5136423744519,
    708.636411695441,
    698.5206085057736,
    701.6053396673599,
    727.692612551773
  ],
  "gppm_normalized": [
    1.3491988254100191,
    1.400321149805514,
    1.440974274248988,
    1.4072600851359973,
    1.401841005298169,
    1.4360443599204358,
    1.460776585450191,
    1.3938263026871536,
    1.3920483491299156,
    1.4927808303205745,
    1.421272342157487,
    1.565752863508002,
    1.4330393525611693,
    1.4406031104204864,
    1.4036606772174578,
    1.4075799927965338,
    1.4024618642495337,
    1.4147191369607641,
    1.429818024334999,
    1.420367615951553,
    1.443065099394998,
    1.45397396321161,
    1.4914753090558601,
    1.4206852930869363,
    1.4281275576513315,
    1.4456909087825667,
    1.4200178562221697,
    1.4367383629599375,
    1.48124323674338
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355
  ],
  "response_lengths": [
    10756,
    2579,
    2319,
    2462,
    2413,
    2393,
    2455,
    2495,
    2616,
    2304,
    2438,
    2253,
    2243,
    2668,
    2259,
    2497,
    2609,
    2205,
    2263,
    2494,
    2168,
    2829,
    1995,
    2605,
    2424,
    2090,
    1963,
    2751,
    2037
  ]
}