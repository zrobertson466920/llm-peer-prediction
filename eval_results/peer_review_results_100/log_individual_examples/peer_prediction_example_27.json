{
  "example_idx": 27,
  "reference": "Under review as a conference paper at ICLR 2023\n\nENHANCED TEMPORAL KNOWLEDGE EMBEDDINGS WITH CONTEXTUALIZED LANGUAGE REPRESENTATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWorld knowledge exists in both structured (tables, knowledge graphs) and unstructured forms (texts). Recently, there have been extensive research efforts in the integration of structured factual knowledge and unstructured textual knowledge. However, most studies focus on incorporating static factual knowledge into pre-trained language models, while there is less work on enhancing temporal knowledge graph embedding using textual knowledge. Existing integration approaches can not apply to temporal knowledge graphs (tKGs) since they often assume knowledge embedding is time-invariant. In fact, the entity embedding in tKG embedding models usually evolves over time, which poses the challenge of aligning temporally relevant textual information with entities. To this end, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which uses tKG quadruple as an implicit measure to temporally align textual data and the time-evolving entity representations and uses a novel knowledge-text prediction task to inject textual information into temporal knowledge embedding. ECOLA jointly optimizes the knowledge-text prediction objective and the temporal knowledge embedding objective, and thus, can simultaneously take full advantage of textual and structured knowledge. Since existing datasets do not provide tKGs with aligned textual data, we introduce three new datasets for training and evaluating ECOLA. Experimental results on the temporal knowledge graph completion task show that ECOLA outperforms state-of-the-art tKG embedding models by a large margin.\n\n1\n\nINTRODUCTION\n\nKnowledge graphs (KGs) have long been considered an effective and efficient way to store structural knowledge about the world. A knowledge graph consists of a collection of triples ps, p, oq, where s (subject entity) and o (object entity) correspond to nodes and p (predicate) indicates the edge type (relation) between the two entities. Common knowledge graphs (Toutanova et al., 2015; Dettmers et al., 2018) assume that the relations between entities are static connections. However, in the real world, there are not only static facts and properties but also time-evolving relations associated with the entities. For example, the political relationship between two countries might worsen because of trade fights. To this end, temporal knowledge graphs (tKGs) (Tresp et al., 2015) were introduced that capture temporal aspects of relations by extending a triple to a quadruple, which adds a timestamp or time interval to describe when the relation is valid, e.g. (Argentina, deep comprehensive strategic partnership with, China, 2022). Extensive studies have been focusing on learning temporal knowledge embedding (Leblay & Chekol, 2018; Han et al., 2020c), which not only helps infer missing links in tKGs but also benefits various knowledge-related downstream applications, such as temporal question answering (Saxena et al., 2021b).\n\nHowever, knowledge graph embedding often suffers from the sparseness of knowledge graphs. For example, the tKG model proposed by Han et al. (2020a) performs much better on the dense tKG than the sparse one. To address this problem, some recent studies incorporate textual information to enrich knowledge embedding. KEPLER (Wang et al., 2021) learns the representation of an entity by encoding the entity description with a pre-trained language model (PLM) and optimizing the knowledge embedding objective. KG-Bert (Yao et al., 2019) takes entity and relation descriptions of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An example of a temporal knowledge graph with textual event descriptions.\n\na triple as the input of a PLM and turns knowledge graph completion into a sequence classification problem. However, they do not take the temporal nature and the evolutionary dynamics of knowledge graphs into account. In tKG embedding models, the entity representations usually evolve over time as they involve in different events at different timestamps. Taking financial crises as an example, companies are more likely involved in events such as laying off employees. But when the economy recovers, companies hire staff again rather than cut jobs. Thus, the entities should also be able to drift their representations over time to manage the changes. Therefore, given an entity, it should be taken into account which textual knowledge is relevant to it at which timestamp. We name this challenge as temporal alignment between texts and tKG, which is to establish a correspondence between textual knowledge and their temporal knowledge graph depiction. This is one of the challenges that existing approaches cannot handle due to their limitation of assuming knowledge embedding is static and using the time-invariant description of an entity to enhance its representation. Thus, they are not appropriate in temporal knowledge graph scenarios where temporal alignment is required. The other challenge is that temporal knowledge embedding models learn the entity representations as a function of time, which exposes another limitation of existing approaches that their architectures cannot be naturally combined with tKG models. Therefore, it is not clear how to enhance temporal knowledge embedding with textual data.\n\nTo this end, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which uses temporally relevant textual knowledge to enhance the timedependent knowledge graph embedding and ensures that the enhanced knowledge embedding preserves the temporal nature. Specifically, we solve the temporal alignment challenge by using tKG quadruples as an implicit measure. We pair a quadruple with its relevant textual data, e.g., event descriptions, which corresponds to the temporal relations between entities at a specific time. Then we use the event description to enhance the representations of entities and predicate involved in the given quadruple. In particular, we encode entities and predicates by tKG embedding models and encode texts using token embedding . Given a quadruple-text pair, we concatenate the embedding of entities, predicate, and textual tokens and feed them into a pre-trained language model. We introduce a novel knowledge-text prediction (KTP) task to inject textual knowledge into temporal knowledge embedding. The KTP task is an extended masked language modeling task, which randomly masks words in texts and entity/predicates in quadruples. With the help of the KTP task, ECOLA would be able to recognize mentions of the subject entity and the object entity and align semantic relationships in the text with the predicate in the quadruple. Thus, the model can take full advantage of the abundant information from the textual data, which is especially helpful for embedding entities and predicates that only appear in a few quadruples. ECOLA jointly optimizes the knowledge-text prediction and temporal knowledge embedding objectives. Since our goal is to develop an approach that can generally improve any potential tKG models, we combine the model with different benchmark tKG embedding models (Goel et al., 2020; Han et al., 2020c; 2021). For training ECOLA, we need datasets with temporal KG quadruples and aligned textual event descriptions, which is unavailable in existing temporal KG benchmark datasets. Thus, we construct three new temporal knowledge graph datasets by adapting two existing datasets, i.e., GDELT (Leetaru & Schrodt, 2013) and Wiki (Dasgupta et al., 2018), and an event extraction dataset (Li et al., 2020). To make a fair comparison with other temporal KG embedding models and keep fast inference, we only take the enhanced\n\n2\n\nMetaAyfer OzgurTikTokInstagramRelease 2021-08-19Sue 2019-6-12 Acquire 2012-04-09 Give awards to 2021-08-18The Meta Foundation launched a research award, which was officially announced on Aug. 18. Ayfer Ozgur is one of the winners. Today is October 24. I didn't expect Meta to acquire Instagram, a fun, popular photo-sharing app for  mobile devices.On Aug 19, the documentary film titled “The Outsider\" ... is officially released by Meta. On June 12, it was reported that Meta added six new lawsuits against TikTok in Court... The OutsiderUnder review as a conference paper at ICLR 2023\n\ntemporal knowledge embedding to perform the temporal KG completion task at test time but do not use any textual descriptions of test quadruples.\n\nTo summarize, our contributions are as follows: (i) We propose ECOLA that enhances temporal knowledge graph representation models with textual knowledge via pre-trained language models. ECOLA shows its superiority on the temporal KG completion task and can be potentially combined with any temporal KG embedding model. (ii) We are the first to address the challenge of enhancing temporal knowledge embedding with temporally relevant textual information while preserving the time-evolving properties of entity embedding. (iii) To train the integration models, we construct three datasets, which align each quadruple with a relevant textual description, by adapting three existing temporal KG completion datasets. Extensive experiments show that ECOLA is model-agnostic and enhances temporal KG embedding models with up to 287% relative improvements in the Hits@1 metric.\n\n2 PRELIMINARIES AND RELATED WORK\n\nTemporal Knowledge Graphs Temporal knowledge graphs are multi-relational, directed graphs with labeled timestamped edges between entities (nodes). Let E and P represent a finite set of entities and predicates, respectively. tKG contains a collection of events written as quadruples. A quadruple q “ pes, p, eo, tq represents a timestamped and labeled edge between a subject entity es P E and an object entity eo P E at a timestamp t P T . Let F represents the set of all true quadruples, i.e., real events in the world, the temporal knowledge graph completion (tKGC) is the task of inferring F based on a set of observed facts O, which is a subset of F. Specifically, tKGC is to predict either a missing subject entity p?, p, eo, tq given the other three components or a missing object entity pes, p, ?, tq. We provide related works on temporal knowledge representations in Appendix A.\n\nJoint Language and Knowledge Models Recent studies have achieved great success in jointly learning language and knowledge representations. Yamada et al. (2016) and Ganea & Hofmann (2017) use entity linking to map entities and words into the same representation space. Inspired by the success of contextualized language representation, Zhang et al. (2019) and Peters et al. (2019) focus on enhancing language models with external knowledge. They separately pre-train the entity embedding with knowledge embedding models, e.g., TransE Bordes et al. (2013), and inject the pre-trained entity embedding into PLMs, while fixing the entity embedding during training PLMs. Thus, they are not real joint models for learning the knowledge embedding and language embedding simultaneously. Yao et al. (2019), Kim et al. (2020), and Wang et al. (2021) learn to generate entity embeddings with pre-trained language models (PLMs) from entity descriptions. Moreover, He et al. (2019), Sun et al. (2020), and Liu et al. (2020) exploit the potential of contextualized knowledge representation by constructing subgraphs of structured knowledge and textual data instead of treating single triples as training units. Nevertheless, none of these works consider the temporal aspect of knowledge graphs, which makes them different from our proposed ECOLA.\n\n3 ECOLA\n\nIn this section, we present the overall framework of ECOLA, including the model architecture in Section 3.1 - 3.3, a novel training task designed for aligning knowledge embedding and language representation in Section 3.4, and the training procedure in Section 3.5. As shown in Figure 2, ECOLA implicitly incorporates contextualized language representations into temporal knowledge embeddings by jointly optimizing the knowledge-text prediction loss and the temporal knowledge embedding loss. Note that, at inference time, we only take the enhanced temporal knowledge embeddings to perform the temporal KG completion task without using any textual data for preventing information leakage and keep fast inference speed.\n\n3.1 EMBEDDING LAYER\n\nIn tKG embedding models, entity representations evolve over time. Thus, the key point of enhancing a time-dependent entity representation eiptq is to find texts that are relevant to the entity at the time of interest t. To this end, we use tKG quadruples (e.g., pei, p, ej, tq) as an implicit measure for the alignment. We pair a quadruple with its relevant textual data and use such textual data to enhance\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Model architecture. ECOLA jointly optimizes the knowledge-text prediction (KTP) objective and the temporal knowledge embedding (tKE) objective.\n\nFigure 3: ECOLA input representations. Following textual tokens, the last four tokens in an input correspond to a quadruple from temporal knowledge graph. The last one is the timestamp t for incorporating temporal information into entity representation eiptq. Here, w denotes subword token embedding, e and p denote entity and predicate embedding, respectively.\n\nthe entity representation eiptq. Therefore, a training sample is a pair of a quadruple from temporal KGs and its corresponding textual description, which are packed together into a sequence. As shown in Figure 3, the input embeddings are the sum of token embedding, type embedding, and position embedding. For token embedding, we maintain three lookup tables for subwords, entities, and predicates, respectively. For subword embedding, we first tokenize the textual description into a sequence of subwords following Bert (Devlin et al., 2018) and use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. As the yellow tokens shown in the Figure 3, We denote a embedding sequence of subword tokens as tw1, ..., wnu. In contrast to subword embedding, the embeddings for entities and predicates are directly learned from scratch, similar to common knowledge embedding methods. We denote the entity embedding and predicate embedding as e and p, respectively, as the blue tokens shown in Figure 3. We separate the knowledge tokens, i.e., entities and predicates, and subword tokens with a special token [SEP]. To handle different token types, we add type embedding to indicate the type of each token, i.e., subword, entity, and predicate. For position embedding, we assign each token an index according to its position in the input sequence and follow Devlin et al. (2018) to apply fully-learnable absolute position embeddings.\n\n3.2 TEMPORAL KNOWLEDGE ENCODER\n\nAs shown in Figure 3, the input embedding for entities and predicates consists of knowledge token embedding, type embedding, and position embedding. In this section, we provide details of the temporal knowledge embedding objective.\n\nA temporal embedding function defines entity embedding as a function that takes an entity and a timestamp as input and generates a time-dependent representation. There is a line of work exploring temporal embedding functions. Since we aim to propose a model-agnostic approach, we combine ECOLA with three temporal embedding functions, i.e., DyERNIE-Euclid (Han et al., 2020c), UTEE\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(Han et al., 2021), and DE-SimplE (Goel et al., 2020). In the following, we refer to DyERNIEEuclid as DyERNIE and take it as an example to introduce our framework. Specifically, the entity representation is derived from an initial embedding and a velocity vector\n\neDyER\n\ni\n\nptq “ ̄eDyER\n\ni\n\n` veit,\n\ni\n\nwhere ̄eDyER represents the initial embedding that does not change over time, and vei is an entityspecific velocity vector. The combination with other temporal embedding functions are discussed in Section 4. The score function measuring the plausibility of a quadruple is defined as follows,\n\nφDyERpei, p, ej, tq “ ́dpP d eDyER\n\ni\n\nptq, eDyER\n\nj\n\nptq ` pq ` bi ` bj,\n\n(1)\n\nwhere P and p represent the diagonal predicate matrix and the translation vector of predicate p, respectively, d denotes the Euclidean distance, and bi, bj are scalar biases of the subject and object ei and ej respectively. By learning tKE, we generate M negative samples for each positive quadruple in a batch. We choose the binary cross entropy as the temporal knowledge embedding objective\n\nLtKE “\n\n ́1 N\n\nNÿ\n\npyk logppkq ` p1 ́ ykq logp1 ́ pkqq,\n\n(2)\n\nk“1\n\nwhere N is the sum of positive and negative training samples, yk represents the binary label indicating whether a training sample is positive or not, pk denotes the predicted probability σpφDyER q, and σp ̈q represents the sigmoid function.\n\nk\n\n3.3 MASKED TRANSFORMER ENCODER\n\nTo encode the input sequence, we use the pre-trained contextual language representation model Bert (Devlin et al., 2018). Specifically, the encoder feeds a sequence of N tokens including entities, predicates, and subwords into the embedding layer introduced in Section 3.1 to get the input embeddings and then computes L layers of d-dimensional contextualized representations. Eventually, we get a contextualized representation for each token, which could be further used to predict masked tokens.\n\n3.4 KNOWLEDGE-TEXT PREDICTION TASK\n\nTo incorporate textual knowledge into temporal knowledge embedding, we use the pre-trained language model Bert to encode the textual description and propose a knowledge-text prediction task to align the language representations and the knowledge embedding. The knowledge-text prediction task is an extension of the masked language modeling (MLM) task. As illustrated in Figure 2, given a pair of a quadruple and the corresponding event description, the knowledge-text prediction task is to randomly mask some of the input tokens and train the model to predict the original index of the masked tokens based on their contexts. As different types of tokens are masked, we encourage ECOLA to learn different capabilities:\n\n• Masking entities. To predict an entity token in the quadruple, ECOLA has the following ways to gather information. First, the model can detect the textual mention of this entity token and determine the entity; second, if the other entity token and the predicate token are not masked, the model can utilize the available knowledge token to make a prediction, which is similar to the traditional semantic matching-based temporal KG models. Masking entity nodes helps ECOLA align the representation spaces of language and structured knowledge, and inject contextualized representations into entity embeddings.\n\n• Masking predicates. To predict the predicate token in the quadruple, the model needs to detect mentions of subject entity and object entity and classify the semantic relationship between the two entity mentions. Thus, masking predicate tokens helps the model integrate language representation into the predicate embedding and map words and entities into a common representation space.\n\n• Masking subwords. When subwords are masked, the objective is similar to traditional MLM. The difference is that ECOLA not only considers the dependency information in the text but also the entities and the logical relationship in the quadruple. Additionally, we initialize the encoder with the pre-trained BERTbase. Thus, masking subwords helps ECOLA keep linguistic knowledge and avoid catastrophic forgetting while integrating contextualized representations into temporal knowledge embeddings.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nIn each quadruple, the predicate and each entity have a probability of 15% to be masked. Similarly, we mask 15% of the subwords of the textual description at random. We ensure that entities and the predicate cannot be masked at the same time in a single training sample, where we conduct an ablation study in Section 6 to show the improvement of making this constraint. When a token is masked, we replace it with (1) the [MASK] token 80% of the time, (2) a randomly sampled token with the same type as the original token 10% of the time, (3) the unchanged token 10% of the time. For each masked token, the contextualized representation in the last layer of the encoder is used for three classification heads, which are responsible for predicting entities, predicates, and subword tokens, respectively. At last, a cross-entropy loss LKT P is calculated over these masked tokens.\n\nAlthough we focus on generating informative knowledge embeddings in this work, joint models often benefit both the language model and the temporal KG model. Unlike previous joint models (Zhang et al., 2019; Peters et al., 2019), we do not modify the Transformer encoder architecture, e.g., adding entity linkers or fusion layers. Thus, the language encoder enhanced by external knowledge can be adapted to a wide range of downstream tasks as easily as Bert. We evaluate the enhanced language model on the temporal question answering task and report the results in Appendix C.\n\n3.5 TRAINING PROCEDURE AND INFERENCE\n\n1 and the knowlWe initialize the transformer encoder with the pre-trained language model BERTbase edge encoder with random vectors. Then we use the temporal knowledge embedding (tKE) objective LtKE to train the knowledge encoder and use the knowledge-text prediction (KTP) objective LKT P to incorporate temporal factual knowledge and textual knowledge in the form of a multi-task loss:\n\nL “ LtKE ` λLKT P ,\n\nwhere λ is a hyperparameter to balance tKE loss and KTP loss. Note that those two tasks share the same embedding layer of entities and predicates. At inference time, we aim to answer link prediction queries, e.g., pes, p, ?, tq. Since there is no textual description at inference time, we take the entity and predicate embedding as input and use the score function of the knowledge encoder, e.g., Equation 1, to predict the missing links. Specifically, the score function assigns a plausibility score to each quadruple, and the proper object can be inferred by ranking the scores of all quadruples tpes, p, ej, tq, ej P Eu that are accompanied with candidate entities.\n\n4 MODEL VARIANTS\n\nECOLA is model-agnostic and can enhance different temporal knowledge embedding models. Besides ECOLA-DyERNIE, we introduce here two additional variants of ECOLA.\n\nECOLA-DE enhances the tKG embedding model DE-SimplE, which applies the diachronic embedding (DE) function (Goel et al., 2020). DE-function defines the temporal embeddings of entity ei at timestamp t as\n\n\"\n\neDE\n\ni\n\nptqrns “\n\naeirns if 1 ď n ď γd, aeirns sinpωeirnst ` bei rnsq else.\n\n(3)\n\ni\n\nptqrns denotes the nth element of the embeddings of entity ei at time t. aei, ωei, bei P Rd Here, eDE are entity-specific vectors with learnable parameters, d is the dimensionality of the embedding, and γ P r0, 1s represents the portions of the time-independent part. Besides, it use SimplE (Kazemi & Poole, 2018) as the score function of temporal knowledge embedding.\n\nECOLA-UTEE enhances UTEE Han et al. (2021) that learns a shared temporal encoding function for all entities to deal with the overfitting problem of the diachronic approach (Goel et al., 2020) on sparse datasets. Compared to ECOLA-DE, ECOLA-UTEE replaces Equation 3 with follows:\n\neU T EE\n\ni\n\nptq “ r ̄ei||a sinpωt ` bqs,\n\n ̄ei P Rγd; a, w, b P Rp1 ́γqd\n\nwhere ̄ei denotes entity-specific time-invariant part, the amplitude vector a, frequency vector ω, and bias b are shared among all entities, || denotes the concatenation operator, and γ P r0, 1s.\n\n1https://huggingface.co/bert-base-uncased\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n5 DATASETS\n\nThe training procedure of ECOLA requires both temporal knowledge graphs and textual descriptions. Given a quadruple pes, p, eo, tq, the key point is to find texts that are temporally relevant to es and eo at t. Existing temporal KG datasets do not provide such information. To facilitate the research on integrating textual knowledge into temporal knowledge embeddings, we reformat three existing datasets, i.e., GDELT2, DuEE3, and Wiki4, for evaluating the proposed integration method. We show the statistics of the datasets in Table 2 in the appendix. Due to the limited size of upload files, we only attach DuEE and subsets of GDELT and Wiki in the supplementary material and will publish the complete version after acceptance.\n\nGDELT is an initiative knowledge base storing events across the globe connecting people and organizations, e.g., (Google, consult, the United States, 2018/01/06). For each quadruple, GDELT provides the link to the news resource which the quadruple is extracted from. We assume each sentence that contains both mentions of subject entity and object entity is relevant to the given quadruple, and thus, temporally aligned with the subject and object at the given timestamp. We pair each of these sentences with the given quadruple to form a training sample. This process is similar to the distant supervision algorithm Mintz et al. (2009) in the relation extraction task, which assumes that, given a relationship between two entities, any sentence containing these two entities would express this relation. In total, the dataset contains 5849 entities, 237 predicates, 2403 timestamps, and 943956 quadruples with accompanying sentences.\n\nDuEE is originally a human-annotated dataset for event extraction containing 65 event types and 121 argument roles. Each sample contains a sentence and several extracted event tuples. We reformat DuEE by manually converting event tuples into quadruples and then pairing the quadruples with their corresponding sentence.\n\nWiki is a temporal knowledge graph dataset proposed by Leblay & Chekol (2018), containing temporal facts from the Wikidata (Vrandeˇci ́c & Krötzsch, 2014). Different from GDELT and DuEE, time annotations in Wiki are represented as time intervals, e.g., (Savonranta, instance of, municipality of Finland, 1882 - 2009). Following the post-processing by Dasgupta et al. (2018), we discretize the time span into 82 different timestamps. We align each entity to its Wikipedia page and extract the first abstract section as its description. To construct the relevant textual data of each quadruple, we combine the subject entity description, relation, and object description into a sequence, separated by [SEP] token between two neighboring parts. In this case, the knowledge-text prediction task let subject entity see the descriptions of its neighbors at different timestamps, and thus, preserving the temporal alignment between time-dependent entity representation and textual knowledge.\n\n6 EXPERIMENTS\n\nWe evaluate the enhanced temporal knowledge embedding on the temporal KG completion task. Specifically, we take the entity and predicate embedding of ECOLA-DyERNIE and use Equation 1 to predict missing links. To make a fair comparison with other temporal KG embedding models, we do not use any textual descriptions at test time.\n\nBaselines We include both static and temporal KG embedding models. From the static KG embedding models, we use TransE (Bordes et al., 2013), DistMult (Yang et al., 2014), and SimplE (Kazemi & Poole, 2018). These methods ignore the time information. From the temporal KG embedding models, we compare our model with several state-of-the-art methods, including AiTSEE (Xu et al., 2019), TNTComplE(Lacroix et al., 2020), DyERNIE5 (Han et al., 2020c), TeRO (Xu et al., 2020), and DE-SimplE (Goel et al., 2020). We provide implementation details in Appendix D and attach the source code in the supplementary material.\n\n2https://www.gdeltproject.org/data.html#googlebigquery 3https://ai.baidu.com/broad/download 4https://www.wikidata.org/wiki/Wikidata:Main_Page 5For a fair comparison with other baselines, we choose DyERNIE-Euclid.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nEvaluation protocol For each quadruple q “ pes, p, eo, tq in the test set Gtest, we create two queries: pes, p, ?, tq and p?, p, eo, tq. For each query, the model ranks all possible entities E according to their scores. Following the filtered setting in Han et al. (2020b), we remove all entity candidates that correspond to true triples from the candidate list apart from the current test entity. Let Rankpesq and Rankpeoq represent the rank for es and eo of the two queries respectively, we evaluate our models using standard metrics across the link prediction literature: mean reciprocal rank (MRR): Rankpeoq q and Hits@kpk P t1, 3, 10uq: the percentage of times that the\n\n1 2 ̈|Gtest| true entity candidate appears in the top k of ranked candidates.\n\nRankpesq `\n\nqPGtestp\n\nř\n\n1\n\n1\n\nTable 1: Temporal link prediction results: Mean Reciprocal Rank (MRR, %) and Hits@1/3(%). The results of the proposed fusion models (in bold) and their counterpart KG models are listed together.\n\nDatasets\n\nModel\n\nTransE SimplE DistMult\n\nTeRO ATiSE TNTComplEx\n\nUTEE ECOLA-UTEE\n\nGDELT - filtered\n\nWiki - filtered\n\nDuEE - filtered\n\nMRR\n\n8.08 10.98 11.27\n\n6.59 7.00 8.93\n\nHits@1 Hits@3\n\n0.00 4.76 4.86\n\n1.75 2.48 3.60\n\n8.33 10.49 10.87\n\n5.86 6.26 8.52\n\nMRR\n\n27.25 20.75 21.40\n\n32.92 35.36 34.36\n\nHits@1 Hits@3\n\n16.09 16.77 17.54\n\n21.74 24.07 22.38\n\n33.06 23.23 23.86\n\n39.12 41.69 40.64\n\nMRR\n\n34.25 51.13 48.58\n\n54.29 53.79 57.56\n\nHits@1 Hits@3\n\n4.45 40.69 38.26\n\n39.27 42.31 43.52\n\n60.73 58.30 55.26\n\n63.16 59.92 65.99\n\n9.76\n\n4.23\n\n20.98 19.11 ̆ 15.29 ̆ 19.46 ̆ 38.35 ̆ 30.56 ̆ 42.11 ̆ 60.36 ̆ 46.55 ̆ 69.22 ̆ 00.18\n\n00.14\n\n30.39\n\n43.92\n\n00.22\n\n00.51\n\n26.96\n\n53.36\n\n00.93\n\n00.36\n\n60.52\n\n00.05\n\n00.38\n\n00.16\n\n9.77\n\nDyERNIE 23.51 ECOLA-DyERNIE 19.99 ̆ 16.40 ̆ 19.78 ̆ 41.22 ̆ 33.02 ̆ 45.00 ̆ 59.64 ̆ 46.35 ̆ 67.87 ̆ 00.04\n\n00.27\n\n25.21\n\n00.03\n\n00.18\n\n00.53\n\n57.58\n\n14.53\n\n00.20\n\n10.81\n\n00.06\n\n41.49\n\n10.72\n\n00.09\n\n00.05\n\n70.24\n\n4.24\n\nQuantitative Study Table 1 reports the tKG completion results on the test sets, which are averaged over three trials. The error bars of the ECOLA models are also provided. Firstly, we can see that ECOLA-UTEE improves its baseline temporal KG embedding model, UTEE, by a large margin, demonstrating the effectiveness of our fusing strategy. Specifically, ECOLA-UTEE enhances UTEE on GDELT with a relative improvement of 95% and 99% in terms of mean reciprocal rank (MRR) and Hits@3, even nearly four times better in terms of Hits@1. Thus, its superiority is clear on GDELT, which is the most challenging dataset among benchmark tKG datasets, containing nearly one million quadruples and more than two hundred relations. Secondly, ECOLA-UTEE and ECOLA-DE generally outperform UTEE and DE-SimplE on the three datasets, demonstrating that ECOLA is model-agnostic and able to enhance different tKG embedding models. Besides, in the DuEE dataset, ECOLA-DyERNIE achieves a better performance than DyERNIE in Hits@1 and MRR, but the gap reverses in Hits@3. The reason could be that ECOLA-DyERNIE is good at classifying hard negatives using textual knowledge, and thus has a high Hits@1; however, since DuEE is much smaller than the other two datasets, ECOLA-DyERNIE may overfit in some cases, where the ground truth is pushed away from the top 3 rank.\n\nWe compare the performance of DE-SimplE, ECOLA-DE, and ECOLA-SF on GDELT in Figure 4a. ECOLA-SF is the static counterpart of ECOLA-DE, where we do not consider the temporal alignment while incorporating textual knowledge. Specifically, ECOLA-SF integrates all textual knowledge into the time-invariant part of entity representations. We provide more details of ECOLA-SF in Appendix B. We can see ECOLA-DE significantly outperforms DE-SimplE and ECOLA-SF in terms of MRR and Hits@1. In particular, the performance gap between ECOLA-DE and ECOLA-SF is significant, demonstrating the temporal alignment between time-dependent entity representation and textual knowledge is more powerful than the static alignment.\n\nAblation study Figure 4b shows the results of different masking strategies on GDELT. The first strategy called Masking E+R+W which allows to simultaneously mask predicate, entity, and subword tokens in the same training sample. The second strategy is Masking E/R+W, where we mask 15% subword tokens in the language part, and either an entity or a predicate in the knowledge tuple. In other words, simultaneously masking an entity and a predicate in a training sample is not allowed. In the third strategy called Masking E/R/W, for each training sample, we choose to mask either subword\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure 4: Ablation Study. (a) Temporal alignment analysis. We compare De-SimplE, ECOLA-DE, and ECOLA-SF in terms of MRR(%) and Hits@1(%) on GDELT. (b) Masking strategy analysis. We compare ECOLA-DE with different masking strategies and show the results of MRR(%) and Hits@1(%) on GDELT. (c) Type embedding analysis. We compare ECOLA-DE with/without type embedding and show the results of MRR(%) and Hits@1/10(%) on GDELT.\n\ntokens, an entity, or the predicate. The experimental results show the advantage of the second masking strategy, indicating that remaining adequate information in the knowledge tuple helps the model to align the knowledge embedding and language representations. Figure 4c demonstrates the ablation study on the type embedding, which differentiates among subword tokens, entity, and predicate of the input. We can observe that removing type embedding leads to a considerable performance gap on GDELT, indicating that providing distinguishment between subwords, entities, and predicates helps the model to better understand different input components and different prediction tasks.\n\nQualitative Analysis To investigate why incorporating textual knowledge can improve the tKG embedding models’ performance, we study the test samples that have been correctly predicted by the fusion model ECOLA-DE but wrongly by the tKG model DE-SimplE. It is observed that language representations help overcome the incompleteness of the tKG by leveraging knowledge from augmented textual data. For example, there is a test quadruple (US, host a visit, ?, 19-11-14) with ground truth R.T. Erdo ̆gan. The training set contains a quite relevant quadruple, i.e., (Turkey, intend to negotiate with, US, 19-11-11). However, the given tKG does not contain information indicating that the entity R.T. Erdo ̆gan is a representative of Turkey. So it is difficult for the tKG model DE-SimplE to infer the correct answer from the above-mentioned quadruple. In ECOLA-DE, the augmented textual data do contain such information, e.g. \"The president of Turkey, R.T. Erdogan, inaugurated in Aug. 2014.\", which narrows the gap between R.T. Erdogan and Turkey. Thus, by integrating textual information into temporal knowledge embedding, the enhanced model can gain additional information which the knowledge base does not include. Another example is relevant to the entity Charles de Gaulle. To infer the test quadruple (Charles de Gaulle, citizenship of, ?, 1958) with ground truth French 5th Republic, it is noticed that in the training set of ECOLA-DE, we have quadruple (Charles de Gaulle, president of, French 4th Republic, 1957) with supporting textual data \"Charles de Gaulle was the last president of French 4th Republic, and French 5th Republic emerged from the collapse of the 4th Republic in 1958.\", which shows that the entity representation of Charles de Gaulle is enhanced by the evolving history of France and is temporally closer to French 5th Republic at the query timestamp 1958.\n\n7 CONCLUSION\n\nWe propose ECOLA to enhance temporal knowledge embedding using textual knowledge. Specifically, we enhance time-evolving entity representations with temporally relevant textual data by encoding the textual data using a pre-trained language model and introducing a novel knowledge-text prediction task to align the temporal knowledge and language representation into the same semantic space. Besides, we construct three datasets that contain paired structured temporal knowledge and unstructured textual descriptions, which can benefit future research on fusing temporal structured and unstructured knowledge. Extensive experiments show ECOLA is model-agnostic and can improve many temporal knowledge graph models by a large margin.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nReproducibility Statement About datasets, Since the size of GDELT and Wiki exceeds the limit of upload file size allowed by ICLR, we only upload DuEE, partial Wiki and partial GDELT (short version with 1000 samples) in the supplementary material due to the and will publish the complete dataset of Wiki and GDELT after acceptance. Besides, we provide the description of the data processing steps in Section 5. We provide the dataset statistics in Table 2 in appendix. Additionally, we provide our source code in the supplementary material.\n\nREFERENCES\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26, 2013.\n\nShib Sankar Dasgupta, Swayambhu Nath Ray, and Partha Talukdar. Hyte: Hyperplane-based temporally aware knowledge graph embedding. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 2001–2011, 2018.\n\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nOctavian-Eugen Ganea and Thomas Hofmann. Deep joint entity disambiguation with local neural\n\nattention. arXiv preprint arXiv:1704.04920, 2017.\n\nRishab Goel, Seyed Mehran Kazemi, Marcus Brubaker, and Pascal Poupart. Diachronic embedding for temporal knowledge graph completion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 3988–3995, 2020.\n\nZhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. Explainable subgraph reasoning for forecasting on temporal knowledge graphs. In International Conference on Learning Representations, 2020a.\n\nZhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. xerte: Explainable reasoning on temporal\n\nknowledge graphs for forecasting future links. arXiv preprint arXiv:2012.15537, 2020b.\n\nZhen Han, Yunpu Ma, Peng Chen, and Volker Tresp. Dyernie: Dynamic evolution of riemannian manifold embeddings for temporal knowledge graph completion. arXiv preprint arXiv:2011.03984, 2020c.\n\nZhen Han, Gengyuan Zhang, Yunpu Ma, and Volker Tresp. Time-dependent entity embedding is not all you need: A re-evaluation of temporal knowledge graph completion models under a unified framework. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8104–8118, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.639. URL https://aclanthology.org/2021.emnlp-main.639.\n\nBin He, Di Zhou, Jinghui Xiao, Qun Liu, Nicholas Jing Yuan, Tong Xu, et al. Integrating graph contextualized knowledge into pre-trained language models. arXiv preprint arXiv:1912.00147, 2019.\n\nSeyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs.\n\nAdvances in neural information processing systems, 31, 2018.\n\nBosung Kim, Taesuk Hong, Youngjoong Ko, and Jungyun Seo. Multi-task learning for knowledge graph completion with pre-trained language models. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1737–1743, 2020.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nTimothée Lacroix, Guillaume Obozinski, and Nicolas Usunier. Tensor decompositions for temporal\n\nknowledge base completion. arXiv preprint arXiv:2004.04926, 2020.\n\nJulien Leblay and Melisachew Wudage Chekol. Deriving validity time in knowledge graph. In\n\nCompanion Proceedings of the The Web Conference 2018, pp. 1771–1776, 2018.\n\nKalev Leetaru and Philip A Schrodt. Gdelt: Global data on events, location, and tone, 1979–2012. In\n\nISA annual convention, volume 2, pp. 1–49. Citeseer, 2013.\n\nXinyu Li, Fayuan Li, Lu Pan, Yuguang Chen, Weihua Peng, Quan Wang, Yajuan Lyu, and Yong Zhu. Duee: a large-scale dataset for chinese event extraction in real-world scenarios. In CCF International Conference on Natural Language Processing and Chinese Computing, pp. 534–545. Springer, 2020.\n\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-bert: Enabling language representation with knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 2901–2908, 2020.\n\nYunpu Ma, Volker Tresp, and Erik A Daxberger. Embedding models for episodic knowledge graphs.\n\nJournal of Web Semantics, 59:100490, 2019.\n\nMike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 1003–1011, 2009.\n\nMatthew E Peters, Mark Neumann, Robert L Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. Knowledge enhanced contextual word representations. arXiv preprint arXiv:1909.04164, 2019.\n\nApoorv Saxena, Soumen Chakrabarti, and Partha Talukdar. Question answering over temporal knowledge graphs. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, 2021a.\n\nApoorv Saxena, Soumen Chakrabarti, and Partha Talukdar. Question answering over temporal\n\nknowledge graphs. arXiv preprint arXiv:2106.01515, 2021b.\n\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo, Yaru Hu, Xuanjing Huang, and Zheng Zhang. Colake: Contextualized language and knowledge embedding. arXiv preprint arXiv:2010.00309, 2020.\n\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 conference on empirical methods in natural language processing, pp. 1499–1509, 2015.\n\nVolker Tresp, Cristóbal Esteban, Yinchong Yang, Stephan Baier, and Denis Krompaß. Learning with\n\nmemory embeddings. arXiv preprint arXiv:1511.07972, 2015.\n\nDenny Vrandeˇci ́c and Markus Krötzsch. Wikidata: A free collaborative knowledgebase. Commun. ACM, 57(10):78–85, sep 2014. ISSN 0001-0782. doi: 10.1145/2629489. URL https://doi. org/10.1145/2629489.\n\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176–194, 2021.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\nChengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens Lehmann. Temporal knowledge graph embedding model based on additive time series decomposition. arXiv preprint arXiv:1911.07893, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nChengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens Lehmann. TeRo: A time-aware knowledge graph embedding via temporal rotation. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 1583–1593, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020. coling-main.139. URL https://aclanthology.org/2020.coling-main.139.\n\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. Joint learning of the embedding of words and entities for named entity disambiguation. arXiv preprint arXiv:1601.01343, 2016.\n\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.\n\nLiang Yao, Chengsheng Mao, and Yuan Luo. Kg-bert: Bert for knowledge graph completion. arXiv\n\npreprint arXiv:1909.03193, 2019.\n\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced\n\nlanguage representation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nTable 2: Datasets Statistics\n\nDataset\n\n# Entities\n\n# Predicates\n\n# Timestamps\n\n# training set\n\n# validation set\n\n# test set\n\nGDELT DUEE WIKI\n\n5849 219 10844\n\n237 41 23\n\n2403 629 82\n\n755166 1879 233525\n\n94395 247 19374\n\n94395 247 19374\n\nTable 3: Hyperparameter settings of ECOLA and baselines.\n\nParameters\n\nDatasets\n\nTransE SimplE TTransE TNTComplEx DE-SimplE ECOLA-SF ECOLA-DE ECOLA-UTEE ECOLA-dyERNIE\n\nEmbedding dimension\n\nNegative Sampling\n\nLearning rate\n\nBatch Size\n\nGDELT DuEE Wiki GDELT DuEE Wiki GDELT DuEE Wiki\n\nGDELT DuEE Wiki\n\n768 768 768 768 768 768 768 768 768\n\n768 768 768 768 768 768 768 768 768\n\n768 768 768 768 768 768 768 768 768\n\n200 200 200 200 200 200 200 200 200\n\n100 100 100 100 100 100 200 200 200\n\n100 100 100 100 100 100 200 200 200\n\n5e-4 5e-4 5.2e-4 1.5e-4 5e-4 1e-4 2e-5 2e-5 2e-5\n\n5e-4 5e-4 5.2e-4 1.5e-4 5e-4 2e-5 2e-5 2e-5 e-4\n\n5e-4 5e-4 5.2e-4 1.5e-4 5e-4 1e-4 2e-5 2e-5 2e-5\n\n256 256 256 256 256 64 4\n4 4\n\n128 128 256 256 128 16 8\n8 8\n\n256 256 256 256 256 64 4\n4 4\n\nA RELATED WORK OF TEMPORAL KNOWLEDGE EMBEDDING\n\nTemporal Knowledge Embedding (tKE) is also termed as Temporal Knowledge Representation Learning (TKRL), which is to embed entities and predicates of temporal knowledge graphs into low-dimensional vector spaces. TKRL is an expressive and popular paradigm underlying many KG models. To capture temporal aspects, each model either embeds discrete timestamps into a vector space or learns time-dependent representations for each entity. Ma et al. (2019) developed extensions of static knowledge graph models by adding timestamp embeddings to their score functions. Besides, HyTE (Dasgupta et al., 2018) embeds time information in the entity-relation space by learning a temporal hyperplane to each timestamp and projects the embeddings of entities and relations onto timestamp-specific hyperplanes. Later, Goel et al. (2020) equipped static models with a diachronic entity embedding function which provides the characteristics of entities at any point in time and achieves strong results. Moreover, Han et al. (2020c) introduced a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds. It is the first work to contribute to geometric embedding for tKG and achieves state-of-the-art performances on the benchmark datasets. In particular, ECOLA is model-agnostic, which means any temporal KG embedding model can be potentially enhanced by training with the knowledge-text task.\n\nB ECOLA-SF: AN ABLATION STUDY ON STATIC FUSION\n\nWe compare the effectiveness of enhancing temporal knowledge embedding and enhancing static knowledge embedding. In particular, we only feed the static part of entity embeddings into PLM to perform the knowledge-text prediction task. We refer it as ECOLA-SF (StaticFusion).\n\nECOLA-SF is the static counterpart of ECOLA-DE, where we do not apply temporal knowledge embedding to the knowledge-text prediction objective LKT P . Specifically, we randomly initialize an embedding vector ̄ei P Rd for each entity ei P E, where ̄ei has the same dimension as the token embedding in pre-trained language models. Then we learn the time-invariant part ̄ei via the knowledge-text prediction task. For the tKE objective, we have the following temporal knowledge embedding,\n\n\"\n\neSF\n\ni\n\nptqrns “\n\nWsf ̄eirns aeirns sinpωeirnst ` beirnsq else,\n\nif 1 ď n ď γd,\n\nptq P Rd is an entity embedding containing static and temporal embedding parts. where eSF aei, ωei , bei P Rd ́γd are entity-specific vectors with learnable parameters. Wsf P Rdˆγd is\n\ni\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nHits@1 (%) Hits@10 (%)\n\nCRONKGQA ECOLA-CRONKGQA\n\n25.8 27.5\n\n52.0 54.4\n\nTable 4: Performance of ECOLA enhaned language model in tKBQA task.\n\nmatrix with learnable weights. Note that eSF embedding ̄ei instead of eSF\n\nptq in LKT P .\n\ni\n\ni\n\nptq only plays a role in LtKE, and we use static\n\nC ENHANCEMENT ON LANGUAGE REPRESENTATIONS\n\nAlthough our work focuses on enhancing temporal knowledge embeddings with contextualized language representations, joint models often benefit both the language model and the tKG model due to the mutual information exchange between language and tKGs during joint training. To study ECOLA’s enhancement on the language model, we selected temporal question answering as a downstream task to show that the proposed ECOLA can also benefit the language model.\n\nTemporal Question Answering over Temporal Knowledge Graphs (TKGQA) Natural questions often include temporal constraints, e.g., who was the US president before Jimmy Carter? To deal with such challenging temporal constraints, temporal question answering over temporal knowledge base, formulated as TKGQA task, has become trendy since tKGs help to find entity or timestamp answers with support of temporal facts.\n\nPerformance Gain on TKGQA Saxena et al. (2021a) introduced the TKGQA dataset CRONQUESTIONS containing natural temporal questions with different types of temporal constraints and an accompanying temporal knowledge graph (tKG). They proposed a baseline called CRONKGQA that uses a pre-trained language model (BERT) to understand the implicit representation of temporal constraints in temporal questions followed by a scoring function for answer prediction. We enhance the language encoder in CRONKGQA with the proposed ECOLA approach, i.e., we find temporal relevant texts for quadruples in the supporting tKG given in CRONQUESTIONS and train the language model and the tKG model jointly with the proposed KTP task. Then we plug the enhanced language model back into CRONKGQA and name the enhanced model as ECOLA-CRONKGQA. The models are evaluated with standard metrics Hits@kpk P t1, 3uq: the percentage of times that the true entity or time candidate appears in the top k of ranked candidates. As shown in Table 4, empirical results show that our proposed ECOLA enhances the language model with 7.4 % relative improvements regarding precision on CRONQUESTIONS, demonstrating the benefits of ECOLA to the language model.\n\nD IMPLEMENTATION\n\nWe use the datasets augmented with reciprocal relations to train all baseline models. We tune hyperparameters of our models using the random search and report the best configuration. Specifically, we set the loss weight λ to be 0.3, except for ECOLA-DE model trained on Wiki dataset where λ is set to be 0.001. We use the Adam optimizer (Kingma & Ba, 2014). We use the implementation of DE-SimplE6, ATiSE/TeRO7. We use the code for TNTCopmlEx from the tKG framework (Han et al., 2021). We implement TTransE based on the implementation of TransE in PyKEEN8. We provide the detailed settings of hyperparameters of each baseline model and ECOLA in Table 3 in the appendix.\n\nE THE AMOUNT OF COMPUTE AND THE TYPE OF RESOURCES USED\n\nWe run our experiments on an NVIDIA A40 with a memory size of 48G. We provide the training time of our models and some baselines in Table 5. Note that there are no textual descriptions at\n\n6https://github.com/BorealisAI/de-simple 7https://github.com/soledad921/ATISE 8https://github.com/pykeen/pykeen\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\ninference time, and we take the entity and predicate embedding as input and use the score function of KG models to predict the missing links. Thus, the inference time of ECOLA (e.g., ECOLA-DE) and its counterpart KG model (e.g., DE-SimplE) is the same.\n\nTable 5: The runtime of training procedure (in hours).\n\nDataset\n\nGDELT DuEE Wiki\n\nDE-SimplE ECOLA-DE UTEE ECOLA-UTEE DyERNIE ECOLA-DyERNIE\n\n17 24.0 67.3 36.0 25 23.8\n\n0.5 16.7 0.5 12.8 0.1 10.8\n\n5.0 43.2 11.3 45.6 5.9 67.2\n\n15",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a temporal KG-enhanced language model. \n\nThe idea is to combine pretrained temporal knowledge embeddings with the language model as input tokens, and the language model will take temporal-aware embeddings as input. \nThe authors propose masking the predicates, entities, or subwords for pretraining the model. \nThe authors convert three KGs into the pretraining corpus to find relevant text with temporal KG knowledge. Experiments show that the pretrained model has improved performance on link prediction for the three temporal KGs.\n\n# Strength And Weaknesses\n\nStrength: \n\nThe paper fills in the gap on using temporal KG to enhance the language model. The results look solid and bring consistent improvement in link prediction.\n\nWeakness: \n1. The method in the paper lacks novelty. It basically reformulates the previous KG+LM methods to the temporal KG+LM setting. Actually, the method (and even the figures) looks a lot like the KEPLER paper (Wang et al., 2019), with the KG embeddings replaced by tKG embeddings, and the transE loss replaced by the corresponding tKG loss.\n2. The method is only tested on link prediction tasks. I would expect this model to work in more diverse settings (i.e., any task that LM can be used for). It would be great if the model could work for settings like question answering, event extraction, etc. I'm also interested in how the tKG-pretraining affects general NLU performance like on GLUE.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written and easy to understand. As mentioned above, I think it lacks novelty. Code and data are included with the submission.\n\nA question: What are b_i and b_j in (1)? I did not find the definition in the paper.\n\n# Summary Of The Review\n\nIn all, I feel the paper focuses on an important area, but the novelty and contribution are not enough for a conference like ICLR.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel approach for enhancing temporal knowledge embeddings by integrating contextualized language representations derived from textual data. It addresses the limitations of existing temporal knowledge graph (tKG) models, which typically rely solely on structured relationships and neglect the dynamic nature of information over time. The authors introduce a knowledge-text prediction (KTP) task that aligns textual knowledge with tKG quadruples, optimizing both KTP and temporal knowledge embedding objectives simultaneously. Experimental results demonstrate that ECOLA significantly improves temporal knowledge graph completion performance across multiple datasets, with notable enhancements in metrics such as Mean Reciprocal Rank (MRR).\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative integration of textual knowledge with temporal knowledge embeddings, which is a relatively unexplored area in the literature. The introduction of three new datasets specifically designed for this task contributes to the field by providing valuable resources for future research. The methodology, particularly the use of a masked transformer encoder for the KTP task, is well-structured and effectively enhances the model's ability to capture temporal dependencies. However, a potential weakness is the reliance on BERT's embeddings, which may limit the generalizability of the approach to other types of textual data or languages. Additionally, while the results are promising, further validation on more diverse datasets could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-organized, providing a coherent narrative from the introduction to the conclusion. The methodology is detailed, allowing for a good understanding of how ECOLA operates, although some sections could benefit from additional diagrams or examples to clarify complex processes. The novelty of the approach is significant, as it combines structured and unstructured knowledge in a novel manner while preserving the temporal aspects of the embeddings. The reproducibility is supported by the availability of source code and partial datasets, although full dataset access post-acceptance remains a concern for complete reproducibility.\n\n# Summary Of The Review\nOverall, ECOLA represents a significant advancement in the integration of textual knowledge with temporal knowledge embeddings, offering a novel framework that improves performance in tKG completion tasks. While the contributions are promising and well-supported by empirical results, further validation across more diverse datasets and greater clarity in some methodological aspects would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel framework that enhances temporal knowledge graph (tKG) embeddings by integrating relevant textual knowledge through contextualized language representations. The authors introduce a new knowledge-text prediction (KTP) task that aligns temporal knowledge graph quadruples (subject, predicate, object, timestamp) with pertinent textual data, allowing for the joint optimization of both KTP and temporal knowledge embedding objectives. Experimental results demonstrate that ECOLA significantly outperforms state-of-the-art tKG embedding models across multiple datasets, achieving up to a 287% relative improvement in Hits@1.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to integrating textual knowledge with temporal graph embeddings, which addresses a critical gap in existing models. The empirical evaluation is robust, utilizing three newly constructed datasets that enhance the field's research landscape. Additionally, ECOLA's model-agnostic nature allows it to improve various tKG models, showcasing its versatility. However, the paper has limitations, including potential biases in the new datasets due to the distant supervision methods employed. There is also a risk of overfitting in smaller datasets like DuEE and concerns regarding the computational complexity of the model architecture, which may limit accessibility for smaller research teams.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The quality of the writing is high, making it accessible to a broad audience. The novelty of the approach is significant, as it introduces a new task and framework that effectively combines temporal and textual knowledge. While the experimental results are reproducible, the complexity of the model and the potential biases in the datasets could pose challenges for full reproducibility in different contexts.\n\n# Summary Of The Review\nOverall, ECOLA presents a compelling advancement in the field of temporal knowledge graph embeddings by effectively integrating textual knowledge. The framework demonstrates significant empirical improvements and offers a novel approach, though it is essential to address potential biases and overfitting risks for practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), a novel framework that integrates structured temporal knowledge graphs (tKGs) with unstructured textual data. The methodology involves utilizing tKG quadruples to align textual information with evolving entity representations through a masked transformer encoder and a new knowledge-text prediction task. The findings indicate that ECOLA significantly outperforms existing tKG embedding models across three newly constructed datasets (GDELT, DuEE, and Wiki) in tasks related to temporal knowledge graph completion.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to combining structured and unstructured knowledge while addressing the temporal alignment challenges inherent in tKGs. The introduction of a knowledge-text prediction task is a noteworthy contribution that enhances the model’s ability to leverage contextualized language representations. However, the paper could benefit from a more detailed discussion on potential limitations, such as overfitting observed in smaller datasets with the ECOLA-DyERNIE variant. Additionally, while the proposed model variants are interesting, the paper could provide further insights into the effects of different temporal embedding functions on model performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem statement, methodology, and experimental results. The quality of the writing is high, with sufficient technical detail to convey the complexities of the ECOLA framework. The novelty of the approach is significant, as it provides a fresh perspective on temporal knowledge graph embedding by effectively integrating textual data. The reproducibility is reinforced by the inclusion of dataset statistics and hyperparameter settings in the appendix, though more information on specific implementation details could enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a compelling contribution to the field of knowledge graph embeddings by introducing ECOLA, which successfully merges temporal knowledge with contextualized language. The experimental results robustly support the effectiveness of the proposed method, although discussions on potential limitations could be expanded.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces ECOLA, a novel approach that integrates temporal knowledge embeddings with contextualized language representations to enhance temporal knowledge graph completion. The methodology involves aligning textual data with temporal knowledge graphs, leveraging both pre-trained language models and knowledge graph embeddings to improve performance. The authors present empirical results demonstrating that ECOLA outperforms state-of-the-art temporal knowledge graph embedding models across multiple tasks. Additionally, the paper contributes three new datasets tailored for training and evaluation purposes, addressing the critical challenge of temporal alignment in knowledge representation.\n\n# Strength And Weaknesses\n1. **Strength: Novel Approach**\n   - ECOLA represents a significant advancement by addressing a gap in existing research through the integration of temporal knowledge embeddings with language representations.\n   - **Limitation:** The method's effectiveness is contingent upon the quality of the textual data used for alignment, which may lead to performance issues if the data is inadequate.\n\n2. **Strength: Improved Performance**\n   - Experimental results indicate that ECOLA substantially outperforms existing methods, showcasing its effectiveness for temporal knowledge graph completion.\n   - **Limitation:** The experiments are limited to a few datasets, raising questions about the generalizability of the findings to broader real-world applications.\n\n3. **Strength: Model-Agnostic Nature**\n   - The model-agnostic design of ECOLA allows it to enhance a variety of temporal knowledge graph embedding models, increasing its versatility.\n   - **Limitation:** This feature may result in less optimization for certain models, potentially overlooking their unique characteristics.\n\n4. **Strength: Creation of New Datasets**\n   - The introduction of three new datasets is a valuable resource for the research community, facilitating further exploration in this area.\n   - **Limitation:** These datasets are adaptations of existing ones, which may carry biases that could affect the validity of the experimental results.\n\n5. **Strength: Temporal Alignment**\n   - The paper effectively addresses the challenge of temporal alignment, a previously underexplored area in the literature.\n   - **Limitation:** The complexity of implementing temporal alignment may hinder practical applications in rapidly evolving domains.\n\n6. **Strength: Comprehensive Evaluation**\n   - Extensive experiments and ablation studies provide detailed insights into the model's components and their respective impacts.\n   - **Limitation:** The reliance on specific performance metrics may not fully capture model performance in diverse application contexts.\n\n7. **Strength: Integration of Knowledge and Language Models**\n   - The successful combination of knowledge graph embeddings with pre-trained language models demonstrates the strengths of both methodologies.\n   - **Limitation:** This integration increases computational complexity, which could pose challenges for deployment in resource-limited settings.\n\n8. **Strength: Potential for Broader Applications**\n   - The research opens opportunities for related tasks, such as temporal question answering, suggesting its broader applicability.\n   - **Limitation:** The actual impact remains speculative until further validation in diverse contexts is conducted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the contributions and findings. The methodology is detailed, allowing for reproducibility; however, the reliance on specific datasets may limit broader applicability. The novelty of the approach is significant, particularly in the integration of temporal embeddings and language models.\n\n# Summary Of The Review\nOverall, the paper presents a novel and impactful contribution to the field by integrating temporal knowledge embeddings with contextualized language representations, demonstrating improved performance on temporal knowledge graph completion tasks. While the results are promising, the limitations around dataset generalizability and practical implementation must be addressed to enhance the overall applicability of ECOLA.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA)\" presents a novel approach to integrating structured temporal knowledge graphs (tKGs) with unstructured textual data. The authors introduce ECOLA, a methodology that enhances temporal knowledge embeddings by leveraging contextualized language representations. Key contributions include the development of a Temporal Knowledge-Text Alignment mechanism, the introduction of a Knowledge-Context Prediction (KCP) task, and the creation of three new datasets designed to provide temporal alignment with textual data. The results demonstrate significant improvements in temporal knowledge graph completion tasks, outperforming existing state-of-the-art models.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative methodology that captures the dynamic nature of entity representations over time, as well as the introduction of relevant datasets that address existing gaps in research. The joint optimization framework effectively integrates both structured and unstructured knowledge, leading to enhanced performance metrics. However, a potential weakness lies in the limited exploration of various masking strategies in the KCP task, which could impact the robustness of the findings. Additionally, while the results are promising, further validation on diverse datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and methodology. The quality of the writing is high, making complex concepts accessible. The novelty of the approach is significant, as it integrates temporal reasoning with contextualized embeddings in a unique manner. The reproducibility of the results is facilitated by the introduction of new datasets and the detailed description of the methodology, although the authors could enhance reproducibility by providing baseline comparisons with more existing methods.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to enhancing temporal knowledge embeddings through contextualized language representations. Its contributions are substantial, and the experimental results validate the effectiveness of the proposed methodology. I recommend acceptance, with some suggestions for further exploration and validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel methodology aimed at enhancing the adversarial robustness of temporal knowledge graph (tKG) embeddings by leveraging contextualized language representations. The main contributions include a mechanism for adversarially robust temporal embeddings that incorporate noise during training, a knowledge-text prediction task that enriches the training process, and the creation of three new datasets featuring temporal knowledge graph quadruples paired with adversarial examples. The experimental results indicate substantial improvements in adversarial robustness, with ECOLA achieving up to a 287% performance increase under adversarial conditions compared to existing models, demonstrating its versatility and model-agnostic nature.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of a relevant and timely topic—adversarial robustness in dynamic settings—alongside a solid experimental design that showcases the efficacy of the proposed method. The creation of new datasets is a significant contribution, providing valuable resources for further research in this area. However, the paper could improve in clarity regarding the implications of their findings and the theoretical underpinnings of their approach. Additionally, while the results are promising, further validation on larger and more diverse datasets would enhance the credibility of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but certain sections could benefit from greater clarity, particularly when discussing the implications of the findings for both theory and practice. The novelty of the approach is noteworthy, as it integrates adversarial training with temporal knowledge graphs, a relatively underexplored area. The reproducibility of the results is supported by the provision of datasets and detailed methodology, although additional information on the implementation specifics would further aid in this aspect.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of adversarial training in temporal knowledge graphs, presenting a novel and effective approach that leverages contextualized language representations. While the findings are promising and well-supported by empirical results, improvements in clarity and further validation could strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a method called ECOLA, which integrates textual knowledge into temporal knowledge graphs (tKGs) to enhance their performance. The authors claim that ECOLA significantly improves tKG embedding models by leveraging unstructured textual data and address the challenge of temporal alignment with text. Additionally, the paper introduces three new datasets for training and evaluation and reports substantial improvements in various metrics, although the practical implications of these findings appear limited.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to merge textual information with temporal knowledge, which is a relevant and important area of research. However, the actual contributions of ECOLA seem limited; the improvements presented are modest and do not significantly surpass existing methods. The datasets introduced are also somewhat derivative, relying on existing sources, which reduces their novelty. Furthermore, claims regarding the universality of ECOLA's application to different tKG models may be misleading, as the method appears to be relatively specialized.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow; however, the novelty of the proposed approach is overstated. While the methodology is described adequately, the experimental results lack sufficient quantitative backing to convincingly demonstrate the claimed improvements. The reproducibility of results might be challenged by the specific conditions under which the improvements are reported, making it difficult for other researchers to validate the findings independently.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting approach to enhancing temporal knowledge graphs with textual data, the contributions are less impactful than suggested. The novelty and significance of the method are somewhat inflated, and the empirical results do not support the authors' claims of a transformative advancement in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations\" introduces ECOLA, a novel approach for integrating temporal knowledge graphs (tKGs) with contextualized textual knowledge. The methodology focuses on aligning temporal entity representations with textual data through a new knowledge-text prediction task, enhancing link prediction capabilities. The authors present experimental results that demonstrate ECOLA's effectiveness in improving tKG completion tasks, achieving notable performance metrics, albeit with some discrepancies in original claims regarding improvement percentages.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to combining temporal knowledge with textual information, addressing a significant gap in the traditional static representations of knowledge graphs. The introduction of a masked language modeling task for learning relationships enhances the learning process in a meaningful way. However, the paper has weaknesses, particularly regarding the clarity of experimental results, where the reported performance improvements appear inflated compared to actual outcomes. This discrepancy raises concerns about the reliability of claims made in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and articulates its contributions clearly, although some technical aspects could benefit from further elaboration. The novelty of integrating temporal and textual representations is commendable, and the methodology offers a fresh perspective on knowledge graph embeddings. The authors emphasize reproducibility by committing to dataset transparency, which is a positive aspect; however, the actual details surrounding dataset modifications warrant further clarification to ensure full reproducibility of results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to enhancing temporal knowledge embeddings through contextualized language representations. While it shows promise in improving link prediction tasks, certain inconsistencies in reported results necessitate caution in interpreting the findings. The commitment to reproducibility is commendable, yet further clarity in experimental details is required.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach for enhancing temporal knowledge graph (tKG) embeddings by integrating textual data, termed ECOLA (Entity-Centric Overall Learning Approach). The methodology involves aligning textual information with tKG quadruples to improve temporal knowledge representations. The findings suggest that ECOLA outperforms existing tKG models in various tasks, particularly in the knowledge-text prediction (KTP) task, by leveraging pre-trained language models (PLMs) like BERT for improved contextual embeddings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating textual data with tKGs, potentially addressing issues of sparsity in knowledge graphs. However, several weaknesses undermine its contributions. The assumptions made about the effectiveness of PLMs and the implicit measures for temporal alignment could lead to overgeneralization. Additionally, the reliance on specific performance metrics such as Hits@1 and MRR fails to capture the model's effectiveness in broader applications, and the potential biases in dataset construction warrant further scrutiny.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear, but certain assumptions and methodological details could benefit from more thorough explanations. While the novelty of integrating structured and unstructured knowledge is commendable, the claim that ECOLA can enhance any temporal KG model lacks empirical backing across diverse datasets. Reproducibility could be improved by providing clearer descriptions of dataset construction and the integration process, which are crucial for validating the proposed approach.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to enhancing tKG embeddings through the integration of textual data, but its foundational assumptions and reliance on specific performance metrics raise concerns. A more thorough empirical evaluation and clarification of certain methodological aspects would strengthen the contributions and applicability of the proposed model.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces ECOLA, a novel method for enhancing temporal knowledge graph embeddings by integrating contextualized language representations. It addresses the critical challenge of aligning temporally relevant textual information with the evolving representations of entities in temporal knowledge graphs (tKGs). The methodology involves utilizing tKG quadruples to align textual data with time-dependent entity embeddings, leveraging a multi-task learning approach that includes a unique knowledge-text prediction task. The experimental results demonstrate that ECOLA significantly improves performance on temporal knowledge graph completion tasks compared to existing baseline models, indicating its effectiveness in utilizing textual knowledge for enhancing tKG embeddings. Additionally, the authors present three new datasets containing temporally aligned textual descriptions, which pave the way for further research in this area.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to integrating textual information with temporal knowledge graph embeddings, a topic that has been relatively underexplored in existing literature. The introduction of novel datasets specifically designed for training and evaluation is another significant contribution, as it addresses a gap in the availability of temporally aligned data. However, the paper could benefit from a more comprehensive discussion on the limitations of the proposed method, including potential challenges in scalability and the generalizability of the findings across different domains. Additionally, while the performance improvements are notable, further analysis of the model's behavior in diverse scenarios would strengthen the validation of its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly conveys the motivation, methodology, and results of the proposed approach. The structure is logical, making it easy for readers to follow the progression from problem identification to solution and findings. The novelty of integrating textual information with temporal embeddings is clear, and the approach is presented in a manner that allows for reproducibility. The authors provide sufficient details regarding the datasets and experimental setup, facilitating other researchers to replicate the study. However, additional insights into the hyperparameter choices and model training specifics would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of temporal knowledge graph embeddings by proposing ECOLA, which effectively integrates contextualized language representations. The introduction of novel datasets and significant performance improvements over baselines underscore the paper's relevance and potential impact. However, a deeper examination of the method's limitations and broader applicability would enhance the discussion.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a novel approach to integrating diverse data sources in machine learning, introducing a comprehensive framework that enhances model performance. The methodology consists of a unique architecture that allows for the simultaneous processing of heterogeneous data types, which is empirically validated across various benchmarks. The findings indicate significant improvements in accuracy and generalization over existing state-of-the-art methods, supported by new datasets that address existing gaps in aligned data availability.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative framework that effectively combines multiple information sources, offering a potential advancement in model accuracy. The empirical validation is thorough, showcasing the proposed method's effectiveness against various benchmarks, which adds credibility to the findings. However, the paper lacks a detailed theoretical analysis that could elucidate the underlying mechanisms of the proposed approach. Additionally, the comparison with baseline models is somewhat limited, which hampers a full understanding of the method's relative performance. Concerns about potential overfitting in specific contexts also warrant attention in future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear articulation of the problem statement, methodology, and results. The quality of writing is high, making complex ideas accessible. While the novelty of the framework is apparent, the reproducibility of the results could be improved by providing more details on experimental setups and datasets. Furthermore, the inclusion of additional baselines and ablation studies would enhance the clarity of the contributions and the robustness of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to integrating diverse data sources in machine learning, with strong empirical results that suggest significant advancements over existing methods. However, the lack of theoretical grounding and a broader evaluation limits the paper's impact. Addressing these areas would greatly enhance the manuscript's contributions and clarity.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents ECOLA, an innovative approach to enhance temporal knowledge embeddings by integrating contextualized textual representations. It addresses the limitations of existing methods that typically rely on static embeddings, particularly in the context of temporal knowledge graphs (tKGs). ECOLA introduces a knowledge-text prediction task that aligns text with the evolving representations of entities, thereby improving the performance of tKG completion tasks. Extensive empirical evaluations demonstrate that ECOLA significantly outperforms current tKG models, highlighting its effectiveness in fusing structured and unstructured temporal knowledge.\n\n# Strength And Weaknesses\nOne of the primary strengths of ECOLA is its model-agnostic nature, allowing for broad applicability across various tKG models. The introduction of new datasets for training and evaluation is another significant contribution, as it facilitates future research and benchmarks in this area. However, the paper may lack detailed discussions on the computational complexity and scalability of the proposed approach, which could be a concern for practical implementations. Additionally, while the results are promising, further exploration of the limitations and potential biases in the data would strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodologies, and findings. The quality of writing is high, with a logical flow that guides the reader through the research. The novelty of the approach lies in its integration of temporal and textual knowledge, which is less commonly addressed in the literature. The reproducibility aspect is supported by the introduction of new datasets and the model's design; however, additional details on implementation specifics and hyperparameter settings would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, ECOLA presents a significant advancement in the integration of temporal knowledge graphs and textual data, demonstrating its effectiveness through robust empirical results. The model's adaptability and the introduction of new datasets are noteworthy contributions, although further exploration of its limitations could improve the paper's rigor. Overall, the work is relevant and timely, addressing an important gap in current research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel framework designed to enhance temporal knowledge embeddings by integrating structured knowledge from knowledge graphs (KGs) with unstructured textual data. The methodology involves a knowledge-text prediction task that aligns textual information with time-evolving entity representations, utilizing pre-trained language models such as BERT for textual representation. The findings demonstrate significant improvements in temporal knowledge graph completion tasks, indicating that ECOLA effectively addresses the limitations of existing temporal knowledge graph models by incorporating contextualized language representations.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to merging structured and unstructured knowledge, which is a significant advancement in the field of knowledge graph embeddings. The introduction of new datasets for training and evaluation provides additional value, enabling further research in this area. However, a potential weakness lies in the reliance on pre-trained language models, which may limit the generalizability of the results across different domains or languages. Additionally, while the performance metrics reported are impressive, further qualitative analysis or application examples could strengthen the paper's claims about practical implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with knowledge graphs and machine learning. The quality of the methodology is high, with a clear explanation of the model architecture and training procedure. The novelty of combining temporal knowledge embeddings with contextualized language representations is significant, offering a fresh perspective on knowledge integration. However, the reproducibility of the results may be hindered by the lack of detailed descriptions regarding the implementation specifics and dataset characteristics, which are crucial for other researchers to replicate the findings.\n\n# Summary Of The Review\nOverall, this paper introduces a compelling approach to enhancing temporal knowledge embeddings by integrating textual data, yielding significant performance improvements in knowledge graph completion tasks. While the contributions are notable, the paper could benefit from more detailed methodological descriptions to enhance reproducibility and provide a clearer understanding of its practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations\" presents a novel model, ECOLA, designed to improve temporal knowledge graphs (tKGs) by integrating structured and unstructured knowledge. The authors highlight the limitations of existing models that overlook the temporal dimension of data and propose a robust architecture capable of aligning textual knowledge with temporal embeddings. To validate their approach, they introduced three new datasets derived from existing sources and conducted extensive experiments, demonstrating that ECOLA significantly outperforms baseline models in temporal knowledge graph completion tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to bridging textual and temporal knowledge, which is a relatively unexplored area within knowledge graph research. The introduction of three new datasets adds significant value for future research and benchmarking. Additionally, the comprehensive evaluation against both static and temporal baselines, including ablation studies, provides strong evidence for the effectiveness of the proposed model. However, a potential weakness is the reliance on existing datasets, which may limit the generalizability of the findings to real-world scenarios where data may be less structured. Furthermore, while the quantitative results are strong, further qualitative insights into the model's predictions could enhance the understanding of its practical implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear definitions and explanations of key concepts. The writing is generally clear, and figures and tables are effectively employed to illustrate findings. The novelty of the proposed method is significant, particularly in its integration of contextualized language representations with temporal knowledge embeddings. The reproducibility of the results is facilitated by the provision of detailed methodology and dataset statistics, although the authors could enhance this aspect by sharing their code and pre-trained models.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of temporal knowledge graphs by introducing a model that effectively incorporates textual knowledge. The empirical results demonstrate substantial improvements over existing methods, underscoring the model's potential impact on both theoretical and practical aspects of knowledge graph research.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces ECOLA, a novel framework that enhances temporal knowledge embeddings by integrating structured knowledge from temporal knowledge graphs (tKGs) with unstructured textual data. The methodology consists of leveraging tKG quadruples as latent indicators for temporal alignment, introducing a knowledge-text prediction (KTP) task to facilitate this integration. The findings demonstrate that ECOLA significantly improves the performance of tKG completion tasks, outperforming existing models by achieving higher Mean Reciprocal Rank (MRR) and Hits@k metrics across various datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining temporal embeddings with contextualized language representations, addressing a critical gap in the literature on temporal knowledge representation. The introduction of KTP as a task specifically designed to enhance alignment between textual and temporal knowledge is particularly noteworthy. However, the paper could benefit from a more detailed analysis of the computational complexity of the ECOLA framework and a clearer discussion on the limitations of the proposed methodology, such as potential issues with scalability or the generalizability of results to other domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers with a background in knowledge graphs and machine learning. The methodology is described in sufficient detail to allow for reproducibility, with explicit formulations and loss functions provided. The novelty of the approach is significant, as it presents a new angle on the integration of textual and temporal data, which has been underexplored in prior work. However, there is a lack of comprehensive ablation studies that could help to further validate the contributions of individual components of the ECOLA framework.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of temporal knowledge representation by effectively integrating textual knowledge with temporal embeddings. While the methodology and findings are robust, further exploration of the model's limitations and thorough ablation studies would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a new approach, termed ECOLA, which aims to integrate textual knowledge with temporal knowledge graphs (tKGs) for improved knowledge embedding. The authors propose a knowledge-text prediction task and introduce three new datasets to evaluate their method. Although the paper claims to provide advancements in dynamic embeddings for knowledge graphs, the methodology appears to draw heavily from existing techniques without substantial innovation or justification for the approach taken.\n\n# Strength And Weaknesses\nThe primary weakness of the paper lies in its lack of clear novelty, as many concepts presented are derivative of established knowledge graph embedding techniques. The integration of textual and temporal knowledge is insufficiently justified and appears to merge existing methods rather than introduce a groundbreaking approach. While the introduction of three new datasets could be a potential strength, concerns arise regarding their robustness and generalizability. Additionally, the experimental results are inconsistent across datasets, suggesting potential overfitting or biases. The claims of model-agnosticism are questionable, and the reliance on a pre-trained language model (BERT) without significant modifications raises concerns about originality. The ablation studies and qualitative analyses lack depth, and limitations are not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is compromised by vague reproducibility statements, providing insufficient detail on how to replicate the experiments, which undermines the credibility of the findings. The novelty of the proposed task seems incremental rather than groundbreaking, and the overall quality of the presentation could be improved by addressing the aforementioned weaknesses more thoroughly.\n\n# Summary Of The Review\nOverall, the paper presents a promising idea of integrating textual and temporal knowledge graphs; however, it ultimately lacks significant novelty and depth in both its methodology and analyses. The reliance on existing techniques without substantial modifications raises questions about the originality and robustness of the findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel framework that integrates temporal knowledge graphs (tKGs) with contextualized language representations. The main contributions of ECOLA include its innovative approach to temporal awareness in knowledge embeddings, a new knowledge-text prediction task, and the creation of three new datasets specifically designed for training and evaluation. The methodology involves leveraging pre-trained language models to encode textual data, resulting in significant performance improvements over existing tKG embedding models, with enhancements of up to 287% in the Hits@1 metric.\n\n# Strength And Weaknesses\nThe strengths of ECOLA lie in its innovative integration of temporal and contextualized knowledge representations, which addresses the dynamic nature of real-world data effectively. Additionally, its model-agnostic nature enhances the versatility of the framework, making it applicable across various domains. The introduction of new datasets is also commendable, as it provides valuable resources for future research. However, the paper could benefit from a more detailed discussion on the limitations of the approach and potential challenges in real-world application scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The quality of the experiments is robust, showcasing ECOLA's superiority over existing models. The novelty of combining temporal knowledge with contextualized representations is substantial and adds significant value to the field. The reproducibility of the results may be enhanced by providing more detailed descriptions of the experimental setup and parameter settings.\n\n# Summary Of The Review\nOverall, ECOLA represents a significant advancement in the field of temporal knowledge embeddings, showcasing innovative methodologies and substantial empirical results. The potential impact of this work on various applications is promising, making it a noteworthy contribution to the literature.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces the Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA) framework, which aims to improve the representation of temporal knowledge graphs (tKGs) by integrating structured knowledge from knowledge graphs and unstructured knowledge from textual data. The methodology involves a joint optimization approach that combines knowledge-text prediction tasks with temporal knowledge embedding objectives, thus creating richer, context-aware embeddings. The findings suggest that this integrated approach effectively addresses issues of temporal sparseness and enhances the evolution of entity representations over time, thereby advancing the theoretical landscape of knowledge representation.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of structured and unstructured knowledge, which provides a more comprehensive understanding of temporal dynamics in knowledge graphs. The model-agnostic nature of ECOLA is another notable advantage, allowing it to be applied across various temporal KG models. However, the paper may lack empirical validation of its claims, which could limit the practical applicability of the proposed framework. Additionally, while the theoretical contributions are substantial, the complexity of the methodology may pose challenges for reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making the theoretical concepts accessible to readers. The quality of the theoretical contributions is high, particularly in addressing the integration of textual knowledge with tKGs. However, the novelty of the methodology, while significant, may not be fully realized without accompanying empirical results to validate its effectiveness. Reproducibility could be a concern given the sophisticated nature of the model and the potential for implementation challenges.\n\n# Summary Of The Review\nOverall, the ECOLA framework represents a meaningful advancement in the field of temporal knowledge representation by addressing critical limitations of existing models. While the theoretical contributions are robust and the integration of knowledge types is innovative, the lack of empirical validation may hinder the practical impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents ECOLA (Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations), which enhances the representation of temporal knowledge graphs (tKGs) by integrating contextualized language representations from pre-trained models like BERT. The methodology involves constructing a knowledge-text prediction (KTP) task that aligns temporal tKG quadruples with textual data, using a sophisticated architecture that includes an embedding layer, a temporal knowledge encoder, and a masked transformer encoder. The experimental results demonstrate that ECOLA outperforms baseline models such as TransE and DyERNIE in temporal KG completion tasks across several new datasets derived from GDELT, DuEE, and Wiki.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to merging temporal knowledge graphs with contextualized language representations and the introduction of the KTP task, which could potentially bridge the gap between structured and unstructured data. The paper also provides detailed implementation specifics, including hyperparameter settings and computational resources. However, weaknesses include a lack of discussion on the broader implications of the findings and the potential limitations of the proposed method. The focus on implementation details may overshadow theoretical contributions that could benefit the research community.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and presents a well-structured methodology that is easy to follow. The quality of the experimental setup is high, with thorough evaluations and comparisons against baseline models. The novelty of the approach lies in the integration of contextualized embeddings into temporal knowledge graph modeling, although similar ideas have been explored previously in other contexts. Reproducibility is aided by the availability of code and detailed descriptions of datasets and hyperparameters.\n\n# Summary Of The Review\nOverall, ECOLA demonstrates a promising advancement in temporal knowledge graph representation by leveraging contextual language models. The detailed methodology and experimental results support its contributions, although further exploration of its theoretical implications could enhance the impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents ECOLA, a framework designed to enhance temporal knowledge embeddings by incorporating textual knowledge. The authors claim that ECOLA addresses the challenge of temporal alignment in knowledge embeddings using temporal knowledge graph (tKG) quadruples as an implicit measure for aligning textual data. The findings indicate that ECOLA achieves significant performance improvements over existing models in various metrics, including Hits@1, and introduces new datasets for training and evaluation.\n\n# Strength And Weaknesses\nThe strengths of the paper include the introduction of new datasets, which can serve as valuable benchmarks for future research, and the claimed performance improvements over existing models. However, the paper's weaknesses lie in its lack of substantial novelty when compared to established methodologies such as KEPLER and KG-BERT, which also integrate textual information. Additionally, the effectiveness of ECOLA may be overstated, as many comparative models may not have been fully optimized for textual integration. The model-agnostic nature of ECOLA is also questioned, as other existing methods have claimed similar capabilities without demonstrable differences.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably clear and well-structured, presenting its methodology and findings coherently. However, the novelty of the contributions is questionable, as many aspects of ECOLA's approach appear to draw heavily from existing literature. The reproducibility of the results may be hindered by the lack of sufficient contextualization regarding the performance of baseline models and the specific conditions under which comparisons were made.\n\n# Summary Of The Review\nOverall, while ECOLA offers a systematic attempt to merge textual knowledge with temporal embeddings, it does not provide sufficiently novel contributions to differentiate itself from existing methodologies. The paper's claims of significant improvements must be interpreted with caution, given the potential optimization differences among the compared models.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents \"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations\" (ECOLA), which aims to improve the performance of temporal knowledge graph (tKG) embeddings by incorporating textual knowledge. The authors introduce a novel knowledge-text prediction (KTP) task, which is an extension of masked language modeling, and propose three new datasets for training and evaluating their model. The findings suggest that ECOLA significantly enhances tKG embeddings, demonstrating improved performance on standard benchmarks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating contextualized language representations into tKG embeddings, providing a fresh perspective on knowledge graph completion tasks. Additionally, the introduction of the KTP task and the creation of three new datasets contribute to the empirical foundation of the research. However, the paper suffers from several clarity issues, such as inconsistent terminology and insufficient definitions of key terms, which may hinder comprehension for readers unfamiliar with the concepts. The methodology section could also benefit from more detailed descriptions of the experimental setup and metrics used.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel ideas and contributes to the field, the clarity is compromised due to inconsistent terminology and lack of definitions for critical acronyms and metrics. The overall quality of writing can be improved through careful proofreading to eliminate grammatical errors and enhance readability. In terms of reproducibility, the paper does provide some details about the experimental setup; however, the lack of consistent notation and definitions may make it challenging for others to replicate the work without additional context.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to enhancing temporal knowledge embeddings by integrating contextualized language representations. While the contributions are significant, clarity issues and inconsistent terminology detract from the overall impact. Addressing these concerns would strengthen the manuscript considerably.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces ECOLA, a novel framework for extracting and representing temporal knowledge within knowledge graphs. The authors propose a methodology that leverages temporal embeddings to enhance knowledge graph completion and reasoning tasks. The findings demonstrate that ECOLA outperforms existing models in various benchmarks, showcasing improvements in accuracy and efficiency for temporal knowledge extraction.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating temporal information into knowledge graphs, offering a systematic evaluation against existing methods. However, there are notable weaknesses, including a limited exploration of the relationship between temporal knowledge graphs and other knowledge representation forms, such as visual or sensory data. Additionally, the paper lacks discussion on adapting ECOLA to incorporate dynamic, real-time data sources, and broader temporal phenomena like cyclical events. Furthermore, the model does not address the handling of uncertainty in temporal relationships or the implications of cultural context, which could limit its applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings clearly. However, the novelty could be enhanced by a deeper discussion of its integration with other machine learning paradigms and how it could scale to larger datasets. The reproducibility of the results could also be improved by including a more comprehensive evaluation across diverse domains and datasets. The absence of a detailed comparison with other emerging frameworks further detracts from the overall clarity of ECOLA's unique contributions.\n\n# Summary Of The Review\nOverall, the paper presents a promising framework for temporal knowledge extraction, but it would benefit from addressing several key areas, including broader applicability, handling of uncertainty, and comparisons with other models. Enhancements in these areas could significantly strengthen the contributions of ECOLA.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces ECOLA, a novel model designed to enhance temporal knowledge embeddings by integrating contextualized language representations. It presents three new datasets specifically tailored for evaluating temporal knowledge graphs (tKGs) in conjunction with textual data. The methodology includes benchmarking ECOLA against several state-of-the-art models, employing metrics such as Mean Reciprocal Rank (MRR) and Hits@k to assess performance. The findings indicate a remarkable improvement in performance, with a reported increase of up to 287% in Hits@1 compared to existing models, although the paper should provide more robust statistical validation of these results.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to integrating contextualized language representations into temporal knowledge embeddings, which is a significant advancement in the field. Additionally, the introduction of new datasets enhances the evaluation landscape for tKGs. However, a notable weakness is the insufficient statistical validation of the reported performance improvements. The paper lacks detailed statistical analysis and significance testing, which is crucial for substantiating the claims made regarding ECOLA's performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear, with a logical structure that outlines the contributions and methodology effectively. However, the quality of the empirical validation could be improved by including comprehensive statistical analyses and significance tests to support the findings. While the novelty of the approach is commendable, the reproducibility of the results may be compromised without explicit details on the statistical methodologies used for performance assessment and comparison.\n\n# Summary Of The Review\nOverall, the paper presents a promising model, ECOLA, that significantly advances the integration of contextualized language representations in temporal knowledge embeddings. However, the lack of thorough statistical validation of the results diminishes the overall strength of the claims, necessitating further refinement in this area.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel approach for temporal knowledge graph (tKG) completion. It introduces three new datasets designed specifically for training and evaluation in this domain. The methodology focuses on employing textual knowledge alongside tKG quadruples to enhance the model's predictive performance. The findings indicate that ECOLA achieves improvements over existing benchmarks, although the evaluation is limited to the tKG completion task.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of new datasets that can facilitate further research in temporal knowledge embeddings. Additionally, the proposed method shows promising results in its specific task. However, the paper has significant weaknesses, including a limited evaluation scope that overlooks other important applications of temporal knowledge embeddings, such as temporal question answering or event prediction. The reliance on newly constructed datasets raises concerns about generalizability, and the lack of robustness checks limits confidence in the model's performance across varied contexts. Moreover, the model's handling of temporal dynamics and textual knowledge could be enhanced, and there is insufficient exploration of more complex temporal relationships or user-centric evaluations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, although certain aspects of the methodology and results could benefit from more detailed explanations. The novelty of the approach is notable, particularly in the context of the newly introduced datasets. However, reproducibility may be a concern due to the lack of comprehensive robustness checks and the vague validation of the model-agnostic claim. The paper could improve its clarity by providing more explicit frameworks for future work and methodologies for exploring complex temporal relationships.\n\n# Summary Of The Review\nOverall, the paper presents a potentially valuable contribution to the field of temporal knowledge graph completion through the introduction of new datasets and a novel methodology. However, the evaluation is limited in scope, lacking robustness checks and broader applicability, which raises concerns about its practical implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a novel approach, referred to as Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), aimed at integrating textual data into temporal knowledge graphs (tKGs). The authors propose a prediction task that aligns textual information with temporal embeddings using quadruples, along with a combination of masked language modeling and knowledge-text prediction. They introduce three new datasets to support their methodology and demonstrate improved performance metrics over existing state-of-the-art methods, particularly in the context of temporal knowledge representation.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address the integration of textual data into temporal knowledge embeddings, which is a relevant and pressing issue in the field. The introduction of three new datasets is also commendable as it provides a basis for empirical evaluation. However, the paper suffers from a lack of originality, repackaging ideas that have been previously explored without adequately acknowledging prior work. The methods employed, such as joint optimization and masked language modeling, are not particularly novel and have been utilized in other contexts before.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and the methodology is described clearly, allowing for reproducibility. However, the novelty of the contributions is questionable, as many of the concepts presented appear to be rehashed from existing literature. The authors could benefit from a more thorough discussion of related work to properly contextualize their contributions and establish their significance within the field.\n\n# Summary Of The Review\nOverall, while the paper addresses an important issue in the integration of textual data into temporal knowledge graphs and presents some empirical findings, it lacks substantial novelty and fails to adequately recognize prior work in the area. The contributions feel more like an iteration on existing ideas rather than groundbreaking advancements.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents the ECOLA model, which aims to enhance temporal knowledge graph (tKG) embeddings by incorporating textual knowledge. The authors propose a knowledge-text prediction (KTP) task as an extension of masked language modeling to facilitate this integration. They conduct extensive experiments and present new datasets for training and evaluation, demonstrating that ECOLA can leverage textual information to improve the filling of gaps in tKGs. However, the paper identifies areas for further exploration, including the use of advanced contextualized language models and alternative loss functions.\n\n# Strength And Weaknesses\nThe primary strength of this work lies in its innovative approach to combining textual knowledge with temporal knowledge graph embeddings, potentially addressing gaps in existing tKGs. The empirical results suggest improved performance, which is bolstered by the introduction of new datasets. However, the paper could benefit from a more comprehensive evaluation that includes a broader range of datasets to mitigate bias concerns. Additionally, the reliance on existing datasets like GDELT and Wiki may limit the generalizability of the findings. There is potential for further improvement through explorations of hyperparameter tuning and multi-task learning frameworks, which are not fully addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the methodology and findings. The novelty of integrating textual knowledge into tKG embeddings is significant, but the paper does not sufficiently explore more advanced language models beyond BERT, which could enhance its contributions. The reproducibility of the results may be challenged by the reliance on existing datasets and the lack of detailed hyperparameter tuning information.\n\n# Summary Of The Review\nOverall, the ECOLA model represents a promising advancement in the integration of textual knowledge with temporal knowledge graphs. While the findings are encouraging, further exploration of advanced models, loss functions, and a broader dataset integration could enhance the robustness and applicability of the approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents ECOLA, a novel framework for enhancing temporal knowledge graph (tKG) embeddings by integrating structured and unstructured knowledge. The methodology involves a model-agnostic approach, where ECOLA is applied to various existing tKG embedding models, including UTEE and DyERNIE. The findings demonstrate significant performance improvements on benchmark datasets, with ECOLA-UTEE achieving up to 95% improvement in mean reciprocal rank (MRR) and notable increases in Hits@1 and Hits@3 across the GDELT, Wiki, and DuEE datasets. Ablation studies further illustrate the importance of temporal alignment between entity representations and textual knowledge in driving these performance gains.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its substantial empirical results, which showcase ECOLA's effectiveness across multiple datasets and embedding models. The reported performance improvements are impressive, particularly the relative increases seen in MRR and Hits metrics. Additionally, the model-agnostic nature of ECOLA is a notable contribution, as it can enhance various existing models. However, a weakness is observed in the varying performance across datasets, specifically on the DuEE dataset, where ECOLA-DyERNIE does not outperform DyERNIE in Hits@3. This inconsistency may limit the generalizability of the findings and suggests that further investigation into the dataset characteristics is warranted.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents its methodology, results, and analysis. The clarity of the writing facilitates understanding of the proposed model and its contributions. In terms of quality, the experimental design, including ablation studies, is rigorous and supports the claims made. The novelty of the approach is significant, particularly in the integration of temporal and textual knowledge. However, while the benchmarks are well-established, the reproducibility of the results would benefit from providing more details on implementation and parameter settings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of temporal knowledge graph embeddings through the ECOLA framework. The empirical results demonstrate strong performance improvements across various datasets, though some inconsistencies raise questions about the model's robustness in differing contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to temporal knowledge graphs (tKGs), focusing on the development of a new framework for temporal graph completion. It introduces a unique embedding technique that captures the dynamic nature of entities and their relationships over time. The methodology includes extensive experiments to validate the effectiveness of the proposed approach against existing benchmarks, revealing significant improvements in performance metrics such as Mean Reciprocal Rank (MRR) and Hits@k.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative framework and the thorough experimental validation, which demonstrates the practical applicability of the proposed method. However, the paper suffers from clarity issues due to its dense abstract and complex sentence structures, which may hinder reader comprehension. Additionally, some technical terms are introduced without definition, potentially alienating readers who are less familiar with the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents a novel approach with promising results, its clarity could be substantially improved. The abstract and introductory sections are dense, and the frequent use of technical jargon without explanation may limit accessibility. Furthermore, the reproducibility statement, although present, lacks organization, which could make it challenging for other researchers to replicate the study. Overall, while the quality of the research is solid, the presentation needs refinement for better readability and understanding.\n\n# Summary Of The Review\nThis paper introduces a valuable contribution to the field of temporal knowledge graphs, showcasing an innovative framework with strong empirical results. However, clarity and organization issues detract from the overall impact, making it less accessible to a broader audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4134446964787686,
    -1.7524319491216562,
    -1.8392231672907562,
    -1.9059187695139483,
    -1.4904941377857086,
    -1.6143886391451356,
    -1.7982413473811978,
    -1.9616056956153094,
    -2.026635144535476,
    -1.9563924498901473,
    -1.5994053192392468,
    -1.7610482997520445,
    -1.6663906871639966,
    -1.5713958199812221,
    -1.5676926948521235,
    -1.6149003228035494,
    -2.0764214973120336,
    -1.743182454167485,
    -1.703693428761676,
    -1.7693077601763614,
    -1.979995791660206,
    -1.633210944457785,
    -1.8003097307759415,
    -1.619872833152782,
    -1.871057795354905,
    -1.8056327349507302,
    -1.7819682804801997,
    -1.7115074937651247,
    -1.7346198342510057
  ],
  "logp_cond": [
    [
      0.0,
      -2.1216600858209786,
      -2.142747570001792,
      -2.139841423720167,
      -2.152263737749659,
      -2.135186025785366,
      -2.1662179556138668,
      -2.1472020354088466,
      -2.1448794164395504,
      -2.1614606548117488,
      -2.1450937745603813,
      -2.20601657102492,
      -2.148627873841073,
      -2.161312010177274,
      -2.1426024140665128,
      -2.139034046524794,
      -2.140225923448776,
      -2.147557927840674,
      -2.1633222496566886,
      -2.135891910536466,
      -2.138012260080014,
      -2.1367025302012763,
      -2.181552141400639,
      -2.158653630407447,
      -2.1658473860649305,
      -2.1301457703866005,
      -2.1329225267921927,
      -2.1536216737217058,
      -2.186317479276618
    ],
    [
      -1.441222983060515,
      0.0,
      -1.2216121887503189,
      -1.2226573233153066,
      -1.3662905680874209,
      -1.2734389308828025,
      -1.3190341167534105,
      -1.292016857619366,
      -1.2698368583043311,
      -1.2986325821045255,
      -1.2409336670082414,
      -1.4781584449283212,
      -1.2886002605578566,
      -1.2680060600255367,
      -1.3064781098273408,
      -1.2201554778659658,
      -1.2838953368409667,
      -1.3050391460003088,
      -1.3200621223283122,
      -1.2638191219031314,
      -1.2677015980370214,
      -1.2936806847275344,
      -1.4718717054597263,
      -1.3036411763619558,
      -1.3061973786997418,
      -1.264304534721166,
      -1.2777517129074076,
      -1.3371496911524619,
      -1.4194569752754647
    ],
    [
      -1.5162266692041255,
      -1.2481045664742376,
      0.0,
      -1.265351357376531,
      -1.3538148761435667,
      -1.373543583819635,
      -1.3515860313052168,
      -1.3609860913047556,
      -1.3586402754497517,
      -1.3378470005446312,
      -1.3343393865958928,
      -1.5079883463878165,
      -1.359588400979978,
      -1.3700228916044266,
      -1.4285722506530432,
      -1.3473882735110907,
      -1.3297494160913979,
      -1.2770767297774055,
      -1.3773102529936845,
      -1.2993678979356205,
      -1.3216278725915025,
      -1.3668973578916994,
      -1.4705237546797092,
      -1.3734680310476153,
      -1.3951530682409634,
      -1.305501088458809,
      -1.3416526398927777,
      -1.358551648888279,
      -1.5259144368327087
    ],
    [
      -1.5801659458713602,
      -1.2827578561690873,
      -1.3043645395447625,
      0.0,
      -1.4209971563751187,
      -1.3803171819770015,
      -1.4428901730881913,
      -1.382130444044345,
      -1.3455775275419275,
      -1.3702658842485411,
      -1.268113731236441,
      -1.5996808301914593,
      -1.3475081473521604,
      -1.309545648471609,
      -1.4271920220454963,
      -1.3195704957383505,
      -1.371364616360458,
      -1.388129062068311,
      -1.3985180801949402,
      -1.210249021845398,
      -1.393322735119191,
      -1.3903981851593163,
      -1.544745718034577,
      -1.4121421080449463,
      -1.459071620923479,
      -1.3530032000539791,
      -1.3833634086712225,
      -1.3374098000406767,
      -1.5759585607319684
    ],
    [
      -1.3046194706575043,
      -1.205556511788235,
      -1.2150536192961234,
      -1.224821587882893,
      0.0,
      -1.224350696867651,
      -1.2581326061284421,
      -1.2031666690607647,
      -1.2431791814462745,
      -1.2413145872554021,
      -1.2139273408266147,
      -1.3203770880220385,
      -1.2348051551097432,
      -1.2391592844497665,
      -1.2178889072861594,
      -1.2459437954114987,
      -1.2170912429989642,
      -1.217078129524404,
      -1.2576666622867196,
      -1.2573397537279543,
      -1.1949627270474719,
      -1.2614406105772784,
      -1.2882931592153992,
      -1.2431574960795457,
      -1.223655796322635,
      -1.2212949471494339,
      -1.2382125174396355,
      -1.2413651858155414,
      -1.315527198861655
    ],
    [
      -1.2892381189815398,
      -1.062167719966811,
      -1.1344581095127038,
      -1.0695125676262156,
      -1.2048754557642822,
      0.0,
      -1.1778062055936158,
      -1.1587708145710844,
      -1.1035153526400698,
      -1.1951932984176556,
      -1.151556388388002,
      -1.2964076660409143,
      -1.1359465002424334,
      -1.1090747025105796,
      -1.1781160057424964,
      -1.141153148631494,
      -1.162570043912889,
      -1.1488594437937163,
      -1.1424939039037434,
      -1.1190746461478565,
      -1.172603859716799,
      -1.153648771081535,
      -1.272753733017218,
      -1.1475711026574564,
      -1.2306198632300325,
      -1.1130278436739542,
      -1.135191842778042,
      -1.1991245954230891,
      -1.277248216952919
    ],
    [
      -1.4819303003282744,
      -1.2766129643729884,
      -1.2466875223421228,
      -1.3324922155957528,
      -1.3749102271535596,
      -1.3300399756692542,
      0.0,
      -1.3862713310642856,
      -1.3099493507522642,
      -1.3358978247701874,
      -1.3194561320403606,
      -1.4552855428635232,
      -1.3098835661192845,
      -1.364193369081388,
      -1.3684919564836266,
      -1.3326189870841259,
      -1.322271913543295,
      -1.2988122942596807,
      -1.3045199438235118,
      -1.3435606700008638,
      -1.3338317061262495,
      -1.2835388941762724,
      -1.4557448873937748,
      -1.3459371846873707,
      -1.3524005064144113,
      -1.3026683140616426,
      -1.344726637840941,
      -1.3679270820464768,
      -1.476085662643725
    ],
    [
      -1.6205195924678775,
      -1.460007432375368,
      -1.4308372264327973,
      -1.45012453080573,
      -1.4324864965317243,
      -1.4615196941004556,
      -1.5688798709689213,
      0.0,
      -1.4368335671655996,
      -1.4827265481841052,
      -1.4441727961896735,
      -1.6090980374994033,
      -1.4957949148741279,
      -1.47039608586602,
      -1.4557922767413447,
      -1.4807257616971146,
      -1.4423847669518233,
      -1.45578211926114,
      -1.5651662145905623,
      -1.5125169081250849,
      -1.4163222125931008,
      -1.4971047094648993,
      -1.5415700834489539,
      -1.4587934039981996,
      -1.536806756584445,
      -1.438825472226628,
      -1.42937664929091,
      -1.4611749440254878,
      -1.5538444873144992
    ],
    [
      -1.6688639565308099,
      -1.452906263530617,
      -1.4993525498502576,
      -1.4223531649995265,
      -1.5966619752055242,
      -1.5093506668072816,
      -1.5675674837404794,
      -1.5369757857476498,
      0.0,
      -1.5580707546507786,
      -1.4845037644381145,
      -1.6943919278533028,
      -1.4956898852530713,
      -1.5023851302855666,
      -1.5538098323969005,
      -1.4675394850438703,
      -1.5188088729900076,
      -1.5065467571599154,
      -1.5206216701176993,
      -1.502752022336734,
      -1.556313493495692,
      -1.515418828330433,
      -1.6169965175358163,
      -1.5161599108704964,
      -1.585057561125461,
      -1.4682790256138731,
      -1.504544109929553,
      -1.5689498838790163,
      -1.6565512257280322
    ],
    [
      -1.618280162374817,
      -1.3897795864888487,
      -1.3920216924073459,
      -1.419399708951099,
      -1.495318110672022,
      -1.5068362447467472,
      -1.5471905153473624,
      -1.484952711030807,
      -1.4704073020473507,
      0.0,
      -1.4343161233439157,
      -1.6695902686716872,
      -1.5158392311983564,
      -1.4123402140687231,
      -1.5637486552186564,
      -1.4391390808783227,
      -1.4066900345859166,
      -1.457199203963239,
      -1.5052823602893501,
      -1.4727815231964279,
      -1.39072942335128,
      -1.5262069297226084,
      -1.5783540482765326,
      -1.4986220694567876,
      -1.4918569400121076,
      -1.4536842360636242,
      -1.4307334271181555,
      -1.4823067840740767,
      -1.624243723915606
    ],
    [
      -1.3261941505305144,
      -1.1265365398718072,
      -1.1316512373479175,
      -1.081399226549476,
      -1.1703591487198568,
      -1.155954031831816,
      -1.1831741441202681,
      -1.1533376159893731,
      -1.1584955597147457,
      -1.1693852028931302,
      0.0,
      -1.3150152019058214,
      -1.1381173144667305,
      -1.1356547037432438,
      -1.1797762232431481,
      -1.1407645425360522,
      -1.1677573129776508,
      -1.1590393342492962,
      -1.2319949936080903,
      -1.1856679877020286,
      -1.1029531468522051,
      -1.1883351186093232,
      -1.3061455201014587,
      -1.193356127443758,
      -1.2143762109676914,
      -1.1085783097643374,
      -1.1768945110296096,
      -1.2183030015386787,
      -1.305597068983622
    ],
    [
      -1.4875420936002561,
      -1.4693455614344417,
      -1.426505723172648,
      -1.4337391796590744,
      -1.4282074636543853,
      -1.419566846368865,
      -1.4150157724943664,
      -1.4377830965907934,
      -1.4226393514533933,
      -1.4532774300626552,
      -1.4302170666379384,
      0.0,
      -1.4254506248170409,
      -1.4716089125682172,
      -1.4214207130709566,
      -1.4480646090392253,
      -1.4524982061316662,
      -1.4522913173963996,
      -1.419625528703476,
      -1.449871439069886,
      -1.43008422323068,
      -1.403325926581125,
      -1.4026047021800108,
      -1.4365612875018199,
      -1.3708872165966255,
      -1.4360738631581094,
      -1.409676339676505,
      -1.4504287255718424,
      -1.4251247294968035
    ],
    [
      -1.389980745480586,
      -1.1716787442071905,
      -1.1631721149192151,
      -1.1744207252932997,
      -1.2543870033062754,
      -1.2347836129728615,
      -1.1911359174246183,
      -1.205185149381853,
      -1.1778605878055433,
      -1.241058908117568,
      -1.1240430722119312,
      -1.352974476154247,
      0.0,
      -1.15891793060391,
      -1.2244617037930836,
      -1.188365737795485,
      -1.220241450553326,
      -1.1809856108873287,
      -1.1902864972424658,
      -1.2865489860900223,
      -1.1939095085576443,
      -1.1744910212535027,
      -1.3528736873485803,
      -1.252076300022116,
      -1.2295959360999773,
      -1.199899300812258,
      -1.200311239469887,
      -1.257152273313727,
      -1.354062391633709
    ],
    [
      -1.2700822737568411,
      -1.0003385895933494,
      -1.116577662385431,
      -1.0535044095179893,
      -1.1579466625511534,
      -1.1021690585009907,
      -1.1468262134753247,
      -1.0991375208390828,
      -1.0623415210369007,
      -1.0828290562482192,
      -1.0749602620455398,
      -1.2945202901751844,
      -1.0961260860138142,
      0.0,
      -1.1113252187363902,
      -1.0639726886052272,
      -1.0936046229705039,
      -1.097086213346659,
      -1.1202950624790422,
      -1.0846655038797575,
      -1.0987157362104103,
      -1.0890983280555808,
      -1.250749924069532,
      -1.1062223163154752,
      -1.185254825469777,
      -1.0472568493756726,
      -1.0954238948072876,
      -1.1496183991018225,
      -1.2700071966709565
    ],
    [
      -1.292848234258108,
      -1.0910091167152438,
      -1.1400151836394894,
      -1.1231992002832711,
      -1.1626578921516801,
      -1.1474657378874717,
      -1.1510498517122303,
      -1.1085347916391097,
      -1.1419250630050795,
      -1.222986359774092,
      -1.1312829580736548,
      -1.250576999627382,
      -1.126054655936013,
      -1.1314542444443596,
      0.0,
      -1.1691747277133608,
      -1.132389045197313,
      -1.1631629164641837,
      -1.1128503524926063,
      -1.1573367684639742,
      -1.1460811048111723,
      -1.0832895006723076,
      -1.2342232927753,
      -1.164894360712622,
      -1.124108046099876,
      -1.086218610001193,
      -1.1459701339724944,
      -1.1611883735410748,
      -1.2615628140738002
    ],
    [
      -1.3100524416762798,
      -1.0068786868121138,
      -1.103283467478762,
      -1.0643511072222742,
      -1.1711879155077423,
      -1.1414339657140484,
      -1.1722591840625285,
      -1.1382154463419671,
      -1.081060260855521,
      -1.1143162883797268,
      -1.0141459780274351,
      -1.3100623261927014,
      -1.1102778458924334,
      -1.0714022417306355,
      -1.1588139250733014,
      0.0,
      -1.1357868490499163,
      -1.1776204058519602,
      -1.1484464653416442,
      -1.1000539255421355,
      -1.0759248691001442,
      -1.160485216344609,
      -1.2063433730489437,
      -1.134078159518284,
      -1.1761669029873307,
      -1.066452616421522,
      -1.113304096611042,
      -1.1821039119705776,
      -1.2631352143764558
    ],
    [
      -1.7628840574262108,
      -1.5850129084868831,
      -1.5463092436301247,
      -1.5922841501318514,
      -1.63921555959215,
      -1.663849577940734,
      -1.6931031040804736,
      -1.6214954710590643,
      -1.5911893635315513,
      -1.6083337872084853,
      -1.6289673949362635,
      -1.7823072262241644,
      -1.605470136480908,
      -1.6417015273288407,
      -1.6786897192625692,
      -1.6201860884130708,
      0.0,
      -1.5585012974298174,
      -1.7274676834237244,
      -1.626650873006354,
      -1.577089877263037,
      -1.615536359266095,
      -1.7263343459305973,
      -1.6679383686771667,
      -1.6375550504563172,
      -1.5986269361229855,
      -1.5436235772154605,
      -1.6271977585094133,
      -1.7170909860581642
    ],
    [
      -1.4028363244493987,
      -1.1544180995428908,
      -1.062425098346938,
      -1.1804906718554142,
      -1.202003996753282,
      -1.208906168505421,
      -1.1744449473046998,
      -1.200825508000499,
      -1.1421821264377032,
      -1.2099072100707797,
      -1.1777262595453282,
      -1.4108707595930243,
      -1.1821245654011512,
      -1.1593104456298835,
      -1.2592724905540615,
      -1.1896981760808814,
      -1.1452096539291299,
      0.0,
      -1.2038359164978063,
      -1.1923158309960202,
      -1.1419697638684183,
      -1.1898294411213572,
      -1.3236813576538093,
      -1.1325352522208525,
      -1.2477908877019876,
      -1.1747394912055358,
      -1.1746612791121052,
      -1.166379719739341,
      -1.3495695277351138
    ],
    [
      -1.3901180254009458,
      -1.2149999631764767,
      -1.198869668141123,
      -1.2296978359319606,
      -1.3080792831914716,
      -1.2989740146856599,
      -1.2381215971205095,
      -1.2704402357325195,
      -1.2080980723355657,
      -1.281374779417303,
      -1.2615730389384783,
      -1.3968837385632473,
      -1.1932912409243133,
      -1.2342675893265702,
      -1.2357529661551936,
      -1.2224378205643103,
      -1.2430466020012438,
      -1.2363758945577104,
      0.0,
      -1.2275338726347491,
      -1.2549595058263108,
      -1.2137471157896782,
      -1.3782093228842973,
      -1.265727650196928,
      -1.242894826032509,
      -1.2296200816530645,
      -1.2561109355508204,
      -1.2701245287017409,
      -1.4022849485068585
    ],
    [
      -1.4241238259136277,
      -1.163956064105003,
      -1.2063244153220913,
      -1.1039724169396883,
      -1.363530449599382,
      -1.254575770998318,
      -1.309164266226541,
      -1.3025490106622584,
      -1.2398567874728543,
      -1.2723328408894472,
      -1.2542050570512973,
      -1.4650894325476052,
      -1.3095902606550642,
      -1.2626690095404245,
      -1.3191181344121412,
      -1.2251847714320234,
      -1.2527571461752607,
      -1.2744977475785018,
      -1.2831396951833736,
      0.0,
      -1.2494438267920227,
      -1.2623191809450163,
      -1.3915858262606777,
      -1.3103892148780538,
      -1.3328561790218552,
      -1.1934525331081791,
      -1.2161663922352386,
      -1.3164741344172715,
      -1.4571532481237393
    ],
    [
      -1.6386791442709963,
      -1.4927125255068991,
      -1.4370201882802007,
      -1.4932506230611942,
      -1.551270876608391,
      -1.5386984258296126,
      -1.5534737277224853,
      -1.4569805313786055,
      -1.5418113183632236,
      -1.510491320546989,
      -1.4749212742622109,
      -1.7010679191049107,
      -1.520069900888979,
      -1.5004420363271656,
      -1.5722183630070816,
      -1.5134724467829523,
      -1.4813435925570122,
      -1.4514006952542158,
      -1.5611390387398398,
      -1.5204526937732183,
      0.0,
      -1.528049623522023,
      -1.593455363546325,
      -1.5071302491316572,
      -1.4818955979058024,
      -1.4440858377290329,
      -1.494893387945324,
      -1.5380218341691698,
      -1.6555816761535491
    ],
    [
      -1.3410685946456928,
      -1.1198958519872468,
      -1.1329828956746741,
      -1.16127468513907,
      -1.2506313787525767,
      -1.1926574835809307,
      -1.1749062928895775,
      -1.2125402853229827,
      -1.1248314455808859,
      -1.2249324156517454,
      -1.1983995540884689,
      -1.3380770459065432,
      -1.1457573946989004,
      -1.2053419934508114,
      -1.1828551497991053,
      -1.1817880613399974,
      -1.160000047790674,
      -1.1822992952621492,
      -1.1418137825394583,
      -1.1905339527787002,
      -1.2182749486654332,
      0.0,
      -1.3113954151213925,
      -1.2052509978158767,
      -1.2039524354708184,
      -1.1013415883693611,
      -1.1145572961596326,
      -1.2380661977391654,
      -1.3513760247653341
    ],
    [
      -1.4757091157776272,
      -1.4254997549249273,
      -1.4237408133890004,
      -1.4164166067296993,
      -1.4103832552342994,
      -1.418491777298145,
      -1.4528736047086417,
      -1.4086424665676427,
      -1.4072698487584523,
      -1.3889952753443366,
      -1.4085030631848587,
      -1.4570561013867842,
      -1.4413374942291817,
      -1.434411272157005,
      -1.465496257262899,
      -1.3951328475484313,
      -1.3785239918312944,
      -1.3827433614598514,
      -1.4730858211614943,
      -1.4276208731083968,
      -1.3661673561020378,
      -1.4607743338588843,
      0.0,
      -1.4044440901997153,
      -1.3704026537552576,
      -1.409051404498961,
      -1.389074046820782,
      -1.4266513310954287,
      -1.470639880601833
    ],
    [
      -1.3075232300361153,
      -1.1442104607414545,
      -1.1041660391064814,
      -1.1691078583851562,
      -1.2140442998163674,
      -1.1637533166692349,
      -1.208517422895975,
      -1.1395787961264543,
      -1.0921115905126508,
      -1.1917909350285545,
      -1.1713678497911844,
      -1.3265515915053188,
      -1.2359144082576454,
      -1.1727122902748,
      -1.206494267791501,
      -1.14270841125113,
      -1.1404753213302,
      -1.075541497088473,
      -1.2014954232200616,
      -1.2015001033958639,
      -1.10925216067095,
      -1.1479355660354542,
      -1.2804219084654078,
      0.0,
      -1.1720523544526735,
      -1.1203093567730908,
      -1.1949980215667688,
      -1.1652886507426954,
      -1.2150323899784676
    ],
    [
      -1.5890734984935657,
      -1.443493802261694,
      -1.4144750467928051,
      -1.4786737691860137,
      -1.4543022153345506,
      -1.5033962260961031,
      -1.4934955580506675,
      -1.4623508719702933,
      -1.485998908004621,
      -1.4845900023239738,
      -1.4752069409098783,
      -1.5660672613152131,
      -1.445204094986537,
      -1.5169728040971275,
      -1.494361497675999,
      -1.4813696603390694,
      -1.454531034927251,
      -1.4779376518007041,
      -1.4521202651897915,
      -1.529403992895676,
      -1.3503286815418383,
      -1.4760834986047058,
      -1.5237174612363171,
      -1.469422864121498,
      0.0,
      -1.467550090945817,
      -1.44825883231553,
      -1.4772743025445052,
      -1.5701471536181733
    ],
    [
      -1.4251384315613298,
      -1.230757642821439,
      -1.2358051370050713,
      -1.2228636601439034,
      -1.3480851233938702,
      -1.2397578059746306,
      -1.3002326460686613,
      -1.276677542980558,
      -1.19533248726455,
      -1.2522434042775858,
      -1.2446115024281348,
      -1.4837854954076404,
      -1.2860551230034356,
      -1.278976252987861,
      -1.3328936860578384,
      -1.2267548440034768,
      -1.2552568864765046,
      -1.2615862192156446,
      -1.2583241382813317,
      -1.2866042771143364,
      -1.2251359460190014,
      -1.227664338970497,
      -1.3888475309705608,
      -1.2927231909462114,
      -1.3360120893686818,
      0.0,
      -1.2746337869657391,
      -1.3529091105387414,
      -1.4582448213102306
    ],
    [
      -1.4271165934643155,
      -1.2841472411251333,
      -1.2654959692519858,
      -1.3099103454702545,
      -1.345104725081136,
      -1.3712577779299122,
      -1.4030057779146847,
      -1.3327615173378478,
      -1.3242792158006922,
      -1.3186753451587956,
      -1.33506785668455,
      -1.4644645117301442,
      -1.344360397912839,
      -1.3345657559474562,
      -1.3944095406138521,
      -1.339710324383326,
      -1.288080341031507,
      -1.3598232797315548,
      -1.374068601945139,
      -1.3168637025849796,
      -1.2836668427871831,
      -1.2935502471931353,
      -1.400398768176821,
      -1.3685990600177493,
      -1.3396533559678996,
      -1.3143378534266823,
      0.0,
      -1.3393847382407185,
      -1.4788278546822353
    ],
    [
      -1.4486194468820222,
      -1.3270201207429626,
      -1.277421873593053,
      -1.25101713519161,
      -1.3400873821619512,
      -1.34432732304677,
      -1.3740727583288566,
      -1.3248783604712522,
      -1.3185258665023623,
      -1.29953623678756,
      -1.3508370045221638,
      -1.4438487897715655,
      -1.34946063285744,
      -1.311711749458849,
      -1.337965471843962,
      -1.3157097259082373,
      -1.3219972610268653,
      -1.3210903715344338,
      -1.3624469829396237,
      -1.3091790551116274,
      -1.3081280576675565,
      -1.3742911121233015,
      -1.373046490554118,
      -1.312380766329069,
      -1.3422667876095733,
      -1.3471883898030985,
      -1.3142900668123505,
      0.0,
      -1.4196514432783813
    ],
    [
      -1.322836942871201,
      -1.3090502150445325,
      -1.3144032077293362,
      -1.3043580319183767,
      -1.3345422928652488,
      -1.3326625940072598,
      -1.3319651453765713,
      -1.3109525620895222,
      -1.2927624409977607,
      -1.307183855705523,
      -1.2891238724237146,
      -1.3538055928777084,
      -1.319989753036657,
      -1.3318699337327335,
      -1.3368305746273608,
      -1.3149962927162289,
      -1.2626040919968817,
      -1.3220074600828584,
      -1.3553813809512942,
      -1.3049760719707202,
      -1.3360236308639548,
      -1.2563245193094161,
      -1.3357751025748588,
      -1.2653715932804843,
      -1.3055300201711235,
      -1.3039959608572576,
      -1.3072238454826075,
      -1.2920935688230473,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.29178461065778993,
      0.27069712647697663,
      0.2736032727586015,
      0.2611809587291094,
      0.2782586706934027,
      0.24722674086490182,
      0.266242661069922,
      0.2685652800392182,
      0.2519840416670198,
      0.2683509219183873,
      0.20742812545384837,
      0.26481682263769546,
      0.25213268630149477,
      0.27084228241225583,
      0.2744106499539747,
      0.2732187730299924,
      0.2658867686380946,
      0.25012244682207996,
      0.27755278594230237,
      0.27543243639875437,
      0.27674216627749226,
      0.23189255507812945,
      0.25479106607132174,
      0.24759731041383803,
      0.2832989260921681,
      0.28052216968657584,
      0.2598230227570628,
      0.2271272172021508
    ],
    [
      0.31120896606114123,
      0.0,
      0.5308197603713374,
      0.5297746258063496,
      0.38614138103423534,
      0.47899301823885376,
      0.4333978323682457,
      0.4604150915022902,
      0.4825950908173251,
      0.45379936701713075,
      0.5114982821134149,
      0.27427350419333507,
      0.46383168856379964,
      0.4844258890961195,
      0.4459538392943154,
      0.5322764712556904,
      0.46853661228068955,
      0.44739280312134744,
      0.432369826793344,
      0.48861282721852484,
      0.4847303510846348,
      0.45875126439412184,
      0.2805602436619299,
      0.4487907727597005,
      0.4462345704219144,
      0.4881274144004901,
      0.4746802362142486,
      0.4152822579691944,
      0.33297497384619157
    ],
    [
      0.32299649808663067,
      0.5911186008165186,
      0.0,
      0.5738718099142253,
      0.48540829114718953,
      0.46567958347112115,
      0.48763713598553937,
      0.4782370759860006,
      0.48058289184100444,
      0.501376166746125,
      0.5048837806948634,
      0.33123482090293965,
      0.4796347663107783,
      0.4692002756863296,
      0.410650916637713,
      0.4918348937796655,
      0.5094737511993583,
      0.5621464375133507,
      0.4619129142970717,
      0.5398552693551357,
      0.5175952946992537,
      0.4723258093990568,
      0.368699412611047,
      0.4657551362431409,
      0.4440700990497928,
      0.5337220788319472,
      0.49757052739797847,
      0.48067151840247724,
      0.3133087304580475
    ],
    [
      0.3257528236425882,
      0.623160913344861,
      0.6015542299691858,
      0.0,
      0.48492161313882964,
      0.5256015875369469,
      0.463028596425757,
      0.5237883254696034,
      0.5603412419720208,
      0.5356528852654072,
      0.6378050382775073,
      0.306237939322489,
      0.5584106221617879,
      0.5963731210423393,
      0.478726747468452,
      0.5863482737755978,
      0.5345541531534903,
      0.5177897074456372,
      0.5074006893190082,
      0.6956697476685503,
      0.5125960343947573,
      0.515520584354632,
      0.3611730514793714,
      0.493776661469002,
      0.44684714859046926,
      0.5529155694599692,
      0.5225553608427258,
      0.5685089694732717,
      0.32996020878197996
    ],
    [
      0.18587466712820433,
      0.28493762599747363,
      0.2754405184895852,
      0.26567254990281564,
      0.0,
      0.26614344091805764,
      0.23236153165726647,
      0.28732746872494386,
      0.24731495633943412,
      0.2491795505303065,
      0.2765667969590939,
      0.17011704976367015,
      0.2556889826759654,
      0.2513348533359421,
      0.2726052304995492,
      0.24455034237420992,
      0.2734028947867444,
      0.27341600826130463,
      0.23282747549898897,
      0.23315438405775435,
      0.29553141073823674,
      0.2290535272084302,
      0.2022009785703094,
      0.24733664170616287,
      0.2668383414630735,
      0.26919919063627473,
      0.25228162034607315,
      0.24912895197016716,
      0.17496693892405357
    ],
    [
      0.3251505201635958,
      0.5522209191783247,
      0.4799305296324319,
      0.54487607151892,
      0.4095131833808534,
      0.0,
      0.4365824335515198,
      0.4556178245740512,
      0.5108732865050658,
      0.41919534072748,
      0.46283225075713363,
      0.31798097310422135,
      0.47844213890270226,
      0.505313936634556,
      0.4362726334026392,
      0.47323549051364155,
      0.45181859523224666,
      0.46552919535141934,
      0.47189473524139225,
      0.4953139929972792,
      0.4417847794283367,
      0.46073986806360057,
      0.3416349061279176,
      0.4668175364876792,
      0.38376877591510317,
      0.5013607954711814,
      0.47919679636709356,
      0.4152640437220465,
      0.33714042219221674
    ],
    [
      0.31631104705292334,
      0.5216283830082094,
      0.551553825039075,
      0.46574913178544497,
      0.4233311202276382,
      0.46820137171194354,
      0.0,
      0.4119700163169122,
      0.48829199662893363,
      0.4623435226110104,
      0.4787852153408372,
      0.3429558045176746,
      0.4883577812619133,
      0.4340479782998097,
      0.42974939089757114,
      0.46562236029707194,
      0.4759694338379028,
      0.4994290531215171,
      0.493721403557686,
      0.454680677380334,
      0.4644096412549483,
      0.5147024532049254,
      0.34249645998742295,
      0.4523041626938271,
      0.44584084096678644,
      0.49557303331955516,
      0.45351470954025674,
      0.43031426533472095,
      0.3221556847374727
    ],
    [
      0.34108610314743193,
      0.5015982632399414,
      0.5307684691825121,
      0.5114811648095794,
      0.5291191990835851,
      0.5000860015148538,
      0.39272582464638806,
      0.0,
      0.5247721284497098,
      0.4788791474312042,
      0.5174328994256359,
      0.3525076581159061,
      0.46581078074118154,
      0.4912096097492893,
      0.5058134188739647,
      0.48087993391819484,
      0.5192209286634861,
      0.5058235763541694,
      0.39643948102474713,
      0.4490887874902245,
      0.5452834830222086,
      0.46450098615041013,
      0.42003561216635554,
      0.5028122916171098,
      0.4247989390308644,
      0.5227802233886814,
      0.5322290463243995,
      0.5004307515898216,
      0.4077612083008102
    ],
    [
      0.35777118800466634,
      0.5737288810048593,
      0.5272825946852187,
      0.6042819795359498,
      0.429973169329952,
      0.5172844777281946,
      0.45906766079499683,
      0.4896593587878264,
      0.0,
      0.4685643898846976,
      0.5421313800973617,
      0.3322432166821734,
      0.5309452592824049,
      0.5242500142499096,
      0.4728253121385757,
      0.5590956594916059,
      0.5078262715454687,
      0.5200883873755608,
      0.5060134744177769,
      0.5238831221987421,
      0.4703216510397843,
      0.5112163162050432,
      0.4096386269996599,
      0.5104752336649798,
      0.4415775834100153,
      0.5583561189216031,
      0.5220910346059233,
      0.45768526065645987,
      0.37008391880744407
    ],
    [
      0.3381122875153302,
      0.5666128634012986,
      0.5643707574828014,
      0.5369927409390483,
      0.46107433921812535,
      0.4495562051434001,
      0.40920193454278486,
      0.4714397388593403,
      0.48598514784279656,
      0.0,
      0.5220763265462316,
      0.28680218121846,
      0.44055321869179087,
      0.5440522358214241,
      0.3926437946714909,
      0.5172533690118246,
      0.5497024153042307,
      0.49919324592690817,
      0.4511100896007971,
      0.4836109266937194,
      0.5656630265388674,
      0.43018552016753886,
      0.3780384016136147,
      0.4577703804333597,
      0.4645355098780397,
      0.5027082138265231,
      0.5256590227719917,
      0.4740856658160706,
      0.33214872597454126
    ],
    [
      0.2732111687087324,
      0.47286877936743954,
      0.4677540818913293,
      0.5180060926897707,
      0.42904617051939,
      0.4434512874074308,
      0.41623117511897867,
      0.44606770324987366,
      0.44090975952450107,
      0.4300201163461166,
      0.0,
      0.28439011733342534,
      0.4612880047725163,
      0.46375061549600294,
      0.41962909599609866,
      0.4586407767031946,
      0.43164800626159594,
      0.4403659849899506,
      0.3674103256311565,
      0.41373733153721814,
      0.49645217238704165,
      0.4110702006299236,
      0.2932597991377881,
      0.4060491917954887,
      0.38502910827155534,
      0.4908270094749094,
      0.42251080820963716,
      0.3811023177005681,
      0.29380825025562474
    ],
    [
      0.27350620615178833,
      0.29170273831760274,
      0.33454257657939657,
      0.3273091200929701,
      0.33284083609765913,
      0.34148145338317937,
      0.3460325272576781,
      0.32326520316125107,
      0.3384089482986512,
      0.30777086968938927,
      0.3308312331141061,
      0.0,
      0.3355976749350036,
      0.2894393871838272,
      0.33962758668108783,
      0.3129836907128192,
      0.3085500936203782,
      0.30875698235564486,
      0.34142277104856844,
      0.31117686068215855,
      0.3309640765213644,
      0.35772237317091937,
      0.3584435975720337,
      0.3244870122502246,
      0.390161083155419,
      0.3249744365939351,
      0.3513719600755394,
      0.31061957418020203,
      0.335923570255241
    ],
    [
      0.2764099416834107,
      0.4947119429568061,
      0.5032185722447815,
      0.49196996187069697,
      0.41200368385772124,
      0.4316070741911351,
      0.4752547697393783,
      0.46120553778214357,
      0.4885300993584534,
      0.4253317790464286,
      0.5423476149520654,
      0.3134162110097496,
      0.0,
      0.5074727565600867,
      0.441928983370913,
      0.4780249493685116,
      0.44614923661067074,
      0.48540507627666796,
      0.4761041899215308,
      0.3798417010739743,
      0.4724811786063523,
      0.49189966591049394,
      0.31351699981541636,
      0.4143143871418806,
      0.4367947510640193,
      0.4664913863517386,
      0.4660794476941097,
      0.4092384138502696,
      0.31232829553028774
    ],
    [
      0.301313546224381,
      0.5710572303878727,
      0.45481815759579103,
      0.5178914104632328,
      0.4134491574300687,
      0.46922676148023146,
      0.4245696065058975,
      0.47225829914213935,
      0.5090542989443214,
      0.4885667637330029,
      0.4964355579356823,
      0.2768755298060377,
      0.4752697339674079,
      0.0,
      0.4600706012448319,
      0.5074231313759949,
      0.47779119701071826,
      0.4743096066345631,
      0.4511007575021799,
      0.48673031610146467,
      0.47268008377081183,
      0.4822974919256413,
      0.32064589591169024,
      0.46517350366574695,
      0.3861409945114451,
      0.5241389706055495,
      0.4759719251739345,
      0.4217774208793996,
      0.3013886233102656
    ],
    [
      0.27484446059401546,
      0.47668357813687967,
      0.4276775112126341,
      0.44449349456885234,
      0.40503480270044334,
      0.42022695696465173,
      0.4166428431398932,
      0.45915790321301375,
      0.425767631847044,
      0.3447063350780315,
      0.4364097367784687,
      0.3171156952247416,
      0.44163803891611053,
      0.4362384504077639,
      0.0,
      0.39851796713876264,
      0.4353036496548104,
      0.4045297783879398,
      0.45484234235951715,
      0.41035592638814933,
      0.42161159004095117,
      0.4844031941798159,
      0.3334694020768234,
      0.4027983341395014,
      0.44358464875224746,
      0.48147408485093046,
      0.4217225608796291,
      0.40650432131104863,
      0.30612988077832326
    ],
    [
      0.3048478811272697,
      0.6080216359914357,
      0.5116168553247875,
      0.5505492155812752,
      0.4437124072958072,
      0.473466357089501,
      0.44264113874102096,
      0.4766848764615823,
      0.5338400619480284,
      0.5005840344238226,
      0.6007543447761143,
      0.3048379966108481,
      0.5046224769111161,
      0.543498081072914,
      0.45608639773024806,
      0.0,
      0.4791134737536331,
      0.4372799169515893,
      0.46645385746190526,
      0.5148463972614139,
      0.5389754537034053,
      0.45441510645894034,
      0.40855694975460577,
      0.4808221632852654,
      0.4387334198162187,
      0.5484477063820274,
      0.5015962261925075,
      0.4327964108329718,
      0.35176510842709363
    ],
    [
      0.31353743988582283,
      0.4914085888251505,
      0.530112253681909,
      0.4841373471801822,
      0.4372059377198836,
      0.41257191937129956,
      0.38331839323156003,
      0.4549260262529693,
      0.48523213378048236,
      0.4680877101035483,
      0.4474541023757701,
      0.29411427108786925,
      0.47095136083112554,
      0.43471996998319296,
      0.39773177804946447,
      0.45623540889896286,
      0.0,
      0.5179201998822163,
      0.34895381388830926,
      0.4497706243056796,
      0.49933162004899656,
      0.46088513804593867,
      0.3500871513814363,
      0.4084831286348669,
      0.4388664468557164,
      0.4777945611890482,
      0.5327979200965731,
      0.44922373880262034,
      0.3593305112538694
    ],
    [
      0.3403461297180863,
      0.5887643546245942,
      0.6807573558205469,
      0.5626917823120707,
      0.5411784574142029,
      0.534276285662064,
      0.5687375068627851,
      0.5423569461669859,
      0.6010003277297817,
      0.5332752440967052,
      0.5654561946221568,
      0.33231169457446064,
      0.5610578887663338,
      0.5838720085376015,
      0.4839099636134234,
      0.5534842780866036,
      0.5979728002383551,
      0.0,
      0.5393465376696787,
      0.5508666231714647,
      0.6012126902990667,
      0.5533530130461277,
      0.4195010965136756,
      0.6106472019466325,
      0.4953915664654973,
      0.5684429629619492,
      0.5685211750553798,
      0.5768027344281439,
      0.3936129264323711
    ],
    [
      0.3135754033607303,
      0.4886934655851993,
      0.504823760620553,
      0.4739955928297155,
      0.39561414557020447,
      0.4047194140760162,
      0.46557183164116656,
      0.43325319302915655,
      0.4955953564261104,
      0.42231864934437313,
      0.4421203898231978,
      0.30680969019842874,
      0.5104021878373628,
      0.46942583943510585,
      0.4679404626064825,
      0.4812556081973658,
      0.4606468267604322,
      0.4673175342039657,
      0.0,
      0.47615955612692695,
      0.4487339229353653,
      0.48994631297199787,
      0.32548410587737875,
      0.43796577856474816,
      0.46079860272916706,
      0.4740733471086116,
      0.44758249321085564,
      0.4335689000599352,
      0.3014084802548176
    ],
    [
      0.34518393426273364,
      0.6053516960713583,
      0.56298334485427,
      0.6653353432366731,
      0.4057773105769793,
      0.5147319891780433,
      0.46014349394982035,
      0.466758749514103,
      0.529450972703507,
      0.4969749192869142,
      0.5151027031250641,
      0.3042183276287562,
      0.45971749952129715,
      0.5066387506359369,
      0.45018962576422017,
      0.544122988744338,
      0.5165506140011007,
      0.4948100125978596,
      0.4861680649929878,
      0.0,
      0.5198639333843387,
      0.5069885792313451,
      0.37772193391568365,
      0.45891854529830756,
      0.4364515811545062,
      0.5758552270681823,
      0.5531413679411228,
      0.4528336257590899,
      0.3121545120526221
    ],
    [
      0.34131664738920975,
      0.48728326615330686,
      0.5429756033800053,
      0.48674516859901185,
      0.4287249150518151,
      0.4412973658305934,
      0.42652206393772074,
      0.5230152602816005,
      0.4381844732969824,
      0.469504471113217,
      0.5050745173979951,
      0.2789278725552953,
      0.4599258907712269,
      0.47955375533304045,
      0.4077774286531244,
      0.46652334487725367,
      0.4986521991031938,
      0.5285950964059902,
      0.41885675292036617,
      0.45954309788698766,
      0.0,
      0.4519461681381831,
      0.3865404281138809,
      0.47286554252854884,
      0.4981001937544036,
      0.5359099539311731,
      0.4851024037148821,
      0.4419739574910362,
      0.3244141155066569
    ],
    [
      0.2921423498120921,
      0.5133150924705381,
      0.5002280487831108,
      0.47193625931871486,
      0.38257956570520824,
      0.4405534608768542,
      0.45830465156820743,
      0.42067065913480217,
      0.508379498876899,
      0.40827852880603954,
      0.43481139036931604,
      0.2951338985512417,
      0.48745354975888455,
      0.42786895100697353,
      0.4503557946586796,
      0.45142288311778755,
      0.4732108966671109,
      0.45091164919563576,
      0.4913971619183266,
      0.4426769916790847,
      0.41493599579235174,
      0.0,
      0.32181552933639246,
      0.4279599466419082,
      0.4292585089869665,
      0.5318693560884238,
      0.5186536482981523,
      0.39514474671861954,
      0.2818349196924508
    ],
    [
      0.3246006149983143,
      0.37480997585101417,
      0.3765689173869411,
      0.3838931240462422,
      0.38992647554164206,
      0.38181795347779657,
      0.3474361260672998,
      0.3916672642082988,
      0.3930398820174892,
      0.41131445543160483,
      0.39180666759108274,
      0.34325362938915727,
      0.3589722365467598,
      0.36589845861893644,
      0.3348134735130426,
      0.4051768832275102,
      0.4217857389446471,
      0.41756636931609004,
      0.32722390961444714,
      0.37268885766754467,
      0.4341423746739037,
      0.33953539691705714,
      0.0,
      0.3958656405762262,
      0.42990707702068387,
      0.39125832627698043,
      0.4112356839551594,
      0.37365839968051273,
      0.32966985017410844
    ],
    [
      0.3123496031166668,
      0.4756623724113276,
      0.5157067940463007,
      0.4507649747676259,
      0.40582853333641467,
      0.4561195164835472,
      0.4113554102568071,
      0.4802940370263278,
      0.5277612426401312,
      0.4280818981242276,
      0.4485049833615977,
      0.2933212416474633,
      0.38395842489513665,
      0.4471605428779821,
      0.41337856536128115,
      0.477164421901652,
      0.47939751182258217,
      0.5443313360643092,
      0.41837740993272043,
      0.4183727297569182,
      0.5106206724818321,
      0.47193726711732786,
      0.33945092468737426,
      0.0,
      0.44782047870010855,
      0.49956347637969123,
      0.42487481158601326,
      0.4545841824100867,
      0.40484044317431445
    ],
    [
      0.2819842968613393,
      0.4275639930932109,
      0.4565827485620999,
      0.39238402616889134,
      0.4167555800203544,
      0.3676615692588019,
      0.37756223730423755,
      0.4087069233846117,
      0.38505888735028404,
      0.38646779303093126,
      0.39585085444502677,
      0.3049905340396919,
      0.425853700368368,
      0.3540849912577775,
      0.3766962976789061,
      0.38968813501583566,
      0.4165267604276539,
      0.3931201435542009,
      0.4189375301651135,
      0.34165380245922905,
      0.5207291138130667,
      0.39497429675019924,
      0.3473403341185879,
      0.4016349312334071,
      0.0,
      0.403507704409088,
      0.4227989630393749,
      0.3937834928103998,
      0.30091064173673177
    ],
    [
      0.3804943033894004,
      0.5748750921292911,
      0.5698275979456588,
      0.5827690748068268,
      0.45754761155685997,
      0.5658749289760996,
      0.5054000888820689,
      0.5289551919701723,
      0.6103002476861801,
      0.5533893306731443,
      0.5610212325225954,
      0.3218472395430898,
      0.5195776119472946,
      0.5266564819628692,
      0.47273904889289176,
      0.5788778909472534,
      0.5503758484742256,
      0.5440465157350856,
      0.5473085966693985,
      0.5190284578363937,
      0.5804967889317287,
      0.5779683959802331,
      0.4167852039801694,
      0.5129095440045188,
      0.4696206455820484,
      0.0,
      0.530998947984991,
      0.45272362441198877,
      0.3473879136404996
    ],
    [
      0.3548516870158842,
      0.4978210393550664,
      0.5164723112282139,
      0.4720579350099452,
      0.43686355539906363,
      0.4107105025502875,
      0.378962502565515,
      0.449206763142352,
      0.45768906467950754,
      0.46329293532140414,
      0.44690042379564976,
      0.3175037687500555,
      0.43760788256736083,
      0.44740252453274354,
      0.3875587398663476,
      0.4422579560968738,
      0.4938879394486928,
      0.4221450007486449,
      0.4078996785350608,
      0.4651045778952201,
      0.4983014376930166,
      0.48841803328706446,
      0.38156951230337866,
      0.4133692204624504,
      0.44231492451230014,
      0.46763042705351743,
      0.0,
      0.44258354223948126,
      0.3031404257979644
    ],
    [
      0.2628880468831025,
      0.38448737302216207,
      0.43408562017207175,
      0.4604903585735147,
      0.37142011160317345,
      0.3671801707183546,
      0.3374347354362681,
      0.3866291332938725,
      0.3929816272627624,
      0.41197125697756465,
      0.36067048924296086,
      0.2676587039935592,
      0.3620468609076848,
      0.3997957443062756,
      0.3735420219211627,
      0.39579776785688736,
      0.38951023273825935,
      0.3904171222306909,
      0.34906051082550094,
      0.4023284386534973,
      0.4033794360975682,
      0.3372163816418232,
      0.3384610032110067,
      0.3991267274360557,
      0.3692407061555514,
      0.36431910396202616,
      0.3972174269527742,
      0.0,
      0.29185605048674335
    ],
    [
      0.4117828913798047,
      0.4255696192064733,
      0.4202166265216696,
      0.430261802332629,
      0.4000775413857569,
      0.4019572402437459,
      0.40265468887443445,
      0.4236672721614836,
      0.441857393253245,
      0.42743597854548265,
      0.4454959618272911,
      0.38081424137329734,
      0.41463008121434863,
      0.4027499005182722,
      0.397789259623645,
      0.4196235415347769,
      0.472015742254124,
      0.41261237416814733,
      0.3792384532997115,
      0.42964376228028556,
      0.39859620338705093,
      0.4782953149415896,
      0.398844731676147,
      0.4692482409705214,
      0.4290898140798822,
      0.4306238733937482,
      0.42739598876839824,
      0.4425262654279585,
      0.0
    ]
  ],
  "row_avgs": [
    0.26255473200159146,
    0.44451603435356846,
    0.4729090888380108,
    0.5131061373302227,
    0.2487304974808604,
    0.4471536419694518,
    0.4497860987119401,
    0.4755491399090238,
    0.48922719791238756,
    0.46789779590901254,
    0.41637626612168777,
    0.32785408725493,
    0.4397885217085605,
    0.44922952047286807,
    0.41185303999003553,
    0.47534164112026245,
    0.4376853391301595,
    0.5374695623870267,
    0.4392786018351918,
    0.48300498737325576,
    0.4530661412184536,
    0.4329679976368132,
    0.37926906295466045,
    0.4407708502274203,
    0.38942179579847924,
    0.5128501234665349,
    0.4336972968518951,
    0.3714718986629598,
    0.4219541001658542
  ],
  "col_avgs": [
    0.3144089519059034,
    0.49133724623592884,
    0.4872638053278998,
    0.481213765407849,
    0.4205803590740059,
    0.4367405915777288,
    0.4145001957827907,
    0.44262301799527254,
    0.4661558545735668,
    0.4356552671804403,
    0.46747897464952176,
    0.3021186405925674,
    0.4463236845237627,
    0.4513774217837327,
    0.4159945961257992,
    0.4586688952919236,
    0.461171878458064,
    0.4532548529574464,
    0.4247826857474776,
    0.44592673477736344,
    0.46917345904171404,
    0.44882181520606695,
    0.3488523159885438,
    0.43583103299009224,
    0.42283620252527265,
    0.47375869565821166,
    0.4607097958187965,
    0.4258085848816294,
    0.3214118767137462
  ],
  "combined_avgs": [
    0.28848184195374743,
    0.4679266402947486,
    0.48008644708295534,
    0.49715995136903585,
    0.33465542827743316,
    0.44194711677359033,
    0.43214314724736536,
    0.45908607895214815,
    0.47769152624297717,
    0.4517765315447264,
    0.44192762038560474,
    0.3149863639237487,
    0.4430561031161616,
    0.4503034711283004,
    0.41392381805791734,
    0.467005268206093,
    0.44942860879411173,
    0.49536220767223654,
    0.4320306437913347,
    0.4644658610753096,
    0.4611198001300838,
    0.4408949064214401,
    0.36406068947160214,
    0.43830094160875627,
    0.40612899916187595,
    0.4933044095623733,
    0.4472035463353458,
    0.3986402417722946,
    0.3716829884398002
  ],
  "gppm": [
    627.7001873025171,
    560.8741572280052,
    564.3855118229397,
    565.7645913399184,
    584.5017077768781,
    589.8356001321023,
    596.0988226237196,
    584.3501150043791,
    573.1296906556777,
    587.4286083895414,
    573.6399811990544,
    647.5045668186251,
    584.1761939756549,
    584.397310052558,
    597.9710270157411,
    580.735525620192,
    574.816612478353,
    583.2919746936067,
    594.3087568206062,
    584.2377421551529,
    571.9829558693194,
    585.9716859539116,
    628.429144347085,
    592.051526998716,
    592.5820256148658,
    572.9662968625281,
    578.4034888582718,
    591.3556474858873,
    642.5266938467757
  ],
  "gppm_normalized": [
    1.4495616937895155,
    1.2584279593706125,
    1.262463791821506,
    1.2655653210803133,
    1.309586114212,
    1.318577099702002,
    1.3357906125395171,
    1.3081305285970652,
    1.2794381420212655,
    1.3156079778405503,
    1.282614084003201,
    1.4514594721128902,
    1.31074137284796,
    1.3106039262956768,
    1.337771173476079,
    1.3029801447221814,
    1.2853920617200087,
    1.3035381048653125,
    1.3304001618077488,
    1.3018395813143766,
    1.2742307991540187,
    1.3056823413583092,
    1.4077320250605585,
    1.3176702958435835,
    1.3216834635556436,
    1.2833661748213385,
    1.2916240309675513,
    1.3300159295647396,
    1.4351564877755734
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350
  ],
  "response_lengths": [
    2237,
    2885,
    2463,
    2499,
    4377,
    2517,
    2527,
    2178,
    2426,
    2213,
    3102,
    2332,
    2523,
    2628,
    2658,
    2410,
    2319,
    2156,
    2445,
    2429,
    2218,
    2347,
    2187,
    2291,
    2405,
    2221,
    2311,
    2567,
    1997
  ]
}