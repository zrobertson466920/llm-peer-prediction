{
  "example_idx": 52,
  "reference": "Under review as a conference paper at ICLR 2023\n\nLEARNING DEEP OPERATOR NETWORKS: THE BENEFITS OF OVER-PARAMETERIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNeural Operators that directly learn mappings between function spaces have received considerable recent attention. Deep Operator Networks (DeepONets) (Lu et al., 2021), a popular recent class of operator networks have shown promising preliminary results in approximating solution operators of parametric partial differential equations. Despite the universal approximation guarantees (Lu et al., 2021; Chen & Chen, 1995) there is yet no optimization convergence guarantee for DeepONets based on gradient descent (GD). In this paper, we establish such guarantees and show that over-parameterization based on wide layers provably helps. In particular, we present two types of optimization convergence analysis: first, for smooth activations, we bound the spectral norm of the Hessian of DeepONets and use the bound to show geometric convergence of GD based on restricted strong convexity (RSC); and second, for ReLU activations, we show the neural tangent kernel (NTK) of DeepONets at initialization is positive definite, which can be used with the standard NTK analysis to imply geometric convergence. Further, we present empirical results on three canonical operator learning problems: Antiderivative, Diffusion-Reaction equation, and Burger’s equation, and show that wider DeepONets lead to lower training loss on all the problems, thereby supporting the theoretical results.\n\n1\n\nINTRODUCTION\n\nReplicating the success of Deep Learning in scientific computing such as developing Neural PDE solvers, constructing surrogate models and developing hybrid numerical solvers has recently captured interest of the broader scientific community. Neural Operators (Li et al., 2021a;b) and Deep Operator Networks (DeepONets) (Lu et al., 2021; Wang et al., 2021) encompass two recent approaches aimed at learning mappings between function spaces. Contrary to a classical supervised learning setup which aims at learning mappings between two finite-dimensional vector spaces, these neural operators/operator networks aim to learn mappings between infinite-dimensional function spaces. The key underlying idea in both the approaches is to parameterize the solution operator as a deep neural network and proceed with learning as in a standard supervised learning setup. Since a neural operator directly learns the mapping between the input and output function spaces, it is a natural choice for learning solution operators of parametric PDE’s where the PDE solution needs to be inferred for multiple instances of these “input parameters” or in the case of inverse problems when the forward problem needs to be solved multiple times to optimize a given functional. While there exist results on the approximation properties and convergence of DeepONets; see, e.g., (Deng et al., 2021) for a convergence analysis of DeepONets – vis-a-vis their approximation guarantees– for the advection-diffusion equation, there do not exist any optimization results on when and why GD converges during the optimization of the DeepONet loss.\n\nIn this work we put forth theoretical convergence guarantees for DeepONets centered around overparameterization and show that over-parameterization based on wider layers (for both branch and trunk net) provably helps in DeepONet convergence. This is reflected in Figure 1 which summarizes an empirical evaluation of over-parameterized DeepONets with ReLU and smooth activations on a prototypical operator learning problem. In order to complement our theoretical results, we present empirical evaluation of our guarantees on three template operator learning problems: (i) Antiderivative operator, (ii) Diffusion-Reaction PDE, and (iii) Burger’s equation and demonstrate that wider DeepONets lead to overall lower training loss at the end of the training process.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) ReLU Activation\n\n(b) Smooth Activation\n\nFigure 1: Benefits of over-parameterization on learning of DeepONets for the Antiderivative Operator: Gθ(u)(x) = (cid:82) x 0 u(ξ) dξ. In both cases m denotes the width of the branch net and the trunk net. For both ReLU and smooth activations, increasing the width m leads to much lower losses. Note that the y-axis is in log-scale.\n\nThe rest of the paper is organized as follows. In Section 2 we review the existing literature on neural operators, operator networks and over-parameterization based approaches for establishing convergence guarantees for deep networks. Next, we devote Section 3 to briefly outline the the DeepONet model, the learning problem and the corresponding architecture. Section 4 contains the first technical result of the paper. In Section 4 we establish convergence guarantees for DeepONets with smooth activations (for both branch and trunk net) based on the Restricted Strong Convexity (RSC) of the loss. Next, in Section 5, we present the second technical result of the paper where we establish optimization guarantees for DeepONets with ReLU activations by showing that the Neural Tangent Kernel (NTK) of the DeepONet is positive definite at initialization. In Section 6 we present simple empirical evaluations of the main results by carrying out a parametric study based on increasing the DeepONet width and noting its effect on the total loss during training. We finally conclude by summarizing the main contributions in Section 7.\n\n2 RELATED WORK\n\n2.1 LEARNING OPERATOR NETWORKS\n\nConstructing operator networks for ordinary differential equations (ODE’s) using learning-based approaches was first studied in (Chen & Chen, 1995) where the authors showed that a neural network with a single hidden layer can approximate a nonlinear continuous functional to arbitrary accuracy. This was, in essence, akin to the Universal Approximation Theorem for classical neural networks (see, e.g., (Cybenko, 1989; Hornik et al., 1989; Hornik, 1991; Lu et al., 2017)). While the theorem only guaranteed the existence of a neural architecture, it was not practically realized until (Lu et al., 2021) which also provided an extension of the theorem to deep networks. Since then a number of works have pursued applications of DeepONets to different problems (see, e.g. (Goswami et al., 2022; Wang et al., 2021; Wang & Perdikaris, 2021)). Recently (Kontolati et al., 2022) studied the influence of over-parameterization on neural surrogates based on DeepONets in context of dynamical systems. While their paper studies the effects of over-parameterization on the generalization properties of DeepONets, an optimization analysis of DeepONets is a largely open problem.\n\n2.2 OPTIMIZATION: NTK, ETC.\n\nOptimization of over-parameterized deep networks have been studied extensively (see, e.g., (Du et al., 2019; Arora et al., 2019b;a; Allen-Zhu et al., 2019; Liu et al., 2021a)). In particular, (Jacot et al., 2018) showed that the neural tangent kernel (NTK) of a deep network converges to an explicit kernel in the limit of infinite network width and stays constant during training. (Liu et al., 2021a) showed that this constancy arises due to the scaling properties of the hessian of the predictor as a function of network width. (Du et al., 2019; Allen-Zhu et al., 2019) showed that gradient descent\n\n2\n\n0200400600800Epochs%10010−1410−1210−1010−810−610−410−2100Lossm=10m=50m=100m=200m=5000200400600800Epochs%10010−1210−1010−810−610−410−2100Lossm=10m=50m=100m=200m=500Under review as a conference paper at ICLR 2023\n\nconverges to zero training error in polynomial time for a deep over-parameterized model, with (Du et al., 2019) showing it for a deep model with residual connections (ResNet) while (Allen-Zhu et al., 2019) showed in context of feed-forward models, CNNs and ResNets. (Karimi et al., 2016) showed that the Polyak-Lojasiewicz (PL) condition, a much weaker condition than strong convexity can be used to explain the linear convergence of gradient-based methods.\n\n3 LEARNING DEEP OPERATOR NETWORKS\n\nLearning neural operators (Li et al., 2020; Lu et al., 2021) is a fundamentally challenging problem as it requires learning parametric maps between two infinite-dimensional function spaces which is in sharp contrast to classical deep learning which learns parametric maps between two finite-dimensional vector spaces. A succinct review of learning in infinite dimensions is provided in Section A.1 in the Appendix. Here we focus on the DeepONet model and outline its main features.\n\n3.1 DEEPONET SETUP\n\nIn what follows, we use the notation f (θf ; u) to denote a deep network fθf : Rm Rn where u denotes the input and θf the learnable parameters. A DeepONet is an operator network that learns a G†(u), where u denotes the input function, and G† denotes parametric map Gθ such that Gθ(u) the “true” operator whose approximation we wish to learn. Following (Lu et al., 2021) a DeepONet predictor can defined as the inner product of two deep networks: f = k=1 known as the branch net and g =\n\nk=1 the trunk net, namely\n\n(cid:55)→\n\nfk\n\ngk\n\n≈\n\n}\n\n{\n\nK\n\nK\n\n{\n\n}\n\nGθ(u)(y) :=\n\nK (cid:88)\n\nk=1\n\nfk(θf ; u)gk(θg; y),\n\n(1)\n\nn\n\n∈\n\nu(i)\n\nqi j=1}\n\ny(i) j }\n\ni=1 with y(i)\n\ndom(Gθ(u)) n\n\nRdu is the input function and y\n\nwhere u ∈\ndata comprises of n input functions, that is\n\nRdv the output locations1. The training i=1 and qi output locations for each G(u(i)), i.e. j denoting the j-th output location for Gθ(u(i)). The branch net f has Rpg . The entire set of parameters for\n\nRpf and the trunk net g has parameters θg f θ⊤\n\n{{ parameters θf the DeepONet is given by θ = [θ⊤ r\n∀ ∈\nR [R] and u(i)(xr) r=1, RK. Similarly, for scalar output locations y(i) which implies f : RR RK. The DeepONet learning problem can then be cast as the minimization of the following empirical risk:\n\nxr R the branch net takes input\n\nRdu . For scalar functions u(i)\n\nRpf +pg . Further, let\n\nRd ⊆\nu(i)(xr)\n\nR we have g : R\n\nr=1 ∈ }\n\ndom(u)\n\n} (cid:55)→\n\nj ∈\n\ng ]⊤\n\n(cid:55)→\n\n⊆\n\n∈\n\n∈\n\n∈\n\n∈\n\n∈\n\n}\n\n{\n\n{\n\n{\n\nR\n\nθ† = arg min\n\nθ∈Θ L\n\n(cid:0)Gθ(u), G†(u)(cid:1) =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\nl\n\n(cid:16)\n\nGθ(u(i))(y(i) j )\n\n−\n\nG†(u(i))(y(i) j )\n\n(cid:17)\n\n,\n\n(2)\n\nwith\n\nGθ(u(i))(y(i)\n\nj ) =\n\n(cid:16)\n\nθf ;\n\nfk\n\nK (cid:88)\n\nk=1\n\nu(i)(xr) }\n\nR r=1\n\n{\n\n(cid:16)\n\n(cid:17)\n\ngk\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n,\n\n(3)\n\nand l(z) := 1/2(z)2 denoting the mean-squared error (MSE) loss. Note that the “true” operator G† whose approximation is sought in (2) can either be explicit, e.g. integral of a function, or implicit, e.g. the solution to a nonlinear partial differential equation (PDE).\n\n3.2 DEEPONET ARCHITECTURE\n\nWe now briefly outline the architecture used throughout the analysis in Sections 4 and 5 and in the numerical experiments in Section 6. We adopt fully connected feedforward neural networks (FNNs) for both the branch and trunk nets which is also the baseline DeepONet model in (Lu et al., 2021). Figure 4 in the Appendix shows a schematic of the architecture and the notation used throughout this paper. For the architecture we adopt the unstacked configuration (see, Fig 1d in (Lu et al., 2021)).\n\n1The original DeepONet paper puts forth the above model and another one with a bias term added to the\n\ninner product. For definiteness, we restrict our attention to the model without bias\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nRemark 3.1 (DeepONet Training Tuple). As mentioned in the prequel, each DeepONet training data\n\ncomprises of the tuple\n\n(i) :=\n\nu(i)(xr)\n\nR\n\nr=1,\n\n(cid:16)\n\n{\n\nD\n\ny(i) j } {\n\nqi j=1,\n\nG(u(i))(y(i) {\n\nj }\n\nqi j=1\n\n}\n\n(cid:17)\n\n.\n\nn\n\n}\n\n(i)\n\n{D\n\ni=1.\n\nis then simply given by the collection\n\nRemark 3.2 (Training Dataset). The entire training dataset of all tuples Remark 3.3 (Widths mf and mg). We denote the width of the branch net by mf and the trunk net by mg and use mf = mg = m interchangeably through Sections 4-5. Similarly, for the experiments we use mf = mg unless otherwise stated. For the latter (i.e. when mf = mg) the analysis in Section 4 still holds with m = min(mf , mg) Remark 3.4 (Last layer of the branch and trunk net). Note that the last layer of the branch and trunk net is simply a linear layer and does not have any nonlinearity/activation.\n\nD\n\n4 OPTIMIZATION GUARANTEES FOR DEEPONETS: SMOOTH ACTIVATIONS\n\nIn this section, we focus on DeepONets based on smooth activation functions. To build up to the optimization analysis, we first establish a bound on the spectral norm of the DeepONet predictor, m ) where, again, m = mf = mg. The spectral in particular showing that norm bound is then used to establish a form of Restricted Strong Convexity (RSC) of the DeepONet loss (2), which in turn is used to establish geometric convergence of gradient descent (GD). For the analysis, analogous to (Liu et al., 2021b), we consider a FNN for the branch net:\n\n2Gθ(u)(y)\n\n2 = O( 1√\n\n∥∇\n\n∥\n\nβ(0)\n\nf = u,\n\nβ(l)\n\nf = φl\n\n(cid:18) 1\n\n√mf\n\nW (l)\n\nf β(l−1)\n\nf\n\n(cid:19)\n\n[L\n\n,\n\nl ∀\n\n∈ fk = β(L) f,k ,\n\n−\n\n1],\n\nf = W (L) β(L)\n\nf β(L−1)\n\nf\n\n(4)\n\nk\n\n∀\n\n∈\n\n[K] ,\n\nwhere mf = m and L denote the width and depth of the branch net respectively, [K] := ,\n} {\nφl is the activation function at layer l, β(l) w(l) denote the weight matrices at layer l. Similarly, we consider a fully connected feedforward network for the trunk net:\n\nf are the outputs at layer l and W (l)\n\n1, . . . , K\n\nf ≡\n\nfij\n\nβ(0)\n\ng = y,\n\nβ(l)\n\ng = φl\n\nW (l)\n\ng β(l−1)\n\ng\n\n(cid:18) 1\n\n√mg\n\n(cid:19)\n\n[L\n\n,\n\nl ∀\n\n∈ gk = β(L) g,k ,\n\n−\n\nk\n\n∀\n\n∈\n\n[K] ,\n\n1],\n\nβ(L)\n\ng = W (L)\n\ng β(L−1)\n\ng\n\n(5)\n\n≡ gij denote the weight matrices at layer l of the trunk net. In order to aid our analysis, we make the\n\nwhere, again, mg = m and L denote the width and depth of the trunk net respectively and W (l) w(l) following assumptions on the activations, the loss and the weights: Assumption 1 (Activation functions). The activation functions φl at each layer l are 1-Lipschitz and βφ-smooth (i.e. φ′′ Assumption 2 (Loss function). We assume the loss li,j is (i) strongly convex, i.e., l′′ smooth, i.e., l′′\n\na > 0, (ii) λ., where we make use of the following notation\n\nβφ) for some βφ > 0.\n\ni,j ≥\n\ng\n\n≤ b, and (iii) Lipschitz, i.e., (cid:12) (cid:12)l′ (cid:17)\n\nG†(u(i))(y(i) j )\n\n,\n\ni,j ≤ Gθ(u(i))(y(i) j )\n\n(cid:16)\n\nl\n\n(cid:12) (cid:12)\n\ni,j\n\nl′′ (cid:16)\n\nGθ(u(i))(y(i) j )\n\nG†(u(i))(y(i) j )\n\n(cid:17)\n\n.\n\n−\n\n−\n\nli,j\n\n≡\n\nl′′ i,j ≡\n\n≤ l′ i,j ≡\n\nl′ (cid:16)\n\nGθ(u(i))(y(i) j )\n\nG†(u(i))(y(i) j )\n\n(cid:17)\n\n,\n\n−\n\n(6)\n\nt=0 = w(l)\n\nAssumption 3 (Initialization of Weights). All weights of the branch and trunk net are initialized as w(l) 0) and w(l) 1] and some constant gij | fij | σ0 > 0 respectively. Furthermore, in order for the model (1) to have a suitable scaling in our analysis, we initialize the weights in the last layer of the branch and trunk nets as wL (0, 1/(m K)) and wL\n\n(0, 1/(m K)) respectively.\n\nt=0 = w(l)\n\ng0, ij ∼ N\n\nf0, ij ∼ N\n\nf0, ij ∼ N\n\n0) for l\n\n(0, σ2\n\n(0, σ2\n\n[L\n\n−\n\n∈\n\nRemark 4.1. For Assumption 1, our analysis straightforwardly extends to general ς-Lipschitz activations, with constants depending ς. For Assumption 2, the Lipschitz loss assumption can be dropped by assuming that the true responses G†(u)(y) are bounded and showing that the prediction responses Gθ(u)(y) are bounded with high probability.\n\ng0, ij ∼ N\n\n4\n\n̸ Under review as a conference paper at ICLR 2023\n\nDefinition 1 (Norm ball). Our analysis focuses on the standard Euclidean norm ball around the initialization θ0, i.e. BEuc\n\n(θ0), where\n\nρ\n\nBEuc\n\nρ\n\n( ̄θ) := (cid:8)θ\n\nRpf +pg\n\n∈\n\n(cid:13) (cid:13)θ\n\n|\n\n ̄θ(cid:13) (cid:13)2 ≤\n\n−\n\nρ(cid:9) .\n\n(7)\n\n4.1 SPECTRAL NORM OF THE HESSIAN OF BRANCH AND TRUNK NETS\n\nThe convergence analysis makes use of the gradients and hessians of the total loss (2) and the predictor (1) with respect to the parameters θ, namely,\n\n(θ) = (cid:2)\n\n∇\n\n;\n\nθf L\n\nθg L\n\n∇\n\n(cid:3) ,\n\nθ\n\n∇\n\nL\n\nand\n\n2\n\nθL\n\n∇\n\n= H (θ) =\n\n(cid:20) Hf f Hf g Hgf Hgg\n\n(cid:21)\n\n,\n\n(8)\n\n(θ)\n\n) to where denote the derivative wrt the parameters θf and this is not a functional gradient. Similarly, the individual blocks in the 2\n\nRpg . Note that we make use of the notation\n\n2 block hessian H(θ) are given by\n\nRpf and\n\nθf ( ·\n\nθf L\n\nθg L\n\n(θ)\n\n∇\n\n∇\n\n∇\n\n∈\n\n∈\n\n×\n\nHf f =\n\n∂2\n\n2 , Hf g = L\n∂θf\n\n∂2 ∂θf ∂θg\n\nL\n\n, Hgf = H ⊤\n\nf g =\n\n∂2 ∂θg∂θf\n\nL\n\n,\n\nand Hgg =\n\n∂2\n\n2 , L\n∂θg\n\n(9)\n\nwhere Hf f ignored for clarity of exposition. Using (2) and rewriting the derivatives in (8) and (9), we get\n\nRpg×pf and the argument θ is\n\nRpg×pg , Hf g\n\nRpf ×pf , Hgg\n\nRpf ×pg , Hgf\n\n∈\n\n∈\n\n∈\n\n∈\n\n∂ L\n∂θf\n\n=\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nl′\n\ni,j\n\nK (cid:88)\n\nj=1\n\nk=1\n\ng(i) k,j∇\n\nθf f (i)\n\nk\n\nand\n\n∂ L\n∂θg\n\n=\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nl′\n\nij\n\nK (cid:88)\n\nj=1\n\nk=1\n\nf (i) k ∇\n\nθg g(i)\n\nk,j, (10)\n\nfor the gradients, and\n\n∂2\n\n2 = L\n∂θf\n\n∂2\n\n2 = L\n∂θg\n\n∂2 ∂θf ∂θg\n\nL\n\n=\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n1 qi\n\n1 qi\n\n1 qi\n\n1 n\n\n1 n\n\n1 n\n\n(cid:124)\n\nqi (cid:88)\n\nl′\n\ni,j\n\nK (cid:88)\n\nj=1\n\nk=1\n\nqi (cid:88)\n\nl′\n\ni,j\n\nk (cid:88)\n\nj=1\n\nk=1\n\ng(i) k,j∇\n\n2 θf\n\nf (i) k +\n\nf (i) k ∇\n\n2 θg\n\ng(i) k,j +\n\n1 n\n\n1 n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n1 qi\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\nqi (cid:88)\n\nj=1\n\n\n\nl′′\n\ni,j\n\n\n\nK (cid:88)\n\nk,ˆk=1\n\nK (cid:88)\n\n\n\nl′′\n\ni,j\n\n\n\n\n\nk,jg(i) g(i)\n\nˆk,j∇\n\nθf f (i)\n\nk ∇\n\nθf f (i)⊤ ˆk\n\n ,\n\nk f (i) f (i)\n\nˆk ∇\n\nθg g(i)\n\nk,j∇\n\n(i)⊤\n\nθg gˆk,j\n\n\n\n ,\n\nqi (cid:88)\n\nj=1\n\nK (cid:88)\n\nl′\n\ni,j\n\n∇\n\nk=1 (cid:123)(cid:122) =H (1)\n\nf g\n\nθf f (i)\n\nk ∇\n\nθg g(i)⊤\n\nk,j\n\n+\n\n(cid:125)\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n1 n\n\n(cid:124)\n\nk,ˆk=1 \n\nl′′\n\ni,j\n\n\n\nK (cid:88)\n\nk,ˆk=1\n\nk,jf (i) g(i)\n\nˆk ∇\n\nθf f (i)\n\nk ∇\n\n(i)⊤\n\nθg gˆk,j\n\n(cid:123)(cid:122) =H (2)\n\nf g\n\n(11)\n\n\n\n, \n\n(cid:125)\n\nfor the individual blocks of the hessian (8) where, we make use of the notation g(i) and f (i)\n\nk = fk(θf ; u(i)).\n\nk,j = gk(θg; y(i) j )\n\nLemma 4.1. Under Assumptions 1, 2 and 3, and for θ for all k\n\n[K]\n\n∈\n\nBEuc ρ\n\n∈\n\n(θ0), with high-probability we have\n\nmax i∈[n]\n\n2 θf\n\nf (i)\n\nk\n\n(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13)\n\nc(f ) √mf\n\n,\n\nρ(f ),\n\n(cid:13) (cid:13) (cid:13) ≤ (cid:13) (cid:13)2 ≤\n\nθf fk\n\nand max i∈[n] (cid:13) (cid:13)\n\nand\n\nmax j∈[qi] (cid:13) (cid:13)\n\nθg gk\n\n(cid:13) (cid:13) (cid:13)∇\n\n2 θg\n\ng(i)\n\nk,j\n\n(cid:13) (cid:13) (cid:13) ≤\n\nc(g) √mg\n\nρ(g) ,\n\n≤\n\n(12)\n\n(13)\n\n∇ where c(f ), c(g), ρ(f ), ρ(g) are suitable constants,\n\n∇\n\n2 θf\n\n(\n\n) = ∂2( ·\n·\n\n∇\n\n)/∂θ2\n\nf and\n\n2 θg\n\n(\n\n) = ∂2( ·\n·\n\n∇\n\n)/∂θ2 g.\n\n)/∂θf , ) = ∂( θf ( ·\n·\n\n∇\n\nθg ( ·\n\n)/∂θg, ) = ∂( ·\n\n∇\n\nProof. The proof follows directly from Theorem 3.2 in (Liu et al., 2021a).\n\n5\n\n∈\n\nθt\n\n{\n\nt≥0\n\n}\n\n(14)\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 GEOMETRIC CONVERGENCE BASED ON RESTRICTED STRONG CONVEXITY\n\nWe now focus on establishing the convergence of gradient descent (GD) by using the Hessian spectral norm bound. The convergence analysis is based on a generalization of the notion of restricted strong convexity (RSC) of the loss (2) (see (Negahban et al., 2012; Negahban & Wainwright, 2012; Zhang & Cheng, 2015; Zhang & Yin, 2013; Lai & Yin, 2013) for a review of RSC and its applications in high-dimensional statistics for linear models). In order to further aid clarity of exposition, we state the main results along with their implications in this section and leave the details of the proofs to Section A.3 in the Appendix.\n\nDefinition 2 (Restricted Strong Convexity (RSC)). A function L\nconvexity (α-RSC) with respect to the tuple (Q, θ) if for any θ′ we have\n\n(θ) +\n\n+ α\n\n(θ′)\n\nθ′\n\nθ′\n\nθ,\n\nθ\n\nθ\n\n∇\n\nL\n\n(θ) ⟩\n\n2 ∥\n\n−\n\nL\n\n≥ L\n\n⟨\n\n−\n\n∈ 2, with α > 0.\n\n⊆\n\n2 ∥\n\nis said to satisfy α-restricted strong Rp, Rp and some fixed θ\n\nQ\n\nNote that denote a sequence of iterates obtained from GD, i.e.,\n\nbeing α-RSC w.r.t. (S, θ) does not need\n\nL\n\nto be convex on Rp. Further, let\n\nL\n\nθt+1 = θt\n\nηt\n\n(θt) .\n\n−\n\nθ\n\n∇\n\nL\n\nf,t θ⊤\n\nt = [θ⊤\n\ng,t]⊤, we will use dynamic restricted sets Qt\n\nRp, where p = pf + pg, to show Since θ⊤ RSC of the DeepONet loss. Note that these sets are parameterized by a constant κ which measures the absolute cosine similarity between suitable vectors (see Section A.3 and specifically Proposition 1 in the Appendix for a detailed discussion on these sets). Further, our optimization for DeepONets is based on a second order Taylor expansion of the loss (2) w.r.t. θt = [θ⊤\n\nκ ⊆\n\nf,t θ⊤\n\ng,t]⊤:\n\n(θ) =\n\n(θt) +\n\nθt,\n\nθ\n\n(θt)\n\n+\n\n(θ\n\nL\n\nL\n\nL (θt) and H( ̃θ) are as in (8), (9), (10), and (11), and ̃θ = τ θ + (1\n\n∇\n\n−\n\n−\n\n−\n\n⟩\n\nθ ⟨\n\nθt)⊤H( ̃θ)(θ\n\nθt) ,\n\n1 2\n\n(15)\n\nτ ) ̄θ for some\n\n−\n\nwhere, τ\n\nθ ∇\n[0, 1].\n\n∈\n\nL\n\nDefinition 3 (Qt\n\nκ sets.). For an iterate θt = [θ⊤\n\nf,t θ⊤\n\ng,t]⊤, consider the singular value decomposition\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nl′\n\ni,j\n\nK (cid:88)\n\nj=1\n\nk=1\n\nθf f (i)\n\nk ∇\n\n∇\n\nθg g(i) ⊤\n\nk,j =\n\n ̃q (cid:88)\n\nh=1\n\nσhahb⊤\n\nh ,\n\n(16)\n\nwhere ̃q vectors, and right singular vectors. Further, let\n\nqk, and σh > 0, ah\n\n≤\n\n∈\n\n∈\n\nRpf , bh\n\nRpg respectively denote the singular values, left singular\n\nThen, for some suitable κ\n\n(cid:26)\n\nQt\n\nκ :=\n\n ̄Gθ =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\nGθ(u(i))(y(i)\n\nj ) .\n\n(0, 1\n\n2 ], we define the set:\n\n∈\n\n(17)\n\nθ′ = [θ′\n\nf ; θ′\n\ng] :\n\ncos(θ′\n\n|\n\nθt,\n\n∇\n\n−\n\nθ ̄Gθt)\n\n| ≥\n\nκ,\n\n ̃q (cid:88)\n\nh=1\n\nσh\n\nθ′ ⟨\n\nf −\n\nθf,t, ah\n\nθ′\n\ng −\n\n⟩⟨\n\nθg,t, bh\n\n0 ,\n\nh\n\n∀\n\n∈\n\n[ ̃q]\n\n⟩ ≥\n\n(18)\n\n(cid:27)\n\n.\n\nRemark 4.2. Note that pf , pg are respectively the number of parameters in the branch and trunk nets and the models can be sufficiently over-parameterized such that they are larger than the number of training examples q = (cid:80)n\n\ni=1 qi.\n\nRpf +pg stem from technicalities in Remark 4.3 (Qt the analysis. For a detailed outline of these sets we refer the reader to Section A.3 and specifically Proposition 1 in the Appendix.\n\nκ sets). The specifics of the restricted set Qt\n\nκ ∈\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 4.2 (RSC of the loss). Under the assumptions Assumptions 1, 2 and 3, Qt\n\n(θ0) with high probability we have\n\nBEuc ρ\n\nθ′\n\n∀\n\n∈\n\nBt\n\nκ :=\n\nκ ∩\n\n(θ′)\n\nL\n\n≥ L\n\n(θt) +\n\nθ′\n\n⟨\n\n−\n\nθt,\n\nθ\n\n∇\n\nL\n\n(θt)\n\n+\n\n⟩\n\nαt 2 ∥\n\nθ′\n\nθt\n\n−\n\n2 , where αt = c1\n\n2 ∥\n\nθ ̄Gt\n\n∥∇\n\n2 2 − ∥\n\nc2 √m\n\n,\n\nwhere ̄Gt = 1 i=1 satisfies RSC w.r.t (Bt\n\n(cid:80)n\n\nn\n\n(cid:80)qi\n\nj=1 Gθt(u(i))(y(i) 1\nqi κ, θt) whenever αt > 0.\n\nj ) and c1, c2 > 0 are constants. Thus, the loss\n\n(19) (θ)\n\nL\n\nProof. A detailed proof is presented in Section A.3 in the Appendix.\n\nTheorem 4.3 (Smoothness of Loss). Under the assumptions Assumptions 1, 2 and 3, with high (θ) is β-smooth with β = bρ2 + c m with c = max(c(f ), c(g)), ρ = probability, for θ max(ρ(f ), ρ(g)) with c(f ), c(g), ρ(f ), ρ(g) as in Lemma 4.1.\n\nBEuc ρ\n\n√ λ√\n\n(θ0),\n\nL\n\n∈\n\nProof. We again refer the reader to Section A.3 in the Appendix for a detailed proof.\n\nLemma 4.4 (RSC = Restricted PL). The RSC and Smoothness of the Loss together imply a form of Polyak-Łojasiewicz (PL) condition w.r.t. the tuple (Bt, θt), unlike standard PL which holds without restrictions (Karimi et al., 2016).\n\n⇒\n\nProof. For a detailed outline of the proof, we refer the reader to Section A.3 in the Appendix.\n\nTheorem 4.5 (Global Loss Reduction). Consider the same conditions as in Theorem 4.3 and αt > 0 for t (0, 2), ∈\n(θ) with where β is defined as in Theorem 4.3. Then, with high probability,\n\n[T ] for a gradient descent update θt+1 = θt\n\nβ for some ωt arginf\n\n(θt) with ηt = ωt\n\nηt\n\n∇\n\n ̄θ\n\n−\n\nL\n\n∈\n\nθ\n\n∀\n\n∈\n\nθ∈BEuc\n\nρ\n\n(θ0)L\n\n0\n\n≤\n\nγt := L( ̄θt+1)−L( ̄θ)\n\nL(θt)−L( ̄θ) < 1 and ̄θt+1 (cid:18)\n\narginf θ∈Qt\n\nκ∩BEuc\n\nρ\n\n∈\n\n(θt+1)\n\nL\n\n− L\n\n( ̄θ)\n\n1\n\n−\n\n≤\n\nαtωt(1 β\n\n−\n\nγt)\n\n(2\n\n(θ), we have\n\n(θ0) L (cid:19)\n\nωt)\n\n−\n\n(θt)\n\n( L\n\n− L\n\n( ̄θ)) .\n\n(20)\n\nProof. We refer the reader to Section A.3 and in particular Lemma 1 in the Appendix for the proof.\n\nRemark 4.4. A direct consequence of Theorem 4.5 is global reduction in the loss in Rp and thus the convergence of gradient descent for the DeepONet optimization problem.\n\n5 OPTIMIZATION GUARANTEES FOR DEEPONETS: RELU ACTIVATIONS\n\nWe now present an alternative optimization analysis2, based on the Neural Tangent Kernel (NTK) (Jacot et al., 2018), to establish guarantees for the convergence of gradient descent and its variants for DeepONets. Although the NTK convergence theory holds for both smooth and ReLU activations (see, e.g. (Allen-Zhu et al., 2019; Du et al., 2019)), we present this in context of the latter. Further, in this work we only present a proof for the positive definiteness of the DeepONet NTK at initialization. This allows one to readily transcribe analogous existing arguments for deep networks, for the convergence of gradient descent, to the DeepONet model. (Allen-Zhu et al., 2019; Du et al., 2019; Nguyen et al., 2021) Now, recall the DeepONet predictor\n\n(cid:16)\n\nu(i)(cid:17)\n\nGθ\n\n(y(i)\n\nj ) :=\n\nK (cid:88)\n\nk=1\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\ngk\n\n(cid:16)\n\nfk\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n,\n\n(21)\n\n2The hessian-based analysis presented in Section 4 constitutes a sufficient condition and is not directly applicable for the case of ReLU activations. In contrast, the NTK analysis presented here applies to both smooth and ReLU activations.\n\n7\n\n∈ λ0,g\n\nUnder review as a conference paper at ICLR 2023\n\nand its corresponding gradient with respect to the parameters θ,\n\nθGθ(u(i))(y(i)\n\nj ) =\n\n∇\n\n\n\ngk\n\n\n\nfk\n\nK (cid:88)\n\nk=1\n\n(cid:17)\n\n(cid:16)\n\nj\n\nθg; y(i) (cid:0)θf ; u(i)(cid:1)\n\n(cid:0)θf ; u(i)(cid:1) (cid:17) (cid:16) θg; y(i)\n\nj\n\n\n\n .\n\nθf fk\n\n∇\n\nθg gk\n\n∇\n\n(22)\n\nThis allows us to write the NTK (cid:104)(cid:68)\n\n(θ) =\n\nK\n\n(θ) of the DeepONet, which is a q ×\nθGθ(u(i′))(y(i′) j′ )\n\nθGθ(u(i))(y(i)\n\nj ) ,\n\nq matrix as: (cid:69)(cid:105)\n\n,\n\nK where, again, q = (cid:80)n i=1 qi. Given the branch and trunk nets are initialized using standard initialization techniques, as typically done in practice for deep networks, (Arora et al., 2019b; Du et al., f,k and trunk net NTK 2019) with the addition of Assumption 3, the resulting branch net NTK\n\nq×q\n\n∇\n\n∇\n\n(23)\n\ng,k, namely\n\nK\n\nf,k =\n\nK\n\ng,k =\n\nK\n\n(cid:104)(cid:68)\n\n(cid:104)(cid:68)\n\n∇\n\n∇\n\nθf,k fk\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\nθg,k gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n,\n\n,\n\nθf,k fk\n\n∇\n\nθg,k gk\n\n∇\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)(cid:105)\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)(cid:69)(cid:105)\n\n,\n\nq×q\n\nn×n\n\n(24)\n\nK\n\ncan be shown to be positive definite with high probability for each k 2021a; Du et al., 2019). In particular, with high probability, for k\n\n∈ [K], we have\n\n[K] (Nguyen, 2021; Liu et al.,\n\nwhere λmin( respectively, and λ0,f , λ0,g > 0 are positive constants (see, e.g., Theorem 4.1 in (Nguyen, 2021)).\n\ng,k) are the minimum eigenvalues of the branch and trunk net NTKs\n\nK\n\nK\n\nλ0,f ,\n\nand\n\nλmin(\n\ng,k)\n\nK\n\n≥\n\nk\n\n∀\n\n∈\n\n[K] ,\n\n(25)\n\nλmin(\n\nf,k)\n\nK ≥\nf,k) and λmin(\n\n(θ) is positive definite at initialization). Given standard initialization for the branch Theorem 5.1 ( and trunk nets, and granted that the individual branch and trunk net NTKs are positive definite (24)-(25), the NTK of DeepONet is positive definite at initialization, i.e.\n\nK\n\nwhere α denotes an arbitrary block unit vector with n blocks, and αi,j corresponds to the j-th entry in the i-th block and c denotes a positive constant.\n\nα⊤E[\n\n(θ)]α\n\nK\n\n≥\n\nc > 0,\n\n(26)\n\nProof. We refer the reader to Section A.4 and specifically Propositions 2, 3 and 4\n\nRemark 5.1. A direct consequence of Theorem 5.1 is that the DeepONet NTK is positive definite at initialization. It is then straightforward to invoke standard NTK analysis to show convergence of gradient descent during training, see, e.g. (Jacot et al., 2018; Du et al., 2019; Arora et al., 2019b;a; Allen-Zhu et al., 2019; Nguyen et al., 2021).\n\n6 EXPERIMENTS\n\nWe now turn to an empirical evaluation of the effect of over-parameterization on the training performance of DeepONets, as measured by the empirical risk over a mini-batch B of the training dataset (3.2), for three canonical operator-learning problems. The results for smooth activations empirically verify the analysis in Section 4 whereas the ones for ReLU activations verify the analysis in Section 5. In all the examples described below, we consider FNN architectures for both branch and trunk nets which are similar to the ones chosen in (Lu et al., 2021). For definiteness, we set the width in each layer of the branch and trunk net to be the same (i.e. mf = mg = m) and then increase it uniformly from m = 10 to m = 500. We monitor the training process over 80, 000 training epochs and report the resulting average loss over each mini-batch with size nB,\n\nDB :=\n\nL\n\n1 nB\n\nnB(cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n(cid:16)\n\nl\n\nGθ(u(i))(y(i) j )\n\n−\n\nG†(u(i))(y(i) j )\n\n(cid:17)\n\n,\n\n(27)\n\nwhere nB denotes the number of input training functions (ui) in the batch B. We refer the reader to Section A.5 in the Appendix for specific details of the setup. Figure 2 shows the training loss (27) as a function of the epochs for DeepONets with smooth activations and Figure 3 shows the same for ReLU activations.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Antiderivative: selu\n\n(b) Diffusion Reaction: selu\n\n(c) Burgers: selu\n\nFigure 2: Training progress of DeepONet with a smooth activation function selu (Klambauer et al., 2017) for (a) Antiderivative Operator, (b) Diffusion-Reaction Equation and (c) Burger’s Equation. We plot the training loss (27) as a function of the epochs with the y-axis on a log-scale to clearly distinguish the effect of increasing width (m). Increasing the width m of the branch and trunk net leads to lower losses for all the problems.\n\n(a) Antiderivative: relu\n\n(b) Diffusion Reaction: relu\n\n(c) Burgers: relu\n\nFigure 3: Training progress of DeepONet with ReLU activations for (a) Antiderivative Operator, (b) Diffusion-Reaction Equation and (c) Burger’s Equation. The y-axis is again plotted on a log-scale to clearly demarcate the effect of increasing width. Increasing the width m again leads to lower training losses.\n\nRemark 6.1. We store the mini-batch training loss at every 100-th training epoch and observe that the training loss measured over the mini-batch is lower for wider DeepONets. This observation is consistent for both smooth (selu) and non-smooth (relu) activations.\n\nRemark 6.2 (Antiderivative Operator). The Antiderivative operator is a linear operator and hence is learned very accurately especially for wider DeepONets (\n\n10−12 at the end of training).\n\nDB ∼\n\nL\n\nRemark 6.3 (Diffusion Reaction). The Diffusion reaction equation also demonstrates lower loss with increasing width, albeit less markedly than the antiderivative operator. This can be attributed in part to the fact that the operator is inherently nonlinear.\n\nRemark 6.4 (Burger’s equation). The operator corresponding to Burger’s equation is more intricate with the added periodicity constraints on the solution (see Section A.5.3 in the appendix for details on the problem). We remark the distinction from the operator learning problem for Burger’s equation studied in (Li et al., 2021a) where the operator only sought to learn the mapping from the input (initial condition t = 0) to the final output t = 1 and not the entire solution space (x, t)\n\n[0, 1].\n\n[0, 1]\n\n∈\n\n×\n\n7 DISCUSSION AND CONCLUSION We present two novel optimization analyses for DeepONets (Lu et al., 2021) based on overparameterization and establish convergence guarantees for the DeepONet models with smooth and ReLU activations. The analysis for smooth activations is built on top of the restricted strong convexity of the DeepONet loss whereas the one for ReLU activations is based on the positive definiteness of the NTK at initialization. To the best of our knowledge, this is the first work to mathematically and empirically show the benefits of over-parameterization on the the learning of DeepONets.\n\n9\n\n0200400600800Epochs%10010−1210−1010−810−610−410−2100Lossm=10m=50m=100m=200m=5000200400600800Epochs%10010−410−2100102Lossm=10m=50m=100m=5000200400600800Epochs%10010−410−310−210−1100Lossm=10m=50m=100m=200m=5000200400600800Epochs%10010−1410−1210−1010−810−610−410−2100Lossm=10m=50m=100m=200m=50002004006008001000Epochs%10010−310−210−1100101Lossm=10m=50m=100m=5000200400600800Epochs%10010−710−610−510−410−310−210−1Lossm=10m=50m=100m=200m=500Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning via Over-Parameterization. Technical Report arXiv:1811.03962, arXiv, June 2019. URL http: //arxiv.org/abs/1811.03962. arXiv:1811.03962 [cs, math, stat] type: article.\n\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019a.\n\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/ paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\nTianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. IEEE Transactions on Neural Networks, 6(4):911–917, 1995.\n\nGeorge Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,\n\nsignals and systems, 2(4):303–314, 1989.\n\nBeichuan Deng, Yeonjong Shin, Lu Lu, Zhongqiang Zhang, and George Em Karniadakis. Convergence rate of deeponets for learning operators arising from advection-diffusion equations. arXiv preprint arXiv:2102.10621, 2021.\n\nTobin A Driscoll, Nicholas Hale, and Lloyd N Trefethen. Chebfun guide, 2014.\n\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675–1685. PMLR, 2019.\n\nSomdatta Goswami, Minglang Yin, Yue Yu, and George Karniadakis. A physics-informed variational DeepONet for predicting the crack path in brittle materials. Computer Methods in Applied Mechanics and Engineering, 391:114587, March 2022. ISSN 00457825. doi: 10.1016/j.cma.2022. 114587. URL http://arxiv.org/abs/2108.06905. arXiv: 2108.06905.\n\nKurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):\n\n251–257, 1991.\n\nKurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359–366, January 1989. ISSN 08936080. doi: 10. 1016/0893-6080(89)90020-8. URL https://linkinghub.elsevier.com/retrieve/ pii/0893608089900208.\n\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\nHamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalIn Joint European Conference on\n\ngradient methods under the polyak-ojasiewicz condition. Machine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nGünter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-Normalizing Neural Networks. arXiv:1706.02515 [cs, stat], September 2017. URL http://arxiv.org/ abs/1706.02515. arXiv: 1706.02515.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKatiana Kontolati, Somdatta Goswami, Michael D Shields, and George Em Karniadakis. On the influence of over-parameterization in manifold based surrogates and deep neural operators. arXiv preprint arXiv:2203.05071, 2022.\n\nMing-Jun Lai and Wotao Yin. Augmented $\\ell_1$ and nuclear-norm models with a globally linearly convergent algorithm. SIAM Journal on Imaging Sciences, 6(2):1059–1091, jan 2013. doi: 10.1137/120863290. URL https://doi.org/10.1137%2F120863290.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural Operator: Graph Kernel Network for Partial Differential Equations. arXiv:2003.03485 [cs, math, stat], March 2020. URL http://arxiv.org/abs/ 2003.03485. arXiv: 2003.03485.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations. arXiv:2010.08895 [cs, math], May 2021a. URL http://arxiv.org/abs/2010. 08895. arXiv: 2010.08895.\n\nZongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Markov Neural Operators for Learning Chaotic Systems. arXiv:2106.06898 [cs, math], June 2021b. URL http://arxiv.org/abs/2106.06898. arXiv: 2106.06898.\n\nChaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when and why the tangent kernel is constant. Technical Report arXiv:2010.01092, arXiv, February 2021a. URL http://arxiv.org/abs/2010.01092. arXiv:2010.01092 [cs, stat] type: article.\n\nChaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. Technical Report arXiv:2003.00307, arXiv, May 2021b. URL http://arxiv.org/abs/2003.00307. arXiv:2003.00307 [cs, math, stat] type: article.\n\nLu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):218–229, Mar 2021. ISSN 2522-5839. doi: 10.1038/s42256-021-00302-5. URL http://dx.doi.org/10.1038/s42256-021-00302-5.\n\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The Expressive Power of Neural Networks: A View from the Width. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf.\n\nSahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1): 1665–1697, 2012.\n\nSahand N. Negahban, Pradeep Ravikumar, Martin J. Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of $m$-estimators with decomposable regularizers. Statistical Science, 27(4), nov 2012. doi: 10.1214/12-sts400. URL https://doi.org/10.1214% 2F12-sts400.\n\nQuynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with linear widths. In International Conference on Machine Learning, pp. 8056–8062. PMLR, 2021.\n\nQuynh Nguyen, Marco Mondelli, and Guido F Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks. In International Conference on Machine Learning, pp. 8119–8129. PMLR, 2021.\n\nSifan Wang and Paris Perdikaris. Long-time integration of parametric evolution equations with physics-informed DeepONets. arXiv:2106.05384 [physics], June 2021. URL http://arxiv. org/abs/2106.05384. arXiv: 2106.05384.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed DeepOnets. arXiv:2103.10974 [cs, math, stat], March 2021. URL http://arxiv.org/abs/2103.10974. arXiv: 2103.10974.\n\nHui Zhang and Lizhi Cheng. Restricted strong convexity and its applications to convergence analysis of gradient-type methods in convex optimization. Optimization Letters, 9(5):961–979, 2015.\n\nHui Zhang and Wotao Yin. Gradient methods for convex minimization: better rates under weaker\n\nconditions, 2013. URL https://arxiv.org/abs/1303.4645.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 LEARNING OPERATORS\n\nU (cid:55)→ V\n\nHere we briefly outline the notion of learning for neural operators (Li et al., 2021a; 2020; Lu et al., 2021). The standard operator learning problem seeks to approximate a possibly nonlinear operator G† : that depends on the learnable parameters G†. Given observations θ. The goal is to learn an optimal set of parameters θ† such that Gθ† u(j) μ is an i.i.d sequence from the probability and G(u(j)) is possibly corrupted with noise, the objective is to find θ† as\n\nby a parametric operator Gθ∈Θ :\n\nj=1 ∈ U {\n} measure μ supported on the solution of the minimization problem\n\nwhere u(j)\n\nj=1 ∈ V }\n\nG†(u(j))\n\nU (cid:55)→ V\n\nand\n\n∼\n\n≈\n\nU\n\n{\n\nn\n\nn\n\nθ† = arg min\n\nθ∈Θ\n\nEu∼μ\n\n(cid:2)\n\nC\n\n(cid:0)Gθ(u), G†(u)(cid:1)(cid:3) ,\n\n(28)\n\nwhere a suitable cost functional. This is analogous to the notion of learning in finite dimensions, which is precisely the setup classical deep learning used for.\n\nare separable Banach spaces and\n\nand\n\nU\n\nV\n\nC\n\nA.2 DEEPONET ARCHITECTURE\n\nFigure 4: A schematic of the unstacked DeepONet architecture (Lu et al., 2021) used in this study. Note that the input functions need not be sampled on a structured grid of points in general.\n\nA.3 OPTIMIZATION GUARANTEES FOR DEEPONETS: SMOOTH ACTIVATIONS\n\nTheorem 4.2 (RSC of the loss). Under the assumptions Assumptions 1, 2 and 3, Qt\n\n(θ0) with high probability we have\n\nBEuc ρ\n\nκ ∩\n\nθ′\n\n∀\n\n∈\n\nBt\n\nκ :=\n\n(θ′)\n\nL\n\n≥ L\n\n(θt) +\n\nθ′\n\n⟨\n\n−\n\nθt,\n\nθ\n\n∇\n\nL\n\n(θt)\n\n+\n\n⟩\n\nαt 2 ∥\n\nθ′\n\nθt\n\n−\n\n2 , where αt = c1\n\n2 ∥\n\nθ ̄Gt\n\n∥∇\n\n2 2 − ∥\n\nc2 √m\n\n,\n\nwhere ̄Gt = 1 i=1 satisfies RSC w.r.t (Bt\n\n(cid:80)n\n\nn\n\n(cid:80)qi\n\nj=1 Gθt(u(i))(y(i) 1\nqi κ, θt) whenever αt > 0.\n\nj ) and c1, c2 > 0 are constants. Thus, the loss\n\n(19) (θ)\n\nL\n\nProof. From the Taylor expansion in (15), to establish (19) it suffices to focus on the second order term and for θ′\n\nBt show\n\n∈\n\n(θ′\n\n−\n\nθt)⊤H( ̃θ)(θ\n\nθt)\n\nαt\n\nθ′\n\n∥\n\nθt\n\n2\n\n2 .\n\n∥\n\n−\n\n≥\n\n−\n\n(29)\n\n13\n\nrrRnnqiTrunk Net:Branch Net:Output LocationsInput Functions“inner product”11gfmfmgmfmgwidth of the branch netwidth of the trunk netKKUnder review as a conference paper at ICLR 2023\n\nGiven the 2 note that\n\n×\n\n2 block structure of the Hessian as in (8), denoting δθ := θ′\n\nθt for compactness, we\n\nδθ⊤H( ̃θ)δθ = δθ⊤\n\nf Hf f δθf (cid:123)(cid:122) (cid:125) T1\n\n+ 2δθ⊤ (cid:124)\n\nf Hf gδθg (cid:125)\n\n(cid:123)(cid:122) T2\n\n+ δθ⊤ (cid:124)\n\n(cid:124)\n\n− g Hggδθg (cid:125)\n\n(cid:123)(cid:122) T3\n\n.\n\n(30)\n\nFocusing on T1 and using the exact form of Hf f as in (11), we have\n\nT1 =\n\n1 n\n\n(a)\n\n≥\n\n1 n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n1 qi\n\n1 qi\n\nj=1\n\nqi (cid:88)\n\nj=1\n\nqi (cid:88)\n\n(cid:42)\n\nl′′\n\ni,j\n\nδθf ,\n\n(cid:43)2\n\nK (cid:88)\n\nk=1\n\ng(i) k,j∇\n\nθf f (i)\n\nk\n\n(cid:68)\n\nδθf ,\n\nl′′\n\ni,j\n\n∇\n\nθf Gθ(u(i))(y(i) j )\n\n+\n\n(cid:69)2\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nl′\n\nij\n\nK (cid:88)\n\nj=1\n\nk=1\n\nλc0c(f ) √mf ∥\n\nδθf\n\n2 ,\n\n2 ∥\n\n−\n\ng(i) k,jδθ⊤\n\nf ∇\n\n2 θf\n\nf (i) k δθf\n\n(31)\n\nwhere (a) follows from Assumption 2 and Lemma 4.1. The analysis for T3 is similar, and we get\n\nT3\n\n≥\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n(cid:68)\n\nl′′\n\ni,j\n\nδθg,\n\n∇\n\nθg Gθ(u(i))(y(i) j )\n\n(cid:69)2\n\nλc0c(g) √mg ∥\n\nδθg\n\n2 .\n\n2 ∥\n\n−\n\n(32)\n\nFocusing on T2 and using the exact forms in terms of H (1)\n\nf g and H (2)\n\nf g as in (11), we have\n\n\n\nT2 = δθ⊤\n\nf\n\n\n\n1 2\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nl′\n\nij\n\nK (cid:88)\n\nj=1\n\nk=1\n\n\n\nθf f (i)\n\nk ∇\n\n∇\n\nθg g(i) ⊤\n\nk,j\n\n δθg\n\n\n\n+ δθ⊤\n\nf\n\n\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:33)\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\nl′′\n\ni,j\n\n(cid:32) K\n\n(cid:88)\n\nk=1\n\ng(i) k,j∇\n\nθf f (i)\n\nk\n\n(cid:33) (cid:32) K\n\n(cid:88)\n\nk′=1\n\n(cid:33)\n\nf (i)\n\nk′\n\n∇\n\nθg g(i) ⊤\n\nk′,j\n\n δθg\n\n(a)\n\n= δθ⊤\n\nf\n\n(cid:32) ̃q\n\n(cid:88)\n\nh=1\n\nσhahb⊤\n\nh\n\nδθg + δθ⊤\n\nf\n\n\n\n=\n\n ̄q (cid:88)\n\nh=1\n\nδθf , ah\n\nσh\n\n⟨\n\nδθg, bh\n\n+\n\n⟩\n\n⟩⟨\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\n\n\n1 n\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\nl′′ i,j∇\n\nθf Gθ(u(i))(y(i) j )\n\n∇\n\nθg Gθ(u(i))(y(i)\n\nj )⊤\n\n δθg\n\n\n\n(cid:68)\n\nδθf ,\n\nl′′\n\ni,j\n\n∇\n\nθf Gθ(u(i))(y(i) j )\n\n(cid:69) (cid:68)\n\nδθg,\n\nθg Gθ(u(i))(y(i) j )\n\n(cid:69)\n\n∇\n\nn (cid:88)\n\ni=1\n\nqi (cid:88)\n\nj=1\n\n(b)\n\n≥\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n(cid:68)\n\nδθf ,\n\nl′′\n\ni,j\n\n∇\n\nθf Gθ(u(i))(y(i) j )\n\n(cid:69) (cid:68)\n\nδθg,\n\nθg Gθ(u(i))(y(i) j )\n\n(cid:69)\n\n.\n\n∇\n\n0, where (a) follows from SVD as in (16), (b) follows since by Definition 3, ⟨\n0. Combining (31), (33), (32), using m = mg = mf and c1 = max(c(f ), c(g)), we have\n\nδθf , ah ⟨\n\n⟩ ≥\n\n(33)\n\nδθg, bh\n\n⟩ ≥\n\nδθ⊤H( ̃θ)δθ\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n≥\n\n(cid:16)(cid:68)\n\nl′′\n\ni,j\n\nδθf ,\n\n∇\n\nθf Gθ(u(i))(y(i) j )\n\n(cid:69)\n\n(cid:68)\n\n+\n\nδθg,\n\nθg Gθ(u(i))(y(i) j )\n\n∇\n\n(cid:69)(cid:17)2\n\nλc0c1 √m ∥\n\nδθ\n\n2 2\n∥\n\n−\n\n(a)\n\n≥\n\na\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\n(cid:16)(cid:68)\n\nj=1\n\nδθf ,\n\n∇\n\nθf Gθ(u(i))(y(i) j )\n\n(cid:69)\n\n(cid:68)\n\n+\n\nδθg,\n\nθg Gθ(u(i))(y(i) j )\n\n∇\n\n(cid:69)(cid:17)2\n\nλc0c1 √m ∥\n\nδθ\n\n2 2\n\n∥\n\n−\n\n ̄Gθ(u(i))(y(i) j )\n\n(cid:69)\n\n(cid:68)\n\n+\n\nδθg,\n\n ̄Gθ(u(i))(y(i) j )\n\n(cid:69)(cid:17)2\n\nλc0c1 √m ∥\n\nδθ\n\n2 2\n∥\n\n−\n\nθg\n\n∇\n\nθf\n\n∇\n\n(b)\n\n(cid:16)(cid:68)\n\na\n\nδθf ,\n\n≥ = a (cid:10)δθ,\n\n(c)\n\naκ2\n\n≥ = αt\n\n∥∇ 2\n∥\n\nδθ\n\n2 ,\n\n∥\n\n(cid:11)2\n\nθ ̄Gθ\n\n∇\n\n−\n\nλc0c1 √m ∥\n\nθ ̄Gθ\n\nδθ\n\n2\n\n2∥\n\n∥\n\n2\n\n2 −\n\n∥\n\nδθ\n\n2 2\n\n∥ λc0c1 √m ∥\n\nδθ\n\n2 2\n\n∥\n\nwhere (a) follows from Assumption 2, (b) follows from Jensen’s inequality and with ̄Gθ = j ) as in Definition 3, (c) follows from Definition 3, and αt =\n\nj=1 Gθ(u(i))(y(i)\n\n(cid:80)qi\n\n(cid:80)n\n\ni=1\n\n1 n\naκ2\n\n1 qi\n\nθ ̄Gθ\n\n∥∇\n\n2\n\n2 −\n\n∥\n\nλc0c1√\n\nm . That completes the proof.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nProposition 1 (Qt the restricted set Qt\n\nκ is non-empty.\n\nκ is non-empty). For over-parameterized branch and trunk nets with pf , pg > qk,\n\nProof. We simply construct a θ′ = [θ′ κ along with the value of κ. Without loss f\nof generality, we make θt the origin of the coordinate system and work with the unit vector ̄g = [ ̄g⊤ . Further, we assume θ′ also to be a unit vector. Then, our problem reduces to feasibility of the following system of two quadratic equations over θ′\n\ng ]⊤ = ∇θ ̄Gθt ∥∇θ ̄Gθt ∥2\n\nRpf , θ′\n\nf ̄g⊤\n\n⊤ θ′ g\n\nRpg :\n\n⊤]⊤\n\nQt\n\n∈\n\nf ∈\n\ng ∈\n\nθ′⊤\n\nf\n\n(cid:32) ̃q\n\n(cid:88)\n\nh=1\n\n(cid:33)\n\nσh1ah1 b⊤\n\nh1\n\nθ′\n\ng ≥\n\n0\n\n(cid:0)\n\nθ′ ⟨\n\nf , ̄gf\n\n+\n\ng, ̄gg\n\nθ′ ⟨\n\n(cid:1)2 ⟩\n\n⟩ Rpf are orthogonal unit vectors, bh\n\n≥\n\nκ2 ,\n\nf ̄g⊤\n\nwhere σh1 , σh2 > 0, ah ̄g = [ ̄g⊤ g ]⊤ ∈\nWithout loss of generality, assume f , ̄gf\n\nRpf +pg and θ′ = [θ′ ̄gf\n\nreduces to\n\n∈\n\nθ′ ⟨\n\n2 ⟩\n\n≥\n\nf ; θ′ g]\n\n∈ ̄gg ≥ ∥\n\nRpf +pg are unit vectors, and we can choose κ\n\n∈\n\nRpg are orthogonal unit vectors, (0, 1]. g = 0 so that our feasibility condition f = ̄gf\n\nso that\n\n∈\n\n∥ ̄gf ∥2\n\n(0, 1]. Finally, set θ′\n\n2. Then, set θ′\n\n2 ∥\nκ2 for some suitably chosen κ\n\n∥\n\n∥\n\n∈\n\nθ′\n\nf , ̄gf\n\n2 =\n\n⟩\n\n⟨\n\n(cid:18) ̄gf ̄gf\n\n∥\n\n∥\n\n(cid:19)2\n\n ̄gf\n\n2\n\n=\n\n ̄gf\n\n∥\n\n2 := κ2 , 2\n∥\n\nso that κ\n\n∈\n\n(0, 1] (in fact, κ\n\n≥\n\n1/√2) as desired. That completes the proof.\n\nTheorem 4.3 (Smoothness of Loss). Under the assumptions Assumptions 1, 2 and 3, with high (θ) is β-smooth with β = bρ2 + c m with c = max(c(f ), c(g)), ρ = probability, for θ max(ρ(f ), ρ(g)) with c(f ), c(g), ρ(f ), ρ(g) as in Lemma 4.1.\n\nBEuc ρ\n\n√ λ√\n\n(θ0),\n\nL\n\n∈\n\nProof. By the second order Taylor expansion about ̄θ, we have 2 (θ′\n\n ̄θ), where ̃θ = ξθ′ + (1\n\n ̄θ)⊤ ∂2L( ̃θ)\n\n∂θ2 (θ′\n\nξ) ̄θ for some ξ\n\nL\n\n1\n\n(θ′) =\n\n( ̄θ) + [0, 1]. Then,\n\nθ′ ⟨\n\nL\n\n ̄θ,\n\n( ̄θ) ⟩\n\nL\n\nθ\n\n∇\n\n−\n\n+\n\n−\n\n−\n\n−\n\n∈\n\n ̄θ)⊤ ∂2\n\n( ̃θ)\n\nL ∂θ2\n\n(θ′\n\n−\n\n ̄θ) = (θ′\n\n(θ′\n\n−\n\n ̄θ)⊤\n\n−\n\n(cid:18) 1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\nl′′ i,j∇\n\nG ̃θ(u(i))(y(i) j )\n\n∇\n\nG ̃θ(u(i))(y(i)\n\nj )⊤\n\n+ l′\n\ni,j∇\n\n2G ̃θ(u(i))(y(i) j )\n\n(cid:19)\n\n(θ′\n\n ̄θ)\n\n−\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n(cid:68)\n\nθ′\n\nl′′\n\ni,j\n\n=\n\n1 n\n\n(cid:124)\n\n ̄θ,\n\n∇\n\n−\n\nG ̃θ(u(i))(y(i) j )\n\n(cid:123)(cid:122) I1\n\n(cid:69)2\n\n(cid:125)\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\n+\n\n1 n\n\n(cid:124)\n\ni,j(θ′ l′\n\n ̄θ)⊤\n\n∇\n\n−\n\n2G ̃θ(u(i))(y(i)\n\nj )(θ′\n\n(cid:123)(cid:122) I2\n\n ̄θ)\n\n.\n\n−\n\n(cid:125)\n\nNow, note that\n\nI1 =\n\n1 n\n\n(a)\n\n≤\n\nb n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n1 qi\n\n1 qi\n\nqi (cid:88)\n\n(cid:68)\n\nθ′\n\nl′′\n\ni,j\n\nj=1\n\nqi (cid:88)\n\nj=1\n\n(cid:13) (cid:13) (cid:13)∇\n\n ̄θ,\n\n∇\n\n−\n\nG ̃θ(u(i))(y(i) j )\n\n(cid:69)2\n\nG ̃θ(u(i))(y(i) j )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2 ∥\n\nθ′\n\n2 2\n\n ̄θ ∥\n\n−\n\n(b)\n\n≤\n\nbρ2\n\nθ′\n\n∥\n\n ̄θ 2\n∥\n\n2 ,\n\n−\n\nwhere (a) follows by the Cauchy-Schwartz inequality and (b) from Lemma 4.1.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFor I2, with Qt,(i,j) = (θ′\n\n ̄θ)⊤\n\n∇\n\n−\n\n2G ̃θ(u(i))(y(i)\n\nj )(θ′\n\n−\n\n ̄θ), we have\n\nQt,(i,j)| ≤ ∥\n\n|\n\nθ′\n\n ̄θ 2\n2 ∥\n\n−\n\n(cid:13) (cid:13) (cid:13)∇\n\n2G ̃θ(u(i))(y(i) j )\n\n(cid:13) (cid:13) (cid:13)2 ≤\n\nc\n\nThen, we have\n\nθ′\n\n∥\n\n− √m\n\n ̄θ ∥\n\n2 2\n\n.\n\nI2 =\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\nqi (cid:88)\n\nj=1\n\ni,j(θ′ l′\n\n ̄θ)⊤\n\n∇\n\n−\n\n2G ̃θ(u(i))(y(i)\n\nj )(θ′\n\n ̄θ)\n\n−\n\n(a)\n\n≤\n\nλ\n\n(cid:32)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n1 qi\n\n(cid:33)1/2\n\nQ2\n\nt,(i,j)\n\nθ′\n\nc\n\nλ\n\n≤\n\n∥\n\n− √m\n\n ̄θ ∥\n\n2 2\n\n,\n\nwhere (a) follows by Cauchy-Schwartz. Putting the upper bounds on I1 and I2 back, we have\n\n ̄θ)⊤\n\n(θ′\n\n−\n\n∇\n\n2G ̃θ(u(i))(y(i)\n\nj )(θ′\n\n ̄θ)\n\n−\n\n≤\n\n(cid:20)\n\nbρ2 +\n\n(cid:21)\n\nc√λt √m\n\nθ′\n\n∥\n\n ̄θ 2\n∥\n\n2 .\n\n−\n\nThis completes the proof.\n\nLemma 4.4 (RSC = Restricted PL). The RSC and Smoothness of the Loss together imply a form of Polyak-Łojasiewicz (PL) condition w.r.t. the tuple (Bt, θt), unlike standard PL which holds without restrictions (Karimi et al., 2016).\n\n⇒\n\nProof. Define\n\nBy Theorem A.3,\n\nFurther, note that ˆ L\n\nˆ θt(θ) := L\n\n(θt) +\n\nL\n\nθ\n\n⟨\n\n−\n\nθt,\n\nθ\n\n(θt) ⟩\n\nL\n\n∇\n\n+\n\nαt 2 ∥\n\nθ\n\nθt\n\n2 .\n\n2 ∥\n\n−\n\nθ′\n\n∀\n\n∈\n\nBt, we have\n\nˆ θt(θ′) . L\nθt(θ) is minimized at ˆθt+1 := θt\n\n(θ′)\n\n≥\n\nL\n\nθ\n\ninf θ\n\nθt(θ) = ˆ ˆ\nL L\n\nθt( ˆθt+1) =\n\n(θt)\n\nL\n\nThen, we have\n\n(34)\n\n(θt)/αt and the minimum value is:\n\n− ∇\n\nL 1\n\n−\n\n2αt ∥∇\n\nθ\n\n(θt)\n\nL\n\n2 .\n\n2 ∥\n\ninf θ∈Bt L\n\n(θ)\n\n(a)\n\n≥\n\ninf θ∈Bt\n\nˆ θt (θ) L\n\ninf θ\n\nˆ θt(θ) = L\n\n≥\n\n(θt)\n\nL\n\n1\n\n−\n\n2αt ∥∇\n\nθ\n\nL\n\nwhere (a) follows from (34). Rearranging terms completes the proof.\n\n(θt)\n\n2 ,\n\n2 ∥\n\nLemma 1 (Local Loss Reduction in Bt). Let αt, β be as in Theorems A.3 and A.3 respectively, and Bt := Qt (θt). Under assumptions Assumptions 1, 2 and 3, for gradient descent (θ0) κ ∩ with step size ηt = ωt (θ), we have with high probability\n\n(0, 2), for any θt+1\n\nBEuc ρ2\n\nBEuc ρ\n\n∩ β , ωt\n\n∈\n\n(θt+1)\n\nL\n\n− L\n\n(θt+1)\n\n≤\n\n(2\n\n−\n\nωt)\n\n(θt)\n\n( L\n\n− L\n\n(θt+1)) .\n\n(35)\n\narginf θ∈Bt L\n\n(cid:19)\n\n∈ αtωt β\n\n(cid:18)\n\n1\n\n−\n\nProof. Since\n\nL\n\nis β-smooth by Theorem A.3, we have\n\n(θt+1)\n\nL\n\n(θt) +\n\n≤ L\n\nθt+1\n\n⟨\n\n−\n\nθt,\n\nθ\n\n∇\n\n+\n\nβ 2 ∥\n\nθt+1\n\nθt\n\n2 2\n∥\n\n−\n\nθ\n\n2 ∥∇\n\nL\n\n∥\n\n(θt)\n\n2 2\n\n(36)\n\n=\n\n=\n\n(θt)\n\nL\n\n(θt)\n\nL\n\n−\n\n−\n\nηt\n\nθ\n\n∥∇ (cid:18)\n\nL\n\nηt\n\n1\n\n−\n\nL\n\n(θt)\n\n⟩ βη2 t\n\n2\n\n2 + ∥\n(cid:19)\n\nθ\n\n∥∇\n\nL\n\n(θt)\n\n2 2\n\n∥\n\n(θt)\n\nβηt 2\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nSince ̄θt+1\n\narginf θ∈Bt L\n\n∈\n\n(θ) and αt > 0 by assumption, from Lemma A.3 we obtain\n\nHence\n\nθ\n\n−∥∇\n\nL\n\n(θt)\n\n2 2 ≤ − ∥\n\n2αt(\n\n(θt)\n\n− L\n\n( ̄θt+1)) .\n\nL\n\n(θt+1)\n\nL\n\n− L\n\n( ̄θt+1)\n\n(θt)\n\n≤ L\n\n− L\n\n( ̄θt+1)\n\n(cid:18)\n\nηt\n\n1\n\n(cid:19)\n\nβηt 2\n\n−\n\n−\n\nθ\n\n∥∇\n\nL\n\n(θt)\n\n2 2\n∥\n\n(a)\n\n≤ L (cid:18)\n\n(θt)\n\n− L\n\n( ̄θt+1)\n\n(cid:18)\n\n=\n\n1\n\n−\n\n2αtηt\n\n1\n\n−\n\nηt\n\n− βηt 2\n\n(cid:19)\n\n(cid:18)\n\n1\n\nβηt 2\n\n−\n\n2αt(\n\nL\n\n(θt)\n\n− L\n\n( ̄θt+1))\n\n(cid:19)(cid:19)\n\n(θt)\n\n( L\n\n− L\n\n( ̄θt+1))\n\nwhere (a) follows for any ηt\n\n2\n\nβ because this implies 1\n\n−\n\n≤\n\n0. Choosing ηt = ωT\n\nβ , ωt\n\n(0, 2),\n\n∈\n\nβηt 2 ≥ (cid:19)\n\nωt)\n\n(θt)\n\n( L\n\n− L\n\n( ̄θt+1)) .\n\n(θt+1)\n\nL\n\n− L\n\n( ̄θt+1)\n\n(cid:18)\n\n1\n\n≤\n\nαtωt β\n\n(2\n\n−\n\n−\n\nThis completes the proof.\n\nTheorem 4.5 (Global Loss Reduction). Consider the same conditions as in Theorem 4.3 and αt > 0 (0, 2), for t ∈\n(θ) with where β is defined as in Theorem 4.3. Then, with high probability,\n\n[T ] for a gradient descent update θt+1 = θt\n\nβ for some ωt arginf\n\n(θt) with ηt = ωt\n\nηt\n\n∇\n\n ̄θ\n\n−\n\nL\n\n∈\n\nθ\n\n∀\n\n∈\n\nθ∈BEuc\n\nρ\n\n(θ0)L\n\n0\n\n≤\n\nγt := L( ̄θt+1)−L( ̄θ)\n\nL(θt)−L( ̄θ) < 1 and ̄θt+1 (cid:18)\n\narginf θ∈Qt\n\nκ∩BEuc\n\nρ\n\n∈\n\n(θt+1)\n\nL\n\n− L\n\n( ̄θ)\n\n1\n\n−\n\n≤\n\nαtωt(1 β\n\n−\n\nγt)\n\n(2\n\n(θ), we have\n\n(θ0) L (cid:19)\n\nωt)\n\n−\n\n(θt)\n\n( L\n\n− L\n\n( ̄θ)) .\n\n(20)\n\nProof. We start by showing γt = L( ̄θt+1)−L(θ∗)\n\nL(θt)−L(θ∗)\n\n ̄θt+1\n\n∈\n\narginf θ∈Bt L\n\n(θ), and θt+1\n\nQt\n\nκ ∩\n\n∈\n\nBEuc ρ\n\nsatisfies 0\n\nγt < 1. Since θ∗\n\n(θ0)L (θ0) by the definition of gradient descent, we have\n\nθ∈BEuc\n\n≤\n\n∈\n\nρ\n\narginf\n\n(θ),\n\n(θ∗)\n\nL\n\n( ̄θt+1)\n\n≤ L\n\n(θt+1)\n\n≤ L\n\n(a)\n\n≤ L\n\n(θt)\n\n1 2β ∥∇\n\nθ\n\n−\n\n(θt)\n\nL\n\n2 <\n\n2 ∥\n\n(θt) ,\n\nL\n\n(θ∗) and\n\n(θt) >\n\nL\n\n≥ L\n\n(θ∗), we have γt\n\nL\n\n0. Further,\n\nwhere (a) follows from (36). Since since\n\n( ̄θt+1) <\n\n(θt), we have γt < 1.\n\nL\n\n( ̄θt+1)\n\nL Now, with ωt\n\nL\n\n(0, 2), we have\n\n∈ (θ∗) =\n\n(θt+1)\n\nL\n\n− L\n\n≥\n\n(cid:19)\n\n(θt+1)\n\nL (cid:18)\n\n1\n\n≤\n\n− L\n\nαtωt β\n\n− (cid:18)\n\n( ̄θt+1) + (cid:19)\n\n( ̄θt+1)\n\nL\n\n(θ∗)\n\n− L\n\n(2\n\n−\n\nωt)\n\n(\n\n( ̄θt+1)\n\n(cid:18)\n\n1\n\n−\n\n−\n\n(θt)\n\nL αtωt β\n\n− L\n\n(cid:19)\n\n(2\n\n−\n\nωt)\n\nL\n\n( ̄θt+1)) +\n\n(cid:18)\n\n1\n\n( ̄θt+1)\n\nαtωt β\n(cid:18)\n\n− (cid:19)\n\n−\n\nL\n\n(2\n\n−\n\nωt)\n\n( L\n\n( ̄θt+1)\n\n− L\n\n(θ∗)\n\n−\n\n(cid:18)\n\n1\n\nαtωt β\n\n(2\n\n−\n\n−\n\n(θ∗))\n\n(cid:19)\n\nωt)\n\n(cid:19)\n\n(θ∗)\n\nL\n\n+\n\n(cid:18)\n\n=\n\n1\n\n(cid:18)\n\n=\n\n1\n\n(cid:18)\n\n=\n\n1\n\n−\n\n−\n\n−\n\nL αtωt β\n\nαtωt β\n\nαtωt β\n\n(cid:19)\n\nωt)\n\n(cid:19)\n\nωt)\n\n(\n\n(\n\n(θt)\n\nL\n\n− L\n\n(θ∗)) +\n\n(θt)\n\nL\n\n− L\n\n(θ∗)) +\n\n(cid:19)\n\nγt)(2\n\nωt)\n\n−\n\n(θt)\n\n( L\n\n− L\n\n(2\n\n(2\n\n(1\n\n−\n\n−\n\n−\n\nαtωt β\n\nαtωt β\n\n(2\n\n(2\n\n−\n\n−\n\n(θ∗)) .\n\n( ̄θt+1)\n\nωt)(\n\nL\n\n− L\n\n(θ∗))\n\nωt)γt(\n\n(θt)\n\nL\n\n(θ∗))\n\n− L\n\nThat completes the proof.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 OPTIMIZATION GUARANTEES FOR DEEPONETS: RELU ACTIVATIONS\n\nRecall the DeepONet predictor (1)\n\n(cid:16)\n\nu(i)(cid:17)\n\nGθ\n\n(y(i)\n\nj ) :=\n\nK (cid:88)\n\nk=1\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\ngk\n\n(cid:16)\n\nfk\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n.\n\n(37)\n\nIn the analysis, it is useful to distinguish between the parameters up to the pre-final layer and the final layer, i.e. dim(θ) = Mf + Mg + (mf + mg)K, where Mf and Mg denote the number of K\nparameters in the branch and trunk nets till the pre-final layer respectively and mf are the number of weights in the last layer of the branch and trunk nets respectively. In essence we have dim(θf ) = Mf + mf K. We note that it is sufficient to show positive definiteness of the above NTK at initialization. Once that has been established, standard approaches (Jacot et al., 2018; Du et al., 2019; Arora et al., 2019b;a; Allen-Zhu et al., 2019) allow one to show the geometric convergence of (S)GD. In the sequel it proves useful to rewrite the branch and trunk net outputs as:\n\nK and dim(θg) = Mg + mg\n\nK and mg\n\n·\n\n·\n\n·\n\n·\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\nfk\n\n=\n\nmf (cid:88)\n\nh=1\n\nw(f )\n\nk,h\n\n ̄fh\n\n(cid:16)\n\nθ ̄f ; u(i)(cid:17)\n\n,\n\n(cid:16)\n\ngk\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n=\n\nmg (cid:88)\n\nh′=1\n\nw(g)\n\nk,h′ ̄gh′\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)\n\n,\n\nk\n\n∀\n\n∈\n\n[K],\n\n(38)\n\n∈\n\nk,h, h\n\n[mf ] are the weights of the linear last layer of the branch net and w(g)\n\nwhere w(f ) [mg] are the weights of the linear last layer of the trunk net. Similarly, θ ̄f and θ ̄g are the parameters leading up to the pre-final layer in branch net with mf outputs [ ̄fh, h [mf ]] and trunk net with mg outputs [ ̄gh′, h′ [mg]] respectively. We will denote by θf,k all the parameters corresponding to fk, [K]. i.e. θf,k :=\n\nk,h′, h′\n\n∈\n\n∈\n\nwhich includes all the parameters needed for fk for each k w(g)\n\nSimilarly we denote by θg,k, all the parameters corresponding to gk, i.e. θg,k := [mg], θ ̄g\n\n∈ w(f ) k,h, h {\n\n[mf ], θ ̄f }\n\n∈ k,h′, h′\n\n∈\n\n∈\n\n{\n\n. }\n\nWe can explicitly write the NTK for the DeepONet model, specifically entry corresponding to the\n\ninputs (cid:68)\n\nu(i), y(i) {\n\nj } θGθ(u(i))(y(i)\n\nj ) ,\n\nand\n\nu(i′), y(i′) {\nθGθ(u(i′))(y(i′) j′ )\n\nas:\n\n}\n\nj′\n\n∇\n\n∇\n\nK (cid:88)\n\n=\n\n(cid:16)\n\ngk\n\nθg; y(i)\n\nj\n\n(cid:69)\n\n(cid:17)\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\ngk′\n\n(cid:17) (cid:68)\n\n∇\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\n,\n\nθf fk\n\nθf fk′\n\n∇\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)\n\nk,k′=1\n\nK (cid:88)\n\n+\n\nk,k′=1\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\nfk\n\nfk′\n\n(cid:16)\n\nθf ; u(i′)(cid:17) (cid:68)\n\n∇\n\nθg gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\nθg gk′\n\n,\n\n∇\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)(cid:69)\n\n=\n\nK (cid:88)\n\nk=1\n\ngk (cid:124)\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\ngk (cid:123)(cid:122) T (1)\n\nk\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)\n\n(cid:68)\n\n∇\n\n(cid:125)\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\n,\n\nθf fk\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)\n\nθf fk\n\n∇\n\n+\n\n(cid:16)\n\nK (cid:88)\n\nk=1\n\nfk (cid:124)\n\nθf ; u(i)(cid:17)\n\nfk (cid:123)(cid:122) T (2)\n\nk\n\n(cid:16)\n\nθf ; u(i′)(cid:17)\n\n(cid:125)\n\nθg gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n(cid:68)\n\n∇\n\nθg gk\n\n,\n\n∇\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)(cid:69)\n\nK (cid:88)\n\n+\n\nk,k′=1 k̸=k′\n\nK (cid:88)\n\n+\n\nk,k′=1 k̸=k′\n\ngk (cid:124)\n\nfk (cid:124)\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:16) θg; y(i′)\n\nj′\n\n(cid:17)\n\n(cid:68)\n\n∇\n\n(cid:125)\n\n(cid:17)\n\ngk′ (cid:123)(cid:122) T (3) k,k′\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\n,\n\nθf fk\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)\n\nθf fk′\n\n∇\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\nfk′ (cid:123)(cid:122) T (4) k,k′\n\n(cid:16)\n\nθf ; u(i′)(cid:17)\n\n(cid:125)\n\nθg gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n(cid:68)\n\n∇\n\nθg gk′\n\n,\n\n∇\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)(cid:69)\n\n.\n\n(39)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(θ) is positive definite at initialization). Given standard initialization for the branch Theorem 5.1 ( and trunk nets, and granted that the individual branch and trunk net NTKs are positive definite (24)-(25), the NTK of DeepONet is positive definite at initialization, i.e.\n\nK\n\nwhere α denotes an arbitrary block unit vector with n blocks, and αi,j corresponds to the j-th entry in the i-th block and c denotes a positive constant.\n\nα⊤E[\n\nK\n\n(θ)]α\n\n≥\n\nc > 0,\n\n(26)\n\nProof. The proof follows as a direct consequence of Proposition 2 together with Propositions 3 and 4\n\nProposition 2. With the branch and trunk net weights initialized using standard initialization techniques and the last layer of branch layer initialized according to Assumption 3, we have\n\nE[T (3)\n\nk,k′\n\nθ ̄g, θ ̄g] = 0 , E[T (4) |\n\nk,k′\n\nθ ̄g, θ ̄g] = 0 , |\n\nk, k′\n\n[K] .\n\n∈\n\n(40)\n\nwhere T (3)\n\nk,k′ is defined in (39)\n\nProof. As noted in Assumption 3, the last layer weights for the branch and trunk nets are initialized as zero mean Gaussians, i.e., w(f ) [mg], similar to the other layers. Now,\n\nmK ) for k\n\nk,h, w(g)\n\n[mf ], h′\n\n[K], h\n\n∼ N\n\n(0,\n\nk,h′\n\n∈\n\n∈\n\n∈\n\n1\n\nT (3)\n\nk,k′ = gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)\n\ngk′\n\n(cid:32) mg (cid:88)\n\nh′=1\n\nmg (cid:88)\n\n=\n\n=\n\nw(g)\n\nk,h′ ̄gh′\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)\n\n(cid:33)  \n\nmg (cid:88)\n\n ̃h′=1\n\nw(g)\n\nk′, ̃h′ ̄g ̃h′\n\n(cid:16) θ ̄g; y(i′)\n\nj′\n\n(cid:17)\n\n\n\n\n\n(41)\n\nw(g)\n\nk,h′w(g)\n\nk′, ̃h′ ̄gh′\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)\n\n(cid:16) θ ̄g; y(i′)\n\nj′\n\n(cid:17)\n\n,\n\n ̄g ̃h′\n\nwhich in turn implies\n\nE[T (3)\n\nk,k′\n\n|\n\nh′, ̃h′=1\n\nmg (cid:88)\n\nθ ̄g, θ ̄g] =\n\nE[w(g)\n\nk,h′w(g)\n\nk′, ̃h′] ̄gh′\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)\n\n(cid:16)\n\nθ ̄g; y(i′)\n\nj′\n\n(cid:17)\n\n ̄g ̃h′\n\nh′, ̃h′=1 mg (cid:88)\n\n(a) =\n\nh′, ̃h′=1\n\n= 0 ,\n\nE[w(g)\n\nk,h′]E[w(g)\n\nk′, ̃h′] ̄gh′\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)\n\n(cid:16)\n\nθ ̄g; y(i′)\n\nj′\n\n(cid:17)\n\n ̄g ̃h′\n\n(42)\n\nwhere (a) follows since w(g) completes the proof.\n\nk,h′ and w(g)\n\nk′, ̃h′ are independent. The analysis for T (4)\n\nk,k′ is similar. This\n\nProposition 3. Given that the terms T (3) for any arbitrary block unit vector α\n\nk,k′ and T (4)\n\nk,k′ can be suitably bounded close to zero, we have\n\nαT\n\nK\n\n(θ)α\n\n≥\n\nλ0,f\n\nK (cid:88)\n\nk=1\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\ngk(θg; y(·) · )\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n+ λ0,g\n\nK (cid:88)\n\nk=1\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\n(cid:16)\n\n1p\n\nfk(θf ; u(·)(cid:17)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\n.\n\n⊗\n\n(43)\n\nwhere 1q denotes the q-dimensional vector of all entries equal to one.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Focusing on a quadratic form of block unit vector α, we have\n\nK\n\n(θ), and ignoring the T (3), T (4)\n\nk,k′ terms, for any arbitrary\n\nαT\n\nK\n\n(θ)α =\n\n(cid:88)\n\nαi,jαi′,j′\n\n(i,j),(i′,j′)\n\n(cid:68)\n\n∇\n\nθGθ(u(i))(y(i)\n\nj ) ,\n\nθGθ(u(i′))(y(i′) j′ )\n\n(cid:69)\n\n∇\n\n(cid:88)\n\n=\n\nαi,jαi′,j′\n\n(i,j),(i′,j′)\n\nK (cid:88)\n\nk=1\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\n(cid:16) θg; y(i′)\n\nj′\n\ngk\n\ngk\n\n(cid:17) (cid:68)\n\n∇\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\n,\n\nθf fk\n\nθf fk\n\n∇\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)\n\n(cid:88)\n\n+\n\nαi,jαi′,j′\n\nK (cid:88)\n\nk=1\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\nfk\n\nfk′\n\n(cid:16)\n\nθf ; u(i′)(cid:17) (cid:68)\n\n∇\n\nθg gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\nθg gk\n\n,\n\n∇\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)(cid:69)\n\nαi,jαi′,j′gk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:16)\n\n(cid:17)\n\ngk\n\nθg; y(i′)\n\nj′\n\n(cid:17) (cid:68)\n\n∇\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\n,\n\nθfk\n\n∇\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)\n\nθfk\n\n(i,j),(i′,j′)\n\nK (cid:88)\n\n=\n\n(cid:88)\n\nk=1\n\n(i,j),(i′,j′)\n\nK (cid:88)\n\n+\n\n(cid:88)\n\nαi,jαi′,j′fk\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\nfk\n\n(cid:16)\n\nk=1\n\n(i,j),(i′,j′)\n\nθf ; u(i′)(cid:17) (cid:68)\n\n∇\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\nθgk\n\nθgk\n\n,\n\n∇\n\n(cid:16)\n\nθg; y(i′)\n\nj′\n\n(cid:17)(cid:69)\n\nK (cid:88)\n\n(cid:88)\n\n(cid:88)\n\n(cid:16)\n\n=\n\nk=1\n\n(j,j′)\n\n(i,i′)\n\nαi,jgk\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)(cid:17) (cid:16)\n\nαi′,j′gk\n\n(cid:16) θg; y(i′)\n\nj′\n\n(cid:17)(cid:17) (cid:68)\n\n∇\n\n(cid:16)\n\nθf ; u(i)(cid:17)\n\n,\n\nθfk\n\n∇\n\n(cid:16)\n\nθf ; u(i′)(cid:17)(cid:69)\n\nθfk\n\nK (cid:88)\n\n+\n\n(cid:88)\n\n(cid:16)\n\nαi,jfk\n\n(cid:16)\n\nθf ; u(i)(cid:17)(cid:17) (cid:16)\n\nαi′,j′fk\n\n(cid:16)\n\nk=1\n\n(i,j),(i′,j′)\n\nθf ; u(i′)(cid:17)(cid:17) (cid:68)\n\n∇\n\n(cid:16)\n\nθg; y(i)\n\nj\n\n(cid:17)\n\nθgk\n\nθgk\n\n,\n\n∇\n\n(cid:16) θg; y(i′)\n\nj′\n\n(cid:17)(cid:69)\n\nK (cid:88)\n\nk=1\n\nλ0,f\n\n≥\n\n≥\n\nλmin(\n\nf,k\n\nK\n\n⊗\n\nIq)\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\ngk(θg; y(·) · )\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n+ λmin(\n\ng,k)\n\nK\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\n(cid:16)\n\n1q\n\n⊗\n\nfk(θf ; u(·)(cid:17)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\nK (cid:88)\n\nk=1\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\ngk(θg; y(·) · )\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n+ λ0,g\n\nK (cid:88)\n\nk=1\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\n(cid:16)\n\n1q\n\nfk(θf ; u(·)(cid:17)(cid:13)\n\n2 (cid:13) (cid:13) 2\n\n,\n\n⊗\n\nwhere Iq denotes the q-dimensional identity matrix since αi,jgk kernel Kf does not; 1q is the q-dimensional all ones vector since for the αi,jfk(θf ; u(i)) terms, for (cid:0)θf ; u(i)(cid:1) stays the same; λmin( a fixed i, αi,j differs with j but fk λ0,g. This completes the proof.\n\nvaries with j whereas the\n\nλ0,f ; and λmin(\n\ng,k)\n\n≥\n\n≥\n\nK\n\nK\n\nf,k\n\nθg; y(i)\n\nj\n\n(cid:16)\n\n(cid:17)\n\nNote that we require α is a block unit vector with αi,j denoting the j-th position in the i-th for convenience in dealing with the quadratic form above. Given that T (3), T (4) k,k′ terms are close to 0 in (θ) is positive definite. expectation, as shown in Proposition 2, we now need to show that the NTK\n\nK\n\nProposition 4. Given that the terms T (3) Proposition 2, we have, for any arbitrary block unit vector α, and some k\n\nk,k′ and T (4)\n\nk,k′ can be suitably bounded close to zero by\n\n[K],\n\n∈\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\ngk(θg; y(·) · )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2 ≥\n\nc1 > 0 ,\n\nor\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\n(cid:16)\n\n1p\n\n⊗\n\nfk(θf ; u(·)(cid:17)(cid:13)\n\n2 (cid:13) (cid:13)\n\n2 ≥\n\nc2 > 0 ,\n\n(44)\n\nwhere 1q denotes the q-dimensional vector of all entries equal to one.\n\nProof. Now, for the first term, for some k\n\n[K], making use of (38), we have\n\n∈\n\nαi,jgk(θg, y(i)\n\nj ) =\n\nmg (cid:88)\n\nh′=1\n\nw(g)\n\nk,h′αi,j ̄gh′\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)\n\n=\n\n(cid:68)\n\nw(g)\n\nk,· , αi,j ̄g·\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)(cid:69)\n\n.\n\nSince the trunk net is a ReLU network, following the argument in Lemma 7.1 in (Allen-Zhu et al.,\n\n2019), with probability at least 1\n\n−\n\nO(qe−Ω(m/4L)), for all (i, j) we have\n\n20\n\n(cid:16)\n\n(cid:13) (cid:13) (cid:13) ̄g·\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)(cid:13) (cid:13) (cid:13)2 ≥\n\n1/2.\n\nUnder review as a conference paper at ICLR 2023\n\nRecalling that w(g)\n\nk,h′\n\nE\n\n∼ N (cid:13) (cid:13) (cid:13)α\n\n⊙\n\n(0, 1\n\nK ), taking expectation over the randomness of w(g)\n\nk,h′, we have\n\ngk(θg; y(·) · )\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n=\n\n=\n\n=\n\n≥\n\n(i,j) (cid:88)\n\n(i,j)\n\n(cid:88)\n\n(cid:88)\n\n(cid:68)\n\nE\n\nw(g)\n\nk,· , αi,j ̄g·\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)(cid:69)2\n\n(cid:68)\n\nE[(w(g)\n\nk,· )2], α2\n\ni,j ̄g2\n\n·\n\n(cid:16)\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)(cid:69)\n\nα2\n\ni,j K\n\n(cid:16)\n\n(cid:13) (cid:13) ̄g2 (cid:13)\n\n·\n\nθ ̄g; y(i)\n\nj\n\n(cid:17)(cid:13) 2\n(cid:13) (cid:13) 2\n\n(i,j) 1\n2K ∥\n\nα\n\n2 =\n\n2 ∥\n\n1 2K\n\n.\n\nHence, we have\n\nλ0,f E\n\nK (cid:88)\n\nk=1\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\ngk(θg; y(·) · )\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2 ≥\n\n1 2\n\nλ0,f ,\n\nand, with a similar argument\n\nλ0,gE\n\nK (cid:88)\n\nk=1\n\n(cid:13) (cid:13) (cid:13)α\n\n⊙\n\n(cid:16)\n\n1p\n\n⊗\n\nAs a result, we have\n\nfk(θf ; u(·)(cid:17)(cid:13)\n\n2 (cid:13) (cid:13)\n\n2 ≥\n\n1 2\n\nλ0,g .\n\nαT E [\n\n(θ)] α\n\nK\n\nλ0,f + λ0,g 2\n\n≥\n\n> 0 .\n\n(45)\n\nThe high probability version of the result can be obtained by applying Hoeffding (for cross terms) and\n\n(cid:68)\n\nw(g)\n\nk,· , αi,j ̄g·\n\nθ ̄g; y(i)\n\nj\n\n(cid:16)\n\n(cid:17)(cid:69)2\n\n. That completes the analysis.\n\nBernstein (for square terms) bounds on\n\nA.5 EXPERIMENTAL DETAILS\n\nFor the optimizer we choose Adam (Kingma & Ba, 2014) with an adaptive learning rate schedule initialized at a learning rate η0 = 10−3. In order to generate training data for all three examples, we sample the input, denoted by u(x), from a zero mean Gaussian process (GP) on a grid [0, 1] and generate outputs corresponding to each sampled function by solving the ODE/PDE (see (Wang et al., 2021; Lu et al., 2021) for a detailed discussion on data generation). For end-to-end training we use the deep learning framework JAX (Bradbury et al., 2018) and build our code on top of (Wang et al., 2021) for Diffusion-Reaction and Antiderivative operators and we develop our own for the Burger’s equation. We now briefly outline the problems below along with the specifics of the training process for each of them.\n\nl=1 ∈\n\nxl\n\n}\n\n{\n\nm\n\nA.5.1 ANTIDERIVATIVE OPERATOR\n\nThe antiderivative (or simply the integral) operator corresponds to a linear operator defined explicitly by a linear ODE (initial value problem) in the unknown function v(x) ,\nand the constant v(0) for mathematical well-posedness, i.e.\n\n, given the input u(x)\n\n∈ U\n\n∈ V\n\ndv(x) dx\n\n= u(x),\n\nx\n\n∈\n\n[0, 1]\n\ns.t.\n\nv(0) = 0.\n\n(46)\n\nWe learn the operator mapping u(x) to its corresponding integral v(x) = Gθ(u)(x) for all x (0, 1]. For generating the training data, we sample the input functions from a univariate Gaussian process as outlined above and the output points randomly on the interval [0, 1] and choose nB = 10000 for our empirical results\n\n∈\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nA.5.2 DIFFUSION-REACTION PDE\n\nIn this example we learn the operator mapping the input forcing function u(x) to the output v(x, t) for the nonlinear Diffusion-Reaction PDE given by\n\n∂v ∂t\n\n= D\n\n∂2v ∂x2 + kv2 + u(x),\n\n(x, t)\n\n(0, 1]\n\n×\n\n∈\n\n(0, 1]\n\ns.t.\n\n \n\n\n\nv(0, x) = 0 v(t, 0) = 0 v(t, 1) = 0\n\n(47)\n\nwhere D = 0.01 and k = 0.01 are constants denoting the diffusivity and reaction rate respectively. Note that in this case we are learning the operator v(x, t) = Gθ(u)(x, t). For each sampled input, the PDE is solved using a backward finite-difference solver on a grid (x, t) of size (150 120). For training, the number of input sensors is fixed at m = 120. The number of input samples (n) is chosen to be 5000 and nB = 10000.\n\n×\n\nA.5.3 BURGER’S EQUATION\n\nFinally, we look at the Burger’s equation benchmark similar to the one investigated in (Li et al., 2021a) with the distinction that we learn a mapping from the initial condition v(x, 0) = u(x) to the solution v(x, t) for (x, t)\n\n(0, 1]\n\n[0, 1]\n\n+ v\n\n× ∂v ∂x −\n\n∈ ∂v ∂t (cid:26)v(x, 0) = u(x), v(1, t) = v(0, t)\n\nν\n\n∂2v ∂x2 = 0,\n\n(0, 1) (0, 1)\n\nx t\n\n∈ ∈\n\n(x, t)\n\n(0, 1)\n\n(0, 1]\n\n×\n\n∈\n\n(48)\n\nWe generate the training data using a stiff PDE integrator chebfun (Driscoll et al., 2014) on a grid resolution of (501, 501) and p = 200 training points sampled randomly on the solution grid.\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThe paper analyzes Deep operator Networks (DeepONets) regarding the optimization convergence guarantees. The author provides optimization analysis on two different networks: smooth activations and ReLU activations. Their analysis shows that the overparameterization on branch and trunk networks allows faster convergence to the global solution.\n\n# Strength And Weaknesses\n\nStrength\n\n1. The paper is well-written and easy to read.\n2. The author's convergence analysis includes both smooth activations and ReLU activations.\n3. Theoretical results also match the empirical observations.\n\nWeakness\n1. Deng et al. (https://arxiv.org/abs/2102.10621) have studied the convergence analysis on DeepONets. The author should cite this work in the modified draft.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: Well written. Please add the Deng et al. to the related work.\nQuality: Good\nNovelty: There exists a previous literature on convergence analysis of DeepONet. However, the analysis technique is different. \nReproducibility: N/A\n\n# Summary Of The Review\n\nOverall, I believe it is a good paper to have the better understanding on DeepONet.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" investigates the optimization guarantees for Deep Operator Networks (DeepONets), which are designed to map between function spaces and approximate solution operators for parametric partial differential equations (PDEs). The authors provide two convergence analyses: one for networks with smooth activation functions and another for ReLU activation functions, establishing that over-parameterization (i.e., using wider layers) enhances optimization convergence. Empirical results across three operator learning tasks—Antiderivative, Diffusion-Reaction equations, and Burger’s equation—demonstrate that wider DeepONets consistently achieve lower training losses, supporting their theoretical claims.\n\n# Strength And Weaknesses\nStrengths of the paper include its solid theoretical foundation, presenting clear and rigorous analyses of convergence for both smooth and ReLU activations. The empirical evaluations support the theoretical findings well, showcasing the practical benefits of over-parameterization in DeepONets. However, the paper could benefit from additional discussions on the limitations of the proposed approaches and potential implications for generalization performance in real-world applications. Additionally, the focus on only specific canonical problems may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its theoretical contributions clearly, making it accessible to readers familiar with deep learning and operator theory. The methodology is rigorously described, allowing for reproducibility of the experiments. The novelty lies in the combination of optimization guarantees with empirical evidence, although the exploration of over-parameterization is becoming more common in the literature. Nonetheless, the detailed analysis of the spectral norm and NTK in the context of DeepONets presents a fresh perspective.\n\n# Summary Of The Review\nThis paper contributes significantly to the understanding of DeepONets by providing both theoretical optimization guarantees and empirical validation of the benefits of over-parameterization. While the paper is strong in theoretical rigor and clarity, further exploration of generalization and broader applicability could enhance its impact.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThis paper investigates Deep Operator Networks (DeepONets) and their ability to learn mappings between function spaces, particularly in the context of parametric partial differential equations (PDEs). The authors establish optimization convergence guarantees for DeepONets when over-parameterized, providing two convergence analyses: one for smooth activations that leverages the spectral norm of the Hessian, and another for ReLU activations based on the positive definiteness of the Neural Tangent Kernel (NTK) at initialization. Empirical evaluations on three operator learning problems—antiderivative operator, diffusion-reaction equation, and Burger’s equation—demonstrate that wider DeepONets lead to significantly lower training losses.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical contributions, which provide a solid understanding of the optimization behavior of DeepONets under over-parameterization, along with extensive empirical validation across diverse operator learning problems. The findings highlight the potential of DeepONets in scientific computing, which is particularly relevant in solving PDEs efficiently. However, the paper's limitations include a narrow scope of problems that may hinder generalization, an insufficient discussion on the implications of over-parameterization on generalization and overfitting, and a lack of exploration regarding the sensitivity of convergence to parameter initialization. Additionally, the computational demands of training wider networks may pose challenges in practical applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-organized, effectively communicating its methodology and findings. The quality of the theoretical analysis is high, as it provides valuable insights into the convergence behavior of the model. The novelty is evident in the establishment of convergence guarantees for over-parameterized DeepONets, contributing to the existing literature on neural operators. While the empirical results are reproducible, the paper could benefit from a more extensive discussion on the practical implications and limitations of the proposed methods.\n\n# Summary Of The Review\nThe paper offers significant theoretical insights into the optimization of DeepONets, demonstrating that over-parameterization can enhance convergence through comprehensive empirical validation. While the findings are promising, addressing the limitations related to problem scope and generalization will be crucial for broader applicability in future work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a comprehensive study on Deep Operator Networks (DeepONets), which are neural network architectures designed to learn mappings between function spaces, particularly for scientific computing tasks such as solving parametric partial differential equations (PDEs). The authors identify a critical gap in optimization convergence guarantees for DeepONets using gradient descent, and they demonstrate that over-parameterization (specifically wider layers) enhances convergence. The methodology includes two distinct analyses: one for smooth activation functions, utilizing bounds on the spectral norm of the Hessian to establish geometric convergence via restricted strong convexity (RSC), and another for ReLU activation functions, showing that the neural tangent kernel (NTK) at initialization is positive definite, leading to similar guarantees. Empirical results from three operator learning problems validate the theoretical insights, showcasing the advantages of over-parameterization in improving optimization outcomes.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its rigorous theoretical framework that provides necessary and sufficient conditions for convergence of DeepONets, which is a significant contribution to the understanding of neural operator learning. The empirical evaluation across different operator learning tasks effectively corroborates the theoretical claims, demonstrating the practical implications of the findings. However, a potential weakness is the reliance on specific assumptions for RSC and the positivity of the NTK, which may limit the generalizability of the results. Additionally, the paper could benefit from a more detailed discussion of limitations and future research directions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear sections that logically develop the arguments and present the findings. The quality of the writing is high, making complex concepts accessible to the reader. The novelty of the approach is significant, as it addresses a previously underexplored area of optimization in neural operator networks. Reproducibility is facilitated through detailed descriptions of the methodologies employed in both the theoretical analyses and empirical experiments, although the availability of code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of neural operator learning by establishing theoretical foundations for the optimization of DeepONets and providing empirical evidence for the benefits of over-parameterization. The work is well-executed and presents novel insights, although some assumptions may limit the broader applicability of the results.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents significant theoretical contributions regarding the optimization convergence of DeepONets, a class of neural networks used for operator learning. It establishes convergence guarantees under two scenarios: smooth and ReLU activations, thereby addressing a notable gap in the literature. Empirical validations are performed on several canonical operator learning problems, where the authors demonstrate that wider DeepONets lead to improved training performance, which aligns with the theoretical findings. The exploration of over-parameterization benefits is also discussed, reflecting current trends in deep learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical advancements, particularly the convergence guarantees for DeepONets and the insightful exploration of over-parameterization, which is a relevant topic in today's deep learning landscape. The empirical results bolster the theoretical claims, albeit limited to three specific problems, raising questions about generalizability. While the paper is well-organized, the complexity of the theoretical analysis may hinder practical application without a strong mathematical background. Additionally, while it builds upon established frameworks, this may compromise the novelty of its contributions. The literature review is comprehensive, yet it could benefit from a more critical evaluation of previous works to further justify the proposed contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, which enhances understanding, though it could further benefit from intuitive explanations or illustrative examples, especially for readers less familiar with optimization and neural networks. The quality of the analysis is high, yet the complexity may impede broader accessibility. While the theoretical foundations are robust, the reliance on specific frameworks like NTK and RSC may limit the perceived novelty. The implementation using JAX is modern, but detailed descriptions of the experimental setup would improve reproducibility, particularly for researchers using different tools or computational resources.\n\n# Summary Of The Review\nOverall, the paper makes substantial theoretical contributions to the understanding of DeepONets and provides empirical support for its claims, though it faces limitations in terms of the breadth of empirical validation and accessibility of its theoretical analysis. The reliance on established frameworks may also affect the perceived novelty of the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel framework for training Deep Operator Networks (DeepONets) aimed at learning mappings between complex function spaces. The authors emphasize the importance of deeper architectures over wider layers, providing theoretical convergence guarantees for these networks. The methodology includes an innovative optimization framework that utilizes spectral properties of the Hessian matrix for smooth activations and the Neural Tangent Kernel (NTK) for ReLU activations. Empirical results across three operator learning benchmarks—Antiderivative, Diffusion-Reaction, and Burger's equations—demonstrate that deeper architectures yield lower training losses and improved predictive accuracy.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of new theoretical insights regarding convergence guarantees for DeepONets, which significantly enhance the understanding of depth's role in optimization. The empirical validation across multiple benchmarks is robust, showing clear advantages in training efficiency and performance. However, a potential weakness is the limited exploration of the trade-offs between depth and other architectural considerations, such as width and over-parameterization, which could provide a more comprehensive view of network design in operator learning.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to both theoretical and empirical audiences. The quality of the writing is high, and the theoretical analysis is rigorous, providing a solid foundation for the claims made. The novelty lies in shifting the focus from width to depth in DeepONets, a perspective that has not been extensively explored in the literature. The reproducibility is supported by detailed descriptions of the experiments and methodologies used, although access to the code and datasets would further enhance this aspect.\n\n# Summary Of The Review\nThis paper significantly advances the understanding of DeepONets by demonstrating that deeper architectures can lead to improved convergence and optimization properties. The combination of rigorous theoretical insights and strong empirical validation makes it a noteworthy contribution to the field. Overall, the paper is a strong candidate for acceptance at ICLR.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" investigates the implications of over-parameterization in neural networks, particularly in the context of adversarial training. The authors establish a theoretical framework that connects wider neural architectures with enhanced convergence properties during adversarial training. Key findings include the demonstration of geometric convergence of over-parameterized networks, the influence of different activation functions on adversarial robustness, and comprehensive empirical validation showing that increased network width leads to lower adversarial loss across benchmark datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its robust theoretical foundation which clarifies the relationship between over-parameterization and adversarial robustness, providing valuable insights for both theoretical and practical applications. The empirical results are strong, demonstrating significant improvements in adversarial loss, thus supporting the theoretical claims effectively. Additionally, the exploration of various activation functions enriches the discussion and offers avenues for future research.\n\nHowever, there are notable weaknesses, such as a lack of in-depth discussion regarding the limitations of over-parameterization, specifically concerning potential overfitting to training data. Furthermore, while the empirical results are promising, the inclusion of a wider variety of datasets and adversarial attack types would enhance the robustness of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making complex theoretical concepts accessible to the reader. The quality of the research is high, with rigorous theoretical analysis and thorough empirical testing. The novelty lies in the integration of over-parameterization theory with adversarial training, a relatively unexplored area. Reproducibility appears to be adequately addressed, as the authors provide sufficient details regarding their methodologies and experiments.\n\n# Summary Of The Review\nThis paper offers significant contributions to the understanding of adversarial training through the lens of over-parameterization. By combining theoretical insights with strong empirical validation, it presents valuable findings that could enhance neural network design for improved robustness against adversarial attacks. Recommended for acceptance with minor revisions to address the noted weaknesses.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" presents significant advancements in the field of Deep Operator Networks (DeepONets), focusing on their application in solving parametric partial differential equations. The authors claim to offer the first comprehensive theoretical convergence guarantees for DeepONets, asserting that over-parameterization plays a crucial role in enhancing training performance. The methodology includes a novel optimization analysis that distinguishes between smooth and ReLU activations, supported by extensive empirical validation across canonical problems. The findings suggest a paradigm shift in the design and training of neural networks, emphasizing the essential nature of over-parameterization.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious theoretical contributions and extensive empirical validation. The introduction of novel convergence guarantees for DeepONets fills a critical gap in the literature and could provide a strong foundation for future research. Furthermore, the empirical results reportedly demonstrate significant improvements in training efficiency and accuracy, which are promising for practical applications. However, the paper may overstate its contributions, as some of its claims overlap with existing literature on operator learning. Additionally, the complexity of the presented methodologies may pose challenges for reproducibility among practitioners in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is generally good, but the intricate mathematical frameworks introduced may require additional effort from readers to fully comprehend. While the novelty of the theoretical guarantees is commendable, it would benefit from clearer distinction from prior work in the field. The quality of empirical validation appears robust, yet the reproducibility of results could be questioned, particularly given the complexity of the proposed methodologies and the potential for a steep learning curve for those unfamiliar with the subject matter.\n\n# Summary Of The Review\nOverall, this paper presents substantial contributions to the understanding of Deep Operator Networks and the role of over-parameterization in their performance. While the theoretical insights and empirical results are promising, the significance of these findings may be overstated in light of existing literature. The paper could be improved by clarifying its distinctions from prior work and addressing potential challenges to reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" investigates Deep Operator Networks (DeepONets) aimed at approximating solution operators of parametric partial differential equations (PDEs). The authors provide theoretical guarantees for the convergence of DeepONets during optimization, emphasizing that over-parameterization through wider layers enhances convergence properties. Two types of convergence analyses are conducted: one for smooth activations and another for ReLU activations, both indicating geometric convergence under specific conditions. Empirical results reveal that wider DeepONets consistently reduce training loss across various operator learning tasks, particularly excelling in the Antiderivative Operator scenario.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical analysis that establishes conditions for convergence, thereby filling a gap in the understanding of DeepONets' optimization behavior. The empirical findings further support the theoretical claims, particularly for the simpler Antiderivative Operator. However, weaknesses include the less impressive results for more complex problems like the Diffusion-Reaction PDE and Burger’s Equation, which suggest that while over-parameterization is beneficial, its effects might not be as pronounced as initially anticipated in non-linear contexts. Additionally, the presentation of results could benefit from clearer comparisons to previous benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the theoretical and empirical findings accessible. The quality of the writing is high, with a coherent flow from theoretical groundwork to experimental validation. While the novelty of establishing theoretical convergence guarantees for DeepONets is notable, the empirical results show diminishing returns for complex problems, which may affect the perceived novelty of the broader application. Reproducibility appears feasible as the authors provide sufficient detail in their methodology, although sharing code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the understanding of DeepONets, particularly regarding the benefits of over-parameterization. While the theoretical insights are robust, the empirical results indicate that the advantages of wider networks may not be as substantial in more complex PDE scenarios. Future exploration of the trade-offs between network width and computational efficiency is warranted.\n\n# Correctness\n4/5 - The theoretical analyses are sound and supported by empirical evidence, though some claims could be overstated based on the empirical results for complex problems.\n\n# Technical Novelty And Significance\n4/5 - The establishment of convergence guarantees for DeepONets is a noteworthy contribution, enhancing the understanding of their optimization dynamics.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results demonstrate the benefits of over-parameterization, the impact observed in complex scenarios is less significant than expected, somewhat limiting the novelty of the findings.",
    "# Summary Of The Paper\nThe paper investigates the theoretical foundations and empirical implications of using DeepONets for operator learning, emphasizing the role of over-parameterization and the convergence guarantees of gradient descent optimization. The authors derive conditions under which DeepONets can approximate specific operators, supported by empirical results on three distinct operator learning problems. However, the paper also highlights the assumptions of universal approximation, potential overfitting due to model complexity, and the sensitivity of convergence guarantees to weight initialization and loss function properties.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its rigorous theoretical analysis of convergence guarantees and the empirical validation of its claims across several operator learning tasks. The discussion around over-parameterization is particularly relevant, as it challenges conventional wisdom regarding model complexity. However, the paper's reliance on certain assumptions—such as the strong convexity of the loss landscape and the behavior of the Hessian—raises concerns about the generalizability of its findings. Additionally, the potential impact of different activation functions on convergence and approximation quality is insufficiently explored, which could limit the applicability of the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings in a clear manner. The methodology is laid out effectively, but some of the theoretical discussions could be more accessible, particularly for readers less familiar with the intricacies of operator learning. While the paper introduces novel contributions, the heavy reliance on specific assumptions may hinder reproducibility and applicability in broader contexts. Empirical results, although promising, are limited to a narrow set of problems, which may restrict the general understanding of the method's effectiveness.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of operator learning through its theoretical insights and empirical evaluations. However, the validity of its assumptions may limit its practical applicability, and a more extensive exploration of activation functions and broader problem types would strengthen its findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper investigates the convergence guarantees of Deep Operator Networks (DeepONets) when trained with gradient descent, highlighting the role of over-parameterization through wider layers. The authors provide two primary analyses: one for networks using smooth activation functions via Restricted Strong Convexity (RSC) and another for those employing ReLU activations through the Neural Tangent Kernel (NTK). Empirical results substantiate the theoretical claims, showing that wider DeepONets effectively reduce training loss across various operator learning tasks, including solving parametric PDEs.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by providing the first comprehensive analysis of optimization guarantees for DeepONets, addressing a notable gap in the existing literature. The dual approach—analyzing both smooth and ReLU activations—broadens the applicability of the findings. A potential weakness lies in the assumption that wider networks inherently lead to better convergence without exploring the trade-offs related to increased model complexity or overfitting in specific scenarios. Additionally, the empirical validation, while consistent with the theoretical claims, could benefit from a more extensive evaluation across diverse operator learning problems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodologies, and findings. The quality of writing is high, with appropriate use of mathematical rigor to support the claims. The novelty of establishing convergence guarantees for DeepONets, especially through the lens of over-parameterization, is significant. The reproducibility of the experiments is supported by clear descriptions of the datasets and methodologies used, although further details on hyperparameter choices and training setups would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of deep learning for scientific computing by providing theoretical and empirical insights into the optimization of DeepONets. While the findings are significant, the exploration of potential downsides related to over-parameterization could strengthen the paper further.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework for optimizing deep learning models through an innovative approach to gradient descent that incorporates adaptive learning rates and momentum adjustments based on the curvature of the loss surface. The authors provide a theoretical analysis demonstrating the method's convergence properties and present empirical results showing improved performance on a range of benchmark datasets compared to traditional optimization techniques. The findings suggest that the proposed method achieves faster convergence rates and better generalization in various training scenarios.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Novelty:** The paper introduces a unique perspective by integrating curvature information into the optimization process, distinguishing it from conventional gradient descent methods.\n2. **Theoretical Contributions:** A rigorous theoretical framework is established, detailing the convergence guarantees and optimization benefits associated with the proposed method.\n3. **Empirical Validation:** Comprehensive experiments are conducted, showing consistent performance improvements over existing state-of-the-art optimization algorithms across multiple datasets.\n4. **Clarity and Structure:** The paper is well-organized, with a logical flow that effectively guides the reader through the methodology, theoretical background, and experimental results.\n\n**Weaknesses:**\n1. **Limited Scope:** The applicability of the proposed method may be confined to specific types of neural network architectures, potentially limiting its generalizability.\n2. **Comparative Analysis:** There is a notable lack of extensive comparisons with a broader range of existing optimization techniques, which could provide more context for the claims made.\n3. **Experimental Details:** Certain experimental setups lack clarity, particularly regarding hyperparameter tuning and dataset preprocessing, which raises concerns about reproducibility.\n4. **Complexity:** The introduction of curvature-based adjustments may add complexity that could complicate the implementation of the method in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections that facilitate understanding. However, some experimental details are insufficiently described, which could hinder reproducibility. The novelty of the approach is significant, presenting a fresh take on optimization techniques that could have meaningful implications in the field.\n\n# Summary Of The Review\nThe paper contributes a novel optimization framework that leverages curvature information to enhance convergence rates in deep learning. While it provides a solid theoretical foundation and empirical evidence supporting its claims, the limited scope and lack of comprehensive comparative analysis detract from its overall impact. Addressing these weaknesses could strengthen the paper's contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" investigates the optimization properties of Deep Operator Networks (DeepONets), which are designed to learn mappings between function spaces, particularly in the context of parametric partial differential equations (PDEs). The authors establish convergence guarantees for gradient descent optimization of DeepONets, showing that over-parameterization via wider layers enhances convergence. Two convergence analyses are performed: one for smooth activation functions using spectral norm bounds and restricted strong convexity, and another for ReLU activations through the positive definiteness of the neural tangent kernel (NTK) at initialization. Empirical results corroborate the theoretical findings, demonstrating that wider DeepONets effectively reduce training loss across various operator learning scenarios.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by providing a theoretical foundation for the optimization of DeepONets, particularly by highlighting the role of over-parameterization in improving convergence. The dual approach to convergence guarantees for both smooth and ReLU activations is a notable strength, as it broadens the applicability of the findings. However, the paper may lack sufficient exploration of the implications of over-parameterization in practical scenarios, such as potential trade-offs in generalization or computational efficiency. Additionally, while empirical results support theoretical claims, the experimental setup could be expanded to include a broader range of benchmarks to further validate the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making complex concepts accessible through thorough explanations. The quality of the methodology is high, and the theoretical analyses are rigorous. The novelty of the work lies in establishing the connection between over-parameterization and optimization guarantees in DeepONets, an area that has received limited attention. The reproducibility of the findings is supported by the empirical results presented, although the authors could enhance this aspect by providing more detailed experimental protocols and code availability.\n\n# Summary Of The Review\nOverall, this paper presents a substantial contribution to the understanding of Deep Operator Networks, particularly concerning the optimization benefits of over-parameterization. The theoretical and empirical analyses are well-executed and provide a solid foundation for future work in the area. The paper is recommended for acceptance, with minor suggestions for enhancing the empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" focuses on Deep Operator Networks (DeepONets) and their optimization convergence guarantees, particularly emphasizing the advantages of over-parameterization. The authors propose two optimization analyses: one for smooth activation functions using spectral norm bounds and another for ReLU activations leveraging the Neural Tangent Kernel (NTK). Empirical results demonstrate that wider DeepONets consistently achieve lower training loss across various operator learning tasks, including the Antiderivative, Diffusion-Reaction PDE, and Burger's equation.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its theoretical contributions that establish convergence guarantees for DeepONets through over-parameterization, which addresses a notable gap in the existing literature. The empirical evidence provided supports the theoretical claims, showing a clear relationship between network width and training loss reduction. However, one weakness may be the limited exploration of the implications of these findings for practical applications, as the focus is primarily on convergence rather than broader applicability to complex PDEs.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with deep learning and optimization. The integration of theoretical and empirical analyses strengthens the quality of the work. The novelty is significant as it provides new insights into the optimization behavior of DeepONets, a relatively underexplored area in the literature. The reproducibility of the results appears high, given the detailed methodology and the clear presentation of the experimental setups.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of neural operator learning by establishing theoretical convergence guarantees for DeepONets and demonstrating the benefits of over-parameterization through empirical evidence. The integration of theory and practice enhances its significance, although a deeper exploration of practical implications would improve the manuscript.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" explores the optimization convergence guarantees for Deep Operator Networks (DeepONets), emphasizing the role of over-parameterization with wider layers in enhancing convergence. It presents two types of convergence analysis—one for smooth activations based on Restricted Strong Convexity (RSC) and another for ReLU activations utilizing the Neural Tangent Kernel (NTK). Empirical evaluations demonstrate that wider DeepONets achieve significantly lower training losses across three operator learning problems: the Antiderivative Operator, Diffusion-Reaction Equation, and Burger’s Equation.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its well-structured theoretical framework that addresses a notable gap in the understanding of optimization for DeepONets. The clear delineation of methodologies and the combination of theoretical and empirical results provide substantial support for the claims made. However, a potential weakness is the limited scope of empirical evaluations, which only focus on three specific operator learning problems. Expanding this empirical analysis to a broader range of applications could enhance the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written with clarity, making complex concepts accessible without sacrificing rigor. The quality of the theoretical analysis is high, with appropriate references to foundational work, allowing for reproducibility. However, while the methods and results are clearly articulated, further details on experimental setups or hyperparameter choices could improve reproducibility for practitioners aiming to implement the findings.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the understanding of DeepONets, particularly in relation to optimization and the benefits of over-parameterization. The combination of theoretical insights and empirical validation provides a comprehensive perspective on the topic. Nonetheless, expanding the empirical scope could strengthen the paper further.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates Deep Operator Networks (DeepONets), a novel approach for function space mapping, and provides theoretical optimization guarantees under gradient descent. It emphasizes the advantages of over-parameterization by demonstrating that wider architectures facilitate convergence. Two analytical frameworks are introduced: one based on spectral norm analysis of the Hessian for smooth activations, leading to geometric convergence through Restricted Strong Convexity (RSC), and another utilizing Neural Tangent Kernel (NTK) positivity for ReLU activations. Empirical validation is conducted on canonical operator problems, confirming that increased network width correlates with reduced training losses.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by establishing a theoretical foundation for DeepONets, which had previously lacked optimization guarantees. The dual analytical frameworks add depth to the theoretical discussion, and the extensive empirical validation strengthens the findings. However, the paper could benefit from a more detailed discussion on the implications of over-parameterization in practical settings and potential limitations of the proposed methods. Additionally, the presentation of results could be clearer, particularly in explaining the relationship between theoretical guarantees and empirical performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its findings clearly, with a logical flow from theoretical foundations to empirical validation. The quality of writing is high, although certain sections could be enhanced for clarity, particularly for readers less familiar with the underlying mathematical concepts. The novelty of linking theoretical optimization guarantees to empirical outcomes is a significant contribution. Reproducibility appears feasible, given the thorough experimental setup described, but it would benefit from additional implementation details for the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a compelling exploration of Deep Operator Networks, offering valuable theoretical insights and empirical evidence that highlight the benefits of over-parameterization. While it makes significant contributions to the field, certain aspects could be improved for clarity and broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents Deep Operator Networks (DeepONets), a novel approach to operator learning that claims to enhance the training dynamics of neural networks for solving various mathematical problems. The authors introduce concepts such as spectral norm bounds and restricted strong convexity (RSC) to support their method, while also claiming that over-parameterization aids in convergence. Empirical evaluations are conducted on canonical operator learning tasks, including antiderivatives, diffusion-reaction equations, and Burger’s equation, with the intention of demonstrating the effectiveness of DeepONets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its introduction of theoretical concepts like spectral norm bounds and RSC, which could provide a foundation for further exploration in operator learning. However, several weaknesses undermine the overall contribution. The lack of practical optimization convergence guarantees raises concerns about the robustness of DeepONets. Additionally, the reliance on over-parameterization may indicate underlying instabilities. The empirical results, while presented as supportive evidence, stem from relatively simplistic problems, which may not generalize well to more complex, real-world scenarios. Furthermore, the paper's organization and superficial discussion of related work detract from its clarity and perceived novelty.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from clarity issues, with a convoluted organization that hampers understanding of the methodologies and findings. The quality of the theoretical analysis, particularly regarding ReLU activations and the positive definiteness of the neural tangent kernel (NTK), seems overly optimistic and may not hold in practical situations. The novelty is diminished by a lack of critical comparison with contemporary methods, and the reproducibility of results is questionable due to insufficient exploration of hyperparameter tuning and limited benchmarking against state-of-the-art techniques.\n\n# Summary Of The Review\nOverall, the paper introduces Deep Operator Networks with some interesting theoretical ideas but fails to convincingly demonstrate their practical significance or robustness. The empirical results do not provide sufficient insights into the advantages of the proposed method, and the paper could benefit from a more thorough exploration of related work and clearer organization.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper introduces a significant advancement in the study of neural operators, specifically through the development of Deep Operator Networks (DeepONets). The authors establish theoretical convergence guarantees that demonstrate the benefits of over-parameterization in enhancing the performance of these networks. Methodologically, they utilize spectral norm bounds and Restricted Strong Convexity (RSC) to analyze the optimization dynamics of DeepONets, while also employing the Neural Tangent Kernel (NTK) framework to investigate the impact of ReLU activations. Empirical findings illustrate that increasing the width of DeepONets leads to lower training loss across various operator learning tasks, such as the Antiderivative and Diffusion-Reaction equations, thereby validating their theoretical claims.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its pioneering theoretical contributions and robust empirical validations, which collectively underscore the advantages of over-parameterization in DeepONets. The convergence guarantees provided are both innovative and impactful, setting a strong foundation for future research. However, the paper could benefit from a discussion of potential limitations or challenges associated with over-parameterization, such as computational costs or overfitting in specific contexts. Additionally, while the empirical demonstrations are compelling, further comparisons with existing state-of-the-art methods would strengthen the case for the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the theoretical and empirical aspects accessible to the reader. The quality of the writing is high, with a logical flow of ideas. The novelty of the work is evident, as it not only introduces DeepONets but also provides a fresh perspective on the role of over-parameterization in neural networks. The reproducibility of the results would benefit from the inclusion of detailed experimental setups and code availability, which could enhance the community's ability to build upon this research.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the field of neural operator learning by providing both theoretical insights and empirical evidence supporting the advantages of over-parameterization in DeepONets. It sets a new benchmark for future explorations in this area and showcases the potential of DeepONets across various applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents Deep Operator Networks (DeepONets), a novel architecture designed to learn mappings between infinite-dimensional function spaces, specifically for solving parametric partial differential equations (PDEs). The authors focus on the theoretical underpinnings of DeepONets, demonstrating the advantages of over-parameterization in enhancing optimization convergence properties. Key contributions include two forms of convergence analysis—one for smooth activations using spectral norm bounds and another for ReLU activations leveraging the Neural Tangent Kernel (NTK). The paper emphasizes the importance of theoretical guarantees over empirical results, establishing a foundation for understanding the optimization landscape of deep networks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its robust theoretical contributions, particularly the detailed convergence guarantees that support the efficacy of DeepONets. The use of Restricted Strong Convexity (RSC) and the positive definiteness of the NTK as tools for convergence analysis is commendable and adds significant value to the theoretical landscape of deep learning. However, a notable weakness is the limited empirical validation; while the authors argue for the predominance of theoretical insights, the lack of comprehensive empirical results may leave practical implications underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical contributions with clarity, making complex concepts accessible to readers familiar with deep learning and optimization. The quality of the mathematical formulations is high, supporting the claims made throughout the paper. The novelty of the theoretical insights into over-parameterization and convergence is significant, although the emphasis on theory over empirical results raises questions about reproducibility in practical scenarios.\n\n# Summary Of The Review\nOverall, this paper provides a strong theoretical foundation for Deep Operator Networks, emphasizing the benefits of over-parameterization and offering detailed convergence guarantees. While the theoretical contributions are significant, the lack of empirical validation may limit the practical applicability of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" investigates the optimization dynamics of Deep Operator Networks (DeepONets) by leveraging over-parameterization. The authors present two optimization analyses that demonstrate convergence guarantees for DeepONets with both smooth and ReLU activations, utilizing concepts such as Restricted Strong Convexity (RSC) and the Neural Tangent Kernel (NTK). Empirical evaluations reveal that increasing the width of DeepONets consistently lowers training losses across various problems, with rigorous experimentation conducted on different types of partial differential equations (PDEs) and ordinary differential equations (ODEs). \n\n# Strength And Weaknesses\nStrengths of the paper include its thorough theoretical analysis alongside empirical validation, providing a solid foundation for understanding the performance of DeepONets under over-parameterization. The convergence guarantees established for different activation functions enhance the paper's contributions to the literature on neural networks. However, a notable weakness is the limited discussion of broader implications or potential applications of the results beyond the immediate implementation concerns, which may restrict the paper's impact on the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodologies, findings, and theoretical underpinnings, making it accessible to readers with a background in deep learning and optimization. The quality of the writing is high, with detailed explanations of the experimental setup and results. The novelty lies primarily in the focus on over-parameterization within the context of DeepONets, although similar concepts have been explored in other contexts. Reproducibility appears to be supported by the detailed implementation specifics provided, including the use of JAX and standard optimization techniques.\n\n# Summary Of The Review\nOverall, the paper presents valuable insights into the optimization of DeepONets through over-parameterization, with both theoretical guarantees and empirical results supporting its claims. However, the lack of broader context or potential applications could limit the paper's overall impact within the field of deep learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper aims to provide optimization convergence guarantees for DeepONets, claiming to offer new insights into their performance. The authors employ established frameworks, notably the Neural Tangent Kernel (NTK), to analyze the effects of over-parameterization and activation functions, specifically comparing smooth and ReLU activations. Their empirical findings suggest that wider DeepONets achieve improved convergence and lower training loss on several canonical problems, such as the Antiderivative and Diffusion-Reaction equations.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to rigorously analyze DeepONets through the lens of optimization theory. However, the weaknesses are significant; the authors fail to adequately differentiate their contributions from existing literature, particularly the foundational work by Lu et al. (2021) on operator learning. Many claimed benefits of over-parameterization and the analysis of activation functions are well-established in prior studies, leading to questions about the novelty of the authors' approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers familiar with the field. However, the novelty of the contributions is questionable, as the authors do not sufficiently differentiate their findings from existing work. The reproducibility of the empirical results is also hampered by the lack of novel methodologies or experimental setups, which closely mirror those of previous studies.\n\n# Summary Of The Review\nOverall, the paper presents a well-organized analysis of DeepONets but falls short in offering significant novel contributions to the field. Many of the insights drawn from the existing literature do not provide new perspectives, leading to concerns about the paper's overall impact.\n\n# Correctness\nRating: 3/5. The methodology is sound, but the reliance on previously established results raises questions about the originality of the findings.\n\n# Technical Novelty And Significance\nRating: 2/5. The paper reiterates concepts well-documented in the literature without presenting distinct advancements, limiting its technical novelty.\n\n# Empirical Novelty And Significance\nRating: 2/5. The empirical results are predictable and align closely with findings from prior works, reducing their significance as new contributions to the field.",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" investigates the advantages of over-parameterization in learning mappings between function spaces through Deep Operator Networks (DeepONets). The authors propose a novel methodology to enhance the performance of DeepONets by leveraging over-parameterization, demonstrating through various experiments that this approach significantly improves the learning of complex operators, such as the Antiderivative Operator. The findings indicate that over-parameterization not only accelerates convergence but also enhances the generalization capabilities of the model.\n\n# Strength And Weaknesses\nThe paper presents a strong theoretical foundation for the benefits of over-parameterization, effectively linking it to empirical results across various problem settings. The methodology is sound, and the experiments are well-designed, showcasing clear improvements in performance metrics. However, the paper lacks clarity in certain areas, such as the description of the DeepONet architecture and the formal definition of key terms. Additionally, the repetitive use of phrases and inconsistent formatting detracts from the overall readability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper makes significant contributions to the understanding of DeepONets and their performance under over-parameterization, clarity issues hinder the reader's ability to fully grasp the methodology and results. The quality of the figures and their captions could be improved for better comprehension. Novelty is present in the exploration of over-parameterization within this context, though some concepts could be articulated more effectively to enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper provides valuable insights into the role of over-parameterization in DeepONets, supported by empirical evidence. However, the clarity and consistency of presentation could be significantly improved to enhance understanding and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents optimization convergence guarantees for DeepONets, a neural architecture tailored for operator learning problems. It employs a theoretical framework to establish convergence properties under specific conditions, primarily focusing on smooth and ReLU activations. The empirical evaluation is conducted on three canonical operator learning tasks, demonstrating the effectiveness of the proposed methods in reducing training loss, though the assessment lacks a comprehensive analysis of validation and test performance.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical contributions, particularly in establishing convergence guarantees for DeepONets, which is a crucial aspect for their practical deployment. However, the paper exhibits notable weaknesses, including a lack of exploration into the robustness of these guarantees against noise, a narrow selection of empirical problems that limits generalizability, and insufficient discussion on potential applications in high-dimensional spaces. Additionally, the analysis of over-parameterization and computational complexity is not adequately developed, potentially undermining the applicability of the proposed methods in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly written, making the theoretical contributions accessible to the reader. However, the novelty of the findings could be enhanced by a deeper exploration of the implications of the results in broader contexts. The reproducibility of the experiments is not fully supported due to a lack of detailed discussions on validation metrics and computational challenges associated with scaling DeepONets, which are critical for practitioners.\n\n# Summary Of The Review\nOverall, while the paper makes significant contributions to the theoretical understanding of DeepONets and presents compelling empirical results, it falls short in addressing important practical considerations and broader applications. A more comprehensive exploration of the limitations and future directions would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the optimization convergence properties of Deep Operator Networks (DeepONets) in approximating solution operators of partial differential equations (PDEs). It establishes theoretical guarantees focused on over-parameterization and wide layers, providing two main types of convergence analysis: one for smooth activation functions using Restricted Strong Convexity (RSC) and another for ReLU activations through positive definiteness of the Neural Tangent Kernel (NTK) at initialization. Empirical evaluations demonstrate that wider networks yield statistically significant reductions in training loss, supporting the theoretical findings and emphasizing the importance of over-parameterization in achieving effective convergence.\n\n# Strength And Weaknesses\nThe strengths of the paper include its comprehensive theoretical framework that bridges the gap between approximation guarantees and optimization guarantees for DeepONets. The use of RSC and NTK analysis provides a robust foundation for the convergence results. Additionally, the empirical validation across various operator-learning problems enhances the credibility of the theoretical claims. However, a potential weakness lies in the reliance on specific assumptions regarding activation functions and initialization, which may limit the generalizability of the results. The paper could also benefit from a more thorough discussion on the implications of these findings for real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe manuscript is well-structured and clearly articulates the theoretical developments and empirical evaluations. The quality of writing is high, with detailed explanations of complex concepts such as RSC and NTK analysis. The novelty of the paper is significant, as it addresses a less explored aspect of DeepONets, particularly the optimization guarantees which are often overlooked. The reproducibility of the results appears to be high, given the thorough methodology and statistical evaluations provided, though the specifics of the experimental setup could be elaborated further to aid in replication.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the understanding of optimization in DeepONets by establishing theoretical convergence guarantees and supporting them with empirical evidence. The clarity and quality of the writing enhance the presentation of novel insights, although the assumptions made may affect the broader applicability of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing DeepONets, focusing on establishing convergence guarantees for specific activation functions such as smooth and ReLU. The methodology involves theoretical analysis underpinned by the assumption of over-parameterization, exploring the implications of these assumptions for model training and performance. The findings indicate that the proposed optimization framework can yield effective results for a limited set of canonical operator learning problems, although the generalizability of these results remains uncertain.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous theoretical foundation, providing convergence guarantees that enhance our understanding of the optimization landscape for DeepONets. However, it is limited by several weaknesses. Notably, the analysis is constrained to specific activation functions, neglecting other prevalent functions like Leaky ReLU or ELU, which may have implications for broader applicability. Additionally, the empirical evaluation is restricted to three canonical problems, raising questions about the generalizability of the findings. The lack of discussion regarding practical applications, computational costs, and robustness to hyperparameter choices further diminishes the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear exposition of the theoretical results. However, the lack of in-depth exploration of practical implications and limitations detracts from its overall clarity and quality. The novelty lies in the establishment of optimization guarantees, although the theoretical contributions could be strengthened by a more comprehensive analysis of related literature. Reproducibility may be a concern due to the absence of sensitivity analyses and empirical validation across a diverse set of benchmarks.\n\n# Summary Of The Review\nOverall, while the paper contributes valuable theoretical insights into the optimization of DeepONets, it falls short in addressing several practical considerations and broader applicability. Future research should focus on expanding the scope of analysis, including a wider range of activation functions and empirical evaluations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Learning Deep Operator Networks: The Benefits of Over-Parameterization\" explores the application of deep learning techniques, specifically DeepONets, to function spaces. The authors aim to establish optimization convergence guarantees for DeepONets, arguing that wider layers lead to improved performance. The methodology involves a theoretical analysis using Restricted Strong Convexity (RSC) to demonstrate convergence, along with empirical validation through experiments that highlight the relationship between network width and training loss.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to provide theoretical guarantees for a relatively recent area of deep learning, which is crucial for the wider adoption of DeepONets in scientific computing. However, the paper suffers from a lack of novelty, as the findings largely reiterate existing knowledge on over-parameterization in neural networks. The literature review appears superficial, failing to engage deeply with prior work. Moreover, the complexity of the notation detracts from accessibility, and the empirical results do not present significant new insights beyond what is already established.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured, the clarity is undermined by convoluted notation and terminology that may confuse readers. The quality of the writing is adequate, but the novelty is minimal; the conclusions drawn are largely expected outcomes in the context of over-parameterization. Reproducibility is not thoroughly addressed, as the experiments, although standard, do not provide sufficient detail on implementation.\n\n# Summary Of The Review\nOverall, this paper presents an important yet largely redundant contribution to the field of deep learning applied to function spaces. The theoretical guarantees and empirical findings, while valid, do not offer substantial new insights and rehash established concepts. The paper would benefit from a more rigorous engagement with existing literature and a clearer presentation of results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents significant advancements in the understanding and application of Deep Operator Networks (DeepONets) for solving partial differential equations (PDEs). It establishes theoretical convergence guarantees based on Restricted Strong Convexity (RSC) and the Neural Tangent Kernel (NTK), emphasizing the potential benefits of over-parameterization in enhancing learning performance. Empirical results demonstrate the effectiveness of DeepONets across various PDEs, showcasing improved efficiency and accuracy compared to traditional numerical methods.\n\n# Strength And Weaknesses\nThe key strength of this paper lies in its rigorous theoretical framework, which provides convergence guarantees that can guide future research in operator learning. The emphasis on over-parameterization is particularly timely, given the current trends in deep learning. However, the paper could benefit from a broader empirical validation across a wider variety of PDEs and comparison with other state-of-the-art methods. Additionally, the reliance on fully connected feedforward architectures may limit the exploration of alternative, potentially more suitable architectures for operator learning tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to both theoretical and empirical audiences. The quality of the theoretical analysis is high, although the empirical section could be expanded for greater impact. The novelty of the findings, particularly regarding over-parameterization and convergence, is noteworthy; however, the reproducibility of the experiments could be improved by providing additional details on the datasets and experimental setups.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of operator learning with DeepONets, combining theoretical insights with empirical findings. While it successfully highlights the advantages of over-parameterization and establishes convergence guarantees, further exploration of alternative architectures and a more extensive empirical analysis are recommended to strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the effectiveness of over-parameterization in DeepONets for three canonical operator learning problems: the Antiderivative, the Diffusion-Reaction equation, and Burger’s equation. The authors demonstrate that increasing the width of both the branch and trunk networks significantly reduces training loss across all tasks, thereby validating the theoretical claims regarding the advantages of wider networks. The empirical results show that the Antiderivative problem achieves a training loss as low as \\(10^{-12}\\) with wider networks, while reductions in loss for the Diffusion-Reaction and Burger’s equations, though less pronounced, still confirm the trend of improved performance with increased network width.\n\n# Strength And Weaknesses\nThe main contribution of this paper lies in its empirical validation of the theoretical advantages of over-parameterization in operator learning tasks. The findings are robust, showing consistent performance improvements with wider networks across multiple problem domains. However, the paper could benefit from a deeper exploration of the underlying reasons for the observed differences in performance across tasks, particularly for the Diffusion-Reaction equation, where the nonlinearity affects the results more than in the Antiderivative case.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and results, making it easy for readers to follow the analysis. The quality of the empirical results is high, and the figures effectively illustrate the key findings. In terms of novelty, while the exploration of over-parameterization is not entirely new, the specific focus on operator learning tasks adds a significant contribution to the field. The reproducibility of the results seems feasible, given the clear description of the experiments; however, providing additional implementation details or code would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper offers a solid empirical investigation into the impact of network width on performance in operator learning tasks, affirming theoretical expectations regarding over-parameterization. While the findings are compelling and well-presented, further exploration of the reasons behind the varying performance across tasks would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to modeling complex nonlinear operators using Deep Operator Networks (DeepONets). The authors propose a methodology that incorporates both spatial and temporal dimensions into the training of these networks, enabling them to generalize across various tasks efficiently. The findings demonstrate that DeepONets significantly outperform traditional methods in terms of accuracy and computational efficiency on benchmark datasets, illustrating their potential applicability in real-world scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to operator learning, which could have substantial implications for various fields such as physics and engineering. The empirical results indicate a clear advantage of DeepONets over existing methods, showcasing their effectiveness. However, the paper suffers from several weaknesses, including a lengthy and complex introduction that may confuse readers, and a discussion section that lacks depth in exploring the implications of the findings. Additionally, the presentation of empirical results could be improved for clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept with potential significance, its clarity is hindered by dense language and inconsistent terminology. The methodology is generally well outlined but could benefit from clearer transitions and section headings. The reproducibility of the results is challenged by sparse explanations of mathematical notation and insufficient context for references. Improving figure captions and providing organized summaries of results would enhance the overall quality and clarity of the paper.\n\n# Summary Of The Review\nThe paper introduces an innovative framework for operator learning, demonstrating promising results in various applications. However, the clarity and organization of the paper could be improved to enhance readability and understanding. Overall, the contributions are significant, but the presentation could be refined for better communication of the ideas.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.302784175596009,
    -1.7149072461979638,
    -1.7793033798548563,
    -1.5833868577251593,
    -2.0451037755829917,
    -1.8211515417918402,
    -1.6561557019406814,
    -1.7868997086801037,
    -1.7912578334350473,
    -1.8030810342454928,
    -1.6001741520611754,
    -1.4535126158062062,
    -1.5838254858969525,
    -1.7404355752675187,
    -1.7972796980659904,
    -1.8740938611789582,
    -1.8990371178014513,
    -1.5540089621765538,
    -1.6857469158744591,
    -1.7051074339940195,
    -1.7307959120695386,
    -1.6509221410810986,
    -1.832706170256192,
    -1.7080718177964835,
    -1.703043084943858,
    -1.85813075721762,
    -1.5534010736802242,
    -1.5913782772115321,
    -1.828532188529967
  ],
  "logp_cond": [
    [
      0.0,
      -1.8372435997742607,
      -1.8317946873145503,
      -1.8280751161863522,
      -1.8438813473116147,
      -1.8611993062870111,
      -1.9108080446057571,
      -1.838364992291911,
      -1.8325935743648756,
      -1.9182544795438161,
      -1.8309455049574415,
      -1.9778325548202038,
      -1.825533409484038,
      -1.814698169956013,
      -1.8130672715830738,
      -1.8592620784703422,
      -1.9201321697985299,
      -1.8896429955684608,
      -1.8582956608446763,
      -1.854930084041089,
      -1.8483367308589482,
      -1.9190337775388724,
      -1.865370479741583,
      -1.8510239057100462,
      -1.891262235756913,
      -1.871767959506017,
      -1.90392740595786,
      -1.9153020082388474,
      -1.944103482946628
    ],
    [
      -1.326909624257012,
      0.0,
      -1.1095376721367278,
      -1.1803620263793826,
      -1.224057400386206,
      -1.1949969828340363,
      -1.3240709201311904,
      -1.2506310516647665,
      -1.1192128039781133,
      -1.3189128954213107,
      -1.2082105071771034,
      -1.4647867216257013,
      -1.1266317283352107,
      -1.0675476673031281,
      -1.0430142319193514,
      -1.2152396662547593,
      -1.2932098057456292,
      -1.2234260147482685,
      -1.2960599119874205,
      -1.2125330956908509,
      -1.2173022455994575,
      -1.2669124812201205,
      -1.2586578262143762,
      -1.1897914046819567,
      -1.3073181306068142,
      -1.2456919619215694,
      -1.3573484487665652,
      -1.229699371855622,
      -1.3659289441588522
    ],
    [
      -1.4040824293903982,
      -1.224382436532688,
      0.0,
      -1.2471616491679014,
      -1.3434675727412109,
      -1.3200700977448945,
      -1.4362168507819497,
      -1.3662722478611578,
      -1.2482386062630022,
      -1.3924655208816623,
      -1.3035998918752865,
      -1.5282145141522263,
      -1.195496786993007,
      -1.2150465258100525,
      -1.2161482519019,
      -1.309221415121486,
      -1.3728575186218037,
      -1.309515356512693,
      -1.3548071556592685,
      -1.3189824119129192,
      -1.3344490062690515,
      -1.4029424597045934,
      -1.3544831536268909,
      -1.270930876603429,
      -1.372717239335686,
      -1.3817790668748349,
      -1.4382685144294938,
      -1.3821781056627331,
      -1.5028926846083135
    ],
    [
      -1.2522276006012052,
      -1.1630376438620984,
      -1.117055240905455,
      0.0,
      -1.2049548496784726,
      -1.1823016245290818,
      -1.3047885872755867,
      -1.2399868864120178,
      -1.1824111434657956,
      -1.2292711998788621,
      -1.1227702539435043,
      -1.373391870265888,
      -1.0529023623853688,
      -1.1379051696788605,
      -1.1345220691511033,
      -1.1411400931506857,
      -1.2243168470594201,
      -1.1897285125404877,
      -1.1502110509635777,
      -1.197537661017059,
      -1.270767165593079,
      -1.2564739446246425,
      -1.2295450571583049,
      -1.1426055684982879,
      -1.2393084534574006,
      -1.1799465874252855,
      -1.2675893300149563,
      -1.2593404177673788,
      -1.2978917608481217
    ],
    [
      -1.7195892240481063,
      -1.6576903818839883,
      -1.6740352461332864,
      -1.611748176789418,
      0.0,
      -1.6718728582749673,
      -1.7163224557208572,
      -1.648177779323038,
      -1.7040416059743233,
      -1.6630106787599983,
      -1.6160458159008122,
      -1.794655293716612,
      -1.607996293975327,
      -1.6107981449808215,
      -1.5217454610048093,
      -1.5810782427997738,
      -1.6681007222708264,
      -1.5982674763667242,
      -1.64654003904919,
      -1.5817773525424919,
      -1.6466665202679334,
      -1.7174370076420773,
      -1.625589686282929,
      -1.6023735317763825,
      -1.6699460069838838,
      -1.6664097849561308,
      -1.6661962452088548,
      -1.685318479735403,
      -1.7568461787463883
    ],
    [
      -1.427375414957189,
      -1.240932093493902,
      -1.2056806818272052,
      -1.2462536332835839,
      -1.3669031528434288,
      0.0,
      -1.4130609314145257,
      -1.378222490571684,
      -1.2478465970934387,
      -1.3988003542463128,
      -1.2795573299722394,
      -1.5395250084218923,
      -1.2487660330688128,
      -1.2255215992043,
      -1.2401765575642658,
      -1.3049536148364758,
      -1.3804797550695738,
      -1.2888911076775609,
      -1.3543231841139924,
      -1.3614763183100356,
      -1.3511804518465755,
      -1.3969716170156563,
      -1.3633646930374224,
      -1.3026135981873062,
      -1.375348174640377,
      -1.3806799435091295,
      -1.3903145820520229,
      -1.338010445848998,
      -1.4771776211221153
    ],
    [
      -1.3428996093019712,
      -1.2555376984842566,
      -1.3108218090323198,
      -1.3127057917460796,
      -1.3093574119050113,
      -1.2794899956016776,
      0.0,
      -1.2559368233291168,
      -1.2442812221995003,
      -1.3195672692651088,
      -1.298142475836837,
      -1.3251788973743628,
      -1.2033499917518904,
      -1.229398857297225,
      -1.2563323458289053,
      -1.267491263899364,
      -1.3249522396120559,
      -1.3021022597190222,
      -1.3484698025516109,
      -1.2384545895968535,
      -1.3145797110270574,
      -1.2638888773777883,
      -1.2850301123913892,
      -1.2856269078374134,
      -1.3037348859684212,
      -1.2421027455529958,
      -1.3239255378329953,
      -1.305635694420663,
      -1.330968782030368
    ],
    [
      -1.4365099905808365,
      -1.3441400102272905,
      -1.3795440713878528,
      -1.3754128119872173,
      -1.3684448515722916,
      -1.364820050067341,
      -1.3799029579530304,
      0.0,
      -1.3846518827213448,
      -1.4043412181707775,
      -1.3791253675047517,
      -1.4997453857705498,
      -1.3249433827324706,
      -1.318948607897503,
      -1.3270262726270847,
      -1.3651905360638565,
      -1.4064424742016253,
      -1.373435961685027,
      -1.3759853980440138,
      -1.3432394371357044,
      -1.3783518452753072,
      -1.3817533009202296,
      -1.3884173230824242,
      -1.3898127595085392,
      -1.3628595717570615,
      -1.348261718361946,
      -1.4198263385643897,
      -1.4202838884912525,
      -1.435277932668531
    ],
    [
      -1.505244758713091,
      -1.354775030668526,
      -1.3647923886178024,
      -1.4016374865893828,
      -1.483188105080479,
      -1.4316694894190685,
      -1.4599400027055356,
      -1.4407096871819418,
      0.0,
      -1.4923684345405388,
      -1.4075644775779919,
      -1.5914729376661008,
      -1.3550631752192135,
      -1.328902342902214,
      -1.3315703786498716,
      -1.401883054392768,
      -1.5145952897158221,
      -1.4424204491819683,
      -1.4717143790575096,
      -1.4226146651802445,
      -1.4478465876343147,
      -1.4791161967287658,
      -1.4732522215471775,
      -1.4142909189039297,
      -1.4788409201986694,
      -1.4148698980002172,
      -1.5182325324573251,
      -1.4808407008850817,
      -1.5288566035981799
    ],
    [
      -1.4870317249965845,
      -1.4590192626078753,
      -1.404037895390551,
      -1.3707085514098922,
      -1.3764530958392753,
      -1.4663077637445083,
      -1.4685233014573051,
      -1.4194826033523946,
      -1.4672207486433273,
      0.0,
      -1.4092298460998915,
      -1.4793232276527817,
      -1.4190934131186166,
      -1.439969427702327,
      -1.3962178981058937,
      -1.4351264910966883,
      -1.4357986806062113,
      -1.4263803125284842,
      -1.4471073300823412,
      -1.4322212740828582,
      -1.4958547517957148,
      -1.4675756741299746,
      -1.4061012011766467,
      -1.4182670541577305,
      -1.3672758562112837,
      -1.4647814165510629,
      -1.45136260276267,
      -1.4453819267111827,
      -1.495331513934677
    ],
    [
      -1.239710239956273,
      -1.138523497780516,
      -1.1625530737694993,
      -1.0557027288956586,
      -1.184214039904091,
      -1.2359181490211983,
      -1.288411572157363,
      -1.2265925224999499,
      -1.193009189477305,
      -1.2195720653718614,
      0.0,
      -1.3391128097037925,
      -1.0098764037963903,
      -1.116314807546006,
      -1.0392587395435573,
      -1.0833102884265204,
      -1.2288765982087868,
      -1.1847895670155064,
      -1.221170897357498,
      -1.1613614801507322,
      -1.261786184903023,
      -1.2795698117142587,
      -1.2187260031292442,
      -1.0647548506631117,
      -1.211397183386878,
      -1.1391785328118622,
      -1.2189869530158435,
      -1.2920876743375604,
      -1.3036667172891732
    ],
    [
      -1.1848865496176906,
      -1.1683601925048932,
      -1.1609120150096814,
      -1.1508986193452329,
      -1.1496258663183576,
      -1.1222054725335708,
      -1.1393756706562137,
      -1.1437722296665036,
      -1.1906577831096232,
      -1.165188253408638,
      -1.1610567918824126,
      0.0,
      -1.1580740681673207,
      -1.1529166128487407,
      -1.1562402181597722,
      -1.1545341912019016,
      -1.1709452378263585,
      -1.1629396573662407,
      -1.1768632600506843,
      -1.1552161529652183,
      -1.1811748607802102,
      -1.1304704297153607,
      -1.164148662893121,
      -1.151009897751987,
      -1.1425752145713637,
      -1.1438764640672976,
      -1.170811922003598,
      -1.168739004414007,
      -1.134050991754944
    ],
    [
      -1.2601677577407266,
      -1.1327079779037796,
      -1.12616579705404,
      -1.035145485179964,
      -1.2129404471504617,
      -1.1951408279636588,
      -1.233491211272431,
      -1.1750923443461538,
      -1.1313936712549677,
      -1.2320721887170212,
      -1.0784060162355629,
      -1.3521818722313734,
      0.0,
      -1.1227302398885004,
      -1.0842762211998025,
      -1.1053262111289368,
      -1.2278435490379584,
      -1.18437524171028,
      -1.1725941900068881,
      -1.155322331681797,
      -1.256659609426863,
      -1.2269386274424052,
      -1.2294611457013591,
      -1.0887351006423485,
      -1.2447157193262042,
      -1.1400986993725115,
      -1.2528224655915066,
      -1.2604232625864227,
      -1.300563953936488
    ],
    [
      -1.3353519183811768,
      -1.1012608148042022,
      -1.1313658058982177,
      -1.180106708755415,
      -1.2687188407940406,
      -1.2169279473557035,
      -1.295640126838934,
      -1.2345530402117972,
      -1.0851151289671583,
      -1.3478580496016805,
      -1.1886641408888428,
      -1.4425168891871785,
      -1.1094240591281324,
      0.0,
      -1.105372273042206,
      -1.2114389956855596,
      -1.2839813175602515,
      -1.1658625665279367,
      -1.2601457092495152,
      -1.1922711282615233,
      -1.2144925560321134,
      -1.3085372864202838,
      -1.2902833708598322,
      -1.1787430218297856,
      -1.330623057123416,
      -1.2020823254751787,
      -1.3241153435338462,
      -1.2327747910157816,
      -1.3782540179628213
    ],
    [
      -1.3797377961167743,
      -1.1706392765833764,
      -1.2241437296604023,
      -1.2055556814990347,
      -1.274743564581096,
      -1.2388572050798083,
      -1.3488169880052772,
      -1.288789021596648,
      -1.2142359897781339,
      -1.3628021030824866,
      -1.1933700997435186,
      -1.49843692538541,
      -1.1822865067260195,
      -1.1349898790224555,
      0.0,
      -1.2714649352571463,
      -1.332190068853067,
      -1.2640910751856245,
      -1.3274756826069818,
      -1.2272041275260106,
      -1.2848141415118233,
      -1.3001443917355635,
      -1.3084043463096524,
      -1.2110741043112034,
      -1.361995311220391,
      -1.2444317880337745,
      -1.3353371873606255,
      -1.3163857940014936,
      -1.3954085701018966
    ],
    [
      -1.4911885032412342,
      -1.3845211936801247,
      -1.3693970633992854,
      -1.2608246339513822,
      -1.4089416028563542,
      -1.4220727887303999,
      -1.500395431565369,
      -1.4186098980179798,
      -1.4610036840001377,
      -1.4680277819821062,
      -1.3698461481185258,
      -1.6323050328523228,
      -1.3210279758146446,
      -1.351953200015185,
      -1.3548421462124216,
      0.0,
      -1.470583640980935,
      -1.4118507676394745,
      -1.3655201794515435,
      -1.4021072006635658,
      -1.47958813806797,
      -1.4442634893056119,
      -1.476293770073812,
      -1.330572650041528,
      -1.467620431543284,
      -1.367127576679768,
      -1.476673327743512,
      -1.4422690579724688,
      -1.5445210361672417
    ],
    [
      -1.550349898156803,
      -1.4646224835725739,
      -1.4444362284663999,
      -1.4208255204006843,
      -1.4705223577958277,
      -1.4879803673519507,
      -1.5948826433940861,
      -1.49500515714991,
      -1.5328798271546908,
      -1.5091342904758103,
      -1.4934654233118239,
      -1.619716079127913,
      -1.4198725006089115,
      -1.444153258685318,
      -1.4403317291221809,
      -1.4326167228816746,
      0.0,
      -1.4271015267927514,
      -1.4338448042443797,
      -1.501380295333177,
      -1.4516400567286005,
      -1.5455281680567363,
      -1.4765183960372332,
      -1.4769673857139267,
      -1.4983675210263427,
      -1.4772496012818834,
      -1.4898378904844134,
      -1.4805071190798174,
      -1.5586542648020925
    ],
    [
      -1.2412845708648308,
      -1.1546243247848218,
      -1.1634139060138122,
      -1.1716465208655051,
      -1.161103753578257,
      -1.1451804763544493,
      -1.247579590798982,
      -1.2098672664134413,
      -1.1817034779341196,
      -1.223965176228316,
      -1.15826953495927,
      -1.3422252717257035,
      -1.1069470739901481,
      -1.1025945906116632,
      -1.105743597810692,
      -1.1226019829748415,
      -1.1378617507775497,
      0.0,
      -1.1692045083890017,
      -1.1590334153330235,
      -1.2066099499715655,
      -1.2196985564372245,
      -1.2225632092654923,
      -1.1873330666918969,
      -1.2315433178892723,
      -1.1813366560389116,
      -1.1853252125102813,
      -1.196769692577112,
      -1.289985706859539
    ],
    [
      -1.2776034344186353,
      -1.234411635312681,
      -1.1861909385892542,
      -1.1363902190318846,
      -1.2577605218408598,
      -1.2078168580908972,
      -1.32857408955219,
      -1.2271728449016854,
      -1.2482315549789678,
      -1.2510337294327092,
      -1.2014800550402467,
      -1.391552572770309,
      -1.1162264244496565,
      -1.1667031983796183,
      -1.1714684967802835,
      -1.1167607179662444,
      -1.1946480731679656,
      -1.1953746652420634,
      0.0,
      -1.1981715186294366,
      -1.3065207350146606,
      -1.2682538071339953,
      -1.2930691728881893,
      -1.1442519253249248,
      -1.2793390265905522,
      -1.202944840255677,
      -1.1952501590035947,
      -1.294341979077906,
      -1.3111961097455098
    ],
    [
      -1.2808384938340822,
      -1.1835543455550463,
      -1.2302373119895844,
      -1.2470070298815095,
      -1.2208270533192926,
      -1.2441563621375467,
      -1.2617759153985648,
      -1.19668271060617,
      -1.1996823573297686,
      -1.3198072937324385,
      -1.2280176657377606,
      -1.4095755814458106,
      -1.173461939116809,
      -1.1665826922244213,
      -1.153199370953903,
      -1.227954043675784,
      -1.3057363049408028,
      -1.2558283813379645,
      -1.2496085693699206,
      0.0,
      -1.2735764514271453,
      -1.2763502688840307,
      -1.2858218809897777,
      -1.2130855930826145,
      -1.3035020241001616,
      -1.1896122070020194,
      -1.2941248311791205,
      -1.3045192991589638,
      -1.358939153921471
    ],
    [
      -1.368371992961194,
      -1.3000781672088633,
      -1.307347026349747,
      -1.3521581720172302,
      -1.3312545781946548,
      -1.3695257888329098,
      -1.4167697124955547,
      -1.364492316394657,
      -1.335217372691936,
      -1.4060564390669703,
      -1.369075705476018,
      -1.4765942383170758,
      -1.3694053657799115,
      -1.2845423867966694,
      -1.299008072045655,
      -1.3665513146881614,
      -1.4145036006053533,
      -1.320698600086363,
      -1.4154014607194099,
      -1.3684813264940803,
      0.0,
      -1.4087766953245047,
      -1.3537636677678122,
      -1.3661031404161537,
      -1.4030726531297735,
      -1.4019916722934191,
      -1.4461071266858383,
      -1.3545499569166628,
      -1.449645774642236
    ],
    [
      -1.3003030107100364,
      -1.1837032681230268,
      -1.2132741312438882,
      -1.2238504547458111,
      -1.22590157670432,
      -1.2028777989575488,
      -1.1990870916960477,
      -1.157642044027423,
      -1.1730110946254309,
      -1.208813324340357,
      -1.259425202839738,
      -1.299927970295004,
      -1.151155805371941,
      -1.1793270312582247,
      -1.137216153944346,
      -1.2483812011388387,
      -1.2364368852395355,
      -1.1932669540906407,
      -1.23899238272619,
      -1.1944601725697288,
      -1.243779424490468,
      0.0,
      -1.2349572213244269,
      -1.232894402720265,
      -1.2410954577119433,
      -1.208039978352019,
      -1.2327587668344937,
      -1.1637635006665281,
      -1.2026715666369165
    ],
    [
      -1.43972343693738,
      -1.3793354438095387,
      -1.3478745562430008,
      -1.371226320295039,
      -1.3213779661089526,
      -1.4052996748761843,
      -1.4347014989395295,
      -1.3706204064376093,
      -1.4579950810670925,
      -1.3775156497774406,
      -1.4038784880956503,
      -1.5021578979865322,
      -1.387798835071665,
      -1.3799227601392083,
      -1.3356426675254138,
      -1.4097015555396537,
      -1.3978396525424444,
      -1.430078074261682,
      -1.430796291523643,
      -1.3775851408776283,
      -1.3545061698186371,
      -1.4227758926690732,
      0.0,
      -1.3869222103764958,
      -1.3142975887084423,
      -1.3908072007060888,
      -1.4406371393393793,
      -1.424717453202867,
      -1.4650128698498819
    ],
    [
      -1.3483756474940076,
      -1.2326326467920858,
      -1.2310351203844747,
      -1.1373932839032839,
      -1.240512904681992,
      -1.2635477917571085,
      -1.3697938278232764,
      -1.2683102993488677,
      -1.2438326911579318,
      -1.275589454374556,
      -1.2008956503133095,
      -1.4433916503020692,
      -1.184517890984656,
      -1.1974369026914446,
      -1.182398096236119,
      -1.189636282749153,
      -1.3095492981824657,
      -1.2768766718856486,
      -1.2838952756599942,
      -1.2288383496513504,
      -1.3019110300981238,
      -1.3277205579896842,
      -1.3026254825251602,
      0.0,
      -1.2932161822673858,
      -1.2474250687212018,
      -1.3213814619510678,
      -1.3174180978522638,
      -1.3843395839834673
    ],
    [
      -1.3725904170956877,
      -1.3456846503620965,
      -1.3017342526431916,
      -1.2992090691450762,
      -1.2624005610646685,
      -1.3162423905502925,
      -1.3986054805972183,
      -1.3280949886860967,
      -1.4141713651590488,
      -1.3199492815806642,
      -1.3449737617400401,
      -1.3988371452385817,
      -1.3348578095707722,
      -1.312983805236335,
      -1.347537053203389,
      -1.3219263927447773,
      -1.3292705210728588,
      -1.3746840881755549,
      -1.356455995945553,
      -1.3655800165932659,
      -1.3480155191347394,
      -1.3968117467910344,
      -1.1997846949781517,
      -1.3330115138304475,
      0.0,
      -1.3647936346331306,
      -1.3914908213661359,
      -1.3678372396781373,
      -1.3777362185107935
    ],
    [
      -1.475719152497869,
      -1.3999563250354474,
      -1.433231757001794,
      -1.3536794320905834,
      -1.4369276969351716,
      -1.4424578811680437,
      -1.4536034920437326,
      -1.3661201548499493,
      -1.4480266821025345,
      -1.4792054367130447,
      -1.4003913121428557,
      -1.5894673416512362,
      -1.3008568537489653,
      -1.3514055571684604,
      -1.321689308254976,
      -1.3922945292160482,
      -1.484509038618022,
      -1.4031460809614655,
      -1.4448818613843215,
      -1.3590378063737862,
      -1.4139991180037683,
      -1.4435786225817742,
      -1.4622915372808307,
      -1.3679109839516703,
      -1.4570209665289027,
      0.0,
      -1.4582291987950566,
      -1.472217128895682,
      -1.5029464791015745
    ],
    [
      -1.2251742542049584,
      -1.1918011290935926,
      -1.1472207359660778,
      -1.1171742743842588,
      -1.0806640444993034,
      -1.1435312279815273,
      -1.2053149768682365,
      -1.1403459392614637,
      -1.171754677212731,
      -1.0716154149749668,
      -1.1043311780058351,
      -1.2552797311331378,
      -1.1091474180536878,
      -1.1508549183328074,
      -1.064967118068404,
      -1.1113227737044256,
      -1.1597715984641634,
      -1.1075533119756433,
      -1.0839384246510877,
      -1.1434295769104188,
      -1.1944788784646974,
      -1.2059279441045159,
      -1.128542317242858,
      -1.0806174336997367,
      -1.1390846211033159,
      -1.09365433176717,
      0.0,
      -1.195856326645114,
      -1.2260290587060365
    ],
    [
      -1.2783045582409904,
      -1.1484740274113987,
      -1.198779571959305,
      -1.2469908583731628,
      -1.2152141223964685,
      -1.171570118712874,
      -1.2734231640978282,
      -1.229678937005524,
      -1.1782416897586885,
      -1.1959047298194951,
      -1.2052164579235287,
      -1.3278466789141579,
      -1.195963864858971,
      -1.1552766441982907,
      -1.1591798014162387,
      -1.1949752898758077,
      -1.2008376592145864,
      -1.166263710148202,
      -1.2616566437694714,
      -1.2603588586748342,
      -1.1941152194454623,
      -1.2029865740316061,
      -1.2338798435561418,
      -1.215741446823925,
      -1.2368156063278202,
      -1.218515107152117,
      -1.2436836464643783,
      0.0,
      -1.2657508577433385
    ],
    [
      -1.4545463790848692,
      -1.453142061444361,
      -1.409649970744107,
      -1.4075164437313197,
      -1.4076768144045053,
      -1.3939295333458621,
      -1.4790055412252832,
      -1.3886394910806414,
      -1.4730367126391808,
      -1.4049897492304686,
      -1.4133026640354438,
      -1.4771875529626262,
      -1.3909378771061331,
      -1.4120081596576635,
      -1.407409014907995,
      -1.4278376576125107,
      -1.3659331389679745,
      -1.397206418530027,
      -1.4387092906824115,
      -1.448982418248053,
      -1.4404958833530088,
      -1.3722927748837315,
      -1.4119644595616485,
      -1.4429276026983604,
      -1.3778412382854297,
      -1.4284049241977932,
      -1.4265783546126636,
      -1.434944156810746,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.4655405758217481,
      0.4709894882814585,
      0.4747090594096566,
      0.4589028282843941,
      0.44158486930899765,
      0.39197613099025164,
      0.4644191833040978,
      0.4701906012311332,
      0.38452969605219267,
      0.4718386706385673,
      0.324951620775805,
      0.4772507661119707,
      0.4880860056399958,
      0.489716904012935,
      0.4435220971256666,
      0.38265200579747893,
      0.41314118002754796,
      0.44448851475133244,
      0.44785409155491984,
      0.45444744473706056,
      0.3837503980571364,
      0.4374136958544257,
      0.45176026988596263,
      0.4115219398390957,
      0.43101621608999174,
      0.39885676963814887,
      0.38748216735716134,
      0.35868069264938085
    ],
    [
      0.3879976219409518,
      0.0,
      0.6053695740612359,
      0.5345452198185812,
      0.49084984581175783,
      0.5199102633639274,
      0.3908363260667733,
      0.46427619453319724,
      0.5956944422198505,
      0.395994350776653,
      0.5066967390208603,
      0.2501205245722624,
      0.588275517862753,
      0.6473595788948356,
      0.6718930142786124,
      0.49966757994320443,
      0.4216974404523346,
      0.49148123144969524,
      0.4188473342105432,
      0.5023741505071129,
      0.4976050005985062,
      0.4479947649778433,
      0.4562494199835876,
      0.5251158415160071,
      0.4075891155911495,
      0.46921528427639436,
      0.3575587974313985,
      0.4852078743423418,
      0.3489783020391115
    ],
    [
      0.37522095046445814,
      0.5549209433221682,
      0.0,
      0.5321417306869549,
      0.43583580711364545,
      0.45923328210996184,
      0.3430865290729066,
      0.41303113199369856,
      0.5310647735918541,
      0.386837858973194,
      0.47570348797956985,
      0.25108886570263,
      0.5838065928618492,
      0.5642568540448039,
      0.5631551279529563,
      0.47008196473337027,
      0.4064458612330526,
      0.4697880233421634,
      0.42449622419558786,
      0.4603209679419371,
      0.4448543735858048,
      0.3763609201502629,
      0.42482022622796545,
      0.5083725032514272,
      0.4065861405191704,
      0.39752431298002144,
      0.34103486542536254,
      0.3971252741921232,
      0.2764106952465428
    ],
    [
      0.3311592571239541,
      0.4203492138630609,
      0.46633161681970425,
      0.0,
      0.37843200804668675,
      0.4010852331960775,
      0.27859827044957264,
      0.3433999713131415,
      0.40097571425936374,
      0.35411565784629717,
      0.46061660378165503,
      0.20999498745927125,
      0.5304844953397905,
      0.44548168804629884,
      0.448864788574056,
      0.44224676457447365,
      0.3590700106657392,
      0.3936583451846716,
      0.4331758067615816,
      0.38584919670810036,
      0.3126196921320803,
      0.32691291310051684,
      0.35384180056685444,
      0.44078128922687143,
      0.3440784042677587,
      0.40344027029987384,
      0.315797527710203,
      0.3240464399577805,
      0.28549509687703756
    ],
    [
      0.3255145515348854,
      0.3874133936990034,
      0.37106852944970536,
      0.43335559879357377,
      0.0,
      0.3732309173080244,
      0.32878131986213455,
      0.3969259962599536,
      0.3410621696086684,
      0.3820930968229934,
      0.4290579596821795,
      0.2504484818663797,
      0.43710748160766477,
      0.4343056306021702,
      0.5233583145781824,
      0.4640255327832179,
      0.3770030533121653,
      0.44683629921626755,
      0.3985637365338017,
      0.46332642304049987,
      0.39843725531505836,
      0.3276667679409144,
      0.4195140893000626,
      0.4427302438066092,
      0.3751577685991079,
      0.37869399062686093,
      0.37890753037413694,
      0.35978529584758867,
      0.28825759683660346
    ],
    [
      0.39377612683465135,
      0.5802194482979381,
      0.615470859964635,
      0.5748979085082564,
      0.45424838894841146,
      0.0,
      0.40809061037731453,
      0.44292905122015624,
      0.5733049446984015,
      0.4223511875455275,
      0.5415942118196009,
      0.2816265333699479,
      0.5723855087230274,
      0.5956299425875402,
      0.5809749842275744,
      0.5161979269553645,
      0.4406717867222665,
      0.5322604341142794,
      0.4668283576778478,
      0.4596752234818047,
      0.4699710899452647,
      0.42417992477618394,
      0.4577868487544179,
      0.5185379436045341,
      0.4458033671514632,
      0.44047159828271076,
      0.4308369597398174,
      0.48314109594284216,
      0.343973920669725
    ],
    [
      0.31325609263871024,
      0.40061800345642484,
      0.3453338929083616,
      0.3434499101946018,
      0.3467982900356701,
      0.37666570633900376,
      0.0,
      0.4002188786115646,
      0.4118744797411811,
      0.3365884326755726,
      0.35801322610384445,
      0.3309768045663186,
      0.452805710188791,
      0.4267568446434564,
      0.3998233561117761,
      0.38866443804131734,
      0.3312034623286255,
      0.35405344222165924,
      0.30768589938907054,
      0.4177011123438279,
      0.341575990913624,
      0.3922668245628931,
      0.3711255895492922,
      0.370528794103268,
      0.35242081597226016,
      0.4140529563876856,
      0.33223016410768613,
      0.35052000752001833,
      0.32518691991031345
    ],
    [
      0.3503897180992672,
      0.44275969845281327,
      0.4073556372922509,
      0.4114868966928864,
      0.41845485710781216,
      0.42207965861276264,
      0.4069967507270733,
      0.0,
      0.40224782595875896,
      0.3825584905093262,
      0.40777434117535205,
      0.2871543229095539,
      0.46195632594763314,
      0.4679511007826007,
      0.459873436053019,
      0.4217091726162472,
      0.38045723447847846,
      0.4134637469950768,
      0.41091431063608996,
      0.44366027154439935,
      0.40854786340479654,
      0.4051464077598741,
      0.39848238559767957,
      0.3970869491715645,
      0.4240401369230422,
      0.43863799031815764,
      0.36707337011571406,
      0.3666158201888512,
      0.35162177601157274
    ],
    [
      0.28601307472195625,
      0.4364828027665213,
      0.4264654448172449,
      0.3896203468456645,
      0.30806972835456836,
      0.35958834401597883,
      0.3313178307295117,
      0.3505481462531055,
      0.0,
      0.2988893988945085,
      0.38369335585705544,
      0.19978489576894654,
      0.4361946582158338,
      0.4623554905328333,
      0.4596874547851757,
      0.38937477904227924,
      0.27666254371922516,
      0.348837384253079,
      0.31954345437753773,
      0.3686431682548028,
      0.34341124580073257,
      0.3121416367062815,
      0.3180056118878698,
      0.37696691453111764,
      0.3124169132363779,
      0.37638793543483007,
      0.27302530097772215,
      0.3104171325499656,
      0.2624012298368674
    ],
    [
      0.3160493092489083,
      0.34406177163761753,
      0.3990431388549418,
      0.4323724828356006,
      0.4266279384062175,
      0.3367732705009845,
      0.3345577327881877,
      0.3835984308930982,
      0.3358602856021655,
      0.0,
      0.39385118814560127,
      0.32375780659271114,
      0.38398762112687623,
      0.3631116065431659,
      0.4068631361395991,
      0.3679545431488045,
      0.3672823536392815,
      0.37670072171700864,
      0.35597370416315166,
      0.37085976016263467,
      0.307226282449778,
      0.33550536011551824,
      0.3969798330688461,
      0.38481398008776235,
      0.43580517803420915,
      0.33829961769442995,
      0.3517184314828228,
      0.3576991075343101,
      0.3077495203108158
    ],
    [
      0.3604639121049025,
      0.4616506542806593,
      0.43762107829167607,
      0.5444714231655168,
      0.4159601121570844,
      0.36425600303997707,
      0.3117625799038124,
      0.37358162956122554,
      0.40716496258387047,
      0.38060208668931406,
      0.0,
      0.2610613423573829,
      0.5902977482647851,
      0.4838593445151693,
      0.5609154125176181,
      0.516863863634655,
      0.3712975538523886,
      0.415384585045669,
      0.37900325470367746,
      0.43881267191044326,
      0.33838796715815245,
      0.32060434034691676,
      0.38144814893193124,
      0.5354193013980637,
      0.38877696867429745,
      0.4609956192493132,
      0.38118719904533194,
      0.30808647772361497,
      0.29650743477200225
    ],
    [
      0.2686260661885156,
      0.28515242330131296,
      0.2926006007965247,
      0.3026139964609733,
      0.30388674948784855,
      0.33130714327263533,
      0.31413694514999246,
      0.30974038613970256,
      0.26285483269658294,
      0.2883243623975682,
      0.29245582392379355,
      0.0,
      0.29543854763888544,
      0.30059600295746547,
      0.297272397646434,
      0.29897842460430457,
      0.28256737797984766,
      0.29057295843996545,
      0.27664935575552185,
      0.2982964628409879,
      0.272337755025996,
      0.32304218609084545,
      0.28936395291308514,
      0.30250271805421924,
      0.3109374012348425,
      0.30963615173890857,
      0.2827006938026082,
      0.28477361139219926,
      0.3194616240512622
    ],
    [
      0.32365772815622584,
      0.45111750799317285,
      0.4576596888429125,
      0.5486800007169885,
      0.3708850387464908,
      0.38868465793329365,
      0.3503342746245215,
      0.40873314155079865,
      0.4524318146419848,
      0.3517532971799313,
      0.5054194696613896,
      0.23164361366557906,
      0.0,
      0.4610952460084521,
      0.49954926469715,
      0.4784992747680157,
      0.35598193685899404,
      0.3994502441866725,
      0.41123129589006435,
      0.4285031542151554,
      0.32716587647008954,
      0.35688685845454726,
      0.3543643401955934,
      0.495090385254604,
      0.3391097665707483,
      0.443726786524441,
      0.3310030203054459,
      0.3234022233105298,
      0.28326153196046455
    ],
    [
      0.40508365688634185,
      0.6391747604633164,
      0.609069769369301,
      0.5603288665121038,
      0.47171673447347806,
      0.5235076279118152,
      0.44479544842858476,
      0.5058825350557214,
      0.6553204463003603,
      0.39257752566583815,
      0.5517714343786759,
      0.2979186860803402,
      0.6310115161393863,
      0.0,
      0.6350633022253127,
      0.5289965795819591,
      0.4564542577072672,
      0.574573008739582,
      0.4802898660180035,
      0.5481644470059954,
      0.5259430192354053,
      0.4318982888472349,
      0.45015220440768644,
      0.561692553437733,
      0.4098125181441026,
      0.53835324979234,
      0.4163202317336725,
      0.5076607842517371,
      0.36218155730469737
    ],
    [
      0.41754190194921614,
      0.626640421482614,
      0.5731359684055881,
      0.5917240165669557,
      0.5225361334848944,
      0.5584224929861821,
      0.44846271006071325,
      0.5084906764693424,
      0.5830437082878566,
      0.4344775949835038,
      0.6039095983224718,
      0.29884277268058046,
      0.6149931913399709,
      0.6622898190435349,
      0.0,
      0.5258147628088441,
      0.4650896292129234,
      0.533188622880366,
      0.46980401545900863,
      0.5700755705399798,
      0.5124655565541671,
      0.4971353063304269,
      0.488875351756338,
      0.586205593754787,
      0.43528438684559934,
      0.5528479100322159,
      0.46194251070536496,
      0.48089390406449684,
      0.40187112796409385
    ],
    [
      0.38290535793772396,
      0.48957266749883344,
      0.5046967977796728,
      0.6132692272275759,
      0.465152258322604,
      0.45202107244855827,
      0.37369842961358923,
      0.4554839631609784,
      0.41309017717882046,
      0.406066079196852,
      0.5042477130604324,
      0.2417888283266354,
      0.5530658853643136,
      0.5221406611637731,
      0.5192517149665365,
      0.0,
      0.4035102201980232,
      0.4622430935394837,
      0.5085736817274147,
      0.4719866605153924,
      0.3945057231109881,
      0.4298303718733463,
      0.39780009110514625,
      0.5435212111374301,
      0.40647342963567423,
      0.5069662844991902,
      0.39742053343544614,
      0.4318248032064893,
      0.3295728250117165
    ],
    [
      0.34868721964464844,
      0.4344146342288775,
      0.4546008893350515,
      0.478211597400767,
      0.4285147600056236,
      0.41105675044950063,
      0.3041544744073652,
      0.40403196065154146,
      0.3661572906467605,
      0.3899028273256411,
      0.4055716944896275,
      0.2793210386735383,
      0.4791646171925399,
      0.45488385911613327,
      0.45870538867927046,
      0.4664203949197767,
      0.0,
      0.4719355910087,
      0.4651923135570717,
      0.3976568224682744,
      0.4473970610728508,
      0.35350894974471503,
      0.4225187217642181,
      0.42206973208752463,
      0.4006695967751086,
      0.4217875165195679,
      0.40919922731703795,
      0.418529998721634,
      0.3403828529993589
    ],
    [
      0.312724391311723,
      0.399384637391732,
      0.3905950561627416,
      0.3823624413110487,
      0.3929052085982969,
      0.4088284858221045,
      0.30642937137757187,
      0.3441416957631125,
      0.37230548424243426,
      0.3300437859482379,
      0.39573942721728383,
      0.21178369045085033,
      0.4470618881864057,
      0.4514143715648906,
      0.44826536436586184,
      0.43140697920171234,
      0.4161472113990041,
      0.0,
      0.3848044537875521,
      0.3949755468435303,
      0.34739901220498837,
      0.33431040573932935,
      0.3314457529110615,
      0.36667589548465696,
      0.3224656442872815,
      0.37267230613764224,
      0.3686837496662725,
      0.35723926959944174,
      0.26402325531701476
    ],
    [
      0.4081434814558238,
      0.4513352805617781,
      0.49955597728520496,
      0.5493566968425745,
      0.4279863940335993,
      0.4779300577835619,
      0.3571728263222691,
      0.4585740709727737,
      0.43751536089549137,
      0.43471318644174994,
      0.48426686083421244,
      0.29419434310415005,
      0.5695204914248027,
      0.5190437174948408,
      0.5142784190941756,
      0.5689861979082147,
      0.4910988427064935,
      0.49037225063239576,
      0.0,
      0.4875753972450225,
      0.37922618085979853,
      0.41749310874046386,
      0.3926777429862698,
      0.5414949905495343,
      0.4064078892839069,
      0.4828020756187821,
      0.49049675687086447,
      0.3914049367965531,
      0.3745508061289493
    ],
    [
      0.4242689401599373,
      0.5215530884389732,
      0.4748701220044351,
      0.4581004041125101,
      0.4842803806747269,
      0.4609510718564729,
      0.4433315185954547,
      0.5084247233878496,
      0.505425076664251,
      0.385300140261581,
      0.47708976825625893,
      0.2955318525482089,
      0.5316454948772105,
      0.5385247417695982,
      0.5519080630401165,
      0.47715339031823545,
      0.3993711290532167,
      0.44927905265605506,
      0.45549886462409894,
      0.0,
      0.4315309825668743,
      0.42875716510998885,
      0.41928555300424186,
      0.492021840911405,
      0.40160540989385796,
      0.5154952269920001,
      0.41098260281489907,
      0.40058813483505573,
      0.3461682800725485
    ],
    [
      0.3624239191083447,
      0.4307177448606754,
      0.42344888571979156,
      0.37863774005230844,
      0.3995413338748839,
      0.3612701232366289,
      0.3140261995739839,
      0.3663035956748817,
      0.39557853937760257,
      0.32473947300256834,
      0.36172020659352055,
      0.25420167375246283,
      0.3613905462896272,
      0.4462535252728692,
      0.4317878400238837,
      0.36424459738137727,
      0.31629231146418535,
      0.41009731198317567,
      0.3153944513501288,
      0.36231458557545837,
      0.0,
      0.322019216745034,
      0.37703224430172644,
      0.364692771653385,
      0.32772325893976517,
      0.3288042397761195,
      0.28468878538370035,
      0.37624595515287584,
      0.28115013742730266
    ],
    [
      0.3506191303710622,
      0.46721887295807174,
      0.4376480098372104,
      0.42707168633528747,
      0.4250205643767786,
      0.4480443421235498,
      0.4518350493850509,
      0.4932800970536755,
      0.47791104645566773,
      0.44210881674074165,
      0.39149693824136067,
      0.35099417078609463,
      0.4997663357091575,
      0.4715951098228739,
      0.5137059871367526,
      0.4025409399422599,
      0.41448525584156304,
      0.45765518699045793,
      0.41192975835490864,
      0.4564619685113698,
      0.4071427165906305,
      0.0,
      0.4159649197566717,
      0.4180277383608335,
      0.4098266833691553,
      0.44288216272907954,
      0.4181633742466049,
      0.48715864041457047,
      0.44825057444418204
    ],
    [
      0.39298273331881206,
      0.4533707264466533,
      0.4848316140131912,
      0.461479849961153,
      0.5113282041472393,
      0.42740649538000763,
      0.3980046713166625,
      0.4620857638185827,
      0.37471108918909946,
      0.4551905204787514,
      0.4288276821605417,
      0.33054827226965977,
      0.44490733518452696,
      0.45278341011698364,
      0.49706350273077815,
      0.4230046147165383,
      0.4348665177137476,
      0.4026280959945099,
      0.401909878732549,
      0.4551210293785637,
      0.47820000043755484,
      0.40993027758711875,
      0.0,
      0.4457839598796962,
      0.5184085815477497,
      0.4418989695501032,
      0.39206903091681267,
      0.40798871705332496,
      0.3676933004063101
    ],
    [
      0.3596961703024759,
      0.4754391710043977,
      0.4770366974120088,
      0.5706785338931997,
      0.4675589131144915,
      0.444524026039375,
      0.3382779899732071,
      0.4397615184476158,
      0.4642391266385517,
      0.4324823634219275,
      0.5071761674831741,
      0.26468016749441436,
      0.5235539268118274,
      0.5106349151050389,
      0.5256737215603646,
      0.5184355350473306,
      0.39852251961401786,
      0.43119514591083496,
      0.42417654213648937,
      0.4792334681451331,
      0.4061607876983597,
      0.3803512598067993,
      0.40544633527132334,
      0.0,
      0.41485563552909777,
      0.4606467490752817,
      0.3866903558454158,
      0.3906537199442197,
      0.3237322338130162
    ],
    [
      0.3304526678481703,
      0.35735843458176153,
      0.4013088323006664,
      0.40383401579878186,
      0.44064252387918956,
      0.38680069439356557,
      0.3044376043466397,
      0.3749480962577614,
      0.2888717197848092,
      0.3830938033631939,
      0.3580693232038179,
      0.3042059397052763,
      0.3681852753730859,
      0.3900592797075231,
      0.355506031740469,
      0.38111669219908073,
      0.3737725638709992,
      0.32835899676830316,
      0.3465870889983049,
      0.33746306835059214,
      0.35502756580911865,
      0.30623133815282366,
      0.5032583899657064,
      0.37003157111341056,
      0.0,
      0.3382494503107274,
      0.31155226357772214,
      0.3352058452657207,
      0.3253068664330645
    ],
    [
      0.382411604719751,
      0.4581744321821726,
      0.42489900021582594,
      0.5044513251270366,
      0.42120306028244836,
      0.41567287604957626,
      0.4045272651738874,
      0.4920106023676707,
      0.4101040751150855,
      0.3789253205045753,
      0.4577394450747643,
      0.26866341556638385,
      0.5572739034686547,
      0.5067252000491596,
      0.5364414489626439,
      0.46583622800157176,
      0.37362171859959803,
      0.45498467625615446,
      0.4132488958332985,
      0.49909295084383376,
      0.4441316392138517,
      0.41455213463584584,
      0.3958392199367893,
      0.4902197732659497,
      0.4011097906887173,
      0.0,
      0.39990155842256336,
      0.385913628321938,
      0.35518427811604547
    ],
    [
      0.3282268194752658,
      0.3615999445866316,
      0.4061803377141464,
      0.4362267992959654,
      0.47273702918092075,
      0.4098698456986969,
      0.34808609681198766,
      0.4130551344187605,
      0.3816463964674932,
      0.48178565870525736,
      0.44906989567438904,
      0.29812134254708633,
      0.4442536556265364,
      0.4025461553474168,
      0.48843395561182024,
      0.4420782999757986,
      0.39362947521606073,
      0.44584776170458085,
      0.4694626490291365,
      0.4099714967698054,
      0.3589221952155268,
      0.3474731295757083,
      0.42485875643736626,
      0.47278363998048745,
      0.4143164525769083,
      0.4597467419130541,
      0.0,
      0.3575447470351101,
      0.32737201497418766
    ],
    [
      0.3130737189705417,
      0.44290424980013343,
      0.39259870525222706,
      0.3443874188383693,
      0.3761641548150636,
      0.41980815849865816,
      0.31795511311370395,
      0.361699340206008,
      0.41313658745284365,
      0.395473547392037,
      0.3861618192880034,
      0.26353159829737427,
      0.39541441235256114,
      0.43610163301324145,
      0.43219847579529347,
      0.39640298733572443,
      0.3905406179969457,
      0.42511456706333006,
      0.32972163344206074,
      0.3310194185366979,
      0.39726305776606985,
      0.388391703179926,
      0.35749843365539036,
      0.3756368303876072,
      0.35456267088371196,
      0.37286317005941516,
      0.3476946307471538,
      0.0,
      0.3256274194681936
    ],
    [
      0.37398580944509785,
      0.37539012708560593,
      0.41888221778586,
      0.42101574479864734,
      0.4208553741254617,
      0.4346026551841049,
      0.34952664730468386,
      0.4398926974493256,
      0.35549547589078623,
      0.4235424392994984,
      0.4152295244945232,
      0.35134463556734086,
      0.43759431142383387,
      0.41652402887230355,
      0.42112317362197205,
      0.4006945309174563,
      0.4625990495619925,
      0.43132576999994,
      0.3898228978475555,
      0.37954977028191395,
      0.3880363051769582,
      0.45623941364623555,
      0.4165677289683185,
      0.3856045858316066,
      0.4506909502445373,
      0.4001272643321738,
      0.40195383391730344,
      0.3935880317192211,
      0.0
    ]
  ],
  "row_avgs": [
    0.4329026386867327,
    0.47783576251933857,
    0.43834308174626585,
    0.37824653800544544,
    0.3904510366145862,
    0.4809941494621967,
    0.36758557305595785,
    0.4055891605743805,
    0.3470337936845569,
    0.3655387183187518,
    0.4102301312814089,
    0.2959581054279582,
    0.3974757656922949,
    0.504132674146357,
    0.5152144733918584,
    0.4493099915086657,
    0.4119517064715259,
    0.3673655279390637,
    0.4570776550312235,
    0.4531765364107165,
    0.3586693290552953,
    0.43710021703162943,
    0.43410803015882937,
    0.4364826320192639,
    0.3592834265392959,
    0.43260212382127833,
    0.40878022955593235,
    0.37439093120029593,
    0.4075644640997949
  ],
  "col_avgs": [
    0.35447685471294005,
    0.4505191296594524,
    0.4524524439633419,
    0.46798146207869784,
    0.4263248078532246,
    0.42196843303085657,
    0.3605427398766932,
    0.42069530759961926,
    0.43140280169363315,
    0.3876807499676798,
    0.44302866344866176,
    0.2788672224091852,
    0.4881712057376536,
    0.47579877725924885,
    0.4889769278975122,
    0.4461042532938143,
    0.3908212121855685,
    0.4326581331543438,
    0.4040649371408353,
    0.4309478162685067,
    0.3996407014660746,
    0.38037790616981176,
    0.4020934781807095,
    0.4495060650617164,
    0.3938734576878106,
    0.42636578740147535,
    0.3728460741341869,
    0.3878837015800613,
    0.32932335253758505
  ],
  "combined_avgs": [
    0.39368974669983636,
    0.4641774460893955,
    0.44539776285480387,
    0.42311400004207167,
    0.4083879222339054,
    0.45148129124652664,
    0.3640641564663255,
    0.4131422340869999,
    0.389218297689095,
    0.3766097341432158,
    0.42662939736503536,
    0.2874126639185717,
    0.44282348571497426,
    0.48996572570280295,
    0.5020957006446853,
    0.44770712240124,
    0.4013864593285472,
    0.40001183054670375,
    0.4305712960860294,
    0.4420621763396116,
    0.37915501526068496,
    0.4087390616007206,
    0.41810075416976944,
    0.44299434854049013,
    0.3765784421135533,
    0.42948395561137687,
    0.3908131518450596,
    0.3811373163901786,
    0.36844390831869
  ],
  "gppm": [
    601.102764179217,
    560.994969473381,
    557.8881226156944,
    553.7550194740546,
    566.7531128412553,
    572.7700277926809,
    601.6348906137316,
    573.3333699627173,
    562.4126050641943,
    588.9458258718762,
    567.1436049528899,
    637.2639565155957,
    545.1994354470071,
    551.0317669800824,
    545.1476880700849,
    562.1134241653185,
    584.4755403889042,
    570.7554284956631,
    583.6450348876483,
    570.6659121987607,
    583.274345802128,
    596.0102373918724,
    584.4084697787706,
    561.5488964368772,
    588.2788841025041,
    572.0053027071579,
    600.365263346516,
    591.0143036211978,
    616.3916986134261
  ],
  "gppm_normalized": [
    1.40452115086851,
    1.2816296582816027,
    1.2712580711119705,
    1.2680898319657607,
    1.284390367790866,
    1.303765407447432,
    1.3811439056756878,
    1.303750871295615,
    1.2795418295434913,
    1.3338620691839984,
    1.2833592994439489,
    1.4612704905055478,
    1.2385617859529483,
    1.253714746384086,
    1.2396504655089589,
    1.284715339472711,
    1.3230647687084232,
    1.3019142322795059,
    1.325562019451215,
    1.2957032095953005,
    1.325165432384178,
    1.3576965226027229,
    1.3264717826236794,
    1.2725347462907994,
    1.331172699846146,
    1.3073487228217964,
    1.3604638319948958,
    1.3441987484711038,
    1.3954951189171776
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318,
    1335,
    451,
    436,
    467,
    386,
    418,
    378,
    384,
    420,
    360,
    404,
    488,
    393,
    433,
    455,
    386,
    427,
    409,
    410,
    433,
    420,
    353,
    359,
    437,
    398,
    362,
    406,
    442,
    394,
    1445,
    447,
    481,
    473,
    523,
    469,
    486,
    497,
    458,
    412,
    423,
    472,
    499,
    473,
    479,
    433,
    403,
    454,
    443,
    476,
    426,
    441,
    417,
    408,
    448,
    410,
    411,
    517,
    354,
    704,
    477,
    420,
    393,
    439,
    429,
    605,
    420,
    445,
    408,
    460,
    522,
    417,
    389,
    440,
    461,
    402,
    434,
    438,
    480,
    390,
    394,
    432,
    448,
    378,
    375,
    385,
    410,
    401,
    389,
    491,
    424,
    450,
    396,
    441,
    392,
    376,
    422,
    476,
    440,
    374,
    446,
    482,
    442,
    441,
    362,
    424,
    410,
    414,
    410,
    353,
    400,
    382,
    444,
    360,
    404,
    406,
    395,
    339,
    446,
    477,
    497,
    454,
    431,
    442,
    455,
    560,
    421,
    424,
    522,
    488,
    411,
    400,
    410,
    451,
    469,
    411,
    433,
    451,
    378,
    376,
    454,
    411,
    399,
    394,
    441,
    372
  ],
  "response_lengths": [
    1608,
    2528,
    2702,
    2900,
    2674,
    2479,
    2662,
    2706,
    3286,
    2480,
    2385,
    3099,
    2865,
    2350,
    2271,
    2492,
    2574,
    2636,
    2409,
    2440,
    2447,
    2160,
    2262,
    2649,
    2385,
    2223,
    2288,
    2477,
    2224
  ]
}