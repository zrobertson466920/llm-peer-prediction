{
  "example_idx": 68,
  "reference": "Under review as a conference paper at ICLR 2023\n\nBT-CHAIN: BIDIRECTIONAL TRANSPORT CHAIN FOR TOPIC HIERARCHIES DISCOVERY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTopic modeling has been an important tool for text analysis. Originally, topics discovered by a model are usually assumed to be independent. However, as a semantic representation of a concept, a topic is naturally related to others, which motivates the development of learning hierarchical topic structure. Most existing Bayesian models are designed to learn hierarchical structure, but they need nontrivial posterior inference. Although the recent transport-based topic models bypass the posterior inference, none of them considers deep topic structures. In this paper, we interpret the document as its word embeddings and propose a novel bidirectional transport chain to discover multi-level topic structures, where each layer learns a set of topic embeddings and the document hierarchical representations are defined as a series of empirical distributions according to the topic proportions and corresponding topic embeddings. To fit such hierarchies, we develop an upward-downward optimizing strategy under the recent conditional transport theory, where document information is first transported via the upward path, and then its hierarchical representations are refined according to the adjacent upper and lower layers in a layer-wise manner via the downward path. Extensive experiments on text corpora show that our approach enjoys superior modeling accuracy and interpretability. Moreover, we also conduct experiments on learning hierarchical visual topics from images, which demonstrate the adaptability and flexibility of our method.\n\n1\n\nINTRODUCTION\n\nTopic models (TMs) like latent Dirichlet allocation (LDA) (Blei et al., 2003), Poisson factor analysis (PFA) (Zhou et al., 2012), and their various extensions (Teh et al., 2006; Hoffman et al., 2010; Blei, 2012; Zhou et al., 2016) are a family of popular techniques for discovering the hidden semantic structure from a collection of documents in an unsupervised manner. In addition to learning shallow topics, mining the potential hierarchical topic structures has obtained much research effort since the hierarchies are ubiquitous in big text corpora (Meng et al., 2020; Lee et al., 2022) and can be applied to a wide range of applications (Grimmer, 2010; Zhang et al.; Guo et al., 2020).\n\nHierarchical Bayesian probabilistic models have been commonly used to learn topic structures (Blei et al., 2010; Paisley et al., 2014; Gan et al., 2015; Henao et al., 2015; Zhou et al., 2016), where a hierarchy of topics are learned and the topics in the higher layers serve as the priors of the topics in the lower layers. Despite the success of Bayesian models in topic structure mining, most of them employ Bayesian posterior inference to optimize their parameters (e.g., Markov Chain Monte Carlo (MCMC) and Variational Inference (VI)), which is usually non-trivial to derive and can be less flexible and efficient for big text corpora (Zhang et al., 2018). Recent developments in Autoencoding Variational Inference (AVI) (Kingma & Welling, 2013; Rezende et al., 2014) provide stronger inference tools for Bayesian models and have inspired several neural topic models (Zhang et al., 2018; Duan et al., 2021a), resulting in improved efficiency and flexibility. However, applying AVI to neural topic models still has some limitations or concerns. First, the estimation of variational posterior always needs a trade-off between accuracy and efficiency as an asymptotically exact method (Salimans et al., 2015). Besides, the latent distributions are required to be reparameterizable and KL divergence is expected to be analytical, both of which are hard to meet for topic models since they usually depend on Dirichlet distribution or the Gamma distribution (Blei et al., 2003; Zhou et al., 2015). Another concern comes from likelihood maximization, in which the inference of topic structure relies on word\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nco-occurrence patterns within a document. This has been recently found to give poor quality topics in case of little evidence of co-occurrences, such as corpus with a small number of documents or containing short context (Huynh et al., 2020; Wang et al., 2022). This concern is even more acute in hierarchies learning where more topics and their correlations need to be inferred (Meng et al., 2020). Several existing studies have targeted to incorporate meta knowledge to improve topic representation. The source of side information may come from various fields, including knowledge graph (Xie et al., 2015; Duan et al., 2021b), pre-trained language model (Bianchi et al., 2021; Meng et al., 2022) and word embeddings (Wang et al., 2022; Duan et al., 2021a).\n\nAnother notable tendency developed recently is the conditional transport (CT) theory (Zheng & Zhou, 2021a). It provides an efficient tool to measure the distance between two probability distributions and has been employed in numerous machine learning problems, such as domain adaptation, generative model, and document representation (Zheng et al., 2021; Tanwisuth et al., 2021; Wang et al., 2022). The CT distance is defined by the bidirectional (forward and backward) transport cost between the source and target distributions, allowing the two distributions not to share the same support. Moreover, the CT distance can be unbiasedly approximated with the discrete empirical distributions, making it amenable to stochastic gradient descent-based optimization. Wang et al. (2022) first introduced CT into topic modeling by minimizing the transport cost between the word and topic space, resulting in better topic quality and document representation. The similar idea is shared with recent optimal transport-based methods (Kusner et al., 2015; Huynh et al., 2020; Zhao et al., 2021). However, they all focus on single-layer topic discovery, ignoring multi-level topic dependencies.\n\nThis paper goes beyond hierarchical Bayesian models for topic structure learning and aims to discover topic hierarchies based on the conditional transport between distributions. To formulate topic structure learning as a transport problem, we first provide a hierarchical, distributional view of topic modeling, where each layer of a topic hierarchy learns a set of topics presented as embedding vectors. Moreover, the to-be-learned topics share the same word embedding space and are organized in a taxonomy where the upper-level topics are more general while the lower-level topics are more specific (Zhang et al., 2018). In detail, we view each document as an empirical distribution of word embeddings and consider that a document can also be presented by the topic embeddings (Wang et al., 2022) at each layer. Those hierarchical empirical distributions have different supports but share semantic consistency across topical levels. With this view, we propose to learn topic hierarchies with a Bidirectional Transport chain (BT-chain) where a document’s topic distributions in two adjacent layers are learned by being pushed close to each other in terms of the CT loss. This results in a more flexible and efficient method than VAI-based NTMs, while keeping the interpretability of Bayesian models.\n\nWith a different mechanism from previous hierarchical topic models, the proposed BT-chain is a straightforward and novel approach for topic structure learning, which can be flexibly integrated with deep neural networks. To achieve an efficient and end-to-end training algorithm, an upwarddownward optimizing strategy is developed carefully, which first warms up the empirical distributions by transporting the input via the bottom-to-top path, and then applies the backward layer-wise refinement by considering the bidirectional information stream from the Bayesian perspective. The main contributions in this paper are as follows: (1) We view the hierarchical topic modeling from a new perspective of multi-layer conditional transport, which facilities us to develop a novel bidirectional transport chain for topic structure learning. (2) To effectively and efficiently implement the proposed method, we propose an upward-downward training algorithm for BT-chain with proper amortizations and strategy. (3) We conduct extensive experiments on text corpora to show that our approach enjoys superior modeling accuracy and interpretability compared with the state-of-the-art hierarchical topic models. To extend the application of topic modeling, we also apply it on learning hierarchical visual topics from images, which shows interesting visualizations.\n\n2 BACKGROUND\n\ni=1 uiδxi, and q = (cid:80)m\n\nIn this section, we recap the background of transport distance between two discrete distributions. Let us consider two discrete probability distributions p and q ∈ P(X) on space X ∈ RH : p = (cid:80)n j=1 vjδyj , where xi and yj are two points in the arbitrary same space X. u ∈ Σn and v ∈ Σm, the simplex of Rn and Rm, denotes two probability values of the discrete states satisfying (cid:80)n j=1 vj = 1. δx refers to a point mass located at coordinate x ∈ RH .\n\ni=1 ui = 1 and (cid:80)m\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:80)\n\ni,j tijcij,\n\nTo measure a distance between such two discrete distributions, the optimal transport (OT) distance (Villani, 2009) between p and q is formulated as an optimization problem: OT(p, q) = s.t. T1m = u, TT 1n = v. The minimum of the transport plan minT∈Π(u,v) T ∈ Rn×m is taken over Π(u, v) with element tij, defined as the set of all possible joint probability measures π on the whole space Rn × Rm, with the marginals constraints. 1m is the m dimensional vector of ones. cij = c(xi, yj) ≥ 0 is the transport cost between the two points xi and yj defined by an arbitrary cost function c(·).\n\n>0\n\nMore recently, the demand on efficient computation and bidirectional asymmetric transport promote the development of conditional transport (CT) (Zheng & Zhou, 2021b), which can be applied to quantify the difference between discrete empirical distributions in various applications (Zheng et al., 2021; Tanwisuth et al., 2021; Wang et al., 2022). Specifically, given the above source and target distributions p and q, the CT cost is defined with a bidirectional distribution-to-distribution transport, where a forward CT measures the transport cost from the source to the target and a backward CT reverses the transport direction. Therefore, the CT problem can be defined as:\n\n( CT(p, q) = min ←− T\n\n−→ T\n\n(cid:88)\n\n−→ t ijcij +\n\n(cid:88)\n\n←− t jicji),\n\ni,j\n\nj,i\n\nwhere\n\npoint yj:\n\n−→ t ij in −→ t ij = ui\n\n−→ T acts as the transport probability (the navigator) from the source point xi to the target −→ T1m = u. Similarly, we have the reversed transport\n\nvj e−dψ (xi,yj )\n\nj′ ) , hence\n\nprobability:\n\n←− t ji = vj\n\n(cid:80)m\n\nj′=1 vj′ e\n\n−dψ (xi,y\n\nuie−dψ (yj ,xi) i′=1 ui′ e−dψ (yj ,x\n\n(cid:80)n\n\ni′ ) , and\n\n←− T1n = v. The distance function dψ(xi, yj)\n\nparameterized with ψ can be implemented by deep neural networks to measure the semantic similarity between two points, making CT amenable to stochastic gradient descent-based optimization.\n\n3 BT-CHAIN: BIDIRECTIONAL TRANSPORT CHAIN\n\nWe introduce the details of the proposed method, including a distributional view of multi-layer document representation, construction of BT-chain, and its upward-downward training algorithm.\n\n3.1 HIERARCHICAL, DISCRETE DISTRIBUTIONAL REPRESENTATIONS OF DOCUMENTS\n\nGiven a collection of corpora with J documents and V distinct tokens, conventional TMs usually represent the jth document as a V dimensional Bag-of-Word vector xj ∈ RV +, where xjv indicates the frequency of the v-th word in document j (Blei et al., 2003; Dieng et al., 2020). With xj and the word embedding matrix E ∈ RH×V , where H is the embedding dimension, we represent each document as an empirical distribution Pj in the word embedding space:\n\nPj =\n\nV (cid:88)\n\nv=1\n\nˆxjvδev , with ev ∈ RH ,\n\n(1)\n\nwhere ˆxj ∈ ΣV is the normalization of xj and ev is the embedding of the v-th word in the vocabulary, i.e., the v-th column of E. Notably that Pj in Eq. 1 not only contains the word co-occurrence patterns but also considers word semantic information, which has been proven useful for high-quality topic learning (Dieng et al., 2020; Meng et al., 2022) but is often ignored by conventional TMs.\n\nThis paper aims to learn L layers of topics in a topic hierarchy, each of which contains Kl topics for satisfying Kl+1 < Kl < Kl−1. It means that in the higher layers, there are fewer yet more abstract topics. Different from conventional TMs that assume a topic as the distribution over words, we view topics as the continuous vectors that lie in the same semantic space of words. Thus, each layer in BT-chain is associated with a set of topic embeddings {α(l) k=1, l = 1, ..., L. Together with the topic proportion of j-th document θ(l) j ∈ RKl + that denotes the topical weights over the Kl topics, we derive the topical distributional representation of the j-th document in layer l as:\n\nk }Kl\n\nQ(l)\n\nj =\n\nKl(cid:88)\n\nk=1\n\nˆθ(l) kj δα(l)\n\nk\n\n, with α(l)\n\nk ∈ RH ,\n\nl = 1, ...L,\n\n(2)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The directed graphical model of BT-chain. The adjacent layers are connected via the bidirectional CT, and L(l) of BT-chain links Q(l−1), Q(l), Q(l+1) via the upward-downward training strategy, guaranteeing their semantic consistency. The solid dots and triangles denote word and topic embeddings in the embedding space, and different color means different layers.\n\nj\n\nj / (cid:80)Kl\n\n:= θ(l)\n\nk=1 θ(l)\n\nwhere ˆθ(l) kj is the normalized topic proportions of document j at l-th layer that ensures the simplex constraint. We follow previous hierarchical topic models (Duan et al., 2021a) and employ an encoder network fω parameterized by ω to infer the multi-layer topic proportions: {θ(l) l=1 = fω(xj). fω is implemented by several stacked fully connected layers and the details can be found in Appendix. B.\n\nj }L\n\nTo summarize, the proposed BT-chain views the j-th document as a series of empirical distributions over the word and topic embeddings: {Q(l) j = xj, K0 = V , we have Q(0) j = Pj which denotes the word-level observed data. Note that those empirical distributions are all defined by the word/topic weights together with the corresponding semantic vectors. They are formulated in a similar form and share semantic consistency but with different topical supports.\n\nl=0. With α(0) = E, θ(0)\n\nj }L\n\n3.2 LINK {Q(l)\n\nj }L\n\nl=0 VIA BT-CHAIN\n\nj }L\n\nGiven the above distributional view of documents, we develop BT-chain to learn hierarchical document distributions {Q(l) l=1, where each layer in BT-chain is linked with its adjacent layers via the conditional transport and corresponds to learning of topic embeddings and topic proportions. Fig. 1 shows the overview of BT-chain. Specifically, in the topic hierarchy l = 1, ...L − 1, the current layer l is connected with its lower layer l − 1 and higher layer l + 1 (the uppermost layer L is only connected to the lower layer L − 1). Note that Q(l−1) capture the semantics of the same document, where Q(l−1) general concepts. Q(l) thus acts here as an intermediate node that integrates information from both directions and learns semantically smooth topics. To meet such properties, it is natural to push Q(l) to Q(l−1) as close as possible. It poses a question on how to define the closeness between two discrete distributions with different supports. Although recent studies have provided several transport-based alternatives to measure such distance (Cuturi, 2013; Yurochkin et al., 2019; Hu et al., 2021; Zhao et al., 2021), most of them focus on shallow transport problems and ignore the deep case. To this end, our proposed BT-chain derives a layer-wise distance with the recent CT technique and expresses the transport loss in l-th layer as:\n\nfocuses on more detailed semantics while Q(l+1)\n\nattends to more\n\nj , Q(l+1)\n\nand Q(l+1)\n\n, Q(l)\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nL(l) (cid:16)\n\nQ(l−1)\n\nj\n\n, Q(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\n= CT\n\n(cid:16)\n\nQ(l−1)\n\nj\n\n(cid:17)\n\n, Q(l)\n\nj\n\n(cid:16)\n\n+ CT\n\nQ(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\n.\n\n(3)\n\nj\n\nj\n\n. Thus for a hierarchy with L layers, we have {L(l)}L−1\n\nL(l) links the adjacent three layers via the two CT costs to guarantee the current layer Q(l) close to Q(l−1) and Q(l+1) l=1 , each of which strengthens the learning of its local Q(l) j by considering the neighborhood information and they together ensure the semantic consistency of the transport chain. Moreover, we minimize the above loss in terms of Q(l) k=1. The detailed parameterizations and implementations are shown in Section A of the Appendix and our designed training strategy for the transport chain is described in Section 3.3.\n\nj , which by definition consists of ˆθ(l)\n\nj and {α(l)\n\nsemantically\n\nk }Kl\n\nj\n\n4\n\nForward CTBackward CTUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Training algorithm for our proposed BT-chain.\n\nInput: documents, pre-trained word embeddings E, topic list {Kl}L Initialize: topic embeddings {α(l) for iter = 1,2,3,... do\n\nk=1,l=1, ω of the inference network.\n\nk }Kl,L\n\nl=1, hyperparameter β.\n\nSample a batch of B documents and get {Q(0) Get {θ(l) j }B,L j=1,l=1 by the encoder network fω # Upward warming-up for l = 1, 2, · · · , L do\n\nj }B\n\nj=1 with Eq. (1)\n\nFix Q(l−1)\n\n1:B and compute Lup = (cid:80)B\n\nj=1 CTreg (cid:16)\n\nQ(l−1)\n\nj\n\n, Q(l)\n\nj\n\n(cid:17)\n\nUpdate {α(l)\n\nk }Kl\n\nk=1 and ω with stochastic gradients of Lup\n\nend for # Downward refining for l = L − 1, · · · , 1 do 1:B and Q(l−1) Fix Q(l+1) Compute Ldown = (cid:80)B Update {α(l)\n\nk }Kl\n\n1:B\n\nend for\n\nend for\n\nj=1 CTreg (cid:16)\n\n+ CTreg (cid:16) k=1 and ω with stochastic gradients of Ldown\n\nQ(l−1)\n\n, Q(l)\n\n(cid:17)\n\nj\n\nj\n\nQ(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\nAs suggested in the previous works (Zhao et al., 2021), we add a regularization term based on the cross entropy for the CT loss in Eq. 3:\n\nCTreg (cid:16)\n\nQ(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\n(cid:16)\n\n= CT\n\nQ(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\n− β ˆθ(l)\n\nj\n\nlog\n\n(cid:16)\n\nΦ(l+1)θ(l+1)\n\nj\n\n(cid:17)\n\n,\n\n(4)\n\nk\n\n= Softmax(Ψ(l)α(l+1)\n\nwhere Φ(l+1) k ; β is the trade-off hyperparameter that balances the weight of the cross entropy regularization. Φ(l+1) acts as the “decoder” that decodes θ(l+1) to layer l and the cross entropy measures how close the decoded one to its “target” θ(l)\n\nj . In the training, we replace the CT distances in Eq. (3) with the regularized ones.\n\n); Ψ(l) ∈ RKl×H each row of which is α(l)\n\nk\n\nj\n\n3.3 UPWARD-DOWNWARD TRAINING ALGORITHM FOR BT-CHAIN\n\nk }Kl,L\n\nk=1,l=1 and the encoder network that infers the topic proportions {θ(l)}L\n\nGiven the training documents and pre-trained word embeddings E, we aim to learn topic embeddings {α(l) l=1. At layer l, the loss in Eq. (3) consists of two CT distances that connect layer l with the lower and higher layers. Simultaneously minimizing such two CT distances can be difficult, so we propose a layer-wise upward-downward training algorithm, which consists of two steps in each training iteration.\n\nUpward Warming-up In this step, we start with the learning of Q(1) embedding space, by minimizing CTreg (cid:16) is learned, we fix it and then learn Q(2) the raw information from the observed word layer to the uppermost topic layer Q(L)\n\nQ(0) by minimizing CTreg (cid:16)\n\nin word , i.e., the first CT distance in Eq. (3). After Q(1)\n\nj given the data Q(0)\n\n. In this way, we transport\n\nstep by step.\n\n, Q(1)\n\n, Q(2)\n\nQ(1)\n\n(cid:17)\n\n(cid:17)\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nj\n\nDownward refining In the upward training step, the lower layer l − 1 is fixed and treated as the data for its higher layer l, i.e., the message flows in the bottom-up direction. This does not consider how the higher layer l affects its lower layer l − 1. Therefore, after the upward step, we further introduce the downward step, where we start with the second uppermost layer L − 1. To learn Q(L−1) , we fix its two adjacent layers Q(L) and minimize L(l), i.e., both terms in Eq. (3). Similarly, we can update the distributions until we reach layer 1. In this way, the message of both layers l + 1 and l − 1 will flow to layer l. The training algorithm is outlined in Algorithm 1.\n\nand Q(L−2)\n\nj\n\nj\n\nj\n\n3.4 DISCUSSIONS\n\nTopic hierarchies learned by BT-chain BT-chain aims to learn a topic hierarchy where the topics in the higher layers are more general and abstract than those in the lower layers. How BT-chain achieves this can be interpreted from an information compression or abstraction perspective. Specifically, in\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nj\n\nto Q(0)\n\nto compress or abstract the information in Q(0)\n\nlies in a K1-dimensional topic embedding space, where\n\nlies in a K0-dimensional word embedding space, which is a sparse representation\n\nlayer 0, the data Q(0) of the semantics of document j. Q(1) K1 < K0. By pushing Q(1) Q(1) j with a denser representation. Similar things happen in the higher layers, as K1 > K2 > · · · > KL. Moreover, as all the topics are presented as embeddings, the topic correlations between the topics of two layers can be simply obtained by the distances between the topic embeddings, i.e., Φ(l) in Eq. (4). Therefore, the topic correlations in BT-chain can be interpreted in the same way as conventional hierarchical TMs (Blei et al., 2010; Zhou et al., 2016).\n\nj as close as possible in terms of the CT cost, BT-chain forces\n\nj\n\nj\n\nj\n\nBayesian Flavor of BT-chain Many deep topic models are implemented based on hierarchical Bayesian probabilistic models (Blei et al., 2010; Paisley et al., 2014; Gan et al., 2015; Henao et al., 2015; Zhou et al., 2016; Zhao et al., 2018), where the topics in the higher layers serve as the priors of the topics in the lower layers. In these Bayesian models, according to Bayes’ theorem, a topic’s posterior distribution consists of two terms: the data distribution (i.e., the word information in a document) and prior distribution (i.e., the topics in the higher layer). Although BT-chain is not a Bayesian model, it is interesting to interpret our method with a Bayesian flavor. For example, the value of Q(1) can be viewed as data and j\nprior, respectively. Instead of learning the model with Bayes’ theorem, we optimize BT-chain by minimizing the CT costs between the distributions, avoiding non-trivial Bayesian posterior inference. 4 RELATED WORK\n\nis learned according to Q(0)\n\n, where Q(0)\n\nand Q(2)\n\nand Q(2)\n\nj\n\nj\n\nj\n\nj\n\nTopic structure learning There is a surge of research interest in capturing the correlations among topics and generating topic structures. For example, there are many models based on hierarchical Bayesian prior such as the Dirichlet process (DP) and Chinese Restaurant Process (CRP), including hLDA (Griffiths et al., 2003), nCRP (Blei et al., 2010) and nHDP (Paisley et al., 2014). Li & McCallum (2006) propose the Pachinko Allocation Model (PAM) to model the co-occurrences of topics via a directed acyclic graph. hPAM (Mimno et al., 2007) is built on PAM and represents the topic hierarchical structure through the Dirichlet-multinomial parameters of the internal node distributions. More recently, various hierarchical extensions of Poisson factor analysis (PFA) (Zhou et al., 2012) have been proposed, including DPFA (Gan et al., 2015), DPFM (Henao et al., 2015), GBN (Zhou et al., 2016), and DirBN (Zhao et al., 2018). Zhang et al. (2018) develop Weibull hybrid autoencoding inference (WHAI) for GBN and Duan et al. (2021a) introduce word embedding into GBN and design the SawETM Connection (SC) to explore the relationship between topics. Although both WHAI and SawETM are the most related works to ours, which are Bayesian generative models and learned by maximizing the evidence lower bound (ELBO), our proposed model views deep topic modeling as a cross-layer transport problem.\n\nTopic models based on transport Using transport between distributions is a recent trend in topic modeling. Xu et al. (2018) propose distilled Wasserstein learning (DWL) where the distance between topics is achieved by the OT between their word distributions built on the embedding-based underlying distance. The OT based LDA (OTLDA) (Huynh et al., 2020) shares a similar idea but aims to minimize the OT distance between documents and topics in the vocabulary space. Yurochkin et al. (2019) view the documents as the distributions over topics, those topics themselves are modeled as the distribution over words and introduce the hierarchical OT as the meta-distance between documents. However, it is not a topic model but a method using topic models to compute document distances. Based on topic embeddings, Zhao et al. (2021) propose the Neural Sinkhorn Topic Model (NSTM) that optimizes the OT distance between the normalized BoW vector and the topic proportions, where the cost matrix is calculated by the word and topic embeddings. In Wang et al. (2022), WeTe views each document as a set of mixtures of word embeddings and a set of mixtures of topic embeddings and employs the conditional transport (CT) cost to quantify the difference between those two sets. Compared with the above shallow topic models, our proposed model aims to learn hierarchical document representations and topic structures.\n\n5 EXPERIMENTS\n\nWe in this section conduct comprehensive experiments on various datasets and compare the proposed BT-chain with different baseline methods to illustrate its superior performance.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Topic Coherence (TC) and Topic Diversity (TD) of hierarchical topic models on the four datasets.\n\nTable 1: Comparison of K-Means clustering purity (km-Purity) and NMI (km-NMI) for various methods. The number in brackets after the method indicates the total number of topic layers L. The best and second best scores of each dataset are highlighted in boldface and with an underline, respectively.\n\nMethod\n\nLDA-Gibbs DVAE ETM NSTM WeTe\n\nWHAI(1) SawETM(1) BT-chain(1)(Ours)\n\nWHAI(3) SawETM(3) BT-chain(3)(Ours)\n\nWS\n\n46.4±0.6 26.6±1.5 32.9±2.3 42.1±0.6 60.8±0.2\n\n50.8 ±0.2 34.4 ±0.4 61.2 ±0.2\n\n52.1 ±0.4 43.7 ±0.8 62.7±0.3\n\nkm-Purity(%) DP\n\nRCV2\n\n20NG(6)\n\nWS\n\nRCV2\n\nDP\n\nkm-NMI(%)\n\n52.4±0.4 52.6±1.2 50.2±0.6 53.8±1.0 62.9±0.5\n\n59.0 ±0.1 62.7 ±0.4 62.4 ±0.3\n\n60.5 ±0.2 64.2 ±0.5 63.7 ±0.3\n\n60.8 ±0.5 67.2 ±1.1 63.1 ±1.5 20.2 ±0.7 77.1 ±1.0\n\n65.1 ±0.1 65.9 ±0.2 77.5 ±0.3\n\n66.9 ±0.1 70.1 ±0.1 78.0 ±0.2\n\n59.2±0.6 64.6 ±1.0 62.6 ±2.2 62.6±1.2 68.5 ±0.2\n\n60.4 ±0.3 70.1 ±0.2 70.4 ±0.1\n\n60.8 ±0.1 72.1 ±0.3 73.3 ±0.1\n\n25.1±0.4 3.7 ±0.8 12.3±2.3 17.4 ±0.6 34.9±0.4\n\n25.8 ±0.2 10.7 ±0.6 35.4 ±0.4\n\n29.9 ±0.5 21.9 ±0.7 35.9 ±0.4\n\n38.2±0.5 31.3±0.9 30.3±1.0 36.8±0.3 42.8±0.3\n\n40.0 ±0.1 44.4 ±0.3 44.3 ±0.2\n\n40.0 ±0.2 45.2 ±0.4 44.6 ±0.3\n\n54.7 ±0.3 50.8 ±0.6 53.2 ±0.7 6.63±0.11 63.7±0.4\n\n53.0 ±0.3 54.0 ±0.3 66.2 ±0.2\n\n55.2 ±0.3 58.9 ±0.2 66.6 ±0.1\n\n20NG(6)\n\n32.4 ±0.4 29.8 ±0.6 29.3 ±1.5 31.1 ±1.2 36.3 ±0.2\n\n29.6 ±0.1 38.4 ±0.2 39.7 ±0.1\n\n30.2 ±0.1 42.7 ±0.1 43.9 ±0.1\n\nDatasets We conduct the experiments on four widely used benchmark corpus: 20 News Group (20NG), Web Snippets (WS) (Phan et al., 2008), DBpedia (DP) (Lehmann et al., 2015) and Reuters Corpus Volume 2 (RCV2) (Lewis et al., 2004). These datasets have very different characteristics in terms of document length, the number of documents, and vocabulary size, where DP and RCV2 are large-scale datasets, WS and DP consist of short documents. We pre-process those datasets following WeTe (Wang et al., 2022), e.g., we tokenize and clean text by excluding standard stop words and low-frequency words. The statistics of the datasets are summarized at Table. C. 1 of Appendix.\n\nBaselines and settings We compare BT-chain against conventional and advanced topic models, including (1) Single-layer baselines: Collapsed Gibbs Sampling LDA (LDA-Gibbs) as described in (Griffiths & Steyvers, 2004); Neural topic models, such as Dirichlet VAE (DVAE) (Burkhardt & Kramer, 2019) and embedded topic model (ETM) (Dieng et al., 2020), the former is a VAE based topic model and ETM is the first NTM that introduces word embeddings; NTMs with transport, Neural Sinkhorn Topic model (NSTM) (Zhao et al., 2021) and (WeTe) (Wang et al., 2022), both of them use transport distance as the loss function; (2) Hierarchical topic models: WHAI (Zhang et al., 2018) and SawETM (Duan et al., 2021a), they are hierarchical generative model based on Gamma belief network and are compared as our multi-layer baselines. Besides the above baselines, we also propose a variant of BT-chain that only uses the upward warming-up path to train the topic embeddings and the encoder, which we name BT-chain-U. For all baselines, we use the default parameters given with the source code or the best settings reported in their paper. For the models\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Topic coherence (TC) and topic diversity (TD) results of single-layer methods on four datasets.\n\nCorpus\n\nmetrics LDA-Gibbs DVAE\n\nETM NSTM WeTe BT-chain\n\n20NG(6)\n\nRCV2\n\nDP\n\nWS\n\nTC TD\n\nTC TD\n\nTC TD\n\nTC TD\n\n0.058 0.631\n\n0.097 0.641\n\n0.074 0.684\n\n0.071 0.760\n\n0.013 0.661\n\n-0.032 0.730\n\n0.065 0.645\n\n0.078 0.540\n\n0.019 0.551\n\n0.067 0.502\n\n0.053 0.583\n\n0.003 0.585\n\n0.103 0.647\n\n0.118 0.639\n\n0.104 0.715\n\n0.122 0.940\n\n0.101 0.591\n\n0.143 0.760\n\n0.108 0.763\n\n0.123 0.764\n\n0.103 0.664\n\n0.152 0.743\n\n0.125 0.790\n\n0.135 0.904\n\nthat work with word embeddings, including ETM, NSTM, WeTe, and our BT-chain, we use the pre-trained GloVe vectors for a fair comparison. We summarize those baselines at Table. C. 2.\n\nEvaluation metrics Notably, our work aims to mine high-quality topic structures, where evaluation metrics about topics are our main focus. Though perplexity is a common-used metric for generative topic model, recent studies suggest that it might not be an appropriate measure of the topic quality (Chang et al., 2009). Besides, CT based topic models (e.g., NSTM and WeTe) are learned by minimizing the transport cost instead of maximizing the log-likelihood, which is important to achieve better perplexity results. Therefore, we put more attention on following metrics. We use Topic Coherence (TC) and Topic Diversity (TD) to evaluate the learned topics from both the interpretability and diversity aspects. In detail, TC is the average Normalized Pointwise Mutual Information (NPMI) over the top 10 words of each topic which is highly correlated to human judgment. TD is calculated by the average percentage of unique words in the top 25 words of all topics. The higher the better for both TC and TD. Besides the topic quality, we also report Purity and Normalized Mutual Information (NMI) (Schütze et al., 2008) on clustering tasks to measure the performance of document representation. We first train the model on the training datasets and infer the topic proportions on the testing documents. Given the collection of topic proportions, we apply KMeans to predict the document label. The clustering number is set as 20 for 20NG, WS, and DP, while 52 for the RCV2 dataset. For 20NG dataset, we follow WeTe and use its six super categories at the first level as the ground truth, and denote it as 20NG(6).\n\nk=1\n\nˆθ(0)\n\nkj δα(0)\n\nj = (cid:80)K0\n\nImplementation details of BT-chain As discussed above, the proposed model takes the bag of features Q(0) as its input. For corpus, α(0) is the word embeddings in the vocabulary, ˆθ(0) is the normalized TF-IDF vector. We set L as 3 and the topic numbers at each layer for HTMs in the baselines and BT-chain K = [100, 64, 32] for a fair comparison. We set K = 100 for all the single-layer models. We use the Adam optimizer with learning rate 0.001, batch size 500. All experiments are performed on the same machine and our model is implemented in PyTorch.\n\nk\n\nj\n\nk\n\nQuantitative results For all the methods, we run the algorithm five times with different random seeds, and report the mean and standard deviation on the clustering task, while choosing the best results for topic quality. We fix the hyperparameter β = 50 for all the datasets and also report the effect of various settings on the document clustering in the Appendix. C.4. Table 1 reports the km-Purity and km-NMI on four corpora. The first group is the single-layer topic models, including LDA, NTMs, and transport based models, and the last two groups are hierarchical topic models (HTMs) with a single layer and three layers respectively. We find that i) In general, our proposed BT-chain outperforms the others in most cases, especially on the short WS and DP corpus. We attribute this to the empirical distribution representation with word/topic embeddings. ii) The deep models achieve better performance on both Purity and NMI than the shallow models. This is mainly because their hierarchical document representations can provide more comprehensive information which is one of our motivations. iii) Our single-layer BT-chain achieves better scores than WeTe, which shows the efficiency of introducing the word weights at layer 0, e.g., the normalized TF-IDF vector, rather than treating them according to the word frequency in WeTe.\n\nTo fully compare the qualities of learned topics, Table 2 and Figure 2 (where x-axis denotes the topic layers.) report the topic coherence (TC) and topic diversity (TD) for single-layer models and deep models respectively. Overall, we observe that the topics learned from our BT-chain has a better TC and TD, especially for the short corpora. This meets with the observation on the clustering task and\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Topic hierarchies learned from 20NG (left) and miniImageNet (right). The links between two adjacent layers are obtained according to the semantic similarities of topics (e.g. Φ in Eq. 4)\n\nagain proves the efficiency of the introduced word/topic semantics. Besides, our three-layer BT-chain outperforms all the other HTMs in most cases, which benefits from the new view of topic hierarchies discovery from the transport theory. The proposed BT-chain guarantees semantic consistency by minimizing the total transport cost via L(l), l = 1, ..., L − 1, resulting in high-quality topic structures. Finally, our BT-chain consistently achieves higher scores than BT-chain-U on all datasets at high layers. Note that BT-chain-U only uses the upward path to transport messages from word space to higher-level topic space. This demonstrates the efficiency of our downward training strategy.\n\nQualitative results To visualize the learned topic structures, we show the topic hierarchies on 20NG in Fig. 3 (left), where it can be observed that topics in higher layers are mixtures of semantically close topics in the lower levels. Recalling that BT-chain receives a set of feature embeddings as its input, this broadens the scope of application of BT-chain beyond text corpora. Here we conduct experiments on miniImageNet and visualize the learned concept hierarchy in Fig. 3 (right). MiniImageNet contains a total number of 100 classes with 600 images in each class, which are extracted from the ImageNet dataset (Russakovsky et al., 2015). To obtain the empirical distribution for image data, we adopt ConceptTransformer (Rigotti et al., 2022) to obtain the bag of features for images, where the j-th image is first divided evenly into N patches xj = (cid:80)N N δenj , and each patch aligns to M concepts via the cross-attention en = (cid:80)M m=1 αnmδcm, where en ∈ Rd is the patch embeddings, and αnm is the attention weights, C ∈ Rd×M is the concept embedding matrix. We average all patches and thus the final Q(0) n=1 αnm)δcm. Once trained on image data, we visualize the images assigned to the learned visual topics (Fig. 3 right), where we can observe interesting semantic relations between visual concepts. For example, the bottom layer contains the clear and concrete concepts, e.g., the regular textures, animal eyes, small objects in pure background, and so on. The second layer contains more complex semantics: the facial part of animal, and the small objects in textured background. It is interesting to see that the topic of warships in the second layer is closely related to the topic of oceans and the topic of missiles in the bottom layer. Similar observation can be found in the topic hierarchy of documents (Fig. 3 left), where we list the most related words of each topic. We report more qualitative results and time complexity analysis in Appendix C.3 and D.\n\nfor image can be expressed as: Q(0)\n\nj = (cid:80)M\n\nm=1( 1\n\n(cid:80)N\n\nn=1\n\nN\n\n1\n\nj\n\n6 CONCLUSION\n\nIn this paper, we present a bidirectional transport chain (BT-chain) for hierarchical representation. BT-chain views each document as a series of empirical distributions over word/topic embeddings, each of which consists of topic proportions and the corresponding embedding vectors. With the fact that those multi-layer representations share semantic consistency of the same document, we propose an effective upward-downward layer-wise training algorithm to learn topic hierarchies and document representations based on the conditional transport theory. Our framework can be viewed as a hierarchical extension to the recently developed topic models based on transport. Extensive experiments on text corpora show that our approach enjoys superior modeling accuracy and interpretability. Moreover, we also have conducted experiments on learning hierarchical visual topics from images, which demonstrate the adaptability and flexibility of our method.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFederico Bianchi, Silvia Terragni, and Dirk Hovy. Pre-training is a hot topic: Contextualized document embeddings improve topic coherence. Association for Computational Linguistics, 2021.\n\nDavid M Blei. Probabilistic topic models. Communications of the ACM, 55(4):77–84, 2012.\n\nDavid M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine\n\nLearning research, 3(Jan):993–1022, 2003.\n\nDavid M Blei, Thomas L Griffiths, and Michael I Jordan. The nested chinese restaurant process and Bayesian nonparametric inference of topic hierarchies. Journal of the ACM (JACM), 57(2):1–30, 2010.\n\nSophie Burkhardt and Stefan Kramer. Decoupling sparsity and smoothness in the dirichlet variational\n\nautoencoder topic model. J. Mach. Learn. Res., 20(131):1–27, 2019.\n\nJonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber, and David M Blei. Reading tea leaves: How humans interpret topic models. In Advances in neural information processing systems, pp. 288–296, 2009.\n\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, pp.\n\n2292–2300, 2013.\n\nAdji B Dieng, Francisco JR Ruiz, and David M Blei. Topic modeling in embedding spaces. Transac-\n\ntions of the Association for Computational Linguistics, 8:439–453, 2020.\n\nZhibin Duan, Dongsheng Wang, Bo Chen, Chaojie Wang, Wenchao Chen, Yewen Li, Jie Ren, and Mingyuan Zhou. Sawtooth factorial topic embeddings guided gamma belief network. In International Conference on Machine Learning, pp. 2903–2913. PMLR, 2021a.\n\nZhibin Duan, Yi Xu, Bo Chen, Chaojie Wang, Mingyuan Zhou, et al. Topicnet: Semantic graphguided topic discovery. Advances in Neural Information Processing Systems, 34:547–559, 2021b.\n\nZhe Gan, Changyou Chen, Ricardo Henao, David Carlson, and Lawrence Carin. Scalable deep Poisson factor analysis for topic modeling. In International Conference on Machine Learning, pp. 1823–1832. PMLR, 2015.\n\nThomas Griffiths, Michael Jordan, Joshua Tenenbaum, and David Blei. Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems, 16, 2003.\n\nThomas L Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the National\n\nacademy of Sciences, 101(suppl 1):5228–5235, 2004.\n\nJustin Grimmer. A bayesian hierarchical topic model for political texts: Measuring expressed agendas\n\nin senate press releases. Political Analysis, 18(1):1–35, 2010.\n\nDandan Guo, Bo Chen, Ruiying Lu, and Mingyuan Zhou. Recurrent hierarchical topic-guided rnn for language generation. In International conference on machine learning, pp. 3810–3821. PMLR, 2020.\n\nRicardo Henao, Zhe Gan, James Lu, and Lawrence Carin. Deep Poisson factor modeling. Advances\n\nin Neural Information Processing Systems, 28, 2015.\n\nMatthew Hoffman, Francis Bach, and David Blei. Online learning for latent dirichlet allocation.\n\nadvances in neural information processing systems, 23, 2010.\n\nYuqing Hu, Vincent Gripon, and Stéphane Pateux. Leveraging the feature distribution in transferbased few-shot learning. In Artificial Neural Networks and Machine Learning - ICANN 2021, 2021.\n\nViet Huynh, He Zhao, and Dinh Phung. Otlda: A geometry-aware optimal transport approach for topic modeling. Advances in Neural Information Processing Systems, 33:18573–18582, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. ICLR, 2013.\n\nMatt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document\n\ndistances. In International conference on machine learning, pp. 957–966. PMLR, 2015.\n\nDongha Lee, Jiaming Shen, Seongku Kang, Susik Yoon, Jiawei Han, and Hwanjo Yu. Taxocom: Topic taxonomy completion with hierarchical discovery of novel topic clusters. In WWW ’22: The ACM Web Conference 2022, Virtual Event, Lyon, France, April 25 - 29, 2022. ACM, 2022.\n\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, Sören Auer, et al. Dbpedia–a largescale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167–195, 2015.\n\nD. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text categorization\n\nresearch. Journal of Machine Learning Research, 5:361–397, 2004.\n\nWei Li and Andrew McCallum. Pachinko allocation: Dag-structured mixture models of topic In Proceedings of the 23rd international conference on Machine learning, pp.\n\ncorrelations. 577–584, 2006.\n\nYu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Chao Zhang, and Jiawei Han. Hierarchical topic mining via joint spherical tree and text embedding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 1908–1917, 2020.\n\nYu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, and Jiawei Han. Topic discovery via latent space\n\nclustering of pretrained language model representations. pp. 3143–3152. ACM, 2022.\n\nDavid Mimno, Wei Li, and Andrew McCallum. Mixtures of hierarchical topics with pachinko allocation. In Proceedings of the 24th international conference on Machine learning, pp. 633–640, 2007.\n\nJohn Paisley, Chong Wang, David M Blei, and Michael I Jordan. Nested hierarchical dirichlet processes. IEEE transactions on pattern analysis and machine intelligence, 37(2):256–270, 2014.\n\nXuan-Hieu Phan, Le-Minh Nguyen, and Susumu Horiguchi. Learning to classify short and sparse text & web with hidden topics from large-scale data collections. In Proceedings of the 17th international conference on World Wide Web, pp. 91–100, 2008.\n\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\n\napproximate inference in deep generative models. In ICML, pp. 1278–1286, 2014.\n\nMattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas Gschwind, and Paolo Scotton. Attentionbased interpretability with concept transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=kAa9eDS0RdO.\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211–252, 2015.\n\nTim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. In International conference on machine learning, pp. 1218–1226. PMLR, 2015.\n\nHinrich Schütze, Christopher D Manning, and Prabhakar Raghavan. Introduction to information\n\nretrieval, volume 39. Cambridge University Press Cambridge, 2008.\n\nKorawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. In NeurIPS 2021: Neural Information Processing Systems, Dec. 2021. (the first three authors contributed equally).\n\nYee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. Hierarchical dirichlet processes.\n\nJournal of the american statistical association, 101(476):1566–1581, 2006.\n\nCédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\n\nshot learning. Advances in neural information processing systems, 29, 2016.\n\nDongsheng Wang, Dandan Guo, He Zhao, Huangjie Zheng, Korawat Tanwisuth, Bo Chen, and Mingyuan Zhou. Representing mixtures of word embeddings with mixtures of topic embeddings. In ICLR 2022: International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=IYMuTbGzjFU.\n\nPengtao Xie, Diyi Yang, and Eric Xing.\n\nIncorporating word correlation knowledge into topic modeling. In Proceedings of the 2015 conference of the north American chapter of the association for computational linguistics: human language technologies, pp. 725–734, 2015.\n\nHongteng Xu, Wenlin Wang, Wei Liu, and Lawrence Carin. Distilled wasserstein learning for word embedding and topic modeling. Advances in Neural Information Processing Systems, 31, 2018.\n\nMikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, and Justin M Solomon. Hierarchical optimal transport for document representation. Advances in Neural Information Processing Systems, 32, 2019.\n\nHao Zhang, Bo Chen, Long Tian, Zhengjue Wang, and Mingyuan Zhou. Variational hetero-encoder randomized gans for joint image-text modeling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n\nHao Zhang, Bo Chen, Dandan Guo, and Mingyuan Zhou. WHAI: weibull hybrid autoencoding inference for deep topic modeling. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=S1cZsf-RW.\n\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text\n\nclassification. arXiv preprint arXiv:1509.01626, 2015.\n\nHe Zhao, Lan Du, Wray Buntine, and Mingyuan Zhou. Dirichlet belief networks for topic structure\n\nlearning. Advances in neural information processing systems, 31, 2018.\n\nHe Zhao, Dinh Phung, Viet Huynh, Trung Le, and Wray L. Buntine. Neural topic model via optimal transport. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum? id=Oos98K9Lv-k.\n\nHuangjie Zheng and Mingyuan Zhou. Act: Asymptotic conditional transport. Advances in Neural\n\nInformation Processing Systems, 2021a.\n\nHuangjie Zheng and Mingyuan Zhou. Exploiting chain rule and Bayes’ theorem to compare proba-\n\nbility distributions. Advances in Neural Information Processing Systems, 34, 2021b.\n\nHuangjie Zheng, Xu Chen, Jiangchao Yao, Hongxia Yang, Chunyuan Li, Ya Zhang, Hao Zhang, Ivor Tsang, Jingren Zhou, and Mingyuan Zhou. Contrastive conditional transport for representation learning. arXiv preprint arXiv:2105.03746, 2021.\n\nMingyuan Zhou, Lauren Hannah, David Dunson, and Lawrence Carin. Beta-negative binomial process and poisson factor analysis. In Artificial Intelligence and Statistics, pp. 1462–1471. PMLR, 2012.\n\nMingyuan Zhou, Yulai Cong, and Bo Chen. The Poisson gamma belief network. Advances in Neural\n\nInformation Processing Systems, 28, 2015.\n\nMingyuan Zhou, Yulai Cong, and Bo Chen. Augmentable gamma belief networks. The Journal of\n\nMachine Learning Research, 17(1):5656–5699, 2016.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA DETAILED DERIVATION OF CT IN BT-CHAIN\n\nj\n\nj , Q(l+1)\n\n) measures the transport cost of two discrete empirical distributions (Zheng & Zhou,\n\nCT(Q(l) 2021b), which consists of a forward CT that constructs a navigator to transport P (l) backward CT that reverse the transport direction:\n\nto P (l+1)\n\nj\n\nand a\n\nj\n\n(cid:16)\n\nQ(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\n(cid:16)\n\n−→ CT\n\n=\n\nQ(l)\n\nj , Q(l+1)\n\nj\n\n(cid:17)\n\n(cid:16)\n\n←− CT\n\n+\n\nQ(l+1)\n\nj\n\n, Q(l)\n\nj\n\n(cid:17)\n\n.\n\nCT\n\nSpecifically, the forward CT tries to minimize the expected transport cost from a set of Kl embeddings to a set of Kl+1 embeddings:\n\n−→ CT = E\n\nE\n\nα\n\n(l)\n\nk ∼Q\n\n(l) j\n\nα\n\n(l+1) k\n\n(cid:16)\n\nα\n\n∼π\n\n(l+1) k\n\n|α\n\n(l) k\n\n(cid:16)\n\n(cid:104) (cid:17)\n\nc\n\nα(l)\n\nk ,α(l+1)\n\nk\n\n(cid:17)(cid:105)\n\n(cid:16)\n\nα(l+1)\n\nk\n\n|α(l)\n\nk\n\n(cid:17)\n\n=\n\n, π\n\n(cid:16)\n\nˆθ(l+1)\n\nkj\n\ns\n\n(cid:17)\n\nα(l) k , α(l+1) (cid:16)\n\nk\n\n(cid:80)Kl+1 k′=1\n\nˆθ(l+1) k′j s\n\nα(l)\n\nk , α(l+1)\n\nk′\n\n(cid:17) ,\n\nwhere we specify transport cost c(·) of two embedding vectors with the inner product:\n\nT\n\nk\n\nk\n\nk\n\nα(l+1)\n\nk , α(l+1)\n\n) = exp(−α(l)\n\nc(α(l) ), e.g., the closer the two vectors are, the smaller the point-topoint transport cost. The navigator π(·) is the conditional probability of a given embedding α(l) being transported to embedding α(l+1) and the similarity score s(·) of the source and target topics. Therefore, it would be easier to transport α(l) describes a more popular topic that is semantically closer to α(l) α(l+1) space. We here also use the inner product to define s(α(l) the computational cost, although other possible choices.\n\nk to in the embedding α(l+1)\n\n, which is determined by both the topic weights θ(l+1)\n\nkl T\n) = exp(α(l)\n\nk , α(l+1)\n\n, if α(l+1)\n\n) to reduce\n\nkj\n\nk\n\nk\n\nk\n\nk\n\nk\n\nk\n\nk\n\nSimilarly, we can derive the backward CT by exchanging the source set and the target set:\n\n←− CT = E\n\nα\n\n(l+1) k\n\n∼Q\n\n(l+1) j\n\nE\n\nα\n\n(cid:16)\n\n(l)\n\nk ∼π\n\nα\n\n(l)\n\nk |α\n\n(l+1) k\n\n(cid:16)\n\n(cid:104) (cid:17)\n\nc\n\nα(l+1)\n\nk\n\n,α(l)\n\nk\n\n(cid:17)(cid:105)\n\n(cid:16)\n\nα(l)\n\nk |α(l+1)\n\nk\n\n(cid:17)\n\n=\n\n, π\n\n(cid:16)\n\nˆθ(l) kj s\n\nα(l+1) k\n(cid:16)\n\n(cid:17)\n\n, α(l)\n\nk\n\n(cid:80)Kl\n\nk′=1\n\nˆθ(l) k′js\n\nα(l+1)\n\nk\n\n, α(l) k′\n\n(cid:17) .\n\nB INFERENCE NETWORK FOR TOPIC PROPORTIONS\n\nTo guarantee the sparse property of the topic proportion θ(l) we leverage a Hierarchical Weibull Reparameterization Encoder (HWRE) to infer θ(l) information of the previous layers:\n\nj and as suggested in Wang et al. (2022), from the\n\nj\n\nθ(l)\n\nj ∼ Weibull\n\n(cid:16)\n\nk(l)\n\nj , λ(l)\n\nj\n\n(cid:17)\n\n, k(l)\n\nj , λ(l)\n\nj = SoftPlus\n\n(cid:16)\n\ng(h(l) j )\n\n(cid:17)\n\n, h(l)\n\nj = f\n\n(cid:16)\n\nh(l−1)\n\nj\n\n(cid:17)\n\n,\n\n(5)\n\nwhere h(0) j = xj, f and g are implemented with neural network, Weibull(k, l) is the Weibull distribution, which is reparameterizable (Zhang et al., 2018): Drawing s ∼ Weibull(k, l) is equivalent to maping s := l(− log(1 − ε))1/k, ε ∼ Uniform(0, 1). The SoftPlus applies log(1 + exp(·)) nonlinearity to ensure the positive Weibull shape and scale parameters.\n\nC DATASETS AND FURTHER EXPERIMENTS\n\nC.1 DATASETS\n\nOur experiments are conducted on four widely-used benchmark text datasets including 20 News Group (20NG), Web Snippets (WS) (Phan et al., 2008), DBpedia (DP) (Lehmann et al., 2015), Reuters Corpus Volume 2 (RCV2) (Lewis et al., 2004) and one additional image dataset miniImageNet (Vinyals et al., 2016). For the text datasets, WS and DP are short documents, RCV2 and DP are large scale datasets. Since RCV2 owns multiple labels for one document, we follow the previous work and remove documents containing multiple labels in the second level, resulting in 0.15M documents. For the image dataset, miniImageNet is a subset of ImageNet (Russakovsky et al., 2015) dataset.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n• 20NG1: 20 Newsgroups consists of newsgroups post including 18,846 articles. We remove stop words and words with document frequency less than 100 times. We also ignore documents that contain only one word from the corpus. We follow WeTe of (Wang et al., 2022) and use the 6 super-categories as 20NG’s ground truth and denote it as 20NG(6) in the clustering task.\n\n• WS: Web Snippets is a short corpus that contains 12,237 web search snippets with 8 categories. There are 10,052 words in the vocabulary and the average length of a snippet is 15.\n\n• DP2: DBpedia is extracted from Wikipedia pages. We follow the pre-processing process in Zhang et al. (2015), where the fields we used for this dataset contain title and abstract of each Wikipedia article.\n\n• RCV23: The original Reuters Corpus Volume 2 dataset consists of 804,414 documents. We here left documents that only contains single label at the second topic level, resulting in 0.15M documents totally, whose vocabulary size is 7282 and average length is 85.\n\n• miniImageNet 4: miniImageNet is a subset randomly sampled from ImageNet. In total, there are 100 classes with 600 samples of 84 × 84 color images per class. These 100 classes are divided into 64, 16 and 20 classes respectively for training, validation and testing.\n\nA summary of text corpora statistics is shown in Table C. 1.\n\nTable C. 1: Statistics of the datasets\n\nNumber of docs Vocabulary size(V)\n\naverage length Number of labels\n\n20NG DP WS RCV2\n\n18,864 449,665 12,337 150,737\n\n22,636 9,835 10,052 7,282\n\n108 22 15 85\n\n6 14 8\n52\n\nC.2 BASELINES\n\nWe summary the baseline methods at Table. C. 2\n\nModel LDA (Griffiths & Steyvers, 2004) DVAE (Burkhardt & Kramer, 2019) ETM (Dieng et al., 2020) NSTM (Zhao et al., 2021) WeTe (Wang et al., 2022) WHAI (Zhang et al., 2018) SawETM (Duan et al., 2021a) BT-chain\n\nVAE-based HTM transport-based Word embedding\n\n✔ ✔\n\n✔ ✔\n\n✔ ✔\n\n✔\n\n✔ ✔\n✔\n\n✔ ✔\n✔\n\n✔ ✔\n\nTable C. 2: Summaries of baseline models. HTM denotes the hierarchical topic model.\n\nC.3 MORE VISUALIZATIONS ON TEXT AND IMAGES\n\nSimilar to the main paper, we provide more visualizations of the learned topic hierarchies in Fig. C. 1 and Fig. C. 2.\n\n1http://qwone.com/ jason/20Newsgroups 2https://en.wikipedia.org/wiki/Main_Page 3https://trec.nist.gov/data/reuters/reuters.html 4https://github.com/yaoyao-liu/mini-imagenet-tools\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure C. 1: Topic hierarchies learned from miniImageNet (left) and DP (right).\n\nFigure C. 2: Topic hierarchies learned from miniImageNet (left) and WS (right).\n\nFigure C. 3: Clustering results with various β.\n\nC.4 HYPERPARAMETER SENSITIVITY\n\nWe fixed the hyperparameter β = 50.0 in all previous experiments for fair comparison. Here we report the document clustering results (Purity and NMI) with different hyperparameter settings on the four corpus in Fig. C. 3. Note that β controls the weight of the cross entropy in Eq.6 in the mainscript. The regularized loss degenerates to the CT loss when β = 0. From Fig. C. 3 we find that the regularization term helps the document representations, as there is a significant improvement from β = 0 to β = 10; Besides, one can get better results than those reported in our experiments by fine-tuning β for each dataset.\n\nD TIME COMPLEXITY ANALYSIS\n\nj , Q(l+1)\n\nThe core computational module in BT-chain is the regularized CT loss between two adjacent layers CTreg(Q(l) ). It mainly contains the cost matrix, which has a time complexity of O(KlKl+1) (Kl is the number of topics at layer l), the bidirectional transport plan π, which has a time complexity of O(6KlKl+1). We note that both of them have a linear complexity over the product of the number topics at two adjacent layers. For layer 0, we compute words within the target document rather than\n\nj\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nall words in vocabulary, e.g. K0 = Nj, Nj is the number of words in document j, Nj ≪ V . Thus the proposed BT-chain has an acceptable time complexity.\n\nE METRICS\n\nIn our experiment, we report Topic Coherence (TC) and Topic Diversity (TD) to evaluate the learned topics from both the interpretability and diversity aspects. Given a reference corpus, TC measures the semantic relevance in the most significant words (top 10 words in our case) of a topic, which is computed by the Normalized Pointwise Mutual Information (NPMI) over the selected words of each topic Dieng et al. (2020):\n\n(cid:20)\n\nf (wi, wj) =\n\nlog\n\n(cid:21)\n\np(wi, wj) p(wi)p(wj)\n\n/ [−logp(wi, wj)] ,\n\nwhere p(wi, wj) is the probability of words wi and wj co-occurring in a document and p(wi) is the marginal probability of word wi, and both of them are estimated with empirical counts. Those models owing higher topic coherence are more interpretable topic models. TD measures how diverse the learned topics are. We define TD with the percentage of the unique word in the top 25 words of all topics Zhao et al. (2021). TD that closes to 0 indicates redundant topics; that closes to 1 means more diverse topics.\n\nWe also report Purity and Normalized Mutual Information (NMI) (Schütze et al., 2008) on clustering tasks to measure the performance of document representation. To compute Purity, each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by the total number of documents:\n\nPurity(S, C) =\n\n1 N\n\n(cid:88)\n\nk\n\nmax j\n\n(sk\n\n(cid:92)\n\ncj)\n\nwhere S = {s1, ..., sK} is the set of clusters, and C = {c1, ..., cJ } is the set of classes. sk and cj are sets of documents in cluster k and class j, respectively. Higher purity means higher matching between S and C. NMI is related to the information theory, which can be calculated as:\n\nNMI(S, C) =\n\n2I(S, C) H(S) + H(C)\n\nwhere I(S, C) is the mutual information of S and C, H(S) is the entropy of S.\n\n16",
  "translations": [
    "# Summary Of The Paper\n\nThis paper proposes BAT-Chain, a hierarchical topic model leveraging multi-layer conditional transport (CT) theory to seize topic structures. The proposal is inspired by previous application of CT to the single-layer topic modeling.  The authors conduct extensive experiments in both textual and visual settings to showcase the merits of their method.\n\n# Strength And Weaknesses\n\n**Strengths**: \n\nThe authors introduce a novel solution for discovering topic hierarchy. The paper is generally smooth to follow, and the experiments can prove the prowess of conditional transport for hierarchical topic modeling.\n\n**Weaknesses**: \n\nFrom my perspective, there exist several drawbacks in the Introduction, Methodology, and Experiment sections:\n\n(1) In the introduction section, I cannot see clearly why reparameterization and KL divergence hurt AVI, especially if AVI and CT are adapted to hierarchical topic modeling, and what merits does CT provide to address the advantages of AVI?\n\n(2) The motivation of CT for hierarchical topic modeling is quite obscure. The introduction only explained that CT was applied because previous single-layer topic modeling had not continued to explore its usage for hierarchical circumstances.\n\n(3) The methodology section mentioned that ``A TOPIC in layer 1 is expected to capture more general information than A WORD in layer 0``. Comparing a topic with a word sounds unnatural to me. Such interpretation somehow makes the hypothetical basis for CT topic hierarchy less persuasive. \n\n(4) The desiderata of the proposed framework is that the upper layers will capture more general topics than the lower layers. Nevertheless, Figure 3 does not clearly indicate the aforementioned relationship. In contrast, the illustrated upper topics have more tendency to display semantic proximity to the lower ones, instead of generalization relation. Additionally, the topics appear to be incoherent as well. For example, why do topic 52 and 24 comprise ``philosopher`` and ``medical``, respectively?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n(1) Clarity: The paper is cogent and polished. In detail, the delineation in the methodology section is sound and intuitive. I can grasp the mechanism of the bidirectional training strategy and the conditional transport objective.\n\n(2) Originality/Novelty: The paper presents a novel framework for hierarchical topic modeling based upon the optimal transport theory basis. \n\n(3) Quality/Significance: The experimental section indicates the improvement over recent SOTA baselines on diverse datasets. This demonstrates the efficiency and robustness of the proposed method.\n\n(4) Reproducibility: The submission provides source code for re-implementation, which would help the reproducibility.\n\n# Summary Of The Review\n\nA novel approach is proposed to tackle the multi-layer topic discovery problem. The elucidation of the CT framework is rational and comprehensible. However, there is some degree of vagueness in the motivation and hypothetical discussion. Moreover, the qualitative analysis is also not convincing, which drives me to my overall judgement.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"BT-CHAIN: Bidirectional Transport Chain for Topic Hierarchies Discovery\" proposes a novel method for discovering multi-level topic structures in documents through the Bidirectional Transport Chain (BT-chain). This method addresses the limitations of traditional topic modeling, which often assumes topic independence, by employing conditional transport theory to model hierarchical topic distributions effectively. The BT-chain methodology involves representing documents as empirical distributions of word embeddings, optimizing these representations using an upward-downward training algorithm. Experimental results demonstrate that BT-chain outperforms state-of-the-art methods in terms of modeling accuracy, interpretability, and adaptability, particularly in learning hierarchical visual topics from images.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to hierarchical topic modeling, leveraging conditional transport theory to enhance the representation and learning of topic structures. The upward-downward training strategy is a significant contribution, allowing for systematic learning of topic hierarchies. Empirical validation across multiple datasets showcases the effectiveness of the proposed method, particularly its high topic coherence and diversity. However, potential weaknesses include the complexity of the BT-chain methodology, which may pose challenges for reproducibility and practical implementation. Additionally, the paper could benefit from a more detailed discussion of the computational efficiency of the proposed model compared to existing methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The theoretical foundations are sufficiently explained, and the experiments are comprehensive, providing clear insights into the effectiveness of the BT-chain. However, certain aspects, such as the implementation details and hyperparameter tuning, are somewhat sparse, which may hinder reproducibility. The novelty of the approach is significant, introducing a new perspective on topic modeling that combines optimal transport with hierarchical learning.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative method for hierarchical topic modeling using the BT-chain framework. While the contributions are significant and empirically validated, some aspects related to reproducibility and implementation details could be improved.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents BT-CHAIN, a novel Bidirectional Transport Chain model aimed at addressing the limitations of traditional topic models, particularly their inability to capture hierarchical structures among topics. The methodology involves representing documents as empirical distributions of word embeddings and utilizing conditional transport theory to facilitate a bidirectional transport process between hierarchical topic layers. The findings demonstrate that BT-CHAIN outperforms several state-of-the-art topic models in terms of topic coherence, diversity, and clustering performance across multiple datasets, including text and image data.\n\n# Strength And Weaknesses\nThe strengths of this paper include its innovative approach to hierarchical topic modeling, which leverages transport theory, thus representing a significant step forward in the field. The model's flexibility allows it to be applied across different data types, and it maintains interpretability in its topic structures, which is often a challenge with complex models. Additionally, the robust performance on various metrics adds to the model's credibility. However, the paper also presents weaknesses such as the increased complexity of the upward-downward training strategy, which may hinder implementation and tuning. Furthermore, the model's computational demands could be high, particularly for large datasets, and its reliance on high-quality pre-trained word embeddings poses a potential limitation on its effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its objectives, methodology, and findings, making it accessible to the reader. The quality of the experiments is commendable, with a thorough comparison against multiple baselines, which strengthens the validity of the results. The novelty of the approach is significant, as it introduces a new perspective to topic modeling. However, reproducibility may be a concern due to the complexity of the model and its dependence on the quality of word embeddings, which could vary across contexts.\n\n# Summary Of The Review\nOverall, the BT-CHAIN paper makes a notable contribution to the field of topic modeling by introducing a sophisticated, transport-based approach that captures hierarchical structures effectively. While it demonstrates robust empirical performance and interpretability, considerations related to its complexity and dependency on embeddings need to be addressed for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces BT-CHAIN, a novel bidirectional transport chain framework designed for discovering hierarchical topic structures in documents. The methodology leverages conditional transport theory to learn hierarchical representations of documents as empirical distributions, optimizing topic embeddings through an upward-downward training strategy. Experimental results demonstrate that BT-CHAIN outperforms existing topic modeling approaches—such as LDA, DVAE, and ETM—in terms of modeling accuracy, interpretability, and the ability to generate meaningful topic hierarchies. Additionally, the framework shows versatility by being applicable to visual topic learning from images.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to hierarchical topic modeling, which addresses the limitations of traditional models that treat topics as independent. The use of conditional transport theory is a significant contribution, allowing for efficient measurement of distances between distributions. The experimental evaluation is robust, with a variety of datasets and comprehensive metrics that demonstrate the effectiveness of the proposed model. However, a potential weakness is the lack of a thorough comparison of BT-CHAIN with more recent developments in the field, which may limit the contextual understanding of its advantages. Additionally, the paper could benefit from providing more detailed insights into the computational complexity involved in training BT-CHAIN.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it easy to follow the proposed methodology and findings. The clarity of the explanations regarding the upward-downward optimization strategy and the mathematical formulations of optimal transport and conditional transport is commendable. The quality of the experiments appears high, with appropriate datasets and evaluation metrics used. However, reproducibility could be enhanced by providing more implementation details, especially regarding hyperparameter settings and the training process. The novelty of the approach is evident, as it extends beyond existing transport-based models by introducing a multi-level hierarchical framework.\n\n# Summary Of The Review\nBT-CHAIN presents a novel and effective method for hierarchical topic modeling, significantly advancing the state-of-the-art by utilizing conditional transport theory. While the contributions are robust and well-supported by empirical evidence, the paper would benefit from a deeper discussion of computational considerations and comparisons with newer models in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"BT-CHAIN: Bidirectional Transport Chain for Topic Hierarchies Discovery\" introduces a novel method for discovering multi-level topic structures in textual data using a bidirectional transport chain (BT-chain). The methodology simplifies the process of hierarchical topic modeling by bypassing complex Bayesian posterior inference, thus enhancing efficiency and flexibility. Empirical results demonstrate that BT-chain outperforms existing hierarchical topic models in terms of modeling accuracy and interpretability. Additionally, the paper presents an innovative upward-downward training strategy for optimizing hierarchical topic embeddings and showcases the method's adaptability to both textual and visual data.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach to hierarchical topic modeling, which enhances interpretability and efficiency. The introduction of the upward-downward training strategy is a notable contribution that improves the learning of hierarchical embeddings. Extensive experimental results validate the method's superiority over state-of-the-art models. However, the paper also presents several limitations; specifically, the complexity of hierarchical structures may hinder practical applications, particularly with large datasets. The reliance on empirical distributions over word embeddings raises concerns about potential biases, and the experiments are primarily confined to specific datasets, which could limit generalizability. Moreover, the performance sensitivity to hyperparameters complicates tuning for diverse applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the methodology and results. However, the theoretical aspects of the transport approach may be challenging for practitioners to grasp without additional clarification. The novelty of the proposed method is significant, as it introduces a fresh perspective on topic modeling. While the experiments are comprehensive, the reproducibility could be enhanced with more detailed implementation guidance, particularly for the visual topic modeling aspects, which are less explored.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to hierarchical topic modeling through the BT-chain method. While it demonstrates strong empirical performance and theoretical contributions, concerns regarding practical applicability, generalizability, and clarity of the theoretical framework warrant consideration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach called Bidirectional Transport Chain (BT-CHAIN) for discovering hierarchical topic structures in text corpora. This method diverges from traditional topic modeling techniques by capturing interrelations among topics through a multi-layer transport mechanism rather than assuming topic independence. Key contributions include a hierarchical topic representation using empirical distributions over word embeddings, a bidirectional transport framework, an upward-downward training strategy for efficient model optimization, and extensive experimental validation demonstrating superior performance in modeling accuracy and interpretability when compared to existing hierarchical topic models.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative methodology and the comprehensive experimental validation that showcases the effectiveness of BT-CHAIN over traditional models. The hierarchical topic representation and the bidirectional transport mechanism enable a richer understanding of topic relationships, which is a significant improvement over existing methods. However, the paper could benefit from a more detailed discussion of the limitations of the proposed method, particularly concerning scalability and application to diverse data types. Additionally, while the experiments validate the approach, further insights into the interpretability of the model results could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The writing quality is high, making complex concepts accessible. The novelty of the approach is evident, particularly in the introduction of the bidirectional transport mechanism and the upward-downward training strategy. However, the reproducibility of the results could be improved by providing more details on the experimental setup, datasets used, and hyperparameter settings. Including the source code or instructions for replication would further strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field of hierarchical topic modeling through the introduction of BT-CHAIN, which effectively captures topic interrelations and enhances semantic coherence. While the methodology is innovative, further discussion on limitations and reproducibility would be beneficial.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework titled \"BT-CHAIN,\" which introduces a bidirectional transport mechanism aimed at enhancing adversarial training in deep learning models. The authors reframe adversarial training as a transport problem, where the objective is to minimize the cost between original and adversarial examples. They propose a hierarchical representation of adversarial examples and an upward-downward training strategy that allows for enhanced robustness through a series of transformations. Empirical results demonstrate that BT-CHAIN significantly outperforms traditional adversarial training methods in terms of robustness, accuracy, and interpretability.\n\n# Strength And Weaknesses\n**Strengths:**\n- The introduction of transport theory into adversarial training is a fresh and creative approach that could pave the way for further research and innovation in the field.\n- The hierarchical representation of adversarial examples enhances interpretability, allowing a deeper understanding of the model's behavior under adversarial conditions.\n- The extensive empirical validation supports the claims made by the authors and showcases the effectiveness of the proposed method across various datasets.\n\n**Weaknesses:**\n- The complexity of the BT-CHAIN framework may hinder its practical implementation, especially in real-time applications where speed is critical.\n- The paper lacks an in-depth exploration of the scalability of the method when applied to larger datasets and more complex model architectures, which could limit its applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear presentation of concepts and methodologies. The quality of the writing is high, and the logical flow of ideas is maintained throughout. The novelty of integrating transport theory into adversarial training is significant and offers new insights into the problem. However, the complexity of the proposed method may affect reproducibility, particularly for practitioners unfamiliar with the nuances of transport theory. Overall, the clarity and quality of the work are commendable, though practical reproducibility may pose challenges.\n\n# Summary Of The Review\nOverall, the paper presents an innovative approach to adversarial training through the BT-CHAIN framework, demonstrating both technical and empirical advancements over traditional methods. While the complexity of the framework raises some concerns regarding real-world applicability and scalability, the novel contributions and strong empirical results warrant consideration for acceptance at ICLR.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"BT-CHAIN: Bidirectional Transport Chain for Topic Hierarchies Discovery\" proposes a novel method for hierarchical topic modeling called the BT-chain. It introduces a multi-layer conditional transport framework and a new upward-downward training strategy, claiming to redefine the landscape of topic discovery. The authors assert that their extensive experiments demonstrate superior modeling accuracy and interpretability compared to existing methods, suggesting that BT-chain is a definitive solution for hierarchical topic modeling.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious claims and the introduction of new methodologies, such as the upward-downward training strategy. However, the weaknesses are pronounced, as the contributions are greatly overstated and lack sufficient empirical support. The methodology, while innovative in its presentation, does not adequately acknowledge or build on existing approaches, leading to a one-sided view of the problem. The claims regarding the elimination of prior methods and the supposed revolutionary nature of the findings are not sufficiently substantiated by the experimental results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by the exaggerated claims and convoluted descriptions of the methodology. Although the authors present visualizations that aim to enhance understanding, the overall narrative lacks the rigor necessary for reproducibility. The novelty is present in the proposed methods, but it is overshadowed by the lack of comparative contextualization with existing methods. As a result, the quality of the work appears diminished due to the inflated claims and insufficient empirical grounding.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to hierarchical topic modeling, but the claims made are inflated and inadequately supported by empirical evidence. The methodology, while novel, is presented as a complete overhaul of existing paradigms without sufficient acknowledgment of prior work. As a result, the findings may not have the transformative impact suggested by the authors.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces BT-CHAIN, a bidirectional transport chain framework designed for discovering multi-level topic hierarchies from word embeddings. It addresses the limitations of traditional topic modeling methods, which typically assume topic independence and often face challenges in posterior inference. The methodology involves representing documents as empirical distributions over word embeddings and employing an upward-downward optimizing strategy to refine hierarchical topic representations. Experimental results demonstrate that BT-CHAIN achieves significant improvements in clustering purity, normalized mutual information, topic coherence, and diversity compared to existing models, particularly on complex datasets and short documents.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to topic modeling, which improves both accuracy and interpretability over traditional Bayesian models. The upward-downward training strategy effectively captures hierarchical relationships among topics, enhancing semantic coherence. Additionally, the flexibility of BT-CHAIN allows its application across various domains, including visual topic modeling. However, a potential weakness is the reliance on the empirical distributions of word embeddings, which may limit the method's effectiveness in datasets with less structured or noisy text. The paper could also benefit from a more comprehensive analysis of the limitations and potential biases inherent in the proposed framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear explanation of the methodology and results. The writing is generally clear, although some technical details may require additional elaboration for readers unfamiliar with the underlying concepts. The novelty of the approach is significant, as it combines elements of transport theory with topic modeling in a way that addresses existing challenges. While the experimental results are promising, the reproducibility could be strengthened by providing more details on the implementation and datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of topic modeling with the BT-CHAIN framework, demonstrating notable improvements in performance and interpretability. Despite some minor concerns regarding clarity and reproducibility, the innovative methodology and robust experimental results suggest that BT-CHAIN could have a meaningful impact on future research in this domain.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel hierarchical topic modeling framework that challenges the traditional assumption of topic independence. It introduces a transport-based approach and employs conditional transport theory to measure distances between distributions, representing documents as empirical distributions over word embeddings. The authors claim that their model enhances interpretability and performance, providing a more nuanced understanding of topic structures compared to existing Bayesian models and flat topic hierarchies.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to hierarchical topic modeling, proposing a framework that integrates deep topic structures and transport-based metrics. However, it raises several critical concerns. For instance, the assumption of inherent topic dependency requires further empirical validation. Additionally, while the authors argue that their model simplifies posterior inference, it is unclear if the proposed model genuinely alleviates the complexities associated with Bayesian inference or merely shifts them. The interpretability of embeddings is another area of concern, as the high-dimensional nature of these spaces complicates claims of enhanced interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written but could benefit from clearer explanations of certain methodological choices, particularly regarding the empirical distribution representations and the conditional transport theory. The novelty of the hierarchical perspective on topic modeling is commendable; however, the lack of comparative analysis with existing metrics raises questions about its robustness and reproducibility. The empirical validation of claims regarding interpretability and performance is also lacking, which diminishes the overall clarity and quality of the contributions.\n\n# Summary Of The Review\nOverall, the paper proposes an interesting hierarchical framework for topic modeling that challenges established assumptions. However, it requires more empirical validation and clearer explanations of its methodologies to substantiate its claims regarding interpretability and performance.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents BT-CHAIN, a novel bidirectional transport chain method designed to uncover hierarchical topic structures in text analysis. It addresses the limitations of existing Bayesian and transport-based models by enabling the learning of multi-level topic structures without complex posterior inference. The methodology employs conditional transport theory to measure distances between probability distributions, facilitating the connection of adjacent topic layers through an upward-downward training strategy. Experimental results demonstrate that BT-CHAIN outperforms traditional topic modeling approaches in terms of clustering purity, topic coherence, and diversity, showcasing its applicability to both textual and visual topics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to hierarchical topic modeling, leveraging conditional transport theory in a way that avoids the pitfalls of traditional Bayesian models. This not only enhances the model's interpretability but also improves its empirical performance across several metrics. However, a potential weakness is the complexity of the methodology, which may pose challenges for practitioners looking to apply the model without a deep understanding of conditional transport theory. Furthermore, while the empirical results are promising, additional experiments on a wider variety of datasets could strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The explanations of the conditional transport theory and the BT-CHAIN method are coherent, which aids in understanding the innovative aspects of the work. However, the depth of the technical details may hinder reproducibility for those less familiar with the underlying concepts. The quality of the experiments is robust, but the paper could benefit from providing more details on implementation and hyperparameter settings to further assist future researchers in replicating the results.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of topic modeling by introducing BT-CHAIN, which effectively addresses limitations of existing methods while providing superior empirical results. Although the methodology is complex, the clarity of the presentation and the robustness of the experimental findings position this work as a valuable addition to the literature.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel method for enhancing the robustness of neural networks against adversarial attacks, focusing on the integration of a new regularization technique into the training process. The authors propose a framework that combines adversarial training with a unique loss function designed to minimize the vulnerability of the model while maintaining its overall performance on clean data. Through extensive experiments on benchmark datasets, the findings demonstrate that the proposed method significantly improves model robustness without sacrificing classification accuracy.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Original Contribution:** The introduction of a new regularization technique that directly targets adversarial robustness is a noteworthy advancement in the field.\n2. **Theoretical Insights:** The authors provide a strong theoretical rationale for their approach, explaining how the proposed loss function interacts with standard training objectives.\n3. **Comprehensive Experiments:** The empirical results are robust, showcasing improvements over existing methods across various datasets and adversarial settings.\n4. **Clarity of Presentation:** The paper is well-written, with a clear structure that facilitates understanding of complex concepts.\n\n**Weaknesses:**\n1. **Comparative Analysis:** The evaluation against a limited set of baseline methods may not fully capture the state-of-the-art landscape, potentially underestimating the contribution of recent advancements.\n2. **Experimental Details:** Some aspects of the experimental setup, such as hyperparameter selection and the specifics of adversarial examples used, are not sufficiently detailed for full reproducibility.\n3. **Limitations Discussion:** The authors do not adequately discuss the potential limitations of their approach, particularly regarding its scalability and performance on unseen adversarial attacks.\n4. **Generalizability:** There is insufficient exploration of how the proposed method would perform on different architectures or real-world applications beyond the tested benchmarks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, making complex ideas accessible. However, the lack of detailed experimental setup and hyperparameter descriptions raises concerns about reproducibility. The novelty in the proposed method is significant, and its potential impact on enhancing adversarial robustness is notable, though the comparative analysis could have been more comprehensive.\n\n# Summary Of The Review\nOverall, the paper introduces a promising new regularization technique aimed at improving the adversarial robustness of neural networks, supported by strong theoretical foundations and compelling experimental results. Addressing the outlined weaknesses, particularly in terms of comparative analysis and reproducibility, would further strengthen the paper's contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces BT-chain, a novel method for hierarchical topic modeling that addresses the limitations of traditional topic models like LDA, particularly their assumptions of topic independence. By employing a bidirectional transport chain framework, the authors propose a mechanism for learning multi-level topic structures through word embeddings and a novel upward-downward optimization strategy. The findings demonstrate that BT-chain significantly enhances modeling accuracy and interpretability for both text and visual topics compared to existing approaches.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to hierarchical topic modeling, which leverages conditional transport theory to effectively capture multi-level topic dependencies. The upward-downward optimization strategy is a notable contribution that refines the hierarchical representations, showing promise in improving interpretability. However, the paper could benefit from a more extensive discussion on the limitations of the proposed method, particularly regarding its computational complexity and scalability in large datasets. Additionally, while the empirical results are compelling, the paper could strengthen its claims with a more detailed comparison against a broader range of baseline models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and results. The novelty of the BT-chain framework is evident, particularly in its integration of transport theory into topic modeling. However, while the methodological details are sufficient for understanding the approach, additional information regarding implementation and potential challenges could enhance reproducibility. The empirical results are well presented, though further clarity on the experimental setup would be beneficial for readers aiming to replicate the study.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in hierarchical topic modeling through the BT-chain framework, showcasing improved accuracy and interpretability over existing models. While the methodology is innovative and the results promising, the paper could address limitations more thoroughly and provide additional details to enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Bidirectional Transport Chain (BT-chain), a novel method for hierarchical topic modeling that overcomes the limitations of traditional Bayesian models, which often struggle with inference and flexibility in large datasets. The methodology conceptualizes documents as empirical distributions over word embeddings, employing a bidirectional transport mechanism to capture multi-level topic structures. The experiments demonstrate that BT-chain outperforms existing models in terms of topic coherence, diversity, and interpretability, particularly on short document datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to topic modeling, which effectively integrates concepts from transport theory with deep learning techniques. The upward-downward optimization strategy enhances the model's interpretability and flexibility, allowing for meaningful hierarchical topic representations. However, the paper could benefit from a more thorough discussion of the computational efficiency of the BT-chain compared to traditional models, especially in very large datasets. Additionally, while empirical results are promising, more extensive validation across diverse domain-specific datasets would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The use of empirical distributions and the explanation of the bidirectional transport mechanism are particularly well-presented. The reproducibility of the results could be improved by providing more details on the implementation and hyperparameter settings used during experiments. Overall, the quality of writing is high, with clear definitions and explanations of key concepts.\n\n# Summary Of The Review\nThe BT-chain presents a compelling advancement in hierarchical topic modeling, offering significant improvements in interpretability and performance over traditional models. Despite its strengths, the paper would be enhanced by a deeper exploration of efficiency and reproducibility aspects. Overall, it represents a noteworthy contribution to the field of topic modeling.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel method for discovering hierarchical topic structures using word embeddings, termed the Bidirectional Transport Chain (BT-chain). The authors propose a multi-layered architecture that leverages conditional transport for topic learning, employing an upward-downward optimizing strategy to refine the hierarchical representations. The findings indicate that the BT-chain outperforms existing topic models, such as LDA and ETM, in terms of modeling accuracy and interpretability, while also showcasing its adaptability to visual topics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to hierarchical topic modeling, which addresses the limitations of traditional methods in capturing nuanced topic relationships. The comprehensive experimentation across various benchmark datasets demonstrates the effectiveness of the proposed method. However, the complexity of the model may pose challenges for understanding and implementation, particularly for practitioners unfamiliar with the underlying principles. Additionally, the paper could benefit from a more thorough analysis of hyperparameter sensitivity and its effects on the proposed method's performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, although some complex ideas may require further simplification for broader accessibility. The novelty is high, particularly with the introduction of the BT-chain and its unique methodology for topic modeling. The quality of the empirical results appears robust, as indicated by the comparative performance against established baselines. However, reproducibility could be improved with more detailed descriptions of the experimental setup and hyperparameter choices.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of hierarchical topic modeling through the introduction of the BT-chain. While the methodology is innovative and the empirical results are promising, there are areas that require further clarification to enhance understanding and reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the BT-chain framework, a novel approach for discovering hierarchical topic structures in textual data by utilizing a bidirectional transport mechanism. It addresses limitations in existing Bayesian frameworks for hierarchical learning and transport-based models by conceptualizing documents as empirical distributions and employing an upward-downward optimization strategy informed by conditional transport theory. Empirical evaluations across multiple datasets demonstrate that BT-chain outperforms traditional models in terms of clustering purity and topic coherence, ultimately enhancing both modeling accuracy and interpretability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of conditional transport theory to hierarchical topic modeling, which effectively captures multi-level topic dependencies and enhances semantic consistency. The upward-downward training paradigm allows for a structured refinement process that is theoretically grounded. However, the paper could benefit from more extensive comparisons with a wider range of baseline models and an in-depth analysis of the computational efficiency of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings, making it accessible to readers familiar with topic modeling and transport theory. The theoretical contributions are significant, providing a fresh perspective in the area of hierarchical topic modeling. Nevertheless, the reproducibility could be improved by providing more details on the implementation specifics and hyperparameter choices used during experimentation.\n\n# Summary Of The Review\nOverall, the BT-chain framework presents a compelling advancement in hierarchical topic modeling through its innovative use of conditional transport theory. While the paper demonstrates strong empirical results and clear methodology, it could enhance reproducibility and provide broader comparative analyses to further establish its significance in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new hierarchical topic modeling approach called BT-chain, which leverages conditional transport theory to improve modeling accuracy and interpretability in text and image data. The authors propose a complex upward-downward optimization strategy and present empirical results that claim significant improvements over existing state-of-the-art models. However, the methodology's complexity and the limited scope of experiments raise questions about its practicality and generalizability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to innovate within the topic modeling domain by utilizing conditional transport theory, which could provide a fresh perspective on hierarchical modeling. However, the paper has notable weaknesses, including an overly complex methodology that may hinder reproducibility, insufficient theoretical grounding for its claims, and a narrow experimental scope that limits the assessment of the method's performance across diverse datasets. Additionally, the authors make performance claims that appear overstated, and the lack of a thorough evaluation against a broad range of baseline methods makes it difficult to ascertain the actual benefits of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from clarity issues due to the heavy use of technical jargon and convoluted formulations, which could alienate readers unfamiliar with the subject. The novelty of the approach is evident, but the lack of a solid theoretical foundation and insufficient discussion of limitations raises concerns about the reliability and applicability of the findings. Reproducibility is also a major concern, given the complexity of the methodology and the absence of robustness testing across different data and parameter settings.\n\n# Summary Of The Review\nOverall, while the paper presents a novel approach to hierarchical topic modeling, its contributions are undermined by significant weaknesses, including an overly complex methodology, limited experimental validation, and insufficient theoretical support. The claims of improved performance and interpretability are not convincingly substantiated, leading to doubts about the method's reliability and broader applicability.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents BT-CHAIN, a novel bidirectional transport chain designed for discovering hierarchical topic structures in document collections. The methodology leverages a cutting-edge upward-downward optimizing strategy that allows for efficient end-to-end training of topic models. The findings indicate that BT-CHAIN significantly outperforms state-of-the-art hierarchical topic models in both accuracy and interpretability across various datasets, while also demonstrating versatility in extending to image data for hierarchical visual topic discovery.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to hierarchical topic modeling, as well as its empirical success in outperforming existing models. The paper provides a robust framework that enhances the clarity and usability of topic models through effective visualization of learned hierarchies. However, potential weaknesses could include a lack of detailed analysis on the scalability of the method and how it handles very large datasets or highly diverse content types, which may affect its applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, effectively communicating the innovative aspects of the BT-CHAIN methodology. The quality of the empirical results is high, showcasing improved performance through rigorous experimentation. The novelty of the approach is significant, as it introduces a new paradigm in topic modeling. Reproducibility appears to be addressed, though the paper could further enhance this aspect by providing more details on the implementation and data used.\n\n# Summary Of The Review\nOverall, BT-CHAIN represents a notable advancement in the field of topic modeling, characterized by its innovative techniques and strong empirical validation. Its flexibility across different data types and potential applications bodes well for future research and practical implementations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces BT-CHAIN, a novel framework for hierarchical topic modeling that leverages Conditional Transport Theory (CT) to address the limitations of traditional Bayesian methods. By conceptualizing document representation as a series of empirical distributions and utilizing a bidirectional transport cost, the methodology enables the learning of topic embeddings that reflect semantic relationships across various hierarchical levels. The findings suggest that BT-CHAIN enhances the interpretability of topic structures while maintaining semantic coherence, promising advancements in both theoretical understanding and practical applications of topic modeling.\n\n# Strength And Weaknesses\nThe BT-CHAIN framework presents significant contributions by integrating Conditional Transport Theory into topic modeling, allowing for efficient computation without the burdensome posterior inference present in traditional Bayesian models. Additionally, its novel upward-downward optimizing strategy facilitates nuanced learning of topic hierarchies, which is a clear advancement over existing methodologies. However, the paper lacks extensive empirical validation, as the proposed framework may require further testing across diverse datasets to substantiate its claimed advantages fully. Furthermore, the theoretical implications, while promising, may necessitate clearer articulation in terms of practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, providing a clear logical flow from theoretical foundations to methodological innovations. The novelty of the approach lies in its integration of Conditional Transport Theory with topic modeling, which is articulated effectively through the lens of hierarchical structures. However, the reproducibility of the proposed method may be a concern due to the complexities associated with its implementation and the absence of a comprehensive evaluation on standard benchmark datasets.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to hierarchical topic modeling through the BT-CHAIN framework, highlighting the use of Conditional Transport Theory as a key advancement. While the theoretical contributions are significant, the lack of empirical validation raises questions about the practical utility of the proposed methodology.\n\n# Correctness\n4/5 – The theoretical framework and proposed methodology appear sound, though further empirical validation is necessary to confirm its effectiveness in practice.\n\n# Technical Novelty And Significance\n5/5 – The integration of Conditional Transport Theory into topic modeling is a notable advancement that could reshape how hierarchical structures are understood and implemented.\n\n# Empirical Novelty And Significance\n3/5 – While the theoretical contributions are strong, the empirical evaluation is lacking, limiting the immediate applicability and significance of the findings in real-world scenarios.",
    "# Summary Of The Paper\nThe paper titled \"BT-CHAIN: Bidirectional Transport Chain for Topic Hierarchies Discovery\" introduces a novel approach for topic modeling that leverages conditional transport theory to represent documents as empirical distributions over word and topic embeddings. The proposed BT-chain architecture employs a hierarchical structure where topic embeddings and document representations are learned through an upward-downward optimization strategy. The authors evaluate their model on several datasets, reporting improvements in topic coherence, topic diversity, clustering purity, and normalized mutual information, demonstrating the model's effectiveness in discovering multi-layer topic structures.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative use of conditional transport theory, which is a fresh perspective in the context of topic modeling. The hierarchical structure of the model and the dual optimization strategy contribute to its robustness and efficiency. However, the paper's weaknesses include a limited focus on broader contributions to the field of topic modeling beyond the implementation details. Additionally, the absence of explicit code and data availability may hinder reproducibility and accessibility for further research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with detailed descriptions of the model architecture, training algorithm, and hyperparameters. The mathematical formulations are presented logically, which aids in understanding the proposed methodology. However, the emphasis on technical implementation may detract from its novelty and broader significance in topic modeling research. The reproducibility of the results is questionable due to the lack of accessible code and datasets, which are only referenced.\n\n# Summary Of The Review\nOverall, the BT-CHAIN paper presents a promising and innovative approach to topic modeling through its use of conditional transport theory and a hierarchical learning structure. While the methodology is sound and the results are compelling, the paper could benefit from a stronger emphasis on its contributions to the field and improved reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the BT-chain method for hierarchical topic modeling, positioning it as a superior alternative to existing Bayesian models. The authors leverage conditional transport theory to propose this novel approach, claiming advantages in modeling accuracy and interpretability. The methodology is validated through extensive empirical experiments, which purportedly show that BT-chain outperforms traditional models on specific metrics, particularly in the context of both text and visual topics.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including the innovative application of transport theory and a unique upward-downward training strategy. However, the claims of superiority over traditional Bayesian models lack sufficient comparative metrics, and the paper does not adequately engage with existing literature, particularly in terms of the strengths and weaknesses of the baselines used. Additionally, the authors' emphasis on the novelty of their approach can be seen as overreaching, given the prior work in transport-based methodologies.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, some claims related to interpretability and accuracy are poorly substantiated, leading to potential confusion regarding the BT-chain's actual advantages. The novelty of the approach is presented compellingly, yet the lack of rigorous comparative analysis raises questions about the reproducibility of the results. The empirical findings could benefit from a more nuanced discussion of their implications within the broader context of existing research.\n\n# Summary Of The Review\nIn summary, the paper presents a novel method for hierarchical topic modeling that is grounded in conditional transport theory, but it suffers from insufficient comparative analysis and justification of its claims. While the empirical results suggest some advantages, the lack of context and critical discussion of baseline models limits the overall impact of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"BT-CHAIN: Bidirectional Transport Chain for Topic Hierarchies Discovery\" introduces a novel framework for discovering topic hierarchies using a bidirectional transport chain method. The methodology involves non-trivial posterior inference and an upward-downward-optimizing strategy to enhance topic representation and improve the interpretability of topic models. The findings demonstrate that the proposed approach outperforms existing models on various benchmark datasets, revealing more coherent and meaningful topic hierarchies.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by presenting a new algorithm that effectively captures the hierarchical structure of topics. The empirical results are compelling, showing superior performance on established metrics. However, the paper suffers from some clarity issues, particularly in defining key terms and notations, which may hinder its accessibility for readers unfamiliar with the topic. Additionally, the inconsistency in equation formatting and notation could confuse the audience, impacting the overall quality.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper's main ideas are innovative and relevant, the clarity of presentation is lacking in several areas. Definitions of key concepts, such as \"meta knowledge\" and \"Empirical distributions,\" are not sufficiently detailed, leading to potential confusion. The methodological descriptions are occasionally vague, which could affect reproducibility. Moreover, the inconsistent formatting of equations and references undermines the overall quality of the presentation. Improvements in these areas would enhance the reader's understanding and facilitate reproduction of the results.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to discovering topic hierarchies, showcasing solid empirical results. However, clarity and consistency issues in presentation detract from its overall impact and accessibility. Addressing these concerns would significantly enhance the paper's readability and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to hierarchical topic modeling using a bidirectional transport chain, aimed at improving the accuracy and interpretability of topic representation. The methodology emphasizes a structured approach to modeling topics across different layers, leveraging meta-knowledge for enhanced topic representation. The findings suggest that the proposed method yields improved results compared to existing techniques, although the paper does not explore the integration of temporal dynamics or multi-modal data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to hierarchical topic modeling, demonstrating improved accuracy and interpretability. However, several weaknesses were identified: the lack of exploration of temporal dynamics in topic evolution, limited evaluation on diverse datasets, and insufficient discussion on scaling the method for large datasets. Additionally, the paper does not adequately address potential limitations, such as assumptions regarding topic independence and the handling of noisy data, which are critical for real-world applications. The interpretability of the model could be further enhanced through comparative analyses with other techniques.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting its contributions and methodology clearly. However, certain areas could benefit from more detailed explanations, particularly concerning the model's interpretability and potential limitations. While the proposed method is novel, its reproducibility may be hindered by the lack of comprehensive details on hyperparameter tuning and the performance implications of various choices. Overall, the quality is high, but certain aspects require further elaboration to enhance clarity and reproducibility.\n\n# Summary Of The Review\nThis paper introduces a promising method for hierarchical topic modeling that improves accuracy and interpretability. However, it falls short in addressing important aspects such as temporal dynamics, scalability, and robustness against noisy data, which limits its practical applicability. A broader evaluation across diverse datasets and contexts is recommended to strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the BT-chain methodology, a novel approach to hierarchical topic modeling that leverages Conditional Transport (CT) theory to optimize document representations. The BT-chain interprets document representations as hierarchical distributions of topic embeddings, employing an upward-downward optimization strategy. The methodology is rigorously evaluated across multiple datasets (20NG, WS, DP, RCV2) using metrics such as K-Means Clustering Purity, Normalized Mutual Information, Topic Coherence, and Topic Diversity. The findings demonstrate that BT-chain outperforms baseline methods, particularly in scenarios involving shorter documents, highlighting its effectiveness in enhancing topic coherence and representation quality.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative use of CT theory to address the limitations of traditional Bayesian models in topic modeling, specifically regarding posterior inference challenges. The upward-downward optimization strategy and the incorporation of a transport loss function with regularization are significant contributions that enhance both interpretability and performance. However, a notable weakness is the sensitivity of the model to hyperparameter settings, which could complicate its application in practice. Additionally, while the empirical results are promising, further exploration of the model's robustness and scalability would strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of the experimental design is commendable, with extensive evaluations across diverse datasets. The novelty of the approach is significant, particularly in its application of transport theory to topic modeling. However, the reproducibility of results may be hindered by the hyperparameter sensitivity, which demands careful tuning for optimal performance. Overall, the clarity and organization of the paper facilitate understanding, though practical implementation details could be elaborated upon.\n\n# Summary Of The Review\nThe BT-chain methodology presents a compelling advancement in hierarchical topic modeling, successfully integrating Conditional Transport theory to improve document representation and topic coherence. While the approach is innovative and yields robust empirical results, its sensitivity to hyperparameter tuning poses challenges for practical application. Overall, the paper makes a notable contribution to the field, meriting further exploration and validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel hierarchical topic modeling approach termed BT-chain, which leverages a transport-based mechanism to enhance topic representation and learning. The methodology involves constructing hierarchical structures to capture relationships between topics more effectively, with the aim of improving interpretability and performance over traditional models. The findings demonstrate that BT-chain achieves competitive results on benchmark datasets, suggesting its potential as an effective tool for topic modeling.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative transport-based approach, which offers a fresh perspective on hierarchical topic modeling. However, several weaknesses undermine its contributions. Notably, the paper does not address the integration of additional meta-knowledge sources that could improve topic understanding. Furthermore, the lack of discussion regarding the model's performance on shorter documents or datasets with minimal co-occurrence raises concerns about its practical applicability. Additionally, the scalability of BT-chain for large datasets is not adequately considered, nor is there a comprehensive comparison with existing state-of-the-art models. The exploration of hyperparameter sensitivity and the generalizability of the model across different domains is also insufficient, limiting the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly; however, the novelty is somewhat diminished by the lack of comparisons with existing models that utilize more complex prior structures. The reproducibility is hindered by the absence of detailed discussions on hyperparameter settings and the impact of training strategies on model performance. Overall, while the paper demonstrates a solid understanding of the topic, it could benefit from deeper explorations of the implications of its findings and methods.\n\n# Summary Of The Review\nOverall, the paper introduces a promising approach to hierarchical topic modeling with the BT-chain, showcasing its potential through empirical results. Nonetheless, it is marred by notable limitations in assessing its applicability, scalability, and comparative performance, which could weaken its overall impact. Future work should aim to address these gaps to fully realize the model's potential.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents BT-CHAIN, a novel topic modeling framework that aims to discover hierarchical topic structures through a bidirectional transport chain approach. The authors propose a training algorithm that utilizes a conditional transport (CT) theory, which they claim allows for bypassing posterior inference typically required in traditional Bayesian models. Experimental results suggest that BT-CHAIN achieves superior modeling accuracy and interpretability compared to standard baselines, although the metrics to support these claims are not clearly defined.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to provide a structured framework for hierarchical topic modeling, which is a relevant area of research. However, the weaknesses are significant; the contributions appear to be largely derivative, relying heavily on existing concepts without substantial innovation. Key aspects of the methodology, such as the use of word embeddings and layer-wise refinement, are commonplace in the field and do not demonstrate originality. Moreover, the paper lacks clarity in defining the actual improvements in performance and interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, the novelty of the proposed method is questionable. The use of jargon and vague claims detracts from its clarity. Reproducibility may be a concern as the experimental metrics used to validate the claims of superior performance are not sufficiently detailed, making it difficult for others to replicate the results.\n\n# Summary Of The Review\nOverall, the paper presents a framework that attempts to address hierarchical topic modeling but fails to deliver significant novel contributions. It rehashes existing methodologies without providing substantial empirical evidence to support its claims, resulting in a work that feels more like a repackaging of known techniques rather than a groundbreaking advancement.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a bidirectional transport chain (BT-chain) designed for hierarchical topic discovery, addressing the limitations of existing Bayesian models that rely heavily on complex posterior inference methods. The authors utilize conditional transport theory for measuring distances between distributions and propose an encoder network to infer multi-layer topic proportions. The findings demonstrate that BT-chain effectively improves visual topic learning from images, though the authors also acknowledge the need for further exploration into enhancing model interpretability and efficiency.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to hierarchical topic modeling and the application of transport theory, which presents a novel framework for distance measurement between distributions. The empirical results underscore the model's performance in visual topic learning, showcasing its potential applicability in real-world scenarios. However, the paper could benefit from considering alternative hierarchical structures for better interpretability and computational efficiency. Additionally, the reliance on empirical distributions over word embeddings may limit the semantic richness of the learned topics, which could be addressed by integrating external knowledge sources.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The experimental design is robust, though it would benefit from a broader evaluation across diverse datasets. The novelty of the BT-chain is significant in the context of hierarchical modeling, but the reproducibility of results could be enhanced by providing additional details on the implementation and hyperparameter settings. Furthermore, the proposal for hybrid inference techniques and multimodal topic modeling presents exciting avenues for future research.\n\n# Summary Of The Review\nOverall, the paper presents a promising contribution to hierarchical topic modeling through the introduction of the BT-chain. While the methodology is innovative and the empirical results are compelling, there are opportunities for improvement in interpretability and the incorporation of external knowledge to enhance topic richness. The suggestions for future research could further solidify the significance of the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the BT-chain model, a novel approach to hierarchical topic modeling that incorporates word/topic embeddings and an upward-downward training strategy. The authors claim that BT-chain significantly outperforms existing methods across various datasets, as evidenced by superior clustering metrics such as K-Means clustering purity (km-Purity) and normalized mutual information (km-NMI), as well as topic quality metrics like topic coherence (TC) and topic diversity (TD). Results indicate that the multi-layer representation of BT-chain enhances both model accuracy and interpretability in topic hierarchies, particularly on short document datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive benchmarking against a range of existing methods, demonstrating the effectiveness of BT-chain in both single-layer and multi-layer configurations. The quantitative results are compelling, particularly the model's performance on short datasets like Web Snippets and DBpedia, which is often a challenging context for topic modeling. However, the paper could benefit from a more detailed discussion regarding the limitations of the BT-chain model and the potential challenges in generalizing the findings to larger datasets or different domains.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and provides a clear presentation of the methodology and results. The visualizations effectively illustrate the learned topic hierarchies, adding to the clarity of the findings. The novelty of the approach lies in its integration of empirical distribution representation and word/topic embeddings, which enhance semantic understanding. While the authors provide sufficient details for reproducing the experiments, additional information on hyperparameter settings and training procedures would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the BT-chain model represents a significant advancement in hierarchical topic modeling, demonstrating strong empirical performance and interpretability. The paper is clear and well-structured, although further elaboration on limitations and reproducibility details would strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to hierarchical modeling using Bayesian techniques, specifically focusing on conditional transport theory. The methodology involves the development of a new algorithm that enhances model performance on complex data structures. The findings demonstrate significant improvements in predictive accuracy compared to existing methods, particularly in scenarios with high-dimensional data and intricate dependencies.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to hierarchical modeling and its empirical results, which indicate a clear advantage over traditional methods. The proposed algorithm is well-justified and demonstrates robustness across various datasets. However, the paper suffers from clarity issues, particularly in the presentation of complex concepts and technical jargon. The lack of clear examples and illustrations makes it challenging for readers to fully grasp the methodology and its implications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by dense language and complex sentence structures that may alienate readers not deeply familiar with the terminology. Quality could be improved through proofreading and simplifying explanations of key concepts. The novelty of the work is substantial, introducing a new algorithm that significantly advances the field. Reproducibility is somewhat compromised by insufficient details on the implementation and lack of explicit benchmarking against existing techniques.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in hierarchical Bayesian modeling through a novel algorithm. However, the clarity of presentation and accessibility of the content could be greatly improved to enhance reader engagement and comprehension.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.576200066981061,
    -1.8243083336095813,
    -1.7410626989406692,
    -1.8600956885037416,
    -1.8990951303851789,
    -1.66560243254036,
    -1.630058715103889,
    -2.0458333216628817,
    -1.7460035341058266,
    -1.9195234731391997,
    -1.7322845174708403,
    -1.3934239363456618,
    -1.7127906596181013,
    -1.7101165160340424,
    -1.73774124123678,
    -1.9105716934761365,
    -1.6745785055497249,
    -1.8734542540487262,
    -1.8925918670942812,
    -1.8790398378492363,
    -2.0094867654223414,
    -1.9151252703812431,
    -1.7051928968090808,
    -1.7642478916193356,
    -1.8316632068411962,
    -1.9187777337126362,
    -2.0698005391883125,
    -1.8426986611813028,
    -1.8556640189387834
  ],
  "logp_cond": [
    [
      0.0,
      -2.3264545797547536,
      -2.325059257634851,
      -2.3082757751391276,
      -2.3516657832316636,
      -2.3574866596422144,
      -2.3930235903673176,
      -2.339888475105212,
      -2.3560415333545897,
      -2.3914204069977822,
      -2.3098739309454785,
      -2.436891982917647,
      -2.333044423681408,
      -2.3716315744591148,
      -2.3439218983045875,
      -2.345121192204743,
      -2.344532528112778,
      -2.363668225965238,
      -2.3461395676986854,
      -2.3277901389696587,
      -2.321744317529829,
      -2.376715547485692,
      -2.37614883877425,
      -2.348418206568196,
      -2.3693374552666735,
      -2.340138530229239,
      -2.3339758620757327,
      -2.3615064805795383,
      -2.3959130865376794
    ],
    [
      -1.3829068114213336,
      0.0,
      -1.2418105677139653,
      -1.12793039624544,
      -1.1430842035186268,
      -1.2781860807796694,
      -1.3589387050161428,
      -1.2234785246443656,
      -1.2344017154109719,
      -1.435396178666536,
      -1.2782899146105773,
      -1.490391052678581,
      -1.2350252271757454,
      -1.3300559004155208,
      -1.2487794655355628,
      -1.2500650685232433,
      -1.3632282789569043,
      -1.297051877641347,
      -1.3084289306742563,
      -1.1925571543848186,
      -1.2814075538604293,
      -1.2785976577389055,
      -1.4232131328875666,
      -1.300587218710498,
      -1.4004347854819925,
      -1.3307754032678258,
      -1.305863261920006,
      -1.302975918012146,
      -1.4419870445276874
    ],
    [
      -1.3643096892448907,
      -1.2187091930256917,
      0.0,
      -1.24872577286926,
      -1.2724018854456973,
      -1.3390093450721707,
      -1.3641201864716919,
      -1.2895802982614326,
      -1.248948315362062,
      -1.390778884076122,
      -1.2703816660188634,
      -1.4766171853012642,
      -1.30290648396937,
      -1.3591028756911085,
      -1.3080067478991044,
      -1.2842019561028672,
      -1.3400775893911154,
      -1.2889122936112947,
      -1.3541925664147534,
      -1.2489471032124673,
      -1.2962573918464126,
      -1.3270347819071957,
      -1.376625362005519,
      -1.2855913320392909,
      -1.351733675365924,
      -1.3125169212855716,
      -1.3217975154620938,
      -1.310879637373189,
      -1.4164678543449774
    ],
    [
      -1.4784344582144378,
      -1.2701505306556427,
      -1.3634084103167816,
      0.0,
      -1.3916804536499616,
      -1.4396810592876073,
      -1.469177123995093,
      -1.3638670008558433,
      -1.3327782633313383,
      -1.4619558144867548,
      -1.3279474693026867,
      -1.5691439725340943,
      -1.3448036797601721,
      -1.4105066835229707,
      -1.3530298510707668,
      -1.3637623041564133,
      -1.4320086290496332,
      -1.3924779333875565,
      -1.4164466859798908,
      -1.335903086577899,
      -1.3898612503594567,
      -1.411147217029436,
      -1.4836426552975224,
      -1.4020855783948822,
      -1.4860988477852155,
      -1.395975690574221,
      -1.3958797087740697,
      -1.4686605692442285,
      -1.512166352142282
    ],
    [
      -1.5427571559953832,
      -1.3270180359165988,
      -1.4333496099169776,
      -1.422883314221443,
      0.0,
      -1.4462059930513576,
      -1.4767520310394298,
      -1.405318201585038,
      -1.365588811490942,
      -1.545730962157228,
      -1.471676225847282,
      -1.6126020635183238,
      -1.432453449248172,
      -1.4506376199233402,
      -1.4188319218271854,
      -1.4724244159578048,
      -1.4819573261042898,
      -1.4967725327237382,
      -1.4463924581694,
      -1.4056929681673058,
      -1.4690119174744514,
      -1.4377389920955197,
      -1.5898617557000978,
      -1.5342051453043846,
      -1.5697499204781509,
      -1.526880250095674,
      -1.4792562902105038,
      -1.5053528552589428,
      -1.6450745625427274
    ],
    [
      -1.3420883103978412,
      -1.105496744519816,
      -1.1766725952015789,
      -1.1421683297272462,
      -1.1581080584643901,
      0.0,
      -1.26320749783952,
      -1.1287078271986484,
      -1.1337414421543197,
      -1.3003842774557972,
      -1.2034651106885486,
      -1.3610862969652264,
      -1.1722485945913936,
      -1.2270072566931103,
      -1.1853947612166695,
      -1.131814939744327,
      -1.2681546501450727,
      -1.1880265735762896,
      -1.2259983017543559,
      -1.1887924070692284,
      -1.1906329576519576,
      -1.2163876477287916,
      -1.2675449384712278,
      -1.273541888564186,
      -1.283872497440042,
      -1.1992553567852966,
      -1.24358036915656,
      -1.213356232984815,
      -1.2891255699079465
    ],
    [
      -1.391891124535373,
      -1.2886113870217366,
      -1.268288922596592,
      -1.3246647329701473,
      -1.2858562241381912,
      -1.3010340299184775,
      0.0,
      -1.2693740569291638,
      -1.309863311717899,
      -1.3923748807119691,
      -1.3083113111859874,
      -1.3647829864312937,
      -1.317725929284769,
      -1.350178045915138,
      -1.3236216933507665,
      -1.3372732808983794,
      -1.3356175373965133,
      -1.297218703594508,
      -1.3064476290003895,
      -1.313850502669882,
      -1.3025907258127032,
      -1.3259757279015092,
      -1.343771439444764,
      -1.3108045575454303,
      -1.3356063975186003,
      -1.3221483535561087,
      -1.3467967001842358,
      -1.3142984589204463,
      -1.3907433860456786
    ],
    [
      -1.6672133237504987,
      -1.4753190486100407,
      -1.5637987859513736,
      -1.519149559538413,
      -1.5191605369818946,
      -1.5517525524507945,
      -1.6333925259659758,
      0.0,
      -1.581206166763767,
      -1.739095144022519,
      -1.5765689520945096,
      -1.7790909939527109,
      -1.5517072950898325,
      -1.6496801323418568,
      -1.5560419365063038,
      -1.5712191715890804,
      -1.6025850685654461,
      -1.5638197527969302,
      -1.6622612955516882,
      -1.4832740285243684,
      -1.524352981606802,
      -1.551962035895582,
      -1.6480004438267841,
      -1.5919985370556455,
      -1.6206165400406356,
      -1.5601179968958077,
      -1.5939758022108463,
      -1.5663576924008098,
      -1.7400062503112517
    ],
    [
      -1.3756650094434109,
      -1.2084026978533102,
      -1.2390498844936797,
      -1.2004822071905676,
      -1.198261215148066,
      -1.2619013050609806,
      -1.3323524741087147,
      -1.2641457714571493,
      0.0,
      -1.3830619708444354,
      -1.2037721867019182,
      -1.4825751656221127,
      -1.2447156818361234,
      -1.2921445087285888,
      -1.2714817235734814,
      -1.2468140081876118,
      -1.348553125873203,
      -1.2735466923378624,
      -1.2847236493204424,
      -1.2557531063507108,
      -1.2547825222821478,
      -1.2753550505946074,
      -1.3480910903225933,
      -1.2717528541021799,
      -1.3533161142691528,
      -1.284095287032947,
      -1.2677137354326122,
      -1.262970394730885,
      -1.4489236932463565
    ],
    [
      -1.6369128475202266,
      -1.5461403788816146,
      -1.5603685543421644,
      -1.5467979743610596,
      -1.5561686543659172,
      -1.5923618104427706,
      -1.6483178131667504,
      -1.5713661732616724,
      -1.5522120038188085,
      0.0,
      -1.5326100575121,
      -1.6307165596666315,
      -1.556595576199728,
      -1.5802923712035364,
      -1.5608861678081052,
      -1.5433765924418335,
      -1.5606036249878092,
      -1.6579468860945594,
      -1.5534195387683511,
      -1.5239532197248014,
      -1.5430104272049714,
      -1.6221489739515416,
      -1.5959393830453683,
      -1.5775744739974007,
      -1.5923057476048532,
      -1.5025667455139655,
      -1.4947180548328005,
      -1.6016789375457325,
      -1.630689423832996
    ],
    [
      -1.3631154551454943,
      -1.2359511097084968,
      -1.2730759741329993,
      -1.2078422493943872,
      -1.2332493435171914,
      -1.304348622407695,
      -1.3458480329160423,
      -1.2630739396163764,
      -1.2041439469481088,
      -1.3528705936053305,
      0.0,
      -1.438823382619113,
      -1.25918028735174,
      -1.3313576067080306,
      -1.2856684362932613,
      -1.236662130723656,
      -1.3330947038856975,
      -1.3069045686369838,
      -1.3021260485707764,
      -1.2414669751850105,
      -1.272676514255615,
      -1.3034712271550293,
      -1.3668476563626015,
      -1.2813962484463264,
      -1.3636872846070098,
      -1.2098246501557584,
      -1.2206493408885806,
      -1.324624804084825,
      -1.372756046889084
    ],
    [
      -1.174218227328587,
      -1.15953053399432,
      -1.1538629773156592,
      -1.1716913716673347,
      -1.1454580228881293,
      -1.159393351288361,
      -1.1343951913059604,
      -1.1414984959017351,
      -1.1733179557813762,
      -1.1429971277812787,
      -1.1730066634306118,
      0.0,
      -1.1549502948789607,
      -1.1557833012735599,
      -1.1460887105361681,
      -1.1740321331381938,
      -1.1437376343659154,
      -1.1702747185673015,
      -1.1574395792975996,
      -1.1471541298229193,
      -1.139053804565812,
      -1.1149064113592093,
      -1.1631911617829023,
      -1.1795079488769824,
      -1.1355450228461146,
      -1.1516145141689078,
      -1.1461708548763494,
      -1.1554686726282386,
      -1.115010617204225
    ],
    [
      -1.399496847534103,
      -1.1344517621075187,
      -1.2494556623995867,
      -1.1934896486809081,
      -1.228329213971971,
      -1.2334259163812398,
      -1.3091874088565358,
      -1.2007586179365737,
      -1.179829103886218,
      -1.3707068611501503,
      -1.2215218619554231,
      -1.4291606946855226,
      0.0,
      -1.2969417734945516,
      -1.152393721043412,
      -1.1804256538738027,
      -1.2489776879915726,
      -1.2773884909050754,
      -1.2821043020919218,
      -1.2368197905067846,
      -1.2018481650706598,
      -1.2847962299441944,
      -1.314004127468281,
      -1.254895884658394,
      -1.3512649990117815,
      -1.238106317666128,
      -1.264603271515107,
      -1.264209915835563,
      -1.3808159277789736
    ],
    [
      -1.3873942247652584,
      -1.1542565677309502,
      -1.234514685541723,
      -1.2036847816615712,
      -1.1390810876585138,
      -1.230867548934673,
      -1.3167044279208393,
      -1.213638867875003,
      -1.1479571944051257,
      -1.2763932516375034,
      -1.2151984647862366,
      -1.3941884580924775,
      -1.2025285552340927,
      0.0,
      -1.203698576569557,
      -1.1557139979042848,
      -1.2109736339441046,
      -1.2902019005355672,
      -1.177771926849745,
      -1.216680444196916,
      -1.182057460589492,
      -1.2531174438971653,
      -1.3050987354956056,
      -1.2163507628194492,
      -1.2709133174881504,
      -1.268120864381316,
      -1.219776303716767,
      -1.2204439845948747,
      -1.3931768415168042
    ],
    [
      -1.3704672810141962,
      -1.1933458338152456,
      -1.251149084149462,
      -1.1996666751738585,
      -1.146700282091831,
      -1.2283471600797098,
      -1.3449252268344964,
      -1.1613739878199973,
      -1.2193857499739558,
      -1.3030755777299727,
      -1.2484141058704914,
      -1.3697815673960818,
      -1.147784728403562,
      -1.2489330514322163,
      0.0,
      -1.2364468058525535,
      -1.1961127297200351,
      -1.234919906149416,
      -1.2571281238671401,
      -1.2348918407572504,
      -1.2414565835675457,
      -1.2438179690740117,
      -1.320174608560172,
      -1.2685211988965983,
      -1.2756884903116243,
      -1.2368295729091612,
      -1.244925327806373,
      -1.27175918846148,
      -1.350687976657469
    ],
    [
      -1.4477068787211111,
      -1.2716203531552146,
      -1.346274584244541,
      -1.2529324202427796,
      -1.4108767580198809,
      -1.3603973673191452,
      -1.4325635249626096,
      -1.3225935301899423,
      -1.2839048129914736,
      -1.4838164141904442,
      -1.284697152122767,
      -1.5866284611548562,
      -1.2909823112341245,
      -1.3807765457419792,
      -1.375677440729439,
      0.0,
      -1.3968977291037494,
      -1.4300382318952063,
      -1.3677634415862316,
      -1.2810805821337319,
      -1.3013674755136853,
      -1.4237330039896543,
      -1.4683976033531512,
      -1.3423592667690158,
      -1.4677262624023435,
      -1.3201882164577154,
      -1.3780838498085621,
      -1.3665725654137073,
      -1.4950539888550762
    ],
    [
      -1.3372007978759104,
      -1.2540429255277175,
      -1.2770385940080262,
      -1.2627047767193846,
      -1.2360383260791052,
      -1.337525386024591,
      -1.3589620750331868,
      -1.2208060088382557,
      -1.2905801776234942,
      -1.3057519805419968,
      -1.2563642703630107,
      -1.396398561663515,
      -1.2263314508308274,
      -1.2620375975868747,
      -1.2368335779699784,
      -1.256967327352311,
      0.0,
      -1.3090470378729193,
      -1.261534034607501,
      -1.2799483120826858,
      -1.2002210452984359,
      -1.3021186485393945,
      -1.3315266884011792,
      -1.26934543984585,
      -1.3138482249637717,
      -1.2707101138593042,
      -1.2716137960100382,
      -1.3307609298689087,
      -1.3787825892537748
    ],
    [
      -1.5118030510641154,
      -1.3193511358245251,
      -1.3734154525016697,
      -1.32170492769983,
      -1.3267736424213612,
      -1.410199762931234,
      -1.4644080717670462,
      -1.3365408467374522,
      -1.356019575133244,
      -1.5221295912445025,
      -1.3864410830804381,
      -1.5401717624581366,
      -1.3642655873076828,
      -1.4361037426598269,
      -1.331148269307747,
      -1.4217948782161896,
      -1.3983317833406312,
      0.0,
      -1.4522153581708745,
      -1.380920936777682,
      -1.3721974165492765,
      -1.368668139040427,
      -1.4363957000965357,
      -1.4351716673896668,
      -1.4567763797125646,
      -1.375883922157787,
      -1.3963434788393372,
      -1.4013390583655312,
      -1.4868524887104546
    ],
    [
      -1.6036076194328903,
      -1.5023567661314612,
      -1.5077656889415596,
      -1.5028313615655522,
      -1.5061541126270512,
      -1.5485861720241378,
      -1.579056195236213,
      -1.4829427360544014,
      -1.4945621689826925,
      -1.5267385148847692,
      -1.4945302738378996,
      -1.6455598268193932,
      -1.4919130403768122,
      -1.522319233712476,
      -1.4985619004240136,
      -1.5091798653149664,
      -1.5057109952337313,
      -1.5656272395976625,
      0.0,
      -1.4852026921335275,
      -1.4942275257024422,
      -1.536672340297429,
      -1.6015681989536363,
      -1.4705833846197045,
      -1.599226161740589,
      -1.4792845758761812,
      -1.5294201752413643,
      -1.595379673694953,
      -1.6069833693592979
    ],
    [
      -1.4658599752612655,
      -1.2694215320725368,
      -1.318257988154558,
      -1.3013839545160353,
      -1.3385550699522208,
      -1.3934324472067248,
      -1.4404287141666456,
      -1.2905953647906738,
      -1.3124088364033781,
      -1.5286650293562025,
      -1.3398501147447612,
      -1.5804959772925706,
      -1.366961741241878,
      -1.3760800648250313,
      -1.3948075411908005,
      -1.3221792060231323,
      -1.4034215780985888,
      -1.3992744030556152,
      -1.3909712905277467,
      0.0,
      -1.3856815672168221,
      -1.3519155381259482,
      -1.4917435168058701,
      -1.347987332737171,
      -1.4423588944180041,
      -1.383965169883189,
      -1.3984510457561365,
      -1.3424919863576894,
      -1.5442155455251725
    ],
    [
      -1.5566700130951316,
      -1.4760692161360534,
      -1.5265222985583768,
      -1.5077307115184273,
      -1.5282948077088376,
      -1.5567067010279834,
      -1.6002429011520416,
      -1.4451364311847434,
      -1.531854395909062,
      -1.6660318012588828,
      -1.477435091041798,
      -1.737264208327337,
      -1.461118708469264,
      -1.5965196222509543,
      -1.5290875544161495,
      -1.4778490999512142,
      -1.5315193343219682,
      -1.572828101848334,
      -1.6162098367155038,
      -1.5471921179542472,
      0.0,
      -1.580993148058637,
      -1.6238430417895466,
      -1.523260337089855,
      -1.5746225665347253,
      -1.5445170100944638,
      -1.5159999216339104,
      -1.5602053964091358,
      -1.6440051709622057
    ],
    [
      -1.5621430231640296,
      -1.3819531946024364,
      -1.4434537357587172,
      -1.4241300128074215,
      -1.3817268611367224,
      -1.4525333040804191,
      -1.4621368479115942,
      -1.4018671692471432,
      -1.4035663284729025,
      -1.608702499098976,
      -1.4461457763010321,
      -1.5952446487387253,
      -1.4698095282136403,
      -1.5044301902071737,
      -1.4667405205985067,
      -1.4716993591855636,
      -1.5432412875811068,
      -1.4255608460677278,
      -1.470799582171637,
      -1.3612369348465851,
      -1.5037510073680407,
      0.0,
      -1.5070040452339137,
      -1.4780020132476308,
      -1.504179986909817,
      -1.4759689585363998,
      -1.4984011937010067,
      -1.4750809762335408,
      -1.5614501367150542
    ],
    [
      -1.381342891696201,
      -1.2716013802193948,
      -1.2927160356178855,
      -1.3202613643307664,
      -1.3177967793450884,
      -1.28206064685649,
      -1.3386594614431226,
      -1.2722615810884046,
      -1.36196336684248,
      -1.3863167782036745,
      -1.3189713241878733,
      -1.4229131768759273,
      -1.3243450918727322,
      -1.3513152492042855,
      -1.3149475549958216,
      -1.341532022588419,
      -1.3553907244961172,
      -1.3244440543686529,
      -1.3905900733159888,
      -1.3354686376113962,
      -1.3207682111014252,
      -1.2807400347916398,
      0.0,
      -1.329203557341704,
      -1.2205091752733168,
      -1.3197221745147447,
      -1.3044315430225624,
      -1.3229962891770484,
      -1.3881801202166597
    ],
    [
      -1.4413086530616437,
      -1.3461408809616136,
      -1.330668384234755,
      -1.3645237724502965,
      -1.3785573996181573,
      -1.454479082312862,
      -1.404516158080346,
      -1.3189460850955643,
      -1.3074761499970002,
      -1.4079095355082918,
      -1.29219911397279,
      -1.517917218258946,
      -1.3676916323116057,
      -1.3672428376817094,
      -1.3807362657389313,
      -1.3453430496737178,
      -1.376578494559042,
      -1.427996952646368,
      -1.345278691888727,
      -1.333269789491264,
      -1.2885454201835096,
      -1.3815395307261202,
      -1.4321078249439487,
      0.0,
      -1.4020961650155954,
      -1.330349228214978,
      -1.3046697533958544,
      -1.3285339355463122,
      -1.4561145380525125
    ],
    [
      -1.508816436388642,
      -1.4315386603234097,
      -1.4177658244972093,
      -1.4343205923464413,
      -1.4326858148493025,
      -1.4507036583459996,
      -1.4816438966794638,
      -1.395525070291336,
      -1.4397822461816487,
      -1.4754845928825828,
      -1.4470905824796056,
      -1.5429704633271857,
      -1.4345682257330934,
      -1.4570491613564351,
      -1.4417857778047833,
      -1.4409999720010598,
      -1.4360532912019344,
      -1.463256126498163,
      -1.5074419050234904,
      -1.4521961263083318,
      -1.3861894741964176,
      -1.4444177352168133,
      -1.373172231737255,
      -1.4279054713928139,
      0.0,
      -1.4431985974814496,
      -1.4183584216880254,
      -1.4323510287170877,
      -1.5151305925884526
    ],
    [
      -1.4927652050729154,
      -1.3951485682942082,
      -1.4162058695371327,
      -1.403280531286625,
      -1.4289587951241003,
      -1.4247189818724424,
      -1.5137123112378223,
      -1.3790926374007484,
      -1.4120612885944908,
      -1.5390057030796276,
      -1.3989621571163768,
      -1.6543390050328566,
      -1.4146247702900692,
      -1.5239233875191607,
      -1.4540791437336784,
      -1.432793237699353,
      -1.4979674528408966,
      -1.4229591628476486,
      -1.4761613967744855,
      -1.434123818615754,
      -1.449037400944158,
      -1.4769803415476455,
      -1.5183251756320681,
      -1.4668861464993048,
      -1.5230874261582033,
      0.0,
      -1.4298655045155957,
      -1.4783867257742527,
      -1.5607816233357943
    ],
    [
      -1.6650245340274874,
      -1.524044086547006,
      -1.562811211005469,
      -1.5316599985603134,
      -1.5546000114837673,
      -1.6135375930481604,
      -1.6736578356031304,
      -1.5857308183744312,
      -1.59367376486615,
      -1.6166423535895642,
      -1.5401092489806862,
      -1.7241562562862602,
      -1.5870043517062944,
      -1.6144845015629168,
      -1.6051632570697314,
      -1.5502433483371574,
      -1.6067606945666693,
      -1.6182590525671103,
      -1.6507368246178007,
      -1.542331075082482,
      -1.5646226716234373,
      -1.6133988158751833,
      -1.6307336152744527,
      -1.567755148028705,
      -1.5973049196573839,
      -1.6256944869471208,
      0.0,
      -1.6288807863456423,
      -1.6613244707958965
    ],
    [
      -1.5112636890342277,
      -1.3970917939597611,
      -1.384665488082796,
      -1.3960079107449552,
      -1.43141000207098,
      -1.3960431435665615,
      -1.4766776340533365,
      -1.371528291506103,
      -1.3503859710735868,
      -1.5285223554413399,
      -1.4140867430251816,
      -1.5638858336860948,
      -1.394861290849461,
      -1.4378465587297302,
      -1.4204696996508341,
      -1.3795633712282391,
      -1.4740216985884385,
      -1.4075949503160157,
      -1.478417648225585,
      -1.3722583051862405,
      -1.3978791289193881,
      -1.4336215683495959,
      -1.4547409680853811,
      -1.3461610937764634,
      -1.4328073644578478,
      -1.4573693814119546,
      -1.445503478589639,
      0.0,
      -1.5351773590698652
    ],
    [
      -1.4868023348567814,
      -1.4243718068435656,
      -1.4219652262569853,
      -1.4443929018790909,
      -1.484439686199433,
      -1.4404095685024585,
      -1.4384968726804281,
      -1.4090747356808102,
      -1.464453134020316,
      -1.4554289883763902,
      -1.4127343067594589,
      -1.4267536848812756,
      -1.4112371023195276,
      -1.5032795109746313,
      -1.4447697925757652,
      -1.4318078324886183,
      -1.4563041433074977,
      -1.4619657833922262,
      -1.4911422916492132,
      -1.4525177117780597,
      -1.423693718725073,
      -1.3761809999354653,
      -1.4525511279919634,
      -1.4719470730647544,
      -1.4652201834443475,
      -1.4338489008588915,
      -1.3713022207962382,
      -1.4520303924249667,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.24974548722630763,
      0.25114080934620997,
      0.26792429184193356,
      0.22453428374939755,
      0.2187134073388468,
      0.1831764766137436,
      0.23631159187584938,
      0.22015853362647153,
      0.18477965998327894,
      0.26632613603558264,
      0.13930808406341422,
      0.24315564329965333,
      0.20456849252194642,
      0.23227816867647366,
      0.23107887477631817,
      0.23166753886828317,
      0.212531841015823,
      0.23006049928237582,
      0.2484099280114025,
      0.2544557494512323,
      0.19948451949536938,
      0.20005122820681098,
      0.2277818604128652,
      0.2068626117143877,
      0.2360615367518224,
      0.24222420490532848,
      0.21469358640152292,
      0.18028698044338176
    ],
    [
      0.4414015221882477,
      0.0,
      0.5824977658956161,
      0.6963779373641414,
      0.6812241300909545,
      0.5461222528299119,
      0.46536962859343856,
      0.6008298089652158,
      0.5899066181986095,
      0.38891215494304543,
      0.546018418999004,
      0.33391728093100026,
      0.589283106433836,
      0.49425243319406054,
      0.5755288680740185,
      0.574243265086338,
      0.461080054652677,
      0.5272564559682342,
      0.5158794029353251,
      0.6317511792247628,
      0.5429007797491521,
      0.5457106758706758,
      0.40109520072201477,
      0.5237211148990832,
      0.4238735481275888,
      0.4935329303417555,
      0.5184450716895754,
      0.5213324155974353,
      0.3823212890818939
    ],
    [
      0.3767530096957785,
      0.5223535059149775,
      0.0,
      0.4923369260714092,
      0.4686608134949719,
      0.4020533538684985,
      0.37694251246897736,
      0.4514824006792366,
      0.49211438357860726,
      0.3502838148645473,
      0.47068103292180585,
      0.26444551363940505,
      0.4381562149712992,
      0.3819598232495607,
      0.4330559510415648,
      0.456860742837802,
      0.4009851095495538,
      0.45215040532937456,
      0.38687013252591584,
      0.49211559572820196,
      0.4448053070942566,
      0.4140279170334735,
      0.3644373369351501,
      0.45547136690137835,
      0.3893290235747453,
      0.4285457776550976,
      0.41926518347857544,
      0.4301830615674802,
      0.3245948445956919
    ],
    [
      0.3816612302893039,
      0.589945157848099,
      0.49668727818696,
      0.0,
      0.46841523485378,
      0.4204146292161344,
      0.3909185645086486,
      0.4962286876478983,
      0.5273174251724033,
      0.3981398740169868,
      0.5321482192010549,
      0.2909517159696473,
      0.5152920087435695,
      0.4495890049807709,
      0.5070658374329748,
      0.4963333843473283,
      0.4280870594541084,
      0.4676177551161851,
      0.44364900252385087,
      0.5241926019258427,
      0.4702344381442849,
      0.4489484714743057,
      0.37645303320621926,
      0.4580101101088594,
      0.3739968407185261,
      0.4641199979295205,
      0.4642159797296719,
      0.39143511925951313,
      0.34792933636145973
    ],
    [
      0.35633797438979564,
      0.57207709446858,
      0.4657455204682013,
      0.47621181616373587,
      0.0,
      0.4528891373338213,
      0.4223430993457491,
      0.4937769288001408,
      0.5335063188942368,
      0.3533641682279509,
      0.4274189045378969,
      0.28649306686685505,
      0.4666416811370069,
      0.4484575104618387,
      0.4802632085579934,
      0.42667071442737403,
      0.4171378042808891,
      0.40232259766144063,
      0.45270267221577876,
      0.49340216221787303,
      0.4300832129107275,
      0.46135613828965916,
      0.30923337468508105,
      0.3648899850807943,
      0.329345209907028,
      0.3722148802895049,
      0.4198388401746751,
      0.39374227512623605,
      0.2540205678424514
    ],
    [
      0.32351412214251885,
      0.560105688020544,
      0.48892983733878115,
      0.5234341028131138,
      0.5074943740759699,
      0.0,
      0.40239493470084,
      0.5368946053417116,
      0.5318609903860403,
      0.3652181550845628,
      0.46213732185181144,
      0.3045161355751336,
      0.49335383794896637,
      0.4385951758472497,
      0.48020767132369047,
      0.5337874927960331,
      0.39744778239528733,
      0.4775758589640704,
      0.43960413078600413,
      0.4768100254711316,
      0.47496947488840235,
      0.4492147848115684,
      0.39805749406913216,
      0.39206054397617396,
      0.38172993510031805,
      0.4663470757550634,
      0.4220220633838001,
      0.4522461995555449,
      0.37647686263241353
    ],
    [
      0.2381675905685161,
      0.3414473280821524,
      0.3617697925072969,
      0.3053939821337417,
      0.34420249096569777,
      0.3290246851854115,
      0.0,
      0.36068465817472517,
      0.32019540338598995,
      0.23768383439191987,
      0.3217474039179016,
      0.2652757286725953,
      0.31233278581912005,
      0.279880669188751,
      0.3064370217531225,
      0.2927854342055096,
      0.29444117770737566,
      0.33284001150938103,
      0.32361108610349953,
      0.3162082124340071,
      0.3274679892911858,
      0.3040829872023798,
      0.28628727565912504,
      0.31925415755845865,
      0.29445231758528867,
      0.3079103615477803,
      0.2832620149196532,
      0.3157602561834427,
      0.2393153290582104
    ],
    [
      0.378619997912383,
      0.570514273052841,
      0.4820345357115081,
      0.5266837621244687,
      0.5266727846809871,
      0.4940807692120872,
      0.41244079569690584,
      0.0,
      0.46462715489911477,
      0.3067381776403626,
      0.46926436956837203,
      0.2667423277101708,
      0.49412602657304916,
      0.39615318932102483,
      0.48979138515657783,
      0.47461415007380126,
      0.44324825309743554,
      0.4820135688659515,
      0.38357202611119345,
      0.5625592931385133,
      0.5214803400560797,
      0.4938712857672998,
      0.39783287783609755,
      0.45383478460723614,
      0.4252167816222461,
      0.485715324767074,
      0.45185751945203534,
      0.4794756292620719,
      0.30582707135163
    ],
    [
      0.3703385246624158,
      0.5376008362525164,
      0.506953649612147,
      0.545521326915259,
      0.5477423189577606,
      0.484102229044846,
      0.41365105999711194,
      0.4818577626486773,
      0.0,
      0.36294156326139126,
      0.5422313474039084,
      0.263428368483714,
      0.5012878522697033,
      0.4538590253772379,
      0.4745218105323452,
      0.49918952591821486,
      0.39745040823262356,
      0.4724568417679642,
      0.4612798847853843,
      0.49025042775511585,
      0.49122101182367883,
      0.47064848351121924,
      0.3979124437832333,
      0.4742506800036468,
      0.3926874198366739,
      0.4619082470728797,
      0.4782897986732144,
      0.48303313937494163,
      0.29707984085947015
    ],
    [
      0.2826106256189731,
      0.3733830942575851,
      0.3591549187970353,
      0.3727254987781401,
      0.36335481877328246,
      0.3271616626964291,
      0.2712056599724493,
      0.3481572998775273,
      0.36731146932039116,
      0.0,
      0.3869134156270997,
      0.28880691347256815,
      0.3629278969394716,
      0.3392311019356633,
      0.3586373053310945,
      0.37614688069736624,
      0.35891984815139044,
      0.26157658704464026,
      0.36610393437084854,
      0.39557025341439833,
      0.3765130459342283,
      0.2973744991876581,
      0.3235840900938314,
      0.341948999141799,
      0.32721772553434647,
      0.41695672762523417,
      0.4248054183063992,
      0.31784453559346715,
      0.2888340493062036
    ],
    [
      0.3691690623253461,
      0.4963334077623436,
      0.459208543337841,
      0.5244422680764531,
      0.49903517395364894,
      0.42793589506314533,
      0.38643648455479807,
      0.46921057785446396,
      0.5281405705227316,
      0.3794139238655099,
      0.0,
      0.2934611348517273,
      0.4731042301191004,
      0.40092691076280973,
      0.446616081177579,
      0.4956223867471843,
      0.3991898135851428,
      0.42537994883385655,
      0.43015846890006393,
      0.49081754228582986,
      0.45960800321522544,
      0.42881329031581106,
      0.3654368611082388,
      0.45088826902451395,
      0.3685972328638305,
      0.522459867315082,
      0.5116351765822598,
      0.40765971338601537,
      0.3595284705817563
    ],
    [
      0.2192057090170747,
      0.2338934023513417,
      0.23956095903000252,
      0.22173256467832703,
      0.2479659134575325,
      0.2340305850573008,
      0.2590287450397013,
      0.2519254404439266,
      0.22010598056428554,
      0.25042680856438304,
      0.22041727291504998,
      0.0,
      0.23847364146670103,
      0.23764063507210187,
      0.24733522580949363,
      0.21939180320746798,
      0.2496863019797464,
      0.22314921777836028,
      0.2359843570480622,
      0.24626980652274244,
      0.2543701317798497,
      0.27851752498645244,
      0.23023277456275948,
      0.21391598746867935,
      0.25787891349954717,
      0.24180942217675394,
      0.24725308146931235,
      0.23795526371742315,
      0.27841331914143685
    ],
    [
      0.3132938120839983,
      0.5783388975105825,
      0.46333499721851457,
      0.5193010109371932,
      0.48446144564613025,
      0.4793647432368615,
      0.40360325076156545,
      0.5120320416815276,
      0.5329615557318832,
      0.342083798467951,
      0.49126879766267817,
      0.28362996493257864,
      0.0,
      0.41584888612354964,
      0.5603969385746892,
      0.5323650057442986,
      0.4638129716265287,
      0.43540216871302584,
      0.43068635752617945,
      0.4759708691113167,
      0.5109424945474415,
      0.4279944296739069,
      0.3987865321498203,
      0.4578947749597073,
      0.36152566060631974,
      0.4746843419519733,
      0.4481873881029943,
      0.4485807437825382,
      0.33197473183912773
    ],
    [
      0.32272229126878393,
      0.5558599483030922,
      0.4756018304923193,
      0.5064317343724711,
      0.5710354283755286,
      0.47924896709936937,
      0.39341208811320305,
      0.4964776481590394,
      0.5621593216289167,
      0.43372326439653897,
      0.49491805124780575,
      0.31592805794156487,
      0.5075879607999496,
      0.0,
      0.5064179394644854,
      0.5544025181297576,
      0.49914288208993773,
      0.4199146154984752,
      0.5323445891842973,
      0.49343607183712646,
      0.5280590554445503,
      0.45699907213687707,
      0.40501778053843673,
      0.49376575321459315,
      0.43920319854589196,
      0.44199565165272636,
      0.4903402123172753,
      0.48967253143916767,
      0.31693967451723815
    ],
    [
      0.36727396022258385,
      0.5443954074215345,
      0.4865921570873182,
      0.5380745660629216,
      0.591040959144949,
      0.5093940811570703,
      0.39281601440228364,
      0.5763672534167827,
      0.5183554912628243,
      0.43466566350680735,
      0.4893271353662887,
      0.3679596738406983,
      0.589956512833218,
      0.48880818980456375,
      0.0,
      0.5012944353842266,
      0.541628511516745,
      0.502821335087364,
      0.48061311736963996,
      0.5028494004795296,
      0.49628465766923435,
      0.49392327216276843,
      0.4175666326766081,
      0.4692200423401818,
      0.46205275092515574,
      0.5009116683276189,
      0.4928159134304071,
      0.4659820527753,
      0.387053264579311
    ],
    [
      0.4628648147550254,
      0.6389513403209219,
      0.5642971092315956,
      0.6576392732333569,
      0.49969493545625565,
      0.5501743261569914,
      0.4780081685135269,
      0.5879781632861942,
      0.6266668804846629,
      0.42675527928569235,
      0.6258745413533695,
      0.3239432323212803,
      0.619589382242012,
      0.5297951477341574,
      0.5348942527466976,
      0.0,
      0.5136739643723871,
      0.48053346158093024,
      0.5428082518899049,
      0.6294911113424047,
      0.6092042179624513,
      0.48683868948648223,
      0.44217409012298536,
      0.5682124267071207,
      0.44284543107379304,
      0.5903834770184211,
      0.5324878436675744,
      0.5439991280624292,
      0.4155177046210603
    ],
    [
      0.3373777076738145,
      0.4205355800220074,
      0.39753991154169865,
      0.41187372883034024,
      0.43854017947061963,
      0.33705311952513384,
      0.3156164305165381,
      0.45377249671146913,
      0.3839983279262307,
      0.36882652500772806,
      0.4182142351867142,
      0.2781799438862098,
      0.4482470547188975,
      0.41254090796285015,
      0.4377449275797465,
      0.4176111781974139,
      0.0,
      0.36553146767680555,
      0.41304447094222385,
      0.3946301934670391,
      0.474357460251289,
      0.37245985701033035,
      0.3430518171485457,
      0.40523306570387496,
      0.36073028058595313,
      0.40386839169042066,
      0.40296470953968666,
      0.34381757568081617,
      0.29579591629595003
    ],
    [
      0.36165120298461084,
      0.5541031182242011,
      0.5000388015470565,
      0.5517493263488962,
      0.546680611627365,
      0.4632544911174923,
      0.40904618228168,
      0.536913407311274,
      0.5174346789154822,
      0.3513246628042237,
      0.4870131709682881,
      0.33328249159058965,
      0.5091886667410435,
      0.43735051138889935,
      0.5423059847409792,
      0.4516593758325367,
      0.475122470708095,
      0.0,
      0.4212388958778517,
      0.4925333172710442,
      0.5012568374994497,
      0.5047861150082993,
      0.4370585539521905,
      0.4382825866590594,
      0.41667787433616166,
      0.49757033189093924,
      0.47711077520938905,
      0.47211519568319504,
      0.3866017653382716
    ],
    [
      0.28898424766139086,
      0.39023510096282,
      0.3848261781527216,
      0.389760505528729,
      0.38643775446722994,
      0.34400569507014334,
      0.3135356718580682,
      0.40964913103987977,
      0.3980296981115887,
      0.365853352209512,
      0.3980615932563816,
      0.247032040274888,
      0.400678826717469,
      0.37027263338180516,
      0.3940299666702676,
      0.3834120017793148,
      0.38688087186054987,
      0.3269646274966187,
      0.0,
      0.40738917496075366,
      0.39836434139183896,
      0.35591952679685224,
      0.29102366814064484,
      0.4220084824745767,
      0.2933657053536922,
      0.41330729121809995,
      0.36317169185291687,
      0.29721219339932814,
      0.2856084977349833
    ],
    [
      0.4131798625879708,
      0.6096183057766995,
      0.5607818496946784,
      0.5776558833332011,
      0.5404847678970155,
      0.48560739064251157,
      0.4386111236825907,
      0.5884444730585625,
      0.5666310014458582,
      0.3503748084930338,
      0.5391897231044751,
      0.2985438605566657,
      0.5120780966073584,
      0.502959773024205,
      0.4842322966584358,
      0.556860631826104,
      0.47561825975064753,
      0.4797654347936211,
      0.48806854732148963,
      0.0,
      0.4933582706324142,
      0.5271242997232881,
      0.3872963210433662,
      0.5310525051120654,
      0.4366809434312322,
      0.4950746679660474,
      0.4805887920930998,
      0.5365478514915469,
      0.33482429232406385
    ],
    [
      0.45281675232720975,
      0.533417549286288,
      0.48296446686396455,
      0.5017560539039141,
      0.48119195771350376,
      0.45278006439435803,
      0.4092438642702998,
      0.564350334237598,
      0.47763236951327936,
      0.34345496416345855,
      0.5320516743805435,
      0.2722225570950043,
      0.5483680569530773,
      0.41296714317138705,
      0.4803992110061919,
      0.5316376654711272,
      0.4779674311003732,
      0.4366586635740073,
      0.3932769287068376,
      0.46229464746809423,
      0.0,
      0.4284936173637044,
      0.38564372363279475,
      0.4862264283324864,
      0.43486419888761607,
      0.46496975532787754,
      0.493486843788431,
      0.4492813690132056,
      0.36548159446013573
    ],
    [
      0.3529822472172135,
      0.5331720757788068,
      0.471671534622526,
      0.4909952575738217,
      0.5333984092445208,
      0.462591966300824,
      0.452988422469649,
      0.5132581011341,
      0.5115589419083406,
      0.30642277128226714,
      0.468979494080211,
      0.31988062164251785,
      0.44531574216760283,
      0.4106950801740694,
      0.44838474978273646,
      0.44342591119567953,
      0.37188398280013635,
      0.4895644243135153,
      0.44432568820960605,
      0.553888335534658,
      0.41137426301320246,
      0.0,
      0.4081212251473294,
      0.43712325713361233,
      0.4109452834714262,
      0.4391563118448434,
      0.4167240766802365,
      0.4400442941477023,
      0.35367513366618897
    ],
    [
      0.32385000511287987,
      0.433591516589686,
      0.4124768611911953,
      0.38493153247831446,
      0.3873961174639924,
      0.4231322499525909,
      0.36653343536595817,
      0.4329313157206762,
      0.34322952996660083,
      0.3188761186054063,
      0.3862215726212075,
      0.2822797199331535,
      0.3808478049363486,
      0.35387764760479534,
      0.3902453418132592,
      0.36366087422066173,
      0.3498021723129636,
      0.38074884244042795,
      0.314602823493092,
      0.3697242591976846,
      0.38442468570765564,
      0.42445286201744103,
      0.0,
      0.3759893394673768,
      0.484683721535764,
      0.3854707222943361,
      0.40076135378651845,
      0.38219660763203245,
      0.3170127765924211
    ],
    [
      0.3229392385576919,
      0.41810701065772204,
      0.4335795073845805,
      0.39972411916903905,
      0.38569049200117833,
      0.30976880930647366,
      0.3597317335389896,
      0.44530180652377127,
      0.45677174162233536,
      0.3563383561110438,
      0.4720487776465456,
      0.2463306733603896,
      0.39655625930772986,
      0.3970050539376262,
      0.3835116258804043,
      0.41890484194561783,
      0.3876693970602936,
      0.33625093897296754,
      0.41896919973060864,
      0.43097810212807164,
      0.475702471435826,
      0.38270836089321536,
      0.33214006667538687,
      0.0,
      0.3621517266037402,
      0.43389866340435757,
      0.45957813822348115,
      0.43571395607302343,
      0.30813335356682314
    ],
    [
      0.32284677045255417,
      0.4001245465177865,
      0.4138973823439869,
      0.39734261449475494,
      0.39897739199189375,
      0.38095954849519664,
      0.3500193101617324,
      0.4361381365498602,
      0.3918809606595475,
      0.3561786139586134,
      0.38457262436159056,
      0.28869274351401053,
      0.3970949811081028,
      0.37461404548476107,
      0.38987742903641287,
      0.39066323484013643,
      0.39560991563926184,
      0.36840708034303327,
      0.3242213018177058,
      0.3794670805328644,
      0.44547373264477863,
      0.38724547162438294,
      0.4584909751039412,
      0.40375773544838234,
      0.0,
      0.3884646093597466,
      0.4133047851531708,
      0.39931217812410846,
      0.3165326142527436
    ],
    [
      0.4260125286397207,
      0.523629165418428,
      0.5025718641755035,
      0.5154972024260112,
      0.48981893858853587,
      0.49405875184019377,
      0.4050654224748138,
      0.5396850963118878,
      0.5067164451181454,
      0.3797720306330086,
      0.5198155765962593,
      0.2644387286797796,
      0.504152963422567,
      0.3948543461934755,
      0.46469858997895774,
      0.4859844960132831,
      0.42081028087173955,
      0.4958185708649876,
      0.4426163369381506,
      0.48465391509688205,
      0.46974033276847815,
      0.4417973921649907,
      0.400452558080568,
      0.45189158721333134,
      0.3956903075544329,
      0.0,
      0.48891222919704047,
      0.4403910079383835,
      0.35799611037684187
    ],
    [
      0.4047760051608251,
      0.5457564526413066,
      0.5069893281828435,
      0.5381405406279991,
      0.5152005277045453,
      0.45626294614015217,
      0.3961427035851821,
      0.48406972081388133,
      0.47612677432216244,
      0.4531581855987483,
      0.5296912902076263,
      0.3456442829020523,
      0.48279618748201814,
      0.45531603762539574,
      0.46463728211858113,
      0.5195571908511551,
      0.46303984462164327,
      0.4515414866212022,
      0.41906371457051184,
      0.5274694641058306,
      0.5051778675648753,
      0.4564017233131292,
      0.4390669239138598,
      0.5020453911596074,
      0.47249561953092867,
      0.4441060522411917,
      0.0,
      0.44091975284267027,
      0.408476068392416
    ],
    [
      0.33143497214707507,
      0.44560686722154164,
      0.4580331730985068,
      0.44669075043634754,
      0.41128865911032286,
      0.4466555176147413,
      0.36602102712796625,
      0.4711703696751999,
      0.49231269010771594,
      0.3141763057399629,
      0.4286119181561212,
      0.27881282749520797,
      0.4478373703318417,
      0.40485210245157255,
      0.4222289615304686,
      0.46313528995306363,
      0.3686769625928643,
      0.4351037108652871,
      0.3642810129557177,
      0.4704403559950623,
      0.4448195322619146,
      0.4090770928317069,
      0.38795769309592165,
      0.49653756740483934,
      0.4098912967234549,
      0.38532927976934817,
      0.39719518259166375,
      0.0,
      0.3075213021114376
    ],
    [
      0.36886168408200204,
      0.43129221209521784,
      0.4336987926817981,
      0.41127111705969255,
      0.3712243327393505,
      0.4152544504363249,
      0.41716714625835527,
      0.4465892832579732,
      0.39121088491846745,
      0.4002350305623932,
      0.44292971217932453,
      0.4289103340575078,
      0.4444269166192558,
      0.3523845079641521,
      0.4108942263630182,
      0.4238561864501651,
      0.3993598756312857,
      0.39369823554655725,
      0.3645217272895702,
      0.40314630716072375,
      0.4319703002137103,
      0.47948301900331813,
      0.40311289094682,
      0.38371694587402905,
      0.39044383549443595,
      0.4218151180798919,
      0.4843617981425452,
      0.40363362651381673,
      0.0
    ]
  ],
  "row_avgs": [
    0.22277757235485834,
    0.521242332523129,
    0.4207471807595477,
    0.4507142142274253,
    0.4200888165986898,
    0.4484645241798528,
    0.30578292806115137,
    0.4514145876881614,
    0.45548913674347485,
    0.3455349384214114,
    0.4381867610347253,
    0.240591813885922,
    0.4492403075323171,
    0.4708127906503364,
    0.4860730042947834,
    0.5330463087510602,
    0.3876127664553695,
    0.46690540728066304,
    0.36092930249368094,
    0.48897335835972316,
    0.455567853085956,
    0.4408052715191908,
    0.37692685035908713,
    0.39165015791853347,
    0.3840774219291094,
    0.4538408134134428,
    0.46800247731579786,
    0.41091784969274553,
    0.41248108920077503
  ],
  "col_avgs": [
    0.3539874097052037,
    0.48586194178521186,
    0.4515206912765146,
    0.471129274777776,
    0.46113804448917567,
    0.4223605616190308,
    0.38040964131695587,
    0.4722317336142518,
    0.46246150507831885,
    0.35286149520251175,
    0.45550334754838995,
    0.29189492943786893,
    0.4558163467396417,
    0.40140199949786715,
    0.4409513663861522,
    0.44696983903411713,
    0.40964431951821306,
    0.41198557693728954,
    0.40943423433613185,
    0.4588114154935324,
    0.4510221430481219,
    0.4224198349698774,
    0.3709848372584612,
    0.4285352052995833,
    0.3837655498123045,
    0.43137816011662167,
    0.4330395030907476,
    0.41552790212944146,
    0.3258490272116076
  ],
  "combined_avgs": [
    0.288382491030031,
    0.5035521371541705,
    0.4361339360180312,
    0.46092174450260065,
    0.44061343054393276,
    0.43541254289944176,
    0.3430962846890536,
    0.4618231606512066,
    0.4589753209108969,
    0.3491982168119616,
    0.44684505429155763,
    0.2662433716618955,
    0.4525283271359794,
    0.4361073950741018,
    0.4635121853404678,
    0.49000807389258866,
    0.3986285429867913,
    0.4394454921089763,
    0.38518176841490637,
    0.4738923869266278,
    0.45329499806703893,
    0.43161255324453407,
    0.37395584380877417,
    0.4100926816090584,
    0.38392148587070696,
    0.4426094867650322,
    0.45052099020327274,
    0.4132228759110935,
    0.3691650582061913
  ],
  "gppm": [
    610.8654977343571,
    589.0770498160912,
    603.6861612177718,
    593.2253747147014,
    597.5903705970853,
    619.4727964406671,
    634.1527037522964,
    593.3242029919371,
    600.0526401700115,
    646.1885918279958,
    602.8523683687771,
    673.692975392753,
    605.3088811657154,
    628.0378748980532,
    611.4936460150524,
    608.2052394534186,
    624.1631982652564,
    622.9328676087289,
    616.8904296071202,
    601.1556895249362,
    604.2180132605491,
    617.2418796841503,
    641.2572020855193,
    612.660707710322,
    632.1317057420628,
    613.8744852641519,
    609.2614410701833,
    620.0173595045154,
    661.7426963175025
  ],
  "gppm_normalized": [
    1.415646102227391,
    1.3015338759415662,
    1.3322977757319243,
    1.3088921245934257,
    1.3177169068508792,
    1.3657795809760567,
    1.4010863991981506,
    1.3074698209875462,
    1.3205133520162369,
    1.4280697776898965,
    1.3284043280261584,
    1.4890574279875535,
    1.3381817574437211,
    1.3888617804713108,
    1.348012499723339,
    1.3445936319991603,
    1.3780996658976028,
    1.3737008813902878,
    1.3620746456517174,
    1.3196976974015422,
    1.3279039650089195,
    1.3545821766386783,
    1.4157459113503716,
    1.3421932925380389,
    1.391146153553084,
    1.3569068406079396,
    1.3406675562759225,
    1.3736264071153064,
    1.457354569404755
  ],
  "token_counts": [
    890,
    478,
    448,
    442,
    445,
    418,
    452,
    445,
    419,
    443,
    428,
    434,
    473,
    470,
    422,
    478,
    439,
    426,
    441,
    384,
    396,
    387,
    433,
    369,
    405,
    468,
    417,
    502,
    384,
    474,
    506,
    445,
    440,
    760,
    432,
    447,
    385,
    403,
    395,
    512,
    407,
    451,
    431,
    460,
    432,
    397,
    381,
    423,
    424,
    400,
    412,
    382,
    404,
    423,
    393,
    416,
    480,
    350,
    915,
    391,
    409,
    451,
    406,
    405,
    455,
    370,
    378,
    427,
    403,
    522,
    462,
    448,
    413,
    439,
    408,
    375,
    424,
    413,
    457,
    403,
    419,
    429,
    472,
    383,
    374,
    454,
    405,
    2538,
    455,
    400,
    445,
    409,
    417,
    434,
    451,
    466,
    401,
    419,
    388,
    402,
    463,
    411,
    449,
    464,
    375,
    398,
    458,
    399,
    499,
    348,
    466,
    428,
    383,
    352,
    501,
    355,
    957,
    556,
    414,
    421,
    711,
    446,
    549,
    400,
    474,
    425,
    417,
    545,
    467,
    418,
    439,
    416,
    407,
    483,
    419,
    485,
    407,
    447,
    409,
    389,
    441,
    349,
    389,
    441,
    400,
    582,
    504,
    431,
    471,
    477,
    481,
    474,
    472,
    456,
    402,
    422,
    361,
    467,
    465,
    480,
    475,
    423,
    417,
    438,
    472,
    382,
    345,
    403,
    454,
    464,
    449,
    395,
    435,
    487,
    735,
    467,
    433,
    388,
    384,
    430,
    381,
    444,
    427,
    420,
    429,
    395,
    434,
    422,
    442,
    435,
    437,
    405,
    391,
    423,
    370,
    402,
    416,
    425,
    405,
    359,
    400,
    416,
    394,
    657,
    523,
    436,
    419,
    474,
    390,
    480,
    426,
    428,
    404,
    410,
    529,
    440,
    402,
    391,
    498,
    406,
    448,
    479,
    502,
    416,
    388,
    440,
    448,
    427,
    397,
    451,
    445,
    376,
    414,
    412,
    416,
    442,
    410,
    414,
    440,
    404,
    432,
    343,
    419,
    544,
    416,
    447,
    415,
    396,
    390,
    354,
    411,
    419,
    383,
    374,
    379,
    418,
    348,
    421,
    448,
    422,
    374,
    695,
    444,
    457,
    470,
    451,
    421,
    487,
    408,
    443,
    385,
    446,
    534,
    403,
    397,
    389,
    359,
    422,
    369,
    489,
    402,
    372,
    387,
    401,
    466,
    437,
    374,
    409,
    411,
    327
  ],
  "response_lengths": [
    3587,
    2661,
    2625,
    2787,
    2685,
    2555,
    2754,
    2289,
    2669,
    2338,
    2624,
    3070,
    2436,
    2326,
    2258,
    2211,
    2408,
    2093,
    3034,
    2334,
    2139,
    2217,
    2381,
    2747,
    2535,
    2099,
    2505,
    2332,
    1942
  ]
}