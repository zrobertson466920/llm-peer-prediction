{
  "example_idx": 5,
  "reference": "Published as a conference paper at ICLR 2023\n\nFROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING\n\nSebastian Damrich IWR at Heidelberg University sebastian.damrich@uni-tuebingen.de\n\nJan Niklas B ̈ohm University of T ̈ubingen jan-niklas.boehm@uni-tuebingen.de\n\nFred A. Hamprecht IWR at Heidelberg University fred.hamprecht@iwr.uni-heidelberg.de\n\nDmitry Kobak University of T ̈ubingen dmitry.kobak@uni-tuebingen.de\n\nABSTRACT\n\nNeighbor embedding methods t-SNE and UMAP are the de facto standard for visualizing high-dimensional datasets. Motivated from entirely different viewpoints, their loss functions appear to be unrelated. In practice, they yield strongly differing embeddings and can suggest conflicting interpretations of the same data. The fundamental reasons for this and, more generally, the exact relationship between t-SNE and UMAP have remained unclear. In this work, we uncover their conceptual connection via a new insight into contrastive learning methods. Noisecontrastive estimation can be used to optimize t-SNE, while UMAP relies on negative sampling, another contrastive method. We find the precise relationship between these two contrastive methods and provide a mathematical characterization of the distortion introduced by negative sampling. Visually, this distortion results in UMAP generating more compact embeddings with tighter clusters compared to t-SNE. We exploit this new conceptual connection to propose and implement a generalization of negative sampling, allowing us to interpolate between (and even extrapolate beyond) t-SNE and UMAP and their respective embeddings. Moving along this spectrum of embeddings leads to a trade-off between discrete / local and continuous / global structures, mitigating the risk of over-interpreting ostensible features of any single embedding. We provide a PyTorch implementation.\n\n1\n\nINTRODUCTION\n\nLow-dimensional visualization of high-dimensional data is a ubiquitous step in exploratory data analysis, and the toolbox of visualization methods has been rapidly growing in the last years (McInnes et al., 2018; Amid & Warmuth, 2019; Szubert et al., 2019; Wang et al., 2021). Since all of these methods necessarily distort the true data layout (Chari et al., 2021), it is beneficial to have various tools at one’s disposal. But only equipped with a theoretic understanding of the aims of and relationships between different methods, can practitioners make informed decisions about which visualization to use for which purpose and how to interpret the results.\n\nThe state of the art for non-parametric, non-linear dimensionality reduction relies on the neighbor embedding framework (Hinton & Roweis, 2002). Its two most popular examples are t-SNE (van der Maaten & Hinton, 2008; van der Maaten, 2014) and UMAP (McInnes et al., 2018). Both can produce insightful, but qualitatively distinct embeddings. However, why their embeddings are different and what exactly is the conceptual relation between their loss functions, has remained elusive.\n\nHere, we answer this question and thus explain the mathematical underpinnings of the relationship between t-SNE and UMAP. Our conceptual insight naturally suggests a spectrum of embedding methods complementary to that of B ̈ohm et al. (2022), along which the focus of the visualization shifts from local to global structure (Fig. 1). On this spectrum, UMAP and t-SNE are simply two instances and inspecting various embeddings helps to guard against over-interpretation of apparent structure. As a practical corollary, our analysis identifies and remedies an instability in UMAP.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure 1: (a – e) Neg-t-SNE embedding spectrum of the MNIST dataset for various values of the fixed normalization constant ̄Z, see Sec. 5. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = (cid:0)n (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) ij)−1 tries to match ̄Z and grows with it. Here, we initialized all Neg-t-SNE runs using ̄Z = |X|/m; without this ‘early exaggeration’, low values of ̄Z yield fragmented clusters (Fig. S11).\n\nij(1+d2\n\n2\n\nWe provide the new connection between t-SNE and UMAP via a deeper understanding of contrastive learning methods. Noise-contrastive estimation (NCE) (Gutmann & Hyv ̈arinen, 2010; 2012) can be used to optimize t-SNE (Artemenkov & Panov, 2020), while UMAP relies on another contrastive method, negative sampling (NEG) (Mikolov et al., 2013). We investigate the discrepancy between NCE and NEG, show that NEG introduces a distortion, and this distortion explains how UMAP and t-SNE embeddings differ. Finally, we discuss the relationship between neighbor embeddings and self-supervised learning (Wu et al., 2018; He et al., 2020; Chen et al., 2020; Le-Khac et al., 2020).\n\nIn summary, our contributions are\n\n1. a new connection between the contrastive methods NCE and NEG (Sec. 4), 2. the exact relation of t-SNE and UMAP and a remedy for an instability in UMAP (Sec. 6), 3. a spectrum of ‘contrastive’ neighbor embeddings encompassing UMAP and t-SNE (Sec. 5), 4. a connection between neighbor embeddings and self-supervised learning (Sec. 7), 5. a unified PyTorch framework for contrastive (non-)parametric neighbor embedding methods.\n\nOur code is available at https://github.com/berenslab/contrastive-ne and https://github.com/hci-unihd/cl-tsne-umap.\n\n2 RELATED WORK\n\nOne of the most popular methods for data visualization is t-SNE (van der Maaten & Hinton, 2008; van der Maaten, 2014). Recently developed NCVis (Artemenkov & Panov, 2020) employs noisecontrastive estimation (Gutmann & Hyv ̈arinen, 2010; 2012) to approximate t-SNE in a samplingbased way. Therefore, we will often refer to the NCVis algorithm as ‘NC-t-SNE’. UMAP (McInnes et al., 2018) has matched t-SNE’s popularity at least in computational biology (Becht et al., 2019) and uses another sampling-based optimization method, negative sampling (Mikolov et al., 2013), also employed by LargeVis (Tang et al., 2016). Other recent sampling-based visualization methods include TriMap (Amid & Warmuth, 2019) and PaCMAP (Wang et al., 2021).\n\nGiven their success, t-SNE and UMAP have been scrutinized to find out which aspects are essential to their performance. Initialization was found to be strongly influencing the global structure\n\n2\n\nDefault NEGPublished as a conference paper at ICLR 2023\n\nin both methods (Kobak & Linderman, 2021). The exact choice of the low-dimensional similarity kernel (B ̈ohm et al., 2022) or the weights of the k-nearest-neighbor graph (Damrich & Hamprecht, 2021) were shown to be largely inconsequential. Both algorithms have similar relevant hyperparameters such as, e.g., the heavy-tailedness of the similarity kernel (Yang et al., 2009; Kobak et al., 2019).\n\nWhile not obvious from the original presentations, the central difference between t-SNE and UMAP can therefore only be in their loss functions, which have been studied by Damrich & Hamprecht (2021); B ̈ohm et al. (2022); Wang et al. (2021), but never conceptually connected. We achieve this by deepening the link between negative sampling (NEG) and noise-contrastive estimation (NCE).\n\nNEG was introduced as an ad hoc replacement for NCE in the context of learning word embeddings (Mikolov et al., 2013). The relationship between NEG and NCE has been discussed before (Dyer, 2014; Levy & Goldberg, 2014; Ruder, 2016; Ma & Collins, 2018; Le-Khac et al., 2020), but here we go further and provide the precise meaning of NEG: We show that, unlike NCE, NEG learns a model proportional but not equal to the true data distribution.\n\nBoth t-SNE and UMAP have parametric versions (van der Maaten, 2009; Sainburg et al., 2021) with very different logic and implementations. Here we present a unified PyTorch framework for nonparametric and parametric contrastive neighbor embeddings. As special cases, it includes UMAP and t-SNE approximations with NCE (like NCVis) and the InfoNCE loss (Jozefowicz et al., 2016; van den Oord et al., 2018), which has not yet been applied to neighbor embeddings.\n\nOur resulting InfoNC-t-SNE elucidates the relationship between SimCLR (Chen et al., 2020) and neighbor embeddings. The parallel work of Hu et al. (2023) makes a qualitatively similar argument, but does not discuss negative samples. Balestriero & LeCun (2022) show how SimCLR recovers Isomap (Tenenbaum et al., 2000), while we connect SimCLR to more modern neighbor embeddings.\n\n3 BACKGROUND\n\n3.1 NOISE-CONTRASTIVE ESTIMATION (NCE)\n\nThe goal of parametric density estimation is to fit a parametric model qθ to iid samples s1, . . . , sN from an unknown data distribution p over a space X. For maximum likelihood estimation (MLE) the parameters θ are chosen to maximize the log-likelihood of the observed samples\n\nθ∗ = arg max\n\nθ\n\n(cid:88)N\n\ni=1\n\nlog (cid:0)qθ(si)(cid:1) .\n\n(1)\n\nThis approach crucially requires qθ to be a normalized model. It is otherwise trivial to increase the likelihood arbitrarily by scaling qθ. To circumvent the expensive computation of the partition function Z(θ) = (cid:80) x∈X qθ(x), Gutmann & Hyv ̈arinen (2010; 2012) introduced NCE. It turns the unsupervised problem of density estimation into a supervised problem in which the data samples need to be identified from a set containing the N data samples and m times as many noise samples t1, . . . tmN drawn from a noise distribution ξ, e.g., the uniform distribution over X. Briefly, NCE fits θ by minimizing the binary cross-entropy between the true class assignment and posterior probabilities P(data | x) = qθ(x)/(cid:0)qθ(x)+mξ(x)(cid:1) and P(noise | x) = 1−P(data | x) (Supp. A.1):\n\n(cid:34)\n\nθ∗ = arg min\n\n−\n\nθ\n\n(cid:18)\n\nlog\n\nN (cid:88)\n\ni=1\n\nqθ(si) qθ(si) + mξ(si)\n\n(cid:19)\n\n−\n\nmN (cid:88)\n\ni=1\n\n(cid:18)\n\nlog\n\n1 −\n\nqθ(ti) qθ(ti) + mξ(ti)\n\n(cid:19)(cid:35)\n\n.\n\n(2)\n\nThe key advantage of NCE is that the model does not need to be explicitly normalized by the partition function, but nevertheless learns to equal the data distribution p and hence be normalized:\n\nTheorem 1 (Gutmann & Hyv ̈arinen (2010; 2012)). Let ξ have full support and suppose there exists some θ∗ such that qθ∗ = p. Then θ∗ is a minimum of (cid:19)\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\nqθ(s) qθ(s) + mξ(s)\n\n− mEt∼ξ log\n\n1 −\n\nqθ(t) qθ(t) + mξ(t)\n\n(3)\n\nLNCE(θ) = −Es∼p log\n\nand the only other extrema of LNCE are minima ̃θ which also satisfy q ̃θ = p.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIn NCE, the model typically includes an optimizable normalization parameter Z which we emphasize by writing qθ,Z = qθ/Z. But importantly, Thm. 1 applies to any model qθ that is able to match the data distribution p, even if it does not contain a learnable normalization parameter.\n\nIn the setting of learning language models, Jozefowicz et al. (2016) proposed a different version of NCE, called InfoNCE. Instead of classifying samples as data or noise, the aim here is to predict the position of a data sample in an (m + 1)-tuple containing m noise samples and one data sample (Supp. A.2). For the uniform noise distribution ξ, this yields the expected loss function\n\nLInfoNCE(θ) = −\n\nE x∼p x1,...,xm∼ξ\n\nlog\n\n(cid:18)\n\nqθ(x) qθ(x) + (cid:80)m\n\ni=1 qθ(xi)\n\n(cid:19)\n\n.\n\n(4)\n\nMa & Collins (2018) showed that an analogue of Thm. 1 applies to InfoNCE.\n\n3.2 NEIGHBOR EMBEDDINGS\n\nNeighbor embeddings (NE) (Hinton & Roweis, 2002) are a group of dimensionality reduction methods, including UMAP, NCVis, and t-SNE that aim to find a low-dimensional embedding e1, . . . , en ∈ Rd of high-dimensional input points x1, . . . , xn ∈ RD, with D ≫ d and usually d = 2 for visualization. NE methods define a notion of similarity over pairs of input points which encodes the neighborhood structure and informs the low-dimensional embedding.\n\nThe exact high-dimensional similarity distribution differs between the NE algorithms, but recent work (B ̈ohm et al., 2022; Damrich & Hamprecht, 2021) showed that t-SNE and UMAP results stay practically the same when using the binary symmetric k-nearest-neighbor graph (skNN) instead of t-SNE’s Gaussian or UMAP’s Laplacian similarities. An edge ij is in skNN if xi is among the k nearest neighbors of xj or vice versa. The high-dimensional similarity function is then given by p(ij) = 1(ij ∈ skNN)/|skNN|, where |skNN| denotes the number of edges in the skNN graph and 1 is the indicator function. NCVis uses the same similarities. We will always use k = 15.\n\nThere are further differences in the choice of low-dimensional similarity between t-SNE and UMAP, but B ̈ohm et al. (2022) showed that they are negligible. Therefore, here we use the Cauchy kernel φ(dij) = 1/(d2 ij + 1) for all NE methods to transform distances dij = ∥ei − ej∥ in the embedding space into low-dimensional similarities. We abuse notation slightly by also writing φ(ij) = φ(dij).\n\nAll NE methods in this work can be cast in the framework of parametric density estimation. Here, p is the data distribution to be approximated with a model qθ, meaning that the space X on which both p and qθ live is the set of all pairs ij with 1 ≤ i < j ≤ n. The embedding positions e1, . . . , en become the learnable parameters θ of the model qθ. For t-SNE, NC-t-SNE (NCVis), and UMAP, qθ is proportional to φ(∥ei − ej∥), but the proportionality factor and the loss function are different.\n\nt-SNE uses MLE and therefore requires a normalized model qθ(ij) = φ(ij)/Z(θ), where Z(θ) = (cid:80)\n\nk̸=l φ(kl) is the partition function. The loss function\n\nLt-SNE(θ) = −Eij∼p log (cid:0)qθ(ij)(cid:1) = −\n\n(cid:88)\n\ni̸=j\n\n(cid:16)\n\np(ij) log (cid:0)φ(ij)(cid:1)(cid:17)\n\n+ log\n\n(cid:16) (cid:88)\n\nk̸=l\n\n(cid:17)\n\nφ(kl)\n\n(5)\n\nis the expected negative log-likelihood of the embedding positions θ, making t-SNE an instance of MLE. Usually t-SNE’s loss function is introduced as the Kullback-Leibler divergence between p and qθ, which is equivalent as the entropy of p does not depend on θ.\n\nNC-t-SNE uses NCE and optimizes the expected loss function\n\nLNC-t-SNE(θ, Z) = −Eij∼p log\n\n(cid:18)\n\nqθ,Z(ij) qθ,Z(ij) + mξ(ij)\n\n(cid:19)\n\n− mEij∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ,Z(ij) qθ,Z(ij) + mξ(ij)\n\n(cid:19)\n\n,\n\n(6)\n\nwhere qθ,Z(ij) = φ(ij)/Z with learnable Z and ξ is approximately uniform (see Supp. D).\n\nAccording to Thm. 1, NC-t-SNE has the same optimum as t-SNE and can hence be seen as a sampling-based approximation of t-SNE. Indeed, we found that Z in NC-t-SNE and the partition function Z(θ) in t-SNE converge approximately to the same value (Fig. S10).\n\nUMAP’s expected loss function is derived in Damrich & Hamprecht (2021):\n\nLUMAP(θ) = −Eij∼p log (cid:0)qθ(ij)(cid:1) − mEij∼ξ log (cid:0)1 − qθ(ij)(cid:1),\n\n(7)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nwith qθ(ij) = φ(ij) and ξ is approximately uniform, see Supp. D. This is the effective loss function actually implemented in the UMAP algorithm, but note that it has only about m/n of the repulsion compared to the loss stated in the original UMAP paper (McInnes et al., 2018), as shown in B ̈ohm et al. (2022) and Damrich & Hamprecht (2021) (Supp. C).\n\nIn practice, the expectations in UMAP’s and NC-t-SNE’s loss functions are evaluated via sampling, like in Eq. (2). This leads to a fast, O(n), stochastic gradient descent optimization scheme. Both loss functions are composed of an attractive term pulling similar points (edges of the skNN graph) closer together and a repulsive term pushing random pairs of points further apart. Similarly, t-SNE’s loss yields attraction along the graph edges while repulsion arises through the normalization term.\n\n4 FROM NOISE-CONTRASTIVE ESTIMATION TO NEGATIVE SAMPLING\n\nIn this section we work out the precise relationship between NCE and NEG, going beyond prior work (Dyer, 2014; Goldberg & Levy, 2014; Levy & Goldberg, 2014; Ruder, 2016). NEG differs from NCE by its loss function and by the lack of the learnable normalization parameter Z. In our setting, NEG’s loss function amounts to1\n\nLNEG(θ) = −Ex∼p log\n\n(cid:18) qθ(x)\n\n(cid:19)\n\nqθ(x) + 1\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ(x) qθ(x) + 1\n\n(cid:19)\n\n.\n\n(8)\n\nIn order to relate it to NCE’s loss function, our key insight is to generalize the latter by allowing to learn a model that is not equal but proportional to the true data distribution. Corollary 2. Let ̄Z, m ∈ R+. Let ξ have full support and suppose there exist some θ∗ such that qθ∗ = ̄Zp. Then θ∗ is a minimum of the generalized NCE loss function\n\nLNCE\n\n ̄Z (θ) = −Ex∼p log\n\n(cid:18)\n\nqθ(x) qθ(x) + ̄Zmξ(x)\n\n(cid:19)\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ(x) qθ(x) + ̄Zmξ(x)\n\n(cid:19)\n\n(9)\n\nand the only other extrema of LNCE Proof. The result follows from Thm. 1 applied to the model distribution ̃qθ := qθ/ ̄Z.\n\nare minima ̃θ which also satisfy q ̃θ = ̄Zp.\n\n ̄Z\n\nDyer (2014) and Ruder (2016) pointed out that for a uniform noise distribution ξ(x) = 1/|X| and as many noise samples as the size of X (m = |X|), the loss functions of NCE and NEG coincide, since mξ(x) = 1. However, the main point of NCE and NEG is to use far fewer noise samples in order to attain a speed-up over MLE. Our Cor. 2 for the first time explains NEG’s behavior in this more realistic setting (m ≪ |X|). If the noise distribution is uniform, the generalized NCE loss function with ̄Z = |X|/m equals the NEG loss function since (|X|/m)mξ(x) = 1. By Cor. 2, any minimum θ∗ of the NEG loss function yields qθ∗ = (|X|/m)p, assuming that there are parameters that make this equation hold. In other words, NEG aims to find a model qθ that is proportional to the data distribution with the proportionality factor |X|/m which is typically huge. This is different from NCE, which aims to learn a model equal to the data distribution.\n\nChoosing m ≪ |X| does not only offer a computational speed-up but is necessary when optimizing NEG for a neural network with SGD, like we do in Sec.7. Only one single mini-batch is passed through the neural network during each iteration and is thus available for computing the loss. Hence, all noise samples must come from the current mini-batch and their number m is upper-bounded by the mini-batch size b. Mini-batches are typically much smaller than |X|. Thus, this common training procedure necessitates m ≪ |X| highlighting the relevance of Cor. 2. While NCE uses a model qθ/Z with learnable Z, we can interpret NEG as using a model qθ/ ̄Z with fixed and very large normalization constant ̄Z = |X|/m. As a result, qθ in NEG needs to attain much larger values to match the large ̄Z. This can be illustrated in the setting of neighbor embeddings. Applying NEG to the neighbor embedding framework yields an algorithm that we call ‘Neg-t-SNE’. Recall that in this setting, |X| = (cid:0)n (cid:1) is the number of pairs of points. B ̈ohm et al. (2022) found empirically that t-SNE’s partition function Z(θ) is typically between 50n and 100n, while in Neg-t-SNE, ̄Z = O(n2) is much larger for modern big datasets. To attain the larger values of φ(ij) required by NEG, points that are connected in the skNN graph have to move much closer\n\n2\n\n1We focus on the loss function, ignoring Mikolov et al. (2013)’s choices specific to word embeddings.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) UMAP, no annealing (b) UMAP with annealing (c) Neg-t-SNE, no ann.\n\n(d) Neg-t-SNE with ann.\n\nFigure 2: Embeddings of the MNIST dataset with UMAP and Neg-t-SNE with and without learning rate annealing in our implementation. UMAP does not work well without annealing because it implicitly uses the diverging 1/d2 ij kernel in NEG, while Neg-t-SNE uses the more numerically stable Cauchy kernel (Sec. 6). UMAP’s reference implementation also requires annealing, see Figs. S1a, d.\n\ntogether in the embedding than in t-SNE. Indeed, using our PyTorch implementation of Neg-t-SNE on the MNIST dataset, we confirmed that Neg-t-SNE (Fig. 1d) produced more compact clusters than t-SNE (Fig. 1f). See Supp. K and Alg. S1 for implementation details.\n\nWe emphasize that our analysis only holds because NEG has no learnable normalization parameter Z. If it did, then such a Z could absorb the term ̄Z in Eq. (9), leaving the parameters θ∗ unchanged.\n\n5 NEGATIVE SAMPLING SPECTRUM\n\nVarying the fixed normalization constant ̄Z in Eq. (9) has important practical effects that lead to a whole spectrum of embeddings in the NE setting. The original NEG loss function Eq. (8) corresponds to Eq. (9) with ̄Z = |X|/m. We still refer to the more general case of using an arbitrary ̄Z in Eq. (9) as ‘negative sampling’ and ‘Neg-t-SNE’ in the context of neighbor embeddings. Figs. 1a–e show a spectrum of Neg-t-SNE visualizations of the MNIST dataset for varying ̄Z. Per Cor. 2, higher values of ̄Z induce higher values for qθ, meaning that points move closer together. Indeed, the scale of the embedding decreases for higher ̄Z as evident from the scale bars in the bottom-right corner of each plot. Moreover, clusters become increasingly compact and then even start to merge. For lower values of ̄Z the embedding scale is larger and clusters are more spread out. Eventually, clusters lose almost any separation and start to overlap for very small ̄Z. Cor. 2 implies that the partition function (cid:80) x qθ(x) should grow with ̄Z, and indeed this is what we observed (Fig. 1i). The match between the sum of Cauchy kernels (cid:80) ij φ(ij) and ̄Z was not perfect, but that was expected: The Cauchy kernel is bounded by 1 from above, so values ̄Z > (cid:0)n (cid:1) are not matchable. Similarly, very small values of ̄Z are difficult to match because of the heavy tail of the Cauchy kernel. See Supp. I for a toy example where the match is perfect. By adjusting the ̄Z value, one can obtain Neg-t-SNE embeddings very similar to NC-t-SNE and t-SNE. If the NC-t-SNE loss function Eq. (6) has its minimum at some θ∗ and Z NC-t-SNE, then the Neg-t-SNE loss function Eq. (9) with ̄Z = Z NC-t-SNE is minimal at the same θ∗. We confirmed this experimentally: Setting ̄Z = Z NC-t-SNE, yields a Neg-t-SNE embedding (Fig. 1c) closely resembling that of NC-t-SNE (NCVis) (Fig. 1g). Similarly, setting ̄Z to the partition function Z(θt-SNE), obtained by running t-SNE, yields a Neg-t-SNE embedding closely resembling that of t-SNE (Figs. 1b, f). We used kNN recall and correlation between pairwise distances for quantitative confirmation (Supp. H).\n\n2\n\nstrongly related to the\n\nThe Neg-t-SNE spectrum is attraction-repulsion spectrum of B ̈ohm et al. (2022). They introduced a prefactor (‘exaggeration’) to the attractive term in t-SNE’s loss, which increases the attractive forces, and obtained embeddings similar to our spectrum when varying this parameter. We can explain this as follows. The repulsive term in the NCE loss Eq. (3) has a prefactor m and our spectrum arises from the loss Eq. (9) by varying ̄Z in the term ̄Zm. Equivalently, our spectrum can be obtained by varying the m value (number of noise samples per one skNN edge) while holding ̄Zm fixed (Fig. S8). In other words, our spectrum arises from varying the repulsion strength in the contrastive setting, while B ̈ohm et al. (2022) obtained the analogous spectrum by varying the repulsion strength in the t-SNE setting (see also Supp. B.2).\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n6 UMAP’S CONCEPTUAL RELATION TO t-SNE\n\nOur comparison of NEG and NCE in Sec. 4 allows us for the first time to conceptually relate UMAP and t-SNE. Originally, McInnes et al. (2018) motivated the UMAP algorithm by a specific choice of weights on the skNN graph and the binary cross-entropy loss. Following LargeVis (Tang et al., 2016), they implemented a sampling-based scheme which they referred to as ‘negative sampling’, although UMAP’s loss function Eq. (7) does not look like the NEG loss Eq. (8). Thus, it has been unclear if UMAP actually uses Mikolov et al. (2013)’s NEG. Here, we settle this question: Lemma 3. UMAP’s loss function (Eq. 7) is NEG (Eq. 8) with the parametric model ̃qθ(ij) = 1/d2\n\nij.\n\nProof.\n\nIndeed, qθ(ij) = 1/(1 + d2\n\nij) = (1/d2\n\nij)/(1/d2\n\nij + 1) = ̃qθ(ij)/(cid:0) ̃qθ(ij) + 1(cid:1).\n\nLem. 3 tells us that UMAP uses NEG but not with a parametric model given by the Cauchy kernel. Instead it uses a model ̃qθ, which equals the inverse squared distance between embedding points.\n\nFor large embedding distances dij both models behave alike, but for nearby points they strongly differ: The inverse-square kernel 1/d2 ij). Despite this qualitative difference, we found empirically that UMAP embeddings look very similar to Neg-t-SNE embeddings at ̄Z = |X|/m, see Figs. 1d and h for the MNIST example.\n\nij diverges when dij → 0, unlike the Cauchy kernel 1/(1+d2\n\nit\n\nis instructive to compare the loss terms of Neg-t-SNE and To explain this observation, ij + 1)(cid:1) = log(1 + d2 UMAP: The attractive term amounts to − log (cid:0)1/(d2 ij) for UMAP and − log (cid:2)1/(d2 ij) for Neg-t-SNE, while the repulsive term equals log (cid:0)(1 + d2 ij)(cid:1), respectively. While the attractive terms ij)/(1 + d2 are similar, the repulsive term for UMAP diverges at zero but that of Neg-t-SNE does not (Fig. S7, Supp. B.2). This divergence introduces numerical instability into the optimization process of UMAP.\n\nij + 1) + 1(cid:1)(cid:3) = log(2 + d2 (cid:1) and log (cid:0)(2 + d2\n\nij + 1)/(cid:0)1/(d2\n\nij)/d2\n\nij\n\nWe found that UMAP strongly depends on annealing its learning rate down to zero (Figs. 2a, b). Without it, clusters appear fuzzy as noise pairs can experience very strong repulsion and get catapulted out of their cluster (Fig. 2a). While Neg-t-SNE also benefits from this annealing scheme (Fig. 2d), it produces a very similar embedding even without any annealing (Fig. 2c). Thus, UMAP’s effective choice of the 1/d2 ij kernel makes it less numerically stable and more dependent on optimization tricks, compared to Neg-t-SNE.2 See Supp. E for more details.\n\nOur conclusion is that at its heart, UMAP is NEG applied to the t-SNE framework. UMAP’s sampling-based optimization is much more than a mere optimization trick; it enables us to connect it theoretically to t-SNE. When UMAP’s loss function is seen as an instance of NEG, UMAP does not use the Cauchy kernel but rather the inverse-square kernel. However, this does not make a strong difference due to the learning rate decay. As discussed in Sec. 4, the fixed normalization constant ̄Z in Neg-t-SNE / UMAP is much larger than the learned Z in NC-t-SNE or t-SNE’s partition function. This explains why UMAP pulls points closer together than both NC-t-SNE and t-SNE and is the reason for the typically more compact clusters in UMAP plots (B ̈ohm et al., 2022).\n\n7 CONTRASTIVE NE AND CONTRASTIVE SELF-SUPERVISED LEARNING\n\nContrastive self-supervised representation learning (van den Oord et al., 2018; Tian et al., 2020; He et al., 2020; Chen et al., 2020; Caron et al., 2020) and ‘contrastive neighbor embeddings’ (a term we suggest for NC-t-SNE, Neg-t-SNE, UMAP, etc.) are conceptually very similar. The key difference is that the latter use a fixed kNN graph to find pairs of similar objects, while the former rely on data augmentations or context to generate pairs of similar objects on the fly. Other differences include the representation dimension (∼ 128 vs. 2), the use of a neural network for parametric mapping, the flavor of contrastive loss (InfoNCE vs. NCE / NEG), and the number of noise samples m. However, these other differences are not crucial, as we establish here with our unified PyTorch framework.\n\nAs an example, we demonstrate that t-SNE can also be optimized using the InfoNCE loss, resulting in ‘InfoNC-t-SNE’ (Supp. K and Alg. S1). Its result on MNIST was similar to that of NC-t-SNE (Figs. 3b, c). For the default number of noise samples, m = 5, the embeddings of both\n\n2Recently, a pull request to UMAP’s GitHub repository effectively changed the kernel of parametric UMAP\n\nto the Cauchy kernel, in order to overcome numerical instabilities via an ad hoc fix, see Supp. E.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Neg-t-SNE\n\n(b) NC-t-SNE\n\n(c) InfoNC-t-SNE\n\n(d) Param. Neg-t-SNE\n\n(e) Param. NC-t-SNE (f) Param. InfoNC-t-SNE\n\nFigure 3: NE embeddings of MNIST are qualitatively similar in the non-parametric (top) and parametric settings (bottom). We used our PyTorch framework with m = 5 and batch size b = 1024.\n\nalgorithms were visibly different from t-SNE proper (Fig. 1f). Recent work in self-supervised learning (Chen et al., 2020) reports improved performance for more noise samples, in agreement with the theory of Gutmann & Hyv ̈arinen (2012) and Ma & Collins (2018). We observed that both NC-t-SNE and InfoNC-t-SNE with m = 500 approximated t-SNE much better (Figs. S18k and S19k). Like t-SNE, but unlike the m = 5 setting, m = 500 required early exaggeration to prevent cluster fragmentation (Figs. S18 and S19). We discuss InfoNC-t-SNE’s relation to TriMap in Supp. F.\n\nNext, we used our PyTorch framework to obtain parametric versions of all contrastive NE algorithms discussed here (NC-t-SNE, InfoNC-t-SNE, Neg-t-SNE, UMAP). We used a fully connected neural network with three hidden layers as a parametric RD → Rd mapping and optimized its parameters using Adam (Kingma & Ba, 2015) (Supp. K). We used batch size b = 1024 and sampled all m negative samples from within the batch; the dataset was shuffled each epoch before batching. Using all four loss functions, we were able to get parametric embeddings of MNIST that were qualitatively similar to their non-parametric versions (Fig. 3). The parametric versions of NCE and InfoNCE produced much larger embeddings than their non-parametric counterparts, however the final loss values were very similar (Figs. S9a, b). For NCE, the larger scale of the parametric embedding was compensated by a smaller learned normalization parameter Z, so that both parametric and non-parametric versions were approximately normalized (Fig. S9c). Our parametric UMAP implementation is very similar to that of Sainburg et al. (2021). But our parametric, approximate t-SNE implementations strongly differ from the parametric t-SNE of van der Maaten (2009), which constructed separate kNN graphs within each batch and optimized the vanilla t-SNE loss function.\n\nNeighbor embeddings methods achieve impressive results with only few noise samples, while existent common practice in the self-supervised learning literature (Bachman et al., 2019; Chen et al., 2020; He et al., 2020) recommends very large m. As a proof of concept, we demonstrate that using only m = 16 non-curated (Wu et al., 2017; Roth et al., 2020) noise samples can suffice for imagebased self-supervised learning. We used a SimCLR setup (Chen et al., 2020) to train representations of the CIFAR-10 dataset (Krizhevsky, 2009) using a ResNet18 (He et al., 2016) backbone, a fixed batch size of b = 1024 and varying number of noise samples m. Other implementation details\n\nTable 1: Test set classification accuracies on CIFAR-10 representations obtained with InfoNCE loss and batch size b = 1024 saturate already at a low number of noise samples m, common for neighbor embeddings. We used the ResNet18 output H ∈ R512 and report mean ± std across three seeds.\n\nm = 2\n\nm = 16\n\nm = 128\n\nm = 512\n\nm = 2b − 2\n\nkNN classifier Linear classifier\n\n86.9 ± 0.2 90.7 ± 0.2\n\n91.7 ± 0.1 93.1 ± 0.2\n\n92.0 ± 0.3 93.3 ± 0.3\n\n92.2 ± 0.2 93.2 ± 0.3\n\n91.9 ± 0.1 93.3 ± 0.1\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nfollow Chen et al. (2020)’s CIFAR-10 setup (Supp. K.4). The classification accuracy, measured on the ResNet output, saturates already at m = 16 noise samples (Tab. 1). Note that, different from the original SimCLR setup, we decoupled the batch size from the number of noise samples. Recent work found conflicting evidence when varying m for a fixed batch size (Mitrovic et al., 2020; Nozawa & Sato, 2021; Ash et al., 2022). Future research is needed to perform more systematic benchmarks into the importance of m. Here, we present these results only as a proof of principle that a similarly low m as in neighbor embeddings (default m = 5) can work in a SimCLR setting.\n\n8 DISCUSSION AND CONCLUSION\n\nIn this work, we studied the relationship between two popular unsupervised learning methods, noisecontrastive estimation (NCE) and negative sampling (NEG). We focused on their application to neighbor embeddings (NE) because this is an active and important application area, but also because NEs allow to directly visualize the NCE / NEG outcome and to form intuitive understanding of how different algorithm choices affect the result. Our study makes three conceptual advances.\n\nFirst, we showed that NEG replaces NCE’s learnable normalization parameter Z by a large constant ̄Z, forcing NEG to learn a scaled data distribution. While not explored here, this implies that NEG can be used to learn probabilistic models and is not only applicable for embedding learning as previously believed (Dyer, 2014; Ruder, 2016; Le-Khac et al., 2020). In the NE setting, NEG led to the method Neg-t-SNE that differs from NCVis / NC-t-SNE (Artemenkov & Panov, 2020) by a simple switch from the learnable to a fixed normalization constant. We argued that this can be a useful hyperparameter because it moves the embedding along the attraction-repulsion spectrum similar to B ̈ohm et al. (2022) and hence can either emphasize more discrete structure with higher repulsion (Fig. S16) or more continuous structure with higher attraction (see Figs. S13–S14 for developmental single-cell datasets). Our quantitative evaluation in Supp. H corroborates this interpretation. We believe that inspection of several embeddings along the spectrum can guard against over-interpreting any single embedding. Exploration of the spectrum does not require specialized knowledge. For UMAP, we always have ̄Z UMAP = (cid:0)n (cid:1)/m; for t-SNE, B ̈ohm et al. (2022) found that the partition function Z t-SNE typically lies in [50n, 100n]. Our PyTorch package allows the user to move along the spectrum with a slider parameter s such that s=0 and s=1 correspond to ̄Z = Z t-SNE and ̄Z = ̄Z UMAP, respectively, without a need for specifying ̄Z directly.\n\n2\n\nA caveat is that Thm. 1 and Cor. 2 both assume the model to be rich enough to perfectly fit the data distribution. This is a strong and unrealistic assumption. In the context of NEs, the data distribution is zero for most pairs of points, which is impossible to match using the Cauchy kernel. Nevertheless, the consistency of MLE and thus t-SNE also depends on this assumption. Even without it, the gradients of MLE, NCE, and NEG are very similar (Supp. B). Finally, we validated our Cor. 2 in a toy setting in which its assumptions do hold (Supp. I).\n\nSecond, we demonstrated that UMAP, which uses NEG, differs from Neg-t-SNE only in UMAP’s implicit use of a less numerically stable similarity function. This isolates the key aspect of UMAP’s success: Instead of UMAP’s appraised (Oskolkov, 2022; Coenen & Pearce, 2022) high-dimensional similarities, the refined Cauchy kernel, or its stated cross-entropy loss function, it is the application of NEG that lets UMAP perform well and makes clusters in UMAP plots more compact and connections between them more continuous than in t-SNE, in agreement with B ̈ohm et al. (2022). To the best of our knowledge, this is the first time UMAP’s and t-SNE’s loss functions, motivated very differently, have been conceptually connected.\n\nThird, we argued that contrastive NEs are closely related to contrastive self-supervised learning (SSL) methods such as SimCLR (Chen et al., 2020) which can be seen as parametric InfoNC-t-SNE for learning representations in S128 based on the unobservable similarity graph implicitly constructed via data augmentations. We feel that this connection has been underappreciated, with the literature on NEs and on self-supervised contrastive learning staying mostly disconnected. Exceptions are the concurrent works of Hu et al. (2023) and B ̈ohm et al. (2023). They propose to use t-SNE’s Cauchy kernel for SSL. Instead, we bridge the gap between NE and SSL with our InfoNC-t-SNE, as well as with parametric versions of all considered NE methods, useful for adding out-of-sample data to an existing visualization. Moreover, we demonstrated that the feasibility of few noise samples in NE translates to the SimCLR setup. We developed a concise PyTorch framework optimizing all of the above, which we hope will facilitate future dialogue between the two research communities.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe thank Philipp Berens for comments and support as well as James Melville for discussions.\n\nThis research was funded by the Deutsche Forschungsgemeinschaft (DFG, Germany’s Research Foundation) via SFB 1129 (240245660; S.D., F.A.H.) as well as via Excellence Clusters 2181/1 “STRUCTURES” (390900948; S.D., F.A.H.) and 2064 “Machine Learning: New Perspectives for Science” (390727645; J.N.B., D.K.), by the German Ministry of Education and Research (T ̈ubingen AI Center, 01IS18039A; J.N.B., D.K.), and by the Cyber Valley Research Fund (D.30.28739; J.N.B.).\n\nThe authors thank the International Max Planck Research School for Intelligent Systems (IMPRSIS) for supporting J.N.B.\n\nETHICS STATEMENT\n\nOur work analyses connections between popular contrastive learning methods and in particular visualization methods. Visualization methods are fundamental in exploratory data analysis, e.g., in computational biology. Given the basic nature of visualization it can of course potentially be used for malicious goals, but we expect mostly positive societal effects such as more reliable exploration of biological data.\n\nSimilarly, contrastive self-supervised learning has the potential to overcome the annotation bottleneck that machine learning faces. This might free countless hours of labelling for more productive use but could also enable institutions with sufficient resources to learn from larger and larger datasets, potentially concentrating power.\n\nREPRODUCIBILITY\n\nAll of our results are reproducible. We included proofs for the theoretical statement Cor. 2 and those in Supp. B and D. The datasets used in the experiments including preprocessing steps are described in Supp. J. We discuss the most important implementation details in the main text, e.g., in Sec. 7. An extensive discussion of our implementation is included in Supp. K and in Alg. S1.\n\nOur code and instructions for how to reproduce the experiments are publicly available. We separated the implementation of contrastive neighbor embeddings from the scripts and notebooks needed to reproduce our results, to provide dedicated repositories for people interested in using our method and people wanting to reproduce our results. The contrastive neighbor embedding implementation can be found at https://github.com/berenslab/contrastive-ne and the scripts and notebooks for reproducing our results at https://github.com/hci-unihd/cl-tsne-umap. The relevant commits in both repositories are tagged iclr2023.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nEhsan Amid and Manfred K Warmuth. TriMap: Large-scale Dimensionality Reduction Using\n\nTriplets. arXiv preprint arxiv: 1910.00204, 2019.\n\nAleksandr Artemenkov and Maxim Panov. NCVis: Noise Contrastive Approach for Scalable Visu-\n\nalization. In Proceedings of The Web Conference 2020, pp. 2941–2947, 2020.\n\nJordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra. Investigating the Role of Negatives in Contrastive Representation Learning. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pp. 7187–7209, 2022.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning Representations by Maximizing Mutual Information across Views. In Advances in Neural Information Processing Systems, volume 32, pp. 15535–15545, 2019.\n\nRandall Balestriero and Yann LeCun. Contrastive and Non-Contrastive Self-Supervised Learning In Advances in Neural Information\n\nRecover Global and Local Spectral Embedding Methods. Processing Systems, 2022.\n\nEtienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan W Newell. Dimensionality reduction for visualizing single-cell data using UMAP. Nature Biotechnology, 37(1):38–44, 2019.\n\nJan Niklas B ̈ohm, Philipp Berens, and Dmitry Kobak. Attraction-Repulsion Spectrum in Neighbor\n\nEmbeddings. Journal of Machine Learning Research, 23(95):1–32, 2022.\n\nJan Niklas B ̈ohm, Philipp Berens, and Dmitry Kobak. Unsupervised visualization of image datasets\n\nusing contrastive learning. In International Conference on Learning Representations, 2023.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised Learning of Visual Features by Contrasting Cluster Assignments. In Advances in Neural Information Processing Systems, volume 33, pp. 9912–9924, 2020.\n\nDavid M Chan, Roshan Rao, Forrest Huang, and John F Canny. t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data. In 2018 30th International Symposium on Computer Architecture and High Performance Computing, pp. 330–338. IEEE, 2018.\n\nTara Chari, Joeyta Banerjee, and Lior Pachter. The Specious Art of Single-Cell Genomics. bioRxiv,\n\n2021.\n\nBenjamin Charlier, Jean Feydy, Joan Alexis Glaun`es, Franc ̧ois-David Collin, and Ghislain Durif. Kernel Operations on the GPU, with Autodiff, without Memory Overflows. Journal of Machine Learning Research, 22(74):1–6, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In Proceedings of the International Conference on Machine Learning, Proceedings of Machine Learning Research, pp. 1597–1607, 2020.\n\nAndy Coenen and Adam Pearce. A deeper dive into UMAP theory. https://pair-code. github.io/understanding-umap/supplement.html, 2022. Accessed: 2022-05-11.\n\nSebastian Damrich and Fred A Hamprecht. On UMAP’s True Loss Function. In Advances in Neural\n\nInformation Processing Systems, volume 34, pp. 5798–5809, 2021.\n\nAaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, volume 27, 2014.\n\nAndrew Draganov, Tyrus Berry, Jakob Rødsgaard Jørgensen, Katrine Scheel Nellemann, Ira Assent, and Davide Mottin. GiDR-DUN; Gradient Dimensionality Reduction–Differences and Unification. arXiv preprint arXiv:2206.09689, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nChris Dyer. Notes on Noise Contrastive Estimation and Negative Sampling. arXiv preprint arxiv:\n\n1410.8251, 2014.\n\nPeter Eisenmann. Fast Visualization of High-Dimensional Data via Parallelized UMAP on GPUs.\n\nMaster’s thesis, Karlsruhe Institue of Technology, 2019.\n\nYoav Goldberg and Omer Levy. word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling\n\nWord-Embedding Method. arXiv preprint arxiv: 1402.3722, 2014.\n\nMichael U Gutmann and Aapo Hyv ̈arinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 297–304, 2010.\n\nMichael U Gutmann and Aapo Hyv ̈arinen. Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics. Journal of Machine Learning Research, 13(2), 2012.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729–9738, 2020.\n\nGeoffrey E Hinton and Sam Roweis. Stochastic Neighbor Embedding.\n\nIn Advances in Neural\n\nInformation Processing Systems, volume 15, pp. 857–864, 2002.\n\nTianyang Hu, Zhili Liu, Fengwei Zhou, Wenjia Wang, and Weiran Huang. Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding. In International Conference on Learning Representations, 2023.\n\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\n\nLimits of Language Modeling. arXiv preprint arxiv: 1602.02410, 2016.\n\nSabina Kanton, Michael James Boyle, Zhisong He, Malgorzata Santel, Anne Weigert, F ́atima Sanch ́ıs-Calleja, Patricia Guijarro, Leila Sidow, Jonas Simon Fleck, Dingding Han, et al. Organoid single-cell genomic atlas uncovers human-specific features of brain development. Nature, 574(7778):418–422, 2019.\n\nDiederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proceedings\n\nof the International Conference on Learning Representations, pp. 1–15, 2015.\n\nDmitry Kobak and Philipp Berens. The art of using t-SNE for single-cell transcriptomics. Nature\n\nCommunications, 10(1):1–14, 2019.\n\nDmitry Kobak and George C Linderman. Initialization is critical for preserving global data structure\n\nin both t-SNE and UMAP. Nature Biotechnology, pp. 1–2, 2021.\n\nDmitry Kobak, George Linderman, Stefan Steinerberger, Yuval Kluger, and Philipp Berens. HeavyTailed Kernels Reveal a Finer Cluster Structure in t-SNE Visualisations. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 124–139. Springer, 2019.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University\n\nof Toronto, 2009.\n\nPhuc H Le-Khac, Graham Healy, and Alan F Smeaton. Contrastive Representation Learning: A\n\nFramework and Review. IEEE Access, 8:193907–193934, 2020.\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n\nOmer Levy and Yoav Goldberg. Neural Word Embedding as Implicit Matrix Factorization.\n\nIn\n\nAdvances in Neural Information Processing Systems, volume 27, pp. 2177–2185, 2014.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nGeorge C Linderman, Manas Rachh, Jeremy G Hoskins, Stefan Steinerberger, and Yuval Kluger. Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data. Nature Methods, 16(3):243–245, 2019.\n\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Warm Restarts. Pro-\n\nceedings of the International Conference on Learning Representations, pp. 1–16, 2017.\n\nZhuang Ma and Michael Collins. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3698–3707, 2018.\n\nLeland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold Approximation and\n\nProjection for Dimension Reduction. arXiv preprint arXiv:1802.03426, 2018.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed RepresenIn Advances in Neural Information\n\ntations of Words and Phrases and their Compositionality. Processing Systems, volume 26, pp. 3111–3119, 2013.\n\nJovana Mitrovic, Brian McWilliams, and Melanie Rey. Less can be more in contrastive learning. In\n\nProceedings on ”I Can’t Believe It’s Not Better!” at NeurIPS Workshops, pp. 70–75, 2020.\n\nAshwin Narayan, fBonnie Berger, and Hyunghoon Cho. Assessing single-cell transcriptomic variability through density-preserving data visualization. Nature Biotechnology, pp. 1–10, 2021.\n\nCorey J Nolet, Victor Lafargue, Edward Raff, Thejaswi Nanditale, Tim Oates, John Zedlewski, and Joshua Patterson. Bringing UMAP Closer to the Speed of Light with GPU Acceleration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 418–426, 2021.\n\nKento Nozawa and Issei Sato. Understanding Negative Samples in Instance Discriminative SelfIn Advances in Neural Information Processing Systems,\n\nsupervised Representation Learning. volume 34, pp. 5784–5797, 2021.\n\nNikolay Oskolkov. How Exactly UMAP Works. https://towardsdatascience.com/\n\nhow-exactly-umap-works-13e3040e1668, 2022. Accessed: 2022-05-11.\n\nJonathan S Packer, Qin Zhu, Chau Huynh, Priya Sivaramakrishnan, Elicia Preston, Hannah Dueck, Derek Stefanik, Kai Tan, Cole Trapnell, Junhyong Kim, Robert H. Waterston, and John I. Murray. A lineage-resolved molecular atlas of C. elegans embryogenesis at single-cell resolution. Science, 365(6459):1265–1274, 2019.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, pp. 8024– 8035, 2019.\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\n\nPavlin G. Poliˇcar, Martin Straˇzar, and Blaˇz Zupan. openTSNE: a modular Python library for t-SNE\n\ndimensionality reduction and embedding. bioRxiv, 2019.\n\nKarsten Roth, Timo Milbich, and Bjorn Ommer. PADS: Policy-Adapted Sampling for Visual Similarity Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6568–6577, 2020.\n\nSebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. http://ruder.\n\nio/word-embeddings-softmax, 2016. Accessed: 2022-05-17.\n\nTim Sainburg, Leland McInnes, and Timothy Q Gentner. Parametric UMAP Embeddings for Rep-\n\nresentation and Semisupervised Learning. Neural Computation, 33(11):2881–2907, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nBenjamin Szubert, Jennifer E Cole, Claudia Monaco, and Ignat Drozdov. Structure-preserving\n\nvisualisation of high dimensional single-cell datasets. Scientific Reports, 9(1):1–10, 2019.\n\nJian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. Visualizing large-scale and highIn Proceedings of the 25th International Conference on World Wide Web,\n\ndimensional data. pp. 287–297, 2016.\n\nClanuwat Tarin, B Mikel, K Asanobu, L Alex, Y Kazuaki, and H David. Deep Learning for Classical Japanese Literature. In Proceedings of 2018 Workshop on Machine Learning for Creativity and Design (Thirty-second Conference on Neural Information Processing Systems), volume 3, 2018.\n\nJoshua B Tenenbaum, Vin De Silva, and John C Langford. A Global Geometric Framework for\n\nNonlinear Dimensionality Reduction. Science, 290(5500):2319–2323, 2000.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive Multiview Coding. In Proceedings of\n\nthe European Conference on Computer Vision, pp. 776–794. Springer, 2020.\n\nA ̈aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Pre-\n\ndictive Coding. arXiv e-prints, pp. arXiv–1807, 2018.\n\nLaurens van der Maaten. Learning a Parametric Embedding by Preserving Local Structure.\n\nIn Artificial Intelligence and Statistics, Proceedings of Machine Learning Research, pp. 384–391, 2009.\n\nLaurens van der Maaten. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine\n\nLearning Research, 15(1):3221–3245, 2014.\n\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine\n\nLearning Research, 9(11):2579–2605, 2008.\n\nDaniel E. Wagner, Caleb Weinreb, Zach M. Collins, James A. Briggs, Sean G. Megason, and Allon M. Klein. Single-cell mapping of gene expression landscapes and lineage in the zebrafish embryo. Science, 360(6392):981–987, 2018.\n\nYingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMAP, and PaCMAP for Data Visualization. Journal of Machine Learning Research, 22:1–73, 2021.\n\nChao-Yuan Wu, R Manmatha, Alexander J Smola, and Philipp Krahenbuhl. Sampling Matters in Deep Embedding Learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2840–2848, 2017.\n\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised Feature Learning via NonParametric Instance Discrimination. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3733–3742, 2018.\n\nZhirong Yang, Irwin King, Zenglin Xu, and Erkki Oja. Heavy-Tailed Symmetric Stochastic Neighbor Embedding. In Advances in Neural Information Processing Systems, pp. 2169–2177, 2009.\n\nZhirong Yang, Jaakko Peltonen, and Samuel Kaski. Scalable optimization of neighbor embedding\n\nfor visualization. In International Conference on Machine Learning, pp. 127–135, 2013.\n\nZhirong Yang, Yuwei Chen, Denis Sedov, Samuel Kaski, and Jukka Corander. Stochastic cluster\n\nembedding. Statistics and Computing, 33(1):12, 2023.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nSUPPLEMENTARY TEXT\n\nA PROBABILISTIC FRAMEWORKS OF NCE AND INFONCE\n\nIn this section we recap the probabilistic frameworks of NCE (Gutmann & Hyv ̈arinen, 2010; 2012) and InfoNCE (Jozefowicz et al., 2016; van den Oord et al., 2018).\n\nA.1 NCE\n\nIn NCE the unsupervised problem of parametric density estimation is turned into a supervised problem in which the data samples need to be identified in a set S containing N data samples s1, . . . , sN and m times as many noise samples t1, . . . , tmN drawn from a noise distribution ξ, which can be (but does not have to be) the uniform distribution. In other words, we are interested in the posterior probability P(y|x) of element x ∈ S coming from the data (y = data) rather than from the noise distribution (y = noise). The probability of sampling x from noise, P(x|noise), is just the noise distribution ξ, and similarly P(x|data) is the data distribution p. Since the latter is unknown, it is replaced by the model qθ. Since S contains m times as many noise samples as data samples, the prior class probabilities are P(data) = 1/(m + 1) and P(noise) = m/(m + 1). Thus, the unconditional probability of an element of S is P(x) = (qθ(x) + mξ(x))/(m + 1). The posterior probability for classifying some given element x of S as data rather than noise is thus\n\nP(data|x) =\n\nP(x|data)P(data) P(x)\n\n=\n\nqθ(x) qθ(x) + mξ(x)\n\n.\n\n(10)\n\nNCE optimizes the parameters θ by maximizing the log-likelihood of the posterior class distributions, or, equivalently, by minimizing the negative log-likelihoods. This is the same as a sum over binary cross-entropy losses (Eq. (2) in the main text):\n\n(cid:34)\n\nθ∗ = arg min\n\n−\n\nθ\n\n(cid:18)\n\nlog\n\nN (cid:88)\n\ni=1\n\nqθ(si) qθ(si) + mξ(si)\n\n(cid:19)\n\n−\n\nmN (cid:88)\n\ni=1\n\n(cid:18)\n\nlog\n\n1 −\n\nqθ(ti) qθ(ti) + mξ(ti)\n\n(cid:19)(cid:35)\n\n.\n\n(11)\n\nIn expectation, we have the loss function (Eq. (4) in the main text)\n\nLNCE(θ) = −Es∼p log\n\n(cid:18)\n\nqθ(s) qθ(s) + mξ(s)\n\n(cid:19)\n\n− mEt∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ(t) qθ(t) + mξ(t)\n\n(cid:19)\n\n.\n\n(12)\n\nSince qθ(x)/(cid:0)qθ(x) + mξ(x)(cid:1) = 1/\n\n(cid:104)\n\n1 +\n\n(cid:16)\n\nqθ(x)/(cid:0)mξ(x)(cid:1)(cid:17)−1(cid:105)\n\n, NCE’s loss function can also be\n\nseen as binary logistic regression loss function with log\n\n(cid:16) qθ(x)\n\n(cid:17)\n\nmξ(x)\n\nas the input to the logistic function:\n\nLNCE(θ) = −Es∼p log\n\n(cid:18)\n\n(cid:18)\n\nσ\n\nlog\n\n(cid:18) qθ(s) mξ(s)\n\n(cid:19)(cid:19)(cid:19)\n\n− mEt∼ξ log\n\n(cid:18)\n\n(cid:18)\n\n1 − σ\n\nlog\n\n(cid:19)(cid:19)(cid:19)\n\n(cid:18) qθ(t) mξ(t)\n\n,\n\n(13)\n\nwhere σ(x) = 1/(cid:0)1 + exp(−x)(cid:1) is the logistic function.\n\nA.2\n\nINFONCE\n\nAgain, we are casting the unsupervised problem of density estimation as a supervised classification problem. We consider a tuple of m + 1 samples T = (x0, . . . , xm) one of which comes from the data and the rest from the noise distribution. Instead of classifying each sample independently as noise or data (as in Sec. A.1), here we are interested in identifying the position of the single data sample. This allows us to see the problem as a multi-class classification problem with m + 1 classes.\n\nLet Y be the random variable that holds the index of the data sample. A priori, we have P(Y = k) = 1/(m + 1) for all k = 0, . . . , m. Moreover, conditioned on sample k coming from the data distribution, all other samples must come from the noise distribution, i.e., we\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nhave P(xi|Y = k) = ξ(xi) for i ̸= k. As the data distribution is unknown, we model it with P(xk|Y = k) = qθ(xk) as in Sec. A.1. This yields the likelihood of tuple T given the data index Y = k\n\nP(T |Y = k) = qθ(xk)\n\n(cid:89)\n\ni̸=k\n\nξ(xi) =\n\nqθ(xk) ξ(xk)\n\nMarginalizing over Y , we obtain\n\nm (cid:89)\n\ni=0\n\nξ(xi).\n\nP(T ) =\n\n1 m + 1\n\nm (cid:89)\n\ni=0\n\nξ(xi)\n\nm (cid:88)\n\nk=0\n\nqθ(xk) ξ(xk)\n\n.\n\nFinally, we can compute the posterior via Bayes’ rule as\n\nP(Y = k|T ) =\n\nP(T |Y = k)P(Y = k) P(T )\n\n=\n\nqθ(xk) ξ(xk)\n\n(cid:80)m\n\ni=0\n\nqθ(xi) ξ(xi)\n\n=\n\nqθ(xk) i=0 qθ(xi)\n\n(cid:80)m\n\n,\n\n(14)\n\n(15)\n\n(16)\n\nwhere the last equality only holds for the uniform noise distribution. The InfoNCE loss is the crossentropy loss with respect to the true position of the data sample, i.e., in expectation and for uniform ξ it reads:\n\nLInfoNCE(θ) = −\n\nE x∼p x1,...,xm∼ξ\n\nlog\n\n(cid:18)\n\nqθ(x) qθ(x) + (cid:80)m\n\ni=1 qθ(xi)\n\n(cid:19)\n\n.\n\n(17)\n\nSimilar to how the NCE loss can be seen as binary logistic regression loss function (Sec. A.1), the InfoNCE loss can be viewed as multinomial logistic regression loss function with the terms\n\nlog\n\n(cid:17)\n\n(cid:16) qθ(xi) ξ(xi)\n\nentering the softmax function.\n\nB GRADIENTS\n\nB.1 GRADIENTS OF MLE, NCE, AND NEG\n\na\n\nHere, we compute the gradients of MLE, NCE, and NEG, elaborating the discusFollowing the discussion in Secs. 3.2 and 4, sion in Artemenkov & Panov (2020). function consider we Z(θ) = (cid:80) x′ φθ(x′) for MLE, a model qθ,Z(x) = φθ(x)/Z with learnable normalization parameter Z for NCE, and a model qθ, ̄Z(x) = φθ(x)/ ̄Z with fixed normalization constant ̄Z for NEG. We show the results both for general ̄Z and noise distribution ξ and for the default value ̄Z = |X|/m and a uniform noise distribution ξ(x) = 1/|X|.\n\nφθ(x)/Z(θ) with\n\nnormalized model\n\npartition\n\nqθ(x)\n\n=\n\nThus, the losses are\n\nLMLE(θ) = −Ex∼p log (cid:0)qθ(x)(cid:1),\n\nLNCE(θ, Z) = −Ex∼p log\n\nLNEG(θ, ̄Z) = −Ex∼p log\n\n(cid:18)\n\n(cid:18)\n\nqθ,Z(x) qθ,Z(x) + mξ(x) qθ, ̄Z(x) qθ, ̄Z(x) + mξ(x)\n\n(cid:19)\n\n(cid:19)\n\n− mEx∼ξ log\n\n− mEx∼ξ log\n\n(cid:18)\n\n(cid:18)\n\n1 −\n\n1 −\n\nqθ,Z(x) qθ,Z(x) + mξ(x) qθ, ̄Z(x) qθ, ̄Z(x) + mξ(x)\n\n(cid:19)\n\n(cid:19)\n\n,\n\n,\n\nLNEG(θ) = −Ex∼p log\n\n(cid:18) φθ(x)\n\n(cid:19)\n\nφθ(x) + 1\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nφθ(x) φθ(x) + 1\n\n(cid:19)\n\n.\n\nFor MLE, we find\n\ndLMLE(θ) dθ\n\n=\n\n=\n\nd dθ\n\nd dθ\n\n(cid:0)−Ex∼p log (cid:0)qθ(x)(cid:1)(cid:1) (cid:32)\n\n(cid:88)\n\n(cid:16)\n\np(x) log (cid:0)φθ(x)(cid:1)(cid:17)\n\n−\n\n+ log\n\n(cid:16) (cid:88)\n\n(cid:17)\n\nφθ(x)\n\n(cid:33)\n\n= −\n\n= −\n\n(cid:88)\n\nx\n\n(cid:88)\n\nx\n\nx\n\n(cid:18) p(x) φθ(x) (cid:18) p(x) qθ(x)\n\n(cid:19)\n\n·\n\ndφθ(x) dθ\n\n−\n\n(cid:80)\n\n1\n\nx φθ(x)\n\n(cid:19)\n\n− 1\n\n·\n\n1 Z(θ)\n\n·\n\ndφθ(x) dθ\n\n.\n\nx\n\n(cid:88)\n\n·\n\nx\n\ndφθ(x) dθ\n\n16\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nPublished as a conference paper at ICLR 2023\n\nFor the NCE loss we will use the templates\n\nd log\n\n(cid:16) f (z)\n\n(cid:17)\n\nf (z)+c\n\ndz\n\n=\n\n(cid:16)\n\nd log\n\n(cid:17)\n\n1 1+c/f (z)\n\ndz\n\nd log(1 + c/f (z)) dz\n\n= −\n\n= −\n\n=\n\n·\n\n1 1 + c/f (z) 1\nf (z)\n\nc f (z) + c\n\n·\n\n·\n\ndf (z) dz\n\n−c\n\nf (z)2 ·\n\ndf (z) dz\n\n,\n\nd log\n\n(cid:16) c\n\n(cid:17)\n\nf (z)+c\n\ndz\n\n= −\n\n= −\n\n= −\n\nd log (f (z)/c + 1) dz\n\ndf (z) dz\n\n1 f (z)/c + 1\n\n1 f (x) + c\n\n·\n\n·\n\n·\n\n1 c\ndf (z) dz\n\nwith some differentiable function f (z) and c independent of z.\n\nFor the NCE loss we get\n\ndLNCE(θ, Z) dθ\n\n=\n\n=\n\n(cid:18)\n\n(cid:18)\n\nd dθ\n\nd dθ\n\n−Ex∼p log\n\n−Ex∼p log\n\n(cid:18)\n\n(cid:18)\n\nqθ,Z(x) qθ,Z(x) + mξ(x)\n\nqθ,Z(x) qθ,Z(x) + mξ(x)\n\n(cid:19)\n\n(cid:19)\n\n− mEx∼ξ log\n\n(cid:18)\n\n1 −\n\nqθ,Z(x) qθ,Z(x) + mξ(x)\n\n− mEx∼ξ log\n\n(cid:18)\n\nmξ(x) qθ,Z(x) + mξ(x)\n\n(cid:19)(cid:19)\n\n(cid:88)\n\n= −\n\n(cid:20)\n\np(x)\n\nx\n\nmξ(x) qθ,Z(x) + mξ(x)\n\n1 qθ,Z(x)\n\ndqθ,Z(x) dθ\n\n+mξ(x)\n\n= −\n\n(cid:18) p(x)\n\n(cid:88)\n\nqθ,Z(x)\n\nx\n\n(cid:21)\n\ndqθ,Z(x) dθ\n\n−1 qθ,Z(x) + mξ(x) mξ(x) qθ,Z(x) + mξ(x)\n\n− 1\n\n(cid:19)\n\n1 Z\n\ndφθ(x) dθ\n\n.\n\n(26)\n\n(27)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\n(cid:19)(cid:19)\n\n(33)\n\n(34)\n\n(35)\n\n(36)\n\n(37)\n\nWe used our templates at the third equality sign with f (z) = qθ,Z(x) and c = mξ(x). The derivation of the gradients for the NEG loss is entirely analogous to that for NCE with qθ,Z(x) replaced by qθ, ̄Z(x).\n\nTogether, the gradients of the three losses are\n\ndLMLE(θ) dθ\n\n= −\n\ndLNCE(θ, Z) dθ\n\ndLNEG(θ, ̄Z) dθ\n\n= −\n\n= −\n\ndLNEG(θ) dθ\n\n= −\n\n(cid:88)\n\nx∈X\n\n(cid:88)\n\nx∈X\n\n(cid:88)\n\nx∈X\n\n(cid:88)\n\nx∈X\n\n(cid:18) p(x) qθ(x) (cid:18) p(x)\n\nqθ,Z(x)\n\n(cid:18) p(x)\n\nqθ, ̄Z(x)\n\n(cid:19)\n\n− 1\n\n1 Z(θ)\n\n(cid:19)\n\n− 1\n\n(cid:19)\n\n− 1\n\nmξ(x) qθ,Z(x) + mξ(x)\n\nmξ(x) qθ, ̄Z(x) + mξ(x)\n\n1 Z\n\n1 ̄Z\n\n(cid:18) p(x) φθ(x)\n\n−\n\nm |X|\n\n(cid:19)\n\n1 φθ(x) + 1\n\ndφθ(x) dθ\n\ndφθ(x) dθ\n\ndφθ(x) dθ\n\ndφθ(x) dθ\n\n,\n\n,\n\n,\n\n.\n\n(38)\n\n(39)\n\n(40)\n\n(41)\n\nThe fractions involving mξ(x) in the gradients of LNCE and LNEG tend to one as m → ∞ highlighting that a higher number of noise samples improves NCE’s approximation of MLE (Gutmann\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n& Hyv ̈arinen, 2010; 2012; Artemenkov & Panov, 2020). All four gradients are the same up to this factor and the different choice of normalization. In all of them, the models qθ, qθ,Z, or qθ, ̄Z try to match the data distribution p according to the first factor in each gradient. This similarity of the gradients holds independent of the assumptions of Thm. 1 and Cor. 2. We optimize all losses with stochastic gradient descent and can therefore expect our analysis to hold even if the assumptions are violated.\n\nB.2 GRADIENTS FOR NEIGHBOR EMBEDDINGS\n\nHere, we rewrite the above gradients for the case of neighbor embeddings. This means that MLE, NCE, and NEG become t-SNE, NC-t-SNE, and Neg-t-SNE, respectively. We include two variants of Neg-t-SNE: for a general value of ̄Z and for the default value ̄Z = (cid:0)n (cid:1)/m used by UMAP. In addition, we also show the gradient of UMAP. Recall that for neighbor embedding, we have X = {ij|1 ≤ i < j ≤ n}, θ = {e1, . . . , en}, and φ(ij) = 1/(1 + ∥ei − ej∥2). The noise distributions are close to uniform, see Supp. D, so that ξ(ij) ≈ (cid:0)n\n\n(cid:1)−1\n\n2\n\n.\n\n2\n\ndLt-SNE dei\n\n= 2\n\ndLNC-t-SNE dei\n\n= 2\n\ndLNeg-t-SNE( ̄Z) dei\n\n= 2\n\ndLNeg-t-SNE dei\n\n≈ 2\n\ndLUMAP dei\n\n= 2\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\n(cid:88)\n\nj̸=i\n\nmξ(ij)\n\nφ(ij)\n\nZ + mξ(ij) (cid:125) (cid:123)(cid:122) →1 for m→∞ (cid:125)(cid:124) mξ(ij)\n\n(cid:123)\n\n(cid:124)\n\n(cid:122)\n\nφ(ij) ̄Z\n\n+ mξ(ij)\n\n1 φ(ij) + 1 (cid:124) (cid:125) (cid:123)(cid:122) ∈[0.5,1]\n\n(cid:18)\n\np(ij) −\n\n(cid:19)\n\nφ(ij) k̸=l φ(kl)\n\n(cid:80)\n\n(cid:18)\n\np(ij) −\n\n(cid:19)\n\nφ(ij) Z\n\n(cid:18)\n\np(ij) −\n\n(cid:18)\n\np(ij) −\n\n(cid:18)\n\np(ij) −\n\n(cid:19)\n\nφ(ij) ̄Z\n\n(cid:19)\n\nφ(ij) (cid:1)/m (cid:0)n 2\n\nφ(ij) (cid:1)/m (cid:0)n 2\n\n(cid:19)\n\n1 1 − φ(ij) (cid:125) (cid:123)(cid:122) (cid:124) numerically unstable for ei ≈ ej\n\nφ(ij)(ej − ei)\n\n(42)\n\nφ(ij)(ej − ei)\n\n(43)\n\nφ(ij)(ej − ei)\n\n(44)\n\nφ(ij)(ej − ei)\n\n(45)\n\nφ(ij)(ej − ei)\n\n(46)\n\nAs above, we see that NC-t-SNE and Neg-t-SNE have terms that go to one for m → ∞. For Neg-t-SNE with default ̄Z, we used the approximation ξ(ij) ≈ (cid:0)n . Here, the corresponding term is restricted to the interval [0.5, 1]. More generally, we see that the repulsive force in Neg-t-SNE scales as 1/ ̄Z. Its default value of n(n − 1)/(2m) is much larger than the typical values of the partition function (cid:80) k̸=l φ(kl) in t-SNE or the learned Z of NC-t-SNE, which are typically in the range [50n, 100n] (B ̈ohm et al., 2022), leading to much smaller repulsion in default Neg-t-SNE and in UMAP than in NC-t-SNE or t-SNE. Finally, the repulsive part of the UMAP gradient contains a term that diverges for ei ≈ ej, leading to φ(ij) ≈ 1, reflecting the discussion of UMAP’s numerical instability in Sec. 6.\n\n(cid:1)−1\n\n2\n\nC UMAP’S LOSS FUNCTION\n\nIn the original UMAP paper, McInnes et al. (2018) define weights μ(ij) ∈ [0, 1] on the skNN graph and state that these shall be reproduced by the low-dimensional similarities φ(ij) by means of a sum of binary cross-entropy loss functions, one for each edge ij\n\n(cid:88)\n\n(cid:104)\n\nμ(ij) log (cid:0)φ(ij)(cid:1) + (cid:0)1 − μ(ij)(cid:1) log (cid:0)1 − φ(ij)(cid:1)(cid:105)\n\n.\n\n−\n\n(47)\n\nij\n\nIndeed, this loss has its minimum at μ(ij) = φ(ij) for all ij. However, it is of course not possible to achieve zero loss for any real-world data using the Cauchy kernel in two dimensions. Experiments show that using this loss function in practice leads to an excess of repulsion and consequently to very\n\n18\n\nPublished as a conference paper at ICLR 2023\n\npoor embeddings (B ̈ohm et al., 2022). The actual UMAP implementation has much less repulsion due to the sampling of repulsive edges, see below.\n\nAs the weights μ(ij) are only supported on the sparse skNN graph, most of the 1 − μ(ij) terms are equal to one. To simplify the loss function, the UMAP paper replaces all 1 − μ(ij) terms by 1, leading to the loss function\n\n(cid:88)\n\n(cid:104)\n\nμ(ij) log (cid:0)φ(ij)(cid:1) + log (cid:0)1 − φ(ij)(cid:1)(cid:105)\n\n.\n\n−\n\n(48)\n\nij\n\nIn the implementation, UMAP samples the repulsive edges, which drastically changes the loss (B ̈ohm et al., 2022) to the effective loss (Damrich & Hamprecht, 2021)\n\n(cid:88)\n\n(cid:104) μ(ij) log (cid:0)φ(ij)(cid:1) +\n\n−\n\nij\n\nm(di + dj) 2n\n\nlog (cid:0)1 − φ(ij)(cid:1)(cid:105)\n\n,\n\n(49)\n\nwhere di = (cid:80) j μ(ij) denotes the degree of node i and the number of negative samples m is a hyperparameter. By default, m = 5. Since di ≈ log(k), the effective loss only has about m log(k)/n of the repulsion in the originally stated loss function. As a result, the μ(ij) are not reproduced in the embedding space by this loss function.\n\nWe rewrite this effective loss function further to fit into our framework. The attractive prefactors μ(ij) sum to (cid:80) ij μ(ij), while the repulsive prefactors add up to m times this factor. Dividing the entire loss function by this term does not change its properties. But then, we can write the ij μ(ij) and ξ(ij) = (cid:0)p(i) + p(j)(cid:1)/2n prefactors as probability distributions p(ij) = μ(ij)/ (cid:80) using p(i) = (cid:80)\n\nj p(ij). With this, we can write the effective loss function as\n\n(cid:88)\n\n−\n\np(ij) log (cid:0)φ(ij)(cid:1) − m\n\n(cid:88)\n\nξ(ij) log (cid:0)1 − φ(ij)(cid:1),\n\nij\n\nij\n\nor in the expectation form as\n\n−Eij∼p log (cid:0)φ(ij)(cid:1) − mEij∼ξ log (cid:0)1 − φ(ij)(cid:1),\n\n(50)\n\n(51)\n\nlike we do in Eq. (7).\n\nD NOISE DISTRIBUTIONS\n\nHere we discuss the various noise distributions used by UMAP, NCVis, and our framework. The main claim is that all these noise distributions are sufficiently close to uniform, even though their exact shape depends on the implementation details.\n\nSince our actual implementation, as well as the reference implementations of UMAP and NCVis, considers edges ij and ji separately, we will do so from now on. Hence, there is now a total of E := 2|skNN| edges. We always assume that p(ij) = p(ji) and adding up the probabilities for both directions yields one: (cid:80)n i,j=1 p(ij) = 1. For a given data distribution over pairs of points ij, we define p(i) = (cid:80)n i p(i) = 1. As discussed in Supp. B of Damrich & Hamprecht (2021), the p(i) values are approximately constant when p(ij) is uniform on the skNN graph or proportional to the UMAP similarities.\n\nj=1 p(ij) so that (cid:80)\n\nD.1 NOISE DISTRIBUTIONS OF UMAP AND NCVIS\n\nUMAP’s noise distribution is derived in Damrich & Hamprecht (2021) and reads in our notation (Supp. C)\n\nξ(ij) =\n\np(i) + p(j) 2n\n\n.\n\n(52)\n\nNote that UMAP uses a weighted version of the skNN graph. Still, ξ is close to uniform, see Supp. B of Damrich & Hamprecht (2021).\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nThe noise distribution of NCVis is also close to being uniform and equals (Artemenkov & Panov, 2020):\n\nξ(ij) =\n\np(i) n\n\n.\n\n(53)\n\nThis is a slightly different noise distribution than in UMAP and in particular it is asymmetric. However, we argue that in practice it is equivalent to the same noise distribution as in UMAP. The noise distribution is used in two ways in NCVis: for sampling noise samples and in the posterior class probabilities\n\nP(data|ij) =\n\nqθ,Z(ij) qθ,Z(ij) + mξ(ij)\n\n.\n\n(54)\n\nBoth in the reference NCVis implementation and in ours, for the second role, the noise distribution is explicitly approximated by the uniform one and we use the posterior probabilities\n\nP(data|ij) =\n\nqθ,Z(ij)\n\nqθ,Z(ij) + m 1\n\n2|skNN|\n\n.\n\n(55)\n\nTogether with the symmetry of the Cauchy kernel this implies that the repulsion on the embedding vectors ei and ej from noise sample ij and ji is the same. As a result, the expectation\n\n(cid:32)\n\nEij∼ξ log\n\n1 −\n\n(cid:33)\n\nqθ,Z(ij)\n\nqθ,Z(ij) + m 1\n\n2|skNN|\n\nis the same for ξ(ij) = p(i)/n and for UMAP’s noise distribution\n\nξ(ij) =\n\np(i) + p(j) 2n\n\n.\n\n(56)\n\n(57)\n\nD.2 EFFECT OF BATCHED TRAINING ON THE NOISE DISTRIBUTION\n\nIn our framework, the noise distribution is influenced by the batched training procedure (Alg. S1) because the negative samples can come only from the current training batch.\n\nIn every epoch, we first shuffle the set of directed edges of the skNN graph and then chunk it into batches. To emphasize, the batches consist of directed edges and not of the original data points. For each edge in a batch, we take its head and sample m indices from the heads and tails of all edges in the batch (excluding the already selected head) and use them as tails to form negative sample pairs.\n\nTo obtain a negative sample pair ij, the batch must contain some directed edge ik, providing the head of the negative sample, and some pair lj or jl, providing the tail. We want to derive the expected number of times that a directed edge ij is considered as a negative sample in a batch. For simplicity, let us assume that the number of batches divides the number of directed edges E. As the set of skNN edges is shuffled every epoch, the expected number of pairs ij as negative samples is the same for all batches.\n\nLet us consider a batch B of size b. We denote by Yrs the random variable that holds the number of times edge rs appears in B. We also introduce random variables Y¬rs = (cid:80) t̸=r Yts and Yr¬s = (cid:80) t̸=s Yrt. Let p(r¬s) := p(¬sr) := p(r) − p(rs). For each occurrence of an i as head of an edge in B, we sample m tails to create negative samples uniformly from all head and tails in B with replacement, but we prevent sampling the identical head i as negative sample tail. If, however, the same node i is part of the other edges in the batch, then it may be sampled and would create a futile negative sample ii. There are m chances for creating a negative sample edge ij for every head i and any occurrence of j in the batch. The number of heads i in the batch is Yij + Yi¬j and the number of occurrences of j is Yij + Yji + Y¬ij + Yj¬i. Since we sample the tail of a negative sample pair uniformly with replacement, any of the occurrences of j has probability 1/(2b−1) to be selected. Hence, the expected number Nij of times that the ordered pair ij with i ̸= j is considered as a negative sample in batch B is\n\nNij = m(Yij + Yi¬j)\n\nYij + Yji + Y¬ij + Yj¬i 2b − 1\n\n.\n\n(58)\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nSince a head i may not choose itself to form a negative sample, the expected number of times that ii appears as negative sample in the batch is\n\nNii = m\n\n(cid:88)\n\nYij\n\nj\n\nYij − 1 + Yi¬j + Yji + Y¬ji 2b − 1\n\n.\n\n(59)\n\nSince the batches are sampled without replacement, the random variables Yrs are distributed according to a multivariate hypergeometric distribution, so that\n\nE(Yrs) = bp(rs), E(Y¬rs) = bp(¬rs),\n\nVar(Yrs) = b\n\nE − b E − 1\n\np(rs)(cid:0)1 − p(rs)(cid:1),\n\nCov(Yrs, Y¬uv) = −b\n\nE − b E − 1\n\np(rs)p(¬uv).\n\n(60)\n\n(61)\n\n(62)\n\n(63)\n\nWe use these expressions and analogous ones together with the symmetries p(rs) = p(sr) to compute (leaving out intermediate algebra steps) the expectation of Nij over the shuffles:\n\nE(Nij) =\n\nmb 2b − 1\n\n(cid:18) E − b E − 1\n\n(cid:18)\n\np(ij) + 2\n\nb −\n\n(cid:19)\n\nE − b E − 1\n\n(cid:19)\n\np(i)p(j)\n\n.\n\n(64)\n\nSince we sample m negative samples for each positive sample and since each batch contains b positive samples, we need to divide E(Nij) by mb to obtain ξ(ij):\n\nξ(ij) =\n\n1 2b − 1\n\n(cid:18) E − b E − 1\n\n(cid:18)\n\np(ij) + 2\n\nb −\n\n(cid:19)\n\nE − b E − 1\n\n(cid:19)\n\np(i)p(j)\n\n.\n\nSimilarly,\n\nE(Nii) =\n\nmb 2b − 1\n\n(cid:18)\n\n−\n\nb − 1 E − 1\n\n(cid:18)\n\np(i) + 2\n\nb −\n\n(cid:19)\n\nE − b E − 1\n\n(cid:19)\n\np(i)2\n\nand hence the noise distribution value for the pair ii is\n\nξ(ii) =\n\n1 2b − 1\n\n(cid:18)\n\n−\n\nb − 1 E − 1\n\n(cid:18)\n\np(i) + 2\n\nb −\n\n(cid:19)\n\n(cid:19)\n\n.\n\np(i)2\n\nE − b E − 1\n\n(65)\n\n(66)\n\n(67)\n\nWe see that the noise distribution depends on the batch size b. This is not surprising: For example, if the batch size is equal to one, the ordered pair ij can only be sampled as a negative sample in the single batch that consists of that pair. Indeed, for b = 1 our formula yields\n\nξ(ij) = p(ij),\n\n(68)\n\nmeaning that the data and the noise distributions coincide. Conversely, if b = E and there is only one batch, we obtain\n\nξ(ij) =\n\n2E 2E − 1\n\np(i)p(j)\n\n(69)\n\nand the noise distribution is close to uniform. For batch sizes between 1 and E the noise distribution is in between these two extremes. For MNIST, E ≈ 1.5 · 106, and in our experiments we used b = 1024. This means that the prefactor of the share of the data distribution is about 0.0005 while that of the near-uniform distribution p(i)p(j) is about 0.9995, so the resulting noise distribution is close to uniform. Note that Thm. 1 and Cor. 2 only require the noise distribution to have full support which is the case for any batch size greater than one.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(a) ζ = 10−3, no annealing\n\n(b) ζ = 10−10, no annealing\n\n(c) ζ = 0, no annealing\n\n(d) ζ = 10−3, with annealing\n\n(e) ζ = 10−10, with annealing\n\n(f) ζ = 0, with annealing\n\nFigure S1: UMAP embeddings of the MNIST dataset, ablating numerical optimization tricks of UMAP’s reference implementation. The learning rate annealing is crucial (bottom row) but safeguarding against divisions by zero in UMAP’s repulsive term Eq. (71) by adding ζ to the denominator has little effect. These experiments were run using the reference implementation, modified to change the ζ value and to optionally switch off the learning rate annealing.\n\nE OPTIMIZATION TRICKS IN UMAP’S REFERENCE IMPLEMENTATION\n\nUMAP’s repulsive term\n\n− log(1 − φ(ij)) = log\n\n(cid:33)\n\n(cid:32)\n\n1 + d2 ij d2 ij\n\n(70)\n\ncan lead to numerical problems if the two points of the negative sample pair are very close. In addition to the learning rate decay, discussed in Sec. 6, UMAP’s implementation uses further tricks to prevent unstable or even crashing training.\n\nIn non-parametric UMAP’s reference implementation, the gradient on embedding position ei exerted by a single sampled repulsive pair ij is actually\n\n2\n\n1 ij + ζ\n\nd2\n\n1 1 + d2 ij\n\n(ej − ei)\n\nwith ζ = 0.001 instead of ζ = 0. This corresponds to the full loss function\n\n−Eij∼p log(φ(ij)) − m\n\n(cid:16)\n\n1 +\n\nζ 1 − ζ\n\n(cid:17)\n\nEij∼ξ log\n\n(cid:18)\n\n1 +\n\n(cid:19)\n\n− φ(ij)\n\n.\n\nζ 1 − ζ\n\n(71)\n\n(72)\n\nHowever, we found that ζ does not have much influence on the appearance of a UMAP embedding. Fig. S1 shows MNIST embeddings obtained using the original UMAP implementation modified to use different values of ζ. Neither a much smaller positive value such as ζ = 10−10 nor setting ζ = 0 substantially changed the appearance of the embedding (even though some runs with ζ = 0 did crash). The learning rate annealing played a much bigger role in how the embedding looked like (Fig. S1, bottom row).\n\nThe reference implementation of parametric UMAP uses automatic differentiation instead of implementing the gradients manually. To avoid terms such as log(0) in the repulsive loss, it clips the argument of the logarithm from below at the value ε = 10−4, effectively using the loss function\n\n(cid:32)\n\n−Eij∼p log\n\nmax\n\n(cid:33)\n\n(cid:111)\n\n(cid:110)\n\nε,\n\n1 1 + d2 ij\n\n(cid:32)\n\n− mEij∼ξ log\n\nmax\n\n(cid:110)\n\nε, 1 −\n\n1 1 + d2 ij\n\n(cid:33)\n\n(cid:111)\n\n.\n\n(73)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n(a) UMAP, ε = 10−4, no ann.\n\n(b) UMAP, ε = 10−10, no ann.\n\n(c) UMAP, ε = 10−4, with ann.\n\n(d) UMAP, ε = 10−10, with ann.\n\n(e) Neg-t-SNE, ε = 10−4, no ann.\n\n(f) Neg-t-SNE, ε = 10−10, no ann.\n\n(g) Neg-t-SNE, ε = 0, no ann.\n\n(h) Neg-t-SNE, ε = 10−4, with ann.(i) Neg-t-SNE, ε = 10−10, with ann.\n\n(j) Neg-t-SNE, ε = 0, with ann.\n\nFigure S2: UMAP and Neg-t-SNE embeddings of the MNIST dataset using different values ε at which we clip arguments to logarithm functions. These experiments were done using our implementation. Varying ε did not strongly influence the appearance of the embedding. But setting ε = 0 led to crashing UMAP runs. Annealing the learning rate is important for UMAP, yet not for Neg-t-SNE.\n\nWe employ a similar clipping in our code whenever we apply the logarithm function. Again, we found that the exact value of ε is not important for our UMAP reimplementation, while using the learning rate annealing is (Fig. S2, top two rows). In the extreme case of setting ε = 0, our UMAP runs crashed. We believe that the reason is that we allow negative sample pairs to be of the form ii, which would not send any gradient, but would lead to a zero argument to the logarithm. The reference implementation of UMAP excludes such negative sample pairs ii.\n\nOur Neg-t-SNE approach does not have any of these problems, as the repulsive term is upper bounded\n\n\n\n− log\n\n1 −\n\n1 1+d2 ij\n\n1 1+d2 ij\n\n+ 1\n\n\n\n = log\n\n(cid:33)\n\n(cid:32)\n\n2 + d2 ij 1 + d2 ij\n\n≤ log(2)\n\n(74)\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nand does not diverge for dij → 0. For this reason, Neg-t-SNE is not very sensitive to the value at which we clip arguments to the logarithm and works even with ε = 0, both with and without learning rate annealing (Fig. S2, bottom two rows).\n\nThe attractive terms in the loss functions do not pose numerical problems in practice due to the heavy tail of the Cauchy kernel. That said, in order to keep different experiments and losses comparable, we used clipping in both the attractive and the repulsive loss terms with ε = 10−10 for all neighbor embedding plots computed with our framework unless otherwise stated.\n\nA recent pull request (#856) to the parametric part of UMAP’s reference implementation proposed another way to ameliorate the numerical instabilities. The clipping of the arguments to the logarithm was replaced with a sigmoid of the logarithm of the Cauchy kernel, so that the attractive and repulsive terms become\n\n− log\n\n(cid:16)\n\nmax (cid:0)ε, φ(ij)(cid:1)(cid:17)\n\n→ − log\n\n(cid:18)\n\n(cid:16)\n\nσ\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)(cid:19)\n\n,\n\n− log\n\n(cid:16)\n\nmax (cid:0)ε, 1 − φ(ij)(cid:1)(cid:17)\n\n(cid:18)\n\n→ − log\n\n1 − σ\n\n(cid:16)\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)(cid:19)\n\n,\n\n(75)\n\n(76)\n\nwhere σ(x) = 1/(cid:0)1 + exp(−x)(cid:1) is the sigmoid function. This change can seem drastic as, e.g., for φ(ij) = 1 we have max (cid:8)ε, φ(ij)(cid:9) = 1, but σ shows that this turns the loss function precisely into our Neg-t-SNE loss function since\n\n= 1/2. But unravelling the definitions\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nlog (cid:0)φ(ij)(cid:1)(cid:17)\n\nσ\n\n=\n\n(cid:16)\n\n1 + exp\n\n1\n\n− log (cid:0)φ(ij)(cid:1)(cid:17) =\n\n1\n\n1 + φ(ij)−1 =\n\nφ(ij) φ(ij) + 1\n\n.\n\n(77)\n\nSo, in order to overcome the numerical problems incurred by UMAP’s implicit choice of 1/d2 ij as similarity kernel, the pull request suggested a fix that turns out to be equivalent to negative sampling using the Cauchy kernel. We encourage this change to UMAP as it makes its loss function equivalent to our Neg-t-SNE and thus also conceptually more related to t-SNE. We also suggest to implement it in the non-parametric case.\n\nF TRIMAP AND INFONC-t-SNE\n\nThe neighbor embedding method TriMap (Amid & Warmuth, 2019) also employs a contrastive technique to compute the layout. TriMap considers triplets (i, j, k), where i and j are data points that are more similar than i and k. It moves the embeddings ei and ej closer together and pushes ei and ek further apart. While a full investigation of TriMap is beyond the scope of our work, here we argue that it bears some similarity to InfoNC-t-SNE with m = 1.\n\nFirst, we describe TriMap in more detail. Most of the triplets (i, j, k) consist of nearest neighbors i and j and a random point k. By default, 12 nearest neighbors j are chosen for each i, using a modified Euclidean distance. For each such pair (i, j) additional distinct points k (by default 4) are chosen randomly from the set of all points excluding i and all the chosen j. This leads to 48 triplets (i, j, k) for each point i. Additionally, several (by default 3) triplets are formed for each i with random j and k only ensuring that the similarity between i and j is larger than between i and k. Thus, for each point i there are in total 51 triplets (i, j, k) leading to a grand total of 51n triplets. These triplets are formed ahead of the layout optimization phase and kept fixed throughout. TriMap also assigns a weight wijk to each triplet. This weight is larger for triplets in which i and j are much more similar to each other than i and k.\n\nIn our notation, TriMap’s loss function can be written as\n\n(cid:88)\n\nijk∈triplets\n\nwijk\n\nφ(ik) φ(ij) + φ(ik)\n\n= const −\n\n(cid:88)\n\nijk∈triplets\n\nwijk\n\nφ(ij) φ(ij) + φ(ik)\n\n.\n\nThis loss is very similar to the loss of InfoNC-t-SNE with m = 1:\n\n(cid:18)\n\nφ(ij) φ(ij) + φ(ik)\n\n(cid:19)\n\n.\n\nlog\n\n− E\n\nij∼p k∼U\n\n24\n\n(78)\n\n(79)\n\nPublished as a conference paper at ICLR 2023\n\n(a) TriMap default\n\n(b) TriMap Eucl. kNN\n\n(c) TriMap no random\n\n(d) TriMap no weights\n\n(e) InfoNC-t-SNE m = 1\n\nFigure S3: Ablating design choices of TriMap on the MNIST dataset. Simplifying TriMap leads to an embedding similar to that of InfoNC-t-SNE with m = 1. The biggest difference results from the omission of random triplets in panel c.\n\nTable S1: Quality of TriMap and InfoNC-t-SNE. Means and standard deviations over three runs.\n\nkNN recall\n\nSpearman correlation\n\nTriMap (default) TriMap (kNN) TriMap (no random) TriMap (no weights) InfoNC-t-SNE (m = 1) InfoNC-t-SNE (m = 5)\n\n0.098 ± 0 0.099 ± 0 0.101 ± 0 0.099 ± 0 0.085 ± 0.001 0.104 ± 0\n\n0.245 ± 0.005 0.232 ± 0.003 0.355 ± 0.004 0.381 ± 0.004 0.381 ± 0.006 0.365 ± 0.005\n\nThe differences are: the absence of log; the presence of weights; the presence of random triplets; nearest neighbors being based not exactly on Euclidean metric; and fixed triplets during optimization. In the following we modify the reference TriMap implementation and ablate these differences in order to see which ones are more or less important (Figure S3). We measure the quality of all embeddings using two metrics (Tab. S1): as a global metric we use the Spearman correlation between high- and low-dimensional pairwise distances between 5000 randomly chosen points (Kobak & Berens, 2019) and as a local metric we use kNN recall with k = 15 (Kobak & Berens, 2019).\n\nFirst, we modified TriMap to use the 15 nearest neighbors in Euclidean metric. This did not change the visual appearance of the MNIST embedding (Figs. S3a, b). Next, we additionally omitted the random triplets. The resulting embedding became visually closer to the InfoNC-t-SNE embedding (Fig. S3c). Finally, additionally setting all the weights wijk to one had only a small visual effect (Fig. S3d). Together, these simplifications noticeably improved the global metric compared to the default TriMap, while leaving the local metric almost unchanged (Tab. S1).\n\nSimilar to the experiments of Wang et al. (2021), our ablations made TriMap look very similar to InfoNC-t-SNE with m = 1 (Fig. S3e), suggesting that the remaining differences between them do not play a major role. This refers to the details of the optimization (e.g. fixed triplets vs. randomly sampled triplets in each epoch, full-batch vs. stochastic gradient descent, momentum and adaptive learning rates, etc.), as well as to the presence/absence of the logarithm in the loss function. Since\n\n(cid:16)\n\nlog (cid:0)\n\nd\n\nφ(ij) φ(ij)+φ(ik)\n\n(cid:1)(cid:17)\n\ndθ\n\n=\n\nφ(ij) + φ(ik) φ(ij)\n\nd(cid:0)\n\nφ(ij) φ(ij)+φ(ik) dθ\n\n(cid:1)\n\n,\n\n(80)\n\nthe gradients of TriMap and InfoNC-t-SNE point in the same direction, but are differently scaled. InfoNC-t-SNE prioritizes triplets with high (cid:0)φ(ij) + φ(ik)(cid:1)/φ(ij), i.e. triplets in which the embedding similarity does not respect the triplet well. Note that this is conceptually different from TriMap’s weights wijk which are based on the high-dimensional similarities. However, at least on MNIST, this difference in gradients did not have a major effect (Figs. S3d, e).\n\nNote that our default for InfoNC-t-SNE is m = 5 instead of m = 1. On MNIST, this setting yields a better local score and a similar global score compared to m = 1, and much better scores than the default TriMap (Tab. S1). The work of Ma & Collins (2018) theoretically underpins our observation in Fig. S19 that the higher m gets, the better InfoNC-t-SNE approximates t-SNE.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nG RELATION TO OTHER VISUALIZATION METHODS\n\nG.1\n\nt-SNE APPROXIMATIONS\n\nA vanilla implementation of t-SNE has to compute the partition function Z(θ) and hence scales quadratically with the sample size n. Therefore, nearly all modern t-SNE packages implement approximations. Common are Barnes-Hut-t-SNE (BH-t-SNE) (Yang et al., 2013; van der Maaten, 2014) and FFT-accelerated interpolation-based t-SNE (FIt-SNE) (Linderman et al., 2019). Both methods approximate the repulsive interaction between all pairs of points and the computation of the partition function, achieving complexities O(kn log(n)2d) and O(kn2d), respectively. In fact, all our t-SNE plots are done with the FIt-SNE implementation of openTSNE (Poliˇcar et al., 2019).\n\nThe focus of our work is not primarily on accelerating t-SNE, but on understanding its relation to contrastive neighbor embeddings. Different from the t-SNE approximations above, these do not approximately compute the partition function, but either learn it (NC-t-SNE), use a fixed normalization parameter (Neg-t-SNE and UMAP), or do not require it at all (InfoNC-t-SNE). They only sample nm repulsive interactions per epoch. This cuts their complexity down to O(kmnd). In particular, they scale linearly with the embedding dimension d and can therefore be used for more general representation learning with d ≫ 2, unlike BH-t-SNE and FIt-SNE.\n\nG.2 OTHER CONTRASTIVE t-SNE APPROXIMATIONS\n\nIn addition to NC-t-SNE and InfoNC-t-SNE, there are two other, recent sampling-based strategies to approximate t-SNE, GDR-t-SNE (Draganov et al., 2022) and SCE (Yang et al., 2023). Both sample m repulsive interactions per each attractive interaction. In order to scale these repulsive interactions properly, an estimate of the partition function is computed.\n\nIn GDR-t-SNE, the sum of the Cauchy kernels of the mkn repulsive interactions over each epoch are added up to obtain an estimate of Z t-SNE(θ) · mkn/(cid:0)n(n − 1)(cid:1). Since both the number of repulsive interactions and the estimate of the partition function are decreased by a factor of mk/(n − 1) compared to the vanilla t-SNE, the overall repulsion strength is correct. In contrast, SCE (Yang et al., 2023) initializes its estimate of the partition function with the maximum value n(n − 1) and then updates it using a moving average after each sampled repulsive pair. Both Draganov et al. (2022) and Yang et al. (2023) use m = 1. Their approaches are similar to NCE in that they approximately estimate Z using the sampled noise points, while NCE treats Z as learnable parameter and performs gradient descent on it. Note, however, that the second factor of the NCE gradient in Eq. (39) is not present in the approaches of Draganov et al. (2022) and Yang et al. (2023).\n\nG.3 LARGEVIS\n\nLargeVis (Tang et al., 2016) was, to the best of our knowledge, the first contrastive neighbor embedding method. It uses NEG. Since it was quickly overshadowed by very similar UMAP, we focused the exposition in the main paper on UMAP. Damrich & Hamprecht (2021) computed LargeVis’ effective loss function in closed form. In our notation it reads\n\nLLargeVis(θ) = −Eij∼p log(φ(ij)) − γmEij∼ξ log(1 − φ(ij)). The are some implementation differences between LargeVis and UMAP, but in terms of the loss function, the main difference is the γ factor in front of the repulsive term, which defaults to 7. Therefore, our analysis of UMAP carries over to LargeVis: Its loss is essentially an instance of NEG with inverse square kernel. The additional factor γ = 7 moves it along the attraction-repulsion spectrum towards more repulsion, that is, towards t-SNE.\n\n(81)\n\nG.4 PACMAP\n\nPairwise Controlled Manifold Approximation Projection (PaCMAP) (Wang et al., 2021) is a recent visualization method also based on sampling m repulsive forces per one attractive force (with m = 2 by default). It employs a large number of additional design choices, such as weak attraction between ‘mid-near’ points in addition to attraction between nearest neighbors (which are based on a modified Euclidean distance, as in TriMap), several optimization regimes with dynamically changing loss weights, etc. Nevertheless, the core parts of its loss can be related to (generalized) Neg-t-SNE.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nThe attractive loss in PaCMAP is\n\nd2 + 1 d2 + 1 + c\n\n= 1 − c ·\n\n1 d2 + 1 + c\n\n(82)\n\nwith c = 10 for nearest neighbors (and c = 10 000 for mid-near points but that part of the loss is switched off during the final stage of the optimization). The attractive loss for Neg-t-SNE with uniform noise distribution and ̄Z = c|X|/m is\n\n− log\n\n(cid:18) φ(d)\n\n(cid:19)\n\nφ(d) + c\n\n= − log\n\n(cid:18)\n\n√\n\n(d/\n\n1\n\nc)2 + 1 + c\n\n(cid:19)\n\n.\n\n(83)\n\nThus, the differences are the logarithm and some rescaling of the distances. Note that Neg-t-SNE with c = 10 typically produces very similar result compared to the default c = 1 (cf. Fig 1).\n\nThe repulsive loss in PaCMAP is\n\n1\n\n2 + d2 = 1 −\n\n1 + d2 2 + d2 .\n\n(84)\n\nAgain it is similar to the repulsive loss of Neg-t-SNE, but now for ̄Z = |X|/m, the default negative sampling value also used by UMAP:\n\n(cid:18)\n\n− log\n\n1 −\n\n(cid:19)\n\nφ(d) φ(d) + 1\n\n= − log\n\n(cid:19)\n\n(cid:18)\n\n1 φ(d) + 1\n\n= − log\n\n(cid:18) 1 + d2 2 + d2\n\n(cid:19)\n\n.\n\n(85)\n\nthe difference is just in the logarithm, similar to the connection between TriMap and\n\nHere, InfoNC-t-SNE in Supp. F.\n\nEmpirically, PaCMAP embeddings of high-dimensional data like MNIST look similar to UMAP embeddings, and hence to Neg-t-SNE embeddings, in agreement with our analysis.\n\nH QUANTITATIVE EVALUATION\n\nThis section provides a quantitative evaluation of the Neg-t-SNE spectrum. For each embedding on the spectrum, we compute two metrics. The first metric, kNN recall with k = 15 (Kobak & Berens, 2019), measures the fraction of nearest neighbors in the embedding that are also nearest neighbors in the reference configuration. It is therefore a measure of local quality. The second metric is the Spearman correlation between the pairwise distances in the embedding and in the reference configuration (Kobak & Berens, 2019). To speed up the computation, we consider all pairwise distances between a sample of 5000 points. Since most random pairs of points are not close to each other, this is a measure of global quality.\n\nAs reference configuration, we use three different layouts for each dataset. We compare against the original high-dimensional data to measure how faithful Neg-t-SNE embeddings are. Additionally, we compare to the corresponding t-SNE and UMAP embeddings to investigate for which ̄Z value Neg-t-SNE matches t-SNE and UMAP the best.\n\nWe found that compared to the high-dimensional data, the kNN recall followed an inverse U-shape across the spectrum (Fig. S4a). kNN recall was low for very large and for very small values of ̄Z. It peaked when ̄Z was close to t-SNE’s partition function Z t-SNE and NC-t-SNE’s learned normalization parameter Z NC-t-SNE. At UMAP’s normalization constant ̄Z UMAP the kNN recall was usually lower. This confirms our observation that the local quality of the embedding improves when decreasing ̄Z from ̄Z UMAP to Z t-SNE in agreement with B ̈ohm et al. (2022). The kNN recall for the Kuzushiji-49 dataset at ̄Z = Z t-SNE was very low, compared to other datasets. We suspect that this was due to incomplete convergence (Fig. S17, Tab. S4). Conversely, the Spearman correlation mostly increased with ̄Z (Fig. S4b), which aligns with our finding that higher attraction improves the global layout of the embedding.\n\nWe also computed the kNN recall and the Spearman correlation for the proper t-SNE and UMAP embeddings (not depicted in Figs. S4a, b). The kNN recall was higher for t-SNE embeddings than for Neg-t-SNE at ̄Z = Z t-SNE (see, e.g., Fig. S18l), likely because proper t-SNE considers the repulsive interaction between all points. The metrics for the UMAP embeddings and the Spearman\n\n27\n\nPublished as a conference paper at ICLR 2023\n\ncorrelation for t-SNE were close to the corresponding values for Neg-t-SNE at the respective ̄Z values.\n\nWhen comparing Neg-t-SNE embeddings to the proper t-SNE embedding, the best fit in terms of kNN recall (Fig. S4c) and in terms of Spearman correlation (Fig. S4d) was usually achieved when ̄Z was close to Z t-SNE, in accordance with our theory. Again, the Kuzushiji-49 dataset was an outlier, likely due to convergence issues.\n\nFinally, when comparing Neg-t-SNE embeddings to the UMAP embedding, the highest Spearman correlation was achieved by ̄Z ≈ ̄Z UMAP (Fig. S4f), in agreement with our theoretical predictions. The highest kNN recall was typically achieved at a slightly lower ̄Z (Fig. S4e).\n\nOverall, these experiments provide empirical support for our interpretation of the Neg-t-SNE spectrum as implementing a trade-off between global and local structure preservation, and they confirm the predicted locations of t-SNE- and UMAP-like embeddings on the spectrum.\n\nWe computed the kNN recall and the Spearman correlation also for the embeddings in Figs. S18 and S19, where we varied the number of noise samples m and the initialization method for NC-t-SNE and InfoNC-t-SNE. As expected, we found that higher m improves the kNN recall, but t-SNE proper achieved higher kNN recall still. The Spearman correlation decreased with m when we initialized randomly and did not change much for other initialization methods. Random initialization hurt the Spearman correlation significantly.\n\nI TOY DATASET\n\nCor. 2 predicts that the partition function of a Neg-t-SNE embedding should equal the ̄Z value. In our real-world examples (panels i in Figs. 1, S11–S16) we observed a monotone relationship between them, but not a perfect agreement. As discussed in Sec. 5, this is due to the shape of the Cauchy kernel and the fact that the data distribution is zero for many pairs of points. Here, we consider a toy example for which we observe a perfect match between Neg-t-SNE’s partition function and the ̄Z value, confirming our Cor. 2.\n\nAs we operate with the binary skNN graph, the only possible high-dimensional similarities are zero and one. Due to the heavy tail of the Cauchy kernel, we would like our toy example to have no pairs of points for which the data distribution is zero. Thus, all pairs of points need to have equal high-dimensional similarity. In two dimensions, only up to three points can be placed equidistantly from each other. Therefore, we consider the simple case of placing three points according to the 6 (there are six directed edges) for various values of ̄Z. We Neg-t-SNE loss function with p ≡ 1 keep the Cauchy kernel as similarity function, which has maximum 1/(1 + 02) = 1. Thus, it is not possible to match values above ̄Z = 6, and the three points end up having the same position in the embedding. But for smaller values of ̄Z, the resulting partition function is indeed exactly equal to ̄Z (Fig. S5).\n\nFor this experiment we decreased the batch size to 6 and the learning rate to 0.01, but kept all other hyperparameters at their default values.\n\nJ DATASETS\n\nWe used the well-known MNIST (LeCun et al., 1998) dataset for most of our experiments. We downloaded it via the torchvision API from http://yann.lecun.com/exdb/mnist/. This website does not give a license. But https://keras.io/api/datasets/mnist/ and http://www.pymvpa.org/datadb/mnist.html name Yann LeCun and Corinna Cortes as copyright holders and claim MNIST to be licensed under CC BY-SA 3.0, which permits use and adaptation. The MNIST dataset consists of 70 000 grayscale images, 28 × 28 pixels each, that show handwritten digits.\n\nThe Kuzushiji-49 dataset (Tarin et al., 2018) was downloaded from https://github.com/ rois-codh/kmnist where it is licensed under CC-BY-4.0. It contains 270 912 grayscale images, 28 × 28 pixels each, that show 49 different cursive Japanese characters.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n(a) Relative to the high-dimensional data\n\n(b) Relative to the high-dimensional data\n\n(c) Relative to t-SNE embeddings\n\n(d) Relative to t-SNE embeddings\n\n(e) Relative to UMAP embeddings\n\n(f) Relative to UMAP embeddings\n\nFigure S4: Quantitative evaluation of embeddings on the Neg-t-SNE spectrum with respect to different reference configurations. Means and standard deviations over three random seeds are plotted. Left column: kNN recall, a measure of local agreement. Right column: Spearman correlation, a measure of global agreement. First row: Comparison of Neg-t-SNE embeddings to the highdimensional input data. Second row: Comparison of Neg-t-SNE embeddings to the corresponding t-SNE embedding. Third row: Comparison of Neg-t-SNE embeddings to the corresponding UMAP embedding.\n\nanother\n\nperformed\n\nstandard machine\n\n2009) The SimCLR experiments were the dataset, sklearn.datasets.fetch openml API from https://www.openml.org/search? type=data&sort=runs&id=40927&status=active. Unfortunately, we were not able to find a license for this dataset. CIFAR-10 consists of 60 000 images, 32 × 32 RGB pixels each, depicting objects from five animal and five vehicle classes.\n\nWe downloaded it via\n\nlearning resource.\n\nthe CIFAR-10\n\n(Krizhevsky,\n\non\n\n(2019) was downloaded from https:// The transcriptomic dataset of Kanton et al. www.ebi.ac.uk/arrayexpress/experiments/E-MTAB-7552/, which permits free use. We only used the 20 272 cells in the human brain organoid cell line ‘409b2’. The transcriptomic dataset of Wagner et al. scanpy version from https://kleintools.hms.harvard.edu/paper_websites/wagner_ zebrafish_timecourse2018/mainpage.html. The full dataset at https://www. ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE112294 is free to download and reproduce. The dataset contains gene expressions for 63 530 cells from a developing zebrafish embryo. The downloaded UMIs of both datasets were preprocessed as in (B ̈ohm et al., 2022; Kobak\n\n(2018) was downloaded in its\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nFigure S5: Partition function of Neg-t-SNE embeddings as a function of ̄Z for a toy dataset with three equidistant points. There is a perfect fit between ̄Z and the partition function until ̄Z = 6, which is the largest partition function a set of three points with six directed edges can have under the Cauchy kernel. Beyond this value, all three points overlap in the embedding. Mean with standard deviation over three random seeds is plotted.\n\n& Berens, 2019). After selecting the 1000 most variable genes, we normalized the library sizes to the median library size in the dataset, log-transformed the normalized values with log2(x + 1), and finally reduced the dimensionality to 50 via PCA.\n\nThe transcriptomic dataset of the C. elegans flatworm (Packer et al., 2019; Narayan et al., 2021) was obtained from http://cb.csail.mit.edu/cb/densvis/datasets/ with consent It is already preprocessed to 100 principal of the authors who license it under CC BY-NC 2.0. components.\n\nK IMPLEMENTATION\n\nK.1 PACKAGES\n\nAll contrastive embeddings were computed with our PyTorch (Paszke et al., 2019) implementation of Neg-t-SNE, NC-t-SNE, UMAP, and InfoNC-t-SNE. Exceptions are Fig. 1 and analogous Figs. S11 – S16. There, for panel g we used the reference implementation of NCVis (Artemenkov & Panov, 2020) (with a fixed number of noise samples m, and not the default schedule), and for panel h we used UMAP 0.5. The t-SNE plots were created with the openTSNE (Poliˇcar et al., 2019) (version 0.6.1) package. Similarly, we used the reference UMAP implementation in Fig. S1 and openTSNE in Figs. S18 and S19. The TriMap plots in Supp. F were computed with version 1.1.4 of the TriMap package by Amid & Warmuth (2019).\n\nWe extended these implementations of NCVis, UMAP, and t-SNE to make them accept custom embedding initializations and unweighted skNN graphs and to log various quantities of interest. We always used the standard Cauchy kernel for better comparability.\n\nK.2\n\nINITIALIZATION, EXAGGERATION AND SkNN GRAPH\n\nAll PCAs were computed with sklearn (Pedregosa et al., 2011). We used PyKeOps (Charlier et al., 2021) to compute the exact skNN graph and to handle the quadratic complexity of computing the partition functions on a GPU. The same PCA initialization and skNN graph with k = 15 were used for all embeddings. The skNN graph for MNIST and Kuzushiji-49 was computed on a 50dimensional PCA of the dataset.\n\nWe initialized all embeddings with a scaled version of PCA (save for Figs. S18 and S19). For t-SNE embeddings we rescaled the initialization so that the first dimension has a standard deviation of 0.0001 (as is default in openTSNE), for all other embeddings to a standard deviation of 1. For TriMap, we stuck to its default of scaling the PCA down by a factor of 0.01.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nWe employed some version of ‘early exaggeration’ (van der Maaten & Hinton, 2008) for the first 250 epochs in most non-parametric plots. For t-SNE it is the default early exaggeration of openTSNE. When varying ̄Z in non-parametric Neg-t-SNE, early exaggeration meant using ̄Z = |X|/m for the first 250 epochs (save for Fig. S11). When varying the number of noise samples in Figs. S8, S18 and S19, we still used m = 5 for the first 250 epochs. In Figs. 2, 3, S1, and S2 as well as in all reference NCVis or UMAP plots, we did not use early exaggeration as neither small ̄Z nor high m made it necessary. When we used some form of early exaggeration and learning rate annealing, the annealing to zero took place over the first 250 epochs, was then reset, and annealed again to zero for the remaining, typically 500, epochs.\n\nK.3 OTHER DETAILS FOR NEIGHBOR EMBEDDING EXPERIMENTS\n\nWhen computing logarithms during the optimization of neighbor embeddings, we clip the arguments to the range [10−10, 1], save for Fig. S2, where we varied this lower bound. The lower bound is smaller than in the reference implementation of parametric UMAP, where it is set to 10−4.\n\nOur defaults were a batch size of 1024, linear learning rate annealing from 1 (non-parametric) or 0.001 (parametric) to 0 (save for Figs. 2, S1, and S2), 750 epochs (save for Figs. S10 and S17) and m = 5 noise samples (save for Figs. S8, S10, S18, and S19).\n\nNon-parametric runs were optimized with SGD without momentum and parametric runs with the Adam optimizer (Kingma & Ba, 2015). Parametric runs used the same feed-forward neural net architecture as the reference parametric UMAP implementation. That is, four layers with dimensions input dimension − 100 − 100 − 100 − 2 with ReLU activations in all but the last one. We used the vectorized, 786-dimensional version of MNIST as input to the parametric neighbor embedding methods (and not the 50-dimensional PCA; but the skNN graph was computed in the PCA space for consistency with non-parametric embeddings).\n\nLike the reference NCVis implementation, we used the fractions qθ,Z(x)/(qθ,Z(x) + m) instead of qθ,Z(x)/(cid:0)qθ,Z(x) + mξ(x)(cid:1). This is a mild approximation as the noise distribution is close to uniform. But it means that the model learns a scaled data distribution (cf. Cor. 2), so that we need to multiply the learned normalization parameter Z by n(n − 1) when comparing to t-SNE or checking normalization of the NC-t-SNE model. Similarly, we also approximate the true noise distribution by the uniform distribution for the fractions qθ(x)/(qθ(x) + ̄Zm/|X|) – instead of qθ(x)/(cid:0)qθ(x) + ̄Zmξ(x)(cid:1) – in our Neg-t-SNE implementation.\n\nWe mentioned in Sec. 5 and showed in Fig. S8 that one can move along the attraction-repulsion spectrum also by changing the number of noise samples m, instead of the fixed normalization constant ̄Z. In UMAP’s reference implementation, there is a scalar prefactor γ for the repulsive forces. Theoretically, adjusting γ should also move along the attraction-repulsion spectrum, but setting it higher than 1 led to convergence problems in (B ̈ohm et al., 2022), Fig. A11. When varying our ̄Z, we did not have such issues. For panels i in Figs. 1, S11 – S16 the Neg-t-SNE spectra were computed for ̄Z equal to Z(θt-SNE), Z NC-t-SNE, and n(n−1) m ·x, where x ∈ {5·10−5, 1·10−4, 2·10−4, 5·10−4, . . . , 1·102, 2·102, 5·102}.\n\nK.4 SIMCLR EXPERIMENTS\n\nFor the SimCLR experiments, we trained the model for 1000 epochs, of which we used 5 epochs for warmup. The learning rate during warmup was linearly interpolated from 0 to the initial learning rate. After the warmup epochs, we annealed the learning rate with a cosine schedule (without restarts) to 0 (Loshchilov & Hutter, 2017). We optimized the model parameters with SGD and momentum 0.9. We used the same data augmentations as in Chen et al. (2020) and their recommended batch size of 1024. We used a ResNet18 (He et al., 2016) as the backbone and a projection head consisting of two linear layers (512 − 1024 − 128) with a ReLU activation in-between. The loss was applied to the L2 normalized output of the projection head, but like Chen et al. (2020) we used the output of the ResNet as the representation for the linear evaluation. As similarity function we used the exponential of the normalized scalar product (cosine similarity) and always kept the temperature at 0.5, as suggested in Chen et al. (2020).\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nTable S2: Run time overview for the most compute-heavy experiments\n\nNumber of runs Time per run [min] (mean±SD)\n\nNeg-t-SNE for Figs. 1, S11, S13, S14, S15 Neg-t-SNE for Fig. S16 Neg-t-SNE for Fig. S17 NC-t-SNE (our implementation) for Fig. S10b SimCLR runs for Tab. 1\n\n360 24 3\n3 12\n\n39 ± 4 121 ± 44 3592 ± 44 786 ± 2 721 ± 16\n\nTable S3: Learned normalization parameter for NC-t-SNE and partition function of t-SNE in our experiments. Mean and standard deviation is computed over three random seeds. In our setup t-SNE is deterministic.\n\nZ NC-t-SNE [106] Z(θt-SNE) [106]\n\nMNIST Fig. 1 MNIST without EE Fig. S11 MNIST imbalanced Fig. S12 Human brain organoid Fig. S13 Zebrafish Fig. S14 C. elegans Fig. S15 Kuzushiji-49 Fig. S16\n\n34.3 ± 0.1 34.3 ± 0.1 6.15 ± 0.06 3.57 ± 0.03 30.8 ± 0.1 36.9 ± 0.7 395 ± 3\n\n8.13 6.25 3.12 1.30 7.98 11.7 89.6\n\nThe ResNet was trained on the combined CIFAR-10 train and test sets. When evaluating the accuracy, we froze the backbone, trained the classifier on the train set, and evaluated its accuracy on the test set. We used sklearn’s KNeighborsClassifier with cosine metric and k = 15 neighbors and sklearn’s LogisticRegression classifier with the SAGA solver (Defazio et al., 2014), no regularization penalty, a tolerance of 0.0001, and max iter=1000. Other parameters were left at the default values.\n\nWhen sampling negative samples for a given head, we excluded that head from the candidates for negative samples. We sampled negative samples with replacement. If the desired number of negative samples m equals twice the batch size minus 2 (m = 2b − 2, ‘full-batch repulsion’), we took the entire batch without the current head and its tail as negative samples for that head.\n\nK.5 CODE AVAILABILITY\n\nOur code is publicly available. The PyTorch implementation of contrastive neighbor embeddings can be found at https://github.com/berenslab/contrastive-ne. Details for reproducing the experiments and figures, alongside scripts and notebooks are at https://github. com/hci-unihd/cl-tsne-umap.\n\nK.6 STABILITY\n\nWhenever we reported a metric or show a graph, we ran the experiments for 3 different random seeds and reported the mean ± the standard deviation. When the standard deviation was very small, we omitted it from the main text and report it in Tab. S3. Save for the usually approximate skNN graph computation, t-SNE does not depend on a random seed. As we computed the skNN exactly with PyKeOps (Charlier et al., 2021), t-SNE is deterministic in our framework.\n\nPanels i in Figs. 1, S11 – S16 show the standard deviation as shaded area. Again, the standard deviations are very small and barely visible. The ratio of standard deviation to mean was never larger than 0.006 in panels i of Figs. 1, S11 – S16. Similarly, the standard deviation in Figs. S9 and S10, shown as shaded area, is mostly smaller than the line width.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure S6: Run times of the embedding optimization phase for the MNIST dataset using different batch sizes, training modes, and hardware. Running on GPU is much faster than on CPU; nonparametric runs are faster than parametric runs. As the batch size increases, the run time decreases strongly in all settings.\n\nK.7 COMPUTE\n\nWe ran our neighbor embedding experiments on a machine with 56 Intel(R) Xeon(R) Gold 6132 CPU @ 2.60GHz, 502 GB RAM and 10 NVIDIA TITAN Xp GPUs. The SimCLR experiments were conducted on a SLURM cluster node with 8 cores of an Intel(R) Xeon(R) Gold 5220 CPU @ 2.20GHz and a Nvidia V100 GPU with a RAM limit of 54 GB. Each experiment used one GPU at most.\n\nOur total compute is dominated by the neighbor embedding runs for Figs. 1, S11, S13–S16 and S10b and by the SimCLR experiments. Tab. S2 lists the number of runs and the average run time. We thus estimate the total compute time to be about 646 hours.\n\nOur implementation relies on pure PyTorch and our experiments were conducted on GPUs. We originally kept the batch size for all our experiments at the default of 1024, motivated by Chen et al. (2020) and Sainburg et al. (2021), but eventually noticed that the run time of the visualization experiments depends crucially on the batch size (Fig. S6). Increasing the batch size to 219 decreased the run time of the embedding optimization on MNIST data (without the skNN graph computation) from about 40 min to just above 20 s. For a batch size of at least 216, our implementation was faster than the reference implementations of UMAP and t-SNE (via openTSNE). The latter run only on CPU, while our experiments ran on GPU. Nevertheless, we observed a substantial improvement in run time for higher batch sizes also when running our PyTorch implementation on CPU and also when training a parametric embedding. Note that the parametric setting with full batch gradient descent (batch size 1,500,006 for MNIST’s skNN graph) exceeded our GPU’s memory. On CPU, our implementation is just shy of the performance of UMAP when using 219 attractive pairs per batch. Dedicated CUDA implementations (Chan et al., 2018; Eisenmann, 2019; Nolet et al., 2021) outperform our implementation on GPU. Compared to the Numba-accelerated CPU implementation of UMAP and the CUDA-accelerated GPU implementations, our pure PyTorch implementation arguably strikes a good speed / complexity trade-off, is easier to study and adapt by the machine learning community, and seamlessly integrates non-parametric and parametric settings as well as all four contrastive loss functions.\n\nNote that the change from NC-t-SNE to Neg-t-SNE is as simple as fixing the learnable normalization parameter to a constant, so the original NCVis code written in C++ can easily be adapted to compute Neg-t-SNE. We have not used this for any of the experiments in our paper.\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm S1: Batched contrastive neighbor embedding algorithm input : list of directed skNN graph edges E = [i1j1, . . . , i|E|j|E|]\n\nparameters θ\n\n// embeddings (non-param.)/ network weights\n\n(param.)\n\nnumber of epochs T learning rate η number of noise samples m Cauchy kernel q\n\n// of embeddings (non-param.)/ network output\n\n(param.)\n\nbatch size b loss mode mode normalization constant ̄Z\n\noutput: final embeddings e1, . . . , en\n\n1 if mode = NC-t-SNE then 2 Z = 1 3 for t = 0 to T do\n\n// Learning rate annealing ηt = η · (1 − t\n\nT )\n\n4 5 α = 0 6 while α < |E| do\n\n// default (cid:0)n\n\n(cid:1)/m, required for\n\nmode = Neg-t-SNE\n\n2\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\nL = 0 for β = 1, . . . , b do\n\n// Treat attractive edge iα+βjα+β and noise edges iα+βj− // Sample noise edge tails but omit head of considered edge j− 1 , . . . , j− // Aggregate loss based on mode if mode = Neg-t-SNE then\n\nm ∼ Uniform({iα+1, jα+1, . . . , jα+b}\\{iα+β})\n\nμ\n\nL = L − log\n\nqθ(iα+β jα+β ) qθ(iα+β jα+β )+ ̄Zm/(n 2)\n\n(cid:19)\n\n− (cid:80)m\n\nμ=1 log\n\n(cid:18)\n\n1 −\n\nqθ(iα+β j− μ ) μ )+ ̄Zm/(n 2)\n\nqθ(iα+β j−\n\n(cid:19)\n\nelse if mode = NC-t-SNE then\n\n(cid:16) qθ(iα+β jα+β )/Z\n\nL = L − log\n\nqθ(iα+β jα+β )/Z+m else if mode = InfoNC-t-SNE then\n\nL = L − log\n\nqθ(iα+β jα+β )\n\nqθ(iα+β jα+β )+(cid:80)m\n\nμ=1 qθ(iα+β j− μ )\n\n(cid:19)\n\n(cid:17)\n\n− (cid:80)m\n\nμ=1 log\n\n(cid:16)\n\n1 −\n\nqθ(iα+β j−\n\nqθ(iα+β j−\n\nμ )/Z μ )/Z+m\n\n(cid:17)\n\nelse if mode = UMAP then\n\nL = L − log\n\nqθ(iα+βjα+β)\n\n(cid:17)\n\n− (cid:80)m\n\nμ=1 log\n\n(cid:16)\n\n1 − qθ(iα+βj− μ )\n\n(cid:17)\n\n(cid:18)\n\n(cid:18)\n\n(cid:16)\n\n// Update parameters with SGD (non-param.) θ = θ − ηt · ∇θL if mode = NC-t-SNE then\n\nor Adam (param.)\n\nZ = Z − ηt∇ZL\n\nα = α + b\n\n22 Shuffle E 23 return θ\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nL ADDITIONAL FIGURES\n\nFigure S7: Attractive and repulsive loss terms of UMAP and Neg-t-SNE. The main difference is that UMAP’s repulsive loss diverges at zero challenging its numerical optimization. The attractive terms are log(1 + d2 ij) for UMAP and Neg-t-SNE, respectively, and the repulsive ones are log (cid:0)(1 + d2\n\nij) and log(2 + d2 ij)/d2\n\nij)(cid:1), respectively.\n\n(cid:1) and log (cid:0)(2 + d2\n\nij)/(1 + d2\n\nij\n\n(a) m = 5\n\n(b) m = 50\n\n(c) m = 500\n\n(d) m = 2b − 2 = 2046\n\nFigure S8: Neg-t-SNE embeddings of the MNIST dataset for varying number of noise samples m and using batch size b = 1024. While for NC-t-SNE and InfoNC-t-SNE more noise samples improve the approximation to t-SNE, see Figs. S18 and S19, changing m in Neg-t-SNE moves the result along the attraction-repulsion spectrum (Fig. 1) with more repulsion for larger m. However, the computational complexity of Neg-t-SNE scales with m, so that moving along the spectrum via changing ̄Z is much more efficient. For the first 250 epochs, m was set to 5, to achieve an effect similar to early exaggeration (Supp. K).\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n(a) InfoNC-t-SNE loss\n\n(b) NC-t-SNE loss\n\n(c) NC-t-SNE normalization\n\nFigure S9: (a, b) Loss curves for the parametric and non-parametric InfoNC-t-SNE and NC-t-SNE optimizations leading to Figs. 3b, c, e, and f. While the embedding scale differs drastically between the non-parametric and the parametric run, the loss values are close. (c) Normalization of the model (cid:80) ij φ(ij)/Z for the parametric and non-parametric NC-t-SNE optimizations. The difference in the embedding scale is compensated by a three orders of magnitude change in Z, so that both versions learn approximately normalized models. These experiments used our NC-t-SNE reimplementation.\n\n(a)\n\n(b)\n\nFigure S10: NC-t-SNE learns to have the same partition function (PF) as t-SNE on the MNIST dataset. The higher the number m of noise samples (a) or the longer the optimization (b), the better the match. Both methods used early exaggeration, which for NC-t-SNE meant to start with m = 5 noise samples for the first 250 epochs. The learned normalization parameter Z converged to but did not exactly equal NC-t-SNE’s partition function (cid:80) ij qθ(ij). Nevertheless, it was of the same order of magnitude. Again, the match was better for more noise samples. Since we reinitialized the learnable Z for NC-t-SNE after the early exaggeration phase, there were brief jumps in the partition function and in Z at the beginning of the non-exaggerated phase.\n\n36\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S11: (a – e) Neg-t-SNE embeddings of the MNIST dataset for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = (cid:0)n (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) ij)−1 tries to match ̄Z and grows with it. In contrast to Fig. 1, we did not use early exaggeration here, but initialized the Neg-t-SNE and t-SNE with PCA rescaled so that the first dimension has standard deviation 1 and 0.0001, respectively. This makes the embeddings with small ̄Z values show cluster fragmentation, similar to the t-SNE embedding in (f) without early exaggeration. For very low ̄Z, the Neg-t-SNE embedding in (a) shows very little structure.\n\nij(1 + d2\n\n2\n\n37\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S12: (a – e) Neg-t-SNE embeddings of an imbalanced version of the MNIST dataset for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function NCVis, or |X|/m = (cid:0)n (cid:80) ij)−1 tries to match ̄Z and grows with it. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset was created by randomly removing 10 · c% of the class of digit c, so that the class sizes linearly decrease from digit 0 to digit 9.\n\nij(1 + d2\n\n2\n\n38\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S13: (a – e) Neg-t-SNE spectrum on the developmental single-cell RNA sequencing dataset from Kanton et al. (2019) for various parameters ̄Z. As ̄Z increases, the scale of the embedding decreases and the continuous structure (corresponding to the developmental stage) becomes more apparent, making higher ̄Z more suitable for visualizing continuous datasets (B ̈ohm et al., 2022). The spectrum produces embeddings very similar to those of (f) t-SNE and (g) NCVis when ̄Z equals the partition function of t-SNE or the learned normalization parameter of NCVis. The UMAP embedding in (h) closely resembles the Neg-t-SNE embedding at ̄Z = |X|/m = (cid:0)n (cid:1)/m. (i) The partition function (cid:80) ij)−1 of the Neg-t-SNE embeddings increased with ̄Z. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains 20 272 cells and is colored by the duration of the development. There are ten times fewer cells collected after 10 days than after one month.\n\nij(1 + d2\n\n2\n\n39\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S14: (a – e) Neg-t-SNE spectrum on the single-cell RNA sequencing dataset of a developing zebrafish embryo (Wagner et al., 2018) for various parameters ̄Z. As ̄Z increases, the scale of the embedding decreases and the continuous structure (corresponding to the developmental stage) becomes more apparent, making higher ̄Z more suitable for visualizing continuous datasets (B ̈ohm et al., 2022). The spectrum produces embeddings very similar to those of (f) t-SNE and (g) NCVis when ̄Z equals the partition function of t-SNE or the learned normalization parameter of NCVis. The UMAP embedding in (h) closely resembles the Neg-t-SNE embedding at ̄Z = |X|/m = (cid:0)n (cid:1)/m. (i) The partition function (cid:80) ij)−1 of the Neg-t-SNE embeddings increased with ̄Z. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains 63 530 cells and is colored by the hours post fertilization (hpf). There are ten times fewer cells collected after 8 hours than after 24.\n\nij(1 + d2\n\n2\n\n40\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\nFigure S15: (a – e) Neg-t-SNE spectrum of the single-cell RNA sequencing dataset of the C. elegans flatworm (Packer et al., 2019; Narayan et al., 2021) for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, the scale of the embedding decreases, and more continuous structure becomes apparent. The Neg-t-SNE spectrum produces embeddings very similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or |X|/m = (cid:0)n (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) ij)−1 tries to match ̄Z and grows with it. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains information on 86 024 cells of 37 types indicated by the colors. It is imbalanced with only 25 cells of the least abundant type but 31 375 cells of unknown type (grey).\n\nij(1 + d2\n\n2\n\n41\n\nPublished as a conference paper at ICLR 2023\n\n(a) ̄Z < Z t-SNE\n\n(b) ̄Z = Z t-SNE\n\n(c) ̄Z = Z NCVis\n\n(d) ̄Z = |X|/m\n\n(e) ̄Z > |X|/m\n\n(f) t-SNE\n\n(g) NCVis\n\n(h) UMAP\n\n(i) Partition function\n\n2\n\nFigure S16: (a – e) Neg-t-SNE embeddings of the Kuzushiji-49 dataset (Tarin et al., 2018) for various values of the fixed normalization constant ̄Z. As ̄Z increases, the scale of the embedding decreases, clusters become more compact and separated before eventually starting to merge. The Neg-t-SNE spectrum produces embeddings similar to those of (f) t-SNE, (g) NCVis, and (h) UMAP, when ̄Z equals the partition function of t-SNE, the learned normalization parameter Z of NCVis, or (cid:1)/m used by UMAP, as predicted in Sec. 4–6. (i) The partition function (cid:80) |X|/m = (cid:0)n ij)−1 tries to match ̄Z and grows with it. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m and only switched to the desired ̄Z for the last two thirds of the optimization. The dataset contains 270 912 images of 49 different Japanese characters. The classes are imbalanced with 456 to 7 000 samples per class. We see that a higher level of repulsion than UMAP’s ̄Z = |X|/m helps to visualize the discrete structure of the dataset. The sampling based Neg-t-SNE embedding at ̄Z = Z t-SNE has less structure than the t-SNE embedding. Fig. S17 and Tab. S4 show that the Neg-t-SNE result improves for longer optimization.\n\nij(1+d2\n\n(a) 500 epochs\n\n(b) 1000 epochs\n\n(c) 5000 epochs\n\n(d) 10000 epochs\n\nFigure S17: Neg-t-SNE embeddings of the Kuzushiji-49 dataset (Tarin et al., 2018) for ̄Z = Z t-SNE show more structure when optimized longer. Similar to early exaggeration in t-SNE we started all Neg-t-SNE runs using ̄Z = |X|/m for 250 epochs and only switched to the desired ̄Z for the remaining number of epochs indicated in the subcaptions.\n\n42\n\nPublished as a conference paper at ICLR 2023\n\nTable S4: Longer run times improve the Neg-t-SNE optimization on the Kuzushiji-49 dataset (Tarin et al., 2018) for ̄Z = Z t-SNE. The KL divergence is computed with respect to the normalized model qθ/((cid:80)\n\nij qθ(ij)).\n\nEpochs\n\n500\n\n1000\n\n5000\n\n10000\n\nPartition function [106] Neg-t-SNE loss KL divergence\n\n18.96 ± 0.02 1021 ± 2 5.52 ± 0.01\n\n13.27 ± 0.01 832 ± 2 5.18 ± 0.01\n\n6.93 ± 0.01 630 ± 1 4.76 ± 0.01\n\n5.75 ± 0.01 599 ± 1 4.65 ± 0.00\n\n(a) m = 5 random init\n\n(b) m = 50 random init\n\n(c) m = 500 random init\n\n(d) t-SNE random init\n\n(e) m = 5 PCA init\n\n(f) m = 50 PCA init\n\n(g) m = 500 PCA init\n\n(h) t-SNE PCA init\n\n(i) m = 5 EE\n\n(j) m = 50 EE\n\n(k) m = 500 EE\n\n(l) t-SNE EE\n\nFigure S18: NC-t-SNE (our implementation) on the MNIST dataset for varying number of noise samples m and different starting conditions. Higher number of noise samples m improves the approximation quality to t-SNE (last column). The first row is initialized with isotropic Gaussian noise and the second and the third rows with PCA (both normalized to have standard deviation of one or 0.0001 in the first dimension for NC-t-SNE or t-SNE, respectively). In the third row, the first 250 epochs used m = 5 and the latter used the given m value for NC-t-SNE. This is similar to t-SNE’s early exaggeration that we used in panel l. NC-t-SNE seems to be less dependent on early exaggeration than t-SNE, especially for low m values. Insets show the kNN recall and the Spearman correlation between distances of pairs of points. Higher m increases the kNN recall, while mostly leaving the Spearman correlation unchanged. Random initialization hurts the Spearman correlation, but not the kNN recall.\n\n43\n\nPublished as a conference paper at ICLR 2023\n\n(a) m = 5 random init\n\n(b) m = 50 random init\n\n(c) m = 500 random init\n\n(d) t-SNE random init\n\n(e) m = 5 PCA init\n\n(f) m = 50 PCA init\n\n(g) m = 500 PCA init\n\n(h) t-SNE PCA init\n\n(i) m = 5 EE\n\n(j) m = 50 EE\n\n(k) m = 500 EE\n\n(l) t-SNE EE\n\nFigure S19: InfoNC-t-SNE on the MNIST dataset for varying number of noise samples m and different starting conditions. Higher number of noise samples m improves the approximation quality to t-SNE (last column). The first row is initialized with isotropic Gaussian noise and the second and the third rows with PCA (both normalized to have standard deviation of one or 0.0001 in the first dimension for InfoNC-t-SNE or t-SNE, respectively). In the third row, the first 250 epochs used m = 5 and the latter used the given m value for InfoNC-t-SNE. This is similar to t-SNE’s InfoNC-t-SNE seems to be less dependent on early early exaggeration that we used in panel l. exaggeration than t-SNE, especially for low m values. Insets show the kNN recall and the Spearman correlation between distances of pairs of points. Higher m increases the kNN recall, while mostly leaving the Spearman correlation unchanged. Random initialization hurts the Spearman correlation, but not the kNN recall.\n\n44",
  "translations": [
    "# Summary Of The Paper\n\nThis paper relates tSNE and UMAP, starting from linking noise contrastive estimation and negative sampling. The authors also provided analysis on linking neighbor embedding and self-supervised learning, and it leads to optimizing tSNE with InfoNCE loss. The provided experimental results demonstrates the authors' arguments.\n\n# Strength And Weaknesses\n\nThe paper extensively covers from neighbor embedding to self-supervised learning, and the authors linked those two concepts, which has the originality. Also, the paper is easy to follow, and the experimental results are promising. \n\nHowever, at the same time, the quantitative result seems to be lacking. Are there any other quantitative result that can be shown in the experiment of dimensionality reduction algorithms?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written, and the theoretical soundness is okay. Also, exploring the connection between two major dimensionality reduction algorithms is novel. Finally, the authors also provide their code as supplementary material.\n\nIn the meantime, the authors rely on the appendix too much, and interpretation of the experimental results lack in the main body. I suggest authors to add up more informative interpretation in the main body.\n\n# Summary Of The Review\n\nWhile I’m positive on this paper, I’m not very familiar with experiment parts of dimensionality reduction algorithms. Hence, I’m willing to see other reviewer’s comments.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nN/A",
    "# Summary Of The Paper\nThe paper titled \"FROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING\" by Damrich et al. explores the mathematical relationship between two widely used dimensionality reduction techniques: t-SNE and UMAP. The authors employ contrastive learning concepts, specifically noise-contrastive estimation (NCE) and negative sampling (NEG), to elucidate the differences and similarities between these methods. They propose a new spectrum of embeddings that interpolates between t-SNE and UMAP, showing that UMAP generally produces more compact embeddings. Through empirical validation on various datasets, they demonstrate that adjustments in the normalization constant can influence the quality of embeddings, improving the understanding of local and global structures in high-dimensional data.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by establishing a new connection between NCE and NEG, providing a mathematical characterization of the relationship between t-SNE and UMAP, and introducing a novel spectrum of contrastive neighbor embeddings. The methodology is rigorous and well-justified, and the inclusion of a unified framework in PyTorch enhances its practical applicability. However, the paper could benefit from a more comprehensive discussion of the limitations of the proposed methods and a deeper exploration of the implications of varying the normalization constant in different contexts. Additionally, while the empirical results are promising, further benchmarking against other recent visualization techniques would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The mathematical formulations are precise, aiding in the reader's understanding of the complex relationships discussed. The quality of writing is high, with a logical flow throughout. The novelty of the work lies in its conceptual framework that bridges contrastive learning with dimensionality reduction techniques. The authors ensure reproducibility by providing a public repository with the necessary code and details about datasets and experiments, which is commendable.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of dimensionality reduction by linking t-SNE and UMAP through contrastive learning. The proposed framework and findings provide new insights into the optimization of embeddings. While the work is strong, it could be enhanced by addressing potential limitations and further comparisons with emerging methods.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper investigates the conceptual relationship between two prominent neighbor embedding methods, t-SNE and UMAP, providing a mathematical characterization of their loss functions and insights into contrastive learning methods. The authors propose a generalized negative sampling approach that allows interpolation between t-SNE and UMAP embeddings, which helps mitigate over-interpretation of visualization results. Through experiments on various datasets, including MNIST and zebrafish, the paper demonstrates how different values of a normalization constant affect the compactness and separation of clusters in the embeddings, ultimately highlighting the effectiveness of the proposed approach.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative theoretical insights that deepen the understanding of t-SNE and UMAP, as well as its practical relevance through the provision of a spectrum of embeddings for better data interpretation. The inclusion of an open-source PyTorch implementation enhances reproducibility and invites further community experimentation. However, the paper has limitations, such as its reliance on assumptions about model fit to data distributions, potential computational complexity issues, and sensitivity to hyperparameters, which may hinder the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology, findings, and implications. The quality of the writing is high, with logical flow and thorough explanations. The novelty of connecting t-SNE and UMAP through contrastive learning is significant, and the provision of a unified PyTorch framework adds to the reproducibility of the results. However, the dependence on specific hyperparameter settings may pose challenges for users attempting to replicate or build on the work.\n\n# Summary Of The Review\nOverall, this paper offers valuable theoretical and practical contributions to the field of dimensionality reduction and visualization through its innovative approach to t-SNE and UMAP. While it presents a robust framework and insightful findings, attention should be given to its assumptions and hyperparameter sensitivity in future applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"From t-SNE to UMAP with Contrastive Learning\" by Damrich et al. investigates the relationship between two prominent dimensionality reduction techniques, t-SNE and UMAP, through the lens of contrastive learning. The authors introduce Noise-Contrastive Estimation (NCE) as a means to enhance t-SNE and propose a generalized negative sampling framework, termed Neg-t-SNE, which allows interpolation between the embeddings generated by t-SNE and UMAP. Empirical findings reveal that UMAP produces more compact embeddings compared to t-SNE, and the authors provide a unified PyTorch framework for implementing various contrastive neighbor embedding methods.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by clarifying the mathematical relationship between t-SNE and UMAP, which has been underexplored in the literature. The introduction of Neg-t-SNE offers a novel perspective on how normalization constants can influence the quality of embeddings. However, the paper could benefit from a more in-depth discussion on practical implications and the limitations of the proposed methods. The experimental validation is solid, yet additional datasets could enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, with logical progression from theoretical foundations to experimental results. The novelty lies in the proposed framework and the insights gained from the relationship between the two methods. Reproducibility is supported by detailed descriptions of the experimental methodology and the implementation environment, although sharing code and detailed parameters would further facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, the paper provides a valuable contribution to the understanding of t-SNE and UMAP, offering new methodologies and insights into their relationship through contrastive learning. While the results are promising, further exploration of practical applications and limitations would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"From t-SNE to UMAP with Contrastive Learning\" introduces a novel conceptual framework that connects the visualization techniques of t-SNE and UMAP through the lens of contrastive learning. The authors propose a spectrum of embeddings that enables interpolation between these two techniques, addressing common pitfalls such as over-interpretation in data visualization. They provide a mathematical characterization of distortion due to negative sampling in UMAP, present a PyTorch implementation of their methods, and highlight improvements to UMAP's robustness by remedying its instability. The findings suggest a trade-off between local and global structure preservation in embeddings, offering insights for practitioners in exploratory data analysis.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative conceptual framework and mathematical rigor, which enhance the understanding of t-SNE and UMAP. The introduction of a spectrum of embeddings is particularly noteworthy, as it allows for practical applications in data visualization. However, the complexity of the explanations may alienate practitioners unfamiliar with contrastive learning. Furthermore, the lack of extensive empirical validation across diverse datasets limits the generalizability of the proposed methods. The reliance on strong assumptions in the mathematical models and the performance of the provided implementation compared to existing methods are additional concerns. Lastly, while the exploration of connections to self-supervised learning is valuable, a deeper analysis would significantly strengthen this contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat hindered by the complexity of the concepts, particularly for readers not versed in contrastive learning. The quality of the mathematical characterization and the provided implementation demonstrates a commitment to reproducibility, yet a comprehensive evaluation of its performance is lacking. The novelty of linking t-SNE and UMAP through contrastive learning is a significant contribution, though the practical implications could be further elucidated. Overall, while the paper presents high-quality theoretical insights, its accessibility and practical utility could be improved.\n\n# Summary Of The Review\nThis paper presents a compelling theoretical framework that connects t-SNE and UMAP through contrastive learning, offering valuable insights for data visualization. However, the complexity of the explanations and limited empirical validation may restrict its accessibility and practical application for some practitioners. Overall, the contributions are significant but require more clarity and robustness to maximize their impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel methodology titled \"Neg-t-SNE,\" which connects t-SNE and UMAP through a contrastive learning framework. It leverages noise-contrastive estimation and negative sampling to create a flexible embedding technique that interpolates between these two popular visualization methods. The authors provide a mathematical characterization of the distortions introduced by negative sampling, demonstrating how their method can produce embeddings that balance local and global data structures. Extensive empirical validation across diverse datasets, including MNIST and single-cell RNA-seq, supports the effectiveness of Neg-t-SNE, which is also accompanied by an open-source PyTorch implementation for reproducibility.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating contrastive learning with established visualization techniques, offering a fresh perspective that broadens the understanding of t-SNE and UMAP. The flexibility of the Neg-t-SNE method, particularly the tunable normalization constant, is a notable advantage for practitioners looking for customizable embedding solutions. Comprehensive experiments across various datasets substantiate the method's effectiveness. However, weaknesses include the potential complexity of implementation due to nuanced parameter selection, which could be challenging for less experienced users. The method's sensitivity to parameter choices could also introduce variability in results if not properly managed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the theoretical underpinnings and practical implications accessible to the reader. The quality of the mathematical characterization is commendable, providing a solid foundation for the proposed method. The novelty of linking t-SNE and UMAP through contrastive learning is significant, offering a new lens through which to understand these techniques. The open-source implementation enhances reproducibility, though additional guidance on parameter selection would improve usability.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful contribution to the field of data visualization by linking t-SNE and UMAP through a novel contrastive learning framework. The introduction of Neg-t-SNE, supported by both theoretical and empirical evidence, offers valuable insights and practical tools for researchers and practitioners. However, the complexity of parameter tuning may limit its accessibility to a broader audience.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates adversarial training by establishing a theoretical foundation that connects standard adversarial training with noise-contrastive estimation (NCE). The authors introduce a new framework that allows for the interpolation and extrapolation between these two techniques, clarifying how different loss functions impact robustness and performance against adversarial attacks. Through extensive empirical evaluations on benchmark datasets, the authors demonstrate that their proposed methods significantly enhance model robustness compared to traditional adversarial training approaches.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical depth, as it provides rigorous mathematical insights that deepen the understanding of the mechanisms behind adversarial training. The proposed spectrum of methods offers practical relevance, enabling practitioners to make informed choices based on specific application needs. Additionally, the comprehensive empirical results robustly support the theoretical claims, showcasing significant improvements in adversarial robustness. However, the complexity of implementing the proposed methods may pose challenges for practitioners lacking a strong theoretical background. Furthermore, while the findings are promising, the generalizability across different domains and adversarial attack types remains a concern that warrants further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings in a clear and coherent manner, making it accessible to both theoretical and practical audiences. The quality of the writing is high, and the mathematical formulations are presented with sufficient clarity. The novelty of the work is significant, as it bridges two previously distinct approaches to adversarial training. However, reproducibility may hinge on the complexity of the methods proposed, which could require careful tuning and a deep understanding of the underlying principles.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the understanding of adversarial training by elucidating the connections between different methodologies and providing a flexible framework for their application. The theoretical insights, combined with empirical validations, position this work as a valuable resource for researchers and practitioners aiming to improve the robustness of machine learning models against adversarial threats.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"From t-SNE to UMAP with Contrastive Learning\" presents a novel theoretical framework that connects t-SNE and UMAP through contrastive learning methods, particularly Noise-Contrastive Estimation (NCE) and Negative Sampling (NEG). The authors assert that this connection offers new insights into existing algorithms and allows for the introduction of a spectrum of embeddings, which they claim enhances flexibility in high-dimensional data visualization. While the methodology includes an extensive analysis of embeddings, the findings suggest incremental improvements rather than transformative advancements. The authors provide visual results that indicate superiority over previous methods, although these differences appear marginal.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambitious attempt to clarify the relationship between t-SNE and UMAP, potentially paving the way for new research avenues in data visualization. The introduction of a new framework for contrastive neighbor embeddings may also prove valuable for practitioners. However, the claims of a fundamental paradigm shift and a significant mathematical characterization of distortion seem overstated, as the practical implications of the findings may be limited. The visual results, while presented as superior, do not convincingly support the authors' bold assertions of advancement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow. However, some of the claims, particularly regarding the significance of the findings, lack sufficient empirical backing. The methodology is adequately described, and the availability of the implementation enhances reproducibility, although this aspect alone does not add substantial novelty to the contribution.\n\n# Summary Of The Review\nOverall, the paper presents interesting insights into the connections between t-SNE and UMAP, but the claimed advancements may be more modest than suggested. The findings could refine existing methodologies in data visualization, yet the authors' claims of revolutionary change warrant cautious interpretation.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper investigates the relationship between t-SNE and UMAP, two widely used techniques for high-dimensional data visualization, by introducing a novel framework based on contrastive learning. The authors establish a conceptual linkage between these two methods through noise-contrastive estimation (NCE) for t-SNE and negative sampling (NEG) for UMAP. The findings indicate that UMAP tends to produce more compact embeddings with tighter clusters compared to t-SNE, primarily due to the impact of negative sampling. The paper includes experimental results that illustrate how varying the normalization constant in their proposed Neg-t-SNE affects the compactness and distinctiveness of clusters, revealing that higher values of the normalization constant lead to less distinct embeddings. Furthermore, the paper provides a PyTorch implementation for public use, facilitating further research in this area.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to connecting t-SNE and UMAP through the lens of contrastive learning, which provides a deeper understanding of how different loss functions affect embedding structures. The empirical findings, while initially promising, reveal significant alterations upon revision, raising concerns about the robustness and generalizability of the results. The visualizations and quantitative evaluations, although informative, also suffered from inconsistencies that may detract from the overall impact of the contributions. The paper does well in addressing practical aspects by providing an implementation, which is a positive addition for reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a logical structure that guides the reader through the concepts and findings. However, the revisions to the results may confuse readers regarding the reliability of the claims made. The novelty of connecting t-SNE and UMAP via contrastive learning is a significant contribution, although the empirical results require careful interpretation due to the altered findings. The availability of a PyTorch implementation enhances reproducibility, though the changing results may pose challenges for future work.\n\n# Summary Of The Review\nOverall, the paper presents a compelling framework for understanding the relationship between t-SNE and UMAP, with notable methodological advancements. However, the significant revisions to empirical findings raise concerns about the reliability and clarity of the results, which may limit the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a unified theoretical framework that connects t-SNE and UMAP through their respective loss functions, emphasizing the stability and convergence properties of these methods. It claims to characterize the distortion introduced by negative sampling in UMAP, highlighting its advantages over t-SNE. The authors propose a spectrum of embeddings that allows for interpolation between these methods based on varying normalization constants, and they draw parallels between insights from contrastive learning and neighbor embedding techniques. The methodology involves mathematical characterization and experimental validation on specific datasets, predominantly MNIST.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its attempt to unify two popular dimensionality reduction techniques, providing theoretical insights and a novel perspective on their underlying mechanics. However, it presents several weaknesses: it assumes direct comparability of non-convex optimization problems and overlooks the context-dependence of distortion. The reliance on fixed normalization constants could undermine generalizability, and the proposed spectrum of embeddings might mislead practitioners regarding the nature of these methods. Additionally, the focus on specific datasets raises concerns about the broader applicability of the findings, and the implications of initialization strategies are not sufficiently addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its exposition, but some concepts, particularly those regarding the spectrum of embeddings and their assumptions, could benefit from further clarification. While it presents a novel theoretical framework, concerns regarding the assumptions made—such as the uniform noise distribution and the efficacy of learning rate annealing—may challenge the robustness and reproducibility of the results. The discussion on computational efficiency is notably absent, which is critical for real-world applications.\n\n# Summary Of The Review\nOverall, the paper offers a valuable theoretical contribution to the understanding of t-SNE and UMAP, but it is hindered by several assumptions that limit the generalizability and practical applicability of its findings. The lack of consideration for computational efficiency also raises concerns about the utility of the proposed methods in larger datasets.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel framework that connects t-SNE and UMAP through contrastive learning, enhancing the understanding of their conceptual relationship. It introduces a generalization of negative sampling to improve embedding quality and explores a spectrum of embeddings to balance local and global structures. The findings suggest that UMAP can be seen as a specific instance of negative sampling, leading to significant insights into clustering behavior and opening avenues for further research in dimensionality reduction techniques.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive theoretical framework that elucidates the connections between established dimensionality reduction methods, particularly t-SNE and UMAP. The introduction of a spectrum of embeddings based on normalization constants is a notable contribution, as it provides practical insights for practitioners in choosing appropriate visualization techniques. However, a potential weakness lies in the heavy theoretical emphasis, which may limit accessibility for practitioners who are less familiar with the underlying mathematical concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it easy to follow the authors' arguments. The quality of the writing is high, supported by a thorough literature review and solid theoretical grounding. The novelty is significant, particularly in the way it connects different concepts in dimensionality reduction and contrastive learning. The authors have demonstrated a commitment to reproducibility by providing detailed datasets, preprocessing steps, and code repositories, which enhances the paper's credibility.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of dimensionality reduction by establishing connections between t-SNE and UMAP through a contrastive learning framework. While the theoretical depth is impressive, it may pose challenges for broader accessibility. Nonetheless, the findings pave the way for future research and practical applications in data visualization.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper proposes a novel framework for enhancing the robustness of machine learning models against adversarial attacks. The authors introduce an innovative training methodology that integrates adversarial examples directly into the training process, allowing models to learn more effectively from these challenging instances. The paper presents extensive empirical results demonstrating that the proposed approach outperforms several state-of-the-art methods in terms of both robustness and accuracy on benchmark datasets.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance**: The focus on adversarial robustness is timely and addresses critical challenges in the deployment of machine learning models in real-world applications.\n2. **Novelty**: The proposed methodology is a significant advancement over traditional adversarial training techniques, offering a fresh perspective on model training.\n3. **Empirical Validation**: The experimental results are convincing and provide a clear demonstration of the method's effectiveness across various datasets.\n\n**Weaknesses:**\n1. **Evaluation Depth**: While the results are promising, the evaluation could benefit from more diverse datasets and additional metrics to assess performance comprehensively.\n2. **Methodological Clarity**: Some components of the methodology lack detailed explanations, which could pose challenges for reproducibility.\n3. **Discussion of Limitations**: The paper does not sufficiently address the limitations of the proposed approach or potential scenarios where it may fail, which is crucial for guiding future research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and generally clear, allowing readers to follow the authors' arguments and methodologies with ease. The writing quality is high, but certain sections could use more detail, particularly regarding implementation specifics. The novelty of the proposed approach is notable, presenting a fresh angle on adversarial training. However, the lack of comprehensive methodological details raises concerns about reproducibility, as other researchers may struggle to replicate the results without clear guidelines.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to improving adversarial robustness in machine learning models, supported by solid empirical evidence. However, to maximize its impact, the authors should enhance the evaluation and provide more detailed methodological descriptions to aid reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"From t-SNE to UMAP with Contrastive Learning\" explores the conceptual connection between two widely used dimensionality reduction techniques: t-SNE and UMAP. The authors propose that both methods can be understood through the lens of contrastive learning, establishing that t-SNE can be optimized using noise-contrastive estimation (NCE) while UMAP utilizes negative sampling (NEG). They introduce a generalized approach to negative sampling that allows for interpolation between the two techniques, enabling a more balanced representation of local and global structures within high-dimensional data. The findings suggest that leveraging this generalized method can reduce misinterpretation risks in visualizations. A PyTorch implementation is provided to facilitate further research and application.\n\n# Strength And Weaknesses\nThe paper makes significant contributions to the understanding of the relationship between t-SNE and UMAP, addressing a gap in the literature regarding their differing embeddings and interpretations. The introduction of a generalized negative sampling method is a notable strength, as it provides a practical framework for exploring various embedding strategies. However, the paper could be critiqued for its potential lack of empirical validation across diverse datasets, which may limit the generalizability of its claims. Furthermore, while the theoretical foundations are compelling, the extent to which they translate into practical improvements in specific applications remains to be fully evaluated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers with a background in machine learning and data visualization. The authors effectively connect theoretical insights with practical implications, which enhances the paper's overall quality. The novelty lies not only in the proposed connections between existing methods but also in the introduction of a flexible approach to negative sampling. The availability of a PyTorch implementation enhances reproducibility, allowing other researchers to build on the authors' work.\n\n# Summary Of The Review\nOverall, the paper provides valuable insights into the relationship between t-SNE and UMAP, supported by a strong theoretical framework and a practical implementation. While the contributions are significant, the need for further empirical validation across diverse contexts may limit the immediate applicability of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"From t-SNE to UMAP with Contrastive Learning\" by Damrich et al. explores the connections between t-SNE and UMAP, two popular dimensionality reduction techniques. The authors present a novel perspective that relates these methods through contrastive learning principles, particularly focusing on noise-contrastive estimation (NCE) and negative sampling (NEG). Key contributions include clarifying the relationship between t-SNE and UMAP, proposing a spectrum of contrastive neighbor embeddings, and providing a public PyTorch implementation for these methods. Experimental results demonstrate the effectiveness of the proposed framework, highlighting the importance of normalization constants in clustering and data representation.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive analysis of the theoretical underpinnings of t-SNE and UMAP, which enhances the understanding of their respective strengths and weaknesses. The establishment of a connection between these methods through contrastive learning is particularly noteworthy, as it provides a new lens through which to evaluate these embeddings. However, a potential weakness is the reliance on theoretical insights without extensive empirical validation across diverse datasets. While the experiments are promising, further validation on additional datasets and real-world applications would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to a broad audience interested in dimensionality reduction techniques. The quality of writing is high, with a logical flow from introduction to conclusion. The novelty of the approach, particularly the connection through contrastive learning, adds significant value to the field. The authors provide a PyTorch implementation, enhancing the reproducibility of their results, although additional documentation or examples could further aid users in applying the methods.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in understanding and applying dimensionality reduction techniques by connecting t-SNE and UMAP through contrastive learning principles. While the theoretical contributions are strong and well-supported by experiments, more empirical validation would enhance the robustness of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"From t-SNE to UMAP with Contrastive Learning\" presents a novel approach to understanding the relationships between the popular dimensionality reduction techniques t-SNE and UMAP through the lens of contrastive learning. The authors propose Neg-t-SNE, a new method that leverages noise-contrastive estimation (NCE) and negative sampling (NEG) to enhance the performance of t-SNE. The findings demonstrate that Neg-t-SNE not only provides improved embeddings compared to t-SNE and UMAP but also offers deeper insights into the underlying data structure, supported by both qualitative visualizations and quantitative metrics such as kNN recall and Spearman correlation.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to bridging the gap between t-SNE and UMAP through contrastive learning, which is a relatively unexplored area. The methodology is well-articulated, with a clear mathematical framework that enhances understanding. The results are compelling, showcasing improvements in embedding quality and providing valuable insights into dimensionality reduction techniques. However, a potential weakness is the limited discussion of the computational costs associated with the proposed methods, which could impact their practicality in large-scale applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-organized sections that logically present the research. The writing is clear, and the use of figures and tables effectively supports the findings. The novelty is significant, as the integration of contrastive learning into the context of t-SNE and UMAP is a fresh perspective. The reproducibility of the experiments is ensured, with publicly available code and detailed instructions, which is a strong point of the paper. \n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the field of data visualization and machine learning by effectively linking t-SNE and UMAP through contrastive learning techniques. The findings are well-supported by empirical evidence, and the clarity of presentation enhances the accessibility of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a comprehensive framework that connects t-SNE and UMAP, two prominent methods for high-dimensional data visualization, through the lens of contrastive learning techniques. The authors demonstrate that noise-contrastive estimation (NCE) is applicable for optimizing t-SNE, while UMAP utilizes negative sampling (NEG). They provide a mathematical analysis of these methods, illustrating the inherent distortions introduced by negative sampling and how it results in more compact embeddings in UMAP compared to t-SNE. The authors further propose a generalized framework for negative sampling that facilitates interpolation between the two methods. Additionally, they present a PyTorch implementation to enable practical applications of their findings.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to bridging the gap between t-SNE and UMAP, addressing a crucial gap in understanding their underlying mechanics. The theoretical contributions are backed by rigorous mathematical formulations, enhancing the paper's credibility. However, the paper may have weaknesses in its accessibility to readers less familiar with the mathematical intricacies of the discussed methods, which could limit its broader impact. Moreover, while the PyTorch implementation is a valuable addition, the empirical validation of the proposed methods could be expanded to strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper maintains a high level of clarity in its presentation, although the heavy reliance on mathematical formulations may pose challenges for some readers. The quality of the work is commendable, with thorough explanations of the methodologies and a logical progression of ideas. The novelty is significant, as the paper not only elucidates the relationships between t-SNE and UMAP but also introduces a new perspective on their optimization via contrastive learning. The availability of code enhances reproducibility; however, additional details regarding experimental setups and benchmarks would further bolster this aspect.\n\n# Summary Of The Review\nThis paper presents a meaningful advancement in understanding the connections between t-SNE and UMAP through contrastive learning. While the theoretical contributions are strong and the implementation is useful, the complexity of the mathematical content may limit its accessibility. Overall, the work is a valuable addition to the field of dimensionality reduction.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper attempts to explore the relationship between t-SNE and UMAP through the lens of contrastive learning. The authors propose a novel method, Neg-t-SNE, which aims to interpolate between t-SNE and UMAP. However, the paper lacks a rigorous theoretical framework to substantiate this connection, and the provided mathematical characterizations are convoluted. The findings are primarily demonstrated through visualizations, which have raised concerns regarding the novelty and effectiveness of the proposed approach.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its ambition to bridge the gap between two widely used dimensionality reduction techniques, t-SNE and UMAP. However, the weaknesses are significant; the proposed generalization of negative sampling is inadequately justified and lacks empirical validation. The complexity of the mathematical explanations may hinder understanding, and the limited diversity in experimental datasets restricts the robustness of the conclusions. Additionally, the lack of a thorough discussion on implementation issues and the absence of ethical considerations further detract from the paper’s overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to convoluted mathematical explanations that may alienate readers. The quality of the work appears to be low, as it does not convincingly advance the theoretical understanding of t-SNE or UMAP. In terms of novelty, the proposed methods do not significantly differentiate themselves from existing techniques. Reproducibility is also a concern, as the paper does not adequately address potential limitations or performance trade-offs in the implementation.\n\n# Summary Of The Review\nOverall, the paper presents an ambitious attempt to connect t-SNE and UMAP through contrastive learning but ultimately falls short in clarity, theoretical contribution, and empirical validation. The insights offered are superficial, and the work does not significantly advance the field of dimensionality reduction.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper \"From t-SNE to UMAP with Contrastive Learning\" presents a novel framework that elucidates the relationship between t-SNE and UMAP using contrastive learning techniques. It introduces a flexible generalization of negative sampling that facilitates interpolation and extrapolation between the embeddings produced by these two widely used dimensionality reduction methods. The authors demonstrate that UMAP generates more compact clusters than t-SNE and offer a spectrum of contrastive neighbor embeddings to enhance visualization quality and interpretability. Furthermore, the paper provides a robust PyTorch implementation and empirical validation across various datasets, highlighting significant improvements in visualization consistency and quality.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its groundbreaking insights into the relationship between t-SNE and UMAP, as well as its innovative approach to negative sampling, which allows users to tailor visualizations effectively. The empirical results are compelling, showcasing UMAP's superior performance in cluster compactness. However, a potential weakness is the reliance on specific datasets for validation, which may limit the generalizability of the findings. Additionally, while the proposed methods are user-friendly, further exploration of their applicability across diverse domains could strengthen the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to a broad audience. The quality of the writing is high, with comprehensive explanations and a logical flow of ideas. The novelty of the work is significant, as it bridges a gap in understanding between established methods. The authors have made their code publicly available, enhancing reproducibility and encouraging broader adoption of their methods. However, some explanations could benefit from further elaboration to ensure full comprehension of the concepts introduced.\n\n# Summary Of The Review\nOverall, this paper represents a substantial contribution to the field of high-dimensional data visualization, providing a deeper understanding of t-SNE and UMAP through innovative contrastive learning techniques. Its empirical validation and user-friendly resources further enhance its significance and potential for impact in exploratory data analysis.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a theoretical exploration of the relationships between two widely used dimensionality reduction techniques, t-SNE and UMAP, through the lens of contrastive learning. It highlights the structural differences in their loss functions, proposing a mathematical characterization that connects them via noise-contrastive estimation (NCE) and negative sampling (NEG). The authors introduce a theoretical spectrum of embeddings influenced by normalization constants, discuss the implications of hyperparameters on algorithm behavior, and propose a unified framework in PyTorch for implementing these theoretical constructs. Overall, the work aims to enhance the understanding of high-dimensional data visualization through rigorous theoretical insights.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its rigorous theoretical analysis, which provides a fresh perspective on the connection between t-SNE and UMAP, contributing to the broader understanding of contrastive learning methods. The mathematical characterization of loss functions and the introduction of a theoretical spectrum of embeddings are particularly noteworthy, as they offer a nuanced view of local versus global structures in data. However, a significant weakness is the lack of empirical validation; while the theoretical insights are compelling, the absence of experimental results means that the practical applicability of the proposed framework remains untested. Additionally, the focus on theoretical constructs might limit the paper's accessibility to practitioners who are more interested in empirical results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates complex theoretical concepts with clarity, making it accessible to readers familiar with the underlying mathematical principles. The quality of the writing is high, with a logical flow that effectively connects various sections. The novelty of linking contrastive learning methods through a unified theoretical framework is significant, although it may not offer immediate empirical advances. The proposed PyTorch framework enhances reproducibility, yet the lack of empirical experiments may hinder broader adoption among users who prioritize practical implementation over theoretical exploration.\n\n# Summary Of The Review\nThis paper makes a valuable theoretical contribution to the understanding of contrastive learning and dimensionality reduction techniques, particularly in connecting t-SNE and UMAP. While the theoretical insights are robust and well-articulated, the absence of empirical validation limits the practical implications of the work. Overall, the paper is a solid theoretical foundation that could inspire future empirical studies in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a PyTorch implementation of several dimensionality reduction techniques, including Neg-t-SNE, NC-t-SNE, UMAP, and InfoNC-t-SNE. The methodology incorporates various settings such as batch size, noise samples, and learning rates to optimize the embedding process. Key findings indicate that the initialization method significantly impacts the quality of embeddings, with PCA yielding better results compared to random initialization, and that increasing the number of noise samples improves approximation but increases computational complexity. The authors provide reproducible code for the experiments, ensuring accessibility and further exploration of the proposed methods.\n\n# Strength And Weaknesses\nStrengths of the paper include the comprehensive implementation of multiple techniques, the attention to detail in experimentation (e.g., varying normalization constants and noise sample sizes), and the clear reporting of results with statistical measures (mean ± standard deviation). However, a potential weakness lies in the reliance on PCA for initialization, which may not be suitable for all datasets, potentially limiting the generalizability of the findings. Additionally, while the paper discusses the impact of parameters, further exploration of their interactions could enhance understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, making it easy to follow the methodology and results. The quality of the implementation is high, and the availability of reproducible code is a significant advantage. In terms of novelty, while the techniques themselves are established, the integration of contrastive loss functions and the detailed examination of noise samples add a unique aspect to the research. The reproducibility is well addressed through the provided links to GitHub repositories, allowing other researchers to validate and build upon the work.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of dimensionality reduction by providing robust implementations of several techniques and offering insightful analysis on parameter selection. The clarity and reproducibility of the work enhance its impact, although the dependency on PCA for initialization raises questions about broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new framework connecting t-SNE and UMAP through the lens of contrastive learning. It claims to provide insights into the qualitative differences between the embeddings produced by these two methods and proposes a novel interpolation technique named Neg-t-SNE. The authors assert that their approach addresses issues such as over-interpretation of embeddings and instabilities in UMAP while providing a PyTorch implementation for practical use.\n\n# Strength And Weaknesses\nWhile the paper attempts to bridge existing gaps between t-SNE and UMAP, many of its contributions seem to echo previously established findings in the literature. The claim of mitigating over-interpretation has been addressed in earlier works, and the proposed Neg-t-SNE interpolation appears derivative of existing concepts. Additionally, the assertion regarding UMAP's instability and the suggestion of noise sample adjustments lack originality, as these discussions have already been explored in prior studies. Although the quantitative evaluations are interesting, they do not present sufficiently novel insights or results that distinguish this work from earlier research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, presenting its methodology and findings in an understandable manner. However, the novelty of the contributions is questionable, as many ideas appear to be reiterations of prior work. The PyTorch implementation provided is a positive aspect, enhancing reproducibility, yet similar frameworks exist, which raises concerns about the necessity of this new codebase.\n\n# Summary Of The Review\nOverall, this paper seeks to connect t-SNE and UMAP through contrastive learning, but it largely reiterates established ideas rather than offering significant advancements. While the methodology is clear and the implementation is available, the lack of originality in the contributions detracts from the paper's impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"From t-SNE to UMAP with Contrastive Learning\" presents a novel approach to dimensionality reduction by integrating contrastive learning techniques into the UMAP (Uniform Manifold Approximation and Projection) framework. The authors aim to enhance the interpretability and efficiency of UMAP by leveraging negative sampling methods traditionally associated with t-SNE (t-distributed Stochastic Neighbor Embedding). The methodology involves a comparative analysis of t-SNE and UMAP, followed by experimental evaluations on various datasets to demonstrate improvements in clustering performance and visual representation. The findings indicate that the proposed method significantly outperforms traditional UMAP and t-SNE in terms of preserving local and global structure in high-dimensional data.\n\n# Strength And Weaknesses\nStrengths of the paper include its clear articulation of the contributions and the innovative integration of contrastive learning with UMAP, which offers a fresh perspective on dimensionality reduction techniques. The experimental results are compelling, showcasing the method's effectiveness across diverse datasets. However, weaknesses include some inconsistencies in notation and terminology throughout the paper, which may hinder the reader’s understanding. Additionally, the conclusion section could be more structured to emphasize the key findings succinctly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper overall maintains a reasonable level of clarity, but certain areas suffer from grammatical errors and inconsistent formatting that could distract readers. The quality of the methodology is strong, yet the reproducibility could be improved by providing clearer code links and ensuring that all variables and parameters are well-defined and consistently used. The novelty of integrating contrastive learning with UMAP is significant; however, some related works were not adequately discussed, which could have contextualized the contributions more effectively.\n\n# Summary Of The Review\nThis paper presents an innovative approach to dimensionality reduction by combining UMAP with contrastive learning, yielding promising results in clustering performance. While the contributions are noteworthy, the paper could benefit from improved clarity and consistency in formatting, as well as a more structured conclusion to better highlight its findings.\n\n# Correctness\nRating: 4/5  \nThe methodology is sound and the results are generally accurate, though minor inconsistencies in notation may lead to confusion.\n\n# Technical Novelty And Significance\nRating: 4/5  \nThe integration of contrastive learning with UMAP is a novel contribution, providing a new avenue for enhancing dimensionality reduction methods while also addressing some limitations of existing techniques.\n\n# Empirical Novelty And Significance\nRating: 4/5  \nThe empirical results demonstrate significant advancements over traditional methods, establishing the relevance and applicability of the proposed approach. However, further exploration on additional datasets could strengthen the claims of generalizability.",
    "# Summary Of The Paper\nThe paper investigates the relationship between t-SNE and UMAP through the lens of contrastive learning, aiming to provide insights into their embedding spectrum. The authors propose a mathematical characterization of distortion resulting from negative sampling and offer a PyTorch implementation. While they present theoretical analyses and highlight the need for better understanding embeddings, they do not sufficiently address comparisons with other dimensionality reduction techniques or provide practical recommendations based on their findings.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its exploration of the connections between popular dimensionality reduction techniques and its contribution to the mathematical understanding of distortion through negative sampling. However, it has notable weaknesses, including a lack of engagement with other relevant methods like TriMap and PaCMAP, insufficient exploration of the implications of their findings in practical applications, and limited discussion on the ethical implications of using embeddings in sensitive fields. Furthermore, the authors fail to provide benchmarks against existing implementations or address the reproducibility of their results across different datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, but the depth of analysis in certain areas could be improved, particularly in discussions surrounding the implications of their findings. The quality of the mathematical formulations is commendable; however, the novelty is somewhat limited, as it does not introduce a comprehensive framework for future research. Reproducibility is a significant concern due to the lack of detailed discussions on how results may vary across different datasets or parameter settings.\n\n# Summary Of The Review\nOverall, the paper presents an interesting exploration of the connections between t-SNE and UMAP through contrastive learning, yet it falls short in its comparative analysis with other techniques and practical implications. The findings are promising but require deeper exploration and more comprehensive discussions to enhance their significance and applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper investigates the relationship between two widely-used dimensionality reduction methods, t-SNE and UMAP, through a statistical lens. It focuses on deriving and comparing their loss functions using probabilistic frameworks, specifically examining Noise-Contrastive Estimation (NCE) and Negative Sampling (NEG). The authors present a rigorous statistical analysis that highlights the differences in normalization techniques and their implications on embedding performance. Empirical evaluations demonstrate the robustness of the proposed methods, showing improvements in kNN recall and Spearman correlation metrics when varying the normalization constants.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough exploration of the theoretical aspects of t-SNE and UMAP, with a strong emphasis on statistical methodologies. The derivation of loss functions and the analysis of convergence and stability provide valuable insights into the optimization landscape of these methods. Additionally, the empirical evaluations are well-structured, showcasing the practical implications of the theoretical findings. However, a potential weakness is the reliance on specific statistical metrics without exploring other possible evaluation criteria, which could limit the comprehensiveness of the analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its findings clearly, making complex statistical concepts accessible. The quality of the methodology is high, with rigorous derivations and empirical evaluations supporting the claims made. The novelty lies in establishing a statistical connection between t-SNE and UMAP, which has not been extensively covered in existing literature. The reproducibility of the results appears strong, as the authors provide sufficient detail about their experiments and statistical analyses.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to understanding the statistical foundations of t-SNE and UMAP, elucidating their differences through rigorous theoretical and empirical analyses. While the findings are robust and well-supported, there is room for further exploration of evaluation metrics to enhance the comprehensiveness of the study.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the Neg-t-SNE method, a novel approach to dimensionality reduction that aims to enhance the representation of both local and global structures in high-dimensional data. The authors propose a unique normalization constant and a new loss function to achieve improved embeddings compared to traditional methods like t-SNE and UMAP. The findings demonstrate that Neg-t-SNE outperforms existing techniques on the MNIST dataset, showcasing better clustering and visualization capabilities.\n\n# Strength And Weaknesses\nThe proposed method shows promise in enhancing the quality of embeddings, particularly in terms of local and global structure representation. However, the paper has several weaknesses. It does not explore the practical applications of the embeddings beyond the MNIST dataset, which raises questions about their effectiveness in real-world scenarios. Additionally, there is a lack of comparison with other dimensionality reduction techniques beyond t-SNE and UMAP, which could limit the understanding of its advantages. Concerns regarding scalability to large datasets, potential overfitting, and the impact of hyperparameter variations remain unaddressed, indicating gaps in the empirical validation of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, the novelty of the approach could be better articulated, particularly in distinguishing it from existing methods. The reproducibility of the results is compromised due to insufficient exploration of the normalization constant and hyperparameters, which could significantly influence the outcomes. A more comprehensive discussion on the implications of using fixed versus learnable parameters is also needed to enhance the clarity and depth of the findings.\n\n# Summary Of The Review\nOverall, while the Neg-t-SNE method presents interesting contributions to dimensionality reduction, the paper falls short in terms of empirical validation and comprehensive comparison with existing techniques. The limitations in addressing scalability, generalizability, and potential overfitting raise concerns about the method's applicability in broader contexts.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"From t-SNE to UMAP with Contrastive Learning\" by Damrich et al. aims to elucidate the relationship between two popular dimensionality reduction techniques: t-SNE and UMAP. The authors propose a conceptual framework that connects these methods through contrastive learning, supported by mathematical characterizations and empirical results. They present a spectrum of embeddings that can be adjusted via parameters, with findings indicating that UMAP generally produces more compact clusters compared to t-SNE. Additionally, the paper includes a practical PyTorch implementation of their approach, which is intended to facilitate reproducibility.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear structure and the attempt to provide a mathematical basis for the connection between t-SNE and UMAP. The inclusion of a PyTorch implementation is also beneficial for practitioners. However, the weaknesses are pronounced; many of the insights presented feel like repackaged common knowledge within the field. The novelty of the contributions is questionable, as the authors do not provide groundbreaking insights that significantly advance the understanding of these algorithms. Instead, the discussion often reiterates well-known concepts, which may be frustrating for seasoned researchers.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the writing is commendable, with a logical flow that guides the reader through the authors’ arguments. The quality of the paper is reasonable, but the novelty is limited, as the authors lean heavily on existing knowledge without introducing substantial new ideas. Reproducibility is enhanced through the provided PyTorch implementation, yet the overall contribution to the field may not warrant significant attention from experienced researchers.\n\n# Summary Of The Review\nThis paper offers a structured examination of the relationship between t-SNE and UMAP through the lens of contrastive learning, but ultimately lacks significant novelty and groundbreaking contributions. While it may serve as a useful introductory resource for newcomers to the field, it does not advance the state of knowledge in a meaningful way for experts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel perspective on the relationship between t-SNE and UMAP using contrastive learning methods. It introduces a spectrum of embeddings that interpolates between these two popular dimensionality reduction techniques, potentially creating a unified framework that could integrate other methods. The authors empirically evaluate their approach across various datasets, showcasing improvements in embedding quality and robustness, particularly in relation to issues like cluster fragmentation and sensitivity to hyperparameters.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to bridging t-SNE and UMAP, providing a theoretical foundation for further exploration in dimensionality reduction techniques. The introduction of the normalization constant and its impact on embedding quality is particularly noteworthy and presents avenues for dynamic adjustments during training. However, the paper could benefit from a more extensive discussion on the implications of instability in UMAP and how alternative techniques might address these issues. Additionally, while the empirical evaluations are solid, a broader range of datasets could strengthen the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions and findings. The methodology is described in sufficient detail, allowing for reproducibility, particularly with the provided PyTorch implementation. The novelty of the approach is significant, as it not only connects existing techniques but also proposes new avenues for exploration in embedding methods. However, the clarity around the trade-offs in the spectrum of embeddings could be improved to enhance understanding among readers.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of dimensionality reduction by providing a new perspective on t-SNE and UMAP through contrastive learning. While the findings are compelling and well-supported, there is room for improvement in discussing the implications of certain weaknesses in existing methods and expanding the empirical evaluations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Neg-t-SNE, a novel method that interpolates between t-SNE and UMAP embeddings to enhance data visualization. The authors evaluate Neg-t-SNE's performance through both visual and quantitative comparisons against established methods like t-SNE and UMAP. The findings indicate that Neg-t-SNE produces more compact and well-separated embeddings, particularly at higher values of the normalization constant \\( \\bar{Z} \\). The study also reveals that the kNN recall metric is optimized when \\( \\bar{Z} \\) is near the values used in t-SNE and NC-t-SNE, and that global structure fidelity improves with increased \\( \\bar{Z} \\). Empirical results demonstrate that Neg-t-SNE can achieve competitive performance with MNIST and CIFAR-10 datasets, highlighting its computational efficiency due to linear scaling with embedding dimensions.\n\n# Strength And Weaknesses\nThe main strengths of the paper include the introduction of a new embedding method that effectively balances local and global structure preservation, as evidenced by the performance benchmarks. The empirical results are robust, showcasing significant improvements over traditional methods in certain scenarios. However, a noted weakness is the reliance on parameter tuning, particularly the normalization constant and the number of noise samples, which may limit the method's accessibility for users unfamiliar with these nuances. Additionally, while the benchmarking results are encouraging, further exploration on diverse datasets could provide a more comprehensive understanding of Neg-t-SNE's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the methodology and findings, making it accessible to readers with a background in machine learning and data visualization. The quality of the empirical results is high, showcasing clear visualizations and quantitative metrics. In terms of novelty, Neg-t-SNE presents a significant advancement in the landscape of embedding methods, although the interpolation concept itself is not entirely new. Reproducibility is facilitated by the clarity of the methodology, though the dependence on specific parameter settings may pose challenges for replication in practice.\n\n# Summary Of The Review\nOverall, Neg-t-SNE presents a compelling approach to data embedding that enhances both local and global structure preservation. While the methodology shows promise and outperforms existing techniques in certain contexts, careful parameter tuning is necessary for optimal results, which may hinder broader usability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"FROM t-SNE TO UMAP WITH CONTRASTIVE LEARNING\" presents a comparative analysis of two prominent dimensionality reduction techniques: t-SNE and UMAP. The authors aim to integrate these techniques with contrastive learning to enhance data representation. The methodology involves leveraging contrastive learning principles to improve the embedding quality of both t-SNE and UMAP, with empirical evaluations conducted on various datasets, including MNIST. The findings indicate that the contrastive approach leads to superior embedding performance, demonstrating the potential for improved data visualization and interpretability.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to combining contrastive learning with established dimensionality reduction methods, providing a fresh perspective on enhancing data embeddings. The empirical results are compelling and demonstrate significant improvements over traditional methods. However, weaknesses include a lack of clarity in the abstract, which could benefit from conciseness, and some inconsistencies in terminology usage throughout the paper. Additionally, the heavy reliance on technical jargon may limit accessibility for a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents novel contributions, its clarity suffers from complex sentence structures and inconsistent formatting. The logical flow between sections, particularly from the introduction to methodology, needs improvement to maintain reader engagement. The reproducibility section is well-structured but could provide more detailed step-by-step instructions for those unfamiliar with the methods. Overall, the quality of writing could be enhanced by simplifying language and ensuring that all technical terms are clearly defined at their first mention.\n\n# Summary Of The Review\nThis paper proposes a novel integration of contrastive learning with t-SNE and UMAP, demonstrating significant improvements in data embeddings and visualization. However, clarity issues and a heavy reliance on jargon may hinder its accessibility. Addressing these concerns could enhance the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4756995760665323,
    -1.6418861715968418,
    -1.7891973062754036,
    -1.721281998575173,
    -1.7545801001592178,
    -1.6792578506424325,
    -1.5696513372779708,
    -2.067258720813741,
    -1.744359380231742,
    -2.001600422968388,
    -1.7529584116585264,
    -1.3350184337708368,
    -1.591227698840888,
    -1.4082839398831732,
    -1.5185761572585421,
    -1.6710984047530137,
    -1.7801339723187795,
    -1.6791193251058532,
    -1.6534997325451848,
    -1.7299375875104879,
    -1.9513832217232518,
    -1.5668287902860072,
    -1.8529132451568315,
    -1.6426227804111393,
    -1.6640835804816134,
    -1.659660536642273,
    -1.7140767915524506,
    -1.731637604249229,
    -1.5595856714297756
  ],
  "logp_cond": [
    [
      0.0,
      -2.1600558118035087,
      -2.1775214810934918,
      -2.196023412877567,
      -2.197991081066894,
      -2.176459469819377,
      -2.234481636600267,
      -2.1834348728958015,
      -2.1845425201924913,
      -2.1768371130405235,
      -2.1538566544466207,
      -2.220857402313051,
      -2.169270045519217,
      -2.164646862297042,
      -2.177947838482399,
      -2.179811442305449,
      -2.178517372796283,
      -2.179695341302774,
      -2.1957421203852254,
      -2.1664104885360884,
      -2.1741754512134333,
      -2.1682136139940384,
      -2.187798856187819,
      -2.1944112338964956,
      -2.1747668532053015,
      -2.1952031498223623,
      -2.1726829274705426,
      -2.194171603841441,
      -2.179973852842363
    ],
    [
      -1.3636342681480875,
      0.0,
      -1.2489325646511746,
      -1.1808882694322917,
      -1.3001702132055117,
      -1.3230796062392927,
      -1.369603014612131,
      -1.239875734987346,
      -1.2837871681530078,
      -1.3002458529196341,
      -1.2553267062258537,
      -1.3966097621322746,
      -1.2411081098259442,
      -1.1882140782951347,
      -1.304087150215085,
      -1.2504089153596327,
      -1.3392326566724644,
      -1.262593537129786,
      -1.2159822926368937,
      -1.3526911559064363,
      -1.3363782704445202,
      -1.3917552265003776,
      -1.3077948593231588,
      -1.3016933890515585,
      -1.3670751633698526,
      -1.2605441846167909,
      -1.2959789061345948,
      -1.3763732465922296,
      -1.3270448152091259
    ],
    [
      -1.4820355931282463,
      -1.358973283890592,
      0.0,
      -1.3398308839378548,
      -1.3533265060225899,
      -1.3787680348729439,
      -1.4596512835461433,
      -1.419175514951882,
      -1.3325141653896084,
      -1.3767255033413583,
      -1.3585718004717722,
      -1.5358584193044114,
      -1.3577453913237518,
      -1.357097858289635,
      -1.44316734562485,
      -1.3628409415000209,
      -1.4068552881221978,
      -1.3553663020705442,
      -1.3244226106062844,
      -1.4061509993627348,
      -1.3549097302637525,
      -1.500938012478817,
      -1.425117869814868,
      -1.4258037679943654,
      -1.4474320337328297,
      -1.3853527041287468,
      -1.3788216480461584,
      -1.4218783912193207,
      -1.462598138671814
    ],
    [
      -1.397590534055464,
      -1.1569338106688651,
      -1.2428021081313425,
      0.0,
      -1.3152906724950375,
      -1.347110250123925,
      -1.4094816141082143,
      -1.280731956714052,
      -1.2219905035109355,
      -1.3122854306024414,
      -1.2896602300765103,
      -1.4544301698532698,
      -1.2615470378904705,
      -1.2253292992711142,
      -1.2791015004276778,
      -1.2645931121292922,
      -1.3037946988992442,
      -1.2291563453592875,
      -1.2947567633558272,
      -1.317468699573481,
      -1.318479945706952,
      -1.4104280280942874,
      -1.3065203331765447,
      -1.295524703428027,
      -1.377115783601443,
      -1.28369511908572,
      -1.3569336950088768,
      -1.3670901722339044,
      -1.359023121825795
    ],
    [
      -1.4776902609461933,
      -1.4362445278905798,
      -1.4076063984580707,
      -1.4693612466579191,
      0.0,
      -1.4138146741061468,
      -1.500756672903479,
      -1.4331714920141152,
      -1.4209164422455491,
      -1.3824328319321617,
      -1.410998772939616,
      -1.5257669406544143,
      -1.4184066957345365,
      -1.411217040220367,
      -1.4714869854745902,
      -1.4035211140593977,
      -1.42924951365479,
      -1.4381461717219222,
      -1.429586220216019,
      -1.5080624369036884,
      -1.3941326932222207,
      -1.5022012333466794,
      -1.406226855227912,
      -1.5160945501139764,
      -1.5062112495032318,
      -1.3973094634843082,
      -1.423101363011488,
      -1.5180424869701148,
      -1.4827157970813558
    ],
    [
      -1.3440495973453734,
      -1.2111031960099312,
      -1.217705994698792,
      -1.2335319233357716,
      -1.1753099647085705,
      0.0,
      -1.3529451138841726,
      -1.246138932014687,
      -1.1781600906254865,
      -1.2461896588641352,
      -1.2477070330529527,
      -1.3805035897083646,
      -1.2251123482088528,
      -1.2507801800162122,
      -1.2646331292274984,
      -1.2327553349521931,
      -1.2410359272603244,
      -1.2553725045529094,
      -1.2417387720995614,
      -1.3002154090400195,
      -1.2225073842589098,
      -1.363402100279328,
      -1.2638288732391088,
      -1.3214056521957351,
      -1.3376137174507652,
      -1.2593868157912442,
      -1.2670875887999828,
      -1.3213613499562504,
      -1.3417538432837042
    ],
    [
      -1.3028868043821697,
      -1.265857433921419,
      -1.2254750689835094,
      -1.299937353631658,
      -1.275308018523972,
      -1.2680755942388386,
      0.0,
      -1.227520517252156,
      -1.2857824402365572,
      -1.232626470055159,
      -1.2648869642234162,
      -1.25333604831232,
      -1.2997969438350498,
      -1.2619375964886355,
      -1.2634066255400296,
      -1.3132599768437008,
      -1.3018530315597425,
      -1.2839491375256584,
      -1.2843071785460398,
      -1.2912687842274098,
      -1.2864177378887003,
      -1.3324396953479396,
      -1.3182251289955753,
      -1.2877528874407749,
      -1.2889308046218824,
      -1.2917658843821842,
      -1.2555318340429293,
      -1.3020180337055063,
      -1.32227436845104
    ],
    [
      -1.7137349076040849,
      -1.5706353045951171,
      -1.6473147796986272,
      -1.6658624394005033,
      -1.6377407661555523,
      -1.6671745852817164,
      -1.71984470399109,
      0.0,
      -1.6516975591084757,
      -1.5908919263846297,
      -1.6989906412969051,
      -1.7868283907809839,
      -1.651953097502414,
      -1.589633280457269,
      -1.6680226508238905,
      -1.653745557663724,
      -1.728239860528207,
      -1.6333304932150228,
      -1.6675677574811067,
      -1.754685982367301,
      -1.6658843874180005,
      -1.7596032267701411,
      -1.7376374320974315,
      -1.711754091952234,
      -1.8097138569367255,
      -1.6736197274920874,
      -1.6904540696950303,
      -1.780295226759476,
      -1.727159471251896
    ],
    [
      -1.4621156174191348,
      -1.3696872387204255,
      -1.3538851979340818,
      -1.3615090962445855,
      -1.4084761602255544,
      -1.3768749419376682,
      -1.4669698585502982,
      -1.3908655939016303,
      0.0,
      -1.4234177286047736,
      -1.3961327458265407,
      -1.50165341890522,
      -1.3414420323257583,
      -1.3599728639523694,
      -1.3863207136146536,
      -1.3553264437164558,
      -1.4017678205292714,
      -1.4181845856233086,
      -1.3835066907016158,
      -1.397284415796522,
      -1.3401558750204146,
      -1.5261355987506633,
      -1.4262972160692904,
      -1.4098776557141417,
      -1.4312238411258336,
      -1.3733122004898612,
      -1.3959987908485911,
      -1.4110630192382283,
      -1.475970722972169
    ],
    [
      -1.6908125177277769,
      -1.6721906595052949,
      -1.5929940813545242,
      -1.6686791883637098,
      -1.6403229247771876,
      -1.6412668474624885,
      -1.70412152217924,
      -1.6216991334527981,
      -1.6574956259768436,
      0.0,
      -1.647086223170716,
      -1.7249104527691406,
      -1.7001008912287503,
      -1.672401156008412,
      -1.7011233709953173,
      -1.6588568830808097,
      -1.666741443227486,
      -1.6694379561332362,
      -1.6418500895623915,
      -1.6558130059308864,
      -1.6499487277347966,
      -1.725079893730705,
      -1.6427131032867797,
      -1.6696652464723112,
      -1.6535581363748812,
      -1.721590427831794,
      -1.6389607800582096,
      -1.6979190185585922,
      -1.7095116518453788
    ],
    [
      -1.36552375591674,
      -1.2582199761312796,
      -1.2922069657127244,
      -1.3409796537282228,
      -1.3029215647740862,
      -1.2972889370460985,
      -1.4333862976265337,
      -1.2842844903736403,
      -1.3239481843691985,
      -1.311063323566414,
      0.0,
      -1.4232696305158807,
      -1.289551077170597,
      -1.2933928678925175,
      -1.3118479323145258,
      -1.3001598343053227,
      -1.3377032850557586,
      -1.3057631669952956,
      -1.3045763391723344,
      -1.3440675413504373,
      -1.3122804494409117,
      -1.416292099497511,
      -1.3500493391582624,
      -1.3422459275045595,
      -1.396978152966003,
      -1.3517235883743481,
      -1.307532004542179,
      -1.367407715371145,
      -1.3573478964797363
    ],
    [
      -1.0943108962353212,
      -1.0482931830086089,
      -1.0651314247640165,
      -1.053525875057893,
      -1.0481518572027841,
      -1.0723718037081558,
      -1.053176951047326,
      -1.067229544863255,
      -1.0383941560710874,
      -1.0517794739359434,
      -1.065689769217248,
      0.0,
      -1.0483486169495069,
      -1.0542049274810987,
      -1.07511507685419,
      -1.0653559582277565,
      -1.0490159747305376,
      -1.058159345256967,
      -1.060860551914712,
      -1.0828666909302807,
      -1.049873882561397,
      -1.0613780647204403,
      -1.0346432199764046,
      -1.0789193015121128,
      -1.0393711117690152,
      -1.0563135927490357,
      -1.068897426073812,
      -1.052847916781965,
      -1.055155884875883
    ],
    [
      -1.29762113091639,
      -1.2040101822532483,
      -1.1775384168146004,
      -1.2309337265969154,
      -1.1880518457775,
      -1.2406654920570623,
      -1.2868446234771171,
      -1.1898140184148855,
      -1.1524712124554555,
      -1.272556823614248,
      -1.1615488207512499,
      -1.316140989259695,
      0.0,
      -1.1757836738586591,
      -1.250476184719386,
      -1.1197049060167152,
      -1.240583887177741,
      -1.1826606341187245,
      -1.2163763730608952,
      -1.2835976374882065,
      -1.1904469807829108,
      -1.3362350195454442,
      -1.2657261596665341,
      -1.261607980160511,
      -1.2893771102412417,
      -1.217610815773153,
      -1.2517735705563584,
      -1.284489713799612,
      -1.2882659816815283
    ],
    [
      -1.1065006230625973,
      -0.9117791457987598,
      -1.0077203879859207,
      -0.9623913434524218,
      -1.0638629209347823,
      -1.0763466376643538,
      -1.1263807804390795,
      -1.006454849275236,
      -1.0149315103359275,
      -1.0657458740766417,
      -1.0227957952169842,
      -1.1492493949012426,
      -1.0004770268869023,
      0.0,
      -1.0617657115935992,
      -1.0256310566027684,
      -1.1009506826352564,
      -1.0202574001669007,
      -0.9713109948439959,
      -1.0611242130180125,
      -1.0545802898083956,
      -1.1599577834526544,
      -1.11546842003482,
      -1.022037670974348,
      -1.109427892860151,
      -1.047644641250644,
      -1.054379852690078,
      -1.1031988502463408,
      -1.1155919901323355
    ],
    [
      -1.1788367050910797,
      -1.068015406451509,
      -1.1677339428226374,
      -1.082602886047149,
      -1.1400484932603605,
      -1.089698511352738,
      -1.223228883817375,
      -1.0949867837812521,
      -1.073524040999704,
      -1.2247190608649046,
      -1.122676183209263,
      -1.2183182672239965,
      -1.1216710869051572,
      -1.0801478105391644,
      0.0,
      -1.1031336531861589,
      -1.1150860038765382,
      -1.1110960679133017,
      -1.1493847859896547,
      -1.099977746308809,
      -1.1199273212597767,
      -1.1704464105419834,
      -1.1721415652605052,
      -1.0729934656427083,
      -1.163497107318938,
      -1.166969684857856,
      -1.1472534461720798,
      -1.1220080353508464,
      -1.1167040391304262
    ],
    [
      -1.3436896011597792,
      -1.2378276120692617,
      -1.3049563849626142,
      -1.2803529164742602,
      -1.266981918496188,
      -1.3089394048244205,
      -1.4115032493553514,
      -1.2827092094373809,
      -1.2168327622202035,
      -1.334365147408967,
      -1.2655527161050375,
      -1.4109983938255117,
      -1.1745907431756417,
      -1.259507912981738,
      -1.2991265149446918,
      0.0,
      -1.3023125014771515,
      -1.2325288512280255,
      -1.2674351069953367,
      -1.3414639841675169,
      -1.3160220185271707,
      -1.378315937732285,
      -1.2841848686303814,
      -1.3203793680045979,
      -1.3863654814144177,
      -1.2979422781465555,
      -1.304730622773721,
      -1.3965706995760714,
      -1.3654433420197385
    ],
    [
      -1.42628918014654,
      -1.3505991483741138,
      -1.299410630634437,
      -1.3080384048174067,
      -1.3223796395373923,
      -1.3145061559945885,
      -1.4600398875739142,
      -1.387174619008472,
      -1.310851288426521,
      -1.3905273246131244,
      -1.3262193806942437,
      -1.480969799360339,
      -1.3222308356262926,
      -1.3701757131059729,
      -1.3389892092839588,
      -1.3027096468459431,
      0.0,
      -1.3452373101287358,
      -1.3405480544946105,
      -1.3727788214827923,
      -1.3454390648291836,
      -1.4462824574803543,
      -1.3282636422140353,
      -1.3998350972076552,
      -1.4030485233062255,
      -1.3364055153698282,
      -1.383140333613504,
      -1.3645347963850474,
      -1.4074667768817037
    ],
    [
      -1.3808033148990422,
      -1.2386820234490918,
      -1.2485394018950227,
      -1.2353072245877537,
      -1.2648636869198213,
      -1.355886759645515,
      -1.3486324698752072,
      -1.292115104432515,
      -1.2669130642688093,
      -1.3326155803838655,
      -1.2769830713174302,
      -1.400232356631393,
      -1.2812550634892077,
      -1.248628872808037,
      -1.330287774421058,
      -1.2719919633735581,
      -1.3270486184635226,
      0.0,
      -1.3324041971210643,
      -1.3405208817623204,
      -1.2900321310073275,
      -1.4002172649808853,
      -1.3134419913891187,
      -1.3395152420126508,
      -1.3866394988564636,
      -1.2794174930038427,
      -1.3168886147849492,
      -1.3935493256279377,
      -1.3713737237428387
    ],
    [
      -1.3808084963396043,
      -1.1968573743670752,
      -1.2419239476730715,
      -1.2985999966166692,
      -1.2832497200599784,
      -1.2514028720286017,
      -1.3580285121848523,
      -1.3000967987490826,
      -1.242165293418024,
      -1.306322954602288,
      -1.2452941375971458,
      -1.3556941669775207,
      -1.2475715111669734,
      -1.2291134513132624,
      -1.280735742234199,
      -1.2684533082190852,
      -1.3346538020992402,
      -1.292327260145235,
      0.0,
      -1.3429558658296612,
      -1.3115111964001058,
      -1.3850540382274965,
      -1.3198733332372428,
      -1.3375033177670241,
      -1.3281558367467488,
      -1.3236451052105027,
      -1.2784471774031922,
      -1.3430115371978315,
      -1.3487274890229175
    ],
    [
      -1.4262474513425385,
      -1.4049721629480123,
      -1.4143469421699417,
      -1.4134925129392095,
      -1.4021144901046343,
      -1.3768588376924669,
      -1.5298847904637154,
      -1.4463822242897257,
      -1.3378186238834908,
      -1.4312817832399019,
      -1.3953425506762744,
      -1.4970636191612756,
      -1.3952365537965328,
      -1.4013872390438467,
      -1.3639457304647162,
      -1.3934455438860915,
      -1.3898337375757561,
      -1.4330110310755233,
      -1.4287434393402725,
      0.0,
      -1.3468498051176596,
      -1.4543085314667545,
      -1.4118560256220645,
      -1.4162675471673616,
      -1.3807426108745169,
      -1.4409934819365642,
      -1.4180704326083,
      -1.3457155947307027,
      -1.3981176021429196
    ],
    [
      -1.5737660719223048,
      -1.5237382859080084,
      -1.4802747154755072,
      -1.4484400955530685,
      -1.4638349591910733,
      -1.5148545616702882,
      -1.6708354711070625,
      -1.585309653835412,
      -1.4485071258723747,
      -1.5588568703602772,
      -1.5446798323751838,
      -1.639984057524843,
      -1.5032053736802151,
      -1.5012283545139617,
      -1.5345787871182344,
      -1.4711381940741453,
      -1.508936751909234,
      -1.5151841295182253,
      -1.55723361202637,
      -1.5276086784208247,
      0.0,
      -1.6626310836923037,
      -1.576306364647258,
      -1.5719966781583268,
      -1.5815312182058625,
      -1.5716411026246728,
      -1.5120177051525416,
      -1.538974422861719,
      -1.6099749667687648
    ],
    [
      -1.2956055387644019,
      -1.3083277995970586,
      -1.308483441188518,
      -1.2907004531552684,
      -1.2965350997771345,
      -1.3156177707513115,
      -1.3069680896621854,
      -1.2435222492928986,
      -1.287475068474298,
      -1.2740118248778973,
      -1.2701909150522126,
      -1.2785990044222324,
      -1.2880519114975988,
      -1.269740058328933,
      -1.2639232385595958,
      -1.2948405753087353,
      -1.2816086997045952,
      -1.2647006799500289,
      -1.3118668696968614,
      -1.2816420039456005,
      -1.2794646293912346,
      0.0,
      -1.2895214204686922,
      -1.2990008485512574,
      -1.3055107433685862,
      -1.2559706627877332,
      -1.2757636710734481,
      -1.306005781712609,
      -1.2029379522400014
    ],
    [
      -1.4861319593438131,
      -1.419937391119412,
      -1.416470119572668,
      -1.4726263929610284,
      -1.3587355375610874,
      -1.4096781904866422,
      -1.514279034132274,
      -1.4346316667461496,
      -1.4231955636650981,
      -1.4291682914001294,
      -1.4439825077800963,
      -1.5440553147527292,
      -1.4631344186320407,
      -1.4504344487187388,
      -1.4711351801898702,
      -1.4117055528500575,
      -1.3856261174113702,
      -1.471471690251853,
      -1.4341338389334424,
      -1.5190484493147807,
      -1.4075451634673661,
      -1.521846993465131,
      0.0,
      -1.491517835476153,
      -1.4665958453806431,
      -1.3573317587756268,
      -1.446750995175031,
      -1.5241878850015145,
      -1.491982537490903
    ],
    [
      -1.3035463451344453,
      -1.2387948964120037,
      -1.2651446174912164,
      -1.2797623932300868,
      -1.3113338851838146,
      -1.2779050339797942,
      -1.3278784568234185,
      -1.2947602538367808,
      -1.2466105497822364,
      -1.2504277553862955,
      -1.262313033495529,
      -1.3588399412633139,
      -1.271138408766296,
      -1.2500140393400656,
      -1.2336968525391176,
      -1.2684150466898745,
      -1.2924916478570325,
      -1.2967351248072012,
      -1.230786736266954,
      -1.297262318936929,
      -1.3011839634514484,
      -1.385748635105552,
      -1.3107802647359232,
      0.0,
      -1.3312550300754107,
      -1.33875750699738,
      -1.273092718504471,
      -1.2913089366570207,
      -1.3513313098868702
    ],
    [
      -1.3685686920582123,
      -1.291549140083254,
      -1.2704090749764143,
      -1.3121493704858758,
      -1.332378543200445,
      -1.2808733710536209,
      -1.4077286738268122,
      -1.3584286458229407,
      -1.2673127191110962,
      -1.2929150230876434,
      -1.2753733977097612,
      -1.3445757916708343,
      -1.2954063267203262,
      -1.3112350244998257,
      -1.302985700014294,
      -1.344752883316667,
      -1.283126032874746,
      -1.3216130005290818,
      -1.3049139877082168,
      -1.2863518724735645,
      -1.2798720372904586,
      -1.3928714458417553,
      -1.3113753011172582,
      -1.3025005900242588,
      0.0,
      -1.351371439510625,
      -1.2748661671989985,
      -1.2556528445378945,
      -1.3494742088611058
    ],
    [
      -1.3320900028282032,
      -1.1812159321131759,
      -1.231402715778152,
      -1.2405347332126309,
      -1.216007916531652,
      -1.295384195849615,
      -1.364801627253039,
      -1.2385121704005935,
      -1.253480787414093,
      -1.3125400717176887,
      -1.2936497233562114,
      -1.3809943697793168,
      -1.2542865202730704,
      -1.209053014798881,
      -1.3051936277120868,
      -1.282365702339046,
      -1.2651322947043762,
      -1.2329807234373409,
      -1.2247053480690202,
      -1.322610983969224,
      -1.2387505023326129,
      -1.3564135118289198,
      -1.2447952450596709,
      -1.336356864942278,
      -1.3582299600670082,
      0.0,
      -1.2954299266556033,
      -1.3588164991463296,
      -1.308081945306414
    ],
    [
      -1.3836387920129567,
      -1.2733741130854008,
      -1.263098525987882,
      -1.3389451095216591,
      -1.2358628027023923,
      -1.3490978668185274,
      -1.3950101324835045,
      -1.3461317367868817,
      -1.2662835729979602,
      -1.3111609475217858,
      -1.2796748965887064,
      -1.4377111504815927,
      -1.321771160968396,
      -1.2693079286710305,
      -1.3738890621931006,
      -1.319472822421047,
      -1.3600448896890305,
      -1.3346608791727812,
      -1.2543746233413153,
      -1.3483724046448757,
      -1.2592648712426175,
      -1.4243020530641135,
      -1.339574903945288,
      -1.3159885012880936,
      -1.3779883731215605,
      -1.323527587891792,
      0.0,
      -1.3593932284527832,
      -1.3743307581530295
    ],
    [
      -1.534409927507787,
      -1.4805553945481802,
      -1.447874228976647,
      -1.4482405139605925,
      -1.4666231110069898,
      -1.4146108425381745,
      -1.488143429999019,
      -1.4640097547288802,
      -1.3865030894980352,
      -1.4567092348123882,
      -1.4418566050502932,
      -1.4810942515855419,
      -1.455217898493497,
      -1.4691778501122794,
      -1.4133309222234873,
      -1.4682623474665757,
      -1.442570618124551,
      -1.4649456138506571,
      -1.4622492156752949,
      -1.3751714836809275,
      -1.4000618559137676,
      -1.4956350898633899,
      -1.4814490733605916,
      -1.4302248786635365,
      -1.4290905762356,
      -1.470452406049156,
      -1.4570755090941143,
      0.0,
      -1.4645115947543876
    ],
    [
      -1.234568128187814,
      -1.1588080344513167,
      -1.1928849293397332,
      -1.2085119191075473,
      -1.2045638060002009,
      -1.210069569773983,
      -1.2543986824269804,
      -1.201251210101686,
      -1.2091657245633445,
      -1.1978811834855052,
      -1.1890216070413997,
      -1.226459110536217,
      -1.200991769826161,
      -1.1846779339464715,
      -1.1735625418980875,
      -1.2400413172009623,
      -1.2187217595049202,
      -1.1866419531433694,
      -1.2297143589770585,
      -1.209896973894341,
      -1.2184983024427447,
      -1.120522124099559,
      -1.1964402999668609,
      -1.248358993961267,
      -1.2163220531127654,
      -1.1926649221299133,
      -1.2020402464817614,
      -1.2380488117634958,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.31564376426302365,
      0.2981780949730406,
      0.27967616318896527,
      0.27770849499963823,
      0.29924010624715525,
      0.24121793946626546,
      0.29226470317073083,
      0.29115705587404106,
      0.29886246302600883,
      0.3218429216199117,
      0.2548421737534814,
      0.3064295305473155,
      0.3110527137694903,
      0.2977517375841332,
      0.29588813376108325,
      0.29718220327024936,
      0.29600423476375815,
      0.2799574556813069,
      0.30928908753044393,
      0.3015241248530991,
      0.3074859620724939,
      0.28790071987871313,
      0.28128834217003673,
      0.30093272286123085,
      0.28049642624417004,
      0.30301664859598976,
      0.2815279722250912,
      0.29572572322416946
    ],
    [
      0.2782519034487543,
      0.0,
      0.39295360694566717,
      0.46099790216455006,
      0.34171595839133007,
      0.31880656535754914,
      0.27228315698471084,
      0.40201043660949587,
      0.35809900344383405,
      0.3416403186772077,
      0.38655946537098806,
      0.2452764094645672,
      0.4007780617708976,
      0.4536720933017071,
      0.33779902138175677,
      0.3914772562372091,
      0.3026535149243774,
      0.37929263446705574,
      0.4259038789599481,
      0.2891950156904055,
      0.3055079011523216,
      0.25013094509646416,
      0.33409131227368305,
      0.3401927825452833,
      0.27481100822698923,
      0.38134198698005095,
      0.34590726546224704,
      0.2655129250046122,
      0.31484135638771593
    ],
    [
      0.30716171314715734,
      0.4302240223848117,
      0.0,
      0.44936642233754887,
      0.4358708002528138,
      0.41042927140245977,
      0.32954602272926037,
      0.37002179132352153,
      0.4566831408857952,
      0.4124718029340453,
      0.43062550580363146,
      0.25333888697099227,
      0.4314519149516518,
      0.4320994479857687,
      0.3460299606505537,
      0.42635636477538275,
      0.3823420181532058,
      0.4338310042048594,
      0.4647746956691192,
      0.38304630691266883,
      0.43428757601165113,
      0.28825929379658666,
      0.36407943646053553,
      0.36339353828103826,
      0.34176527254257394,
      0.40384460214665685,
      0.41037565822924527,
      0.36731891505608294,
      0.32659916760358954
    ],
    [
      0.3236914645197091,
      0.5643481879063079,
      0.47847989044383055,
      0.0,
      0.40599132608013555,
      0.3741717484512481,
      0.3118003844669588,
      0.440550041861121,
      0.49929149506423753,
      0.40899656797273165,
      0.43162176849866274,
      0.2668518287219033,
      0.45973496068470254,
      0.49595269930405883,
      0.44218049814749527,
      0.4566888864458809,
      0.4174872996759289,
      0.49212565321588553,
      0.4265252352193458,
      0.40381329900169205,
      0.402802052868221,
      0.3108539704808857,
      0.41476166539862835,
      0.425757295147146,
      0.3441662149737301,
      0.4375868794894531,
      0.3643483035662962,
      0.35419182634126867,
      0.36225887674937796
    ],
    [
      0.27688983921302457,
      0.31833557226863807,
      0.34697370170114716,
      0.2852188535012987,
      0.0,
      0.34076542605307103,
      0.2538234272557389,
      0.32140860814510264,
      0.3336636579136687,
      0.3721472682270561,
      0.3435813272196018,
      0.22881315950480352,
      0.33617340442468135,
      0.34336305993885086,
      0.2830931146846276,
      0.3510589860998201,
      0.3253305865044278,
      0.3164339284372957,
      0.32499387994319884,
      0.24651766325552948,
      0.3604474069369972,
      0.2523788668125384,
      0.3483532449313058,
      0.2384855500452414,
      0.24836885065598602,
      0.35727063667490966,
      0.3314787371477299,
      0.23653761318910305,
      0.271864303077862
    ],
    [
      0.33520825329705906,
      0.4681546546325013,
      0.46155185594364045,
      0.44572592730666094,
      0.503947885933862,
      0.0,
      0.3263127367582599,
      0.43311891862774554,
      0.501097760016946,
      0.43306819177829725,
      0.4315508175894798,
      0.2987542609340679,
      0.45414550243357965,
      0.42847767062622033,
      0.4146247214149341,
      0.44650251569023935,
      0.4382219233821081,
      0.4238853460895231,
      0.4375190785428711,
      0.37904244160241296,
      0.4567504663835227,
      0.31585575036310454,
      0.41542897740332374,
      0.35785219844669736,
      0.3416441331916673,
      0.41987103485118826,
      0.41217026184244965,
      0.35789650068618206,
      0.3375040073587283
    ],
    [
      0.26676453289580104,
      0.30379390335655176,
      0.34417626829446135,
      0.2697139836463127,
      0.2943433187539988,
      0.30157574303913215,
      0.0,
      0.34213082002581485,
      0.2838688970414136,
      0.3370248672228118,
      0.3047643730545546,
      0.31631528896565086,
      0.26985439344292095,
      0.30771374078933533,
      0.3062447117379412,
      0.25639136043427,
      0.26779830571822827,
      0.28570219975231237,
      0.285344158731931,
      0.27838255305056103,
      0.2832335993892705,
      0.2372116419300312,
      0.2514262082823955,
      0.2818984498371959,
      0.28072053265608843,
      0.2778854528957866,
      0.31411950323504145,
      0.2676333035724645,
      0.24737696882693072
    ],
    [
      0.3535238132096561,
      0.4966234162186238,
      0.4199439411151138,
      0.4013962814132377,
      0.42951795465818865,
      0.4000841355320246,
      0.34741401682265094,
      0.0,
      0.41556116170526525,
      0.47636679442911123,
      0.3682680795168358,
      0.2804303300327571,
      0.41530562331132703,
      0.4776254403564719,
      0.3992360699898505,
      0.41351316315001685,
      0.33901886028553396,
      0.4339282275987182,
      0.3996909633326342,
      0.31257273844644007,
      0.40137433339574047,
      0.3076554940435998,
      0.32962128871630947,
      0.3555046288615069,
      0.2575448638770155,
      0.39363899332165353,
      0.37680465111871064,
      0.2869634940542649,
      0.3400992495618449
    ],
    [
      0.2822437628126073,
      0.37467214151131656,
      0.3904741822976603,
      0.3828502839871566,
      0.33588322000618764,
      0.36748443829407385,
      0.27738952168144393,
      0.35349378633011175,
      0.0,
      0.3209416516269685,
      0.3482266344052014,
      0.2427059613265221,
      0.40291734790598377,
      0.3843865162793727,
      0.3580386666170885,
      0.38903293651528625,
      0.3425915597024707,
      0.32617479460843346,
      0.3608526895301263,
      0.34707496443522,
      0.40420350521132753,
      0.2182237814810788,
      0.3180621641624517,
      0.3344817245176004,
      0.3131355391059085,
      0.37104717974188084,
      0.34836058938315095,
      0.3332963609935138,
      0.26838865725957306
    ],
    [
      0.3107879052406113,
      0.32940976346309325,
      0.4086063416138639,
      0.3329212346046784,
      0.3612774981912006,
      0.3603335755058996,
      0.29747890078914807,
      0.37990128951559,
      0.3441047969915445,
      0.0,
      0.3545141997976722,
      0.27668997019924757,
      0.30149953173963784,
      0.32919926695997614,
      0.30047705197307084,
      0.34274353988757844,
      0.33485897974090206,
      0.33216246683515194,
      0.3597503334059966,
      0.3457874170375017,
      0.3516516952335915,
      0.2765205292376831,
      0.3588873196816085,
      0.33193517649607696,
      0.3480422865935069,
      0.2800099951365942,
      0.3626396429101786,
      0.30368140440979596,
      0.29208877112300935
    ],
    [
      0.3874346557417865,
      0.49473843552724683,
      0.460751445945802,
      0.4119787579303036,
      0.45003684688444023,
      0.4556694746124279,
      0.3195721140319927,
      0.46867392128488605,
      0.4290102272893279,
      0.4418950880921124,
      0.0,
      0.32968878114264566,
      0.4634073344879295,
      0.4595655437660089,
      0.4411104793440006,
      0.4527985773532037,
      0.41525512660276775,
      0.4471952446632308,
      0.448382072486192,
      0.4088908703080891,
      0.4406779622176147,
      0.3366663121610154,
      0.402909072500264,
      0.4107124841539669,
      0.35598025869252337,
      0.40123482328417825,
      0.4454264071163474,
      0.38555069628738137,
      0.3956105151787901
    ],
    [
      0.24070753753551566,
      0.286725250762228,
      0.2698870090068204,
      0.28149255871294376,
      0.2868665765680527,
      0.2626466300626811,
      0.28184148272351095,
      0.2677888889075819,
      0.2966242776997494,
      0.28323895983489344,
      0.26932866455358884,
      0.0,
      0.28666981682132997,
      0.2808135062897381,
      0.2599033569166469,
      0.2696624755430803,
      0.2860024590402992,
      0.27685908851386976,
      0.2741578818561248,
      0.2521517428405562,
      0.28514455120943993,
      0.27364036905039657,
      0.3003752137944322,
      0.25609913225872405,
      0.29564732200182164,
      0.27870484102180115,
      0.26612100769702485,
      0.28217051698887174,
      0.27986254889495377
    ],
    [
      0.2936065679244979,
      0.3872175165876397,
      0.41368928202628763,
      0.36029397224397264,
      0.403175853063388,
      0.35056220678382566,
      0.30438307536377085,
      0.40141368042600245,
      0.4387564863854325,
      0.31867087522664006,
      0.4296788780896381,
      0.2750867095811931,
      0.0,
      0.4154440249822289,
      0.34075151412150206,
      0.47152279282417275,
      0.35064381166314695,
      0.40856706472216353,
      0.3748513257799928,
      0.30763006135268145,
      0.4007807180579772,
      0.2549926792954438,
      0.32550153917435387,
      0.3296197186803771,
      0.3018505885996463,
      0.3736168830677349,
      0.3394541282845296,
      0.3067379850412759,
      0.3029617171593597
    ],
    [
      0.3017833168205759,
      0.4965047940844134,
      0.40056355189725257,
      0.4458925964307514,
      0.34442101894839094,
      0.33193730221881945,
      0.2819031594440937,
      0.40182909060793714,
      0.3933524295472457,
      0.34253806580653157,
      0.3854881446661891,
      0.2590345449819307,
      0.4078069129962709,
      0.0,
      0.346518228289574,
      0.3826528832804048,
      0.3073332572479168,
      0.38802653971627254,
      0.4369729450391774,
      0.3471597268651607,
      0.35370365007477766,
      0.2483261564305188,
      0.2928155198483533,
      0.3862462689088253,
      0.29885604702302215,
      0.3606392986325293,
      0.3539040871930952,
      0.30508508963683245,
      0.2926919497508378
    ],
    [
      0.3397394521674624,
      0.45056075080703306,
      0.3508422144359047,
      0.43597327121139307,
      0.3785276639981816,
      0.42887764590580413,
      0.29534727344116707,
      0.42358937347729,
      0.4450521162588381,
      0.29385709639363755,
      0.395899974049279,
      0.3002578900345456,
      0.39690507035338496,
      0.4384283467193777,
      0.0,
      0.41544250407238326,
      0.4034901533820039,
      0.4074800893452404,
      0.36919137126888746,
      0.41859841094973316,
      0.39864883599876544,
      0.3481297467165587,
      0.34643459199803694,
      0.44558269161583386,
      0.35507904993960415,
      0.3516064724006862,
      0.3713227110864623,
      0.39656812190769575,
      0.40187211812811596
    ],
    [
      0.32740880359323454,
      0.433270792683752,
      0.3661420197903995,
      0.39074548827875355,
      0.40411648625682584,
      0.36215899992859324,
      0.2595951553976623,
      0.3883891953156329,
      0.4542656425328102,
      0.3367332573440467,
      0.40554568864797624,
      0.260100010927502,
      0.49650766157737203,
      0.4115904917712758,
      0.37197188980832196,
      0.0,
      0.36878590327586225,
      0.4385695535249883,
      0.40366329775767706,
      0.32963442058549686,
      0.35507638622584303,
      0.29278246702072885,
      0.38691353612263235,
      0.35071903674841587,
      0.284732923338596,
      0.37315612660645825,
      0.36636778197929276,
      0.27452770517694236,
      0.30565506273327525
    ],
    [
      0.3538447921722394,
      0.4295348239446657,
      0.4807233416843424,
      0.4720955675013727,
      0.4577543327813871,
      0.4656278163241909,
      0.3200940847448652,
      0.3929593533103075,
      0.46928268389225836,
      0.389606647705655,
      0.45391459162453573,
      0.29916417295844044,
      0.45790313669248683,
      0.4099582592128066,
      0.44114476303482064,
      0.4774243254728363,
      0.0,
      0.43489666219004364,
      0.439585917824169,
      0.40735515083598717,
      0.4346949074895958,
      0.3338515148384251,
      0.4518703301047442,
      0.38029887511112426,
      0.3770854490125539,
      0.44372845694895124,
      0.3969936387052755,
      0.4155991759337321,
      0.3726671954370757
    ],
    [
      0.298316010206811,
      0.4404373016567613,
      0.4305799232108305,
      0.4438121005180995,
      0.41425563818603184,
      0.3232325654603381,
      0.3304868552306459,
      0.3870042206733382,
      0.4122062608370438,
      0.34650374472198764,
      0.402136253788423,
      0.27888696847446015,
      0.39786426161664545,
      0.43049045229781613,
      0.3488315506847952,
      0.407127361732295,
      0.35207070664233053,
      0.0,
      0.3467151279847889,
      0.33859844334353273,
      0.38908719409852566,
      0.2789020601249679,
      0.3656773337167345,
      0.33960408309320234,
      0.2924798262493895,
      0.3997018321020105,
      0.3622307103209039,
      0.2855699994779155,
      0.30774560136301443
    ],
    [
      0.27269123620558045,
      0.4566423581781096,
      0.4115757848721133,
      0.35489973592851554,
      0.3702500124852064,
      0.4020968605165831,
      0.29547122036033246,
      0.3534029337961022,
      0.4113344391271607,
      0.3471767779428967,
      0.408205594948039,
      0.29780556556766413,
      0.4059282213782114,
      0.42438628123192235,
      0.37276399031098584,
      0.3850464243260996,
      0.31884593044594456,
      0.3611724723999499,
      0.0,
      0.3105438667155236,
      0.34198853614507896,
      0.2684456943176883,
      0.333626399307942,
      0.31599641477816065,
      0.32534389579843603,
      0.32985462733468207,
      0.37505255514199254,
      0.3104881953473533,
      0.3047722435222673
    ],
    [
      0.3036901361679494,
      0.32496542456247557,
      0.31559064534054615,
      0.31644507457127835,
      0.3278230974058536,
      0.353078749818021,
      0.20005279704677248,
      0.28355536322076214,
      0.39211896362699705,
      0.298655804270586,
      0.33459503683421343,
      0.23287396834921226,
      0.3347010337139551,
      0.3285503484666412,
      0.36599185704577164,
      0.33649204362439633,
      0.34010384993473175,
      0.2969265564349646,
      0.3011941481702154,
      0.0,
      0.38308778239282826,
      0.27562905604373333,
      0.3180815618884234,
      0.3136700403431263,
      0.349194976635971,
      0.2889441055739237,
      0.31186715490218786,
      0.3842219927797852,
      0.3318199853675683
    ],
    [
      0.37761714980094707,
      0.42764493581524343,
      0.4711085062477447,
      0.5029431261701833,
      0.48754826253217853,
      0.43652866005296365,
      0.2805477506161893,
      0.36607356788783996,
      0.5028760958508771,
      0.39252635136297465,
      0.40670338934806805,
      0.31139916419840885,
      0.44817784804303673,
      0.4501548672092901,
      0.4168044346050175,
      0.4802450276491066,
      0.4424464698140178,
      0.4361990922050265,
      0.39414960969688195,
      0.42377454330242714,
      0.0,
      0.2887521380309481,
      0.3750768570759939,
      0.37938654356492507,
      0.36985200351738934,
      0.379742119098579,
      0.4393655165707102,
      0.4124087988615328,
      0.341408254954487
    ],
    [
      0.2712232515216053,
      0.2585009906889486,
      0.2583453490974892,
      0.2761283371307388,
      0.27029369050887264,
      0.2512110195346957,
      0.2598607006238218,
      0.3233065409931086,
      0.27935372181170925,
      0.29281696540810986,
      0.2966378752337946,
      0.2882297858637748,
      0.2787768787884084,
      0.2970887319570741,
      0.3029055517264114,
      0.2719882149772719,
      0.28522009058141196,
      0.3021281103359783,
      0.2549619205891458,
      0.28518678634040673,
      0.28736416089477257,
      0.0,
      0.27730736981731496,
      0.2678279417347498,
      0.261318046917421,
      0.310858127498274,
      0.29106511921255906,
      0.26082300857339824,
      0.36389083804600575
    ],
    [
      0.3667812858130184,
      0.43297585403741956,
      0.4364431255841634,
      0.38028685219580316,
      0.4941777075957441,
      0.4432350546701893,
      0.3386342110245575,
      0.4182815784106819,
      0.4297176814917334,
      0.4237449537567022,
      0.40893073737673524,
      0.30885793040410237,
      0.3897788265247908,
      0.4024787964380927,
      0.3817780649669613,
      0.44120769230677404,
      0.46728712774546133,
      0.38144155490497855,
      0.41877940622338916,
      0.33386479584205087,
      0.4453680816894654,
      0.33106625169170045,
      0.0,
      0.3613954096806786,
      0.3863173997761884,
      0.4955814863812047,
      0.4061622499818005,
      0.32872536015531706,
      0.3609307076659285
    ],
    [
      0.33907643527669395,
      0.4038278839991356,
      0.37747816291992287,
      0.3628603871810525,
      0.33128889522732474,
      0.36471774643134514,
      0.31474432358772075,
      0.34786252657435845,
      0.3960122306289029,
      0.3921950250248438,
      0.38030974691561026,
      0.2837828391478254,
      0.37148437164484327,
      0.3926087410710737,
      0.40892592787202164,
      0.3742077337212648,
      0.35013113255410677,
      0.3458876556039381,
      0.4118360441441853,
      0.3453604614742103,
      0.3414388169596909,
      0.2568741453055874,
      0.33184251567521605,
      0.0,
      0.3113677503357286,
      0.30386527341375924,
      0.3695300619066684,
      0.35131384375411856,
      0.29129147052426907
    ],
    [
      0.2955148884234011,
      0.3725344403983595,
      0.3936745055051991,
      0.35193420999573766,
      0.3317050372811685,
      0.38321020942799255,
      0.2563549066548012,
      0.30565493465867277,
      0.3967708613705172,
      0.37116855739397003,
      0.3887101827718522,
      0.3195077888107791,
      0.36867725376128724,
      0.35284855598178777,
      0.3610978804673195,
      0.31933069716494633,
      0.3809575476068674,
      0.3424705799525316,
      0.35916959277339666,
      0.3777317080080489,
      0.3842115431911548,
      0.27121213463985816,
      0.3527082793643552,
      0.36158299045735465,
      0.0,
      0.3127121409709883,
      0.38921741328261494,
      0.4084307359437189,
      0.3146093716205076
    ],
    [
      0.32757053381406975,
      0.4784446045290971,
      0.428257820864121,
      0.4191258034296421,
      0.443652620110621,
      0.3642763407926579,
      0.294858909389234,
      0.4211483662416795,
      0.40617974922818,
      0.34712046492458426,
      0.36601081328606155,
      0.2786661668629562,
      0.40537401636920256,
      0.45060752184339203,
      0.3544669089301862,
      0.37729483430322697,
      0.3945282419378968,
      0.4266798132049321,
      0.43495518857325277,
      0.33704955267304904,
      0.4209100343096601,
      0.3032470248133532,
      0.4148652915826021,
      0.3233036716999951,
      0.30143057657526473,
      0.0,
      0.3642306099866697,
      0.30084403749594335,
      0.3515785913358589
    ],
    [
      0.33043799953949393,
      0.4407026784670498,
      0.45097826556456866,
      0.3751316820307915,
      0.4782139888500583,
      0.36497892473392324,
      0.31906665906894616,
      0.36794505476556894,
      0.4477932185544904,
      0.4029158440306648,
      0.43440189496374426,
      0.2763656410708579,
      0.39230563058405465,
      0.44476886288142015,
      0.34018772935935004,
      0.3946039691314036,
      0.35403190186342015,
      0.37941591237966943,
      0.4597021682111353,
      0.3657043869075749,
      0.4548119203098331,
      0.2897747384883371,
      0.37450188760716263,
      0.398088290264357,
      0.3360884184308901,
      0.3905492036606586,
      0.0,
      0.3546835630996674,
      0.3397460333994211
    ],
    [
      0.19722767674144204,
      0.2510822097010488,
      0.2837633752725819,
      0.28339709028863647,
      0.26501449324223914,
      0.3170267617110545,
      0.24349417425020992,
      0.2676278495203488,
      0.3451345147511937,
      0.27492836943684074,
      0.28978099919893574,
      0.2505433526636871,
      0.27641970575573205,
      0.26245975413694955,
      0.31830668202574164,
      0.2633752567826533,
      0.28906698612467796,
      0.26669199039857183,
      0.2693883885739341,
      0.3564661205683015,
      0.33157574833546133,
      0.23600251438583908,
      0.25018853088863735,
      0.30141272558569243,
      0.30254702801362887,
      0.261185198200073,
      0.27456209515511465,
      0.0,
      0.26712600949484133
    ],
    [
      0.3250175432419615,
      0.4007776369784588,
      0.36670074209004233,
      0.35107375232222826,
      0.3550218654295747,
      0.3495161016557926,
      0.3051869890027952,
      0.35833446132808966,
      0.3504199468664311,
      0.36170448794427035,
      0.37056406438837586,
      0.3331265608935585,
      0.3585939016036146,
      0.3749077374833041,
      0.38602312953168805,
      0.3195443542288132,
      0.3408639119248553,
      0.37294371828640616,
      0.32987131245271706,
      0.3496886975354345,
      0.34108736898703085,
      0.43906354733021646,
      0.3631453714629147,
      0.31122667746850863,
      0.34326361831701013,
      0.3669207492998623,
      0.3575454249480141,
      0.32153685966627976,
      0.0
    ]
  ],
  "row_avgs": [
    0.29300312927196565,
    0.34256084595433495,
    0.38877123405725744,
    0.4113225114534587,
    0.30692038120583065,
    0.40985299261168834,
    0.28797889573497176,
    0.37961528600268224,
    0.3391655200617755,
    0.33242717443980035,
    0.4200294117530884,
    0.27611191668238133,
    0.3564807734467455,
    0.35335666344241795,
    0.38583232171654663,
    0.364254492319656,
    0.4164164274103174,
    0.3625197995647729,
    0.3523502953010909,
    0.3194257698047461,
    0.408623610145821,
    0.2830221116574026,
    0.40050822086912985,
    0.3518615053169793,
    0.35084674813854255,
    0.37630993246812106,
    0.38421058815066117,
    0.27842127147157386,
    0.353702519023866
  ],
  "col_avgs": [
    0.31015044501759526,
    0.4024390753362126,
    0.3895868912387342,
    0.3758706220079503,
    0.3814428765936748,
    0.3636957171615897,
    0.29138435531991164,
    0.36713361775033654,
    0.39927816131027305,
    0.35926847366129244,
    0.37672848655577135,
    0.2802641468502692,
    0.38269900549733055,
    0.3925247668946947,
    0.35874855332937755,
    0.3789150111246822,
    0.35337678070518397,
    0.3726104353127054,
    0.374744288872205,
    0.34250397262182647,
    0.37469431642940215,
    0.28899767092856365,
    0.3459447692542525,
    0.34084152437485143,
    0.3178416644235637,
    0.35805696260638203,
    0.3589157119629394,
    0.32642307148786215,
    0.3208209748481921
  ],
  "combined_avgs": [
    0.3015767871447804,
    0.3724999606452738,
    0.38917906264799584,
    0.39359656673070453,
    0.34418162889975273,
    0.386774354886639,
    0.2896816255274417,
    0.3733744518765094,
    0.36922184068602426,
    0.34584782405054637,
    0.39837894915442984,
    0.27818803176632523,
    0.369589889472038,
    0.3729407151685563,
    0.3722904375229621,
    0.3715847517221691,
    0.3848966040577507,
    0.36756511743873915,
    0.36354729208664793,
    0.33096487121328627,
    0.3916589632876116,
    0.28600989129298315,
    0.37322649506169114,
    0.34635151484591536,
    0.3343442062810531,
    0.3671834475372515,
    0.3715631500568003,
    0.302422171479718,
    0.33726174693602906
  ],
  "gppm": [
    612.7070923491937,
    576.6196064746654,
    582.6612249880031,
    590.051445099082,
    583.180025624686,
    593.2834571678688,
    627.9961356399473,
    588.4884079453315,
    575.529528740907,
    591.9020803456284,
    589.9852494564665,
    635.3173655697744,
    586.3586032110501,
    586.1106986429744,
    599.3805736969372,
    586.7833309245967,
    599.4737495678372,
    590.5013686306195,
    588.0077678162712,
    603.741804787891,
    588.2266322615113,
    624.4054469480526,
    602.1394803717579,
    606.5940189209547,
    616.1726512014988,
    597.5132111098344,
    598.1122660088389,
    607.3393750129488,
    616.589576070343
  ],
  "gppm_normalized": [
    1.5014931361458872,
    1.3860289635899081,
    1.3902112499280403,
    1.4131539654472431,
    1.3916742925419066,
    1.4212907643178576,
    1.5095256601525295,
    1.4093427868003892,
    1.3724775585191495,
    1.4127541934221186,
    1.4070316885886829,
    1.5315812202217638,
    1.4054715537760296,
    1.4012067183345955,
    1.4353154278643903,
    1.4042542366869697,
    1.436761263655415,
    1.4021894594804858,
    1.3965029395716564,
    1.4506950868698552,
    1.4037113685192022,
    1.4891566325593915,
    1.4401870909525785,
    1.4545743149805739,
    1.477233958606784,
    1.4280770266025524,
    1.430515753961949,
    1.4608994304861687,
    1.466618692046678
  ],
  "token_counts": [
    844,
    485,
    390,
    428,
    396,
    426,
    466,
    423,
    385,
    389,
    374,
    530,
    438,
    406,
    419,
    417,
    428,
    341,
    345,
    457,
    385,
    373,
    405,
    429,
    420,
    401,
    409,
    456,
    333,
    354,
    486,
    423,
    408,
    496,
    475,
    436,
    410,
    490,
    437,
    391,
    468,
    467,
    443,
    440,
    469,
    405,
    446,
    490,
    431,
    391,
    548,
    393,
    414,
    417,
    434,
    401,
    501,
    406
  ],
  "response_lengths": [
    1814,
    2706,
    2364,
    2212,
    2875,
    2703,
    2617,
    2289,
    2715,
    2525,
    2274,
    2634,
    2651,
    2513,
    2303,
    2625,
    2184,
    2539,
    2892,
    2441,
    2097,
    3159,
    2322,
    2380,
    2341,
    2336,
    2316,
    2712,
    2315
  ]
}