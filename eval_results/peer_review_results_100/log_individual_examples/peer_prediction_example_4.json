{
  "example_idx": 4,
  "reference": "Published as a conference paper at ICLR 2023\n\nA STATISTICAL FRAMEWORK FOR PERSONALIZED FEDERATED LEARNING AND ESTIMATION: THEORY, ALGORITHMS, AND PRIVACY\n\nKaan Ozkara∗, Antonious M. Girgis∗, Deepesh Data & Suhas Diggavi Department of Electrical and Computer Engineering University of California, Los Angeles {kaan,amgirgis}@ucla.edu,deepesh.data@gmail.com,suhas@ee.ucla.edu\n\nABSTRACT\n\nA distinguishing characteristic of federated learning is that the (local) client data could have statistical heterogeneity. This heterogeneity has motivated the design of personalized learning, where individual (personalized) models are trained, through collaboration. There have been various personalization methods proposed in literature, with seemingly very different forms and methods ranging from use of a single global model for local regularization and model interpolation, to use of multiple global models for personalized clustering, etc. In this work, we begin with a statistical framework that unifies several different algorithms as well as suggest new algorithms. We apply our framework to personalized estimation, and connect it to the classical empirical Bayes’ methodology. We develop novel private personalized estimation under this framework. We then use our statistical framework to propose new personalized learning algorithms, including AdaPeD based on information-geometry regularization, which numerically outperforms several known algorithms. We develop privacy for personalized learning methods with guarantees for user-level privacy and composition. We numerically evaluate the performance as well as the privacy for both the estimation and learning problems, demonstrating the advantages of our proposed methods.\n\n1\n\nINTRODUCTION\n\nThe federated learning (FL) paradigm has had huge recent success both in industry and academia (McMahan et al., 2017; Kairouz et al., 2021), as it enables to leverage data available in dispersed devices for learning while maintaining data privacy. Yet, it was recently realized that for some applications, due to the statistical heterogeneity of local data, a single global learning model may perform poorly for individual clients. This motivated the need for personalized learning achieved through collaboration, and there have been a plethora of personalized models proposed in the literature as well (Fallah et al., 2020; Dinh et al., 2020; Deng et al., 2020; Mansour et al., 2020; Acar et al., 2021; Li et al., 2021; Ozkara et al., 2021; Zhang et al., 2021; Hu et al., 2020). However, the proposed approaches appear to use very different forms and methods, and there is a lack of understanding of an underlying fundamental statistical framework. Such a statistical framework could help develop theoretical bounds for performance, suggest new algorithms as well as perhaps give grounding to known methods. Our work addresses this gap.\n\nIn particular, we consider the fundamental question of how one can use collaboration to help personalized learning and estimation for users who have limited data that they want to keep private. Our proposed framework is founded on the requirement not only of personalization but also privacy, as maintaining local data privacy is what makes the federated learning framework attractive - and thus any algorithm that aims to be impactful needs to also give formal privacy guarantees. The goal of this paper is to develop a statistical framework that leads to new algorithms with provable privacy guarantees, and performance bounds. Our main contributions are (i) Development of a statistical framework for federated personalized estimation and learning (ii) Theoretical bounds and\n\n∗Equal Contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nnovel algorithms for private personalized estimation (iii) Design and privacy analysis of new private personalized learning algorithms; as elaborated below. Omitted proofs/details are in appendices.\n\n• Statistical framework: We connect this problem to the classical empirical Bayes’ method, pioneered by Stein (1956); James & Stein (1961); Robbins (1956), which proposed a hierarchical statistical model Gelman et al. (2013). This is modeled by an unknown population distribution P from which local parameters {θi} are generated, which in turn generate the local data through the distribution Q(θi). Despite the large literature on this topic, especially in the context of statistical estimation, creating a framework for FL poses new challenges. In contrast to classical empirical Bayes’ estimation, we introduce a distributed setting and develop a framework that allows information (communication and privacy) constraints1. This framework enables us to develop statistical performance bounds as well as suggests (private) personalized federated estimation algorithms. Moreover, we develop our framework beyond estimation, for (supervised) distributed learning, where clients want to build local predictive models with limited local (labeled) samples; we develop this framework in Section 3, which leads to new (private) personalized learning algorithms.\n\n• Private personalized estimation: Our goal is to estimate individual (local) parameters, when each user has very limited (heterogeneous) data. Such a scenario motivates federated estimation of individual parameters, privately. More precisely, the users observe data generated by an unknown distribution parametrized by their individual (unknown) local parameters θi, and want to estimate their local parameters θi leveraging very limited local data; see Section 2 for more details. For the hierarchical statistical model, classical results have shown that one can enhance the estimate of individual parameters based on the observations of a population of samples, despite having independently generated parameters from an unknown population distributions. However, this has not been studied for the distributed case, with privacy and communication constraints, which we do (see Theorem 2 for the Gaussian case and Theorem 4 for the Bernoulli case, and also for mixture population models in Appendix D). We estimate the (parametrized) population distribution under these privacy and communication constraints and use this as an empirical prior for local estimation. The effective amplification of local samples through collaboration, in Section 2, gives us theoretical insight about when collaboration is most useful, under privacy and/or communication constraints. Our results suggest how to optimally balance estimates from local and population models. We also numerically evaluate these methods, including application to polling data (see Section 4 and Appendices) to show advantages of such collaborative estimation compared to local methods.\n\n• Private personalized learning: The goal here is to obtain individual learning models capable of predicting labels with limited local data in a supervised learning setting. This is the use case for federated learning with privacy guarantees. It is intimately related to the estimation problem with distinctions including (i) to design good label predictors rather than just estimate local parameters (ii) the focus on iterative methods for optimization, requiring strong compositional privacy guarantees. Therefore, the statistical formulation for learning has a similar flavor to that in estimation, where there is a population model for local (parametrized) statistics for labeled data; see Section 3 for more details. We develop several algorithms, including AdaPeD (in Section 3.2), AdaMix (in Section 3.1), and DP-AdaPeD (in Section 3.3), inspired by the statistical framework. AdaPeD uses information divergence constraints along with adaptive weighting of local models and population models. By operating in probability (rather than Euclidean) space, using information-geometry (divergence), enables AdaPeD to operate with different local model sizes and architectures, giving it greater flexibility than existing methods. We integrate it with user-level privacy to develop DP-AdaPeD, with strong compositional privacy guarantees (Theorem 5). AdaMix is inspired by mixture population distributions, which adaptively weighs multiple global models and combines it with local data for personalization. We numerically evaluate these algorithms for synthetic and real data in Section 4.\n\nRelated Work. Our work can be seen in the intersection of personalized learning, estimation, and privacy. Below we give a brief description of related work; a more detailed comparison which connects our framework to other personalized algorithms is given in Appendix J.\n\nPersonalized FL: Recent work adopted different approaches for learning personalized models, which can be explained by our statistical framework for suitable choices of population distributions as explained in Appendix J: These include, meta-learning based methods (Fallah et al., 2020; Acar et al., 2021; Khodak et al., 2019); regularization (Deng et al., 2020; Mansour et al., 2020; Hanzely\n\n1The homogeneous case for distributed estimation is well-studied; see (Zhang, 2016) and references.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n& Richt ́arik, 2020); clustered FL (Zhang et al., 2021; Mansour et al., 2020; Ghosh et al., 2020; Smith et al., 2017) (Marfoq et al., 2021); using knowledge distillation (Lin et al., 2020; Ozkara et al., 2021); multi-task Learning (Dinh et al., 2020; Hanzely & Richt ́arik, 2020; Smith et al., 2017; Vanhaesebrouck et al., 2017; Zantedeschi et al., 2020); and using common representations (Du et al., 2021; Raghu et al., 2020; Tian et al., 2020; Collins et al., 2021) and references therein. Our work enables not just a unified view of these methods, but suggests new algorithms developed in this paper, along with privacy guarantees.\n\nAfter the conclusion of our work (Ozkara et al., 2022, July), we found two concurrent and independent works (Kotelevskii et al., 2022, June; Chen et al., 2022) that use a hierarchical Bayes approach to construct personalized learning algorithms, and are closest to our statistical framework. (Kotelevskii et al., 2022, June) is based on using a MCMC method2 to estimate a population distribution; such methods could be computationally intensive (see the discussion in (Blei et al., 2003); (Chen et al., 2022) considers the unimodal Gaussian prior, and effectively does what the classical empirical Bayes suggests (see also Theorem 1). None of these works consider privacy, which we do both for estimation and learning algorithms (see Theorems 2, 4, Appendix D, and for DP-AdaPeD in Theorem 5). Note that MCMC methods could have detrimental privacy implications. Also, they do not include information-geometric techniques (like our AdaPeD) or methods inspired by mixture distributions (e.g., AdaMix).\n\nPrivacy for Personalized Learning. There has been a lot of work in privacy for FL when the goal is to learn a single global model (see (Girgis et al., 2021b) and references therein); though there are fewer papers that address user-level privacy (Liu et al., 2020; Levy et al., 2021; Ghazi et al., 2021). There has been more recent work on applying these ideas to learn personalized models (Girgis et al., 2022; Jain et al., 2021b; Geyer et al., 2017; Hu et al., 2020; Li et al., 2020). These are for specific algorithms/models, e.g., Jain et al. (2021b) focuses on the common representation model for linear regression described earlier or on item-level privacy (Hu et al., 2020; Li et al., 2020). We believe that DP-AdaPeD proposed in this paper is among the first user-level private personalized learning algorithms with compositional guarantees, applicable to general deep learning architectures.\n\n2 PERSONALIZED ESTIMATION\n\nWe consider a client-server architecture, where there are m clients. Let P(Γ) denote a global population distribution that is parameterized by an unknown Γ and let θ1, . . . , θm are sampled i.i.d. from P(Γ) and are unknown to the clients. Client i is given a dataset Xi := (Xi1, . . . , Xin), where Xij, j ∈ [n] are sampled i.i.d. from some distribution Q(θi), parameterized by θi ∈ Rd. Note that heterogeneity in clients’ datasets is induced through the variance in P(Γ), and if the variance of P(Γ) is zero, then all clients observe i.i.d. datasets sampled from the same underlying distribution.\n\nThe goal at client i for all i ∈ [m] is to estimate θi through the help of the server. We focus on one-round communication schemes, where client j applies a (potentially randomized) mechanism q on its dataset Xj and sends qj := q(Xj) to the server, who aggregates the received messages, which is denoted by Agg(q1, . . . , qm), and broadcasts that to all clients. Based on (Xi, Agg(q1, . . . , qm)), client i outputs an estimate (cid:98)θi of θi. We measure the performance of (cid:98)θi through the Bayesian risk for mean squared error (MSE), as defined below (where P is the true prior distribution with associated density π, θi ∼ P is the true local parameter, and (cid:98)θi = (cid:98)θ(Xi, Agg(q1, . . . , qm)) is the estimator): (cid:90)\n\nEθi∼PE\n\n(cid:98)θi,q,X1,...,Xm\n\n(cid:107)(cid:98)θi − θi(cid:107)2 =\n\nE\n\n(cid:98)θi,q,X1,...,Xm\n\n(cid:107)(cid:98)θi − θi(cid:107)2π(θi)dθi.\n\n(1)\n\nThe above statistical framework can model many different scenarios, and we will study in detail three settings: Gaussian and Bernoulli models (Sections 2.1, 2.2 below), and Mixture model (Appendix D).\n\n2.1 GAUSSIAN MODEL\n\nId) for all i ∈ [m], which In the Gaussian setting, P(Γ) = N (μ, σ2 Id) i.i.d. for i ∈ [m]. implies that θ1, . . . , θm ∼ N (μ, σ2 Here, σθ ≥ 0, σx > 0 are known, and μ, θ1, . . . , θm are unknown. For the case of a single local\n\nId) and Q(θi) = N (θi, σ2 Id) i.i.d. and Xi1, . . . , Xin ∼ N (θi, σ2\n\nx\n\nx\n\nθ\n\nθ\n\n2In our understanding their numerics seem to be restricted to a unimodal Gaussian population model.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nsample this is identical to the classical James-Stein estimator (James & Stein, 1961); Theorem 1 does a simple extension for multiple local samples and is actually a stepping stone for the information constrained estimation result of Theorem 2. Omitted proofs/details are provided in Appendix B.\n\nOur proposed estimator. Since there is no distribution on μ, and given μ, we know the distribution of θi’s, and subsequently, of Xij’s. So, we consider the maximum likelihood estimator:\n\n(cid:98)θ1, . . . , (cid:98)θm, (cid:98)μ := arg max\n\nθ1,...,θm,μ\n\np{θi,Xi}|μ (θ1, . . . , θm, X1, . . . , Xm|μ)\n\n(2)\n\nTheorem 1. Solving (2) yields the following closed form expressions for (cid:98)μ and (cid:98)θ1, . . . , (cid:98)θm:\n\n(cid:98)μ =\n\n1 m\n\nm (cid:88)\n\ni=1\n\nX i\n\nand\n\n(cid:98)θi = aX i + (1 − a)(cid:98)μ, for i ∈ [m],\n\nwhere a =\n\nThe above estimator achieves the MSE: Eθi,X1,...,Xm (cid:107)(cid:98)θi − θi(cid:107)2 ≤ dσ2\n\nx\n\nn\n\n(cid:0) 1−a\n\nm + a(cid:1).\n\nσ2 θ\nθ + σ2\n\nσ2\n\nx/n\n\n.\n\n(3)\n\nn\n\nm\n\n(cid:80)n\n\n(cid:80)m\n\ni=1 qi back to the clients.\n\nx/mn. Otherwise, when σ2\n\nIt follows that the mechanism q and the aggregation function Agg for the estimators in (3) (as described in (1)) are just the average functions, where client i sends qi = q(Xi) := X i = 1 j=1 Xij to the server, and the server sends (cid:98)μ := Agg(q1, . . . , qm) = 1 Remark 1 (Personalized estimate vs. local estimate). When σθ → 0, then a → 0, which implies that (cid:98)θi → (cid:98)μ and MSE → dσ2 x/n or n → ∞, then a → 1, which implies that (cid:98)θi → X i and MSE → dσ2 x/n. These conform to the facts that (i) when there is no heterogeneity, then the global average is the best estimator, and (ii) when heterogeneity is not small, and we have a lot of local samples, then the local average is the best estimator. Observe that the multiplicative gap between the MSE of the proposed personalized estimator and the MSE of the local estimator (based on local data only, which gives an MSE of dσ2 m + a) ≤ 1 that proves the superiority of the personalized model over the local model, which is equal to 1/m when σθ = 0 and equal to 0.01 when m = 104, n = 100 and σ2 Remark 2 (Optimality of our personalized estimator). In Appendix B, we show the minimax lower m + a(cid:1), which exactly matches the upper bound: inf bound on the MSE in Theorem 1, thus establishes the optimality our personalized estimator in (3).\n\nθ is large in comparison to σ2\n\nx)(cid:107)(cid:98)θ(X) − θ(cid:107)2 ≥ dσ2\n\nθ = 10−3, for example.\n\nx/n) is given by ( 1−a\n\nEX∼N (θ,σ2\n\nx = 10, σ2\n\n(cid:98)θ supθ∈Θ\n\n(cid:0) 1−a\n\nn\n\nx\n\nPrivacy and communication constraints. Observe that the scheme presented above does not protect privacy of clients’ data and messages from the clients to the server can be made communicationefficient. These could be achieved by employing specific mechanisms q at clients: For privacy, we can take a differentially-private q, and for communication-efficiency, we can take q to be a quantizer. Inspired by the scheme presented above, here we consider q to be a function q : Rd → Y, that takes the average of n data points as its input, and the aggregator function Agg to be the average function. Define (cid:98)μq := 1\n\ni=1 q(X i) and consider the following personalized estimator for the i-th client:\n\n(cid:80)m\n\nm\n\n(cid:98)θi = aX i + (1 − a)(cid:98)μq,\n\nfor some a ∈ [0, 1].\n\n(4)\n\nTheorem 2. Suppose for all x ∈ Rd, q satisfies E[q(x)] = x and E(cid:107)q(x) − x(cid:107)2 ≤ dσ2 finite σq. Then the personalized estimator in (4) has MSE:\n\nq for some\n\nEθi,q,X1,...,Xm(cid:107)(cid:98)θi − θi(cid:107)2 ≤\n\ndσ2 x\nn\n\n(cid:16) 1 − a m\n\n(cid:17)\n\n+ a\n\nwhere\n\na =\n\nσ2 θ + σ2\n\nσ2\n\nθ + σ2\n\nq/m−1\n\nq/m−1 + σ2\n\nx/n\n\n.\n\n(5)\n\nFurthermore, assuming μ ∈ [−r, r]d for some constant r (but μ is unknown), we have:\n\n1. Communication efficiency: For any k ∈ N, there is a q whose output can be represented using k-bits (i.e., q is a quantizer) that achieves the MSE in (5) with probability at least 1 − 2/mn and with σq = b\n\n(cid:112)log(m2n) + σx√\n\n(cid:112)log(m2n).\n\n(2k−1) , where b = r + σθ\n\nn\n\n2. Privacy: For any (cid:15)0 ∈ (0, 1), δ > 0, there is a q that is user-level ((cid:15)0, δ)-locally differentially private, that achieves the MSE in (5) with probability at least 1 − 2/mn and with σq = b\n(cid:15)0\n\n(cid:112)8 log(2/δ), where b = r + σθ\n\n(cid:112)log(m2n) + σx√\n\n(cid:112)log(m2n).\n\nn\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n2.2 BERNOULLI MODEL\n\nFor the Bernoulli model, P is supported on [0, 1], and p1, . . . , pm are sampled i.i.d. from P, and client i is given n i.i.d. samples Xi1, . . . , Xin ∼ Bern(pi). This setting has been studied in (Tian et al. (2017); Vinayak et al. (2019)) for estimating P, whereas, our goal is to estimate individual parameter pi at client i using the information from other clients. In order to derive a closed form MSE result, we assume that P is the Beta distribution.3 Here, Γ = (α, β), p1, . . . , pm are unknown, and client i’s goal is to estimate pi such that the Bayesian risk Epi∼πE (cid:98)pi,X1,...,Xm((cid:98)pi − pi)2 is minimized, where π denotes the density of the Beta distribution. Omitted proofs/details are provided in Appendix C.\n\nAnalogous to the Gaussian case, we can show that if α, β are known, then the posterior mean estimator has a closed form expression: (cid:98)pi = aX i + (1 − a) α α+β , where a = n/α+β+n and α/(α+β) is the mean of the beta distribution. When α, β are unknown, inspired by the above discussion, a natural approach would be to estimate the global mean μ = α/(α+β) and the weight a = n/(α+β+n), and use that in the above estimator. Note that, for a we need to estimate α + β, which is equal to μ(1−μ)/σ2 − 1, where σ2 = αβ/(α+β)2(α+β+1) is the variance of the beta distribution. Therefore, it is enough to estimate μ and σ2 for the personalized estimators {(cid:98)pi}. In order to make calculations simpler, instead of making one estimate of μ, σ2 for all clients, we let each client make its own estimate of μ, σ2 (without using i = 1 their own data) as: (cid:98)μi = 1 l(cid:54)=i(X l − (cid:98)μl)2,4 and then define the local weight as (cid:98)ai =\n\ni −1+n . Now, client i uses the following personalized estimator:\n\nl(cid:54)=i X l and (cid:98)σ2\n\n(cid:98)μi(1− (cid:98)μi)/(cid:98)σ2\n\nm−2\n\nm−1\n\n(cid:80)\n\n(cid:80)\n\nn\n\n(cid:98)pi = (cid:98)aiX i + (1 − (cid:98)ai)(cid:98)μi.\n\n(6)\n\nTheorem 3. With probability at least 1− 1 (α+β)2(α+β+1) + 3 log(4m2n) Epi∼πEX1,...,Xm ((cid:98)pi − pi)2 ≤ E[(cid:98)a2 Remark 3. When n → ∞, then (cid:98)ai → 1, which implies that MSE tends to the MSE of the local estimator X i, which means if local samples are abundant, collaboration does not help much. When σ2 = αβ/(α+β)2(α+β+1) → 0, i.e. there is very small heterogeneity in the system, then (cid:98)ai → 0, which implies that MSE tends to the error due to moment estimation (the last term in the MSE in Theorem 3).\n\nmn , the MSE of the personalized estimator in (6) is given by: (cid:1).\n\n(cid:1) + E[(1 −(cid:98)ai)2](cid:0)\n\nαβ n(α+β)(α+β+1)\n\ni ](cid:0)\n\nm−1\n\nαβ\n\nPrivacy constraints. For any privacy parameter (cid:15)0 > 0 and input x ∈ [0, 1], define qpriv : [0, 1] → R:\n\nqpriv(x) =\n\n(cid:40) −1\n\ne(cid:15)0 −1 w.p. e(cid:15)0 −1 w.p.\n\ne(cid:15)0\n\ne(cid:15)0\n\ne(cid:15)0 +1 − x e(cid:15)0 −1 e(cid:15)0 +1 , e(cid:15)0 +1 + x e(cid:15)0 −1 e(cid:15)0 +1 .\n\n1\n\n(7)\n\nThe mechanism qpriv is unbiased and satisfies user-level (cid:15)0-LDP. Thus, the ith client sends qpriv(X i) to the server, which computes (cid:98)μpriv i = 1 =\n\n(cid:80)\n\ni\n\nl\n\ni\n\npriv (cid:98)μ i\n\nn )/(cid:98)σ\n\npriv (1− (cid:98)μ i\n\n1 m−2\n\nl(cid:54)=i(qpriv(X l)) − (cid:98)μpriv i =\n\n(cid:80) )2 and sends ((cid:98)μpriv and uses (cid:98)ppriv defines (cid:98)apriv Theorem 4. With probability at least 1 − 1 fined above is given by: Epi∼πEqpriv,X1,...,Xm ((cid:98)ppriv (α+β)2(α+β+1) + (e(cid:15)0 +1)2 log(4m2n) (cid:98)apriv\n\n3(e(cid:15)0 −1)2(m−1)\n\n)2](cid:0)\n\n2(priv) i\n\n(cid:1).\n\n+n\n\nαβ\n\ni\n\ni\n\nm−1\n\nl(cid:54)=i qpriv(X l) and the variance (cid:98)σ2(priv) ) to client i. Upon receiving this, client i\n\ni X i + (1 − (cid:98)apriv\n\n, (cid:98)σ2(priv) i = (cid:98)apriv mn , the MSE of the personalized estimator (cid:98)ppriv )2](cid:0)\n\n)(cid:98)μpriv to estimate pi.\n\ni − pi)2 ≤ E[((cid:98)apriv\n\nαβ n(α+β)(α+β+1)\n\nde- (cid:1) + E[(1 −\n\ni\n\ni\n\ni\n\nSee Remark 4 (in Appendix B) and Remarks 6 and 7 (in Appendix C) for a discussion on privacy, communication efficiency, and client sampling.\n\n3 PERSONALIZED LEARNING\n\nConsider a client-server architecture with m clients. There is an unknown global population distribution P(Γ)5 over Rd from which m i.i.d. local parameters θ1, . . . , θm ∈ Rd are sampled. Each client\n\n3Beta distribution has a density Beta(α, β) = 1 where B(α, β) is a normalizing constant. Its mean is α\n\nB(α,β) xα−1(1−x)β−1 is defined for α, β > 0 and x ∈ [0, 1],\n\nα+β and the variance is\n\nαβ\n\n(α+β)2(α+β+1) .\n\n4Upon receiving {X i} from all clients, the server can compute {(cid:98)μi, (cid:98)σ2 i ) to the i-th client. 5For simplicity we will consider this unknown population distribution P to be parametrized by unknown\n\ni } and sends ((cid:98)μi, (cid:98)σ2\n\n(arbitrary) parameters Γ.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\ni ∈ [m] is provided with a dataset consisting of n data points {(Xi1, Yi1), . . . , (Xin, Yin)}, where Yij’s are generated from (Xij, θi) using some distribution pθi(Yij|Xij). Let Yi := (Yi1, . . . , Yin) and Xi := (Xi1, . . . , Xin) for i ∈ [m]. The underlying statistical model for our setting is given by\n\np{θi,Yi}|{Xi}(θ1, . . . , θm, Y1, . . . , Ym|X1, . . . , Xm) =\n\nm (cid:89)\n\ni=1\n\np(θi)\n\nm (cid:89)\n\nn (cid:89)\n\ni=1\n\nj=1\n\npθi(Yij|Xij).\n\n(8)\n\nNote that if we minimize the negative log likelihood of (8), we would get the optimal parameters:\n\n(cid:98)θ1, . . . , (cid:98)θm := arg min θ1,...,θm\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n− log(pθi(Yij|Xij)) +\n\nm (cid:88)\n\ni=1\n\n− log(p(θi)).\n\n(9)\n\nHere, fi(θi) := (cid:80)n j=1 − log(pθi(Yij|Xij)) denotes the loss function at the i-th client, which only depends on the local data, and R({θi}) := (cid:80)m i=1 − log(p(θi)) is the regularizer that depends on the (unknown) global population distribution P (parametrized by unknown Γ). Note that when clients have little data and we have large number of clients, i.e., n (cid:28) m – the setting of federated learning, clients may not be able to learn good personalized models from their local data alone (if they do, it would lead to large loss). In order to learn better personalized models, clients may utilize other clients’ data through collaboration, and the above regularizer (and estimates of the unknown prior distribution P, through estimating its parameters Γ) dictates how the collaboration might be utilized. The above-described statistical framework (9) can model many different scenarios, as detailed below:\n\n2\n\nθ,l\n\n2σ2\n\nl=1, {μl}k\n\n2 log(2πσ2\n\n1. When P(Γ) ≡ GM({pl}k\n\nl=1, {μl}k i=1 log (cid:0) (cid:80)k\n\nl=1) is a Gaussian mixture,\n\nl=1, {σ2 l=1}(cid:1) : pl ≥ 0, (cid:80)k\n\nl=1, {σθ,l}k l=1 pl exp(− (cid:107)μl−θi(cid:107)2\n\nθ,l}k for Γ = l=1 pl = 1, σθ,l ≥ 0, μl ∈ Rd(cid:9), then R({θi}) = )/((2πσθ,l)d/2)(cid:1). Here, the client models θ1, . . . , θm are Id) with prob. pl, for l = 1, . . . , k. For k = 1,\n\n(cid:8)(cid:0){pl}k − (cid:80)m drawn i.i.d. from P(Γ), where θi ∼ N (μl, σ2 R({θi}) = md . Here, unknown μ can be connected to the global model and θi’s as local models, and the alternating iterative optimization optimizes over both. This justifies the use of (cid:96)2 regularizer in earlier personalized learning works (Dinh et al., 2020; Ozkara et al., 2021; Hanzely & Richt ́arik, 2020; Hanzely et al., 2020; Li et al., 2021). 2. When P(Γ) ≡ Laplace(μ, b), for Γ = {μ, b > 0}, then R({θi}) = m log(2b) + (cid:80)m 3. When pθi(Yij|Xij) is according to N (θi, σ2\n\nx), then fi(θi) is the quadratic loss as in linear regression. When pθi (Yij|Xij) = σ((cid:104)θi, Xij(cid:105))Yij (1 − σ((cid:104)θi, Xij(cid:105)))(1−Yij ), where σ(z) = 1/1+e−z for any z ∈ R, then fi(θi) is the cross-entropy (or logistic) loss as in logistic regression.\n\nθ ) + (cid:80)m\n\n(cid:107)μ−θi(cid:107)2 2σ2 θ\n\n(cid:107)θi−μ(cid:107)1 b\n\ni=1\n\ni=1\n\nθ,l\n\n.\n\n2\n\n3.1 AD AMI X: ADAPTIVE PERSONALIZATION WITH GAUSSIAN MIXTURE PRIOR\n\nNow we write the full objective function for the Gaussian mixture prior model for a generic local loss function fi(θi) at client i (the case of linear/logistic regression with (single) Gaussian prior and solving using alternating gradient descent is discussed in Appendices E, F.): m\n(cid:88)\n\nm (cid:88)\n\nk (cid:88)\n\n(cid:17)\n\n(cid:16)\n\nF gm\n\ni\n\n(θi) :=\n\nfi(θi) − log(\n\npl exp(−\n\n)/((2πσθ,l)d/2))\n\narg min {θi},{μl},{pl},{σθ,l}\n\n(cid:107)μl − θi(cid:107)2 2σ2\n\n2\n\nθ,l\n\ni=1\n\ni=1\n\nl=1\n\n(10) A common example of fi(θi) is a generic neural network loss function with multi-class softmax output layer and cross entropy loss, i.e., fi(θi) := (cid:80)n j=1 − log(pθi(Yij|Xij)), where pθi(Yij|Xij) = σ((cid:104)θi, Xij(cid:105))Yij (1 − σ((cid:104)θi, Xij(cid:105)))(1−Yij ), where σ(z) = 1/1+e−z for any z ∈ R. To solve (10), we can either use an alternating gradient descent approach, or we can use a clustering based approach where the server runs a (soft) clustering algorithm on received personalized models. We adopt the second approach here (described in Algorithm 1) as it provides an interesting point of view and can be combined with DP clustering algorithms. Here clients receive the global parameters from the server and do a local iteration on the personalized model (multiple local iterations can be introduced as in FedAvg (McMahan et al., 2017)), later the clients send the personalized models. Receiving the personalized models, server initiates GMM algorithm that outputs global parameters 6\n\n6A discrete mixture model can be proposed as a special case of GM with 0 variance. With this we can recover\n\na similar algorithm as in Marfoq et al. (2021). Further details are presented in Appendix G.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n3.2 AD APED: ADAPTIVE PERSONALIZATION VIA DISTILLATION\n\nIt has been empirically observed that the knowledge distillation (KD) regularizer (between local and global models) results in better performance than the (cid:96)2 regularizer (Ozkara et al., 2021). In fact, using our framework, we can define, for the first time, a certain prior distribution that gives the KD regularizer (see Appendix H). We use the following loss function at the i-th client:\n\nfi(θi) +\n\n1 2\n\nlog(2ψ) +\n\nf KD i\n\n(θi, μ) 2ψ\n\n,\n\n(11)\n\nwhere μ denotes the global model, θi denotes the personalized model at client i, and ψ can be viewed as controlling heterogeneity. The goal for each client is to minimize its local loss function, so individual components cannot be too large. For the second term, this implies that ψ cannot be unbounded. For the third term, if f KD (θi, μ) is large, then ψ will also increase i\n(implying that the local parameters are too deviated from the global parameter), hence, it is better to emphasize local training loss to make the first term small. If f KD (θi, μ) is small, then i\nψ will also decrease (implying that the local parameters are close to the global parameter), so it is better to collaborate and learn better personalized models. Such adaptive weighting quantifies the uncertainty in population distribution during training, balances the learning accordingly, and improves the empirical performance over nonadaptive methods, e.g., (Ozkara et al., 2021).\n\nAlgorithm 1 Personalized Learning with Gaussian Mixture Prior (AdaMix) Input: Number of iterations T , local datasets (Xi, Yi) for i ∈ [m], learning rate η. 1 , . . . , θ(0)\n\nand\n\nm\n\nθ(0) k , σ(0)\n\n1 , . . . , μ(0)\n\nθ,1, . . . , σ(0) θ,k.\n\n1: Initialize P(0), μ(0)\n\n2: for t = 1 to T do 3: On Clients: 4: 5:\n\nfor i = 1 to m: do\n\nReceive P(t−1), μ(t−1) σ(t−1) , . . . , σ(t−1) Update the personalized parameters:\n\n, . . . , μ(t−1) from the server\n\nθ,k\n\nθ,1\n\nk\n\n1\n\n, and\n\ni ← θ(t−1) θ(t)\n\ni\n\n− η∇θ(t−1)\n\ni\n\nF gm\n\ni\n\n(θ(t−1)\n\ni\n\n)\n\nSend θ(t)\n\ni\n\nto the server\n\nend for At the Server: Receive θ(t) 1 , . . . , θ(t) Update the global parameters:\n\nm from the clients\n\nP(t), μ(t)\n\n1 , . . . , μ(t)\n\nθ,1, . . . , σ(t)\n\nθ,k\n\nk , σ(t) ← GMM(cid:0)θ(t)\n\n1 , . . . , θ(t)\n\nm , k(cid:1)\n\nBroadcast P(t), {μ(t) clients\n\ni }k\n\ni=1, {σ(t)\n\nθ,i}k\n\ni=1 to all\n\n6:\n\n7: 8: 9: 10: 11:\n\n12:\n\n13: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\nTo optimize (11) we propose an alternating minimization approach, which we call AdaPeD; see Algorithm 2. Besides the personalized model θt i, each client i keeps local copies of the global model μt i , and at synchronization times, server aggregates them to obtain global versions of these μt, ψt. In this way, the local training of θt i. In the end, clients have learned their personalized models {θT\n\ni also incorporates knowledge from other clients’ data through μt i=1.\n\ni and of the dissimilarity term ψt\n\ni }m\n\n3.3 DP-AD APED: DIFFERENTIALLY PRIVATE ADAPTIVE PERSONALIZATION VIA DISTILL.\n\nNote that client i communicates μt the gradients ht i, kt order to obtain DP-AdaPeD, we replace lines 13 and 15, respectively, by the update rules:\n\ni (which are updated by accessing the dataset for computing i . In\n\ni , client i adds appropriate noise to hk\n\ni ) to the server. So, to privatize μt\n\ni, ψt\n\ni, ψt\n\ni , kt\n\nμt+1\n\ni = μt\n\ni − η2\n\n(cid:16)\n\ni\n\nht max{(cid:107)ht i(cid:107)/C1, 1} Id) and ν2 ∼ N (0, σ2\n\n+ ν1\n\n(cid:17)\n\nand ψt+1\n\ni = ψt\n\ni − η3\n\n(cid:16)\n\nkt i\nmax{|kt i |/C2, 1}\n\n(cid:17)\n\n,\n\n+ ν2\n\nwhere ν1 ∼ N (0, σ2 q1 privacy level and C1, C2, which are some predefined constants.\n\nq2\n\n), for some σq1, σq2 > 0 that depend on the desired\n\nThe theorem below (proved in Appendix I) states the R ́enyi Differential Privacy (RDP) guarantees. Theorem 5. After T iterations, DP-AdaPeD satisfies (α, (cid:15)(α))-RDP for α > 1, where (cid:15)(α) = (cid:0) K m denotes the sampling ratio of clients at each global iteration. m\n\n(cid:16) C2 1\nKσ2\n\n, where K\n\n+ C2\n\n2 Kσ2\n\nτ α\n\n6 T\n\n(cid:1)2\n\n(cid:17)\n\nq1\n\nq2\n\nWe bound the RDP, as it gives better privacy composition than using the strong composition (Mironov et al., 2019). We can also convert our results to user-level ((cid:15), δ)-DP by using the standard conversion from RDP to ((cid:15), δ)-DP (Canonne et al., 2020). See Appendix A for background on privacy.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n(a) Private Estimation (Sec. 2.1)\n\n(b) Private Learning (Sec. 3.3)\n\nFigure 1: In Fig. 1a, we plot MSE vs. (cid:15)0 for personalized estimation. In Fig. 1b, we plot Test Accuracy vs. (cid:15) on FEMNIST with client sampling 0.33, for DP-AdaPeD with unsampled client iterations. Since local training is private, both plots remain constant against (cid:15).\n\n4 EXPERIMENTS\n\nPersonalized Estimation. We run one experiment for Bernoulli setting with real political data and the other for Gaussian setting with synthetic data. The latter one is differentially private.\n\n• Political tendencies on county level. One natural application of Bernoulli setting is modeling bipartisan elections (Tian et al., 2017). We did a case study by using US presidential elections on county level between 2000-2020, with m = 3112 counties in our dataset. For each county the goal is to determine the political tendency parameter pi. Given 6 election data we did 6-fold cross validation, with 5 elections for training and 1 election for test data. Local estimator takes an average of 5 training samples and personalized estimator is the posterior mean. To simulate a Bernoulli setting we set the data equal to 1 if Republican party won the election and 0 otherwise. We observe the personalized estimator provides MSE (averaged over 6 runs) gain of 10.7 ± 1.9% against local estimator.\n\nAlgorithm 2 Adaptive Personalization via Distillation (AdaPeD) Parameters: local variances {ψ0 models {θ0 {μ0 gap τ\n\ni }, personalized i }, local copies of the global model i }, learning rates η1, η2, η3, synchronization\n\nif τ divides t then On Server do: Choose a subset Kt ⊆ [n] of K clients Broadcast μt and ψt On Clients i ∈ Kt (in parallel) do: Receive μt, ψt; set μt\n\n1: for t = 0 to T − 1 do 2: 3: 4: 5: 6: 7: 8: 9: On Clients i ∈ Kt (in parallel) do: i (θt f KD 2ψt i\n\ni = μt, ψt\n\nCompute gt\n\ni := ∇θt\n\ni = ψt\n\nfi(θt\n\nend if\n\ni) +\n\n∇θt\n\ni\n\ni\n\n10:\n\ni,μt i)\n\ni)/2ψt\n\ni\n\ni\n\ni\n\ni\n\n,μt\n\n11: 12:\n\n13: 14:\n\ni − η1gt i (θt+1 f KD i − η2ht − f KD i − η3kt\n\ni = θt i := ∇μt i = μt i := 1 i = ψt\n\nUpdate: θt+1 Compute ht Update: μt+1 Compute kt Update: ψt+1 if τ divides t + 1 then Clients send μt Server receives {μt Server computes μt+1 = 1 and ψt+1 = 1\n\n• DP personalized estimation. To measure the performance tradeoff of the DP mechanism described in Section 2.1, we create a synthetic experiment for Gaussian setting. We let m = 10000, n = 15 and σθ = 0.1, σx = 0.5, and create a dataset at each client as described in Gaussian setting. Applying the DP mechanism we obtain the following result in Figure 1a. Here, as expected, when privacy is low ((cid:15)0 is high) the private personalized estimator recovers the regular personalized estimator. For higher privacy the private estimator’s performance starts to become worse than the non-private estimator.\n\nend if 20: 21: end for Output: Personalized models (θT\n\n15: 16: 17: 18: 19:\n\ni i (θt+1\n\ni and ψt\n\ni∈Kt ψt\n\ni )m\n\n2ψt i\n\n(cid:80)\n\ni=1\n\nK\n\nK\n\ni\n\ni\n\ni\n\ni\n\ni to Server i}i∈Kt and {ψt (cid:80)\n\n,μt+1\n\n)/2(ψt\n\ni )2\n\ni }i∈Kt i∈Kt μt\n\ni\n\nPersonalized Learning. First we describe the experiment setting and then the results.\n\n• Experiment setting. We consider image classification on MNIST, FEMNIST (Caldas et al., 2018), CIFAR-10, CIFAR-100 (experimental details for CIFAR-100 is given in Appendix K); and train a CNN, similar to the one considered in (McMahan et al., 2017), that has 2 convolutional and 3 fully connected layers. We set m = 66 for FEMNIST and m = 50 for MNIST, CIFAR-10, CIFAR-100. For FEMNIST, we use a subset of 198 writers so that each client has access to data from 3 authors, which results in a natural type of data heterogeneity due to writing styles of authors. On MNIST, CIFAR-10 we introduce pathological heterogeneity by letting each client sample data from 3 and 4 randomly selected classes only, respectively. We set τ = 10 and vary the batch size so that each epoch consists of 60 iterations. On MNIST we train for 50 epochs, on CIFAR-10 for 250 epochs, on FEMNIST for 40 and 80 epochs, for 0.33 and 0.15 client sampling ratio, respectively. We discuss further details in Appendix K.\n\n8\n\n0.10.20.30.40.50.60.70.8ε00.60.81.01.21.41.6MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized Estimator456789ε93.093.594.094.595.095.596.096.597.0Test Accuracy (in %)Local trainingDP-AdaPeDPublished as a conference paper at ICLR 2023\n\nTable 1: Test accuracy (in %) for CNN model. The CIFAR-10, MNIST, and FEMNIST columns have client sampling ratios K\n\nn of 0.2, 0.1, and 0.15, respectively.\n\nMethod\n\nCIFAR-10\n\nCIFAR-100\n\nFEMNIST\n\nFedAvg FedAvg+fine tuning (Jiang et al., 2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\n60.86 ± 0.94 63.12 ± 0.31 72.49 ± 0.42 69.53 ± 0.16 59.95 ± 0.79 71.61 ± 0.70 71.09 ± 0.67\n\n30.48 ± 0.33 39.98 ± 0.26 53.11 ± 0.34 43.65 ± 0.18 34.78 ± 0.41 51.94 ± 0.21 50.42 ± 0.26\n\n92.18 ± 0.13 94.12 ± 0.26 96.55 ± 0.32 94.95 ± 0.55 93.51 ± 0.31 95.99 ± 0.08 95.12 ± 0.18\n\nMethod\n\n(cid:15) = 3.35\n\n(cid:15) = 13.16\n\n(cid:15) = 27.30\n\nDP-FedAvg DP-AdaPeD (Ours)\n\n11.73 ± 0.85 93.32 ± 1.18\n\n29.91 ± 1.28 98.51 ± 0.90\n\n55.79 ± 0.29 99.01 ± 0.65\n\nTable 2: (DP-AdaPeD) Test Accuracy (in %) vs. (cid:15) on MNIST without client sampling.\n\nMethod\n\nn = 10\n\nn = 20\n\nn = 30\n\nLocal Training AdaMix\n\n39.93 ± 0.13 10.42 ± 0.15\n\n30.02 ± 0.08 3.12 ± 0.04\n\n19.97 ± 0.07 2.55 ± 0.04\n\nTable 3: Mean squared error of our AdaMix algorithm and the local training for linear regression.\n\n• Results. In Table 1 we compare AdaPeD against FedAvg (McMahan et al., 2017), FedAvg+ (Jiang et al., 2019) and various personalized FL algorithms: pFedMe (Dinh et al., 2020), Per-FedAvg (Fallah et al., 2020), QuPeD (Ozkara et al., 2021) without model compression, and Federated ML (Shen et al., 2020). We report further results in Appendix K. We observe AdaPeD consistently outperforms other methods. It can be seen that methods that use knowledge distillation perform better; on top of this, AdaPeD enables us adjust the dependence on collaboration according to the compatibility of global and local decisions/scores. For instance, we set σ2 θ to a certain value initially, and observe it progressively decrease, which implies clients start to rely on the collaboration more and more. Interestingly, this is not always the case: for DP-AdaPeD, we first observe a decrease in σ2 θ and later it increases. This suggests: while there is not much accumulated noise, clients prefer to collaborate, and as the noise accumulation on the global model increases due to DP noise, clients prefer not to collaborate. This is exactly the type of autonomous behavior we aimed with adaptive regularization.\n\n• DP-AdaPeD. In Figure 1b and Table 2, we observe performance of DP-AdaPeD under different (cid:15) values. DP-AdaPeD outperforms DP-FedAvg because personalized models do not need to be privatized by DP mechanism, whereas the global model needs to be. Our experiments provide user-level privacy (more stringent, but appropriate in FL), as opposed to the item-level privacy.\n\n• DP-AdaPeD with unsampled client iterations. When we let unsampled clients to do local iterations (free in terms of privacy cost and a realistic scenario in cross-silo settings) described in Appendix H, we can increase DP-AdaPeD’s performance under more aggressive privacy constants (cid:15). For instance, for FEMNIST with 1/3 client sampling we obtain the result reported in Figure 1b.\n\n• AdaMix. We consider linear regression on synthetic data, with m = 1000 clients and each client has n ∈ {10, 20, 30} local samples. Each local model θi ∈ Rd is drawn from a mixture of two Gaussian distributions N (μ, Σ) and N (−μ, Σ), where Σ = 0.001 × Id and d = 50. Each client sample (Xij, Yij) is distributed as Xij ∼ N (0, Id) and Yij = (cid:104)Xij, θi(cid:105) + wij, where wij ∼ N (0, 0.1). Table 3 demonstrates the superior performance of AdaMix against the local estimator.\n\n5 CONCLUSION\n\nWe proposed a statistical framework leading to new personalized federated estimation and learning algorithms (e.g., AdaMix, AdaPeD); we also incorporated privacy (and communication) constraints into our algorithms and analyzed them. Open questions include information theoretic lower bounds and its comparison to proposed methods; examination of how far the proposed alternating minimization methods (such as in AdaMix, AdaPeD) are from global optima.\n\nThe work in this paper was partially supported by NSF grants 2139304, 2007714 and gift funding by Meta\n\nand Amazon.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nDurmus Alp Emre Acar, Yue Zhao, Ruizhao Zhu, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Debiasing model updates for improving personalized federated training. In International Conference on Machine Learning, pp. 21–31. PMLR, 2021.\n\nSara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for kmeans and euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, 49(4): FOCS17–97, 2019.\n\nDan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communicationefficient sgd via gradient quantization and encoding. Advances in Neural Information Processing Systems, 30, 2017.\n\nBorja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing interpretations and renyi differential privacy. In Silvia Chiappa and Roberto Calandra (eds.), International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 of Proceedings of Machine Learning Research, pp. 2496–2506. PMLR, 2020.\n\nLeighton Pate Barnes, Yanjun Han, and Ayfer Ozgur. Lower bounds for learning distributions under communication constraints via fisher information. Journal of Machine Learning Research, 21 (236):1–30, 2020.\n\nJeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. signsgd: Compressed optimisation for non-convex problems. In International Conference on Machine Learning, pp. 560–569. PMLR, 2018.\n\nDavid M. Blei, Michael I. Jordan, and A. Ng. Hierarchical bayesian models for applications in\n\ninformation retrieval. 2003.\n\nMark Bun, Cynthia Dwork, Guy N. Rothblum, and Thomas Steinke. Composable and versatile privacy via truncated CDP. In ACM SIGACT Symposium on Theory of Computing (STOC), pp. 74–86, 2018.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nCl ́ement L. Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential\n\nprivacy. In Neural Information Processing Systems (NeurIPS), 2020.\n\nHuili Chen, Jie Ding, Eric Tramel, Shuang Wu, Anit Kumar Sahu, Salman Avestimehr, and Tao Zhang. Self-aware personalized federated learning. CoRR, abs/2204.08069, 2022. doi: 10.48550/ arXiv.2204.08069. URL https://doi.org/10.48550/arXiv.2204.08069.\n\nLiam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In Marina Meila and Tong Zhang (eds.), International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 2089–2099. PMLR, 2021.\n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated\n\nlearning. arXiv preprint arXiv:2003.13461, 2020.\n\nCanh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau\n\nenvelopes. In Advances in Neural Information Processing Systems, 2020.\n\nSimon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=pW2Q2xLwIMD.\n\nCynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations\n\nand Trends in Theoretical Computer Science, 9(3-4):211–407, 2014.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity\n\nin private data analysis. In Theory of Cryptography Conference (TCC), pp. 265–284, 2006.\n\nAlireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning: A meta-\n\nlearning approach. In Advances in Neural Information Processing Systems, 2020.\n\nAndrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin.\n\nBayesian Data Analysis. Chapman and Hall/CRC, 2013.\n\nRobin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client\n\nlevel perspective. arXiv preprint arXiv:1712.07557, 2017.\n\nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. Differentially private clustering: Tight approxima-\n\ntion ratios. Advances in Neural Information Processing Systems, 33:4040–4054, 2020.\n\nBadih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private learning via correlated sampling. In Neural Information Processing Systems (NeurIPS), pp. 20172–20184, 2021.\n\nAvishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for\n\nclustered federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nRichard Gill and Boris Levit. Applications of the van trees inequality: A bayesian cram ́er-rao bound.\n\nBernoulli, 1:59–79, 03 1995. doi: 10.2307/3318681.\n\nAntonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of federated learning: Privacy, accuracy and communication trade-offs. IEEE Journal on Selected Areas in Information Theory, 2(1):464–478, 2021a.\n\nAntonious M. Girgis, Deepesh Data, Suhas N. Diggavi, Peter Kairouz, and Ananda Theertha Suresh. In International Conference on Shuffled model of differential privacy in federated learning. Artificial Intelligence and Statistics (AISTATS), volume 130 of Proceedings of Machine Learning Research, pp. 2521–2529. PMLR, 2021b.\n\nAntonious M. Girgis, Deepesh Data, and Suhas Diggavi. Distributed user-level private mean estimation. In 2022 IEEE International Symposium on Information Theory (ISIT), pp. 2196–2201, 2022. doi: 10.1109/ISIT50566.2022.9834713.\n\nFilip Hanzely and Peter Richt ́arik. Federated learning of a mixture of global and local models. arXiv\n\npreprint arXiv:2002.05516, 2020.\n\nFilip Hanzely, Slavom ́ır Hanzely, Samuel Horv ́ath, and Peter Richt ́arik. Lower bounds and optimal algorithms for personalized federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nRui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated\n\nlearning with differential privacy. IEEE Internet of Things Journal, 7(10):9530–9539, 2020.\n\nPrateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Differentially private model personalization. Advances in Neural Information Processing Systems, 34, 2021a.\n\nPrateek Jain, John Rush, Adam Smith, Shuang Song, and Abhradeep Guha Thakurta. Differentially private model personalization. In Advances in Neural Information Processing Systems, volume 34, 2021b.\n\nWilliam James and Charles Stein. Estimation with quadratic loss. In Proceedings Berkeley Symposium\n\non Mathematics and Statistics, Vol 1, pp. 361–379. University of California Press, 1961.\n\nYihan Jiang, Jakub Koneˇcn`y, Keith Rush, and Sreeram Kannan.\n\nImproving federated learning\n\npersonalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2019.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nShiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.\n\nWhat can we learn privately? SIAM Journal on Computing, 40(3):793–826, 2011.\n\nMikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based meta-\n\nlearning methods. In Advances in Neural Information Processing Systems, 2019.\n\nNikita Kotelevskii, Maxime Vono, Eric Moulines, and Alain Durmus. Fedpop: A bayesian approach\n\nfor personalised federated learning. CoRR, abs/2206.03611, 2022, June.\n\nDaniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, and Ananda Theertha Suresh. Learning with user-level privacy. In Neural Information Processing Systems (NeurIPS), pp. 12466–12479, 2021.\n\nDaliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv\n\npreprint arXiv:1910.03581, 2019.\n\nJeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially private metalearning. In International Conference on Learning Representations, 2020. URL https:// openreview.net/forum?id=rJgqMRVYvr.\n\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In International Conference on Machine Learning, pp. 6357–6368. PMLR, 2021.\n\nTao Lin, Lingjing Kong, Sebastian U. Stich, and Martin Jaggi. Ensemble distillation for robust model\n\nfusion in federated learning. In Advances in Neural Information Processing Systems, 2020.\n\nYuhan Liu, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Michael Riley. Learning discrete distributions: user vs item-level privacy. In Neural Information Processing Systems (NeurIPS), 2020.\n\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\n\n129–137, 1982.\n\nFrederic M. Lord. Estimating true-score distributions in psychological testing (an empirical bayes estimation problem)*. ETS Research Bulletin Series, 1967(2):i–51, 1967. doi: https://doi.org/10. 1002/j.2333-8504.1967.tb00535.x. URL https://onlinelibrary.wiley.com/doi/ abs/10.1002/j.2333-8504.1967.tb00535.x.\n\nYishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.\n\nOthmane Marfoq, Giovanni Neglia, Aur ́elien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp. 1273–1282. PMLR, 2017.\n\nIlya Mironov. R ́enyi differential privacy. In IEEE Computer Security Foundations Symposium (CSF),\n\npp. 263–275, 2017.\n\nIlya Mironov, Kunal Talwar, and Li Zhang. R ́enyi differential privacy of the sampled gaussian mechanism. CoRR, abs/1908.10530, 2019. URL http://arxiv.org/abs/1908.10530.\n\nKaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. Quped: Quantized personalization via distillation with applications to federated learning. Advances in Neural Information Processing Systems, 34, 2021.\n\nKaan Ozkara, Antonious M Girgis, Deepesh Data, and Suhas Diggavi. A generative framework for personalized learning and estimation: Theory, algorithms, and privacy. arXiv preprint arXiv:2207.01771, 2022, July.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgMkCEtPB.\n\nHerbert Robbins. An empirical bayes approach to statistics. In The Proceedings of Third Berkeley\n\nSymposium on Mathematical Statistics and Probability, pp. 157–163, 1956.\n\nTao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, Gang Huang, Pan Zhou, Kun Kuang, Fei Wu, and\n\nChao Wu. Federated mutual learning. arXiv preprint arXiv:2006.16765, 2020.\n\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar. Federated multi-task\n\nlearning. In Advances in Neural Information Processing Systems, pp. 4424–4434, 2017.\n\nCharles Stein. Inadmissibility of the usual estimator for the mean of a multivariate normal distribution. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, 1954– 1955, vol. I, pp. 197–206. University of California Press, Berkeley-Los Angeles, Calif., 1956.\n\nUri Stemmer. Locally private k-means clustering. In SODA, pp. 548–559, 2020.\n\nKevin Tian, Weihao Kong, and Gregory Valiant. Learning populations of parameters. Advances in\n\nneural information processing systems, 30, 2017.\n\nYonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot image classification: a good embedding is all you need? In European Conference on Computer Vision, pp. 266–282. Springer, 2020.\n\nPaul Vanhaesebrouck, Aur ́elien Bellet, and Marc Tommasi. Decentralized collaborative learning of personalized models over networks. In Artificial Intelligence and Statistics, pp. 509–517. PMLR, 2017.\n\nRamya Korlakai Vinayak, Weihao Kong, Gregory Valiant, and Sham Kakade. Maximum likelihood In International Conference on Machine\n\nestimation for learning populations of parameters. Learning, pp. 6448–6457. PMLR, 2019.\n\nMartin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-\n\nbridge University Press, 2019.\n\nValentina Zantedeschi, Aur ́elien Bellet, and Marc Tommasi. Fully decentralized joint learning of personalized models and collaboration graphs. In International Conference on Artificial Intelligence and Statistics, pp. 864–874. PMLR, 2020.\n\nMichael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M. Alvarez. Personalized federated learning with first order model optimization. In International Conference on Learning Representations, 2021.\n\nYuchen Zhang. Distributed machine learning with communication constraints. PhD thesis, EECS Department, University of California, Berkeley, May 2016. URL http://www2.eecs. berkeley.edu/Pubs/TechRpts/2016/EECS-2016-47.html.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA PRELIMINARIES\n\nWe give standard privacy definitions that we use in Section A.1, some existing results on RDP to DP conversion and RDP composition in Section A.2, and user-level differential privacy in Section A.3.\n\nA.1 PRIVACY DEFINITIONS\n\nIn this subsection, we define different privacy notions that we will use in this paper: local differential privacy (LDP), central different privacy (DP), and Renyi differential privacy (RDP), and their userlevel counterparts. Definition 1 (Local Differential Privacy - LDP (Kasiviswanathan et al., 2011)). For (cid:15)0 ≥ 0, a randomized mechanism R : X → Y is said to be (cid:15)0-local differentially private (in short, (cid:15)0-LDP), if for every pair of inputs d, d(cid:48) ∈ X , we have\n\nPr[R(d) ∈ S] ≤ e(cid:15)0 Pr[R(d(cid:48)) ∈ S],\n\n∀S ⊂ Y.\n\n(12)\n\n1, . . . , x(cid:48)\n\nLet D = {x1, . . . , xn} denote a dataset comprising n points from X . We say that two datasets n} are neighboring (and denoted by D ∼ D(cid:48)) if they differ D = {x1, . . . , xn} and D(cid:48) = {x(cid:48) in one data point, i.e., there exists an i ∈ [n] such that xi (cid:54)= x(cid:48) i and for every j ∈ [n], j (cid:54)= i, we have xj = x(cid:48) j. Definition 2 (Central Differential Privacy - DP (Dwork et al., 2006; Dwork & Roth, 2014)). For (cid:15), δ ≥ 0, a randomized mechanism M : X n → Y is said to be ((cid:15), δ)-differentially private (in short, ((cid:15), δ)-DP), if for all neighboring datasets D ∼ D(cid:48) ∈ X n and every subset S ⊆ Y, we have\n\nPr [M(D) ∈ S] ≤ e(cid:15)0 Pr [M(D(cid:48)) ∈ S] + δ.\n\n(13)\n\nIf δ = 0, then the privacy is referred to as pure DP. Definition 3 ((λ, (cid:15)(λ))-RDP (Renyi Differential Privacy) (Mironov, 2017)). A randomized mechanism M : X n → Y is said to have (cid:15)(λ)-Renyi differential privacy of order λ ∈ (1, ∞) (in short, (λ, (cid:15)(λ))-RDP), if for any neighboring datasets D ∼ D(cid:48) ∈ X n, the Renyi divergence between M(D) and M(D(cid:48)) is upper-bounded by (cid:15)(λ), i.e.,\n\nDλ(M(D)||M(D(cid:48))) =\n\n1 λ − 1\n\n(cid:32)\n\nlog\n\nEθ∼M(D(cid:48))\n\n(cid:34)(cid:18) M(D)(θ) M(D(cid:48))(θ)\n\n(cid:19)λ(cid:35)(cid:33)\n\nwhere M(D)(θ) denotes the probability that M on input D generates the output θ. For convenience, instead of (cid:15)(λ) being an upper bound, we define it as (cid:15)(λ) = supD∼D(cid:48) Dλ(M(D)||M(D(cid:48))).\n\n≤ (cid:15)(λ),\n\nA.2 RDP TO DP CONVERSION AND RDP COMPOSITION\n\nAs mentioned after Theorem 5, we can convert the RDP guarantees of DP-AdaPeD to its DP guarantees using existing conversion results from literature. To the best of our knowledge, the following gives the best conversion. Lemma 1 (From RDP to DP (Canonne et al., 2020; Balle et al., 2020)). Suppose for any λ > 1, a mechanism M is (λ, (cid:15) (λ))-RDP. Then, the mechanism M is ((cid:15), δ)-DP, where (cid:15), δ are define below:\n\nFor a given δ ∈ (0, 1) :\n\n(cid:15) = min\n\nλ\n\n(cid:15) (λ) +\n\nlog (1/δ) + (λ − 1) log (1 − 1/λ) − log (λ) λ − 1\n\nFor a given (cid:15) > 0 :\n\nδ = min\n\nλ\n\nexp ((λ − 1) ((cid:15) (λ) − (cid:15))) λ − 1\n\n(cid:18)\n\n1 −\n\n(cid:19)λ\n\n.\n\n1 λ\n\nThe main strength of RDP in comparison to other privacy notions comes from composition. The following result states that if we adaptively compose two RDP mechanisms with the same order, their privacy parameters add up in the resulting mechanism. Lemma 2 (Adaptive composition of RDP (Mironov, 2017, Proposition 1)). For any λ > 1, let M1 : X → Y1 be a (λ, (cid:15)1(λ))-RDP mechanism and M2 : Y1 × X → Y be a (λ, (cid:15)2(λ))-RDP mechanism. Then, the mechanism defined by (M1, M2) satisfies (λ, (cid:15)1(λ) + (cid:15)2(λ))-RDP.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.3 USER-LEVEL DIFFERENTIAL PRIVACY LEVY ET AL. (2021)\n\nConsider a set of m users, each having a local dataset of n samples. Let Di = {xi1, . . . , xin} denote the local dataset at the i-th user for i ∈ [m], where xij ∈ X and X ⊂ Rd. We define D = (D1, . . . , Dm) ∈ (X n)m as the entire dataset.\n\nWe have already defined DP, LDP, and RDP in Section A.1 w.r.t. the item-level privacy. Here, we extend those definition w.r.t. the user-level privacy. In order to do that, we need a generic neighborhood relation between datasets: We say that two datasets D, D(cid:48) are neighboring with respect to distance metric dis if we have dis(D, D(cid:48)) ≤ 1. Item-level DP/RDP vs. User-level DP/RDP. By choosing dis(D, D(cid:48)) = (cid:80)m ij}, we recover the standard definition of the DP/RDP from Definitions 2, 3, which we call item-level DP/RDP. In the item-level DP/RDP, two datasets D, D(cid:48) are neighboring if they differ in a single item. On the other hand, by choosing dis(D, D(cid:48)) = (cid:80)m i}, we call it user-level DP/RDP, where two datasets D, D(cid:48) ∈ (X n)m are neighboring when they differ in a local dataset of any single user. Observe that when each user has a single item (n = 1), then both item-level and user-level privacy are equivalent.\n\n1{Di (cid:54)= D(cid:48)\n\n1{xij (cid:54)= x(cid:48)\n\n(cid:80)n\n\nj=1\n\ni=1\n\ni=1\n\nUser-level Local Differential Privacy (LDP). When we have a single user (i.e., m = 1 and D = X n), by choosing dis (D, D(cid:48)) = 1{D (cid:54)= D(cid:48)} for D, D(cid:48) ∈ X n, we call it user-level LDP. In this case each user privatize her own local dataset using a private mechanism.\n\nWe can define user-level LDP/DP/RDP analogously to their item-level counterparts using the neighborhood relation dis defined above.\n\nB PERSONALIZED ESTIMATION – GAUSSIAN MODEL\n\nB.1 PROOF OF THEOREM 1\n\nWe will derive the optimal estimator and prove the MSE for one dimensional case, i.e., for d = 1; the final result can be obtained by applying these to each of the d coordinates separately.\n\nThe posterior estimators of the local means θ1, . . . , θm and the global mean μ are obtained by solving the following optimization problem:\n\nˆθ1, . . . , ˆθm, ˆμ = arg max\n\nθ1,...,θm,μ\n\n= arg min θ1,...,θm,μ\n\npX|θ (X1, . . . , Xm|θ1, . . . , θm) pθ|μ(θ1, . . . , θm|μ)\n\n− log (cid:0)pX|θ (X1, . . . , Xm|θ1, . . . , θm)(cid:1) − log (cid:0)pθ|μ(θ1, . . . , θm|μ)(cid:1)\n\n= arg min θ1,...,θm,μ\n\nC +\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n(cid:17)2\n\n(cid:16)\n\nX j\n\ni − θi σ2 x\n\n+\n\nm (cid:88)\n\ni=1\n\n(θi − μ)2 σ2 θ\n\n,\n\nwhere the second equality is obtained from the fact that the log function is a monotonic function, and C is a constant independent of the variables θ = (θ1, . . . , θm). Observe that the objective function F (θ, μ) = (cid:80)m obtained by setting the derivative to zero as it is an unbounded optimization problem.\n\nis jointly convex in (θ, μ). Thus, the optimal is\n\n(θi−μ)2 σ2 θ\n\ni −θi)2 σ2 x\n\n+ (cid:80)m\n\n(cid:80)n\n\n(X j\n\nj=1\n\ni=1\n\ni=1\n\n∂F ∂θi\n\n∂F ∂μ\n\n(cid:12) (cid:12) (cid:12) (cid:12)μ=ˆμ,θi=ˆθi (cid:12) (cid:12) (cid:12) (cid:12)μ=ˆμ,θi=ˆθi\n\n=\n\n=\n\n(cid:80)n\n\n(cid:80)m\n\nj=1 2(ˆθi − X j i ) σ2 x\ni=1 2(ˆμ − ˆθi) σ2 θ\n\n= 0.\n\n+\n\n2(ˆθi − ˆμ) σ2 θ\n\n= 0,\n\n∀i ∈ [m]\n\nBy solving these m + 1 equations in m + 1 unknowns, we get: \n\n\n\n\n\nˆθi = α\n\n\n\nX j\n\ni\n\n + (1 − α)\n\n\n\nn (cid:88)\n\n1 n\n\nm (cid:88)\n\nn (cid:88)\n\n\n\nX j\n\ni\n\n ,\n\n(14)\n\n1 mn\n\ni=1\n\nj=1\n\nj=1\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nθ\n\nwhere α = σ2 σ2 θ + (cid:80)m i=1 X i.\n\nα) 1 m\n\nσ2 x\nn\n\n. By letting X i = 1\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni for all i ∈ [m], we can write ˆθi = αX i + (1 −\n\nObserve that E an unbiased estimate of {θi}. Substituting the ˆθi in the MSE, we get that\n\n= αθi + 1−α\n\nm\n\nl=1 θl, where θ = (θ1, . . . , θm). Thus, the estimator (14) is\n\n(cid:80)m\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\nEX1,...,Xm\n\n(cid:20)(cid:16)ˆθi − θi\n\n(cid:17)2(cid:21)\n\n= Eθ\n\n(cid:20) EX1,...,Xm\n\n= Eθ\n\n(cid:20) EX1,...,Xm\n\n= Eθ\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)ˆθi − θi (cid:20)(cid:16)ˆθi − E (cid:20)(cid:16)ˆθi − E\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|θ\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n+ E\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(cid:105)(cid:17)2\n\n(cid:104)ˆθi|θ\n\n|θ\n\n(cid:21)(cid:21)\n\n+ Eθ\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(15)\n\nClaim 1. (cid:20) EX1,...,Xm\n\nEθ\n\n(cid:20)(cid:16)ˆθi − E\n\n(cid:104)ˆθi|θ\n\n(cid:105)(cid:17)2\n\n|θ\n\n(cid:21)(cid:21)\n\n= α2 σ2\n\nx n\n\n+ (1 − α)2 σ2\n\nx mn\n\n+ 2α(1 − α)\n\nσ2 x\nmn\n\n(cid:20) EX1,...,Xm\n\nEθ\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n\n\n(cid:32)\n\n= (1 − α)2Eθ\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\nθk − θi\n\n(cid:33)2\n\n ≤ (1 − α)2 σ2\n\nθ (m − 1) m\n\nProof. For the first equation:\n\n(cid:20) EX1,...,Xm\n\n(cid:20)(cid:16)ˆθi − E\n\n(cid:104)ˆθi|θ\n\nEθ\n\n(cid:105)(cid:17)2\n\n|θ\n\n(cid:21)(cid:21)\n\n= Eθ\n\n EX1,...,Xm\n\n\n\n(cid:32)\n\n\n\nα(X i − θi) + (1 − α)\n\n(cid:33)2\n\n\n\n\n\n(X k − θk)\n\n| θ\n\n\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n= α2E (cid:2)E (cid:2)(X i − θi)2 | θ(cid:3)(cid:3) + (1 − α)2E\n\n E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\n(cid:33)2\n\n\n\n\n\n(X k − θk)\n\n| θ\n\n\n\n\n\nk=1\n\n(cid:34)\n\n(cid:34)\n\n+ 2α(1 − α)E\n\nE\n\n1 m\n\n= α2 σ2\n\nx n\n\n+ (1 − α)2 σ2\n\nx mn\n\n+ 2α(1 − α)\n\nFor the second equation, first note that E (cid:1): α) (cid:0) 1\n\nk=1 θk − θi\n\n(cid:80)m\n\nm\n\nm (cid:88)\n\n(X i − θi)(X k − θk) | θ\n\n(cid:35)(cid:35)\n\nk=1 σ2 x\nmn\n\n(cid:104)ˆθi|θ\n\n(cid:105)\n\n− θi = αθi + 1−α\n\nm\n\n(cid:80)m\n\nk=1 θk − θi = (1 −\n\n(cid:20) EX1,...,Xm\n\nEθ\n\n(cid:20)(cid:16)\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n= (1 − α)2E\n\n=\n\n(1 − α)2 m2\n\nE\n\n\n\n\n\n \n\n\n\n2\n\n\n\n\n\n \n\n(cid:88)\n\n(θk − θi)\n\nk(cid:54)=i\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2 \n\nθk − θi\n\n\n\n(cid:88)\n\n\n\nk(cid:54)=i\n\n\n\n(cid:88)\n\n\n\n=\n\n(1 − α)2 m2\n\n≤\n\n(1 − α)2 m2\n\n=\n\n(1 − α)2 m2\n\nE(θk − θi)2 +\n\n(cid:88)\n\nE(θk − θi)(θl − θi)\n\n\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\n\n[E(θk − μ)2 + E(θi − μ)2] +\n\nk(cid:54)=i\n\n 2(m − 1)σ2\n\nθ +\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n\n\n(cid:88)\n\nE(θk − θi)(θl − θi)\n\n\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n16\n\n(cid:88)\n\nE(θk − θi)(θl − θi)\n\n\n\n\n\nPublished as a conference paper at ICLR 2023\n\n=\n\n=\n\n(1 − α)2 m2\n\n(1 − α)2 m2\n\n 2(m − 1)σ2\n\nθ +\n\n(cid:88)\n\nE(μ − θi)2\n\nk(cid:54)=i,l(cid:54)=i,k(cid:54)=l\n\n  (Since E[θk] = μ for all k ∈ [m])\n\n(cid:2)2(m − 1)σ2\n\nθ + (m − 1)(m − 2)σ2\n\nθ\n\n(cid:3)\n\n= (1 − α)2 σ2\n\nθ (m − 1) m\n\nThis concludes the proof of Claim 1.\n\nSubstituting the result of Claim 1 into (15), we get (cid:17)2(cid:21)\n\n≤ α2 σ2\n\n+ (1 − α)2 σ2\n\n(cid:20)(cid:16)ˆθi − θi\n\nEX1,...,Xm\n\nx n\n(cid:18)\n\n+ 2α(1 − α)\n\nσ2 x\nmn\n\n+ (1 − α)2 σ2\n\nθ (m − 1) m\n\n(16)\n\nx mn\n\n(a)=\n\nσ2 x\nn σ2 x\nn where in (a) we used α = σ2 θ +\n\n=\n\nσ2 x\nn\n\nσ2\n\nθ\n\nα2 +\n\n(1 − α)2 + 2α(1 − α) m\n\n+ α(1 − α)\n\n(cid:19)\n\nm − 1 m\n\n(cid:18)\n\nα +\n\n(cid:19)\n\n,\n\n1 − α m\n\nfor the last term to write (1 − α)2 σ2\n\nθ (m−1) m\n\n= σ2\n\nx\n\nn α(1 − α) m−1 m .\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni , and the global estimator ˆμ = 1\n\nObserve that the estimator in (14) is a weighted summation between two estimators: the local estimator X i = 1 i=1 X i. Thus, the MSE in (a) consists of four terms: 1) The variance of the local estimator ( σ2 n ). 2) The variance of the global estimator ( σ2 nm ). 4) The (cid:20) EX1,...,Xm\n\nnm ). 3) The correlation between the local estimator and the global estimator ( σ2\n\n. This completes the proof of Theorem 1.\n\nbias term Eθ\n\n(cid:104)ˆθi|θ\n\n− θi\n\n(cid:80)m\n\n(cid:20)(cid:16)\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|θ\n\nE\n\nm\n\n(cid:105)\n\nx\n\nx\n\nx\n\nB.2 PROOF OF THEOREM 2, EQUATION (5)\n\nSimilar to the proof of Theorem 1, here also we will derive the optimal estimator and prove the MSE for the one dimensional case, and the final result can be obtained by applying these to each of the d coordinates separately.\n\nLet θ = (θ1, . . . , θm) denote the personalized models vector. For given a constraint function q, we set the personalized model as follows: \n\n\n\nˆθi = α\n\n\n\nX j\n\ni\n\n + (1 − α)\n\nq(X i)\n\n∀i ∈ [m],\n\n(17)\n\n(cid:33)\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\ni=1\n\n1 n\n\nn (cid:88)\n\nj=1\n\nwhere X i = 1\n\nn\n\n(cid:80)n\n\nj=1 X j\n\ni . From the second condition on the function q, we get that\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n= αθi +\n\n1 − α m\n\nm (cid:88)\n\nl=1\n\nθl,\n\n(18)\n\nThus, by following similar steps as the proof of Theorem 1, we get that:\n\n(cid:20)(cid:16)ˆθi − θi\n\nE\n\n(cid:17)2(cid:21)\n\n= E\n\n= E\n\n= E\n\n(cid:20)\n\nE\n\n(cid:20)\n\nE\n\n(cid:20)\n\nE\n\n(cid:20)(cid:16)ˆθi − θi (cid:20)(cid:16)ˆθi − E (cid:20)(cid:16)ˆθi − E\n\n(cid:21)(cid:21)\n\n(cid:17)2\n\n|θ\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n+ E\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(cid:105)(cid:17)2\n\n(cid:104)ˆθi|θ\n\n|θ\n\n(cid:21)(cid:21)\n\n(cid:20)(cid:16)\n\n(cid:20)\n\nE\n\n+ E\n\nE\n\n(cid:105)\n\n(cid:104)ˆθi|θ\n\n(cid:17)2\n\n|θ\n\n− θi\n\n(cid:21)(cid:21)\n\n(a)\n\n= α2 σ2\n\nx n\n\n+ (1 − α)2E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nl=1\n\n(cid:33)2\n\n\n\n|θ\n\n\n\nq(X l) − θl\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n+ 2α(1 − α)E\n\n(cid:34) (cid:0)X i − θi\n\n(cid:1)\n\n(cid:32)\n\n1 m\n\nm (cid:88)\n\nl=1\n\n(cid:33)\n\n(cid:35)\n\nq(X l) − θl\n\n|θ\n\n+ (1 − α)2E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2 \n\nθk − θi\n\n(1 − α)2 (cid:16) σ2\n\nx\n\nn + σ2\n\nq\n\nm\n\n(cid:17)\n\n+\n\n2α(1 − α)σ2 x\nmn\n\n+ (1 − α)2E\n\n\n\n(cid:32)\n\n\n\n1 m\n\nm (cid:88)\n\nk=1\n\n(cid:33)2 \n\nθk − θi\n\n(1 − α)2 (cid:16) σ2\n\nx\n\nn + σ2\n\nq\n\nm\n\n(cid:17)\n\n+ 2α(1 − α)\n\nσ2 x\nmn\n\n+ (1 − α)2 σ2\n\nθ (m − 1) m\n\nα2 +\n\n(1 − α)2 + 2α(1 − α) m\n\n+ α(1 − α)\n\nm − 1 m\n\n(cid:19)\n\n(cid:18)\n\nα +\n\n(cid:19)\n\n,\n\n1 − α m\n\n(19)\n\n+\n\n+\n\n(b)\n\n= α2 σ2\n\nx n\n\n≤ α2 σ2\n\nx n\n(cid:18)\n\n(c) =\n\n=\n\nσ2 x\nn σ2 x\nn\n\nwhere step (a) follows by substituting the expectation of the personalized model from (18). Step (b) follows from the first and third conditions of the function q. Step (c) follows by choosing\n\nα =\n\nσ2\n\nθ +\n\nσ2 q\nm−1\n\nσ2\n\nθ +\n\nσ2 q\nm−1 +\n\nσ2 x\nn\n\n. This derives the result stated in (5) in Theorem 2.\n\nB.2.1 PROOF OF THEOREM 2, PART 1\n\nThe proof consists of two steps. First, we use the concentration property of the Gaussian distribution to show that the local sample means {X i} are bounded within a small range with high probability. Second, we apply an unbiased stochastic quantizer on the projected sample mean.\n\ni , . . . , X n\n\nThe local samples X 1 variance σ2\n\nx, and hence, we have that X i ∼ N (θi, σ2\n\ni are drawn i.i.d. from a Gaussian distribution with mean θi and n ). Thus, from the concentration property − nc2 of the Gaussian distribution, we get that Pr[|X i − θi| > c1] ≤ exp for all i ∈ [m]. Similarly, the models θ1, . . . , θm are drawn i.i.d. from a Gaussian distribution with mean μ ∈ [−r, r] and variance σ2 for all i ∈ [m]. Let E = (cid:8)X i ∈ [−a, a] : ∀i ∈ [m](cid:9), where a = r + c1 + c2. Thus, from the union bound, we get that\n\nθ , hence,, we get Pr[|θi − μ| > c2] ≤ exp\n\n− c2\n\n1 σ2 x\n\n2 σ2 θ\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nx\n\nPr[E] > 1 − m(e that a = r + σx√\n\nn\n\n−\n\nnc2 1\nσ2\n\nx + e\n\n−\n\nc2 2\nσ2 θ ). By setting c1 =\n\n(cid:113) σ2\n\n(cid:112)log(m2n) + σθ\n\n(cid:112)log(m2n), and Pr[E] = 1 − 2\n\nx\n\nn log(m2n) and c2 = (cid:112)σ2 mn .\n\nθ log(m2n), we get\n\nLet qk : [−a, a] → Yk be a quantization function with k-bits, where Yk is a discrete set of cardinality |Yk| = 2k. For given x ∈ [−a, a], the output of the function qk is given by:\n\nqk(x) =\n\n2a 2k − 1\n\n((cid:98) ̃x(cid:99) + Bern ( ̃x − (cid:98) ̃x(cid:99))) − a,\n\n(20)\n\nwhere Bern(p) is a Bernoulli random variable with bias p, and ̃x = 2k−1 2a (x + a) ∈ [0, 2k − 1]. Observe that the output of the function qk requires only k-bits for transmission. Furthermore, the function qk satisfies the following conditions:\n\nE [qk(x)] = x,\n\nσ2 qk\n\n= E (cid:2)(qk(x) − x)2(cid:3) ≤\n\na2 (2k − 1)2 .\n\n(21)\n\n(22)\n\n(cid:3) and sends Let each client applies the function qk on the projected local mean ̃Xi = Proj[−a,a] the output to the server for all i ∈ [m]. Conditioned on the event E, i.e., X i ∈ [−a, a] ∀i ∈ [m], and using (19), we get that\n\n(cid:2)X i\n\nM SE = Eθ,X\n\n(cid:20)(cid:16)ˆθi − θi\n\n(cid:17)2(cid:21)\n\n≤\n\nσ2 x\nn\n\n(cid:18) 1 − α m\n\n(cid:19)\n\n+ α\n\n,\n\n(23)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nwhere α =\n\nσ2\n\nθ +\n\na2 (2k −1)2(m−1)\n\nσ2\n\nθ +\n\na2 (2k −1)2(m−1)\n\n+\n\nσ2 x\nn\n\nand a = r + σx√\n\nn\n\n(cid:112)log(m2n) + σθ\n\n(cid:112)log(m2n). Note that the event\n\nE happens with probability at least 1 − 2\n\nmn .\n\nB.2.2 PROOF OF THEOREM 2, PART 2\n\nWe define the (random) mechanism qp : [−a, a] → R that takes an input x ∈ [−a, a] and generates a user-level ((cid:15)0, δ)-LDP output y ∈ R, where y = qp(x) is given by:\n\nqp(x) = x + ν,\n\n(24)\n\nwhere ν ∼ N (0, σ2 , we get that the output of the (cid:15)0 function qp(x) is ((cid:15)0, δ)-LDP from Dwork & Roth (2014). Furthermore, the function qp satisfies the following conditions:\n\n) is a Gaussian noise. By setting σ2 (cid:15)0\n\n(cid:15)2 0\n\n= 8a2 log(2/δ)\n\nE [qp(x)] = x,\n\nσ2 qp\n\n= E (cid:2)(qp(x) − x)2(cid:3) ≤\n\n8a2 log(2/δ) (cid:15)2 0\n\n.\n\n(25)\n\n(26)\n\nSimilar to the proof of Theorem 2, Part 1, let each client applies the function qp on the projected local (cid:3) and sends the output to the server for all i ∈ [m]. Conditioned on the mean ̃Xi = Proj[−a,a] event E, i.e., X i ∈ [−a, a] ∀i ∈ [m], and using (19), we get that\n\n(cid:2)X i\n\nMSE = Eθ,X\n\n(cid:20)(cid:16)ˆθi − θi\n\n(cid:17)2(cid:21)\n\n≤\n\nσ2 x\nn\n\n(cid:18) 1 − α m\n\n(cid:19)\n\n+ α\n\n,\n\n(27)\n\nθ + 8a2 log(2/δ) σ2\n\n(cid:15)2\n\nwhere α =\n\n0(m−1) θ + 8a2 log(2/δ) σ2 0(m−1) happens with probability at least 1 − 2\n\nσ2 x\nn\n\n(cid:15)2\n\n+\n\nand a = r + σx√\n\nn\n\nmn .\n\n(cid:112)log(m2n) + σθ\n\n(cid:112)log(m2n). Note that the event E\n\nRemark 4 (Privacy with communication efficiency). Note that our private estimation algorithm for the Gaussian case adds Gaussian noise (which is a real number) but that can also be made communication-efficient by alternatively adding a discrete Gaussian noise (Canonne et al., 2020).\n\nB.3 LOWER BOUND\n\nHere we discuss the lower bound using Fisher information technique similar to Barnes et al. (2020). In particular we use a Bayesian version of Cramer-Rao lower bound and van Trees inequality Gill & Levit (1995). Let us denote f (X|θ) as the data generating conditional density function and π(θ) as the prior distribution that generates θ. Let us denote Eθ as the expectation with respect to the randomness of θ and E as the expectation with respect to randomness of X and θ. First we define two types of Fisher information:\n\nIX (θ) = Eθ∇θ log(f (X|θ))∇θ log(f (X|θ))T\n\nI(π) = E∇θ log(π(θ))∇θ log(π(θ))T\n\nnamely Fisher information of estimating θ from samples X and Fisher information of prior π. Here the logarithm is elementwise. For van Trees inequality we need the following regularity conditions:\n\n• f (X|·) and π(·) are absolutely continuous and π(·) vanishes at the end points of Θ. • Eθ∇θ log(f (X|θ)) = 0 • We also assume both density functions are continuously differentiable.\n\nThese assumptions are satisfied for the Gaussian setting for any finite mean μ, they are satisfied for Bernoulli setting as long as parameters α and β are larger than 1. Assuming local samples X are generated i.i.d with f (x|θ), the van Trees inequality for one dimension is as follows:\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nE((cid:98)θ(X) − θ)2 ≥\n\n1 nEIx(θ) + I(π)\n\nwhere IX (θ) = Eθ log(f (X|θ))(cid:48)2 and I(π) = E log(π(θ))(cid:48)2. Assuming θ ∈ Rd and each dimension is independent from each other, by Gill & Levit (1995) we have:\n\nE(cid:107)(cid:98)θ(X) − θ(cid:107)2 ≥\n\nd2 nETr(Ix(θ)) + Tr(I(π))\n\n(28)\n\nNote, the lower bound on the average risk directly translates as a lower bound on supθ∈Θ θ(cid:107)2. Before our proof we have a useful fact:\n\nEX (cid:107)(cid:98)θ(X)−\n\nFact 1. Given some random variable X ∼ N (Y, σ2 σ2\n\ny).\n\ny) where Y ∼ N (Z, σ2\n\nz ) we have X ∼ N (z, σ2\n\nz +\n\nProof. We will give the proof in one dimension, however, it can easily be extended to multidimensional case where each dimension is independent. For all t ∈ R we have,\n\nEX [exp(itX)] = EY EX [exp(itX)|Y ] = EY [exp(itY −\n\nxt2 σ2 2\n\n)]\n\n= exp(−\n\n= exp(−\n\nσ2 xt2 2\nxt2 σ2 2\n\n)EY [exp(itY )]\n\n) exp(itz −\n\n= exp(itz −\n\ny)t2\n\n(σ2\n\nx + σ2 2\n\nσ2\n\nyt2 2\n\n)\n\n)\n\nwhere the last line is the characteristic function of a Gaussian with mean z and variance σ2\n\nx + σ2 y.\n\nGaussian case with perfect knowledge of prior. In this setting we know that θi ∼ N (μ1, σ2 hence, I(π) = 1 Id. Then, σ2 θ\n\nId, similarly IX (θ) = 1\n\nσ2 x\n\nθ Id),\n\nE(cid:107) (cid:98)θi(X) − θi(cid:107)2 ≥\n\nsup θi\n\nd2\n\nnE d σ2 x\n\n+ d σ2 θ\n\n=\n\ndσ2\n\nx\n\nθ σ2 θ + σ2\n\nx\n\nnσ2\n\n(29)\n\nmn\n\n(cid:80)m,n\n\ni,j X j\n\nGaussian case with estimated population mean. In this setting instead of a true prior we have a prior whose mean is the average of all data spread across clients, i.e., we assume θi ∼ N ((cid:98)μ, σ2 θ Id) where (cid:98)μ = 1 i |θj ∼ N (θj, σ2 θ Id). While the true prior is parameterized with mean μ, θi in this form is not parameterized by μ but by (cid:98)μ which itself has randomness due X j i . However, using Fact 1 θ + σ2 twice we can write θi ∼ N (μ, (σ2 mn )Id). Then using the van Trees inequality similar to the lower bound in perfect case we can obtain:\n\ni . We additionally know that there is a Markov relation such that X j\n\nxId) and θj ∼ N (μ, σ2\n\nm + σ2\n\nx\n\nθ\n\nσ2 θ σ2 nσ2\n\nx mn\n\nx + σ4 θ + σ2\n\nx\n\nEX (cid:107)(cid:98)θi(X) − θi(cid:107)2 ≥ d\n\nsup θi∈Θ\n\n20\n\n(30)\n\nPublished as a conference paper at ICLR 2023\n\nC PERSONALIZED ESTIMATION – BERNOULLI MODEL\n\nC.1 WHEN α, β ARE KNOWN\n\nAnalogous to the Gaussian case, we can show that if α, β are known, then the posterior mean estimator has a closed form expression: (cid:98)pi = aX i + (1 − a) α α+β (where a = n/α+β+n) and achieves the MSE: Epi∼πE α+β+n . We show this below.\n\n(cid:98)pi,X1,...,Xm ((cid:98)pi − pi)2 ≤\n\nαβ n(α+β)(α+β+1)\n\nn\n\nFor a client i, let π(pi) be distributed as Beta(α, β). In this setting, we model that each client generates local samples according to Bern(pi). Consequently, each client has a Binomial distribution regarding the sum of local data samples. Estimating Bernoulli parameter pi is related to Binomial distribution Bin(n, pi) (the sum of data samples) Zi since it is the sufficient statistic of Bernoulli distribution. The distribution for Binomial variable Zi given pi is P (Zi = zi|pi) = (cid:0) n i (1 − pi)n−zi. It is a known fact that for any prior, the Bayesian MSE risk minimizer is the posterior mean E [pi|Zi = zi].\n\n(cid:1)pzi\n\nzi\n\nWhen pi ∼ Beta(α, β), we have posterior\n\nf (pi|Zi = zi) =\n\n=\n\n=\n\nπ(pi)\n\nP (zi|pi) P (zi) (cid:1)pzi\n\n(cid:0) n zi\n\ni (1 − pi)n−zi\n\nP (zi)\n\npα−1\n\ni\n\n(1 − pi)β−1 B(α, β)\n\n(cid:1)\n\n(cid:0) n zi P (zi)\n\nB(α + zi, β + n − zi) B(α, β)\n\npα+zi−1\n\ni\n\n(1 − pi)β+n−zi−1\n\nB(α + zi, β + n − zi)\n\n,\n\nwhere B(α, β) = Γ(α)Γ(β)\n\nΓ(α+β) , and\n\n(cid:90)\n\nP (zi) =\n\nP (zi|pi)π(pi)dpi\n\n=\n\n=\n\npα−1\n\n(cid:19)\n\npzi i (1 − pi)n−zi\n\n(cid:90) (cid:18)n zi (cid:19) B(zi + α, n − zi + β) B(α, β)\n\n(cid:18)n zi\n\ni\n\n(1 − pi)β−1 B(α, β) (cid:90) pα+zi−1\n\ni\n\ndpi\n\n(1 − pi)β+n−zi−1\n\n(cid:124)\n\nB(α + zi, β + n − zi) (cid:123)(cid:122) integral of a Beta distribution\n\ndpi\n\n(cid:125)\n\n=\n\n(cid:18)n zi\n\n(cid:19) B(zi + α, n − zi + β) B(α, β)\n\nThus, we get that the posterior distribution f (pi|Zi = zi) = pα+zi−1 distribution Beta(zi + α, n − zi + β). As a result, the posterior mean is given by:\n\ni\n\nB(α+zi,β+n−zi)\n\n(1−pi)β+n−zi−1\n\n(cid:98)pi =\n\nα + Zi α + β + n α+β+n . Observe that Epi∼Beta(α,β)[pi] = α\n\n= a\n\n(cid:18) Zi n\n\n(cid:19)\n\n+ (1 − a)\n\n(cid:18) α\n\nα + β\n\n(cid:19)\n\n,\n\nwhere a = n tion between the local estimator zi We have Rpi ((cid:98)pi) = EπE((cid:98)pi − pi)2. The MSE of the posterior mean is given by:\n\nn and the global estimator μ = α\n\nα+β .\n\nα+β , i.e., the estimator is a weighted summa-\n\nis a beta\n\n(31)\n\nMSE = E[(ˆpi − pi)2] (cid:20)(cid:16)\n\n= E\n\n(cid:17)\n\n+ (1 − a)(μ − pi)\n\n(cid:17)2(cid:21)\n\n− pi\n\n= a2E\n\n(cid:17)2(cid:21)\n\n− pi\n\n+ (1 − a)2E\n\n(cid:104)\n\n(μ − pi)2(cid:105)\n\na\n\n(cid:16) zi n\n(cid:20)(cid:16) zi n\n\n= a2Epi∼π(pi)\n\n(cid:21)\n\n(cid:20) pi(1 − pi) n\n\n+ (1 − a)2\n\nαβ (α + β)2(α + β + 1)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n= a2\n\nαβ n(α + β)(α + β + 1) (cid:18)\n\nαβ n(α + β)(α + β + 1)\n\n=\n\nαβ (α + β)2(α + β + 1) (cid:19)\n\n+ (1 − a)2\n\nn α + β + n\n\n.\n\nThe last equality is obtained by setting a = n\n\nα+β+n .\n\nRemark 5. Note that X i := Zi n is the estimator based only on the local data and α/(α+β) is the true global mean, and (cid:98)pi = aX i + (1 − a) α α+β , where a = n/α+β+n (see (31)) is the estimator based on all the data. Observe that when n → ∞, then a → 1, which implies that (cid:98)pi → X i. Otherwise, when α + β is large (i.e., the variance of the beta distribution is small), then a → 0, which implies that (cid:98)pi → α/(α+β). Both these conclusions conform to the conventional wisdom as mentioned in the Gaussian case. It can be shown that the local estimate X i achieves the Bayesian risk of Epi∼Beta(α,β)EXi[(X i − pi)2] = Epi∼Beta(α,β)(pi(1−pi))/n = αβ/n(α+β)(α+β+1), which implies that the personalized estimation with perfect prior always outperforms the local estimate with a multiplicative gain a = n/(n+α+β) ≤ 1.\n\nC.2 WHEN α, β ARE UNKNOWN: PROOF OF THEOREM 3\n\nThe personalized model of the ith client with unknown parameters α, β is given by:\n\nˆpi = aiX i + (1 − ai) (ˆμi) ,\n\n(32)\n\n, the empirical mean ˆμi = 1\n\nm−1\n\n+n\n\n(cid:80)\n\nl(cid:54)=i X l, and the empirical variance\n\nl(cid:54)=i(X l − ˆμi)2. From (Tian et al., 2017, Lemma 1), with probability 1 − 1\n\nm2n , we get\n\nn\n\nˆμi(1− ˆμi) ˆσ2 i\n\nwhere ai =\n\n(cid:80)\n\ni = 1\n\nm−2\n\nˆσ2 that\n\n|μ − ˆμi| ≤\n\n|σ2 − ˆσ2\n\ni | ≤\n\n(cid:114) 3 log(4m2n) m − 1 (cid:114) 3 log(4m2n) m − 1\n\n,\n\nwhere μ = α\n\nα+β , σ2 =\n\nαβ\n\n(α+β)2(α+β+1) are the true mean and variance of the beta distribution,\n\nrespectively. Let c = ∀i ∈ [m]} that happens with probability at least 1 − 1\n\nm−1\n\n(cid:113) 3 log(4m2n)\n\nmn , we get that:\n\n. Conditioned on the event E = {|μ − ˆμi| ≤ c, |σ2 − ˆσ2\n\ni | ≤ c :\n\n(cid:104)\n\nE\n\n(ˆpi − pi)2 |Z−i\n\n(cid:105)\n\n= a2E\n\n(cid:34)(cid:18) Zi\n\nn\n\n(cid:19)2(cid:35)\n\n− pi\n\n+ (1 − a)2E\n\n(cid:104)\n\n(ˆμi − pi)2 |Z−i\n\n(cid:105)\n\n(cid:18)\n\n(cid:18)\n\n(cid:18)\n\n= a2\n\n= a2\n\n≤ a2\n\nαβ n(α + β)(α + β + 1)\n\nαβ n(α + β)(α + β + 1)\n\nαβ n(α + β)(α + β + 1)\n\n(cid:19)\n\n(cid:19)\n\n(cid:19)\n\n+ (1 − a)2 (cid:16)\n\nE\n\n(cid:104)\n\n(μ − pi)2(cid:105)\n\n+ (μ − ˆμi)2(cid:17)\n\n+ (1 − a)2\n\n+ (1 − a)2\n\n(cid:18)\n\n(cid:18)\n\nαβ (α + β)2(α + β + 1)\n\nαβ (α + β)2(α + β + 1)\n\n(cid:19)\n\n+ (μ − ˆμi)2\n\n(cid:19)\n\n,\n\n+ c2\n\nwhere the expectation is with respect to zi ∼ Binom(pi, n) and pi ∼ Beta(α, β) and Z−i = {z1, . . . , zi−1, zi+1, . . . , zm} denotes the entire dataset except the ith client data (zi). By taking the expectation with respect to the datasets Z−i, we get that the MSE is bounded by:\n\nMSE ≤ E (cid:2)a2(cid:3)\n\n(cid:18)\n\nαβ n(α + β)(α + β + 1)\n\n(cid:19)\n\n+E (cid:2)(1 − a)2(cid:3)\n\n(cid:18)\n\nαβ (α + β)2(α + β + 1)\n\n+\n\n3 log(4m2n) m − 1\n\n(cid:19)\n\n,\n\nwith probability at least 1 − 1\n\nmn . This completes the proof of Theorem 3.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nC.3 WITH PRIVACY CONSTRAINTS: PROOF OF THEOREM 4\n\nFirst, we prove some properties of the private mechanism qp. Observe that for any two inputs x, x(cid:48) ∈ [0, 1], we have that:\n\nPr[qp(x) = y] Pr[qp(x(cid:48)) = y]\n\n=\n\ne(cid:15)0\n\ne(cid:15)0 +1 − x e(cid:15)0 −1 e(cid:15)0 +1 − x(cid:48) e(cid:15)0 −1\n\ne(cid:15)0 +1\n\ne(cid:15)0 +1\n\ne(cid:15)0\n\n≤ e(cid:15)0,\n\n(33)\n\ne(cid:15)0 −1 . Similarly, we can prove (33) for the output y = e(cid:15)0\n\nfor y = −1 user-level (cid:15)0-LDP. Furthermore, for given x ∈ [0, 1], we have that\n\ne(cid:15)0 −1 . Thus, the mechanism qp is\n\nThus, the output of the mechanism qp is an unbiased estimate of the input x. From the Hoeffding’s inequality for bounded random variables, we get that:\n\nE [qp(x)] = x.\n\n(34)\n\nPr[|ˆμ(p)\n\ni − μ| > t] ≤ 2 exp\n\nPr[|ˆσ2(p)\n\ni − σ2| > t] ≤ 2 exp\n\n(cid:18) −3(e(cid:15)0 − 1)2(m − 1)t2 (e(cid:15)0 + 1)2 (cid:18) −3(e(cid:15)0 − 1)2(m − 1)t2 (e(cid:15)0 + 1)2\n\n(cid:19)\n\n(cid:19)\n\n(35)\n\nThus, we have that the event E = {|ˆμ(p) probability at least 1 − 1 non-private estimator, we get the fact that the MSE of the private model is bounded by: (cid:19)\n\ni − σ2| ≤ cp : ∀i ∈ [m]} happens with . By following the same steps as the\n\ni − μ| ≤ cp, |ˆσ2(p)\n\n(cid:113) (e(cid:15)0 +1)2 log(4m2n)\n\nmn , where cp =\n\n3(e(cid:15)0 −1)2(m−1)\n\n(cid:18)\n\nMSE ≤ E (cid:2)a2(cid:3)\n\nαβ n(α + β)(α + β + 1)\n\n+ E (cid:2)(1 − a)2(cid:3)\n\n(cid:18)\n\nαβ (α + β)2(α + β + 1)\n\n+\n\n(e(cid:15)0 + 1)2 log(4m2n) 3(e(cid:15)0 − 1)2(m − 1)\n\n(cid:19)\n\n, (36)\n\nwhere a(p) =\n\nˆμ\n\n(p) i\n\nn\n\n(1− ˆμ\n\nˆσ\n\n2(p) i\n\n(p) i\n\nand the expectation is with respect\n\nto the clients data\n\n)\n\n+n\n\n{z1, . . . , zi−1, zi+1, . . . , zm}and the randomness of the private mechanism qp. This completes the proof of Theorem 4.\n\nRemark 6 (Privacy with communication efficiency). Note that our private estimation algorithm for the Bernoulli case is already communication-efficient as each client sends only one bit to the server.\n\nRemark 7 (Client sampling). For simplicity, in the theoretical analysis in Gaussian and Bernoulli models, we assume that all clients participate in the estimation process. However, a simple modification to our analysis also handles the case where only K out of m clients participate: in all our theorem statements we would have to modify to have K instead m. Note that we do client sampling for our experiments in Table 1.\n\nD PERSONALIZED ESTIMATION – MIXTURE MODEL\n\nConsider a set of m clients, where the i-th client has a local dataset Xi = (Xi1, . . . , Xin) of n samples for i ∈ [m], where Xij ∈ Rd. The local samples Xi of the i-th client are drawn i.i.d. from a Gaussian distribution N (θi, σ2\n\nId) with unknown mean θi and known variance σ2\n\nId.\n\nx\n\nx\n\nIn this section, we assume that the personalized models θ1, . . . , θm are drawn i.i.d. from a discrete distribution P = [p1, . . . , pk] for given k candidates μ1, . . . , μk ∈ Rd. In other works, Pr[θi = μl] = pl for l ∈ [k] and i ∈ [m]. The goal of each client is to estimate her personalized model {θi} that minimizes the mean square error defined as follows:\n\nMSE = E{θi,Xi}(cid:107)θi − ˆθi(cid:107)2, where the expectation is taken with respect to the personalized models θi and the local samples Id)}. Furthermore, ˆθi denotes the estimate of the personalized model θi for {Xij ∼ N (θi, σ2 i ∈ [m].\n\n(37)\n\nx\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFirst, we start with a simple case when the clients have perfect knowledge of the prior distribution, i.e., the i-th client knows the k Gaussian distributions N (cid:0)μ1, σ2 (cid:1) and the prior distribution α = [α1, . . . , αk]. This will serve as a stepping stone to handle the more general case when the prior distribution is unknown.\n\n(cid:1) , . . . , N (cid:0)μk, σ2\n\nθ\n\nθ\n\nD.1 WHEN THE PRIOR DISTRIBUTION IS KNOWN\n\nIn this case, the i-th client does not need the data of the other clients as she has a perfect knowledge about the prior distribution. Theorem 6. For given a perfect knowledge α = [α1, . . . , αk] and N (cid:0)μ1, σ2 the optimal personalized estimator that minimizes the MSE is given by:\n\n(cid:1) , . . . , N (cid:0)μk, σ2\n\nθ\n\nθ\n\n(cid:1),\n\nˆθi =\n\nk (cid:88)\n\nl=1\n\na(i)\n\nl μl,\n\n(38)\n\n(cid:18)\n\n−\n\npl exp\n\n(cid:80)k\n\ns=1 ps exp\n\n(cid:18)\n\n(cid:80)n\n\n(cid:19)\n\nj=1 (cid:107)Xij −μl (cid:107)2 2σ2 x\n(cid:107)Xij −μs(cid:107)2 2σ2 x\n\nj=1\n\n(cid:80)n\n\n−\n\nwhere α(i)\n\nl =\n\nfor l ∈ [k].\n\n(cid:19) denotes the weight associated to the prior model μl\n\nProof. Let θi ∼ P, where P = [p1, . . . , pk] and pl = Pr[θi = μl] for l ∈ [k]. The goal is to design an estimator ˆθi that minimizes the MSE given by:\n\nMSE = Eθi∼PE{Xij ∼N (θi,σ2\n\nx)}\n\n(cid:104)\n\n(cid:107)ˆθi − θi(cid:107)2(cid:105)\n\n.\n\n(39)\n\nLet Xi = (Xi1, . . . , Xin). By following the standard proof of the minimum MSE, we get that: Eθi\n\n(cid:107)ˆθi − θi(cid:107)2(cid:105)\n\n= EXi\n\nEθi|Xi\n\nEXi\n\n(cid:104)\n\n(cid:105)\n\n(cid:104)\n\n(cid:107)ˆθi − E[θi|Xi] + E[θi|Xi] − θi(cid:107)2(cid:12) (cid:2) (cid:107)E[θi|Xi] − θi(cid:107)2(cid:12) (cid:2) (cid:107)E[θi|Xi] − θi(cid:107)2(cid:12)\n\n(cid:3) + EXi (cid:3) ,\n\nEθi|Xi\n\n(cid:12) Xi\n\n(cid:12) Xi\n\n(cid:12) (cid:12) Xi (cid:104)\n\n= EXi\n\n≥ EXi\n\nEθi|Xi Eθi|Xi\n\n(cid:107)E[θi|Xi] − ˆθi(cid:107)2(cid:12)\n\n(cid:105)\n\n(cid:12) (cid:12) Xi\n\n(40) where the last inequality is achieved with equality when ˆθi = E[θi|Xi]. The distribution on θi given the local dataset Xi is given by:\n\nPr[θi = μl|Xi] =\n\nf (Xi|θi = μl) Pr[θi = μl] f (Xi)\n\n=\n\n=\n\nf (Xi|θi = μl) Pr[θi = μl] s=1 f (Xi|θi = μs) Pr[θi = μs]\n\n(cid:80)k\n\n(cid:18)\n\npl exp\n\n−\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μl(cid:107)2\n\n2σ2 x\n\n(cid:19)\n\n(cid:80)k\n\ns=1 ps exp\n\n(cid:16)\n\n−\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μs(cid:107)2\n\n2σ2 x\n\n(cid:17) = α(i)\n\nl\n\nAs a result, the optimal estimator is given by:\n\nThis completes the proof of Theorem 6.\n\nˆθi = E[θi|Xi] =\n\nk (cid:88)\n\nl=1\n\nα(i)\n\nl μl.\n\n(41)\n\n(42)\n\nThe optimal personalized estimation in (38) is a weighted summation over all possible candidates vectors μ1, . . . , μk, where the weight α(i) increases if the prior pl increases and/or the local samples {Xij} are close to the model μl for l ∈ [k]. Observe that the optimal estimator ˆθi in Theorem 6 that . Furthermore, minimizes the MSE is completely different from the local estimator\n\n(cid:80)n\n\n(cid:16) 1\n\n(cid:17)\n\nl\n\nj=1 Xij\n\nn\n\nit is easy to see that the local estimator has the MSE\n\n(cid:17)\n\n(cid:16) dσ2 n\n\nx\n\nwhich increases linearly with the data\n\ndimension d. On the other hand, the MSE of the optimal estimator in Theorem 6 is a function of the prior distribution P = [p1, . . . , pk], the prior vectors μ1, . . . , μk, and the local variance σ2 x.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nD.2 WHEN THE PRIOR DISTRIBUTION IS UNKNOWN\n\nNow, we consider a more practical case when the prior distribution P = [p1, . . . , pk] and the candidates μ1, . . . , μk are unknown to the clients. In this case, the clients collaborate with each other by their local data to estimate the priors P and μ1, . . . , μk, and then, each client uses the estimated priors to design her personalized model as in (38).\n\ni\n\ni\n\n1\n\n1\n\nk\n\nk\n\n(cid:80)n\n\n:= 1 n\n\n], μ(t+1)\n\n, . . . , p(t+1)\n\n, . . . , μ(t+1)\n\nfor given local models {θ(t)\n\n} for given global priors P(t+1) and μ(t+1)\n\nWe present Algorithm 3 based on alternating minimization. The algorithm starts by initializing the local models {θ(0) j=1 Xij}. Then, the algorithm works in rounds alternating between estimating the priors P(t+1) = [p(t+1) i } and estimating the personalized models {θ(t+1) , . . . , μ(t+1) .\nObserve that for given the prior information P(t), {μt l}, each client updates her personalized model in Step 6 which is the optimal estimator for given priors according to Theorem 6. On the other hand, for given personalized models {θ(t) l} using clustering algorithm with k sets in Step 11. The algorithm Cluster takes m vectors a1, . . . , am and an integer k as its input, and its goal is to generate a set of k cluster centers μ1, . . . , μk that minimizes (cid:80)m i=1 minl∈k (cid:107)ai − μl(cid:107)2. Furthermore, these clustering algorithms can also return the prior distribution P, by setting pl := |Sl| m , where Sl ⊂ {a1, . . . , am} denotes the set of vectors that are belongs to the l-th cluster. There are lots of algorithms that do clustering, but perhaps, Lloyd’s algorithm Lloyd (1982) and Ahmadian Ahmadian et al. (2019) are the most common algorithms for k-means clustering. Our Algorithm 3 can work with any clustering algorithm.\n\ni }, we estimate the priors P(t), {μt\n\nk\n\n1\n\nAlgorithm 3 Alternating Minimization for Personalized Estimation Input: Number of iterations T , local datasets (Xi1, . . . , Xin) for i ∈ [m].\n\n(cid:80)n\n\nj=1 Xij for i ∈ [m].\n\n1: Initialize θ0 i = 1 n\n2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do\n\nReceive P(t), μ(t) Update the personalized model:\n\n1 , . . . , μ(t)\n\nk from the server\n\nθt\n\ni ←\n\nk (cid:88)\n\nl=1\n\nα(i)\n\nl μ(t)\n\nl\n\nand\n\nα(i)\n\nl =\n\n(cid:18)\n\nexp\n\n−\n\np(t)\n\nl\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μ(t) 2σ2 x\n\nl (cid:107)2\n\n(cid:19)\n\n(cid:80)k\n\ns=1 p(t)\n\ns exp\n\n(cid:18)\n\n−\n\n(cid:80)n\n\nj=1 (cid:107)Xij −μ(t) 2σ2 x\n\ns (cid:107)2\n\n(cid:19)\n\nSend θt\n\ni to the server\n\n7: 8: 9: 10:\n\n11:\n\nend for At the Server: Receive θ(t) Update the global parameters: P(t), μ(t) Broadcast P(t), μ(t)\n\nm from the clients\n\n1 , . . . , θ(t)\n\n1 , . . . , μ(t)\n\nk to all clients\n\n12: 13: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n1 , . . . , μ(t)\n\nk ← Cluster\n\n(cid:16)\n\nθ(t)\n\n1 , . . . , θ(t)\n\nm , k\n\n(cid:17)\n\nD.3 PRIVACY/COMMUNICATION CONSTRAINTS\n\nIn the personalized estimation Algorithm 3, each client shares her personalized estimator θ(t) to the server at each iteration which is not communication-efficient and violates the privacy. In this section we present ideas on how to design communication-efficient and/or private Algorithms for personalized estimation. Lemma 3. Let μ1, . . . μk ∈ Rd be unknown means such that (cid:107)μi(cid:107)2 ≤ r for each i ∈ [k]. Let θ1, . . . , θm ∼ P, where P = [p1, . . . , pk] and pl = Pr[θi = μl]. For i ∈ [m], let Xi1, . . . , Xin ∼\n\ni\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nN (θi, σ2\n\nx), i.i.d. Then, with probability at least 1 − 1\n\nmn , the following bound holds for all i ∈ [m]:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\nj=1\n\nXij\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2\n\n(cid:114)\n\n≤ 4\n\nd\n\nσ2 x\nn\n\n(cid:114)\n\n+ 2\n\nlog(m2n)\n\nσ2 x\nn\n\n+ r.\n\n(43)\n\nProof. Observe that the vector (X i − θi) = 1 proxy σ2\n\nn . As a result, we have that:\n\nn\n\nx\n\n(cid:80)n\n\ni=1 Xij − θi is a sub-Gaussian random vector with\n\nσ2 x\nn with probability at least 1 − η from Wainwright (2019). Since μ1, . . . , μk ∈ Rd are such that (cid:107)μi(cid:107)2 ≤ r for each i ∈ [k], we have:\n\n(cid:107)X i − θi(cid:107)2 ≤ 4\n\nlog(1/η)\n\nσ2 x\nn\n\n(44)\n\n+ 2\n\nd\n\n,\n\n(cid:114)\n\n(cid:114)\n\n(cid:107)X i(cid:107)2 ≤ 4\n\nd\n\n+ 2\n\nlog(1/η)\n\n(cid:114)\n\n(cid:114)\n\nσ2 x\nn\n\nσ2 x\nn\n\n+ r,\n\n(45)\n\nwith probability 1 − η from the triangular inequality. Thus, by choosing η = 1 union bound, this completes the proof of Lemma 3.\n\nm2n and using the\n\nLemma 3 shows that the average of the local samples {X i} has a bounded (cid:96)2 norm with high probability. Thus, we can design a communication-efficient estimation Algorithm as follows: Each client clips her personal model θ(t) n + r. Then, each client applies a vector-quantization scheme (e.g., Bernstein et al. (2018); Alistarh et al. (2017); Girgis et al. (2021a)) to the clipped vector before sending it to the server.\n\ni within radius 4\n\nlog(m2n) σ2\n\nn + 2\n\nd σ2\n\n(cid:113)\n\n(cid:113)\n\nx\n\nx\n\n(cid:113)\n\ni within radius 4\n\nTo design a private estimation algorithm with discrete priors, each client clips her personalized estimator θ(t) n + r. Then, we can use a differentially private algorithm for clustering (see e.g., Stemmer (2020) for clustering under LDP constraints and Ghazi et al. (2020) for clustering under central DP constraints.). Since, we run T iterations in Algorithm 3, we can obtain the final privacy analysis ((cid:15), δ) using the strong composition theorem Dwork & Roth (2014).\n\nlog(m2n) σ2\n\nn + 2\n\nd σ2\n\n(cid:113)\n\nx\n\nx\n\nE PERSONALIZED LEARNING – LINEAR REGRESSION\n\nIn this section, we present the personalized linear regression problem. Consider A set of m clients, where the i-th client has a local dataset consisting of n samples (Xi1, Yi1), . . . , (Xin, Yin), where Xij ∈ Rd denotes the feature vector and Yij ∈ R denotes the corresponding response. Let Yi = (Yi1, . . . , Yi1) ∈ Rn and Xi = (Xi1, . . . , Xin) ∈ Rn×d denote the response vector and the feature matrix at the i-th client, respectively. Following the standard regression, we assume that the response vector Yi is obtained from a linear model as follows:\n\n(46) where θi denotes personalized model of the i-th client and wi ∼ N (cid:0)0, σ2 (cid:1) is a noise vector. The clients’ parameters θ1, . . . , θm are drawn i.i.d. from a Gaussian distribution θ1, . . . , θm ∼ N (μ, σ2 θ\n\nYi = Xiθi + wi,\n\nId), i.i.d.\n\nIn\n\nx\n\nOur goal is to solve the optimization problem stated in (9) (for the linear regression setup) and learn the optimal personalized parameters {(cid:98)θi}. The following theorem characterizes the exact form of the optimal {(cid:98)θi} and computes their minimum mean squared error w.r.t. the true parameters {θi}. Theorem 7. The optimal personalized parameters at client i with known μ, σ2 x is given by:\n\nθ , σ2\n\n(cid:98)θi =\n\n(cid:18) I\n\nσ2 θ\n\n+\n\nX T i Xi σ2 x\n\n(cid:19)−1 (cid:18) X T i Yi σ2 x\n\n+\n\n(cid:19)\n\n.\n\nμ σ2 θ\n\nThe mean squared error (MSE) of the above (cid:98)θi is given by:\n\nEwi,θi\n\n(cid:13) (cid:13) (cid:13)(cid:98)θi − θi\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= Tr\n\n(cid:32)(cid:18) I σ2 θ\n\n+\n\nX T i Xi σ2 x\n\n(cid:19)−1(cid:33)\n\n,\n\n(47)\n\n(48)\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nProof. The personalized model with perfect prior is obtained by solving the optimization problem stated in (9), which is given below for convenience. Note that for linear regression with Gaussian prior, we have P(Γ) ≡ N (μ, σ2\n\nId) and pθi(Yij|Xij) according to N (θi, σ2\n\nx).\n\nθ\n\n(cid:98)θi = arg min\n\nθi\n\n= arg min\n\nθi\n\nn (cid:88)\n\nj=1\n\nn (cid:88)\n\nj=1\n\n− log(pθi(Yij|Xij)) − log(p(θi)).\n\n(Yij − Xijθi)2 2σ2 x\n\n+\n\n(cid:107)θi − μ(cid:107)2 2σ2 θ\n\n.\n\n= arg min\n\nθi\n\n(cid:107)Yi − Xiθi(cid:107)2 2σ2 x\n\n+\n\n(cid:107)θi − μ(cid:107)2 2σ2 θ\n\n.\n\nBy taking the derivative with respect to θi, we get\n\n∂ ∂θi\n\n=\n\nX T\n\ni (Xiθi − Yi) σ2 x\n\n+\n\nθi − μ σ2 θ\n\n.\n\n(49)\n\nEquating the above partial derivative to zero, we get that the optimal personalized parameters (cid:98)θi is given by:\n\n(50)\n\n(51)\n\n(cid:98)θi =\n\n(cid:18) I\n\nσ2 θ\n\n+\n\nX T i Xi σ2 x\n\nTaking the expectation w.r.t. wi, we get:\n\n(cid:19)−1 (cid:18) X T i Yi σ2 x\n\n+\n\n(cid:19)\n\n.\n\nμ σ2 θ\n\nEwi[(cid:98)θi] =\n\n(cid:18) I\n\nσ2 θ\n\n+\n\nX T i Xi σ2 x\n\n(cid:19)−1 (cid:18) X T\n\ni Xiθi σ2 x\n\n+\n\n(cid:19)\n\n,\n\nμ σ2 θ\n\nThus, we can bound the MSE as following:\n\nEwi,θi\n\n(cid:13) (cid:13) (cid:13)(cid:98)θi − θi\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= Ewi,θi\n\n= Ewi,θi\n\n= Ewi,θi\n\n(cid:13) (cid:13) 2\n(cid:13)(cid:98)θi − Ewi[(cid:98)θi] (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13)(cid:98)θi − Ewi[(cid:98)θi] (cid:13) (cid:13) (cid:13)\n\n+ Ewi,θi\n\n(cid:13) (cid:13) 2\n(cid:13)(cid:98)θi − Ewi [(cid:98)θi] + Ewi [(cid:98)θi] − θi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:68)\n\nEwi[(cid:98)θi] − θi\n\nEwi[(cid:98)θi] − θi\n\n+ Ewi,θi\n\nIn\n\nthe\n\nlast\n\nequality,\n\nwe\n\nused\n\nEwi,θi\n\n(cid:98)θi − Ewi[(cid:98)θi], Ewi[(cid:98)θi] − θi\n\n(cid:69)\n\n=\n\nEwi[(cid:98)θi] − Ewi[(cid:98)θi], Ewi [(cid:98)θi] − θi\n\n(cid:68)\n\nEθi independent of wi.\n\n(cid:69)\n\n= 0, where the first equality holds because Ewi[(cid:98)θi] − θi is\n\nLetting M = I\n\nσ2 θ\n\n+ X T\n\ni Xi σ2 x\n\n, and Tr denoting the trace operation, we get\n\nEwi,θi\n\n(cid:13) (cid:13) (cid:13)(cid:98)θi − θi\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:32)\n\n= Tr\n\nM −1Ewi\n\n(cid:34)(cid:18) X T i wi σ2 x\n\n(cid:19) (cid:18) X T i wi σ2 x\n\n(cid:19)T (cid:35)\n\n(cid:33)\n\nM −1\n\n(cid:32)\n\n+ Tr\n\nM −1Eθi\n\n(cid:34)(cid:18) θi − μ\n\n(cid:19) (cid:18) θi − μ\n\n(cid:19)T (cid:35)\n\nσ2 θ\n\nσ2 θ\n\n(cid:33)\n\nM −1\n\n(cid:19)\n\n(cid:18)\n\nM −1\n\n+ Tr\n\nM −1\n\n(cid:19)\n\nM −1\n\nI\n\nσ2 θ\n\n(cid:18)\n\n= Tr\n\nM −1 X T\n\ni Xi σ2 x\n\n= Tr (cid:0)M −1(cid:1) .\n\nThis completes the proof of Theorem 7.\n\nObserve that the local model of the i-th client, i.e., estimating θi only from the local data (Yi, Xi), is given by:\n\n(l)\n\ni = (cid:0)X T\n\ni Xi\n\n(cid:98)θ\n\n(cid:1)−1\n\nX T\n\ni Yi,\n\n(52)\n\n27\n\n+ 2Ewi,θi\n\n(cid:68)\n\n(cid:98)θi − Ewi [(cid:98)θi], Ewi[(cid:98)θi] − θi\n\n(cid:69)\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 Linear Regression GD Input: Number of iterations T , local datasets (Yi, Xi) for i ∈ [m], learning rate η.\n\ni for i ∈ [m], μ0, σ2,0\n\nx , σ2,0 θ .\n\n1: Initialize θ0 2: for t = 1 to T do 3: On Clients: 4: 5:\n\nfor i = 1 to m: do\n\nReceive and set μt\n\nUpdate the personalized model: θt\n\ni = μt, σ2,t\n\nθ,i = σ2,t\n\nx,i = σ2,t (cid:18)\n\nx\n\nθ , σ2,t i ← θt−1\n\ni + η\n\n(cid:80)n\n\nj=1\n\nXij (Yij −Xij θt−1 σ2,t−1\n\ni\n\nx,i\n\n(cid:19)\n\n)\n\n+ μt−1\n\ni −θt−1 σ2,t−1\n\ni\n\nθ,i\n\nUpdate local version of mean: μt\n\ni ← μt−1\n\ni − η (cid:18)\n\n(cid:18)\n\nμt−1\n\ni −θt−1 σ2,t−1\n\ni\n\nθ,i\n\n(cid:19)\n\nUpdate local variance: σ2,t\n\nx,i ← σ2,t−1\n\nx,i − η\n\nUpdate global variance: σ2,t\n\nθ,i ← σ2,t−1\n\nθ,i\n\n− η\n\n(cid:19)\n\n)2\n\nn 2σ2,t−1 (cid:18)\n\nx,i\n\n− (cid:80)n\n\nj=1\n\n(Yij −Xij θt−1 i\n2(σ2,t−1 )2 (cid:19)\n\nx,i\n\nd 2σ2,t−1\n\nθ,i\n\n− (cid:107)μt−1\n\ni −θt−1 2(σ2,t−1 )2\n\ni\n\nθ,i\n\n(cid:107)2\n\nend for At the Server: (cid:80)m Aggregate mean: μt = 1 Aggregate global variance: σ2,t Aggregate local variance: σ2,t Broadcast μt, σ2,t\n\nθ , σ2,t\n\nm\n\nx\n\ni\n\ni=1 μt θ = 1 x = 1\n\nm\n\nm\n\n15: 16: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n(cid:80)m i=1 σ2,t i=1 σ2,t\n\nx,i\n\nθ,i\n\n(cid:80)m\n\n6:\n\n7:\n\n8:\n\n9:\n\n10: 11: 12:\n\n13:\n\n14:\n\nwhere we assume the matrix X T local estimate achieves the MSE given by:\n\ni Xi has a full rank (otherwise, we take the pseudo inverse). This\n\nE\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:98)θ\n\n(l)\n\ni − θi\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n= Tr\n\n(cid:16)(cid:0)X T\n\ni Xi\n\n(cid:1)−1(cid:17)\n\nσ2 x,\n\n(53)\n\nwe can prove it by following similar steps as the proof of Theorem 7. When σ2 see that the local estimate (52) matches the personalized estimate in (47).\n\nθ → ∞, we can easily\n\nTo make the regression problem more practical, we assume that the mean μ, the local variance σ2 θ are unknown. Hence, we estimate the personalized parameters by minimizing the negative log likelihood:\n\nx, and the global variance σ2\n\n(cid:98)θ1, . . . , (cid:98)θm = arg min\n\n{θi},μ,σ2\n\nx,σ2\n\nθ\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n− log (pθi (Yij|Xij)) +\n\nm (cid:88)\n\ni=1\n\n− log (p (θi))\n\n= arg min\n\nnm 2\n\nlog(2πσ2\n\nx) +\n\nm (cid:88)\n\nn (cid:88)\n\ni=1\n\nj=1\n\n(Yij − Xijθi)2 2σ2 x\n\n+\n\nmd 2\n\nlog(2πσ2\n\nθ ) +\n\nm (cid:88)\n\ni=1\n\n(cid:107)θi − μ(cid:107)2 2σ2 θ\n\n.\n\n(54)\n\nInstead of solving the above optimization problem explicitly, we can optimize it through gradient descent (GD) and the resulting algorithm is presented in Algorithm 4. In addition to keeping the personalized models {θt x,i} and updates all these parameters by taking appropriate gradients of the objective in (54) and synchronize them with the server to update the global copy of these parameters {μt, σt\n\ni}, each client also maintains local copies of {μt\n\nθ,i, σt\n\ni, σt\n\nθ, σt\n\nx}.\n\nF PERSONALIZED LEARNING – LOGISTIC REGRESSION\n\nAs described in Section 3, by taking P(Γ) ≡ N (μ, σ2 Id) and pθi(Yij|Xij) = σ((cid:104)θi, Xij(cid:105))Yij (1 − σ((cid:104)θi, Xij(cid:105)))(1−Yij ), where σ(z) = 1/1+e−z for any z ∈ R, then the overall optimization problem becomes:\n\nθ\n\n28\n\nPublished as a conference paper at ICLR 2023\n\narg min {θi},μ,σθ\n\nm (cid:88)\n\nn (cid:88)\n\n(cid:20)\n\ni=1\n\nj=1\n\nYij log\n\n(cid:18)\n\n1 1 + e−(cid:104)θi,Xij (cid:105)\n\n(cid:19)\n\n+ (1 − Yij) log\n\n(cid:18)\n\n1 1 + e(cid:104)θi,Xij (cid:105)\n\n(cid:19)(cid:21)\n\n+\n\nmd 2\n\nlog(2πσ2\n\nθ ) +\n\nm (cid:88)\n\ni=1\n\n(cid:107)μ − θi(cid:107)2 2σ2 θ\n\n2\n\n.\n\n(55)\n\nWhen μ and σ2 regression case. The corresponding algorithm is described in Algorithm 5.\n\nθ are unknown, we would like to learn them by gradient descent, as in the linear\n\nAlgorithm 5 Logistic Regression SGD Input: Number of iterations T , local datasets (Yi, Xi) for i ∈ [m], learning rate η.\n\ni for i ∈ [m], μ0, σ2,0 θ .\n\n1: Initialize θ0 2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do Receive (μt, σ2,t Update the personalized model:\n\nθ ) from the server and set μt\n\ni := μt, σ2,t\n\nθ,i := σ2,t\n\nθ\n\nθt\n\ni ← θt−1\n\ni − η\n\n\n\n\n\nn (cid:88)\n\nj=1\n\n∇θt−1\n\ni\n\nl(p) CE(θt−1\n\ni\n\n, (X j\n\ni , Y j\n\ni )) +\n\nμt−1\n\ni − θt−1 σ2,t−1\n\ni\n\nθ,i\n\n\n\n ,\n\nwhere l(p)\n\nCE denotes the cross-entropy loss. i ← μt−1\n\nUpdate local version of mean: μt\n\ni − η\n\nUpdate global variance: σ2,t θ,i\n\n← σ2,t−1 θ,i\n\n− η\n\n(cid:18)\n\n(cid:19)\n\n(cid:18)\n\nμt−1\n\ni −θt−1 σ2,t−1 θ,i\n\ni\n\nd 2σ2,t−1\n\nθ,i\n\n− (cid:107)μt−1\n\ni −θt−1 2(σ2,t−1 )2\n\ni\n\nθ,i\n\n(cid:19)\n\n(cid:107)2\n\n7:\n\n8:\n\n9: 10: 11: 12:\n\n13:\n\n14:\n\nSend (μt\n\ni, σ2,t\n\nθ,i ) to the server\n\ni, σ2,t\n\nend for At the Server: Receive {(μt (cid:80)m Aggregate mean: μt = 1 Aggregate global variance: σ2,t Broadcast (μt, σ2,t\n\nθ ) to all clients\n\nm\n\nθ,i )} from the clients\n\ni\n\ni=1 μt θ = 1\n\nm\n\n15: 16: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n(cid:80)m\n\ni=1 σ2,t\n\nθ,i\n\nG PERSONALIZED LEARNING – MIXTURE MODEL\n\nIn this section, we present the linear regression problem as a generalization to the estimation problem with discrete priors. This model falls into the framework studied in Marfoq et al. (2021) and is illustrated to show how our framework also captures it.\n\nConsider a set of m clients, where the i-th client has a local dataset (Xi1, Yi1), . . . , (Xin, Yin) of m samples, where Xij ∈ Rd denotes the feature vector and Yij ∈ R denotes the corresponding response. Let Yi = (Yi1, . . . , Yi1) ∈ Rn and Xi = (Xi1, . . . , Xin) ∈ Rn×d denote the response vector and the feature matrix at the i-th client, respectively. Following the standard regression, we assume that the response vector Yi is obtained from a linear model as follows:\n\n(56) where θi denotes personalized model of the i-th client and wi ∼ N (cid:0)0, σ2 (cid:1) is a noise vector. The clients models are drawn i.i.d. from a discrete distribution θ1, . . . , θm ∼ P, where P = [p1, . . . , pk] such that pl = Pr[θi = μl] for i ∈ [m] and l ∈ [k].\n\nYi = Xiθi + wi,\n\nIn\n\nx\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nOur goal is to solve the optimization problem stated in (9) (for the linear regression with the above discrete prior) and learn the optimal personalized parameters {(cid:98)θi}. We assume that the discrete distribution P and the prior candidates {μl}k l=1 are unknown to the clients. Inspired from Algorithm 3 for estimation with discrete priors, we obtain Algorithm 6 for learning with discrete prior. Note that this is not a new algorithm, and is essentially the algorithm proposed in Marfoq et al. (2021) applied to linear regression. We wanted to show how our framework captures mixture model in Marfoq et al. (2021) through this example.\n\nDescription of Algorithm 6. Client i initializes its personalized parameters θ(0) i Yi, which is the optimal as a function of the local dataset at the i-th client without any prior knowledge. In any iteration t, for a given prior information P(t), {μ(t) l }, the i-th client updates the personalized\n\ni Xi)−1X T\n\ni = (X T\n\nmodel as θt\n\ni = (cid:80)k\n\nl=1 α(i)\n\nl μ(t)\n\nl\n\n, where the weights α(i)\n\nl ∝ p(t)\n\nl\n\nexp\n\n(cid:18)\n\n− (cid:107)Xiμ(t)\n\nl −Yi(cid:107)2 2σ2 x\n\n(cid:19)\n\nand sends its\n\ncurrent estimate of the personalized parameter θt will run Cluster algorithm to update the global parameters P, μ(t) the clients.\n\ni to the server. Upon receiving θt\n\n1 , . . . , μ(t)\n\n1, . . . , θt m, server k , and broadcasts them to\n\nAlgorithm 6 Alternating Minimization for Personalized Learning Input: Number of iterations T , local datasets (Xi, Yi) for i ∈ [m].\n\n1: Initialize θ0\n\ni = (X T\n\ni Xi)−1X T\n\ni Yi for i ∈ [m] (if X T\n\ni Xi is not full-rank, take the pseudo-inverse).\n\n2: for t = 1 to T do 3: On Clients: 4: 5: 6:\n\nfor i = 1 to m: do\n\nReceive P(t), μ(t) Update the personalized parameters and the coefficients:\n\nk from the server\n\n1 , . . . , μ(t)\n\nθt\n\ni ←\n\nk (cid:88)\n\nl=1\n\nα(i)\n\nl μ(t)\n\nl\n\nand\n\nα(i)\n\nl =\n\n(cid:18)\n\np(t)\n\nl\n\nexp\n\n− (cid:107)Xiμ(t)\n\nl −Yi(cid:107)2 2σ2 x\n\n(cid:19)\n\n(cid:80)k\n\ns=1 p(t)\n\ns exp\n\n(cid:16)\n\n− (cid:107)Xiμ(t)\n\ns −Yi(cid:107)2 2σ2 x\n\n(cid:17)\n\nSend θ(t)\n\ni\n\nto the server\n\n7: 8: 9: 10:\n\n11:\n\nend for At the Server: Receive θ(t) Update the global parameters: P(t), μ(t) Broadcast P(t), μ(t)\n\nm from the clients\n\n1 , . . . , θ(t)\n\n1 , . . . , μ(t)\n\nk to all clients\n\n12: 13: end for Output: Personalized models θT\n\n1 , . . . , θT m.\n\n1 , . . . , μ(t)\n\nk ← Cluster\n\n(cid:16)\n\nθ(t)\n\n1 , . . . , θ(t)\n\nm , k\n\n(cid:17)\n\nH PERSONALIZED LEARNING – ADAPED\n\nH.1 KNOWLEDGE DISTILLATION POPULATION DISTRIBUTION\n\nIn this section we discuss what type of a population distribution can give rise to algorithms/problems that include a knowledge distillation (KD) (or KL divergence) penalty term between local and global models. From Section 3, Equation (9), consider pθi(y|x) as a randomized mapping from input space X to output class Y, parameterized by θi. For simplicity, consider the case where |X | is finite, e.g. for MNIST it could be all possible 28 × 28 black and white images. Every pθi(y|x) corresponds to a probability matrix (parameterized by θi) of size |Y| × |X |, where the (y, x)’th represents the probability of the class y (row) given the data sample x (column). Therefore, each column is a probability vector. Since we want to sample the probability matrix, it suffices to restrict our attention to any set of |Y| − 1 rows, as the remaining row can be determined by these |Y| − 1 rows.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nSimilarly, for a global parameter μ, let pμ(y|x) define a randomized mapping from X to Y, parameterized by the global parameter μ. Note that for a fixed global parameter μ, the randomized map pμ(y|x) is fixed, whereas, our goal is to sample pθi (y|x) for i = 1, . . . , m, one for each client. For simplicity of notation, define pθi := pθi (y|x) and pμ := pμ(y|x) to be the corresponding probability matrices, and let the distribution for sampling pθi(y|x) be denoted by ppμ (pθi). Note that different mappings pθi(y|x) correspond to different θi’s, so we define p(θi) (in Equation (9)) as ppμ (pθi), which is the density of sampling the probability matrix pθi(y|x).\n\nFor the KD population distribution, we define this density ppμ(pθi) as:\n\nppμ (pθi) = c(ψ)e−ψDKL(pμ(y|x)(cid:107)pθi (y|x))\n\n(57)\n\nwhere ψ is an ‘inverse variance’ type of parameter, c(ψ) is a normalizing function that depends on (ψ, pμ), and DKL(pμ(y|x)(cid:107)pθi(y|x)) = (cid:80) tional KL divergence, where p(x) denotes the probability of sampling a data sample x ∈ X . Now all we need is to find c(ψ) given a fixed μ (and therefore fixed pμ(y|x)). Here we consider DKL(pμ(cid:107)pθi), but our analysis can be extended to DKL(pθi (cid:107)pμ) or (cid:107)pθi − pμ(cid:107)2 as well.\n\n(cid:16) pμ(y|x) pθi (y|x)\n\ny∈Y pμ(y|x) log\n\nx∈X p(x) (cid:80)\n\nis the condi-\n\n(cid:17)\n\nFor simplicity and to make the calculations easier, we consider a binary classification task with Y = {0, 1} and define pμ(x) := pμ(y = 1|X = x) and qi(x) := pθi(y = 1|X = x). We have: (cid:16)\n\nDKL(pμ(y|x)(cid:107)pθi(y|x)) =\n\np(x)\n\npμ(x)(log pμ(x) − log qi(x))\n\n(cid:88)\n\nx\n\n+ (1 − pμ(x))(log(1 − pμ(x)) − log(1 − qi(x)))\n\n(cid:17)\n\n.\n\nHence, after some algebra we have, ppμ(pθi) = c(ψ)eψ (cid:80)\n\nx p(x)H(pμ(x))eψ (cid:80)\n\nx p(x)(pμ(x) log(qi(x))+(1−pμ(x)) log(1−qi(x))))\n\nThen,\n\nc(ψ)\n\n(cid:104) (cid:90) 1\n\n(cid:89)\n\nx\n\n0\n\nNote that\n\n(cid:105) eψp(x)H(pμ(x))eψp(x)(pμ(x) log(qi(x))+(1−pμ(x)) log(1−qi(x))))dqi(x)\n\n= 1.\n\n(cid:90) 1\n\n0\n\neψp(x)(pμ(x) log(qi(x))+(1−pμ(x)) log(1−qi(x))))dqi(x) = B\n\n(cid:18)\n\n1 +\n\npμ(x) ψp(x)\n\n, 1 +\n\n1 − pμ(x) ψp(x)\n\n(cid:19)\n\nAccordingly, after some algebra, we can obtain c(ψ) =\n\nShannon entropy. Substituting this in (57), we get\n\ne−ψ (cid:80) (cid:16) 1+\n\nx p(x)H(pμ(x)) pμ(x) ψp(x) ,1+\n\n1−pμ (x) ψp(x)\n\n(cid:81)\n\nx B\n\n(cid:17) , where H is binary\n\nppμ (pθi) =\n\ne−ψ (cid:80) x B(1 + pμ(x)\n\n(cid:81)\n\nx p(x)H(pμ(x))\n\nψp(x) , 1 + 1−pμ(x) ψp(x) )\n\ne−ψDKL(pμ(y|x)(cid:107)pθi (y|x))\n\nwhich is the population distribution that can result in a KD type regularizer. Note that when we take the negative logarithm of the population distribution we obtain KL divergence loss and an additional term that depends on ψ and pμ. This is the form seen in Section 3.2 Equation (11) for AdaPeD\n\n(cid:18)\n\ne−ψ (cid:80) x B(1+\n\nx p(x)H(pμ(x)) pμ(x) ψp(x) ,1+\n\n1−pμ (x)\n\nψp(x) )\n\n(cid:19)\n\nto\n\nalgorithm. For numerical purpose, we take the additional term − log\n\n(cid:81)\n\nbe simple 1 2 log(2ψ). As mentioned in Section 3.2, this serves the purpose of regularizing ψ. This is in contrast to the objective considered in Ozkara et al. (2021), which only has the KL divergence loss as the regularizer, without the additional term.\n\nH.2 ADAPED WITH UNSAMPLED CLIENT ITERATIONS\n\nWhen there is a flexibility in computational resources for doing local iterations, unsampled clients can do local training on their personalized models to speed-up convergence at no cost to privacy. This can be used in cross-silo settings, such as cross-institutional training for hospitals, where privacy is\n\n31\n\nPublished as a conference paper at ICLR 2023\n\ncrucial and there are available computing resources most of the time. We propose the algorithm for AdaPeD with with unsampled client iterations in Algorithm 7:\n\nAlgorithm 7 Adaptive Personalization via Distillation (AdaPeD) with unsampled client iterations\n\nParameters: model {μ0 K.\n\nlocal variances {ψ0 i }, synchronization gap τ ,\n\ni }, personalized models {θ0\n\nlocal copies of the global learning rates η1, η2, η3, number of sampled clients\n\ni },\n\nif τ divides t then On Server do: Choose a subset Kt ⊆ [n] of K clients Broadcast μt and ψt On Clients i ∈ Kt (in parallel) do: i = μt, ψt Receive μt and ψt; set μt\n\n1: for t = 0 to T − 1 do 2: 3: 4: 5: 6: 7: 8: 9: On Clients i /∈ Kt (in parallel) do:\n\nend if\n\ni = ψt\n\n10:\n\nCompute gt\n\ni := ∇θt\n\ni\n\nfi(θt\n\ni) +\n\n∇θt\n\ni\n\nreceived global parameters from the server Update: θt+1\n\ni = θt 11: 12: On Clients i ∈ Kt (in parallel) do:\n\ni − η1gt\n\ni\n\nt(cid:48) i\ni )\n\ni,μ\n\ni (θt f KD t(cid:48) i\n2ψ i\n\nwhere t(cid:48)\n\ni is the last time index where client i\n\n∇θt\n\ni\n\ni,μt i)\n\nf KD i (θt 2ψt i\n\n13:\n\n14:\n\n15:\n\n16:\n\n17:\n\nCompute gt Update: θt+1\n\ni := ∇θt i = θt ∇μt\n\ni\n\ni) +\n\nfi(θt i − η1gt i (θt+1 f KD i\n2ψt i\n\ni\n\ni\n\n,μt\n\ni)\n\ni i (θt+1 i\n2(ψt\n\n2ψt i\n\ni − η2ht − f KD i − η3kt\n\ni := i = μt i := 1 i = ψt\n\nCompute ht Update: μt+1 Compute kt Update: ψt+1 if τ divides t + 1 then Clients send μt Server receives {μt Server computes μt+1 = 1\n\n18: 19: 20: 21: 22: end if 23: 24: end for Output: Personalized models (θT\n\ni and ψt\n\ni\n\nK\n\ni )m\n\ni=1\n\n,μt+1\n\n)\n\ni\n\ni )2\n\ni to Server i}i∈Kt and {ψt\n\n(cid:80)\n\ni }i∈Kt i∈Kt μt\n\ni and ψt+1 = 1\n\nK\n\n(cid:80)\n\ni∈Kt ψt\n\ni\n\nOf course, when a client is not sampled for a long period of rounds this approach can become similar to a local training; hence, it might be reasonable to put an upper limit on the successive number of local iterations for each client.\n\nI PERSONALIZED LEARNING – DP-ADAPED\n\nProof of Theorem 5\n\nTheorem (Restating Theorem 5). After T iterations, DP-AdaPeD satisfies (α, (cid:15)(α))-RDP for α > 1, where (cid:15)(α) = (cid:0) K m denotes the sampling ratio of the clients at each global iteration.\n\n, where K\n\n(cid:16) C2 1\nKσ2\n\n+ C2\n\n6 (cid:0) T\n\n2 Kσ2\n\n(cid:1) α\n\n(cid:1)2\n\n(cid:17)\n\nm\n\nq2\n\nq1\n\nτ\n\nProof. In this section, we provide the privacy analysis of DP-AdaPeD. We first analyze the RDP of a single global round t ∈ [T ] and then, we obtain the results from the composition of the RDP over total T global rounds. Recall that privacy leakage can happen through communicating {μi} and {ψt i } and we privatize both of these. In the following, we do the privacy analysis of privatizing {μi} and a similar analysis could be done for {ψt\n\ni } as well.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nAt each synchronization round t ∈ [T ], the server updates the global model μt+1 as follows:\n\nμt+1 =\n\n1 K\n\n(cid:88)\n\ni∈Kt\n\nμt i,\n\n(58)\n\nwhere μt iterations at the i-th client. At each of the local iterations, the client clips the gradient ht threshold C1 and adds a zero-mean Gaussian noise vector with variance σ2 q1 noise added at the local iterations, the norm-2 sensitivity of updating the global model μt+1 synchronization round t is bounded by:\n\ni is the update of the global model at the i-th client that is obtained by running τ local i with Id. When neglecting the at the\n\ni\n\n∆μ = max Kt,K(cid:48) t\n\n(cid:107)μt+1 − μ(cid:48)t+1(cid:107)2\n\n2 ≤\n\nτ C 2 1\nK 2 ,\n\n(59)\n\n(cid:80)\n\n(cid:48)t ⊂ [m] are neighboring sets that differ in only one client. Additionally, μt+1 = where Kt, K 1\ni and μ(cid:48)t+1 = 1 i∈Kt μt q1 at K\neach local iteration at each client, and then, we take the average of theses vectors over K clients,\n\ni. Since we add i.i.d. Gaussian noises with variance σ2\n\ni∈K(cid:48)t μt\n\n(cid:80)\n\nK\n\nτ σ2 q1 K . it is equivalent to adding a single Gaussian vector to the aggregated vectors with variance Thus, from the RDP of the sub-sampled Gaussian mechanism in (Mironov et al., 2019, Table 1), Bun et al. (2018), we get that the global model μt+1 of a single global iteration of DP-AdaPeD is (α, (cid:15)(1)\n\nt (α))-RDP, where (cid:15)t(α) is bounded by:\n\n(cid:15)(1) t (α) =\n\n(cid:18) K m\n\n(cid:19)2 6αC 2 1\nKσ2 q1\n\n.\n\n(60)\n\nSimilarly, we can show that the global parameter ψt+1 at any synchronization round of DP-AdaPeD is (α, (cid:15)(2)\n\nt (α))-RDP, where (cid:15)t(α) is bounded by:\n\n(cid:15)(2) t (α) =\n\n(cid:18) K m\n\n(cid:19)2 6αC 2 2\nKσ2 q2\n\n.\n\n(61)\n\nUsing adaptive RDP composition (Mironov, 2017, Proposition 1), we get that each synchronization round of DP-AdaPeD is (α, (cid:15)(1) t (α))-RDP. Thus, by running DP-AdaPeD over T /τ synchronization rounds and from the composition of the RDP, we get that DP-AdaPeD is (α, (cid:15)(α))- RDP, where (cid:15)(α) = (cid:0) T\n\nt (α)). This completes the proof of Theorem 5.\n\nt (α) + (cid:15)(2)\n\nt (α) + (cid:15)(2)\n\n(cid:1) ((cid:15)(1)\n\nτ\n\nJ EXPANDED RELATED WORK AND CONNECTIONS TO EXISTING METHODS\n\nIn Section 1, we mentioned that the our framework has connections to several personalized FL methods. In this appendix we provide a few more details related to these connections.\n\nRegularization: As noted earlier using (9) with the Gaussian population prior connects to the use of (cid:96)2 regularizer in earlier personalized learning works Dinh et al. (2020); Ozkara et al. (2021); Hanzely & Richt ́arik (2020); Hanzely et al. (2020); Li et al. (2021), which also iterates between local and global model estimates. This form can be explicitly seen in Appendix E, where in Algorithm 4, we see that the Gaussian prior along with iterative optimization yields the regularized form seen in these methods. In these cases8, P(Γ) ≡ N (μ, σ2 Id) for unknown parameters Γ = {μ}. Note that since the parameters of the population distribution are unknown, these need to be estimated during the iterative learning process. In the algorithm, 4 it is seen the μ plays the role of the global model (and is truly so for the linear regression problem studied in Appendix E).\n\nθ\n\nClustered FL: If one uses a discrete mixture model for the population distribution then the iterative algorithm suggested by our framework connects to (Zhang et al., 2021; Mansour et al., 2020; Ghosh et al., 2020; Smith et al., 2017; Marfoq et al., 2021). In particular, consider a population model with parameters in the m-dimensional probability simplex {α : α = [α1, . . . , αk], αi ≥ 0, ∀i, (cid:80) i αi = 1} which describing a distribution. If there are m (unknown) discrete distributions {D1, . . . , Dm}, one can consider these as the unknown description of the population model in addition to α. Therefore,\n\n8One can generalize these by including σ2\n\nθ in the unknown parameters.\n\n33\n\nPublished as a conference paper at ICLR 2023\n\neach local data are generated either as a mixture as in (Marfoq et al., 2021) or by choosing one of the unknown discrete distributions with probability α dictating the probability of choosing Di, when hard clustering is used (e.g., (Mansour et al., 2020)). Each node j chooses a mixture probability α(j) uniformly from the m-dimensional probability simplex. In the former case, it uses this mixture probability to generate a local mixture distribution. In the latter it chooses Di with probability α(j) .\n\ni\n\nAs mentioned earlier, not all parametrized distributions can be written as a mixture of finite number distributions, which is the assumption for discrete mixtures. Consider a unimodal Gaussian population Id), for node i, we sample distribution (as also studied in Appendix E). Since P(Γ) ≡ N (μ, σ2 (cid:62)x, σ2). θi ∼ P(Γ). We see that the actual data distribution for this node is pθi(y|x) = N (θi Clearly the set of such distributions {pθi(y|x)} cannot be written as any finite mixture as θi ∈ Rd and pθi(y|x) is a unimodal Gaussian distribution, with same parameter θi for all data generated in node i. Essentially the generative framework of finite mixtures (as in (Marfoq et al., 2021)) could be restrictive as it does not capture such parametric models.\n\nθ\n\nKnowledge distillation: The population distribution related to a regularizer based on KullbackLeibler divergence (knowledge distillation) has been shown in Appendix H. Therefore this can be cast in terms of information geometry where the probability falls of exponentially with in this geometry. Hence these connect to methods such as Lin et al. (2020); Li & Wang (2019); Shen et al. (2020); Ozkara et al. (2021), but the exact regularizer used does not take into account the full parametrization, and one can therefore improve upon these methods.\n\nFL with Multi-task Learning (MTL): In this framework, a fixed relationship between tasks is usually assumed (Smith et al., 2017). Therefore one can model this as a Gaussian model with known parameters relating the individual models. The individual models are chosen from a joint Gaussian with particular (known) covariance dictating the different models, and therefore giving the quadratic regularization used in FL-MTL (Smith et al., 2017). In this the parameters of the Gaussian model are known and fixed.\n\nj=1 Bw(i)\n\nCommon representations: The works in Du et al. (2021); Jain et al. (2021b) use a linear model where y ∼ N (x(cid:62)θi, σ2) can be considered a local generative model for node i. The common representation approach assumes that θi = (cid:80)k j , for some k (cid:28) d, where θi ∈ Rd. Therefore, one can parametrize a population by this (unknown) common basis B, and under a mild assumption that the weights are bounded, we can choose a uniform measure in this bounded cube to choose w(i) for each node i. The alternating optimization iteratively discovers the global common representation and the local weights as done in Du et al. (2021); Jain et al. (2021b) (and references therein). This common linear representation approach was generalized in Du et al. (2021); Collins et al. (2021) to neural networks, where a set of parameters to obtain a common representation (“head”) at each client was obtained and each local client appendd it with a “tail” combining the representation to obtain the final model. This also fits into our statistical framework, where the common representation (head) parameters are chosen from a population model (like the common subspace in the linear case) and the tail parameters are independently chosen (again as in the linear case).\n\nEmpirical and Hierarchical Bayes: As mentioned in Section 1, our work is inspired by empirical Bayes framework, introduced in (Stein, 1956; Robbins, 1956; James & Stein, 1961), which is the origin of hierarchical Bayes methods; see also (Gelman et al., 2013, pp. 132). (Stein, 1956; James & Stein, 1961) studied jointly estimating Gaussian individual parameters, generated by an unknown (parametrized) Gaussian population distribution. They showed a surprising result that one can enhance the estimate of individual parameters based on the observations of a population of Gaussian random variables with independently generated parameters from an unknown (parametrized) Gaussian population distribution. Effectively, this methodology advocated estimating the unknown population distribution using the individual independent samples, and then using it effectively as an empirical prior for individual estimates.9 This was studied for Bernoulli variables with heterogeneously generated individual parameters by Lord (1967) and the optimal error bounds for maximum likelihood estimates for population distributions were recently developed in (Vinayak et al., 2019). Hierarchical Bayes, builds on empirical Bayes framework and is sometimes associated with a fully Bayes method. Our choice to use empirical Bayes framework as the foundation is also because\n\n9This was shown to uniformly improve the mean-squared error averaged over the population, compared to an\n\nestimate using just the single local sample.\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nit is more computationally feasible than a fully Bayes method. The subtle difference between the two is that empirical Bayes uses a point estimate of a (parametrized) prior, whereas, the terminology hierarchical Bayes often refers to a fully Bayes method where the (non-parametric) prior is estimated by computationally intensive methods like MCMC (see the discussion in (Blei et al., 2003)). As mentioned in Section 1, a contribution of our work is to connect a well studied statistical framework of empirical (hierarchical) Bayes to model heterogeneity in personalized federated learning. This statistical model yields a framework for personalized FL and leads to new algorithms and bounds especially in the local data starved regime.\n\nK ADDITIONAL DETAILS AND RESULTS FOR EXPERIMENTS\n\nK.1\n\nIMPLEMENTATION DETAILS\n\nIn this section we give further details on implementation and setting of the experiments that were used in Section 4.\n\nCIFAR-100 Experiment Setting. We do additional experiments on CIFAR-100. CIFAR-100 is a dataset consisting of 100 classes and 20 superclasses. Each superclass corresponds to a category of 5 classes (e.g. superclass flowers correspond to orchids, poppies, roses, sunflowers, tulips). To introduce heterogeneity we let each client sample data samples from 2 super classes (the classification task is still to classify among 100 classes). For classification on CIFAR-100 dataset we consider a 5-layer CNN with 2 convolutional layers of 64 filters and 5x5 filter size, following that we have 2 fully connected layers with activation sizes of 384,192 and finally an output layer of dimension 100. We set number of local epochs to be 2, batch size to be 25 per client; number of clients is 50, client participation K n = 0.2, and number of epochs 200 (100 communication rounds). In this new dataset the classification task is more complex given the increased number of labels.\n\nHyperparameters. We implemented Per-FedAvg and pFedMe based on the code from GitHub,10. Other implementations were not available online, so we implemented ourselves. For each of the methods we tuned learning rate in the set {0.3, 0.2, 0.15, 0.125, 0.1, 0.075, 0.05} and have a decaying learning schedule such that learning rate is multiplied by 0.99 at each epoch. We use weight decay of 1e − 4. For MNIST and FEMNIST experiments for both personalized and global models we used a 5-layer CNN, the first two layers consist of convolutional layers of filter size 5 × 5 with 6 and 16 filters respectively. Then we have 3 fully connected layers of dimension 256 × 120, 120 × 84, 84 × 10 and lastly a softmax operation. For CIFAR-10 experiments we use a similar CNN, the only difference is the first fully connected layer is of dimension 400 × 120.\n\n• AdaPeD11: We fine-tuned ψ in between 0.5 − 5 with 0.5 increments and set it to 3.5. We set η3 = 5e − 2. We manually prevent ψ becoming smaller than 0.5 so that local loss does not become dominated by the KD loss. We use η2 = 0.1 and η1 = 0.1. 12 When taking the derivative with respect to ψ we observed sometimes multiplying the right term (consist of KD loss function) by some constant (5 in our experiments) gives better performance.\n\n• Per-FedAvg Fallah et al. (2020) and pFedMe Dinh et al. (2020): For Per-FedAvg, we used 0.075 as the learning rate and α = 0.001. For pFedMe we used the same learning rate schedule for main learning rate, K = 3 for the number of local iterations; and we used λ = 0.5, η = 0.2.\n\n• QuPeD Ozkara et al. (2021): We choose λp = 0.25, η1 = 0.1 and η3 = 0.1 as stated.\n\n• Federated Mutual Learning Shen et al. (2020): Since authors do not discuss the hyperparameters in the paper, we used α = β = 0.25, global model has the same learning schedule as the personalized models.\n\nK.2 ADDITIONAL EXPERIMENTS\n\nConvergence plots for AdaPeD. We put the experimental convergence plots (test accuracy vs no. of iteration) for AdaPeD in Figure 2.\n\n10https://github.com/CharlieDinh/pFedMe 11For federated experiments we have used PyTorch’s Data Distributed Parallel package. 12We use https://github.com/tao-shen/FEMNIST_pytorch to import FEMNIST dataset.\n\n35\n\nPublished as a conference paper at ICLR 2023\n\n(a) AdaPeD Test Accuracy (in %) vs iteration on MNIST with 0.1 sampling ratio.\n\n(b) AdaPeD Test Accuracy (in %) vs iteration on FEMNIST with 0.33 sampling ratio.\n\nFigure 2: Convergence plots (test accuracy vs no. of iteration) for AdaPeD.\n\nPersonalized estimation: synthetic experiments in Bernoulli setting. For this setting, for P we consider three distributions that (Tian et al., 2017) considered: normal, uniform and ‘3-spike’ which have equal weight at 1/4, 1/2, 3/4. Additionally, we consider a Beta prior. We compute squared error of personalized estimators and local estimators ( Zi n ) w.r.t. the true pi and report the average over all clients. We use m = 10000 clients and 14 local samples similar to (Tian et al., 2017). Personalized estimator provides a decrease in MSE by 37.1 ± 3.9%, 12.0 ± 1.6%, 24.3 ± 2.8%, 34.0 ± 3.7%, respectively, for each aforementioned population distribution compared to their corresponding local estimators. Furthermore, as theoretically noted, less spread out prior distributions (low data heterogeneity) results in higher MSE gap between personalized and local estimators.\n\nLinear regression. For this, we create a setting similar to (Jain et al., 2021a). We set m = 10, 000, n = 10; and sample client true models according to a Gaussian centered at some randomly chosen μ with variance σ2 θ . We randomly generate design matrices Xi and create Yi at each client by adding a zero mean Gaussian noise with true variance σ2 x = 0.05 and we sample each component of μ from a Gaussian with 0 mean and 0.1 standard deviation and each component of X from a Gaussian with 0 mean and variance 0.05, both i.i.d. We measure the average MSE over all clients with and compare personalized and local methods. When d = 50, personalized regression has an MSE gain of 8.0 ± 0.8%, 14.8 ± 1.2%, and when d = 100, 9.2±1.1%, 12.3±2.0% compared to local and FedAvg regression, respectively. Moreover, compared to personalized regression where μ, σθ, σx are known, alternating algorithm only results in 1% and 4.7% increase in MSE respectively for d = 50 and d = 100.\n\nx to Xiθi. We set true values σ2\n\nθ = 0.01, σ2\n\nEstimation Experiments. We provide more results for the estimation setting discussed in Figure 1a. In Figure 3a we have a setting with 1000 clients and 5 local samples and in Figure 3b 500 clients and 5 local samples per client. We observe with as the number of clients increase DP-Personalized Estimator can converge to Personalized Estimator with less privacy budget. We also observe compared to Figure 1a, less number of local samples increases the performance discrepancy between personalized and local estimator.\n\n(a) Private Estimation with m=1000, n=5\n\n(b) Private Estimation with m=500, n=5\n\n36\n\nFigure 3: In Figure 1a, we plot MSE vs. (cid:15)0 for personalized estimation with different number of clients, this is the same setting as Figure 1a except the number of clients and local samples.\n\n01020304050iteration405060708090100Test Accuracy (in %)0510152025303540iteration20406080100Test Accuracy (in %)0.10.20.30.40.50.60.70.8ε012345MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized Estimator0.10.20.30.40.50.60.70.8ε012345MSE (in 1e-2)Local EstimatorPersonalized EstimatorDP Personalized EstimatorPublished as a conference paper at ICLR 2023\n\nAdditional Learning Experiments with Different Number of Clients. We do additional experiments with different number of clients. On FEMNIST we use the same model and same data sample per client as in Section4, number of clients is 30, total number of epochs is 30 and we fix the local iteration to be 40 per epoch, we do full client sampling to simulate a cross-silo environment. As seen in Table 4, AdaPeD continues to outperform the competing methods following the trend in Section 4.\n\nTable 4: Test accuracy (in %) for FEMNIST with m = 30 clients.\n\nMethod\n\nFedAvg FedAvg+fine tuning Jiang et al. (2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\nFEMNIST\n\n95.91 ± 0.78 96.22 ± 0.57 98.10 ± 0.09 96.03 ± 0.50 96.71 ± 0.14 97.72 ± 0.16 96.80 ± 0.13\n\nOn CIFAR-10 we use the same model as in Section4, and divide the dataset to 30 clients where each client has access to data samples from 4 classes. Total number of epochs is 250 and we fix the local iteration to be 40 per epoch; we set K n = 0.2 and number of local epochs to be 2. AdaPeD outperforms the competing methods in parallel to the experiments in Section4, as can be seen in Table 5.\n\nTable 5: Test accuracy (in %) for CIFAR-10 with m = 30 clients.\n\nMethod\n\nFedAvg FedAvg+fine tuning Jiang et al. (2019) AdaPeD (Ours) pFedMe (Dinh et al., 2020) Per-FedAvg (Fallah et al., 2020) QuPeD (FP) (Ozkara et al., 2021) Federated ML (Shen et al., 2020)\n\nCIFAR-10\n\n53.92 ± 0.94 67.44 ± 1.11 73.86 ± 0.39 71.97 ± 0.09 64.09 ± 0.46 73.21 ± 0.44 72.53 ± 0.36\n\nAdditional Experiment Implementation Details.\n\nWe use the same strategy as in Appendix K.1 to tune the main learning rates. We use 1e-4 weight decay.\n\n• AdaPeD: We fine-tuned ψ in between 0.5 − 5 with 0.5 increments and set it to 4 for CIFAR-10/100 and to 3 for FEMNIST. We manually prevent ψ becoming smaller than 1 so that local loss does not become dominated by the KD loss. We use η2 = 0.075 and η1 = 0.075 for CIFAR-10 and CIFAR-100 and η2 = 0.1 and η1 = 0.1 for FEMNIST.\n\n• Per-FedAvg (Fallah et al., 2020) and pFedMe (Dinh et al., 2020): For Per-FedAvg, we used 0.1 as the learning rate and α = 0.0001. For pFedMe we used the same learning rate schedule for main learning rate, L = 3 for the number of local approximation iterations; and we used λ = 0.1, η = 0.1.\n\n• QuPeD Ozkara et al. (2021): We set λp = 0.25, η1 = 0.1 for local learning rate and η2 = 0.1 for\n\nglobal learning rate.\n\n• Federated Mutual Learning Shen et al. (2020): Since authors do not discuss the hyperparameters in\n\nthe paper, we used α = β = 0.25.\n\n37",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes algorithms that search for suitable personalized models in a client-server type federated learning setup. The algorithms are inspired by the classical theory of parametric Bayesian risk minimization. In the personalized parameter estimation regime, the authors assume two population distributions: Gaussian and Bernoulli. Under Gaussian distribution, they report that in case the parent population becomes degenerate (i.e., variation tending to nullity), the global average turns out to be the ‘best’ estimator. Moreover, the posterior personalized mean estimator in this setup also turns out to be optimal in general. If the parent population follows a Bernoulli law, having sufficient observations from local sub-populations suggests against collaborating. The following ‘personalization’ algorithms utilize different prior distributions and regularization schemes to ensure client privacy.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe language of the article is lucid, and the presentation is also of good quality. The discussion leading up to the theoretical analyses and the algorithms is precise. I find the statistical analysis rigorous and very well represented. Prior works and relevant references are well-placed throughout the paper. \n\nWeakness/Issues:\n\nThe authors have altered the standard structure of the article, as guided by ICLR instructions. The abstract should not be full page-wide. This is a violation of the code and gives them an undue advantage over others. \n\nThe current article looks incomplete, lacking a ‘Conclusion’ section. Also, sufficient discussion regarding limitations and future work is missing.\n\nI suggest the authors present accompanying codes maintaining anonymity. \n\nIt would be very helpful if the problem statement is presented more precisely in the introduction. The authors provide a lot of references to prior work. However, amidst such a crowd, the motivation somehow fades. \n\nAs I have acknowledged, the discussion is quite rigorous. However, the is significant room for improvement when it comes to organization. \n\nThe empirical results seem insufficient, and I suggest the authors put more datasets to test if feasible.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper presents an overall well investigated research work. To enhance clarity the authors need to answer the following questions:\n\nQuestions:\n\nWhat does it mean statistically to “estimate i through the help of the server.”?\n\nIn the section ‘Personalized Estimation’, should it be “client i outputs an estimate i of i” instead?\n\nDoes the prior distribution necessarily need to have a density in general? How ‘realistic’ are the special case assumptions of Gaussian and Bernoulli as global population distributions?\n\nIn the statement of Theorem 2., should it be “[-r,r]d” instead? \n\nDoes the notation Ɛ mean expectation [In Theorem 2, 3, etc.]? If so, kindly maintain any one symbol throughout. \nThe authors also use the same notation for a different purpose in Section B.2.1.\n\nShouldn’t we have the sum also over the quantity on the right-hand side of equation (10) [Section 3.1]?\n\nThe quantity it may penalize the heterogeneity, but does not denote the variance. The authors should call it something else instead [Section 3.2, last paragraph].\n\nIs the strict imposition of the value ‘1’ necessary in the definition of ‘neighborhood’ [Section A.3], since there is a clear possibility to generalize the result even if two datasets D and D' differ at multiple terms?\n\nIn Theorem 2, the upper bound on MSE  (5) loses its worth in a higher-dimensional setup. Can the authors talk about any remedy to the same?\n\n# Summary Of The Review\n\nThe paper may be considered for acceptance provided the authors address the above listed concerns.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" presents a novel statistical framework aimed at addressing statistical heterogeneity in federated learning (FL). The authors, Kaan Ozkara et al., propose a unified approach that integrates existing personalization methods and introduces new algorithms, specifically AdaPeD and DP-AdaPeD, which leverage information geometry and differential privacy. The framework is built upon empirical Bayes methodology and provides theoretical bounds for personalized estimation under privacy and communication constraints. Experimental results demonstrate significant performance improvements in personalized estimation and learning across various datasets, including MNIST and CIFAR-10.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its comprehensive development of a statistical framework that not only addresses personalization but also incorporates privacy considerations in federated learning. The introduction of novel algorithms, particularly AdaPeD, and their empirical validation against existing methods showcases the practical impact of the proposed framework. However, the paper could benefit from a more extensive discussion of the limitations of the proposed methods, especially regarding their scalability in real-world applications with larger datasets and more heterogeneous client distributions. Additionally, while the theoretical foundations are presented, further exploration of their implications in practice could enhance the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly structured, with a logical flow from the introduction of problems in federated learning to the presentation of the proposed framework and algorithms. The quality of the writing is high, with technical details well-articulated and supported by empirical results. The novelty of the paper is significant, particularly in its approach to integrate privacy with personalized federated learning. Reproducibility is addressed through detailed experimental setups and the inclusion of appendices with proofs and additional results, though the practical implementation details could be better articulated for researchers seeking to replicate the study.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of personalized federated learning by providing a robust statistical framework that addresses both estimation and privacy concerns. The introduction of innovative algorithms, along with solid empirical validation, underscores the relevance and applicability of the research. Overall, the paper is well-structured and presents a compelling case for its findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a statistical framework for personalized federated learning (FL) that addresses the challenges of heterogeneity in client data while ensuring privacy. It proposes a hierarchical modeling approach for local parameter estimation and develops innovative algorithms—including AdaPeD, DP-AdaPeD, and AdaMix—that leverage information geometry for personalized learning. The empirical results demonstrate that these algorithms significantly outperform traditional FL methods in both personalized estimation and learning tasks across various datasets, while maintaining strong privacy guarantees.\n\n# Strength And Weaknesses\nThe strengths of the paper include its provision of a unified statistical framework that integrates various personalization approaches within federated learning, as well as the introduction of novel algorithms that show marked improvements over existing methods. Additionally, the formal privacy analysis enhances the credibility of the proposed solutions. However, the paper has limitations, including potential scalability issues for large federated networks, sensitivity to hyperparameter tuning, and a focus primarily on Gaussian and Bernoulli distributions, which may restrict the generalizability of the framework to other data types and distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of the experiments is high, with extensive validation on both synthetic and real-world datasets, affirming the reproducibility of results. The technical novelty is significant, as it combines established statistical methods with innovative algorithms tailored for personalized FL, although the exploration of only limited distributions reduces its breadth.\n\n# Summary Of The Review\nOverall, this paper presents a noteworthy advancement in the field of personalized federated learning by introducing a comprehensive statistical framework and demonstrating the effectiveness of novel algorithms. While the contributions are substantial, certain limitations regarding scalability and generalization to diverse data distributions should be addressed in future work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" presents a unified statistical framework to address the challenges posed by statistical heterogeneity in federated learning (FL). The authors introduce novel algorithms, including AdaPeD, which leverage empirical Bayes methodology to enhance personalized estimation and learning under privacy constraints. The methodology includes a client-server architecture for estimating local parameters from limited data, with rigorous performance measurements. Empirical evaluations demonstrate that the proposed personalized estimators and learning models outperform traditional methods in various datasets while satisfying differential privacy guarantees.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its comprehensive approach to combining theoretical insights with practical algorithmic implementations, effectively addressing the dual challenges of personalization and privacy in FL. The connection to empirical Bayes adds depth to the theoretical framework, allowing for robust performance analysis. However, a notable weakness is the limited scope of experimental evaluations; while the results are promising, additional datasets and real-world scenarios could bolster the empirical validation of the proposed methods. Furthermore, the paper could benefit from a clearer discussion on the computational complexity of the proposed algorithms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodologies, and findings. The clarity of the theoretical exposition is commendable, making complex concepts accessible. The quality of the writing is high, with appropriate use of mathematical notation and careful explanations. The novelty of combining empirical Bayes with personalized FL is significant, marking a valuable advancement in the field. Reproducibility is supported by the detailed descriptions of methodologies and algorithms, although providing code or supplementary materials would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning by providing a solid theoretical foundation and practical algorithms for personalized learning under privacy constraints. While the empirical results are promising, further validation across diverse datasets would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a unified statistical framework for integrating various personalization methods in federated learning, enhancing the theoretical understanding of personalized learning tasks. It introduces novel algorithms, AdaPeD and AdaMix, which utilize information-geometry regularization and adaptive weighting to improve performance. The findings demonstrate substantial numerical improvements over existing methods across different datasets while ensuring user-level privacy guarantees.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its comprehensive framework that consolidates multiple personalization techniques, fostering a deeper understanding of the domain. The introduction of AdaPeD and AdaMix represents a significant contribution, although the performance claims are primarily supported by numerical evaluations rather than extensive theoretical scrutiny or empirical validation across diverse datasets. The emphasis on privacy is commendable, yet the associated computational overhead may hinder practical scalability. While the experimental setup is detailed, the reliance on synthetic or limited datasets raises questions about the robustness of the findings in real-world applications.\n\n**Limitations** include the potential oversimplification of personalization techniques, the need for broader empirical validation, and the sensitivity of results to specific experimental conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, with a logical flow that facilitates understanding of the proposed methods and their implications. While the novelty of the algorithms is notable, the reproducibility may be challenged due to the limited variety of datasets used in experiments. A more diverse set of real-world scenarios could enhance the credibility of the findings and their applicability.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning by presenting a unified framework and novel algorithms that emphasize personalization and privacy. However, it would benefit from broader empirical validation and a more comprehensive theoretical analysis to strengthen its claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" addresses the challenges of statistical heterogeneity in federated learning (FL) by proposing a unified statistical framework. The authors introduce Adaptive Personalized Estimation (AdaPeD), which adapts local model parameters based on global distributions while maintaining strong privacy guarantees via differential privacy mechanisms. Extensive numerical evaluations demonstrate that the proposed methods outperform existing FL approaches in terms of mean squared error (MSE) and test accuracy, particularly in scenarios characterized by high data heterogeneity.\n\n# Strength And Weaknesses\nStrengths of the paper include its well-justified theoretical foundations that connect empirical Bayes principles to FL, the introduction of AdaPeD which shows significant advancements in personalized learning, and a comprehensive analysis of privacy mechanisms that ensure data security. However, the paper's weaknesses lie in the need for further empirical validation across diverse datasets to support the generalizability of the findings. Additionally, the complexity of the proposed algorithms may present implementation challenges in resource-constrained environments, which is a common concern in federated learning applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, with a logical flow from theoretical foundations to practical implications. The quality of the methodology is strong, supported by rigorous theoretical analysis and extensive empirical validation. In terms of novelty, the integration of personalized learning with robust privacy guarantees represents a significant contribution to the field. However, reproducibility may be hindered by the complexity of the algorithms and the necessity for detailed computational insights that are not fully addressed.\n\n# Summary Of The Review\nThis paper presents a noteworthy advancement in personalized federated learning through a unified statistical framework, innovative algorithms, and strong privacy protections. While the theoretical contributions are robust, the empirical validation could benefit from a wider range of datasets, and the complexity of the proposed methods may limit practical implementation.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"A Statistical Framework for Adversarial Training: Theory, Algorithms, and Robustness\" introduces a new statistical framework specifically designed for adversarial training in the context of heterogeneous datasets. The authors highlight the limitations of traditional adversarial training methods that do not adequately consider the diverse distributions of data across different clients. They propose a unified statistical model that supports new algorithms, including an adaptive adversarial training method that modifies the training process according to local data distributions. Empirical results demonstrate that these methods significantly enhance model robustness and performance against adversarial attacks, validated through extensive experimentation on various datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its strong theoretical grounding, which provides a clear understanding of adversarial training in heterogeneous settings. The focus on robustness against adversarial attacks addresses a pressing challenge in machine learning, making the work relevant and timely. Furthermore, the empirical validation across multiple datasets supports the effectiveness of the proposed algorithms. However, the increased complexity of the framework may pose implementation challenges for practitioners. Additionally, the generalizability of the findings to other adversarial scenarios or domains remains to be fully explored, which could limit the framework's applicability in broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, with a logical flow from theory to algorithms and empirical evaluation. The quality of the writing is high, making the complex concepts accessible. The novelty of the proposed statistical framework and algorithms is significant, presenting a fresh perspective on adversarial training. However, the reproducibility of the results may be affected by the complexity of the new methods, which could hinder adoption by the community without additional resources or simplifications.\n\n# Summary Of The Review\nThis paper presents a valuable contribution to the field of adversarial training by introducing a robust statistical framework that addresses data heterogeneity and enhances model robustness. The empirical validation of the proposed algorithms is compelling, although the complexity of implementation may pose challenges for practitioners. Overall, the work is well-articulated and likely to advance the understanding and application of adversarial training methodologies.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a statistical framework for personalized federated learning (FL) that integrates various existing algorithms into a unified theory, claiming to redefine the field. The authors introduce novel algorithms, such as AdaPeD and AdaMix, which they assert outperform all previous methods in diverse scenarios. The work emphasizes significant advancements in user privacy within FL and provides empirical evaluations demonstrating superior performance across multiple datasets. Furthermore, the authors connect their approach to classical empirical Bayes methods, suggesting profound implications for both theoretical understanding and practical applications in FL.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious attempt to unify existing methods under a comprehensive statistical framework, which could simplify the understanding of FL. Additionally, the introduction of new algorithms with claimed universal applicability is noteworthy. However, the weaknesses include an overstatement of contributions, as the claims regarding the obsolescence of prior methods and the revolutionary nature of the framework lack sufficient empirical backing. The paper also downplays the complexities and challenges that remain in the field, presenting a somewhat unbalanced view of its findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly outlines its contributions, although the exaggeration of claims may lead to misunderstandings about the actual impact of the work. While the methodology is described in sufficient detail, the reproducibility of results could be questioned due to the broad claims of superiority without thorough comparative analyses. The novelty is present in the proposed algorithms and their integration into a unified framework, but the implications of connecting classical methods to modern FL approaches require more rigorous exploration.\n\n# Summary Of The Review\nOverall, the paper presents an ambitious framework for personalized federated learning that claims to unify and advance the field significantly. However, the overstatement of its contributions and potential impact detracts from the credibility of the findings. A more balanced presentation of the results and challenges would improve the clarity and robustness of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a statistical framework for personalized federated learning, addressing the challenges posed by local data heterogeneity among clients. It introduces new algorithms, particularly AdaPeD, which leverages information-geometry regularization to enhance model performance while ensuring privacy. Key contributions include theoretical bounds for performance and privacy guarantees, along with empirical evaluations demonstrating significant improvements in mean squared error (MSE) and accuracy over existing methods across various datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive approach to integrating various algorithms into a unified statistical framework, enhancing the understanding of personalized federated learning. The introduction of AdaPeD, with demonstrated performance improvements in both MSE and accuracy, marks a significant advancement in the field. However, the paper could further elaborate on the theoretical underpinnings of the proposed algorithms and provide additional empirical evaluations to bolster the claims made. The clarity of the presented results could also be improved by providing a more detailed discussion on the implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a structured presentation of the framework and results. However, some sections may benefit from further elaboration, particularly in the methodology and theoretical analysis. The novelty is substantial, as the paper introduces new algorithms and connects existing methods in a meaningful way. Reproducibility is supported by the provided experimental results, but additional details on the implementation and datasets would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of personalized federated learning by presenting a novel framework and demonstrating the effectiveness of the AdaPeD algorithm. While the results are promising, further clarification and additional empirical studies would strengthen the paper's claims and overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper proposes new methodologies for federated learning that focus on personalized model training while incorporating privacy guarantees and addressing communication constraints. The authors introduce an empirical Bayes framework to estimate parameters based on local client data, assuming statistical heterogeneity among clients. The findings suggest that their algorithms, particularly AdaPeD, achieve optimality under specific theoretical conditions, though they may struggle with practical limitations in diverse real-world scenarios.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative integration of personalized models and privacy mechanisms within the federated learning paradigm. The empirical Bayes approach offers a compelling framework for parameter estimation. However, the paper is weakened by several assumptions that may not hold in practice, such as the reliance on statistical heterogeneity and the sufficiency of local data for training. Additionally, the potential overfitting on individual models due to personalized approaches raises concerns about generalization. The claims of optimality are based on conditions that may not be met in real-world applications, highlighting a gap between theoretical bounds and practical performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper provides a clear presentation of its methodology and results, making it accessible to readers familiar with federated learning. However, the novelty is somewhat tempered by the reliance on established frameworks and assumptions that may limit reproducibility. The authors do not adequately address how their results generalize across different datasets and scenarios, which could hinder the applicability of their findings in varied contexts.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning by proposing an innovative framework for personalized model training with privacy considerations. However, the numerous underlying assumptions and potential gaps in practical applicability warrant further scrutiny, particularly regarding the generalizability of the results.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel statistical framework for personalized federated learning, addressing the challenges posed by data heterogeneity among clients while ensuring privacy. The authors connect personalized federated learning to empirical Bayes methodology, proposing collaborative estimation methods and adaptive algorithms for training individualized models. Empirical results from diverse tasks, such as Bernoulli estimation and image classification, demonstrate the framework's advantages over existing approaches, showcasing improved prediction accuracy and privacy guarantees.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive approach to integrating personalized learning with privacy, filling a notable gap in the literature. The proposed hierarchical model and the introduction of new collaborative estimation methods are particularly noteworthy contributions. However, the paper could benefit from a more detailed comparison with existing methods, as some related works are only briefly mentioned without in-depth analysis. Additionally, while the empirical results are promising, further experiments on larger-scale datasets could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The writing is generally accessible, though some sections, particularly those involving theoretical aspects, may require deeper engagement for full comprehension. The novelty of the approach lies in its unique integration of statistical frameworks with federated learning, and the reproducibility is supported by detailed descriptions of the algorithms and experimental setups, though additional code or dataset availability would enhance it further.\n\n# Summary Of The Review\nOverall, this paper makes significant contributions to the field of personalized federated learning by introducing a robust statistical framework and demonstrating its effectiveness through empirical experiments. While the clarity and quality of the writing are commendable, some areas could benefit from deeper comparative analysis and broader empirical validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework aimed at addressing a critical issue in the domain of machine learning, specifically focusing on improving model interpretability and robustness. The authors introduce a hybrid model that combines decision trees and neural networks to leverage the strengths of both approaches. Through extensive experimentation on benchmark datasets, the findings indicate that the proposed method not only enhances interpretability but also achieves competitive performance compared to traditional state-of-the-art models.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The topic is highly relevant, as model interpretability is a growing concern in machine learning, particularly in sensitive applications.\n2. **Innovative Approach:** The hybridization of decision trees and neural networks presents a novel perspective that could bridge the gap between interpretability and predictive power.\n3. **Theoretical Justification:** The authors provide a solid theoretical background that underpins the proposed methodology, enhancing the credibility of their claims.\n4. **Clarity and Structure:** The paper is well-structured, allowing for clear communication of complex ideas, and includes sufficient background information for readers unfamiliar with the topic.\n\n**Weaknesses:**\n1. **Limited Experimental Scope:** While the experiments are promising, the paper could benefit from a broader range of datasets to validate the robustness of the proposed method across different contexts.\n2. **Comparative Analysis:** A more comprehensive comparison with existing interpretability-focused methods would clarify the advantages and limitations of the proposed approach.\n3. **Scalability Concerns:** The scalability of the hybrid model to larger datasets and real-world applications is not adequately addressed, which may raise concerns regarding practical usability.\n4. **Assumptions in Framework:** The assumptions underlying the proposed model may not hold in all scenarios, potentially limiting the applicability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically organized, facilitating reader comprehension. The methodology is described in sufficient detail to allow for reproducibility, though some additional clarity on hyperparameter tuning and implementation specifics could be beneficial. The novelty of combining decision trees with neural networks is significant, although the empirical results need further validation to establish their robustness.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of machine learning by proposing a hybrid model that enhances interpretability and performance. However, to strengthen its impact, the authors should address the limitations in experimental validation and provide a more thorough comparative analysis with existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a statistical framework for personalized federated learning (FL) that addresses the challenges posed by statistical heterogeneity in client data. The authors propose a unified approach to various personalization algorithms, grounded in empirical Bayes methodology, while ensuring privacy through new algorithms designed for personalized estimation and learning. They introduce AdaPeD, which leverages information geometry regularization, and provide numerical evaluations demonstrating the effectiveness of their methods in both estimation and learning tasks. The work contributes significantly to the theoretical understanding of personalized FL and sets the stage for future research on information-theoretic bounds and convergence properties.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive theoretical framework that connects existing personalization methods in FL and the introduction of novel algorithms that maintain user privacy. The empirical evaluations support the claims of improved performance and provide a strong foundation for the proposed methods. However, the paper could be strengthened by providing more extensive experimental results across diverse datasets to better validate the generalizability of the proposed approaches. Additionally, while the theoretical contributions are substantial, the practical implications in real-world scenarios could be better articulated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of writing is high, with appropriate use of terminology and sufficient detail for understanding the proposed framework and algorithms. The novelty of the work lies in its integration of various personalization methods into a coherent statistical framework, which has significant implications for the field of federated learning. Reproducibility is enhanced by the clarity of the algorithms presented, although the authors could improve this aspect by providing access to code or datasets used in the numerical evaluations.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of personalized federated learning by providing a robust statistical framework and introducing novel algorithms that prioritize user privacy. While the theoretical insights and empirical results are promising, further validation across different datasets and clearer articulation of practical implications would enhance the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a unified statistical framework for personalized federated learning and estimation, addressing the challenges posed by statistical heterogeneity across decentralized client data. The authors develop new algorithms, connecting them to empirical Bayes methodologies, and establish theoretical performance bounds and privacy guarantees for personalized learning. Empirical evaluations demonstrate that the proposed algorithms significantly outperform existing methods on datasets like MNIST and CIFAR-10, particularly in scenarios characterized by data heterogeneity.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its comprehensive approach to integrating theoretical insights with practical algorithm development in personalized federated learning. The introduction of a hierarchical model for local parameters and the focus on privacy constraints are particularly commendable. However, the paper could benefit from a more extensive discussion of the limitations of the proposed methods, particularly in terms of computational complexity and scalability in real-world applications. Additionally, while the empirical results are promising, further exploration of the performance under diverse scenarios and data distributions would enhance the contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The quality of the writing is high, making complex concepts accessible to readers. The novelty of the approach is significant, as it provides a fresh perspective on personalized federated learning by synthesizing theoretical foundations with practical algorithmic developments. Reproducibility is generally supported through detailed descriptions of the methods and experiments; however, the authors could enhance this aspect by providing more specifics on hyperparameter tuning and experimental setups in the supplementary materials.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of personalized federated learning by offering a robust theoretical framework and demonstrating effective algorithms that address privacy and performance challenges. While the findings are promising, further examination of the methods' limitations and broader applicability would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a statistical framework designed for personalized federated learning and estimation, addressing the challenges posed by statistical heterogeneity in federated settings. It introduces novel algorithms, notably AdaPeD and AdaMix, while incorporating privacy considerations into the personalized learning paradigm. Theoretical bounds for performance and privacy guarantees are established, and numerical evaluations on datasets such as MNIST, CIFAR-10, and FEMNIST demonstrate the proposed methods' advantages over existing algorithms in terms of mean squared error (MSE) for estimation and accuracy for learning tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its well-defined problem statement and the introduction of a comprehensive statistical framework that merges personalized learning with privacy concerns. The algorithms proposed are innovative, and the theoretical analysis offers valuable insights into their performance and privacy implications. However, the paper could benefit from a more thorough discussion of the limitations or assumptions inherent in the proposed framework, as understanding these aspects is critical for practical application. Additionally, while the empirical results are promising, further validation on more diverse datasets could enhance the findings' generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear headings and a logical flow that aids in understanding the complex concepts involved. Diagrams and figures effectively illustrate the methodology and results, contributing to the paper's overall clarity. The novelty of the approach is significant, particularly in the integration of privacy guarantees within personalized federated learning. Reproducibility is supported by detailed descriptions of the algorithms and experimental setups; however, providing more specific implementation details would further strengthen this aspect.\n\n# Summary Of The Review\nThis paper makes a notable contribution to the field of personalized federated learning by developing a statistical framework that effectively addresses privacy and heterogeneity challenges. While the methodology and empirical results are robust, a deeper exploration of the framework's limitations would enhance the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a unified statistical framework for personalized federated learning (FL) that addresses the challenges posed by statistical heterogeneity in client data. The authors propose a hierarchical model based on classical empirical Bayes methodologies, enabling personalized estimation and the introduction of novel algorithms, including AdaPeD, which leverages information geometry regularization. The findings demonstrate that the proposed methods significantly improve empirical performance in personalized estimation while maintaining privacy through robust guarantees.\n\n# Strength And Weaknesses\nStrengths of the paper include its comprehensive approach to integrating personalization and privacy in federated learning, as well as the introduction of theoretical performance bounds that support the proposed algorithms. The use of classical statistical methods adds a solid grounding to the framework. However, the paper could benefit from clearer explanations of some technical concepts, especially regarding the implications of the theorems presented. Additionally, while the empirical results show promise, further comparisons with a wider array of baseline methods could strengthen the claims regarding the superiority of the proposed algorithms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, though some sections could use more detailed explanations to ensure that a broader audience can fully grasp the implications of the theoretical contributions. The quality of the writing is solid, with a logical structure that guides the reader through complex concepts. In terms of novelty, the statistical framework and the AdaPeD algorithm present significant advancements in the field of personalized federated learning and privacy. The reproducibility of the results is supported by detailed appendices containing proofs and algorithmic implementations, although the availability of code or datasets would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper offers a substantial contribution to personalized federated learning by introducing a robust statistical framework and innovative algorithms that prioritize privacy. While the theoretical and empirical findings are compelling, certain areas could be clarified to improve accessibility and comprehensiveness. \n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper attempts to unify various personalized federated learning methods by proposing a statistical framework intended to enhance personalization in federated learning scenarios. However, the methodology presented is convoluted, lacking clarity and cohesion, which results in difficulty understanding its significance and practical applications. The performance evaluations are superficial and fail to convincingly demonstrate the proposed methods' superiority over existing approaches, leading to inflated claims regarding their effectiveness.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambition to address the challenges of personalized federated learning by proposing new algorithms, such as AdaPeD and AdaMix. However, this strength is significantly undermined by a lack of engagement with existing literature, leading to questions about the originality of the contributions. The performance evaluation methods are not rigorous, lacking statistical testing to support the claims made, which diminishes the reliability of the findings. Additionally, the vague implementation details regarding privacy guarantees further compromise the paper's practical applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is notably lacking; the convoluted framework makes it challenging for readers to grasp its implications fully. The quality of the work is diminished by an insufficient discussion of related works, which diminishes the perceived novelty of the proposed methods. Furthermore, the lack of detailed algorithm descriptions hinders reproducibility, as practitioners may struggle to replicate the results presented in the paper. Overall, the paper does not meet the standards expected in terms of clarity, quality, novelty, and reproducibility.\n\n# Summary Of The Review\nIn summary, while the paper attempts to contribute to the field of personalized federated learning, its convoluted framework, superficial evaluations, and lack of engagement with existing literature significantly undermine its impact. The paper does not adequately address the challenges of the domain, leading to a lack of confidence in the proposed methods and their practical applicability.\n\n# Correctness\nRating: 2/5\n\n# Technical Novelty And Significance\nRating: 2/5\n\n# Empirical Novelty And Significance\nRating: 2/5",
    "# Summary Of The Paper\nThe paper presents a novel statistical framework for personalized federated learning, aiming to enhance collaboration among clients with heterogeneous data. The authors introduce innovative algorithms, namely AdaPeD and AdaMix, which utilize information geometry and adaptive weighting to optimize personalized models. The findings demonstrate superior numerical performance over existing methods, along with robust privacy guarantees, making the framework applicable across various domains such as healthcare and finance.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its groundbreaking approach to merging theoretical foundations with practical applications, particularly through the introduction of new algorithms that outperform traditional methods. The integration of strong privacy protections is a significant contribution, addressing contemporary concerns about data confidentiality. However, the paper could benefit from a more extensive discussion on the limitations of the proposed methods and potential scenarios where they may underperform compared to existing approaches.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly presents its methodology and findings, contributing to its overall clarity. The quality of the experiments and the robustness of the theoretical connections to empirical Bayes are commendable. In terms of novelty, the proposed framework and algorithms are indeed innovative, but the reproducibility could be further enhanced by providing more detailed descriptions of the experimental setups and datasets used, as well as code availability.\n\n# Summary Of The Review\nOverall, the paper represents a significant advancement in personalized federated learning, combining innovative algorithms with strong theoretical foundations and privacy considerations. Its practical relevance across various domains underscores its importance for both researchers and practitioners in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a comprehensive theoretical framework for personalized federated learning (FL) that integrates classical empirical Bayes methodologies. It introduces a hierarchical statistical model that facilitates personalized estimation through collaboration among clients while ensuring privacy. The authors derive theoretical performance bounds that demonstrate the optimality of personalized estimators and propose algorithms like AdaPeD and AdaMix, emphasizing adaptive weighting mechanisms. Overall, the work establishes a strong theoretical foundation crucial for advancing personalized FL methodologies.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its rigorous theoretical insights and the establishment of performance bounds, which provide a solid basis for evaluating personalized models. The connection to classical statistical methods enhances the understanding of modern FL strategies. However, a notable weakness is the limited empirical evaluation of the proposed algorithms; while numerical results are provided, they primarily serve to illustrate theoretical claims rather than validate the practical effectiveness of the methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making complex theoretical concepts accessible. The quality of the writing and the presentation of mathematical arguments are commendable. The novelty of connecting personalized learning to classical empirical Bayes methods is significant, though the empirical validation could benefit from more robust experiments to enhance reproducibility and real-world applicability.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of personalized federated learning by establishing a strong theoretical foundation and proposing innovative algorithms. While the theoretical insights are compelling, the limited empirical evaluation raises questions about the practical impact of the proposed methods.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" presents a comprehensive approach to personalized federated learning (FL) that addresses statistical heterogeneity in client data. The authors propose a unified statistical framework that encompasses several novel algorithms: AdaPeD, which employs information-geometry regularization; AdaMix, which leverages mixture distributions for adaptive personalization; and DP-AdaPeD, a differentially private variant of AdaPeD. The experimental results, conducted on datasets such as MNIST, FEMNIST, CIFAR-10, and CIFAR-100, demonstrate that AdaPeD consistently outperforms other methods in terms of mean squared error (MSE) and test accuracy across various configurations.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the challenges posed by heterogeneous client data in federated learning. The proposed algorithms are well-defined and demonstrate significant improvements in performance metrics compared to existing methods. However, a notable weakness is the reliance on specific hyperparameter tuning for optimal performance, which may limit the generalizability of the results. Additionally, while the paper mentions reproducibility through code availability, more explicit details on the implementation nuances could enhance clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents its methodologies and findings. The quality of the experiments is satisfactory, with thorough documentation of settings and parameters. The novelty of the algorithms is commendable, especially in their approach to federated learning and privacy. However, the reproducibility aspect could be improved by providing more detailed guidance on hyperparameter selection and configuration for different datasets, as well as discussing the limitations of the custom implementations.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in personalized federated learning through the introduction of novel algorithms that effectively tackle statistical heterogeneity. While the contributions are valuable and the results promising, the paper would benefit from further clarity on hyperparameter tuning and implementation details to enhance reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper proposes a unifying statistical framework for personalized federated learning, introducing new algorithms such as AdaPeD and AdaMix, which are claimed to outperform existing methods. The authors emphasize their contributions in terms of privacy guarantees and algorithmic efficiency while linking their approach to classical empirical Bayes methodology. However, the paper does not sufficiently acknowledge prior works in the field, leading to questions about the novelty and significance of its contributions.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its theoretical insights and the introduction of new algorithms aimed at enhancing personalized federated learning. However, the paper exhibits weaknesses in its comparative analysis, often diminishing the contributions of existing methods without providing a balanced view of their strengths. The claims regarding privacy guarantees and algorithm efficiency lack adequate contextualization, which undermines the paper's credibility. Furthermore, the authors make strong assumptions about local data distributions that may not hold in practice, limiting the applicability of their findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but the clarity suffers from a lack of acknowledgment of related work, which could confuse readers regarding the true novelty of the proposed methods. The quality of the numerical evaluations presented is good, yet they do not provide a comprehensive comparison with other methods, impacting reproducibility. The novelty of the algorithms is presented as significant, but the connections to existing methodologies, such as knowledge distillation, are inadequately explored.\n\n# Summary Of The Review\nWhile the paper offers interesting insights and methodologies for personalized federated learning, it often overlooks the contributions of previous research, which could diminish its overall impact. A more balanced approach in comparisons and a critical engagement with prior work would enhance the paper's credibility and clarity.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a statistical framework for personalized federated learning and estimation, focusing on the interplay between theory, algorithms, and privacy concerns. The authors develop novel algorithms that leverage statistical properties of client data to improve personalized model performance while ensuring data privacy. Empirical evaluations demonstrate significant improvements in model accuracy compared to baseline methods, highlighting the effectiveness of the proposed framework in heterogeneous data environments.\n\n# Strength And Weaknesses\nThe strengths of the paper include its comprehensive theoretical foundation and practical algorithmic contributions that address key challenges in personalized federated learning. The empirical results are robust, showcasing a clear advantage of the proposed methods over existing approaches. However, the paper has notable weaknesses in clarity and presentation; several sections are convoluted, and inconsistencies in notation and formatting detract from the overall readability. Additionally, while the theoretical claims are sound, the practical implementation details lack sufficient depth for reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by complex sentence structures and inconsistent terminology, making it difficult for the reader to grasp the core concepts easily. The quality of the mathematical notation and the presentation of results could also be improved with uniform formatting and clearer explanations. In terms of novelty, the approach is innovative and addresses significant gaps in the literature. However, the reproducibility of the results is questionable due to insufficient methodological detail in the experimentation section.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of personalized federated learning with its robust theoretical and empirical findings. Nonetheless, clarity and presentation issues need to be addressed to enhance the accessibility and reproducibility of the research.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to personalized federated learning, focusing on the development of algorithms that cater to the statistical heterogeneity of local data across clients. It proposes a framework that emphasizes the importance of personalization while maintaining privacy guarantees. The methodology includes theoretical derivations of bounds for the proposed algorithms, and empirical evaluations demonstrate their effectiveness under specific conditions. However, the study's findings would benefit from assessments in more challenging scenarios, such as noisy data and extreme heterogeneity.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous approach to addressing the challenges of personalized federated learning and the provision of privacy guarantees. The theoretical contributions offer valuable insights into the algorithm's behavior under varying conditions. However, the paper could be enhanced by exploring the effects of extreme data heterogeneity, the implications of client dropout, and the integration of multi-modal data. Furthermore, while the empirical evaluations are informative, they are limited in scope and could benefit from a broader array of datasets and models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of ideas and methodologies. The quality of the theoretical analysis is high, though the limitations of the derived bounds could be discussed further. In terms of novelty, the paper introduces significant advancements in personalized federated learning; however, it could better contextualize its contributions against existing works. Reproducibility is a concern, as the limited range of experiments may hinder external validation of the findings.\n\n# Summary Of The Review\nOverall, the paper provides a solid contribution to the field of personalized federated learning, with promising algorithms and theoretical insights. Yet, to fully realize its potential impact, the work should address the limitations identified, broaden its empirical evaluations, and explore additional dimensions such as scalability and ethical implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a comprehensive statistical framework for personalized federated learning (FL), addressing the challenges posed by statistical heterogeneity in client data. It unifies various personalization methods through empirical Bayes methodology, proposing novel algorithms for private personalized estimation. Theoretical performance and privacy guarantees are established, demonstrating that collaborative estimation can significantly enhance learning outcomes in federated settings. Empirical evaluations across multiple datasets (MNIST, FEMNIST, CIFAR-10) underscore the proposed algorithms' effectiveness in balancing performance and privacy.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its rigorous theoretical foundation, which connects personalized FL to empirical Bayes methods, and the development of algorithms that provide strong privacy guarantees while improving estimation accuracy. The empirical results are compelling and demonstrate clear advantages over local methods, highlighting the practical applicability of the proposed algorithms. A potential weakness lies in the complexity of the statistical framework, which may pose challenges for practitioners seeking to implement these methods without a strong statistical background. Additionally, while the paper presents a robust theoretical analysis, further exploration of statistical lower bounds and more extensive empirical comparisons could strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers with a background in both statistics and machine learning. The quality of the theoretical results is high, supported by appropriate statistical literature. The novelty of the approach lies in its integration of classical statistical methodologies with modern federated learning challenges. However, the reproducibility of the results may be hindered by the complexity of the algorithms and the specific conditions under which the empirical evaluations were conducted, as detailed implementation guidelines are not fully provided.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the understanding and implementation of personalized federated learning through a rigorous statistical framework. While it demonstrates strong theoretical and empirical contributions, further clarity on implementation and additional empirical validation would enhance its applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a unifying statistical framework for personalized federated learning, aiming to address the challenges associated with data heterogeneity and privacy. The authors propose a set of algorithms that leverage specific statistical models, such as Gaussian and Bernoulli distributions, to enhance personalization in federated learning settings. Findings indicate that the proposed methods outperform baseline approaches on the MNIST and FEMNIST datasets, demonstrating improved model performance while maintaining a focus on privacy.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its theoretical foundation, providing a robust framework for personalized federated learning that integrates statistical principles. However, it has notable weaknesses, including a limited exploration of existing personalization techniques, which hampers the contextualization of its contributions. The empirical validation is restricted to a narrow set of datasets, raising concerns about generalizability. Furthermore, the reliance on specific data distribution assumptions and the insufficient evaluation of privacy guarantees limit the practical applicability of the proposed methods. Additionally, scalability, hyperparameter optimization, and practical deployment challenges remain inadequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although some concepts could benefit from further elaboration to enhance clarity. The novelty of the approach is moderate, as it combines existing ideas within a new framework rather than introducing fundamentally new techniques. The reproducibility of the results may be hindered by the lack of comprehensive details regarding the experimental setup and the impact of hyperparameters. Furthermore, the absence of qualitative evaluations or user feedback limits the assessment of practical effectiveness.\n\n# Summary Of The Review\nOverall, the paper offers a theoretically sound framework for personalized federated learning but falls short in empirical validation and practical considerations. While the contributions are valuable, the limitations in dataset diversity, privacy evaluations, and scalability suggest that further work is needed to enhance the framework's applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy\" aims to unify various personalization methods within the realm of federated learning, which is a well-established area in machine learning. The authors propose a statistical framework that connects to empirical Bayes and outlines methods for private personalized estimation of local parameters. The paper presents theoretical bounds and guarantees, along with numerical evaluations that claim superiority over existing algorithms in federated learning contexts.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its attempt to create a unified theoretical framework for personalized federated learning, which could be beneficial for future research in this area. However, the contributions seem to lack originality, as many concepts discussed are well-established, and the novelty of the proposed methods is questionable. The paper heavily relies on existing literature without offering significant new insights or advancements. Furthermore, the claims of numerical superiority need to be substantiated with more robust experimental validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by its heavy reliance on jargon and complex mathematical formulations that may not be accessible to all readers. While the quality of the writing is adequate, the perceived lack of novelty detracts from its overall impact. Reproducibility may be challenging, as the experimental setup and parameters are not described in sufficient detail, making it difficult for others to replicate the findings.\n\n# Summary Of The Review\nOverall, the paper offers a theoretically sound framework for personalized federated learning, yet it lacks significant novelty and empirical validation. While the authors have made an effort to connect established methodologies, the contributions do not break new ground in the field. The paper would benefit from clearer communication of its findings and a more thorough exploration of its empirical results.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents a unified statistical framework for personalized federated learning, introducing novel algorithms such as AdaPeD and AdaMix to enhance personalization across heterogeneous data environments. The authors emphasize the connection between personalized learning and empirical Bayes methods, highlighting the importance of user-level privacy. Through numerical evaluations using both synthetic and real-world data, the paper demonstrates the efficacy of the proposed methods while suggesting areas for future exploration, including adaptive mechanisms, alternative loss functions, and multi-task learning.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to personalized federated learning through the introduction of new algorithms and a solid statistical framework. The connection to empirical Bayes methods adds depth to the discussion, and the focus on user privacy is commendable. However, the evaluation could benefit from a broader range of datasets to test the methods' scalability and adaptability, particularly in non-IID scenarios. Additionally, the paper could explore alternative privacy-preserving techniques and loss functions to further enhance its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making it accessible to readers with varying backgrounds. The quality of the experiments and the robustness of the experimental design are notable, though incorporating cross-validation techniques could improve the reliability of the findings. The novelty of the proposed algorithms is significant, but further exploration of connections to multi-task learning and alternative loss functions could bolster its impact. Reproducibility is generally supported through detailed descriptions of the methods, although additional empirical evaluations on diverse datasets are warranted.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of personalized federated learning by introducing innovative algorithms and a comprehensive statistical framework. While the work is solid and well-presented, there are opportunities for further exploration that could enhance its significance and applicability in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents two novel methods, AdaPeD and DP-AdaPeD, aimed at enhancing personalized federated learning. The authors conduct extensive benchmarking across multiple datasets, including MNIST, FEMNIST, CIFAR-10, and CIFAR-100, demonstrating that AdaPeD outperforms existing baseline methods like FedAvg and pFedMe in terms of accuracy and mean squared error (MSE). Notably, DP-AdaPeD maintains competitive performance even under varying privacy constraints, indicating a significant trade-off between privacy and accuracy. The findings also suggest that increasing the number of clients leads to improved performance and convergence rates, affirming the efficacy of the proposed methods in practical applications.\n\n# Strength And Weaknesses\nThe strengths of the paper include its comprehensive benchmarking across diverse datasets, which effectively showcases the proposed methods' advantages over established baselines. The empirical results are robust, particularly the performance gains observed with AdaPeD and DP-AdaPeD. The paper also provides valuable insights into the trade-offs between privacy and accuracy, which is a critical concern in federated learning. However, a potential weakness lies in the limited exploration of the theoretical underpinnings of the algorithms, which could provide a deeper understanding of their performance across different scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that facilitates understanding of the methodology and findings. The quality of the experiments is high, with well-defined metrics and thorough analyses. While the proposed methods are innovative, further justification of their novelty in terms of theoretical contributions could enhance the paper's impact. The reproducibility of the findings is promising, as the authors provide sufficient details about the experimental setup, though additional emphasis on code availability or detailed algorithms could further strengthen this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of personalized federated learning by introducing effective methods that improve upon existing benchmarks. The experimental results are compelling, showcasing substantial performance enhancements while addressing privacy concerns. However, a deeper theoretical analysis could bolster its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel framework for enhancing personalized models through an empirical Bayes methodology integrated with information-geometry regularization. The authors propose a systematic approach to optimize the performance of these models by addressing the challenges of overfitting and generalization. Key findings indicate that their method outperforms existing techniques on benchmark datasets, demonstrating improved accuracy and robustness across various scenarios.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to combining empirical Bayes with information-geometry regularization, which is not commonly explored in the literature. The empirical results are compelling, showing significant improvements over baseline models. However, the paper exhibits weaknesses in clarity and structure, with dense technical jargon that may impede understanding. Additionally, the methodology section lacks detailed explanations that could aid reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by complex sentence structures and the use of undefined technical terms early in the text. While the novelty of the proposed methodology is noteworthy, the quality suffers from a lack of clear transitions and signposting between sections. The reproducibility of the results is questionable due to insufficient detail in the methodology and results sections, which may hinder other researchers from replicating the study.\n\n# Summary Of The Review\nOverall, the paper introduces a promising methodology for personalized modeling, but it requires significant improvements in clarity and structure to enhance accessibility and understanding. The findings are impactful, yet the presentation undermines the potential significance of the contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.860462611338239,
    -1.652800850515479,
    -1.8749055025354484,
    -1.7660767784398768,
    -1.883731341816142,
    -1.7197362996444472,
    -1.5730394274175505,
    -1.908389353986789,
    -1.6639113037487894,
    -1.9020974800419075,
    -1.7381829075888562,
    -1.3833051684859283,
    -1.6914883610089289,
    -1.6271574157082862,
    -1.6650906066038673,
    -1.7948548735398056,
    -1.6707595799993669,
    -1.955929304181675,
    -1.9261243501715675,
    -1.6381209611113545,
    -1.9102393847612311,
    -1.668776766120888,
    -1.6887517588178806,
    -1.6886629521545304,
    -1.6020301864297268,
    -1.7418662149648003,
    -1.7532274202482152,
    -1.517341530238462,
    -1.801163369774051
  ],
  "logp_cond": [
    [
      0.0,
      -2.732638746840973,
      -2.717682558601503,
      -2.7277008523254045,
      -2.7359436984609746,
      -2.7308411741602043,
      -2.766173745633629,
      -2.737067975825671,
      -2.7265906176590766,
      -2.7302374297921124,
      -2.7279890090155985,
      -2.7674323648154497,
      -2.722456853875668,
      -2.726134298089724,
      -2.7129618759887815,
      -2.7333084545511963,
      -2.738372086056108,
      -2.734667636456662,
      -2.723696455978436,
      -2.7127584570530003,
      -2.7260370745101943,
      -2.732400660426022,
      -2.733193599365305,
      -2.7198980681959513,
      -2.725714947934821,
      -2.738806214611435,
      -2.729914810183562,
      -2.7449759466950736,
      -2.7623680601035914
    ],
    [
      -1.4079634381441517,
      0.0,
      -1.2193919159204705,
      -1.186716107895414,
      -1.2721422840173333,
      -1.2426668281232394,
      -1.3341591969356374,
      -1.2515705943566922,
      -1.245715834344954,
      -1.3023405395533396,
      -1.324019347039603,
      -1.3852883444246626,
      -1.1942129228247205,
      -1.2716308605497852,
      -1.239699015341941,
      -1.1876263054994187,
      -1.3459952404113036,
      -1.2855994303448948,
      -1.3082009308639684,
      -1.1403838423578643,
      -1.2676274615315202,
      -1.2912982084948277,
      -1.344153378329777,
      -1.256349897226145,
      -1.3323914609524319,
      -1.2368165062481835,
      -1.2880994659064937,
      -1.274294120724346,
      -1.3395398297835885
    ],
    [
      -1.4971326098272966,
      -1.3364047710692128,
      0.0,
      -1.3889523308780347,
      -1.3701284120647137,
      -1.4470237489386197,
      -1.5050967582456092,
      -1.3974894308266725,
      -1.4144105816054566,
      -1.417350998556422,
      -1.403859703597047,
      -1.5336841546514954,
      -1.365932740225244,
      -1.4220867023691997,
      -1.3901907962875273,
      -1.3898256831274656,
      -1.4682232510322304,
      -1.3805233149221559,
      -1.4375042495346424,
      -1.3414971153450004,
      -1.4117278994402365,
      -1.4734375162672118,
      -1.4973676611985336,
      -1.4373283512151118,
      -1.4131409084367927,
      -1.4094212302853486,
      -1.3863415915981177,
      -1.43184623097179,
      -1.5447835090811262
    ],
    [
      -1.3998012899467116,
      -1.2059461773536138,
      -1.3268664972276585,
      0.0,
      -1.354582552957774,
      -1.2802357533348587,
      -1.4024794507132872,
      -1.329873192829876,
      -1.3050537596660456,
      -1.3287700972073715,
      -1.282882303658161,
      -1.414992066750692,
      -1.194890807895355,
      -1.2611082587686377,
      -1.2427453391955832,
      -1.2291365273080137,
      -1.3996045856779364,
      -1.356215692406358,
      -1.2981266230540123,
      -1.2779548089966442,
      -1.3379162050496096,
      -1.2998625833700181,
      -1.3748039152474034,
      -1.265581328236917,
      -1.3711383437565325,
      -1.2806931700075328,
      -1.3103988552464203,
      -1.3505514052992589,
      -1.3904464321392358
    ],
    [
      -1.5839854857933042,
      -1.5785440324962856,
      -1.478588364310577,
      -1.551756529637542,
      0.0,
      -1.5491947508294108,
      -1.5544377425529101,
      -1.4814173143345706,
      -1.4454323365961212,
      -1.483067347811114,
      -1.5988640000873462,
      -1.5779920935366374,
      -1.484915642999542,
      -1.5855945938013773,
      -1.5118466021528187,
      -1.473060138067769,
      -1.492902978119304,
      -1.4723305833326983,
      -1.5059233774310539,
      -1.4211200151813814,
      -1.47010385699607,
      -1.5418788398349494,
      -1.5419731905245013,
      -1.5647556343769167,
      -1.5153457598936138,
      -1.4693195205697067,
      -1.4954853183858836,
      -1.5875733610508518,
      -1.5712766110201453
    ],
    [
      -1.4307452465425348,
      -1.1921587995880523,
      -1.2758606587203158,
      -1.1863447169418788,
      -1.301610477204265,
      0.0,
      -1.3689548453346896,
      -1.3017282674895556,
      -1.2815699322556329,
      -1.292508575473011,
      -1.3354709405590732,
      -1.408820718098837,
      -1.2321296942422495,
      -1.2877370273992381,
      -1.231929956922486,
      -1.239525980950002,
      -1.3310075191570214,
      -1.2743849253184851,
      -1.3176795738513558,
      -1.20167983848618,
      -1.2626382727599934,
      -1.2677149333879123,
      -1.3583970602841293,
      -1.286496395134571,
      -1.3419532713660662,
      -1.2652047107065025,
      -1.2716569067592027,
      -1.320157984077895,
      -1.377288102587301
    ],
    [
      -1.3559133812021191,
      -1.2414641350348574,
      -1.243595700171942,
      -1.2566802960550034,
      -1.2410233607021965,
      -1.2512555055888366,
      0.0,
      -1.247514042518078,
      -1.2618672704193372,
      -1.2842406625678169,
      -1.2712475025646894,
      -1.294978559169203,
      -1.2902567043281692,
      -1.287102755908633,
      -1.274872131525057,
      -1.2434543544446295,
      -1.2919570018634143,
      -1.3161380531621925,
      -1.28415263824747,
      -1.2117725354716273,
      -1.269609112047718,
      -1.251907806225399,
      -1.2690385845320586,
      -1.2758815650054667,
      -1.2513853012007312,
      -1.2248139776205482,
      -1.2868107797466535,
      -1.2866353409738769,
      -1.3149423662841182
    ],
    [
      -1.5983431319241592,
      -1.5115503204456933,
      -1.4665630788144408,
      -1.473920933037698,
      -1.5143736343284226,
      -1.5061003190848052,
      -1.5996332962679378,
      0.0,
      -1.4643414854349646,
      -1.500006394124992,
      -1.4791620371461696,
      -1.5641848059178707,
      -1.446189203513399,
      -1.5067268164844383,
      -1.4505396270668798,
      -1.4833137912849697,
      -1.5251674838653533,
      -1.4663213471073868,
      -1.4567566938913203,
      -1.483882702135856,
      -1.420798334818211,
      -1.5491089772747142,
      -1.6135633415491264,
      -1.4897622005256674,
      -1.5158682815301876,
      -1.542856876884808,
      -1.392565483450347,
      -1.5657382929520436,
      -1.572828198089315
    ],
    [
      -1.3281674735382967,
      -1.243877674701162,
      -1.2156048797051096,
      -1.2100463731000919,
      -1.2006588009431218,
      -1.2530866446749345,
      -1.321376507779456,
      -1.1487791441000699,
      0.0,
      -1.2683995961673407,
      -1.1991102604370998,
      -1.3405033162172613,
      -1.1466544478829819,
      -1.2018112488125308,
      -1.1654255182782198,
      -1.2126203712700259,
      -1.2938315181294848,
      -1.282785319653409,
      -1.3078713312820152,
      -1.1901588262017673,
      -1.2382167124026888,
      -1.3033935471794091,
      -1.3236420503727635,
      -1.2404351567977765,
      -1.2938707112948795,
      -1.2679809879205481,
      -1.252387542863741,
      -1.2820662976892498,
      -1.2793616802503998
    ],
    [
      -1.5622987887112045,
      -1.525554084971772,
      -1.559281229725748,
      -1.5209948386470102,
      -1.526228959832957,
      -1.5551711229943403,
      -1.640748120916343,
      -1.470391978971691,
      -1.523372336180931,
      0.0,
      -1.5528442964941833,
      -1.597519238322033,
      -1.5212120964573346,
      -1.5411272299526884,
      -1.5203627886227187,
      -1.5490195446742705,
      -1.5450104924629382,
      -1.5281681350432805,
      -1.511706508347351,
      -1.574243141974674,
      -1.449520823235477,
      -1.606155890689599,
      -1.6050935528940369,
      -1.556897512063755,
      -1.5821041797146582,
      -1.559780067296382,
      -1.5253456861584482,
      -1.5507411349391131,
      -1.5693195780407978
    ],
    [
      -1.3747402083085154,
      -1.2880670246719819,
      -1.285137189483231,
      -1.3187776751809632,
      -1.3967110105019673,
      -1.3203731195346042,
      -1.3677856566216273,
      -1.2984040264775094,
      -1.2952551458307056,
      -1.3155223091788708,
      0.0,
      -1.422666300934929,
      -1.277712866975313,
      -1.3032750355624383,
      -1.295741114615076,
      -1.254005281624351,
      -1.347197116693386,
      -1.3184462847198641,
      -1.3272065122280148,
      -1.3363633696432826,
      -1.300542671053543,
      -1.3246908184968182,
      -1.3931220943346008,
      -1.2808673009419398,
      -1.3314944419206176,
      -1.3179821351513707,
      -1.3495151683874995,
      -1.3581966700876462,
      -1.3658288695491445
    ],
    [
      -1.1639255211711947,
      -1.1527954375256022,
      -1.141045421381499,
      -1.1390563228929063,
      -1.1201763252542662,
      -1.1443815445261478,
      -1.1640717134955527,
      -1.1212839503474537,
      -1.1408654432586685,
      -1.09752987988933,
      -1.1224821457797205,
      0.0,
      -1.137850370092484,
      -1.1402682440254344,
      -1.1363681075263847,
      -1.1448170743187673,
      -1.1624355456432653,
      -1.1491525532441091,
      -1.149247704854004,
      -1.1575941398381588,
      -1.1075820125611104,
      -1.135567557904234,
      -1.13978376872029,
      -1.14185007645802,
      -1.098817648199867,
      -1.145872345665712,
      -1.1250188127497045,
      -1.1282776711365579,
      -1.1239686901479347
    ],
    [
      -1.374575169978071,
      -1.2956127350915192,
      -1.305510738061238,
      -1.2453001035890143,
      -1.308448848239134,
      -1.2972547163758867,
      -1.4094442112351717,
      -1.2844632550968316,
      -1.2775038079279526,
      -1.3148848801803723,
      -1.3219442971994102,
      -1.4089465502760117,
      0.0,
      -1.288250561961295,
      -1.2973518484821436,
      -1.185163924162404,
      -1.3733928362623458,
      -1.3654457851782977,
      -1.3626895024444987,
      -1.240943329606448,
      -1.296891918813387,
      -1.3954836536938917,
      -1.3833207376886174,
      -1.2780612830388143,
      -1.3233100693294664,
      -1.283652420447615,
      -1.3413506995028595,
      -1.3952535766877228,
      -1.3940426846327014
    ],
    [
      -1.2809650024508332,
      -1.1503194386275062,
      -1.222629939778264,
      -1.1368046705019854,
      -1.2631687485681184,
      -1.164860849660366,
      -1.2556533493028612,
      -1.2234639681580997,
      -1.1795412592645802,
      -1.1761993880632065,
      -1.1533382631121911,
      -1.3021333973863,
      -1.1928627268365004,
      0.0,
      -1.1185131241647501,
      -1.1893801798616985,
      -1.2987129107808482,
      -1.277780034782604,
      -1.2158067670163357,
      -1.207347255688752,
      -1.2177402734393636,
      -1.2425264306362704,
      -1.2324925837473915,
      -1.157961560061323,
      -1.2072651912381365,
      -1.1535955700348435,
      -1.2335525140839978,
      -1.2638383266784814,
      -1.2691827914257237
    ],
    [
      -1.3723774787195888,
      -1.2604769282907364,
      -1.2886991489363857,
      -1.2279909085374439,
      -1.2777967498048615,
      -1.2620245313591876,
      -1.3180050855291447,
      -1.2411863385766675,
      -1.1839995478485772,
      -1.265806991213732,
      -1.2458959245910326,
      -1.3646362508994223,
      -1.2622144750041775,
      -1.230659462749428,
      0.0,
      -1.22763379948131,
      -1.295610939111434,
      -1.3058629205059737,
      -1.2942956668543826,
      -1.2387518611459878,
      -1.2320586420671678,
      -1.2717579832880854,
      -1.3443224042607025,
      -1.2695901110450973,
      -1.2828000094018521,
      -1.2629435207661168,
      -1.246739722450195,
      -1.2981477291614487,
      -1.323031203302436
    ],
    [
      -1.4862500701492203,
      -1.329388658533105,
      -1.3678196314295354,
      -1.3595270873833525,
      -1.3245258362775028,
      -1.3692790342613366,
      -1.423270023691163,
      -1.4104272280140275,
      -1.3332332166089862,
      -1.4011842508657624,
      -1.4051986196817357,
      -1.4975155935245719,
      -1.2509286533830937,
      -1.3908084910134484,
      -1.365703145123334,
      0.0,
      -1.4061679452568772,
      -1.4025628055431485,
      -1.3602095259209843,
      -1.289617018380658,
      -1.3847407971744137,
      -1.3856324775206725,
      -1.4322507935787974,
      -1.387260504247741,
      -1.4698175160494922,
      -1.3478743853530526,
      -1.4093575224588606,
      -1.4746183132592343,
      -1.4663672048521978
    ],
    [
      -1.4068141704967996,
      -1.3788284966998887,
      -1.3078013657137761,
      -1.3541601851903213,
      -1.254028474988082,
      -1.3822439674702283,
      -1.3705327964994263,
      -1.3024839156885832,
      -1.3272491763686594,
      -1.3364607194079483,
      -1.3832465770381344,
      -1.4584952531737263,
      -1.339416761609104,
      -1.4018820413481754,
      -1.306986862709127,
      -1.334152783652678,
      0.0,
      -1.2984922628476847,
      -1.3321639378598207,
      -1.2913143991095712,
      -1.2606090831219097,
      -1.386043294412332,
      -1.3736537836716323,
      -1.3528018662914796,
      -1.3521649210826716,
      -1.3301116264614319,
      -1.269440369700807,
      -1.3766465010444682,
      -1.4506386022757558
    ],
    [
      -1.584943313814551,
      -1.4488100103222348,
      -1.4331583519375568,
      -1.4332080206721955,
      -1.316948324452116,
      -1.395668813211026,
      -1.5710105755469783,
      -1.3562087836017973,
      -1.4457673673886513,
      -1.4433126037564952,
      -1.3839957991262133,
      -1.569104684158447,
      -1.4137454397179938,
      -1.4568317140723215,
      -1.4376624670367506,
      -1.3982238571095793,
      -1.498372222856973,
      0.0,
      -1.3568836169129614,
      -1.4501927928718121,
      -1.3056634195816834,
      -1.4744818050394846,
      -1.5413736975328365,
      -1.397043822015519,
      -1.516042577234959,
      -1.4438011049816692,
      -1.3164562598357612,
      -1.5327755237630698,
      -1.5256209496094897
    ],
    [
      -1.457242764338719,
      -1.4498582545908598,
      -1.3884244634111955,
      -1.4158907646955745,
      -1.38658856725119,
      -1.4521590370379833,
      -1.4673473553529028,
      -1.3505586627845583,
      -1.4531987603549135,
      -1.3769745026584685,
      -1.4097737488920197,
      -1.509755022466022,
      -1.4060761531880404,
      -1.408128979783779,
      -1.3931268281676625,
      -1.3828855685717893,
      -1.4258404467509929,
      -1.3952677425383613,
      0.0,
      -1.414232270385652,
      -1.34583639349668,
      -1.4608856036564295,
      -1.507088397812304,
      -1.4406882670268624,
      -1.4814457288155163,
      -1.3810798017346606,
      -1.3469976346128312,
      -1.481893324565601,
      -1.4878150600543518
    ],
    [
      -1.3859713972477996,
      -1.1986434098329144,
      -1.2510065549086486,
      -1.1824023209265924,
      -1.2048212352052305,
      -1.2080322753205328,
      -1.3027725172329194,
      -1.27948684235371,
      -1.234618859417264,
      -1.307920868172447,
      -1.3443812413163292,
      -1.3863212307111816,
      -1.2222862850174316,
      -1.2720276635222456,
      -1.2223216539251447,
      -1.2151069846578681,
      -1.2675289829958942,
      -1.2667835418382265,
      -1.2980784774154144,
      0.0,
      -1.2827894381161293,
      -1.2696052250208258,
      -1.2910078594924428,
      -1.2815043795575782,
      -1.333217661963656,
      -1.2425728258671616,
      -1.2408963141279061,
      -1.2607361915174184,
      -1.3372572290100653
    ],
    [
      -1.5845873452016561,
      -1.496402128674568,
      -1.4493442075298928,
      -1.488317887067283,
      -1.44896249143814,
      -1.4879902015847217,
      -1.581566627364257,
      -1.3904785766754626,
      -1.4663269926091798,
      -1.467078875526588,
      -1.5400468506704825,
      -1.5728275528111677,
      -1.4675785419725436,
      -1.4943567760917718,
      -1.4602647139739302,
      -1.516678512556619,
      -1.49534885043997,
      -1.4645260389266321,
      -1.4559832903928447,
      -1.5280709572245976,
      0.0,
      -1.5889305459228265,
      -1.5600647407617274,
      -1.493678768825538,
      -1.5365029132708745,
      -1.5436598524221226,
      -1.384627483286798,
      -1.5628867850699166,
      -1.5783123442061586
    ],
    [
      -1.3146618468542581,
      -1.286354868076549,
      -1.3436909343478989,
      -1.2490688107635055,
      -1.3034505368417395,
      -1.2696502369790175,
      -1.262922723430705,
      -1.3415525643806276,
      -1.3136045131766418,
      -1.3454354920359834,
      -1.302931842955549,
      -1.3376546735865633,
      -1.3402902213592875,
      -1.272371978944367,
      -1.258105959920593,
      -1.2510896906719546,
      -1.3236188823921042,
      -1.318005022696205,
      -1.3134073617389588,
      -1.1996085462848147,
      -1.3228705443254416,
      0.0,
      -1.2605751723148932,
      -1.3056991939437173,
      -1.3172165172585737,
      -1.2173248996723742,
      -1.3338142355982,
      -1.3381738265570953,
      -1.2994124830087528
    ],
    [
      -1.4279440210299468,
      -1.4107499517435742,
      -1.3867073154190601,
      -1.404076265349901,
      -1.3494923114008326,
      -1.3928650602818677,
      -1.411546922191631,
      -1.381416901013141,
      -1.3833801665879146,
      -1.3456656476973206,
      -1.4219554750019876,
      -1.4474236607746354,
      -1.398672570998201,
      -1.407003088432043,
      -1.3689767826909118,
      -1.3564007246303178,
      -1.3764648492160756,
      -1.3859060629394255,
      -1.4463701688999688,
      -1.360990032183907,
      -1.3368280953093246,
      -1.3615204783519792,
      0.0,
      -1.4208277277936754,
      -1.3793156115370375,
      -1.3520058144776728,
      -1.3207309801470863,
      -1.4122943101026195,
      -1.4440407918275961
    ],
    [
      -1.3537016146573013,
      -1.240887760612254,
      -1.279715473079881,
      -1.2583402184014163,
      -1.340921234713216,
      -1.2660240909403235,
      -1.3607686332512885,
      -1.2772046067050271,
      -1.3006268760974724,
      -1.3121732847721599,
      -1.2450715146572504,
      -1.3722045662654816,
      -1.2162677056324407,
      -1.2527642348053054,
      -1.2759829405751832,
      -1.253073854708484,
      -1.338416663660148,
      -1.3740378772718242,
      -1.2650647337850083,
      -1.2625761650062133,
      -1.2837469742262106,
      -1.3009891556131963,
      -1.3513929383377346,
      0.0,
      -1.296974177682058,
      -1.2585165559832157,
      -1.284895580941311,
      -1.32333369542712,
      -1.3629398420568677
    ],
    [
      -1.3271070011811281,
      -1.3219965006964203,
      -1.2336961290767336,
      -1.3241716394727803,
      -1.2570014996325996,
      -1.315798463008171,
      -1.2894613565806714,
      -1.277852659524678,
      -1.2962819126045593,
      -1.2605416820315074,
      -1.2996237700906745,
      -1.297386770861141,
      -1.3005457109031993,
      -1.2747147291677166,
      -1.3024213128870394,
      -1.3187334475266919,
      -1.2532493614556681,
      -1.323334348991074,
      -1.311012470226646,
      -1.2713660819909973,
      -1.2467450876352437,
      -1.3421174346952316,
      -1.3077281692627354,
      -1.2947826467339825,
      0.0,
      -1.282176580950139,
      -1.246061275439582,
      -1.3044030641784077,
      -1.3206730366882344
    ],
    [
      -1.4508896764216395,
      -1.30738898397405,
      -1.413648603157393,
      -1.3247057738323211,
      -1.3545160781522447,
      -1.3129411461899627,
      -1.3647352763959282,
      -1.3694799802483848,
      -1.3974006595805917,
      -1.4045832705858745,
      -1.394206534605167,
      -1.4704202340416115,
      -1.374213042087748,
      -1.3299304197498225,
      -1.3602715202030804,
      -1.375697579073118,
      -1.3801892659592747,
      -1.39681440783276,
      -1.418891906652235,
      -1.307231414319317,
      -1.3878655725515383,
      -1.3431929565379228,
      -1.3880768792098592,
      -1.350889009681756,
      -1.3932149198440928,
      0.0,
      -1.3652374135427925,
      -1.4387700764496494,
      -1.4211026925072974
    ],
    [
      -1.4307831761023861,
      -1.4106111596948727,
      -1.3167803079278626,
      -1.3759559586295926,
      -1.29883389379248,
      -1.3991731485404664,
      -1.4357656700167498,
      -1.2793026279959958,
      -1.3705079235882593,
      -1.36307569318722,
      -1.4104532674035932,
      -1.4230108355550835,
      -1.3724289489405859,
      -1.3786709510267146,
      -1.3511063659875173,
      -1.4217944867398182,
      -1.3524256677351512,
      -1.343348398717989,
      -1.3502118173609297,
      -1.3782957272792318,
      -1.2401914395537592,
      -1.451798127096569,
      -1.3896981643863542,
      -1.4207458881735584,
      -1.361379009543956,
      -1.400044587236096,
      0.0,
      -1.437498142113188,
      -1.459416872418352
    ],
    [
      -1.2699019346194644,
      -1.200368158025468,
      -1.174618647231333,
      -1.2091521881390754,
      -1.2048555497678135,
      -1.2059007419276768,
      -1.274150499908497,
      -1.2071752664460136,
      -1.2057495632709594,
      -1.1914165597845008,
      -1.210095486802112,
      -1.2475111761759532,
      -1.214240772762553,
      -1.2416785199838898,
      -1.1995335690038247,
      -1.229083360800571,
      -1.232674525217244,
      -1.1901719745582577,
      -1.2289532063323811,
      -1.215388963471662,
      -1.2089394698969231,
      -1.2325081226958916,
      -1.2542438956438096,
      -1.2095545903756204,
      -1.2124952546381282,
      -1.2184140291103005,
      -1.1905237734064338,
      0.0,
      -1.2572688600972053
    ],
    [
      -1.4424393839015088,
      -1.3549760025015685,
      -1.376088007251889,
      -1.3731475096104229,
      -1.34226698977334,
      -1.4018734583780916,
      -1.4185833392472835,
      -1.3663214555720815,
      -1.3187471189094002,
      -1.361352883564121,
      -1.3369612918577918,
      -1.367859187651919,
      -1.3289152538438602,
      -1.3564363022397399,
      -1.3711125167175382,
      -1.3423891251078992,
      -1.411720630375213,
      -1.3940204844538953,
      -1.3819478119291153,
      -1.353550738077452,
      -1.3649387267451278,
      -1.3416996825394707,
      -1.4024419844347047,
      -1.3757290722163527,
      -1.3739873839846783,
      -1.4057902708941252,
      -1.3726813679813483,
      -1.350400876007858,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.12782386449726602,
      0.142780052736736,
      0.1327617590128347,
      0.12451891287726458,
      0.12962143717803487,
      0.09428886570461037,
      0.12339463551256813,
      0.13387199367916258,
      0.13022518154612683,
      0.13247360232264072,
      0.09303024652278946,
      0.13800575746257104,
      0.13432831324851513,
      0.14750073534945773,
      0.1271541567870429,
      0.12209052528213116,
      0.1257949748815772,
      0.1367661553598034,
      0.14770415428523886,
      0.13442553682804492,
      0.12806195091221717,
      0.12726901197293428,
      0.14056454314228795,
      0.13474766340341837,
      0.12165639672680406,
      0.13054780115467723,
      0.11548666464316559,
      0.09809455123464783
    ],
    [
      0.24483741237132728,
      0.0,
      0.4334089345950085,
      0.46608474262006494,
      0.38065856649814567,
      0.4101340223922396,
      0.31864165357984153,
      0.4012302561587868,
      0.40708501617052506,
      0.35046031096213937,
      0.3287815034758761,
      0.26751250609081634,
      0.4585879276907585,
      0.38116998996569373,
      0.41310183517353805,
      0.46517454501606026,
      0.3068056101041754,
      0.3672014201705842,
      0.3445999196515106,
      0.5124170081576147,
      0.3851733889839588,
      0.3615026420206513,
      0.308647472185702,
      0.3964509532893339,
      0.3204093895630471,
      0.41598434426729547,
      0.36470138460898527,
      0.3785067297911331,
      0.31326102073189044
    ],
    [
      0.37777289270815184,
      0.5385007314662356,
      0.0,
      0.4859531716574137,
      0.5047770904707347,
      0.4278817535968287,
      0.36980874428983923,
      0.47741607170877587,
      0.4604949209299918,
      0.4575545039790263,
      0.47104579893840137,
      0.341221347883953,
      0.5089727623102045,
      0.45281880016624876,
      0.4847147062479211,
      0.48507981940798284,
      0.406682251503218,
      0.49438218761329256,
      0.437401253000806,
      0.533408387190448,
      0.46317760309521194,
      0.40146798626823665,
      0.3775378413369148,
      0.4375771513203366,
      0.46176459409865567,
      0.4654842722500998,
      0.48856391093733076,
      0.4430592715636583,
      0.33012199345432225
    ],
    [
      0.36627548849316516,
      0.560130601086263,
      0.4392102812122183,
      0.0,
      0.4114942254821028,
      0.48584102510501803,
      0.3635973277265896,
      0.43620358561000083,
      0.4610230187738311,
      0.43730668123250527,
      0.48319447478171584,
      0.35108471168918487,
      0.5711859705445217,
      0.504968519671239,
      0.5233314392442936,
      0.5369402511318631,
      0.3664721927619403,
      0.4098610860335188,
      0.4679501553858645,
      0.4881219694432326,
      0.42816057339026714,
      0.4662141950698586,
      0.39127286319247334,
      0.5004954502029597,
      0.3949384346833442,
      0.48538360843234396,
      0.45567792319345646,
      0.4155253731406179,
      0.37563034630064096
    ],
    [
      0.2997458560228379,
      0.3051873093198565,
      0.40514297750556505,
      0.33197481217860014,
      0.0,
      0.3345365909867313,
      0.329293599263232,
      0.4023140274815715,
      0.43829900522002085,
      0.40066399400502806,
      0.2848673417287959,
      0.3057392482795047,
      0.39881569881660006,
      0.29813674801476475,
      0.37188473966332336,
      0.41067120374837307,
      0.39082836369683815,
      0.4114007584834438,
      0.37780796438508824,
      0.4626113266347607,
      0.41362748482007206,
      0.34185250198119266,
      0.3417581512916408,
      0.31897570743922543,
      0.36838558192252835,
      0.4144118212464354,
      0.3882460234302585,
      0.2961579807652903,
      0.31245473079599684
    ],
    [
      0.2889910531019124,
      0.5275775000563949,
      0.44387564092413134,
      0.5333915827025684,
      0.41812582244018226,
      0.0,
      0.3507814543097576,
      0.4180080321548916,
      0.4381663673888143,
      0.4272277241714362,
      0.38426535908537396,
      0.31091558154561016,
      0.4876066054021977,
      0.43199927224520906,
      0.4878063427219612,
      0.4802103186944451,
      0.38872878048742576,
      0.44535137432596206,
      0.4020567257930914,
      0.5180564611582672,
      0.4570980268844538,
      0.4520213662565349,
      0.36133923936031787,
      0.43323990450987626,
      0.37778302827838095,
      0.45453158893794465,
      0.44807939288524445,
      0.3995783155665522,
      0.34244819705714624
    ],
    [
      0.2171260462154314,
      0.3315752923826931,
      0.32944372724560855,
      0.3163591313625471,
      0.332016066715354,
      0.32178392182871396,
      0.0,
      0.3255253848994726,
      0.3111721569982133,
      0.28879876484973366,
      0.3017919248528611,
      0.27806086824834764,
      0.28278272308938135,
      0.28593667150891755,
      0.29816729589249347,
      0.32958507297292106,
      0.2810824255541362,
      0.2569013742553581,
      0.2888867891700806,
      0.3612668919459232,
      0.30343031536983256,
      0.32113162119215155,
      0.3040008428854919,
      0.29715786241208386,
      0.3216541262168193,
      0.3482254497970023,
      0.28622864767089706,
      0.28640408644367366,
      0.25809706113343234
    ],
    [
      0.3100462220626299,
      0.39683903354109584,
      0.4418262751723483,
      0.4344684209490912,
      0.39401571965836646,
      0.40228903490198387,
      0.3087560577188513,
      0.0,
      0.44404786855182454,
      0.4083829598617972,
      0.42922731684061954,
      0.3442045480689184,
      0.46220015047339014,
      0.4016625375023508,
      0.45784972691990933,
      0.42507556270181945,
      0.3832218701214358,
      0.4420680068794023,
      0.45163266009546876,
      0.42450665185093306,
      0.48759101916857817,
      0.3592803767120749,
      0.2948260124376627,
      0.41862715346112167,
      0.3925210724566015,
      0.365532477101981,
      0.5158238705364422,
      0.3426510610347455,
      0.33556115589747404
    ],
    [
      0.3357438302104927,
      0.4200336290476274,
      0.4483064240436798,
      0.4538649306486975,
      0.46325250280566754,
      0.4108246590738549,
      0.34253479596933345,
      0.5151321596487195,
      0.0,
      0.3955117075814487,
      0.4648010433116896,
      0.3234079875315281,
      0.5172568558658075,
      0.4621000549362586,
      0.49848578547056954,
      0.4512909324787635,
      0.37007978561930455,
      0.38112598409538045,
      0.3560399724667742,
      0.4737524775470221,
      0.42569459134610055,
      0.36051775656938023,
      0.34026925337602587,
      0.4234761469510129,
      0.37004059245390986,
      0.39593031582824123,
      0.4115237608850484,
      0.3818450060595395,
      0.3845496234983896
    ],
    [
      0.33979869133070295,
      0.37654339507013557,
      0.3428162503161596,
      0.38110264139489725,
      0.37586852020895045,
      0.34692635704756714,
      0.26134935912556445,
      0.43170550107021644,
      0.37872514386097644,
      0.0,
      0.34925318354772417,
      0.3045782417198746,
      0.3808853835845729,
      0.3609702500892191,
      0.3817346914191888,
      0.353077935367637,
      0.3570869875789693,
      0.37392934499862696,
      0.3903909716945564,
      0.3278543380672334,
      0.45257665680643044,
      0.2959415893523085,
      0.2970039271478706,
      0.34519996797815256,
      0.31999330032724926,
      0.3423174127455255,
      0.37675179388345925,
      0.35135634510279434,
      0.3327779020011097
    ],
    [
      0.3634426992803408,
      0.4501158829168743,
      0.4530457181056251,
      0.41940523240789296,
      0.34147189708688885,
      0.41780978805425195,
      0.37039725096722886,
      0.4397788811113468,
      0.44292776175815063,
      0.4226605984099854,
      0.0,
      0.3155166066539272,
      0.46047004061354313,
      0.4349078720264179,
      0.4424417929737803,
      0.4841776259645052,
      0.39098579089547014,
      0.41973662286899205,
      0.4109763953608414,
      0.40181953794557357,
      0.43764023653531314,
      0.41349208909203794,
      0.3450608132542554,
      0.4573156066469164,
      0.4066884656682386,
      0.42020077243748544,
      0.38866773920135667,
      0.37998623750120997,
      0.3723540380397117
    ],
    [
      0.21937964731473358,
      0.23050973096032612,
      0.24225974710442943,
      0.24424884559302207,
      0.2631288432316621,
      0.2389236239597805,
      0.2192334549903756,
      0.2620212181384747,
      0.24243972522725987,
      0.2857752885965983,
      0.26082302270620783,
      0.0,
      0.2454547983934443,
      0.24303692446049396,
      0.2469370609595436,
      0.238488094167161,
      0.22086962284266298,
      0.23415261524181918,
      0.23405746363192437,
      0.22571102864776948,
      0.2757231559248179,
      0.2477376105816944,
      0.24352139976563825,
      0.24145509202790838,
      0.2844875202860613,
      0.23743282282021627,
      0.2582863557362238,
      0.25502749734937047,
      0.25933647833799367
    ],
    [
      0.3169131910308578,
      0.39587562591740966,
      0.3859776229476908,
      0.4461882574199145,
      0.38303951276979475,
      0.3942336446330421,
      0.28204414977375714,
      0.4070251059120973,
      0.41398455308097626,
      0.37660348082855655,
      0.36954406380951865,
      0.2825418107329172,
      0.0,
      0.4032377990476339,
      0.3941365125267853,
      0.506324436846525,
      0.3180955247465831,
      0.3260425758306311,
      0.32879885856443014,
      0.4505450314024808,
      0.3945964421955419,
      0.29600470731503714,
      0.30816762332031145,
      0.4134270779701146,
      0.3681782916794625,
      0.4078359405613139,
      0.3501376615060694,
      0.2962347843212061,
      0.29744567637622743
    ],
    [
      0.346192413257453,
      0.47683797708078,
      0.40452747593002214,
      0.4903527452063008,
      0.36398866714016775,
      0.46229656604792013,
      0.371504066405425,
      0.40369344755018655,
      0.447616156443706,
      0.45095802764507975,
      0.4738191525960951,
      0.32502401832198613,
      0.4342946888717858,
      0.0,
      0.5086442915435361,
      0.4377772358465877,
      0.32844450492743804,
      0.34937738092568216,
      0.4113506486919505,
      0.4198101600195343,
      0.40941714226892256,
      0.38463098507201576,
      0.39466483196089475,
      0.4691958556469633,
      0.41989222447014973,
      0.4735618456734427,
      0.3936049016242884,
      0.3633190890298048,
      0.35797462428256255
    ],
    [
      0.2927131278842785,
      0.4046136783131309,
      0.3763914576674816,
      0.43709969806642346,
      0.38729385679900585,
      0.40306607524467974,
      0.34708552107472257,
      0.4239042680271998,
      0.4810910587552901,
      0.39928361539013535,
      0.4191946820128347,
      0.300454355704445,
      0.4028761315996898,
      0.43443114385443926,
      0.0,
      0.4374568071225573,
      0.3694796674924332,
      0.3592276860978936,
      0.37079493974948474,
      0.42633874545787953,
      0.43303196453669957,
      0.3933326233157819,
      0.32076820234316483,
      0.39550049555877,
      0.3822905972020152,
      0.4021470858377505,
      0.4183508841536723,
      0.3669428774424186,
      0.34205940330143125
    ],
    [
      0.30860480339058527,
      0.4654662150067006,
      0.4270352421102701,
      0.43532778615645307,
      0.4703290372623028,
      0.425575839278469,
      0.3715848498486425,
      0.384427645525778,
      0.46162165693081936,
      0.3936706226740432,
      0.3896562538580699,
      0.2973392800152337,
      0.5439262201567119,
      0.40404638252635716,
      0.42915172841647165,
      0.0,
      0.3886869282829284,
      0.39229206799665706,
      0.43464534761882123,
      0.5052378551591477,
      0.4101140763653919,
      0.409222396019133,
      0.3626040799610082,
      0.4075943692920645,
      0.32503735749031337,
      0.44698048818675296,
      0.38549735108094496,
      0.32023656028057124,
      0.3284876686876077
    ],
    [
      0.2639454095025673,
      0.29193108329947814,
      0.3629582142855907,
      0.31659939480904553,
      0.41673110501128496,
      0.2885156125291386,
      0.3002267834999406,
      0.3682756643107836,
      0.3435104036307075,
      0.33429886059141856,
      0.2875130029612325,
      0.21226432682564056,
      0.3313428183902629,
      0.26887753865119146,
      0.36377271729023986,
      0.3366067963466888,
      0.0,
      0.37226731715168215,
      0.3385956421395462,
      0.3794451808897956,
      0.4101504968774572,
      0.2847162855870349,
      0.2971057963277346,
      0.3179577137078873,
      0.31859465891669525,
      0.340647953537935,
      0.4013192102985599,
      0.2941130789548987,
      0.22012097772361106
    ],
    [
      0.370985990367124,
      0.5071192938594402,
      0.5227709522441182,
      0.5227212835094794,
      0.6389809797295589,
      0.560260490970649,
      0.38491872863469667,
      0.5997205205798777,
      0.5101619367930237,
      0.5126167004251798,
      0.5719335050554617,
      0.3868246200232279,
      0.5421838644636812,
      0.4990975901093535,
      0.5182668371449244,
      0.5577054470720957,
      0.4575570813247021,
      0.0,
      0.5990456872687135,
      0.5057365113098629,
      0.6502658845999916,
      0.4814474991421904,
      0.4145556066488385,
      0.558885482166156,
      0.43988672694671593,
      0.5121281992000057,
      0.6394730443459138,
      0.4231537804186052,
      0.4303083545721853
    ],
    [
      0.46888158583284856,
      0.47626609558070765,
      0.537699886760372,
      0.510233585475993,
      0.5395357829203775,
      0.47396531313358414,
      0.45877699481866463,
      0.5755656873870092,
      0.472925589816654,
      0.5491498475130989,
      0.5163506012795478,
      0.4163693277055456,
      0.5200481969835271,
      0.5179953703877884,
      0.5329975220039049,
      0.5432387815997781,
      0.5002839034205746,
      0.5308566076332062,
      0.0,
      0.5118920797859154,
      0.5802879566748875,
      0.46523874651513797,
      0.4190359523592635,
      0.4854360831447051,
      0.4446786213560512,
      0.5450445484369069,
      0.5791267155587363,
      0.4442310256059665,
      0.43830929011721564
    ],
    [
      0.2521495638635549,
      0.4394775512784401,
      0.3871144062027059,
      0.45571864018476216,
      0.433299725906124,
      0.4300886857908217,
      0.3353484438784351,
      0.3586341187576445,
      0.4035021016940905,
      0.3302000929389075,
      0.29373971979502533,
      0.2517997304001729,
      0.4158346760939229,
      0.3660932975891089,
      0.41579930718620983,
      0.4230139764534864,
      0.3705919781154603,
      0.371337419273128,
      0.34004248369594015,
      0.0,
      0.3553315229952252,
      0.3685157360905287,
      0.34711310161891173,
      0.3566165815537763,
      0.30490329914769854,
      0.39554813524419297,
      0.3972246469834484,
      0.3773847695939361,
      0.30086373210128925
    ],
    [
      0.325652039559575,
      0.4138372560866632,
      0.4608951772313383,
      0.42192149769394804,
      0.4612768933230911,
      0.4222491831765094,
      0.3286727573969741,
      0.5197608080857685,
      0.44391239215205136,
      0.4431605092346431,
      0.3701925340907486,
      0.3374118319500634,
      0.44266084278868756,
      0.41588260866945936,
      0.4499746707873009,
      0.3935608722046122,
      0.4148905343212612,
      0.445713345834599,
      0.45425609436838643,
      0.3821684275366335,
      0.0,
      0.32130883883840466,
      0.3501746439995037,
      0.41656061593569316,
      0.3737364714903566,
      0.36657953233910856,
      0.5256119014744332,
      0.3473525996913145,
      0.3319270405550725
    ],
    [
      0.35411491926662997,
      0.382421898044339,
      0.3250858317729892,
      0.4197079553573826,
      0.3653262292791486,
      0.39912652914187063,
      0.4058540426901831,
      0.32722420174026046,
      0.35517225294424626,
      0.32334127408490465,
      0.3658449231653391,
      0.3311220925343248,
      0.32848654476160055,
      0.39640478717652106,
      0.41067080620029506,
      0.41768707544893346,
      0.34515788372878387,
      0.3507717434246831,
      0.3553694043819293,
      0.4691682198360734,
      0.3459062217954465,
      0.0,
      0.40820159380599486,
      0.36307757217717085,
      0.3515602488623144,
      0.45145186644851387,
      0.3349625305226882,
      0.33060293956379283,
      0.3693642831121353
    ],
    [
      0.26080773778793387,
      0.27800180707430644,
      0.30204444339882053,
      0.2846754934679796,
      0.33925944741704805,
      0.2958866985360129,
      0.2772048366262496,
      0.3073348578047397,
      0.305371592229966,
      0.3430861111205601,
      0.26679628381589304,
      0.2413280980432453,
      0.29007918781967956,
      0.2817486703858376,
      0.3197749761269688,
      0.33235103418756284,
      0.31228690960180505,
      0.3028456958784551,
      0.2423815899179118,
      0.3277617266339736,
      0.3519236635085561,
      0.32723128046590144,
      0.0,
      0.26792403102420526,
      0.3094361472808431,
      0.3367459443402079,
      0.3680207786707943,
      0.2764574487152611,
      0.2447109669902845
    ],
    [
      0.3349613374972291,
      0.4477751915422765,
      0.40894747907464946,
      0.43032273375311414,
      0.34774171744131444,
      0.4226388612142069,
      0.32789431890324194,
      0.4114583454495033,
      0.38803607605705803,
      0.37648966738237055,
      0.44359143749728003,
      0.31645838588904884,
      0.47239524652208975,
      0.43589871734922503,
      0.4126800115793472,
      0.43558909744604635,
      0.35024628849438244,
      0.3146250748827062,
      0.42359821836952216,
      0.42608678714831716,
      0.4049159779283198,
      0.38767379654133416,
      0.3372700138167959,
      0.0,
      0.3916887744724724,
      0.4301463961713148,
      0.4037673712132195,
      0.36532925672741046,
      0.32572311009766275
    ],
    [
      0.2749231852485987,
      0.28003368573330656,
      0.3683340573529932,
      0.27785854695694656,
      0.3450286867971273,
      0.2862317234215559,
      0.31256882984905543,
      0.32417752690504886,
      0.30574827382516756,
      0.34148850439821943,
      0.3024064163390523,
      0.30464341556858576,
      0.3014844755265276,
      0.3273154572620103,
      0.2996088735426874,
      0.283296738903035,
      0.3487808249740587,
      0.2786958374386528,
      0.2910177162030809,
      0.3306641044387295,
      0.3552850987944831,
      0.25991275173449524,
      0.29430201716699145,
      0.30724753969574436,
      0.0,
      0.31985360547958774,
      0.35596891099014494,
      0.29762712225131915,
      0.2813571497414924
    ],
    [
      0.29097653854316086,
      0.4344772309907503,
      0.3282176118074074,
      0.4171604411324792,
      0.3873501368125556,
      0.4289250687748376,
      0.37713093856887214,
      0.3723862347164155,
      0.34446555538420864,
      0.3372829443789258,
      0.34765968035963324,
      0.27144598092318883,
      0.36765317287705224,
      0.4119357952149778,
      0.3815946947617199,
      0.36616863589168225,
      0.3616769490055256,
      0.3450518071320403,
      0.32297430831256535,
      0.4346348006454832,
      0.354000642413262,
      0.3986732584268775,
      0.3537893357549411,
      0.39097720528304425,
      0.34865129512070747,
      0.0,
      0.37662880142200783,
      0.30309613851515094,
      0.32076352245750295
    ],
    [
      0.32244424414582906,
      0.3426162605533425,
      0.4364471123203526,
      0.37727146161862257,
      0.4543935264557353,
      0.35405427170774884,
      0.3174617502314654,
      0.4739247922522194,
      0.38271949665995586,
      0.3901517270609951,
      0.342774152844622,
      0.33021658469313175,
      0.38079847130762934,
      0.37455646922150065,
      0.4021210542606979,
      0.331432933508397,
      0.40080175251306405,
      0.40987902153022615,
      0.40301560288728555,
      0.37493169296898343,
      0.513035980694456,
      0.30142929315164624,
      0.363529255861861,
      0.3324815320746568,
      0.39184841070425924,
      0.35318283301211917,
      0.0,
      0.31572927813502716,
      0.29381054782986316
    ],
    [
      0.24743959561899764,
      0.31697337221299415,
      0.34272288300712916,
      0.30818934209938664,
      0.31248598047064857,
      0.31144078831078525,
      0.24319103032996514,
      0.3101662637924485,
      0.3115919669675027,
      0.3259249704539613,
      0.30724604343635,
      0.2698303540625089,
      0.30310075747590903,
      0.2756630102545723,
      0.3178079612346374,
      0.2882581694378912,
      0.2846670050212181,
      0.32716955568020434,
      0.28838832390608093,
      0.30195256676680016,
      0.30840206034153894,
      0.28483340754257047,
      0.2630976345946525,
      0.30778693986284167,
      0.30484627560033384,
      0.2989275011281616,
      0.3268177568320283,
      0.0,
      0.2600726701412568
    ],
    [
      0.3587239858725422,
      0.44618736727248254,
      0.42507536252216194,
      0.42801586016362814,
      0.4588963800007111,
      0.3992899113959594,
      0.3825800305267675,
      0.4348419142019695,
      0.4824162508646508,
      0.4398104862099299,
      0.4642020779162592,
      0.43330418212213195,
      0.4722481159301908,
      0.4447270675343111,
      0.4300508530565128,
      0.4587742446661518,
      0.389442739398838,
      0.40714288532015575,
      0.4192155578449357,
      0.44761263169659893,
      0.43622464302892316,
      0.4594636872345803,
      0.3987213853393463,
      0.42543429755769835,
      0.42717598578937266,
      0.3953730988799258,
      0.4284820017927027,
      0.45076249376619293,
      0.0
    ]
  ],
  "row_avgs": [
    0.12767819443802034,
    0.3750903752245251,
    0.44945149354979425,
    0.4491961347505367,
    0.36270684104027057,
    0.42533075208736015,
    0.3023068765396276,
    0.4026691008099614,
    0.41347830590429524,
    0.35458986010134547,
    0.41083907120636287,
    0.24644495317848622,
    0.36832785582381733,
    0.4133132544457386,
    0.3902578803573467,
    0.4044428610563661,
    0.32365728728743026,
    0.5113825928187776,
    0.5019436321359989,
    0.36704598015810525,
    0.4064750686005785,
    0.37082806683101766,
    0.2997670521025358,
    0.39192677465933784,
    0.30913789559066773,
    0.36341959734382057,
    0.37382355393591765,
    0.2981783638065491,
    0.43014983921091543
  ],
  "col_avgs": [
    0.31262819670648184,
    0.3951695914354042,
    0.39015575877134295,
    0.39924928548569605,
    0.3969387797504042,
    0.3815863384797427,
    0.32688323702508154,
    0.4012589698747777,
    0.3932892961710302,
    0.3811458984830984,
    0.37224961080088825,
    0.30513036749106615,
    0.4093442173505693,
    0.3803552735382006,
    0.40503603463207927,
    0.4048631357650216,
    0.35450088149345593,
    0.3659000632813772,
    0.368644887497014,
    0.4132577412346509,
    0.40632887014900654,
    0.3561734635357503,
    0.3344859965388265,
    0.379522819001168,
    0.35913639842457196,
    0.3908302234667363,
    0.39993189544975116,
    0.343505636141951,
    0.31635664702036276
  ],
  "combined_avgs": [
    0.2201531955722511,
    0.3851299833299646,
    0.4198036261605686,
    0.4242227101181164,
    0.3798228103953374,
    0.4034585452835514,
    0.31459505678235455,
    0.4019640353423696,
    0.4033838010376627,
    0.3678678792922219,
    0.39154434100362556,
    0.2757876603347762,
    0.38883603658719335,
    0.3968342639919696,
    0.397646957494713,
    0.40465299841069385,
    0.3390790843904431,
    0.4386413280500774,
    0.4352942598165065,
    0.3901518606963781,
    0.40640196937479256,
    0.363500765183384,
    0.3171265243206811,
    0.38572479683025296,
    0.3341371470076199,
    0.3771249104052784,
    0.38687772469283443,
    0.32084199997425006,
    0.3732532431156391
  ],
  "gppm": [
    582.1289080644607,
    604.4077224596673,
    607.3882626950451,
    603.3773605754233,
    604.9515000758844,
    611.5886167596782,
    635.0535682312492,
    601.5633271387325,
    609.5444715238986,
    611.3132896906594,
    618.2023075056948,
    644.5398874960648,
    600.1068602864051,
    614.6771843562263,
    603.067824378454,
    601.5370066442675,
    623.8041257489488,
    619.4224275099897,
    618.4399090179197,
    598.0304471913508,
    601.2738023354526,
    625.3347407041686,
    632.409457784546,
    612.8301624788322,
    622.6709004261926,
    608.824536862241,
    604.9788814022113,
    629.010947120809,
    642.4686252880177
  ],
  "gppm_normalized": [
    1.4162204265097986,
    1.3768098089531178,
    1.3738913326164657,
    1.368684546048383,
    1.3667931030916656,
    1.3880025101541733,
    1.4479447856169376,
    1.3615922696754832,
    1.3782933023163015,
    1.3819300508509083,
    1.3966804304064415,
    1.4764738786842255,
    1.3612534862346148,
    1.3925399638077212,
    1.3665044458961433,
    1.362042906658346,
    1.4168371356992129,
    1.3952533209047806,
    1.3935729593323618,
    1.359245940918304,
    1.356456296873596,
    1.4152353684382923,
    1.4362094428388954,
    1.3911757920431016,
    1.4146562399670124,
    1.3779682710485954,
    1.3684608417340303,
    1.4333115971136074,
    1.4503240919660672
  ],
  "token_counts": [
    844,
    485,
    390,
    428,
    396,
    426,
    466,
    423,
    385,
    389,
    374,
    530,
    438,
    406,
    419,
    417,
    428,
    341,
    345,
    457,
    385,
    373,
    405,
    429,
    420,
    401,
    409,
    456,
    333
  ],
  "response_lengths": [
    4233,
    2889,
    2339,
    2592,
    2332,
    2493,
    2755,
    2465,
    2255,
    2298,
    2296,
    3024,
    2662,
    2490,
    2466,
    2477,
    2384,
    2083,
    2142,
    2522,
    2212,
    2200,
    2310,
    2658,
    2465,
    2243,
    2421,
    2529,
    1953
  ]
}