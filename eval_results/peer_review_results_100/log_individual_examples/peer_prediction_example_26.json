{
  "example_idx": 26,
  "reference": "Under review as a conference paper at ICLR 2023\n\nLEARNING FOR EDGE-WEIGHTED ONLINE BIPARTITE MATCHING WITH ROBUSTNESS GUARANTEES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMany real-world problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but they lack robustness and can perform arbitrarily badly. In this paper, we propose a novel RL-based approach to edgeweighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and good worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judiciously-designed condition to hedge against future uncertainties, decides whether to follow the expert’s decision or the RL decision for each online item arrival. We prove that for any ρ ∈ [0, 1], LOMAR is ρ-competitive against any given expert online algorithm. To improve the average performance, we train the RL policy by explicitly considering the online switching operation. Finally, we run empirical experiments to demonstrate the advantages of LOMAR compared to existing baselines.\n\n1\n\nINTRODUCTION\n\nOnline bipartite matching is a classic online problem of practical importance (Mehta, 2013; Kim & Moon, 2020; Fahrbach et al., 2020; Antoniadis et al., 2020b; Huang & Shu, 2021; Gupta & Roughgarden, 2020). In a nutshell, online bipartite matching assigns online items to offline items in two separate sets: when an online item arrives, we need to match it to an offline item given applicable constraints (e.g., capacity constraint), with the goal of maximizing the total rewards collected (Mehta, 2013). For example, numerous applications, including scheduling tasks to servers, displaying advertisements to online users, recommending articles/movies/products, among many others, can all be modeled as online bipartite matching or its variants.\n\nThe practical importance, along with substantial algorithmic challenges, of online bipartite matching has received extensive attention in the last few decades (Karp et al., 1990; Fahrbach et al., 2020). Concretely, many algorithms have been proposed and studied for various settings of online bipartite matching, ranging from simple yet effective greedy algorithms to sophisticated ranking-based algorithms (Karp et al., 1990; Kim & Moon, 2020; Fahrbach et al., 2020; Aggarwal et al., 2011; Devanur et al., 2013). These expert algorithms typically have robustness guarantees in terms of the competitive ratio — the ratio of the total reward obtained by an online algorithm to the reward of another baseline algorithm (commonly the optimal offline algorithm) — even under adversarial settings given arbitrarily bad problem inputs (Karp et al., 1990; Huang & Shu, 2021). In some settings, even the optimal competitive ratio for adversarial inputs has been derived (readers are referred to (Mehta, 2013) for an excellent tutorial). The abundance of competitive online algorithms has clearly demonstrated the importance of performance robustness in terms of the competitive ratio, especially in safety-sensitive applications such as matching mission-critical items or under contractual obligations (Fahrbach et al., 2020). Nonetheless, as commonly known in the literature, the necessity of conservativeness to address the worst-case adversarial input means that the average performance is typically not optimal (see, e.g., (Christianson et al., 2022; Zeynali et al., 2021) for discussions in other general online problems).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nMore recently, online optimizers based on reinforcement learning (RL) (Chen et al., 2022; Georgiev & Lió, 2020; Wang et al., 2019; Alomrani et al., 2021; Du et al., 2019; Zuzic et al., 2020) have been proposed in the context of online bipartite matching as well as other online problems. Specifically, by exploiting statistical information of problem inputs, RL models are trained offline and then applied online to produce decisions given unseen problem inputs. These RL-based optimizers can often achieve high average rewards in many typical cases. Nonetheless, they may not have any performance robustness guarantees in terms of the competitive ratio. In fact, a crucial pain point is that the worst-case performance of many RL-based optimizers can be arbitrarily bad, due to, e.g., testing distribution shifts, inevitable model generalization errors, finite samples, and/or even adversarial inputs. Consequently, the lack of robustness guarantees has become a key roadblock for wide deployment of RL-based optimizers in real-world applications.\n\nIn this paper, we focus on an important and novel objective — achieving both good average performance and guaranteed worst-case robustness — for edge-weighted online bipartite matching (Fahrbach et al., 2020; Kim & Moon, 2020). More specifically, our algorithm, called LOMAR (Learning-based approach to edge-weighted Online bipartite MAtching with Robustness guarantees), integrates an expert algorithm with RL. The key novelty of LOMAR lies in a carefully-designed online switching step that dynamically switches between the RL decision and the expert decision online, as well as a switching-aware training algorithm. For both no-free-disposal and free-disposal settings, we design novel switching conditions as to when the RL decisions can be safely followed while still guaranteeing robustness of being ρ-competitive against any given expert online algorithms for any ρ ∈ [0, 1]. Furthermore, if the expert itself has a competitive ratio of λ ≤ 1 against the optimal offline algorithm (OPT), then it will naturally translate into LOMAR being ρλ-competitive against OPT. To improve the average performance of LOMAR, we train the RL policy in LOMAR by explicitly taking into account the introduced switching operation. Importantly, to avoid the “no supervision” trap during the initial RL policy training, we propose to approximate the switching operation probabilistically. Finally, we offer empirical experiments to demonstrate that LOMAR can improve the average cost (compared to existing expert algorithms) as well as lower the competitive ratio (compared to pure RL-based optimizers).\n\n2 RELATED WORKS\n\nOnline bipartite matching has been traditionally approached by expert algorithms (Mehta, 2013; Karande et al., 2011; Huang et al., 2019; Devanur et al., 2013). A simple but widely-used algorithm is the (deterministic) greedy algorithm (Mehta, 2013), achieving reasonably-good competitive ratios and empirical performance (Alomrani et al., 2021). Randomized algorithms have also been proposed to improve the competitive ratio (Ting & Xiang, 2014; Aggarwal et al., 2011). In addition, competitive algorithms based on the primal-dual framework have also been proposed (Mehta, 2013; Buchbinder et al., 2009). More recently, multi-phase information and predictions have been leveraged to exploit stochasticity within each problem instance and improve the algorithm performance (Kesselheim et al., 2013). For example, (Korula & Pál, 2009) designs a secretary matching algorithm based on a threshold obtained using the information of phase one, and exploits the threshold for matching in phase two. Note that stochastic settingsconsidered by expert algorithms (Mehta, 2013; Karande et al., 2011) mean that the arrival orders and/or rewards of different online items within each problem instance are stochastic. By contrast, as shown in equation 2, we focus on an unknown distribution of problem instances whereas the inputs within each instance can still be arbitrary.\n\nAnother line of algorithms utilize RL to improve the average performance (Wang et al., 2019; Georgiev & Lió, 2020; Chen et al., 2022; Alomrani et al., 2021). Even though heuristic methods (such as using adversarial training samples (Zuzic et al., 2020; Du et al., 2022)) are used to empirically improve the robustness, they do not provide any theoretically-proved robustness guarantees.\n\nML-augmented algorithms have been recently considered for various problems (Rutten et al., 2022; Christianson et al., 2022; Chł ̨edowski et al., 2021; Lykouris & Vassilvitskii, 2021; Gupta & Roughgarden, 2017). By viewing the ML prediction as blackbox advice, these algorithms strive to provide good competitive ratios when the ML predictions are nearly perfect, and also bounded competitive ratios when ML predictions are bad. But, they still focus on the worst case without addressing the average performance or how the ML model is trained. By contrast, the RL model in LOMAR is trained by taking into account the switching operation and performs inference based on the actual state\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n(rather than its own independently-maintained state as a blackbox). Assuming a given downstream algorithm, (Wang et al., 2021; Liu & Grigas, 2021; Wilder et al., 2019; Elmachtoub & Grigas, 2017; Du et al., 2021; Anand et al., 2021) focus on learning the ML model to better serve the end goal in completely different (sometimes, offline optimization) problems.\n\nLOMAR is relevant to conservative bandits/RL (Wu et al., 2016; Kazerouni et al., 2017; Yang et al., 2022; Garcelon et al., 2020). With unknown reward functions (as well as transition models if applicable), conservative bandits/RL leverages an existing policy to safeguard the exploration process. But, they only consider the cumulative reward without addressing future uncertainties when deciding exploration vs. rolling back to an existing policy. Thus, as shown in Section 4, this cannot guarantee robustness in our problem. Also, constrained policy optimization (Yang et al., 2020; Kumar et al., 2020; Schulman et al., 2015; Achiam et al., 2017; Thomas et al., 2021; Berkenkamp et al., 2017) focuses on average (cost) constraints in the long run, whereas LOMAR achieves stronger robustness (relative to an expert algorithm) for each episode with even adversarial inputs.\n\n3 PROBLEM FORMULATION\n\nWe focus on edge-weighted online bipartite matching, which includes un-weighted and vertexweighted matching as special cases (Fahrbach et al., 2020; Kim & Moon, 2020). In the following, we also drop “edge-weighted” if applicable when referring to our problem.\n\nThe goal of the agent is to match items (a.k.a. vertices) between two sets U and V to gain as high total rewards as possible. Suppose that U is fixed and contains offline items u ∈ U, and that the online items v ∈ V arrive sequentially: in each time slot, an online item v ∈ V arrives and the weight/reward information {wuv | wu,min ≤ wuv ≤ wu,max, u ∈ U} is revealed, where wuv represents the reward when the online item v is matched to each offline u ∈ U. We denote one problem instance by G = {U, V, W}, where W = {wuv | u ∈ U, v ∈ V}. We denote xuv ∈ {0, 1} as the matching decision indicating whether u is matched to v. Also, any offline item u ∈ U can be matched up to cu times, where cu is essentially the capacity for offline item u known to the agent. The objective is to maximize the total collected reward (cid:80) v∈V,u∈U xuvwuv. With a slight abuse of notations, we denote xv ∈ U as the index of item in U that is matched to item v ∈ V. The set of online items matched to u ∈ U is denoted as Vu = {v ∈ V | xuv = 1}.\n\nThe edge-weighted online bipartite matching problem has been mostly studied under two different settings: no free disposal and with free disposal (Mehta, 2013). In the no-free-disposal case, each offline item u ∈ U can only be matched strictly up to cu times; in the free-disposal case, each offline item u ∈ U can be matched more than cu times, but only the top cu rewards are counted when more than cu online items are matched to u. Compared to the free-disposal case, the no-free-disposal case is significantly more challenging with the optimal competitive ratio being 0 in the strong adversarial setting unless additional assumptions are made (e.g., wu,min > 0 for each u ∈ U (Kim & Moon, 2020) and/or random-order of online arrivals) (Fahrbach et al., 2020; Mehta, 2013). The free-disposal setting not only makes the problem more tractable, but also is practically motivated by the display ad application where the advertisers (i.e., offline items u ∈ U) will not be unhappy if they receive more impressions (i.e., online items v ∈ V) than their budgets cu, even though only the top cu items count.\n\nLOMAR can handle both no-free-disposal and free-disposal settings. For better presentation of our key novelty and page limits, we focus on the no-free-disposal setting in the body of the paper, while deferring the free-disposal setting to Appendix B. Specifically, the offline problem with no free disposal can be expressed as:\n\nmax\n\n(cid:88)\n\nxuvwuv, s.t.,\n\nxuv∈{0,1},u∈U ,v∈V\n\n(cid:88)\n\nv∈V\n\nxuv ≤ cu, and\n\n(cid:88)\n\nu∈U\n\nxuv ≤ 1, ∀u ∈ U, v ∈ V,\n\n(1)\n\nwhere the constraints specify the offline item capacity limit and each online item v ∈ V can only be matched up to one offline item u ∈ U.\n\nGiven a problem instance G and an online algorithm α, we use f α collected for offline item u ∈ U, and Rα(G) = (cid:80) We will also drop the superscript α for notational convenience wherever applicable.\n\nu (G) to denote the total reward u (G) to denote the total collected reward.\n\nu∈U f α\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nObjective. Solving the problem in equation 1 is very challenging in the online case, where the agent has to make irreversible decisions without knowing the future online item arrivals. Next, we first define the following generalized competitiveness as a metric of robustness.\n\nDefinition 1 (Competitiveness). An online bipartite matching algorithm α is said to be ρ−competitive with ρ ≥ 0 against the algorithm π if for any problem instance G, its total collected reward Rα(G) satisfies Rα(G) ≥ ρRπ(G) − B, where B ≥ 0 is a constant independent of the problem input, and Rπ is the total reward of the algorithm π.\n\nCompetitiveness against a given (online) algorithm π is common in the literature (Christianson et al., 2022): the greater ρ ≥ 0, the better robustness of the online algorithm, although the average rewards can be worse. Additionally, the constant B ≥ 0 relaxes the strict competitive ratio by allowing an additive regret (Antoniadis et al., 2020a). When B = 0, the competitive ratio becomes the strict one.\n\nIn this paper, we focus on a setting where the problem instance G = {U, V, W} follows an unknown distribution, whereas both the rewards W and online arrival order within each instance G can be adversarial. We consider both average performance and worst-case robustness guarantees as formalized below:\n\nmax EG [Rα(G)] ,\n\ns.t. Rα(G) ≥ ρRπ(G) − B, ∀G,\n\n(2)\n\nwhere the expectation EG [Rα(G)] is over the randomness G = {U, V, W}.\n\nNote carefully that some manually-designed algorithms focus on a stochastic setting where the arrival order is random and/or the rewards {wuv | wu,min ≤ wuv ≤ wu,max, u ∈ U} of each online item is independently and identically distributed (i.i.d.) within each problem instance G Mehta (2013). By contrast, our settings are significantly different — we only assume an unknown distribution for the entire problem instance G = {U, V, W} while both the rewards W and online arrival order within each instance G can be adversarial in our problem.\n\n4 ONLINE SWITCHING FOR ROBUSTNESS GUARANTEES\n\nWe present LOMAR, which includes an online switching operation to dynamically decide to follow the ML decision or the expert decision, to achieve robustness guarantees with respect to the expert.\n\n4.1 ONLINE SWITCHING\n\nWhile switching is common in online algorithms, “how to switch”is highly non-trivial and a key merit for algorithm designs Antoniadis et al. (2020a); Christianson et al. (2022); Rutten et al. (2022). To guarantee robustness (i.e., ρ-competitive against a given expert for any ρ ∈ [0, 1]), we propose a novel online algorithm (Algorithm 1). In the algorithm, we independently run an expert online algorithm π — the cumulative reward and item matching decisions are all maintained virtually for the expert, but not used as the actual decisions. Meanwhile, instead of being independently executed to provide blackbox advice based on its own virtual state (like in the prior ML-augmented online algorithm (Christianson et al., 2022)), the RL model in LOMAR makes online decisions based on the actual state at each step.\n\nThe most crucial step for safeguarding RL decisions is online switching: Lines 13–19 in Algorithm 1. The key idea for this step is to switch between the expert decision xπ v and the RL decision ̃xv in order to ensure that the actual online decision xv meets the ρ-competitive requirement (against the expert π). Specifically, we follow the RL decision ̃xv only if it can safely hedge against any future uncertainties (i.e., the expert’s future reward increase); otherwise, we need to roll back to the expert’s decision xπ\n\nv to stay on track for robustness.\n\nNote that naive switching conditions, e.g., only ensuring that the actual cumulative reward is at least ρ times of the expert’s cumulative reward at each step (Wu et al., 2016; Yang et al., 2022), can fail to meet the competitive ratio requirement in the end. The reason is that, even though the competitive ratio requirement is met (i.e., Rv ≥ ρRπ v − B) at the current step v, the expert can possibly obtain much higher rewards from future online items v + 1, v + 2, · · · if it has additional offline item capacity that the actual algorithm LOMAR does not have. Thus, we must carefully design the switching conditions to hedge against future risks. The no-free-disposal and free-disposal settings\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Inference of Robust Learning-based Online Bipartite Matching (LOMAR)\n\n1: Initialization: The actual set of items matched to u ∈ U is Vu,v after sequentially-arriving item v’s assignment with Vu,0 = ∅, the actual remaining capacity is bu = cu for u ∈ U, and the actual cumulative reward is R0 = (cid:80) u∈U fu(Vu,0) = 0. The same notations apply to the expert algorithm π by adding the superscript π. Competitive ratio requirement ρ ∈ [0, 1] and slackness B ≥ 0 with respect to the expert algorithm π.\n\n2: for v = 1 to |V| do 3: 4:\n\n5: 6: 7: 8: 9: 10: 11:\n\n12:\n\n13:\n\nv ,v = V π\n\nRun the algorithm π and match the item v to u ∈ U based on the expert’s decision u = xπ v . Update the expert’s decision set and reward for offline item u = xπ v : (V π V π xπ xπ Update the expert’s cumulative reward Rπ Get the set of available items Ua = {u ∈ U | |Vu,v−1| < cu} for u in Ua do\n\n(cid:83){v} and fxπ\n\nv ,v). v = (cid:80)\n\nu∈U fu\n\n= fxπ\n\nv ,v−1\n\nxπ\n\nv\n\nv\n\nCollect the available history information Iu about item u Run the RL model to get score: su = wuv − hθ(Iu, wuv) where θ is the network weight\n\nend for Calculate the probability of choosing each available item or skip:\n\n{{ ̃su}u∈Ua , ̃sskip} = softmax {{su}u∈Ua , 0}.\n\nObtain RL decision: ̃xv = arg maxu∈Ua if Rv−1 + w ̃xv,v ≥ ρ Select xv = ̃xv.\n\nv + (cid:80)\n\nRπ\n\nu∈U\n\n(cid:16)\n\nelse if xπ\n\nv is available for matching (i.e., |Vxπ\n\n//Follow the RL decision v ,v−1| < cxπ\n\nv ) then\n\nSelect xv = xπ v .\n\n//Follow the expert\n\n(cid:83){skip} {{ ̃su}u∈Ua , ̃sskip}. (cid:1)+\n\nu,v| + Iu= ̃xv\n\n(cid:0)|Vu,v−1| − |V π\n\n· wu,max\n\n(cid:17)\n\n− B then\n\nSelect xv = skip.\n\nend if Update assignment and reward: Vxv,v = Vxv,v−1\n\n(cid:83){v} and Rv = Rv−1 + wxv,v\n\nelse\n\n14: 15: 16: 17: 18: 19: 20: 21: end for\n\nrequire different switching conditions. Due to the page limit, we focus on the no-free-disposal setting below, while referring readers to Appendix B for more details about the free-disposal setting.\n\n4.2 ROBUSTNESS CONSTRAINT\n\nIn the no-free-disposal case, an offline item u ∈ U cannot receive any additional online items if it has been matched for cu times up to its capacity. By assigning more online items to u ∈ U than the expert algorithm at step v, LOMAR can possibly receive a higher cumulative reward than the expert’s cumulative reward. But, such advantages are just temporary, because the expert may receive an even higher reward in the future by filling up the unused capacity of item u. Thus, to hedge against the future uncertainties, LOMAR chooses the RL decisions only when the following condition is satified:\n\n(cid:32)\n\nRv−1 + w ̃xv,v ≥ ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B,\n\n(3)\n\nv , plus the term (cid:80)\n\nwhere Iu= ̃xv = 1 if and only if u = ̃xv and 0 otherwise, (·)+ = max(·, 0), ρ ∈ [0, 1] and B ≥ 0 are the hyperparameters indicating the desired robustness with respect to the expert algorithm π. The interpretation of equation 3 is as follows. The left-hand side is the total reward of LOMAR after assigning the online item v based on the RL decision (i.e. ̃xt). The right-hand side is the expert’s (cid:0)|Vu,v−1| − |V π cumulative cost Rπ · wu,max which indicates the maximum reward that can be possibly received by the expert in the future. This reservation term is crucial, especially when the expert has more unused capacity than LOMAR. Specifically, |Vu,v−1| is the number of online items (after assigning v − 1 items) already assigned to the offline item u ∈ U, and hence (cid:0)|Vu,v−1| − |V π represents the number of more online items that LOMAR has assigned to u than the expert if LOMAR follows the RL decision at step v. If LOMAR assigns fewer items than the expert for an offline item u ∈ U, there is no need for any hedging because LOMAR is guaranteed to receive more rewards by filling up the item u up to the expert’s assignment level.\n\nu,v| + Iu= ̃xv\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n(cid:1)+\n\nu∈U\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nThe term wu,max in equation 3 is the set as the maximum possible reward for each decision. Even when wu,max is unknown in advance, LOMAR still applies by simply setting wu,max = ∞. In this case, LOMAR will be less “greedy” than the expert and never use more resources than the expert at any step for any u ∈ U.\n\n4.3 ROBUSTNESS ANALYSIS\n\nWe now formally show the competitive ratio of LOMAR. The proof is available in the appendix.\n\nTheorem 4.1. For any 0 ≤ ρ ≤ 1 and B ≥ 0 and any expert algorithm π, LOMAR achieves a competitive ratio of ρ against the algorithm π, i.e., R ≥ ρRπ − B for any problem input.\n\nThe hyperparameters 0 ≤ ρ ≤ 1 and B ≥ 0 govern the level of robustness we would like to achieve, at the potential expense of average reward performance. For example, by setting ρ = 1 and B = 0, we achieve the same robustness as the expert but leave little to none freedom for RL decisions. On the other hand, by setting a small ρ > 0 and/or large B, we provide higher flexibility to RL decisions for better average performance, while potentially decreasing the robustness. In fact, designing an algorithm that is guaranteed to simultaneously outperform RL and the expert is very challenging, if not impossible, and such tradeoff is necessary in the broad context of ML-augmented online algorithms (Rutten et al., 2022; Christianson et al., 2022). Additionally, in case of multiple experts, we can first combine these experts into a single expert and then apply LOMAR as if it works with a single combined expert.\n\nWhile the competitive ratio of all online algorithms against the optimal offline algorithm is zero in the no-free-disposal and general adversarial setting, there exist provably competitive online expert algorithms under some mild assumptions and other settings (Mehta, 2013). For example, the\n\nsimple greedy algorithm achieves for the adversarial no-free-disposal setting (Kim & Moon, 2020), and 1 2 for the free-disposal setting (Fahrbach et al., 2020), and there also exist 1/e-competitive algorithms against the optimal offline algorithm for the random-order setting (Mehta, 2013). Thus, an immediate result follows.\n\nunder bounded weights assumptions\n\n1 + maxu∈U\n\n(cid:16)\n\n(cid:17)−1\n\nwu,max wu,min\n\nCorollary 4.1.1. For any 0 ≤ ρ ≤ 1 and B ≥ 0, by using Algorithm 1 and an expert online algorithm π that is λ-competitive against the optimal offline algorithm OPT, then under the same assumptions for π to be λ-competitive, LOMAR is ρλ-competitive against OPT.\n\nCorollary 4.1.1 provides a general result that applies to any λ-competitive expert algorithm π under its respective required assumptions. For example, if the expert π assumes an adversarial or random-order setting, then Corollary 4.1.1 also holds true under the same adversarial or random-order setting.\n\nFinally, we comment on the randomized setting where randomization is over the algorithm choice and potentially increases the competitive ratio (Gupta & Roughgarden, 2020; Mehta, 2013). For the randomized setting, the competitive ratio is modified as E(R) ≥ ρROP T − B, where E is the expectation and ROP T is the optimal reward of the optimal offline algorithm (Mehta, 2013). We make no assumptions on the expert π in Algorithm 1. Thus, if the expert π itself is randomized in Algorithm 1, then LOMAR will also be randomized. Also, for any ρ ∈ [0, 1] and B ≥ 0, by directly applying Theorem 4.1 and Corollary 4.1.1 to the randomized setting, the competitive ratio of LOMAR will be ρλ-competitive against OPT if the randomized expert π itself is λ-competitive against OPT.\n\n5 RL POLICY TRAINING WITH ONLINE SWITCHING\n\nThe existing ML-augmented online algorithms typically assume a pre-trained standalone RL model (Christianson et al., 2022; Rutten et al., 2022). While the standalone RL model may perform well on its own, some already good actions can be replaced by expert’s action due to switching for robustness in inference. In other words, there will be a objective mismatch between training and testing. To rectify the mismatch, we propose a novel approach to train the RL model in LOMAR by explicitly considering the switching operation.\n\nRL architecture. For solving online bipartite matching, there exist various network architectures, e.g., fully-connected networks and scalable invariant network for arbitrary graph sizes. The prior study (Mehta, 2013) has shown using extensive empirical experiments that the invariant network\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\narchitecture, where each offline-online item pair runs through a separate neural network with shared weights among all the item pairs, is empirically advantageous, due to its scalability to large graph sizes and high average performance. We denote the RL model as hθ(Iu, wuv) where θ is the network parameter. By feeding the item weight wuv and applicable history information Iu for each offline-online item pair (u, v), we can use the RL model to output a threshold for possible item assignment, following threshold-based algorithms (Huang et al., 2019; Mehta, 2013). The history information Iu includes, but is not limited to, the average value and variance of weights assigned to u, average in-degree of u, and maximum weight for the already matched items. More details about the information can be found in the appendix. Then, with the RL output, we can obtain a score su = wuv − hθ(Iu, wuv), for each possible assignment, and the RL uses the offline item u ∈ U (plus “skip” with sskip = 0 in the no-free-disposal setting) with the largest su as its candidate action ̃xv when checking the switching condition in Algorithm 1.\n\nPolicy training. Training the RL model by considering switching in Algorithm 1 is highly non-trivial. Most critically, the initial RL decisions can perform arbitrarily badly upon policy initialization, which means that the initial RL decisions are almost always overridden by the expert’s decisions for robustness. Due to following the expert’s decisions, the RL agent almost always receive a good reward, which actually has nothing to do with the RL’s own decisions and hence provides little to no supervision to improve the RL policy. Consequently, this creates a gridlock for RL policy training. While using an offline pre-trained standalone RL model without considering online switching (e.g., (Alomrani et al., 2021)) as an initial policy may partially address this gridlock, this is certainly inefficient as we have to spend resources for training another RL model, let alone the likelihood of being trapped into the standalone RL model’s suboptimal policies (e.g. local minimums).\n\nTo address these issues, we introduce another softmax probability with temperature t to approximate the hard switching process during training. The switching probability depends on the cumulative reward difference Rdif f in the switching condition, which is\n\nRdif f = Rv−1 + w ̃xv,v − ρ\n\nRπ\n\nv +\n\n(cid:32)\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n+ B (4)\n\nThen the probability of following RL is pos = eRdif f /t 1+eRdif f /t , where t is the temperature of softmax function. This softmax probability is differentiable and hence allows backpropagation to supervise the training of the RL model weight θ. We train the RL agent by applying REINFORCE (Williams, 1992) to optimize the policy parameter θ. Denote τ = {x1, · · · , xv} as an action trajectory sample and pθ(τ ) as the possibility of the trajectory given the RL policy, where pθ is calculated based on the selection probability of RL model and expert. Our goal is to maximize the expected total reward Rθ = Eτ ∼pθ [wxv,v]. Thus, at each training step, given an RL policy with parameter θ, we sample n action trajectories {τi = {x1,i, · · · , xv,i}, i ∈ [n]} and record the corresponding rewards. We can get the approximated average reward as ˆRθ = 1 xi,v,v. Then, we calculate the gradient of the RL policy parameter as v∈V ∇θ log pθ(xv,i | Iu,i)(cid:1) (cid:16)(cid:80) ∇θ ˆRθ = (cid:80)n . Then, we update the parameter θ by θ = θ + α∇θ ˆRθ, where α controls the training step size.\n\ni=1 wi (cid:0)(cid:80)\n\nv∈V wi\n\n(cid:80)n\n\nxv,i,v\n\ni=1\n\n(cid:17)\n\nn\n\nBy changing the temperature t for softmax, we are able to balance exploration and exploitation. Specifically, at the beginning of the policy training, we can set a high temperature to encourage the RL model to explore more aggressively, instead of sticking to the expert’s decisions. As the RL model performance continuously improves, we can reduce the temperature in order to make the RL agent more aware of the downstream switching operation. The training process is performed offline as in the existing RL-based optimizers (Alomrani et al., 2021; Du et al., 2022) and described in Algorithm 3.\n\n6 EXPERIMENT\n\n6.1 SETUP\n\nTo validate the effectiveness of LOMAR, we conduct experiments based on the movie recommendation application. Specifically, when an user (i.e., online item v) arrives, we recommend a movie (i.e.,\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\noffline item u) to this user and receive a reward based on the user-movie preference information. We choose the MovieLens dataset (Harper & Konstan, 2015), which provides a total of 3952 movies, 6040 users and 100209 ratings. We preprocess the dataset to sample movies and users randomly from the dataset to generate subgraphs, following the same steps as used by (Dickerson et al., 2019) and (Alomrani et al., 2021). In testing dataset, we empirically evaluate each algorithm using average reward (AVG) and competitive ratio (CR, against OPT), which represents the average performance and worst case performance, respectively. Thus, the value of CR is the empirically worst reward ratio in the testing dataset. For fair comparison, all the experimental settings like capacity cu follow those used in (Alomrani et al., 2021). More details about the problem setup and training details are deferred to Appendix A.\n\nBaseline Algorithms. We consider the following baselines. All the RL policies are trained offline with the same architecture and applicable hyperparameters.\n\n• OPT: The offline optimal oracle has the complete information about the bipartite graph. We\n\nuse the Gurobi optimizer to find the optimal offline solution.\n\n• Greedy: At each step, Greedy selects the available offline item with highest weight.\n\n• DRL: It uses the same architecture as in LOMAR, but does not consider online switching for training or inference. That is, the RL model is both trained and tested with ρ = 0. More specifically, our RL architecture has 3 fully connected layers, each with 100 hidden nodes.\n\n• DRL-OS (DRL-OnlineSwitching): We apply online switching to the same RL policy used by DRL during inference. That is, the RL model is trained with ρ = 0, but tested with a different ρ > 0.\n\nThe choice of baselines include all those considered in (Alomrani et al., 2021). In the no-free-disposal setting, the best competitive ratio is 0 in general adversarial cases (Mehta, 2013). Here, we use Greedy as the expert algorithm, because the recent study (Alomrani et al., 2021) has shown that Greedy performs better than other alternatives and is a strong baseline.\n\n6.2 RESULTS\n\nTest ρ = 0.4 ρ = 0.6 ρ = 0.8\n\nDRL-OS\n\nAVG 12.315 11.919 11.524\n\nCR 0.800 0.787 0.773\n\nLOMAR (ρ = 0.4) AVG 12.364 11.982 11.538\n\nCR 0.819 0.807 0.766\n\nLOMAR (ρ = 0.6) AVG 12.288 11.990 11.543\n\nCR 0.804 0.807 0.762\n\nLOMAR (ρ = 0.8) AVG 12.284 11.989 11.561\n\nCR 0.804 0.800 0.765\n\nGreedy\n\nAVG 11.000 11.000 11.000\n\nCR 0.723 0.723 0.723\n\nTable 1: Comparison under different ρ. In the top, LOMAR (ρ = x) means LOMAR is trained with the value of ρ = x. The average reward and competitive ratio are represented by AVG and CR, respectively — the higher, the better. The highest value in each testing setup is highlighted in bold. The AVG and CR for DRL are 12.909 and 0.544 respectively. The average reward for OPT is 13.209.\n\nWe show the comparison of LOMAR with baseline algorithms in Table 1. First, we see that DRL has the highest average reward, but its empirical competitive ratio is the lowest. The expert algorithm Greedy is fairly robust, but has a lower average award than RL-based policies. Second, DRL-OS can improve the competitive ratio compared to DRL. But, its RL policy is trained alone without being aware of the online switching. Thus, by making the RL policy aware of online switching, LOMAR can improve the average reward compared to DRL-OS. Specifically, by training LOMAR using the same ρ as testing it, we can obtain both the highest average cost and the highest competitive ratio. One exception is the minor decrease of competitive ratio when ρ = 0.8 for testing. This is likely due to the dataset and a few hard instances can affect the empirical competitive ratio, which also explains why the empirical competitive ratio is not necessarily monotonically increasing in the ρ ∈ [0, 1]. Nonetheless, unlike DRL that may only work well empirically without guarantees, LOMAR offers provable robustness guarantees while exploiting the power of RL to improve the average performance. The boxplots in Fig. 1 visualizes the reward ratio distribution of LOMAR, which further validates the importance of switching-aware training.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Testing with ρ = 0.4\n\n(b) Testing with ρ = 0.6\n\n(c) Testing with ρ = 0.8\n\nFigure 1: Boxplot for reward ratio with different ρ within testing dataset. Greedy and DRL-OS are also shown here for comparison. The best average performance in each figure is achieved by choosing the same ρ during training and testing.\n\n(a) ρ = 0.0 (i.e., DRL)\n\n(b) ρ = 0.4\n\n(c) ρ = 0.6\n\n(d) ρ = 0.8\n\nFigure 2: Histogram of bi-competitive reward ratios of DRL-OS (against Greedy and DRL) under different ρ.\n\nTo show the effect of switching with different ρ, we calculate the bi-competitive reward ratios. Specifically, for each problem instance, the bi-competitive ratio compares the actual reward against those of Greedy and RL model, respectively. To highlight the effect of online switching, we focus on DRL-OS (i.e., training the RL with ρ = 0) whose training process of RL model is not affected by ρ, because the RL model trained with ρ > 0 in LOMAR does not necessarily perform well on its own and the reward ratio of LOMAR to its RL model is not meaningful. The histogram of the bi-competitive ratios are visualized in Fig. 2. When ρ = 0, the ratio of DRL-OS/ DRL is always 1 unsurprisingly, since DRL-OS are essentially the same as DRL in this case (i.e., both trained and tested with ρ = 0). With a large ρ (e.g. 0.8) for testing, the reward ratios of DRL-OS/Greedy for most samples are around 1, which means the robustness is achieved, as proven by our theoretical analysis. But on the other hand, DRL-OS has limited flexibility and can less exploit the good average performance of DRL. Thus, the hyperparameter ρ ∈ [0, 1] governs the tradeoff between average performance and robustness relative to the expert and, like other hyperparameters, can be tuned to maximize the average performance subject to the robustness requirement.\n\nWe also consider a crowdsourcing application, as provided by the gMission dataset (Chen et al., 2014). Additional results for LOMAR and baselines in gMission are deferred to Appendix A.\n\n7 CONCLUSION\n\nIn this paper, we propose LOMAR to achieve both good average-case and good worst-case performance for edge-weighted online bipartite matching. LOMAR includes a novel online switching operation to decide whether to follow the expert’s decision or the RL decision for each online item arrival. We prove that for any ρ ∈ [0, 1], LOMAR is ρ-competitive against any expert online algorithms, which directly translates a bounded competitive ratio against OPT if the expert algorithm itself has one. We also train the RL policy by explicitly considering the online switching operation so as to improve the average performance. Finally, we run empirical experiments to validate LOMAR.\n\n9\n\n0.40.60.8DRL-OSGreedyTraining 0.700.750.800.850.900.951.00Reward Ratio0.40.60.8DRL-OSGreedyTraining 0.700.750.800.850.900.951.00Reward Ratio0.40.60.8DRL-OSGreedyTraining 0.700.750.800.850.900.951.00Reward Ratio0.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 200.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 200.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 200.80.91.01.11.21.3DRL-OS / GREEDY Reward Ratio0.80.91.01.1DRL-OS / DRL Reward Ratio 0 5 10 15 20Under review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nThe details of proving Theorem 4.1 are included in Appendix C. The experimental codes and settings are based on the open-sourced resources in Alomrani et al. (2021). The implementation of LOMAR mainly includes adding the switching condition for training and testing based on the standard DRL-based algorithm Alomrani et al. (2021) and will be released upon publication.\n\nREFERENCES\n\nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In\n\nInternational conference on machine learning, pp. 22–31. PMLR, 2017.\n\nGagan Aggarwal, Gagan Goel, Chinmay Karande, and Aranyak Mehta. Online vertex-weighted bipartite matching and single-bid budgeted allocations. In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms, pp. 1253–1264. SIAM, 2011.\n\nMohammad Ali Alomrani, Reza Moravej, and Elias B. Khalil. Deep policies for online bipartite matching: A reinforcement learning approach. CoRR, abs/2109.10380, 2021. URL https: //arxiv.org/abs/2109.10380.\n\nKeerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. A regression approach to learningaugmented online algorithms. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=GgS40Y04LxA.\n\nAntonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online\n\nmetric algorithms with untrusted predictions. In ICML, 2020a.\n\nAntonios Antoniadis, Themis Gouleakis, Pieter Kleer, and Pavel Kolev. Secretary and online matching problems with machine learned advice. Advances in Neural Information Processing Systems, 33: 7933–7944, 2020b.\n\nFelix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe modelbased reinforcement learning with stability guarantees. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS’17, pp. 908–919, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.\n\nNiv Buchbinder, Joseph Seffi Naor, et al. The design of competitive online algorithms via a primal– dual approach. Foundations and Trends® in Theoretical Computer Science, 3(2–3):93–263, 2009.\n\nWeirong Chen, Jiaqi Zheng, Haoyu Yu, Guihai Chen, Yixin Chen, and Dongsheng Li. Online learning bipartite matching with non-stationary distributions. ACM Trans. Knowl. Discov. Data, 16(5), mar 2022. ISSN 1556-4681. doi: 10.1145/3502734. URL https://doi.org/10.1145/ 3502734.\n\nZhao Chen, Rui Fu, Ziyuan Zhao, Zheng Liu, Leihao Xia, Lei Chen, Peng Cheng, Caleb Chen Cao, Yongxin Tong, and Chen Jason Zhang. gmission: A general spatial crowdsourcing platform. Proceedings of the VLDB Endowment, 7(13):1629–1632, 2014.\n\nJakub Chł ̨edowski, Adam Polak, Bartosz Szabucki, and Konrad Tomasz ̇Zołna. Robust learning-\n\naugmented caching: An experimental study. In ICML, 2021.\n\nNicolas Christianson, Tinashe Handina, and Adam Wierman. Chasing convex bodies and functions\n\nwith black-box advice. In COLT, 2022.\n\nNikhil R Devanur, Kamal Jain, and Robert D Kleinberg. Randomized primal-dual analysis of ranking for online bipartite matching. In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms, pp. 101–107. SIAM, 2013.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJohn P. Dickerson, Karthik Abinav Sankararaman, Aravind Srinivasan, and Pan Xu. Balancing relevance and diversity in online bipartite matching via submodularity. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’19/IAAI’19/EAAI’19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.33011877. URL https://doi.org/10.1609/aaai.v33i01. 33011877.\n\nBingqian Du, Chuan Wu, and Zhiyi Huang. Learning resource allocation and pricing for cloud profit maximization. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI’19/IAAI’19/EAAI’19. AAAI ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.33017570. URL https: Press, 2019. //doi.org/10.1609/aaai.v33i01.33017570.\n\nBingqian Du, Zhiyi Huang, and Chuan Wu. Adversarial deep learning for online resource allocation. ACM Trans. Model. Perform. Eval. Comput. Syst., 6(4), feb 2022. ISSN 2376-3639. doi: 10.1145/ 3494526. URL https://doi.org/10.1145/3494526.\n\nElbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning\" into learningaugmented algorithms for frequency estimation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 2860–2869. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/du21d.html.\n\nAdam N. Elmachtoub and Paul Grigas. Smart “predict, then optimize”. CoRR, abs/1710.08005, 2017.\n\nURL https://arxiv.org/abs/1710.08005.\n\nMatthew Fahrbach, Zhiyi Huang, Runzhou Tao, and Morteza Zadimoghaddam. Edge-weighted online bipartite matching. In 2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS), pp. 412–423, 2020. doi: 10.1109/FOCS46700.2020.00046.\n\nEvrard Garcelon, Mohammad Ghavamzadeh, Alessandro Lazaric, and Matteo Pirotta. Conservative exploration in reinforcement learning. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 1431–1441. PMLR, 26–28 Aug 2020.\n\nDobrik Georgiev and Pietro Lió. Neural bipartite matching. CoRR, abs/2005.11304, 2020. URL\n\nhttps://arxiv.org/abs/2005.11304.\n\nRishi Gupta and Tim Roughgarden. A pac approach to application-specific algorithm selection. SIAM\n\nJournal on Computing, 46(3):992–1017, 2017.\n\nRishi Gupta and Tim Roughgarden. Data-driven algorithm design. Commun. ACM, 63(6):87–94, May 2020. ISSN 0001-0782. doi: 10.1145/3394625. URL https://doi.org/10.1145/ 3394625.\n\nF. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History and context. ACM Trans. Interact. Intell. Syst., 5(4), dec 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/2827872.\n\nZhiyi Huang and Xinkai Shu. Online Stochastic Matching, Poisson Arrivals, and the Natural Linear Program, pp. 682–693. Association for Computing Machinery, New York, NY, USA, 2021. ISBN 9781450380539.\n\nZhiyi Huang, Zhihao Gavin Tang, Xiaowei Wu, and Yuhao Zhang. Online vertex-weighted bipartite matching: Beating 1-1/e with random arrivals. ACM Transactions on Algorithms (TALG), 15(3): 1–15, 2019.\n\nChinmay Karande, Aranyak Mehta, and Pushkar Tripathi. Online bipartite matching with unknown distributions. In Proceedings of the forty-third annual ACM symposium on Theory of computing, pp. 587–596, 2011.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nR. M. Karp, U. V. Vazirani, and V. V. Vazirani. An optimal algorithm for on-line bipartite matching. In Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computing, STOC ’90, pp. 352–358, New York, NY, USA, 1990. Association for Computing Machinery. ISBN 0897913612. doi: 10.1145/100216.100262. URL https://doi.org/10.1145/100216.100262.\n\nAbbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi Yadkori, and Benjamin Van Roy. Conservative contextual linear bandits. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper/2017/file/bdc4626aa1d1df8e14d80d345b2a442d-Paper.pdf.\n\nThomas Kesselheim, Klaus Radke, Andreas Tönnis, and Berthold Vöcking. An optimal online algorithm for weighted bipartite matching and extensions to combinatorial auctions. In European symposium on algorithms, pp. 589–600. Springer, 2013.\n\nGwang Kim and Ilkyeong Moon. Online advertising assignment problem without free disposal. Applied Soft Computing, 93:106370, 2020. ISSN 1568-4946. doi: https://doi.org/10.1016/j.asoc. 2020.106370. URL https://www.sciencedirect.com/science/article/pii/ S1568494620303100.\n\nNitish Korula and Martin Pál. Algorithms for secretary problems on graphs and hypergraphs. In International Colloquium on Automata, Languages, and Programming, pp. 508–520. Springer, 2009.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1179–1191. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 0d2b2061826a5df3221116a5085a6052-Paper.pdf.\n\nHeyuan Liu and Paul Grigas. Risk bounds and calibration for a smart predict-then-optimize method. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= pSitk34qYit.\n\nThodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned advice. J.\n\nACM, 68(4), July 2021.\n\nAranyak Mehta. Online matching and ad allocation. Foundations and Trends in Theoretical Computer\n\nScience, 8 (4):265–368, 2013. URL http://dx.doi.org/10.1561/0400000057.\n\nDaan Rutten, Nico Christianson, Debankur Mukherjee, and Adam Wierman. Online optimization with untrusted predictions. CoRR, abs/2202.03519, 2022. URL https://arxiv.org/abs/ 2202.03519.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1889–1897, Lille, France, 07–09 Jul 2015. PMLR.\n\nGarrett Thomas, Yuping Luo, and Tengyu Ma. Safe reinforcement learning by imagining the near future. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 13859–13869. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ 73b277c11266681122132d024f53a75b-Paper.pdf.\n\nHingfung Ting and Xiangzhong Xiang. Near optimal algorithms for online maximum weighted b-matching. In International Workshop on Frontiers in Algorithmics, pp. 240–251. Springer, 2014.\n\nKai Wang, Sanket Shah, Haipeng Chen, Andrew Perrault, Finale Doshi-Velez, and Milind Tambe. Learning MDPs from features: Predict-then-optimize for sequential decision making by reinforcement learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=-mGv2KxQ43D.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYansheng Wang, Yongxin Tong, Cheng Long, Pan Xu, Ke Xu, and Weifeng Lv. Adaptive dynamic bipartite graph matching: A reinforcement learning approach. In 2019 IEEE 35th International Conference on Data Engineering (ICDE), pp. 1478–1489, 2019. doi: 10.1109/ICDE.2019.00133.\n\nBryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decisionfocused learning for combinatorial optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 1658–1665, 2019.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\nYifan Wu, Roshan Shariff, Tor Lattimore, and Csaba Szepesvári. Conservative bandits. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pp. 1254–1262. JMLR.org, 2016.\n\nTsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based constrained policy optimization. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rke3TJrtPS.\n\nYunchang Yang, Tianhao Wu, Han Zhong, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric, Liwei Wang, and Simon Shaolei Du. A reduction-based framework for conservative bandits and reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=AcrlgZ9BKed.\n\nAli Zeynali, Bo Sun, Mohammad Hassan Hajiesmaili, and Adam Wierman. Data-driven competitive algorithms for online knapsack and set cover. In AAAI, 2021. URL https://ojs.aaai.org/ index.php/AAAI/article/view/17294.\n\nGoran Zuzic, Di Wang, Aranyak Mehta, and D. Sivakumar. Learning robust algorithms for online allocation problems using adversarial training. In https://arxiv.org/abs/2010.08418, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nIn the appendix, we show the experimental setup and additional results (Appendix A), algorithm details for the free-disposal setting (Appendix B), and finally the proof of Theorem 4.1 (Appendix C).\n\nA EXPERIMENTAL SETTINGS AND ADDITIONAL RESULTS\n\nOur implementation of all the considered algorithms, including LOMAR, is based on the source codes provided by Alomrani et al. (2021), which includes codes for training the RL model, data pre-proposing and performance evaluation. We conduct experiments on two real-world datasets: MovieLens Harper & Konstan (2015) and gMission Chen et al. (2014).\n\nA.1 MOVIELENS\n\nA.1.1 SETUP AND TRAINING\n\nWe first sample u0 movies from the original MovieLens dataset Harper & Konstan (2015). We then sample v0 users and make sure each user can get at least one movie; otherwise, we remove the users that have no matched movies, and resample new users. After getting the topology graph, we use Gurobi to find the optimal matching decision. In our experiment, we set u0 = 10 and v0 = 60 to generate the training and testing datasets. The number of graph instances in the training and testing datasets are 20000 and 1000, respectively. For the sake of reproducibility and fair comparision, our settings follows the same setup of Alomrani et al. (2021). In particular, the general movie recommendation problem belongs to online submodular optimization, but it can actually be equivalently mapped to edge-weighted online bipartite matching with no free disposal under the setting considered in Alomrani et al. (2021). So by default, the capacity cu for each offline node is set as 1 and wu,max = 5. While LOMAR can use any RL architecture, we follow the design of inv-ff-hist proposed by Alomrani et al. (2021), which empirically demonstrates the best performance among all the considered architectures.\n\nThe input to our considered RL model is the edge weights wuv revealed by the online items plus some historical information, which includes: Mean and variances of each offline node’s weights; Average degree of each offline nodes; Normalized step size; Percentage of offline nodes connected to the current node; Statistical information of these already matched nodes’ weights (maximum, minimum, mean and variance); Ratio of matched offline node; Ratio of skips up to now; Normalized reward with respect to the offline node number. For more details of the historical information, readers are referred to Table 1 in Alomrani et al. (2021).\n\nFor applicable algorithms (i.e., DRL, DRL-OS, and LOMAR), we train the RL model for 300 epochs in the training dataset with a batch size of 100. In LOMAR, the parameter B = 0 is used to follow the strict definition of competitive ratio. We test the algorithms on the testing dataset to obtain the average reward and the worst-case competitive ratio empirically. By setting ρ = 0 for training, LOMAR is equivalent to the vanilla inv-ff-hist RL model (i.e., DRL) used in Alomrani et al. (2021). Using the same problem setup, we can reproduce the same results shown in Alomrani et al. (2021), which reaffirms the correctness of our data generation and training process.\n\nFigure 3: Tail reward ratio comparison. In this experiment, we set ρ = 0.4 for DRLOS and LOMAR.\n\nAdditionally, training the RL model in LOMAR usually takes less than 8 hours on a shared research cluster with one NVIDIA K80 GPU, which is almost the same as the training the model for DRL in a standalone manner (i.e., setting ρ = 0 without considering online switching).\n\nA.1.2 ADDITIONAL RESULTS\n\nIn Table 1, we have empirically demonstrated that LOMAR achieves the best tradeoff between the average reward and competitive ratio. In Fig 3, we further demonstrate that LOMAR not only achieves\n\n14\n\n99.099.299.499.699.8100.0Percentile0.50.60.70.80.91.0Tail Reward RatioDRLLOMARDRL-OSGreedyUnder review as a conference paper at ICLR 2023\n\na better worst-case competitive ratio (at 100.0%). The tail reward ratio of LOMAR is also good compared to the baseline algorithms. Specifically, we show the percentile of reward ratios (compared to the optimal offline algorithm) — the 100% means the worst-case empirical reward ratio (i.e., competitive ratio). We see that DRL has a bad high-percentile reward ratio and lacks performance robustness, although its lower-percentile cost ratio is better. This is consistent with the good average performance of LOMAR. Because of online switching, both DRL-OS and LOMAR achieve better robustness, and LOMAR is even better due to its awareness of the online switching operation during its training process. The expert Greedy has a fairly stable competitive ratio, showing its good robustness. But, it can be outperformed by other algorithms when we look at lower-percentile reward ratio.\n\nA.1.3 RESULTS FOR ANOTHER EXPERT ALGORITHM\n\nOptimally competitive expert algorithms have been developed under the assumptions of random oder and/or i.i.d. rewards of different online items. In particular, by considering the random order setting, OSM (online secretary matching) has the optimal competitive ratio of 1/e (Kesselheim et al., 2013). Note that the competitive ratio for OSM is average over the random order of online items, while the rewards can be adversarially chosen. We show the empirical results in Fig. 4. As OSM skips the first |V|/e online items, it actually does not perform (in terms of the empirical worst-case cost ratio) as well as the default expert Greedy in our experiments despite its guaranteed competitive ratio against OPT. That said, we still observe the same trend as using Greedy for the expert: by tuning ρ ∈ [0, 1], LOMAR achieves a good average performance while guaranteeing the competitiveness against the expert OSM (and against OPT as OSM itself is optimally competitive against OPT).\n\n(a) Cost ratio against OPT\n\n(b) Cost ratio against OSM\n\nFigure 4: Cost ratio distribution (OSM as the expert)\n\nFig. 4 shows the empirical results in our testing dataset, which does not strictly satisfy the random order assumption required by OSM. Next, to satisfy the random order assumption, we select a typical problem instance and randomly vary the arrival orders of online items. We show the cost ratio averaged over the random arrival order in Table 2. Specifically, we calculate each cost ratio by 100 different random orders, and repeat this process 100 times. We show the mean and stand deviation of the average cost ratios (each averaged over 100 different random orders). We see that LOMAR improves the average cost ratio compared to OSM under the random order assumption. While DRL has a better average cost for this particular instance, it does not provide any guaranteed worst-case robustness as LOMAR.\n\nPure ML 0.9794 0.0074\n\nLOMAR ρ = 0.2 0.9688 0.0082\n\nLOMAR ρ = 0.4 0.9431 0.0078\n\nLOMAR ρ = 0.6 0.9095 0.0086\n\nLOMAR ρ = 0.8 0.8799 0.0084\n\nOSM 0.8459 0.0084\n\nMean Std\n\nTable 2: Cost ratio (averaged over the random arrival order) for a typical graph instance\n\nA.2 GMISSION\n\nThe gMission dataset Chen et al. (2014) considers a crowdsourcing application, where the goal is to assign the tasks (online items) to workers (offline items). The edge weight between a certain online task and each worker can be calculated by the product of the task reward and the worker’s success probability, which is determined by the physical location of workers and the type of tasks.\n\n15\n\nMethods0.50.60.70.80.91Reward RatioDRLLOMAR_0.2LOMAR_0.4LOMAR_0.6LOMAR_0.8OSMMethods0.50.70.91.11.31.5Reward RatioDRLLOMAR_0.2LOMAR_0.4LOMAR_0.6LOMAR_0.8Under review as a conference paper at ICLR 2023\n\nOur goal is to maximize the total reward given the capacity of each worker, which perfectly fits into our formulation in Eqn. equation 1.\n\nWe use the same data processing and RL architecture design as introduced in Section A.1.1. We train LOMAR with different ρ in the gMission dataset by setting u0 = 10, v0 = 60, wu,max = 1. Again, we use Greedy as the expert, which is an empirically strong baseline algorithm as shown in Alomrani et al. (2021). Our results are all consistent with those presented in Alomrani et al. (2021).\n\nA.2.1 TESTING ON 10 × 60\n\nIn our the first result, we generate a testing dataset with u0 = 10 and v0 = 60, which is the same setting as our training dataset. In other words, the training and testing datasets have similar distributions. Specifically, Greedy’s average reward and competitive ratio are 4.508 and 0.432, while these two values for DRL are 5.819 and 0.604, respectively. Thus, DRL performs outperforms Greedy in both average performance and the worst-case performance.\n\nρ in Testing 0.4 0.6 0.8 0.9\n\nDRL-OS\n\nAVG 5.553 5.389 5.102 4.836\n\nCR 0.599 0.591 0.543 0.495\n\nLOMAR ρ = 0.4 AVG 5.573 5.429 5.115 4.836\n\nCR 0.598 0.619 0.543 0.495\n\nLOMAR ρ = 0.6 AVG 5.553 5.420 5.111 4.839\n\nCR 0.598 0.619 0.523 0.495\n\nLOMAR ρ = 0.8 AVG 5.523 5.403 5.110 4.839\n\nCR 0.598 0.623 0.521 0.540\n\nLOMAR ρ = 0.9 AVG 5.535 5.402 5.107 4.839\n\nCR 0.598 0.623 0.521 0.540\n\nTable 3: Performance comparison in gMission 10 × 60 for different ρ. LOMAR with ρ = y means LOMAR is trained with ρ = y.\n\nNext, we show the results for LOMAR and DRL-OS under different ρ ∈ [0, 1] in Table 3. In general, by setting a larger ρ for inference, both LOMAR and DRL-OS are closer to the expert algorithm Greedy, because there is less freedom for the RL decisions. As a result, when ρ increases for inference, the average rewards of both DRL-OS and LOMAR decrease, although they have guaranteed robustness whereas DRL does not. Moreover, by training the RL model with explicit awareness of online switching, LOMAR can have a higher average cost than DRL-OS, which reconfirms the benefits of training the RL model by considering its downstream operation. Interestingly, by setting ρ identical for both training and testing, the average reward may not always be the highest for LOMAR. This is partially because of the empirical testing dataset. Another reason is that, in this test, DRL alone already performs the best (both on average and in the worst case). Hence, by setting a smaller ρ for inference, LOMAR works better empirically though it is trained under a different ρ. Nonetheless, this does not void the benefits of guaranteed robustness in LOMAR. The empirically better performance of DRL lacks guarantees, which we show as follows.\n\nA.2.2 TESTING ON 100 × 100\n\nIn our second test, we consider an opposite case compared to the first one. We generate a testing dataset with u0 = 100 and v0 = 100, which is different from the training dataset setting. As a result, the training and testing datasets have very different distributions, making DRL perform very badly. Specifically, Greedy’s average reward and competitive ratio are 40.830 and 0.824, and these two values for DRL are 32.938 and 0.576, respectively. DRL has an even lower average reward than Greedy, showing its lack of performance robustness.\n\nWe show the results for LOMAR and DRL-OS under different ρ ∈ [0, 1] in Table 4. In general, by setting a larger ρ for inference, both LOMAR and DRL-OS are closer to the expert algorithm Greedy. As Greedy works empirically much better than DRL in terms of the average performance and the worst-case performance, both LOMAR and DRL-OS have better performances when we increase ρ to let Greedy safeguard the RL decisions more aggressively. Moreover, by training the RL model with explicit awareness of online switching, LOMAR can have a higher average cost than DRL-OS, which further demonstrates the benefits of training the RL model by considering its downstream operation. Also, interestingly, by setting ρ identical for both training and testing, the average reward may not be the highest for LOMAR, partially because of the empirical testing dataset. Another reason is that, in this test, DRL alone already performs very badly (both on average and in the worst case)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nρ in Testing 0.4 0.6 0.8 0.9\n\nDRL-OS\n\nAVG 33.580 34.973 37.939 39.772\n\nCR 0.604 0.680 0.758 0.794\n\nLOMAR ρ = 0.4 AVG 37.030 37.490 38.866 40.057\n\nCR 0.707 0.731 0.775 0.803\n\nLOMAR ρ = 0.6 AVG 38.199 38.518 39.502 40.377\n\nCR 0.750 0.762 0.782 0.806\n\nLOMAR ρ = 0.8 AVG 38.324 38.505 39.385 40.239\n\nCR 0.750 0.756 0.794 0.812\n\nLOMAR ρ = 0.9 AVG 38.538 38.727 39.552 40.332\n\nCR 0.766 0.767 0.781 0.798\n\nTable 4: Performance comparison on gMission 100 × 100 for different ρ. LOMAR with ρ = y means LOMAR is trained with ρ = y.\n\ndue to the significant training-testing distributional discrepancy. Hence, by setting a higher ρ, LOMAR works better empirically though it is tested under a different ρ. An exception is when testing LOMAR with ρ = 0.9: setting ρ = 0.6/0.8 for training makes LOMAR perform slightly better in terms of the average performance and worst-case performance, respectively. But, setting ρ = 0.9 for training still brings benefits to LOMAR compared to DRL-OS that does not consider the downstream online switching operation.\n\nTo sum up, our experimental results under different settings demonstrate: LOMAR’s empirical improvement in terms of the average reward compared to DRL-OS; the improved competitive ratio of LOMAR and DRL-OS compared to DRL, especially when the training-testing distributions differ significantly; and the improved average reward of LOMAR compared to Greedy when RL is good. Therefore, LOMAR can exploit the power of RL while provably guaranteeing the performance robustness.\n\nB FREE DISPOSAL\n\nThe offline version of bipartite matching with free disposal can be expressed as:\n\nWith Free Disposal:\n\nmax\n\n(cid:88)\n\nxuv∈{0,1},u∈U\n\n(cid:32)\n\nmax S⊆Vu,|S|≤cu\n\n(cid:33)\n\n(cid:88)\n\nv∈S\n\nwuv\n\ns.t.\n\nVu = {v ∈ V | xuv = 1} ∀u ∈ U,\n\n(cid:88)\n\nu∈U\n\nxuv ≤ 1, ∀v ∈ V,\n\n(5)\n\nwhere Vu = {v ∈ V | xuv = 1} is the set of online items matched to u ∈ U and the objective maxS∈Vu,|S|≤cu\n\nv∈S xuvwuv indicates that only up to top cu rewards are counted for u ∈ U.\n\n(cid:80)\n\nIn the free-disposal setting, it is more challenging to design the switching conditions to guarantee the robustness. The reason is the additional flexibility allowed for matching decisions — each offline item u ∈ U is allowed to be matched more than cu times although only up to top cu rewards actually count Mehta (2013); Fahrbach et al. (2020). For example, even though LOMAR and the expert assign the same number of online items to an offline item u ∈ U and LOMAR is better than the expert at a certain step, future high-reward online items can still be assigned to u ∈ U, increasing the expert’s total reward or even equalizing the rewards of LOMAR and the expert (i.e., high-reward future online items become the top cu items for u ∈ U for both LOMAR and the expert). Thus, the temporarily “higher” rewards received by LOMAR must be hedged against such future uncertainties. Before designing our switching condition for the free-disposal setting, we first define the set containing the top cu online items for u ∈ U after assignment of v:\n\nEu,v(Vu,v) = arg\n\nmax E⊆Vu,v,|E|=cu\n\n(cid:88)\n\nv∈E\n\nwuv,\n\n(6)\n\nwhere Vu,v is the set of all online items matched to u ∈ U so far after assignment of item v ∈ V. When there are fewer than cu items in Vuv, we will simply add null items with reward 0 to Eu,v such that |Eu,v| = cu. We also sort the online items denoted as eu,i, for i = 1, · · · , cu, contained in Eu,v according to their weights in an increasing order such that wu,eu,1 ≤ · · · ≤ wu,eu,cu . Similarly, we define the same top-cu item set for the expert algorithm π by adding the superscript π.\n\nNext, we define the following value which indicates the maximum possible additional reward for the expert algorithm π if LOMAR simply switches to the expert and follows it for all the future steps\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 2 Inference of LOMAR (Free Disposal)\n\n1: Initialization: The actual set of items matched to u ∈ U is Vu,v after sequentially-arriving item v’s assignment with Vu,0 = ∅, the actual remaining capacity is bu = cu for u ∈ U, and the actual cumulative reward is R0 = (cid:80) u∈U fu(Vu,0) = 0. The same notations apply to the expert algorithm π by adding the superscript π. Competitive ratio requirement ρ ∈ [0, 1] and slackness B ≥ 0 with respect to the expert algorithm π.\n\n2: for v = 1 to |V| do 3: 4:\n\nRun the algorithm π and match the item v to u ∈ U based on the expert’s decision u = xπ v . Update the expert’s decision set and reward for offline item u = xπ v : (V π V π xπ xπ Update the expert’s cumulative reward Rπ for u in U do\n\n(cid:83){v} and fxπ\n\nv ,v). v = (cid:80)\n\nv ,v = V π\n\nu∈U fu\n\n= fxπ\n\nv ,v−1\n\nxπ\n\nv\n\nv\n\nCollect the available history information Iu about item u Run the RL model to get score: su = wuv − hθ(Iu, wuv) where θ is the network weight\n\nend for Calculate the probability of choosing each item u: {{ ̃su}u∈U } = softmax {{su}u∈U }. Obtain RL decision: ̃xv = arg maxu∈U (cid:83){skip} {{ ̃su}u∈U }. Find ∆f ̃xv in Eqn. equation 8 and G (cid:0) ̃xv, {Vu,v−1}u∈U , {V π if Rv−1 + ∆f ̃xv ≥ ρ (cid:0)Rπ u,v}u∈U\n\nu,v}u ∈ U(cid:1) in Eqn. equation 14\n\n(cid:1)(cid:1) − B then\n\nv + G (cid:0) ̃xv, {Vu,v−1}u∈U , {V π //Follow the ML action\n\nSelect xv = ̃xv.\n\n5: 6: 7: 8: 9: 10: 11:\n\n12:\n\nSelect xv = xπ v .\n\n//Follow the expert\n\nend if Update assignment and reward: Vxv,v = Vxv,v−1\n\n(cid:83){v} and Rv = (cid:80)\n\nu∈U fu(Vu,v)\n\nelse\n\n13: 14: 15: 16: 17: 18: 19: end for\n\nv + 1, v + 2, · · · :\n\nG (cid:0) ̃xv, {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) =\n\n\n\n(cid:88)\n\nu∈U\n\n max\n\ni=1,··· ,cu\n\ni (cid:88)\n\n(wu,eu,j − wu,eπ\n\nu,j\n\n\n\n+\n\n)\n\n\n\n,\n\n(7)\n\nj=1\n\nwhere eπ Vu,v−1\n\nu,j ∈ E π (cid:83){v} if ̃xv = u.\n\nu (V π\n\nu,v), and eu,j ∈ Eu( ̃Vu,v) in which ̃Vu,v = Vu,v−1 if ̃xv ̸= u and ̃Vu,v =\n\nThe interpretation is as follows. Suppose that LOMAR follows the RL decision for online item v. If it has a higher cumulative reward for the j-th item in the top-cu item set Eu,v than the expert algorithm π, then the expert can still possibly offset the reward difference wu,eu,j − wu,eπ by receiving a highreward future online item that replaces the j-th item for both LOMAR and the expert. Nonetheless, in the free-disposal model, the items in the top-cu set Eu,v are removed sequentially — the lowest-reward item will be first removed from the sorted set Eu,v, followed by the next lowest-reward item, and so on. Thus, in order for a high-reward item to replace the i-th item in the sorted set Eu,v, the first (i − 1) items have to be removed first by other high-reward online items. As a result, if LOMAR has a lower reward for the j-th item (for j ≤ i) in the top-cu item set Eu,v than the expert algorithm π, then it will negatively impact the expert’s additional reward gain in the future. Therefore, for item u ∈ U we only\n\nu,j\n\nneed to find the highest total reward difference, , that can be offset for the expert algorithm π by considering that i items are replaced by future high-reward (cid:80)i online items for i = 1, · · · , cu. If maxi=1,··· ,cu ) is negative (i.e., the expert algorithm cannot possibly gain higher rewards than LOMAR by receiving high-reward online items to replace its existing ones), then we use 0 as the hedging reward.\n\nj=1(wu,eu,j − wu,eπ\n\nj=1(wu,eu,j − wu,eπ\n\nmaxi=1,··· ,cu\n\nu,j\n\nu,j\n\n(cid:16)\n\n(cid:80)i\n\n(cid:17)+ )\n\nFinally, by summing up the hedging rewards for all the offline items u ∈ U, we obtain the total hedging reward in Eqn. equation 7. Based on this hedging reward, we have the condition (Line 28 in Algorithm 1 for LOMAR to follow the RL decision: Rv−1 + ∆f ̃xv ≥ (cid:1)(cid:1)−B, where ∆f ̃xv defined below is the additional reward ρ (cid:0)Rπ that would be obtained by following the RL decision:\n\nv + G (cid:0) ̃xv, {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n∆f ̃xv = f ̃xv (V ̃xv,v\n\n(cid:91)\n\n{v}) − f ̃xv (V ̃xv,v−1),\n\n(8)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3 Training LOMAR with Online Switching (No Free Disposal) Input: Available history information {Iu | u ∈ U}; set of weights for the online item arrival v , expert’s set of items u,v; up to the online node v − 1, the actual cumulative reward Rv−1, the\n\n{wuv, u ∈ U}; up to the online item v, the expert’s cumulative reward Rπ matched to u ∈ U is V π actual set of items matched to u ∈ U is Vu,v−1;\n\nsu = wuv − hθ(Iu, wuv)\n\n1: for u in Ua do 2: 3: end for 4: Calculate the probabilities of selecting an available item or skip:\n\n{{ ̃su}u∈Ua , ̃sskip} = softmax {{su}u∈Ua , 0}.\n\n5: Obtain the RL action: ̃xv = arg maxUa 6: Calculate Rdif f based on the switching condition\n\n(cid:83){skip} {{ ̃su}u∈Ua , ̃sskip}.\n\n(cid:16)\n\nRdif f = Rv−1 + w ̃xv,v − ρ\n\nRπ\n\nv + (cid:80)\n\nu∈U\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v| + Iu= ̃xv\n\n(cid:1)+\n\n· wu,max\n\n7: if xπ\n\nv ∈ Ua then The probability of each action is\n\n8:\n\n(cid:110)\n\n{ ̃sπ\n\nu}u∈Ua , ̃sπ\n\nskip\n\n9: else\n\n10:\n\nThe probability of each action is\n\n(cid:110)\n\n{ ̃sπ\n\nu}u∈Ua, ̃sπ\n\nskip\n\n(cid:111)\n\n(cid:111)\n\n= {{1}u=xπ\n\nv\n\n, {0}u̸=xπ\n\nv ,skip}\n\n= {{0}u∈Ua, 1}\n\n(cid:17)\n\n+ B\n\n11: end if 12: With online switching, the probabilities of following RL or expert are\n\n{ ̃sOS, ̃sπ\n\nOS} = softmax{Rdif f /t, 0}\n\n13: Calibrated probabilities of choosing an available offline item u or skip are (cid:111) .\n\n{{ˆsu}u∈Ua , ˆsskip} = ̃sOS · {{ ̃su}u∈Ua , ̃sskip} + ̃sπ\n\nu}u∈Ua , ̃sπ\n\n{ ̃sπ\n\nOS ·\n\nskip\n\n(cid:110)\n\nin which fu = fu(V ′) = maxS∈V ′,|S|≤cu v∈S wuv is the reward function for an offline item u ∈ U in the free-disposal model. The condition means that if LOMAR can maintain the competitive ratio ρ against the expert algorithm π by being able to hedge against any future uncertainties even in the worst case, then it can safely follow the RL decision ̃xv at step v.\n\n(cid:80)\n\nTraining with free disposal. The training process for the free-disposal setting is the same as that for the no-free-disposal setting, except for Line 6 of Algorithm 3 in which we need to modify Rdif f based on the switching condition (i.e., Line 13 of Algorithm 2) for the free-disposal setting.\n\nC PROOF OF THEOREM 4.1\n\nThe key idea of proving Theorem 4.1 is to show that there always exist feasible actions (either following the expert or skip) while being able to guarantee the robustness if we follow the switching condition. Next, we prove Theorem 4.1 for the no-free-disposal and free-disposal settings, respectively.\n\nC.1 NO FREE DISPOSAL\n\nDenote Vu,v as the actual set of items matched to u ∈ U after making decision for v. Denote V π the expert’s set of items matched to u ∈ U. We first prove a technical lemma.\n\nu,v as\n\n(cid:16)\n\nLemma C.1. Assuming that the robustness condition is met after making the decision for v − 1,\n\nRπ\n\nv−1 + (cid:80)\n\ni.e. Rv−1 ≥ ρ arrives and the expert’s decision xπ (cid:0)|Vu,v| − |V π\n\nRv ≥ ρ\n\nv + (cid:80)\n\nRπ\n\nu∈U\n\nu,v−1|(cid:1)+\n\n(cid:0)|Vu,v−1| − |V π If at the step when v v is not available for matching, then xv = skip always satisfies u,v|(cid:1)+\n\n· wu,max\n\n· wu,max\n\n− B.\n\n− B.\n\n(cid:16)\n\n(cid:17)\n\nu∈U\n\n(cid:17)\n\nProof. If the item xπ which means |Vxπ xv = skip, we have Rv = Rv−1 and Vu,v = Vu,v−1,\n\nv is not available for matching, it must have been consumed before v arrives, v either). Since ∀u ∈ U. Then, by the robustness assumption\n\nv ,v−1| ≥ 1 (since otherwise the expert cannot choose xpi\n\nv ,v−1| − |V π\n\nxπ\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nof the previous step, we have\n\n(cid:32)\n\nRv = Rv−1 ≥ρ\n\nRπ\n\nv−1 +\n\n(cid:88)\n\nu∈U\n\n(cid:32)\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v−1|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\n≥ρ\n\nRπ\n\nv−1 + wxπ\n\nv ,v − wxπ\n\nv ,max +\n\n(cid:0)|Vu,v−1| − |V π\n\nu,v−1|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\n(9)\n\n(cid:88)\n\nu∈U\n\n(cid:32)\n\n=ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v| − |V π\n\nu,v|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\nwhere the last equality holds because (|Vu,v| − |V π and (|Vu,v| − |V π\n\nu,v|)+ − (|Vu,v−1| − |V π\n\nu,v−1|)+ = 0 otherwise.\n\nu,v|)+ − (|Vu,v−1| − |V π\n\nu,v−1|)+ = −1 if u = xπ v ,\n\nWe next prove by induction that the condition\n\n(cid:32)\n\nRv ≥ ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,v| − |V π\n\nu,v|(cid:1)+\n\n(cid:33)\n\n· wu,max\n\n− B\n\n(10)\n\nholds for all steps by Algorithm 1.\n\n(cid:1) − B, we select the RL At the first step, if ̃xv is not the same as xπ decision xv = ̃xv, and the robustness condition equation 10 is satisfied. Otherwise, we select the expert action xv = xπ v ,v − B holds when ρ ≤ 1 and B ≥ 0.\n\nv and the condition still holds since Av = Cv and wxv,v ≥ ρwxπ\n\nv and w ̃xv,v ≥ ρ (cid:0)wxπ\n\nv ,v + wu,max\n\nThen, assuming that the robustness condition in equation 10 is satisfied after making the decision for v − 1, we need to prove it is also satisfied after making the decision for v. If the condition in equation 3 in Algorithm 1 is satisfied, then xv = ̃xv and so equation 10 holds naturally. Otherwise, if the expert action xπ v . Then, we ∀u ∈ U, and hence the have wxv,v ≥ ρwxπ condition equation 10 still holds. Other than these two cases, we also have the option to “skip”, i.e. xv = skip. By Lemma C.1, the condition equation 10 still holds. Therefore, we prove that the condition equation 10 holds for every step.\n\nv is available for matching, then we select expert action xv = xπ u,v| = |Vu,v−1| − |V π\n\nv ,v − B and |Vu,v| − |V π\n\nu,v−1|,\n\nAfter the last step v = |V|, we must have\n\n(cid:32)\n\nRv ≥ ρ\n\nRπ\n\nv +\n\n(cid:88)\n\nu∈U\n\n(cid:0)|Vu,ˆv| − |V π\n\nu,ˆv|(cid:1)+\n\n· wu,max\n\n(cid:33)\n\n− B ≥ ρRπ\n\nv − B\n\n(11)\n\nwhere Rv and Rπ v = |V|, respectively. This completes the proof for the no-free-disposal case.\n\nv are the total rewards of LOMAR and the expert algorithm π after the last step\n\nC.2 WITH FREE DISPOSAL\n\nWe now turn to the free-disposal setting which is more challenging than the no-free-disposal setting because of the possibility of using future high-reward items to replace existing low-reward ones.\n\nWe first denote ∆fxπ\n\nv as the actual additional reward obtained by following the expert’s decision xπ v ,\n\n∆fxπ\n\nv\n\n= fxπ\n\nv\n\n(Vxπ\n\nv ,v\n\n(cid:91)\n\n{v}) − fxπ\n\n(Vxπ\n\nv ,v−1),\n\nv\n\nAdditionally, we denote ∆f π xπ v\n\nas the expert’s additional reward of choosing xπ\n\nv , where\n\n∆f π xπ v\n\n= fxπ\n\nv\n\n(V π xπ\n\nv ,v\n\n(cid:91)\n\n{v}) − fxπ\n\nv\n\n(V π xπ\n\nv ,v−1).\n\nFor presentation convenience, we rewrite the hedging reward as ̃G (cid:0){Vu,v}u∈U , {V π u,v}u∈U \n\n\n\n+\n\n ̃G (cid:0){Vu,v}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) =\n\n(cid:88)\n\nu∈U\n\n max\n\ni=1,··· ,cu\n\ni (cid:88)\n\nj=1\n\n(wu,eu,j − wu,eπ\n\nu,j\n\n) \n\n,\n\n(14)\n\n(12)\n\n(13)\n\n(cid:1) as\n\nwhere eπ\n\nu,j ∈ E π\n\nu (V π\n\nu,v), eu,j ∈ Eu(Vu,v), and Eu is defined in Eqn. equation 6.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nLemma C.2. Assuming that the robustness condition is met after making the decision for v − 1, i.e.\n\n(cid:16)\n\nRv−1 ≥ ρ G (cid:0)xπ\n\nv−1 + ̃G (cid:0){Vu,v−1}u∈U , {V π u,v}u∈U\n\nu,v−1}u∈U (cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nv , {Vu,v−1}u∈U , {V π\n\nRπ\n\n(cid:1)(cid:17)\n\nu,v−1}u∈U\n\n(cid:1).\n\n− B. At step v, we have ∆fxπ\n\n− ∆f π xπ v\n\n≥\n\nv\n\nProof. We begin with “G (cid:0)xπ Lemma C.2. By definition, it can be written as\n\nv , {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)” in\n\nv , {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\nG (cid:0)xπ \n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π \n\n\n\n+\n\nu,v−1}u∈U\n\n(cid:1)\n\n=\n\n max\n\ni=1,··· ,cu\n\n(wu,ˆeu,j − wu,ˆeπ\n\nu,j\n\n) \n\n−\n\n max\n\ni=1,··· ,cu\n\n(wu,eu,j − wu,eπ\n\nu,j\n\ni (cid:88)\n\nj=1\n\ni (cid:88)\n\nj=1\n\n\n\n+\n\n) \n\n(15)\n\nwhere u = xπ E π\n\nu (V π\n\nu,v−1), and eu,j ∈ Eu(Vu,v−1).\n\nv , ˆeπ\n\nu,j ∈ E π\n\nu (V π\n\nu,v−1\n\n(cid:83){v}), and ˆeu,j ∈ Eu(Vu,v−1\n\n(cid:83){v}). Besides, eπ\n\nu,j ∈\n\nTo prove the lemma, we consider four possible cases for wu,v to cover all the cases.\n\n(cid:83){v}) and v /∈ Eu(V π\n\nCase 1: If the reward for v is small enough such that wu,v < wu,eu,1 and wu,v < wu,eπ u,1 , then = 0, since both the v /∈ Eu(Vu,v−1 expert and LOMAR cannot gain any reward from the online item v. From Eqn. equation 15, we can find that the right-hand side is also 0. Therefore, the conclusion in Lemma C.2 holds with the equality activated.\n\n(cid:83){v}). Then we have ∆fxπ\n\n= ∆f π xπ v\n\nu,v−1\n\nv\n\nu,v−1}u∈U )+ ≥ wu,eu,1 − wu,eπ\n\nCase 2: If the reward for v is large enough such that wu,v > wu,eu,1 and wu,v > wu,eπ v ∈ Eu(Vu,v−1 reward item eu,1 /∈ Eu(Vu,v−1\n\nu,1, then (cid:83){v}). In other words, we will remove the smallestu,1 /∈ Eu(V π (cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\n(cid:83){v}) and v ∈ Eu(V π\n\n(cid:1) ≤ −wu,eu,1 + wu,eπ\n\nv , {Vu,v−1}u∈U , {V π\n\n(cid:83){v}) and eπ\n\n(cid:83){v}). Then\n\nu,v}u∈U\n\nG (cid:0)xπ\n\nu,v−1\n\nu,v−1\n\nu,1\n\nThe inequality holds because (wu,eu,1 − wu,eπ wu,v − wu,eu,1 and ∆f π\n\n= wu,v − wu,eπ\n\nu,1\n\nxπ v\n\nu,1. Therefore, the conclusion in Lemma C.2 holds.\n\nu,1. In this case, ∆fxπ\n\nv\n\n=\n\nu,v−1\n\n(cid:83){v}) u,1 , then v ∈ Eu(Vu,v−1 Case 3: If the reward for v satisfies wu,v ≥ wu,eu,1 and wu,v ≤ wu,eπ (cid:83){v}) (i.e., the online item v (cid:83){v}). In other words, even if v ∈ Eu(Vu,v−1 and v /∈ Eu(V π produces additional rewards for LOMAR), the reward of v is still smaller than the smallest reward for (cid:1) = the expert. Then, we have G (cid:0)xπ 0. In this case, ∆fxπ = 0. Therefore, the conclusion in Lemma C.2 still holds.\n\nv , {Vu,v−1}u∈U , {V π = wu,v − wu,eu,1 ≥ 0 and ∆f π\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v}u∈U xπ v\n\nu,v−1}u∈U\n\nv\n\nCase 4: If the reward for v satisfies wu,v ≤ wu,eu,1 and wu,v ≥ wu,eπ u,1, then in this case, only the current smallest-reward item is replaced with v for the expert, while the reward of LOMAR remains unchanged. Thus, we have\n\nG (cid:0)xπ\n\nv , {Vu,v−1}u∈U , {V π = 0 and ∆f π xπ v\n\nv\n\nIn this case, ∆fxπ with the equality activated.\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1) = wu,eπ\n\nu,1\n\n− wu,v.\n\n= wu,v − wu,eπ\n\nu,1. Then the conclusion in Lemma C.2 still holds\n\nWe next prove by induction that the condition\n\n(cid:16)\n\nRv ≥ ρ\n\nRπ\n\nv + ̃G (cid:0){Vu,v}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1)(cid:17)\n\n− B\n\n(16)\n\nholds for all steps by Algorithm 1.\n\n(cid:1) = 0, and it At the first step, by using xv = xπ is obvious that the condition in equation 16 is satisfied. Thus, there is at least one solution xv = xπ for our robustness condition in equation 16.\n\nv and ̃G (cid:0){Vu,v}u∈U , {V π\n\nv , we have Rv = Rπ\n\nu,v}u∈U\n\nv\n\nStarting from the second step, assume that after the step v − 1, we already have\n\n(cid:16)\n\nRv−1 ≥ ρ\n\nRπ\n\nv−1 + ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n− B\n\n(17)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nIf the condition in Line 28 of Algorithm 1 is already satisfied, we can just use xv = ̃xv, which directly satisfies equation 16. Otherwise, we need to follow the expert by setting xv = xπ v . We prove xv = xπ\n\nv satisfies the robustness condition at any step v.\n\nFrom Lemma C.2, since 0 ≤ ρ ≤ 1 and ∆fxπ\n\nv\n\n≥ 0 we have\n\n∆fxπ\n\nv\n\n≥ ρ\n\n(cid:16)\n\n∆f π xπ v\n\n+ G (cid:0)xπ\n\nv , {Vu,v−1}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n.\n\nThen, by substituting it back to Eqn. equation 17, we have\n\nRv−1 + ∆fxπ\n\nv\n\n≥ρ\n\nu,v}u∈U\n\n(cid:1) − ̃G (cid:0){Vu,v−1}u∈U , {V π\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n(cid:16)\n\n∆f π xπ v\n\n(cid:16)\n\n+ ρ\n\nRπ\n\n+ G (cid:0)xπ\n\nv , {Vu,v−1}u∈U , {V π v−1 + ̃G (cid:0){Vu,v−1}u∈U , {V π + G (cid:0)xπ\n\n(cid:16)\n\n(cid:16)\n\n=ρ\n\n=ρ\n\nRπ\n\nxπ v\n\nv−1 + ∆f π v + ̃G (cid:0){Vu,v}u∈U , {V π\n\nv , {Vu,v−1}u∈U , {V π (cid:1)(cid:17)\n\nu,v}u∈U\n\n− B.\n\nRπ\n\nu,v}u∈U\n\nu,v−1}u∈U\n\n(cid:1)(cid:17)\n\n− B\n\n(cid:1)(cid:17)\n\n− B\n\n(18)\n\nTherefore, after the last step v, LOMAR must satisfy\n\n(cid:16)\n\nRv ≥ ρ\n\nRπ\n\nv + ̃G (cid:0){Vu,v}u∈U , {V π\n\nu,v}u∈U\n\n(cid:1)(cid:17)\n\n− B ≥ ρRπ\n\nv − B,\n\nwhere Rv and Rπ v = |V|, respectively. Thus, we complete the proof for the free-disposal setting.\n\nv are the total rewards of LOMAR and the expert algorithm π after the last step\n\n22",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes a RL-based approach for online weighted bipartite matching. A key novelty of the paper is to augment the decisions of the RL agent with a classic online algorithm to obtain robustness guarantees. Upon the arrival of an online vertex, the algorithm queries both a base online algorithm (called expert) and the RL agent. It follows the action of the RL agent unless doing so has the possibility that the algorithm incurs significant loss with respect to the expert.\n\n# Strength And Weaknesses\n\nStrengths:\n+ Unlike most work on learning augmented algorithms that aim to augment online algorithms with some predictions / advice from ML models; the paper takes the complementary view of an online algorithm used as a safety railing to guide a ML model.\n\nWeaknesses:\n- The model feels too brittle - in the general case when the edge weights are unknown - the algorithm essentially only follows the decisions of the expert and can not use the RL agent at all. Maybe a restriction to a more specific problem would yield better insights.\n\n- It’s not clear what the focus of the paper is. Section 4 is concerned with how to use the RL agent’s decisions while maintaining robustness against a fixed expert. It would be better to restructure this as a meta-algorithm that takes in two algorithms A (expert) and B (RL agent): The meta algorithm switches between the actions of A and B and guarantees to maintain a given robustness wrt A while maximizing the number of times it uses actions of B. (Note that this section is completely independent of RL). Indeed, viewed in this light, the meta-algorithm and its analysis is almost trivial. [ Also worth referring to “combining” algorithms for paging and metrical task systems (e.g. Fiat et al)]. \n\n- Section 5 then deals with the challenges of training the RL agent when used along with the switching algorithm above. I would much rather see more time and space allotted to this section and clarify the training process in more details.\n\n- The empirical section includes preliminary experiments and does not demonstrate strong positive results.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI found the paper a bit hard to read and follow - as I mentioned earlier in the review, in my view, the paper focused on the wrong aspects of the work. The switching framework itself is not particularly novel within the online algorithms and is indeed the first thing one would think of when faced with such a question.\n\n# Summary Of The Review\n\nThe paper proposes a framework to use an expert algorithm to guide the decisions of an RL agent for online bipartite matching. I like the problem direction but find the current paper version a bit lacking with respect to details of (i) RL training procedure, (ii) Difficulty in naively adapting standard RL techniques, (iii) thorough empirical evaluation.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper presents LOMAR (Learning-based approach to edge-weighted Online bipartite MAtching with Robustness guarantees), a novel method to tackle the problem of online bipartite matching, particularly in applications like online ad displays. It innovatively combines reinforcement learning (RL) with traditional expert algorithms through a dynamic online switching mechanism that selects between RL recommendations and expert decisions based on future uncertainties. The theoretical contributions include establishing LOMAR's competitive ratio of ρ against expert algorithms, providing robustness guarantees while achieving strong average performance. Empirical results demonstrate that LOMAR significantly outperforms existing baseline algorithms in both average reward and competitive ratio.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive approach to bridging the gap between robust performance and average-case improvement in online bipartite matching. The rigorous theoretical analysis and the introduction of the switching mechanism are significant contributions to the field, ensuring that the method is both practical and theoretically sound. However, the paper could be improved by providing more extensive experimental results across a wider range of datasets and settings to better generalize the findings. The reliance on a specific application (movie recommendation) may limit the perceived versatility of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methods, and results. The quality of writing is high, with a logical flow that aids comprehension. The novelty of the approach is evident in the integration of RL with expert algorithms and the innovative switching mechanism. Reproducibility is supported by detailed descriptions of the experimental setup and a commitment to releasing the implementation upon publication, which is commendable.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of online bipartite matching by combining reinforcement learning with expert strategies to achieve robust performance. While the theoretical contributions are strong, the empirical validation could be expanded to enhance the generalizability of the results.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees\" (LOMAR) addresses the challenge of online bipartite matching, which is crucial in various applications such as online advertising and scheduling. The authors propose LOMAR, a novel method that combines expert algorithms with reinforcement learning (RL) to achieve both good average performance and robustness against adverse outcomes. LOMAR introduces an online switching mechanism that decides between expert and RL strategies based on specific conditions, ensuring theoretical robustness guarantees. The experimental results demonstrate that LOMAR consistently outperforms baseline algorithms, achieving superior average rewards while maintaining competitive ratios across multiple datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of robustness guarantees that enhance the reliability of the proposed method compared to traditional RL approaches, which typically lack such guarantees. The integration of dynamic decision-making through the online switching operation is a notable contribution, allowing LOMAR to adaptively leverage expert knowledge alongside learned policies. Empirical validation on real-world datasets (MovieLens and gMission) showcases consistent performance improvements over baseline algorithms, reinforcing the practical applicability of LOMAR.\n\nHowever, the paper also presents some weaknesses. The complexity involved in implementing the proposed method, due to the integration of expert algorithms with RL and the design of switching conditions, may pose challenges in practical settings. Additionally, LOMAR's competitive performance is heavily reliant on the quality of the selected expert algorithm; if the expert performs poorly, the effectiveness of LOMAR may be compromised. Lastly, while the findings are validated in specific applications, the generalizability of LOMAR to other domains remains to be fully explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem addressed, the proposed methodology, and the experimental results. The quality of the writing is high, with sufficient detail provided in both the methodological and experimental sections. The novelty lies in the combination of expert algorithms with RL in a way that guarantees robustness, which is not commonly addressed in existing literature. Reproducibility is somewhat facilitated by the clear description of datasets used and the experimental setup; however, additional details regarding the implementation of LOMAR could enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in online bipartite matching by effectively merging expert algorithms with reinforcement learning to achieve both robust performance and theoretical guarantees. While the approach shows promise and performs well empirically, the complexities of implementation and dependence on expert performance may limit its broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a reinforcement learning-based approach to edge-weighted online bipartite matching, which addresses the challenges of sequentially revealing online item information and making irreversible matching decisions. LOMAR integrates a novel online switching mechanism that allows the algorithm to choose between expert and RL decisions, ensuring robust performance with both average-case and worst-case guarantees. Theoretical results demonstrate that LOMAR achieves a competitive ratio against expert algorithms, and empirical experiments on a movie recommendation dataset show that it outperforms existing baselines in terms of both average rewards and competitive ratios.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to combining expert algorithms with reinforcement learning, which effectively addresses the limitations of existing methods in terms of robustness and average performance. The theoretical grounding provided for the switching mechanism adds credibility to the proposed method. However, a notable weakness is the reliance on specific assumptions in the theoretical framework, such as the conditions for the switching mechanism, which may limit its applicability in more diverse scenarios. Additionally, while the empirical evaluation shows promising results, further testing on varied datasets would strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem statement, methodology, and findings. The theoretical foundations are presented in a logical manner, supported by relevant equations and theorems. The quality of writing is high, making complex concepts accessible. The novelty of integrating online switching with RL in the context of bipartite matching is significant. However, the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter tuning processes, as well as making the code publicly available.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in the field of online bipartite matching by combining expert algorithms with reinforcement learning through a novel switching mechanism. While the theoretical and empirical results are promising, further validation across diverse contexts would be beneficial for establishing broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LOMAR, a novel reinforcement learning (RL)-based method designed for edge-weighted online bipartite matching. The main contributions include a unique dynamic switching mechanism for decision-making, theoretical robustness guarantees against expert algorithms, and a comprehensive training methodology that incorporates switching-aware training. Empirical experiments conducted on datasets such as MovieLens demonstrate that LOMAR outperforms existing baseline algorithms in terms of average rewards and competitive ratios, suggesting its effectiveness in practical applications.\n\n# Strength And Weaknesses\nThe contributions of the paper are significant, particularly in its introduction of LOMAR, which addresses a gap in the literature by combining expert algorithm performance with RL methods. The robustness guarantees of LOMAR (ρ-competitiveness) provide a much-needed theoretical foundation, although the trade-offs between robustness and average performance remain under-explored. The dynamic switching mechanism enhances flexibility but may introduce computational complexity. While empirical validation shows promising results, the limited dataset scope necessitates further testing across diverse conditions. The training methodology effectively mitigates the \"no supervision\" trap, yet the reliance on expert decisions could hinder the RL’s exploratory capabilities. Overall, the paper is well-structured and clear, though certain complex mathematical proofs could benefit from more intuitive presentations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-organized, effectively presenting its problem formulation, algorithm design, and theoretical analysis. However, the paper could enhance clarity by simplifying some of the mathematical proofs for a broader audience. The novelty lies in its hybrid approach that synergizes RL with expert knowledge, which is a significant contribution to the field. The reproducibility statement is a positive aspect, emphasizing the availability of experimental codes and settings; however, transparency about implementation details and dataset availability is crucial for ensuring reproducibility.\n\n# Summary Of The Review\nIn summary, the paper presents a notable advancement in online bipartite matching through the introduction of LOMAR, which effectively integrates reinforcement learning and expert algorithms. While the method shows promise and theoretical robustness, further empirical validation across diverse datasets and clearer explanations of complex concepts would strengthen the paper's overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "## ICLR Review: Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees\n\n### Summary Of The Paper\nThis paper introduces LOMAR (Learning-based Online Matching with Robustness), a novel algorithm designed for online bipartite matching that integrates reinforcement learning (RL) techniques with expert algorithms. The key contributions of the paper include a robust framework that provides performance guarantees against expert algorithms, an innovative dynamic decision-making process that adapts based on risk assessment of future rewards, and extensive empirical validation demonstrating LOMAR's superiority over existing methods. The findings indicate that LOMAR achieves both average-case and worst-case performance improvements in various datasets, positioning it as a strong candidate for real-world applications in online matching scenarios.\n\n### Strength And Weaknesses\nThe strengths of the paper lie in its solid theoretical foundation and the introduction of robustness guarantees in adversarial settings, which significantly enhance the reliability of RL methods in online matching. The dynamic decision-making process is a notable innovation that adds depth to the algorithm. However, the complexity of the switching mechanism could hinder practical implementation, particularly in high-dimensional spaces, which may limit its applicability in some scenarios. Additionally, the paper could benefit from further exploration of generalization to other online matching problems beyond bipartite graphs.\n\n### Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodologies, and experimental results. The theoretical guarantees are articulated clearly, making the claims about robustness and performance comprehensible. However, the complexity of the proposed switching conditions may challenge reproducibility, particularly for practitioners aiming to implement LOMAR in real-time applications. The novelty of combining RL with expert algorithms while ensuring robustness is commendable and adds substantial value to the existing literature.\n\n### Summary Of The Review\nOverall, LOMAR represents a significant advancement in online bipartite matching, successfully merging the strengths of expert decision-making with reinforcement learning while providing robustness guarantees. Despite some concerns regarding implementation complexity and generalization, the theoretical and empirical contributions are impressive and contribute meaningfully to the field.\n\n### Correctness\n5\n\n### Technical Novelty And Significance\n4\n\n### Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces LOMAR (Learning-based Optimization for Machine-learning Adversarial Robustness), a novel adversarial training framework that integrates reinforcement learning (RL) to enhance robustness against adversarial attacks while optimizing average performance metrics. The authors present an online switching mechanism that allows the model to dynamically decide between following adversarial training decisions and more conservative expert decisions, thereby mitigating uncertainties from adversarial inputs. The empirical results demonstrate that LOMAR provides strong robustness guarantees and significantly outperforms existing adversarial training methods on benchmark datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining adversarial training with reinforcement learning, which is a significant departure from traditional methods that often focus solely on robustness without considering performance. The online switching mechanism is another notable contribution, allowing for adaptability in decision-making under adversarial conditions. However, the paper could benefit from a more detailed explanation of the tuning process for the hyperparameter ρ, as this is crucial for practitioners aiming to apply the model effectively. The theoretical guarantees provided are compelling, but the paper could further emphasize the practical implications of these results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, making it accessible to readers with a background in machine learning and adversarial robustness. The quality of writing is high, with appropriate use of terminology and concise explanations of complex concepts. The novelty of the approach is substantial, as it blends reinforcement learning with adversarial training in an innovative way. The reproducibility of the results appears strong, given the detailed description of the methodology and the extensive empirical validation across multiple datasets.\n\n# Summary Of The Review\nThe paper makes a significant contribution to the field of adversarial training by introducing LOMAR, which effectively combines reinforcement learning with robustness guarantees. While the approach is novel and backed by empirical results, further clarification on hyperparameter tuning could enhance its practical applicability. Overall, LOMAR represents a promising direction for future research in adversarial robustness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel algorithm designed for edge-weighted online bipartite matching that promises robustness guarantees against expert algorithms. It introduces a unique online switching mechanism that allows for dynamic decision-making between reinforcement learning (RL) and expert strategies. The authors claim significant improvements in average-case performance and robustness, providing both empirical experiments and a theoretical framework to support their claims.\n\n# Strength And Weaknesses\nThe primary strength of LOMAR lies in its purported robustness guarantees, which claim to enhance its ρ-competitiveness compared to traditional expert algorithms. The innovative online switching operation is another noteworthy contribution, as it attempts to integrate RL into the decision-making process effectively. However, the weaknesses include an overstatement of the contributions and empirical results, with marginal gains over existing methods and statistical significance that may not support the claims of revolutionary improvements. Furthermore, while the theoretical analysis is extensive, it relies on well-established concepts that do not necessarily indicate a groundbreaking advance in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. However, the novelty of the contributions is overstated, and the claims made regarding performance improvements may mislead readers into perceiving them as more significant than they are. The reproducibility of the results appears to be feasible, as empirical validation is provided, yet the modest nature of the improvements could affect the perceived impact of the work.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting algorithm with claims of robustness and improved performance in online bipartite matching, the novelty and significance of its contributions are overstated. The empirical results, although indicating some improvement, lack statistical robustness, making the claims less compelling than presented.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel learning-based approach to edge-weighted online bipartite matching that integrates reinforcement learning (RL) with expert algorithms to ensure both robustness and competitive performance. LOMAR introduces a new online switching operation that dynamically decides between expert and RL decisions to hedge against uncertainties. The empirical results demonstrate that LOMAR achieves a competitive ratio of 0.85 while significantly improving average performance over existing RL methods, with average rewards reported at 12.6 compared to expert baselines which achieved 11.5.\n\n# Strength And Weaknesses\nA key strength of LOMAR is its successful combination of RL and expert strategies, yielding strong performance in both average and worst-case scenarios. The paper provides thorough empirical validation, showcasing LOMAR's advantages over current baselines, including DRL-OS and the Greedy algorithm. However, the methodology could be critiqued for potentially lacking clarity on the theoretical foundations of the online switching operation, which may hinder reproducibility. Additionally, while the empirical results are promising, the experiments are conducted on a single dataset (MovieLens), which may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, presenting its contributions and empirical results in a structured manner. The novelty lies in the introduction of the online switching operation and the integration of RL with expert algorithms, addressing both average and worst-case performance. However, the reproducibility of the results may be impacted by the limited dataset and the potential lack of detailed methodological explanations, particularly regarding the tuning of parameters and the conditions under which the switching occurs.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of online bipartite matching through the introduction of LOMAR, which effectively balances robustness and performance. While the empirical results are compelling, greater clarity in the methodology and broader validation across various datasets would strengthen the contributions.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces LOMAR, a novel algorithm designed for online bipartite matching under adversarial conditions. It employs a reinforcement learning (RL) framework to manage the sequential revelation of items and proposes a probabilistic switching mechanism to enhance decision-making. Key findings indicate that LOMAR achieves competitive ratios that suggest robustness against various expert algorithms, although the empirical evaluations are based on specific datasets like MovieLens and gMission.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its novel application of RL to the online matching problem and the introduction of a probabilistic framework for decision-making, which could potentially improve performance under certain conditions. However, several weaknesses are notable, including the reliance on a fixed expert algorithm as a benchmark, which may not universally represent performance across different problem instances. Additionally, the assumptions regarding independent item rewards and the robustness of the model against evolving adversarial strategies pose significant concerns. The lack of generalizability of empirical results to other domains further limits the practical applicability of LOMAR.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, but the assumptions underlying the model could be better articulated and defended. While the technical novelty is commendable, the reproducibility of results may be compromised due to the specific datasets used for evaluation and the lack of comprehensive testing across varied scenarios. Some theoretical guarantees provided are limited to specific conditions, which could hinder broader applicability.\n\n# Summary Of The Review\nOverall, while the paper presents a compelling approach to online bipartite matching with LOMAR, significant concerns regarding its assumptions, robustness, and generalizability limit its impact. The contributions are noteworthy, but the practical implications and performance in diverse environments require further exploration.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces LOMAR, a novel reinforcement learning-based approach for edge-weighted online bipartite matching, which aims to balance robustness with strong average-case performance. The methodology incorporates an online switching mechanism that allows the algorithm to dynamically choose between following expert decisions and reinforcement learning (RL) decisions, ensuring a competitive ratio against expert algorithms. The findings demonstrate that LOMAR outperforms baseline algorithms in both average rewards and competitive ratios, particularly in applications such as movie recommendation and crowdsourcing.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative integration of expert algorithms with reinforcement learning, providing a comprehensive solution to the challenges faced in online bipartite matching. The robustness analysis and the introduction of the online switching mechanism are significant contributions that address both average performance and worst-case scenarios. However, a potential weakness lies in the complexity of the switching mechanism, which may complicate understanding for practitioners not well-versed in reinforcement learning or online algorithms. Additionally, the experiments, while demonstrating improved performance, could benefit from a broader range of applications or comparative benchmarks to strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its contributions, methodology, and findings. The novelty of combining RL with expert algorithms is evident, and the rigorous analysis enhances the quality of the work. However, the technical details surrounding the online switching mechanism could be more clearly explained to aid reproducibility. The overall presentation is solid, but additional clarifications could improve accessibility for a wider audience.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in online bipartite matching through the introduction of LOMAR, which effectively combines robust performance with competitive guarantees. While the contributions are noteworthy and the empirical results are promising, further clarity on the complex aspects of the methodology could enhance its impact and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel framework for enhancing the robustness of machine learning models through a dual-optimization approach that simultaneously addresses accuracy and adversarial vulnerability. The methodology combines established techniques in adversarial training with a new regularization strategy that penalizes complexity in model decisions. The findings indicate that the proposed method significantly improves model performance on standard benchmark datasets while achieving greater stability under adversarial attacks.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative integration of dual objectives—accuracy and robustness—within a single optimization framework. This approach effectively addresses a critical gap in existing literature, where models often prioritize one objective at the expense of the other. However, a notable weakness is the lack of extensive empirical validation across diverse datasets. The experiments predominantly focus on a limited set of benchmarks, which may not fully represent the method's versatility and robustness in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately presents complex concepts, making them accessible to readers. Key terms and methodologies are defined clearly, contributing to the overall clarity of the paper. The novelty of the proposed framework is evident, as it introduces a unique approach to adversarial training. However, the reproducibility of the results could be enhanced by providing additional details regarding the experimental setup and hyperparameter choices, which are currently minimal.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to improving the robustness of machine learning models through a dual-optimization strategy. While the theoretical contributions are substantial, the empirical validation requires further expansion to fully establish the method's effectiveness across various scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel reinforcement learning-based algorithm designed for edge-weighted online bipartite matching, which aims to achieve both strong average-case and worst-case performance guarantees. The authors highlight the limitations of existing expert algorithms, which, while robust, often underperform on average, and the challenges posed by traditional RL methods that lack robustness. LOMAR introduces an innovative online switching mechanism that dynamically selects between expert and RL decisions based on anticipated future uncertainties. The paper provides theoretical proofs of LOMAR's robustness and competitive performance, alongside empirical experiments that demonstrate its superiority over existing methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its dual focus on both average-case performance and robustness in the context of online bipartite matching, an area of significant practical relevance. The introduction of the online switching mechanism is a notable contribution that enhances decision-making in real-time, addressing a key gap in prior RL approaches. However, the paper could benefit from a more comprehensive evaluation against a wider array of baseline algorithms to validate its claims, as the empirical results are somewhat limited in scope. Additionally, the explanation of the theoretical proofs could be made clearer for readers less familiar with the underlying mathematical concepts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that guides the reader through the problem statement, methodology, and results. The novelty of integrating RL with a robust expert algorithm framework is significant, contributing to the advancement of the field. However, some sections, particularly those detailing the theoretical proofs, may require further clarification to enhance reproducibility for future research. Overall, the quality of the writing is high, and the paper presents its findings in a coherent manner.\n\n# Summary Of The Review\nLOMAR presents a compelling advancement in the field of online bipartite matching by effectively merging reinforcement learning with established expert algorithms to achieve both robustness and high average performance. While the paper is well-structured and contributes novel insights, it would benefit from a broader empirical evaluation and clearer explanations of some theoretical aspects.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel reinforcement learning (RL) based algorithm designed for edge-weighted online bipartite matching, which addresses challenges associated with sequentially revealed item information and irreversible decisions. The authors integrate traditional expert algorithms with RL to ensure robustness while leveraging the advantages of RL for improved average-case performance. The experimental results demonstrate that LOMAR outperforms existing baseline algorithms in both average rewards and competitive ratios, highlighting its effectiveness in practical applications such as movie recommendation and crowdsourcing.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining RL with classical algorithms to enhance robustness in online bipartite matching. The methodology is well-defined, and the robust analysis provides theoretical guarantees that support the proposed algorithm. However, a potential weakness is that the paper could have included a broader range of experimental settings or applications to further validate LOMAR's performance across diverse scenarios. Additionally, while the switching operation is a novel contribution, the practical implications of its implementation could benefit from more detailed exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it easy to follow the proposed methodology and findings. The clarity of the problem formulation and the detailed robustness analysis contribute positively to the overall quality. The novelty of the work is significant due to the integration of RL with expert strategies, which addresses a notable gap in existing literature. The reproducibility statement is reassuring, as it provides necessary details about experimental setups and code availability, enhancing the paper's credibility.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the field of online bipartite matching by presenting LOMAR, an algorithm that effectively combines reinforcement learning with expert decision-making to achieve robust performance. While the theoretical foundations and empirical results are compelling, further exploration of practical implementation and a wider range of tests could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper, titled \"Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees,\" introduces a novel reinforcement learning-based algorithm called LOMAR to tackle the challenges of online bipartite matching with sequentially-revealed information. The main contributions include a robust approach that combines expert algorithms with reinforcement learning, a new online switching mechanism for decision-making, and theoretical proofs demonstrating LOMAR's competitive performance compared to expert algorithms. Empirical results show that LOMAR achieves significant improvements in both average-case and worst-case performance across various datasets, including MovieLens and gMission.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its clear problem formulation and the introduction of a robust learning approach that addresses existing limitations in online bipartite matching algorithms. The methodology is well-detailed, providing a comprehensive understanding of LOMAR's design and operation. However, a potential weakness is that while the empirical results are promising, more extensive comparisons with additional state-of-the-art algorithms could strengthen the claims about performance improvements. Furthermore, the reliance on specific datasets may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, making it easy to follow the methodology and results. The novelty of the proposed LOMAR algorithm is significant, as it addresses robustness in a novel way within the context of online bipartite matching. The authors provide sufficient details for reproducing their experiments, including a dedicated reproducibility statement and appendices with additional information on experimental setups and theoretical proofs.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of online bipartite matching through the introduction of a robust reinforcement learning approach. The methodology is well-explained, and the empirical results demonstrate the effectiveness of LOMAR. However, further validation against a wider range of baseline algorithms could enhance the robustness of the claims made.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR (Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees), a novel reinforcement learning (RL) approach designed to tackle the challenges of edge-weighted online bipartite matching. LOMAR integrates an innovative online switching mechanism to dynamically choose between expert decisions and RL outputs, thus hedging against future uncertainties. The authors demonstrate that LOMAR maintains a ρ-competitive ratio against any expert algorithm, ensuring robustness in both average-case and worst-case scenarios. Empirical experiments validate LOMAR's effectiveness, showing it outperforms traditional and RL-based baselines in real-world applications, particularly on the MovieLens dataset.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its innovative combination of expert algorithms and RL techniques, enhancing both average performance and robustness against uncertainties. The theoretical framework provided offers a solid foundation for understanding competitiveness in online matching scenarios. However, a potential weakness is the reliance on specific datasets for empirical validation, which may limit the generalizability of results. Additionally, while the online switching mechanism is well-conceived, the complexity it introduces may pose practical implementation challenges.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodologies. The quality of writing is high, making complex concepts accessible without oversimplification. The novelty of LOMAR's approach to integrating RL with expert decision-making in online matching is significant. The reproducibility is enhanced by the inclusion of appendices detailing experimental setups and proofs, although the reliance on specific datasets may require further validation in different contexts.\n\n# Summary Of The Review\nOverall, LOMAR presents a compelling advancement in the field of online bipartite matching by effectively combining robust theoretical guarantees with practical RL applications. The results are promising, though further validation across diverse datasets would strengthen the claims made.\n\n# Correctness\n4/5 - The theoretical claims and empirical results are generally well-supported, though some assumptions may warrant further scrutiny.\n\n# Technical Novelty And Significance\n5/5 - The integration of expert and RL methodologies in a robust framework is a significant contribution to the field, addressing a critical gap in existing literature.\n\n# Empirical Novelty And Significance\n4/5 - While the empirical results are strong and demonstrate LOMAR's advantages, the exploration of additional datasets would enhance the significance of these findings.",
    "# Summary Of The Paper\nThe paper presents LOMAR, an approach that integrates expert algorithms with reinforcement learning (RL) to address online bipartite matching problems. The authors claim that their method provides robustness guarantees and improves average performance compared to existing baselines. However, the methodology relies heavily on theoretical constructs, and empirical validation is limited to a single dataset (MovieLens), with a lack of exploration into diverse scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to combine established techniques in a novel framework. However, the contributions appear overstated as the integration of expert algorithms and RL is not a significant advancement within the field. The robustness guarantees proposed are based on theoretical assumptions that may not hold in practical situations, especially with adversarial inputs. Moreover, the empirical results are restricted, focusing solely on one dataset and failing to demonstrate the performance of LOMAR across varied conditions. The selective presentation of results and the lack of discussion regarding failure modes during the switching process raise concerns about the method's generalizability and reliability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by vague discussions, particularly regarding the trade-off between average performance and robustness. The methodology, while presented in a structured manner, lacks sufficient empirical validation, which affects its reproducibility. The novelty is questionable, as the concepts presented seem to reiterate existing ideas without providing substantial new insights or rigorous experimental evidence.\n\n# Summary Of The Review\nOverall, the paper proposes LOMAR as an innovative approach to online bipartite matching; however, its contributions are overstated and lack robust empirical validation. The theoretical claims and methodology do not convincingly demonstrate that LOMAR offers significant advantages over existing methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces LOMAR, a pioneering reinforcement learning (RL) approach designed for edge-weighted online bipartite matching that guarantees robustness. The main contributions include a novel dynamic online switching operation that allows the algorithm to flexibly choose between expert decisions and RL strategies based on incoming data. The authors demonstrate that LOMAR achieves ρ-competitiveness against any expert online algorithm, ensuring both strong average performance and robust worst-case guarantees. Empirical results indicate LOMAR's superiority over traditional greedy algorithms and pure RL models, making it applicable in various real-world scenarios such as online advertising and task scheduling.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to integrating RL with online matching algorithms, which addresses the critical need for robustness in real-time decision-making. The dynamic switching mechanism is particularly noteworthy, as it enhances adaptability in changing environments. However, the paper could benefit from a more detailed exploration of the computational complexity associated with LOMAR, as this is crucial for practical applications in industry. Additionally, while the empirical results are promising, further validation across diverse real-world datasets would strengthen the claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clear, effectively communicating the methodology and findings. The quality of the experimental design is high, showcasing a comprehensive evaluation against established baselines. The novelty of combining RL with competitive guarantees in online bipartite matching is significant, setting a new standard in the field. However, reproducibility could be improved by providing more detailed descriptions of the experimental setup and hyperparameter tuning processes.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in online bipartite matching through the introduction of LOMAR, which effectively combines robustness with high performance. While the contributions are compelling and the empirical results strong, there are areas for improvement regarding computational complexity discussions and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel online algorithm for bipartite matching that integrates reinforcement learning (RL) with expert algorithms. The methodology involves a dynamic switching mechanism that allows LOMAR to choose between RL and expert decisions based on uncertainty in future inputs. The main findings indicate that LOMAR achieves a competitive ratio of ρ against any expert algorithm, thus maintaining robust performance across a range of scenarios, even under adversarial conditions. The theoretical framework emphasizes the importance of careful design in switching conditions to ensure competitive performance metrics.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical analysis and the introduction of a well-defined switching mechanism, which enhances the algorithm's robustness in challenging online environments. The theoretical contributions are significant, providing a solid foundation for understanding the competitive nature of LOMAR. However, a notable weakness is the limited empirical validation, which appears to serve merely as supplementary evidence rather than a comprehensive evaluation of the algorithm's performance in real-world scenarios. This may hinder the practical applicability of LOMAR despite its strong theoretical guarantees.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the theoretical basis for LOMAR, making it accessible to readers familiar with online algorithms and competitive analysis. The quality of the theoretical framework is high, with clear proofs supporting the claims made. The novelty of integrating RL with expert algorithms through a switching mechanism is commendable. However, the reproducibility of results could be enhanced by providing more detailed empirical evaluations and possibly code or data to allow for independent validation of the findings.\n\n# Summary Of The Review\nOverall, the paper makes a substantial theoretical contribution to the field of online bipartite matching through the introduction of LOMAR, which combines RL and expert algorithms effectively. While it offers strong theoretical guarantees, the empirical validation is somewhat lacking, which could limit its practical impact. \n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents LOMAR, a novel algorithm designed for edge-weighted online bipartite matching that incorporates robustness guarantees through a learning-based framework. The methodology integrates expert algorithms with reinforcement learning (RL), utilizing a unique online switching mechanism to dynamically choose between expert and RL-driven decisions. The proposed approach is evaluated across two scenarios—no-free-disposal and free-disposal—using datasets such as MovieLens and gMission. Notable findings indicate that LOMAR outperforms baseline algorithms, including OPT, Greedy, and DRL variants, showcasing its efficacy in balancing exploration and exploitation through a carefully crafted training process.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative integration of RL with expert algorithms, which addresses the complexities of online bipartite matching effectively. The online switching operation adds an important layer of adaptability to the decision-making process. However, the paper could benefit from a more thorough analysis of the limitations of the proposed method in various practical scenarios, particularly in terms of computational efficiency and scalability beyond the tested datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, with a logical flow from the introduction of concepts to the detailed description of methodology and results. The novelty of the proposed method is significant, particularly in the realm of online matching problems. The reproducibility is supported by the commitment to release implementation details post-publication, although more explicit sharing of hyperparameters and configurations used in the experiments would enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper offers a compelling contribution to the field of online bipartite matching through the innovative LOMAR algorithm, demonstrating effective integration of RL with expert strategies. While the experimental results are promising, additional insights into computational aspects and broader applicability could strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel approach to edge-weighted online bipartite matching, claiming improvements in average-case performance and robustness compared to existing methods. The authors introduce a unique online switching operation designed to balance exploration and exploitation in matching decisions. Empirical results suggest that LOMAR outperforms traditional reinforcement learning (RL) methods and greedy algorithms in average rewards, though the paper does not sufficiently contextualize these findings within the broader landscape of bipartite matching literature.\n\n# Strength And Weaknesses\nWhile LOMAR purports to offer advancements over conventional algorithms, its contributions appear to lack novelty when juxtaposed with established methods, particularly greedy algorithms that are well-documented in the literature. The paper critiques existing RL approaches for their robustness, but fails to adequately acknowledge similar efforts made in prior works, such as conservative bandits. Furthermore, the empirical results, while favorable, do not provide a comprehensive comparison against foundational algorithms, potentially leading to an inflated perception of LOMAR's efficacy. The authors' framing often diminishes the strengths of established methods, making it challenging to assess LOMAR's true merit.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but its clarity suffers from a lack of nuanced discussion regarding the innovation of the online switching operation and its differentiation from existing techniques in the field. Quality of empirical evaluation is present, yet it is marred by the absence of thorough comparative analyses against a wider range of algorithms, particularly foundational ones. While reproducibility seems feasible, the lack of detailed comparisons raises concerns about the robustness of the findings.\n\n# Summary Of The Review\nLOMAR presents a potentially interesting approach to online bipartite matching, but it struggles to establish its novelty and significance against well-established algorithms. The empirical results are promising; however, the framing and comparisons made in the paper do not adequately support the claims of superiority over traditional methods.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees\" presents a novel reinforcement learning framework aimed at addressing the challenges in online bipartite matching under edge-weighted conditions. The authors introduce algorithms that leverage sequentially revealed item information to enhance decision-making in dynamic environments. Their findings indicate that the proposed methods achieve competitive ratios that outperform existing approaches, thereby demonstrating both theoretical robustness and practical applicability in various settings.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its introduction of a new learning framework that effectively balances the complexity of edge-weighted matching with the robustness required for online applications. The methodology is sound, and the empirical results support the claims made regarding the performance improvements over baseline methods. However, the paper suffers from several issues related to clarity and consistency, such as the use of technical jargon without adequate definitions and an inconsistent presentation of key equations and algorithms. These weaknesses could hinder the understanding and reproducibility of the research.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper contributes novel insights to the field of online matching, the clarity of presentation is subpar in several areas. The abstract contains complex sentences that detract from its readability, and inconsistent notation throughout the text may confuse readers. Furthermore, the authors' choice of terminology could benefit from clearer definitions. Although the theoretical contributions appear robust, the lack of structured organization in the proof sections and algorithm descriptions may pose challenges for reproducibility. Overall, the quality of writing needs improvement to enhance reader comprehension.\n\n# Summary Of The Review\nThe paper makes a significant contribution to online bipartite matching by introducing a new reinforcement learning framework that demonstrates both theoretical and empirical advancements. However, the clarity and organization of the paper detract from its overall impact, necessitating revisions to enhance readability and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a method for edge-weighted online bipartite matching, focusing on its robustness guarantees under adversarial conditions. The authors propose a unique switching condition design to balance average performance and worst-case guarantees. Empirical evaluations are conducted using two datasets, MovieLens and gMission, demonstrating LOMAR's effectiveness in specific scenarios, yet leaving questions regarding its generalizability and adaptability.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its focus on robustness in online bipartite matching, which is a relevant and underexplored area. However, its limitations include a narrow application scope, with no discussion of dynamic resource allocation or real-time recommendation systems. The empirical evaluations are limited to two datasets, raising concerns about the generalizability of the findings. Furthermore, the paper lacks a thorough exploration of hyperparameter tuning and does not address alternative switching strategies or potential failure modes, which could enrich the robustness analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, making it accessible to readers. However, it lacks depth in discussing limitations, interpretability, and ethical implications, which diminishes its overall quality. While the proposed methods present some novel ideas, the lack of comparisons with state-of-the-art methods and exploration of mixed scenarios hampers the perceived novelty. The reproducibility of the results could be improved with additional datasets and a more comprehensive analysis of hyperparameter tuning.\n\n# Summary Of The Review\nOverall, the paper makes a commendable effort in addressing robustness in online bipartite matching but falls short in its applicability, thoroughness, and comparative analysis. Enhancing the scope of empirical evaluations and a deeper discussion of practical implications would significantly strengthen the contributions of this work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces LOMAR, a novel reinforcement learning (RL)-based approach designed for edge-weighted online bipartite matching, with a focus on ensuring robustness through competitive ratios. The authors establish a comprehensive framework for evaluating the performance of online algorithms, emphasizing both average-case and worst-case scenarios. Key findings include a theoretical guarantee that LOMAR achieves a competitive ratio of ρ against any expert algorithm, supported by empirical results demonstrating significant improvements in both average rewards and competitive ratios compared to baseline algorithms.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its rigorous mathematical formulation and comprehensive statistical methodology, particularly the introduction of competitive ratios that provide a nuanced evaluation of algorithm performance under uncertainty. The dual focus on robustness and average-case performance enhances its applicability in real-world scenarios. However, the paper may benefit from a deeper exploration of potential limitations in the switching mechanism and the implications of its assumptions. Additionally, while the empirical results are promising, further experiments across diverse datasets could strengthen the validation of LOMAR's efficacy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its ideas clearly, making it accessible to the readership. The quality of the mathematical definitions and statistical analyses is high, providing a solid foundation for the claims made. The novelty of the approach lies in its integration of RL with competitive ratio analysis, a combination that has not been extensively explored in the literature. Reproducibility is supported by a clear description of the experimental setup and the statistical significance of results, though additional details on implementation would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of online bipartite matching by introducing LOMAR, which effectively combines robust theoretical guarantees with empirical validation. While the methodology is sound and the results are compelling, further exploration of potential limitations and broader experimental validation would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents LOMAR, a novel approach to edge-weighted online bipartite matching, which aims to provide robustness guarantees against expert algorithms. The methodology involves a combination of online switching mechanisms and parameter tuning for robustness (ρ and B) to enhance performance. The findings indicate that LOMAR achieves competitive results on datasets like MovieLens and gMission, demonstrating its efficacy in specific contexts.\n\n# Strength And Weaknesses\nThe strengths of the paper include the introduction of LOMAR as a robust algorithm capable of handling edge-weighted online bipartite matching. However, several weaknesses limit its applicability and understanding. Notably, the paper does not address scalability to larger or more complex scenarios, which is critical for real-world applications. Additionally, the exploration of the robustness parameters (ρ and B) is insufficient, and the experiments are limited to only two datasets, raising concerns about generalizability. The lack of analysis of computational complexity, potential tuning strategies for hyperparameters, and the robustness against adversarial attacks further detracts from the overall contribution. Moreover, the absence of comprehensive ablation studies limits insights into the effects of different components of LOMAR.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly. However, the lack of discussions on certain key aspects, such as the performance of LOMAR in noisy environments and the impact of expert algorithm choice, hinders its quality and comprehensiveness. While the methodology is novel in its approach to robustness, the empirical validation is not sufficiently robust to ensure reproducibility and general applicability. Future research directions are indicated, but they do not fully compensate for the current limitations.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting approach with LOMAR for online bipartite matching, it has significant limitations regarding scalability, empirical validation, and the exploration of robustness parameters. These factors diminish the impact of its contributions and suggest that further investigation is needed to substantiate its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees\" attempts to address the problem of online bipartite matching by introducing a new algorithm called LOMAR. The authors propose a methodology that combines a switching mechanism to adapt to uncertain outcomes, aiming to provide robustness guarantees. Through experiments on standard datasets, such as MovieLens, they claim to demonstrate competitive performance metrics, including provable robustness against specific constraints.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to tackle a well-known problem within the field of combinatorial optimization, specifically online bipartite matching. The introduction of LOMAR, despite its lack of novelty, offers a systematic approach to the switching mechanism. However, the weaknesses are pronounced: the paper lacks originality, as much of its content reiterates existing knowledge without substantial innovation. The choice of the MovieLens dataset for experiments is also criticized for being overly conventional, reflecting a missed opportunity for creativity in empirical validation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers from dense mathematical formulations that may obscure the key contributions. While the authors attempt to provide theoretical backing for their claims, the complexity may detract from reader comprehension. The novelty of the proposed method is questionable, as it largely appears to be a rehash of existing techniques rather than presenting a groundbreaking advancement. Reproducibility is potentially achievable given the straightforward nature of the experiments; however, the lack of innovative presentation diminishes the impact of their findings.\n\n# Summary Of The Review\nOverall, the paper presents a familiar approach to an established problem without offering significant new insights or methodologies. The contributions, while theoretically sound, feel redundant and the empirical validation lacks creativity, rendering the overall impact of the work limited.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces LOMAR, a novel reinforcement learning (RL)-based approach aimed at edge-weighted online bipartite matching. LOMAR integrates the robustness of classical expert algorithms with RL's potential for average-case improvements, employing a dynamic switching mechanism between expert and RL decisions. The empirical evaluation on the MovieLens dataset demonstrates LOMAR's effectiveness compared to existing baselines, highlighting its potential in online decision-making scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative combination of RL and expert algorithms, which addresses the existing gap in robustness guarantees for RL methods. The dynamic switching mechanism is a notable contribution, allowing LOMAR to adaptively leverage the strengths of both approaches. However, the paper also has weaknesses, including a lack of deeper exploration into the theoretical foundations of the proposed methods and limited generalizability due to the sole reliance on the MovieLens dataset. The potential for future enhancements, such as integrating advanced RL techniques and exploring multi-agent settings, is not fully realized within the current work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and findings. However, the lack of a robust theoretical framework may hinder the understanding of the underlying principles governing LOMAR's performance. The novelty is commendable, particularly in the context of RL applications in online matching, though the exploration of empirical results could benefit from additional datasets and sensitivity analyses. The reproducibility of the results could be improved by providing more details on hyperparameter tuning and the specifics of the implementation.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of online bipartite matching through the introduction of LOMAR. While the contributions are noteworthy and the methodology is sound, there are areas for improvement, particularly concerning theoretical rigor and empirical validation across diverse datasets.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper introduces LOMAR, a novel algorithm for edge-weighted online bipartite matching. The authors compare LOMAR's performance against various baselines, including Greedy, DRL, and DRL-OS, using metrics such as average reward (AVG) and competitive ratio (CR). The findings demonstrate that LOMAR consistently outperforms the baselines, particularly in robustness when training and testing distributions differ, highlighting its effectiveness in balancing average performance with worst-case guarantees.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive empirical evaluation, demonstrating LOMAR's superior performance across different scenarios and hyperparameter settings. The visualization of results through boxplots and histograms provides clarity in understanding the distribution of outcomes. However, a potential weakness is the lack of a detailed theoretical analysis accompanying the empirical results, which could strengthen the claims of robustness and performance improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and results, making it accessible to readers. The quality of the experiments appears high, with rigorous comparisons against established baselines. The novelty of LOMAR is significant, as it offers a new approach to online bipartite matching; however, reproducibility could be enhanced with additional details about experimental setups, particularly regarding hyperparameter tuning.\n\n# Summary Of The Review\nOverall, the paper presents a compelling new algorithm that significantly advances the performance of edge-weighted online bipartite matching. While the empirical results are strong and well-presented, the addition of theoretical insights could further validate the contributions made by LOMAR.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to online bipartite matching, integrating concepts from expert algorithms and reinforcement learning. The authors propose a new algorithm that optimizes matching efficiency under dynamic conditions and evaluate its performance against existing methods. The findings indicate significant improvements in matching quality and computational efficiency, highlighting the algorithm's potential applications in various real-world scenarios.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative combination of established techniques and the empirical results demonstrating the algorithm's superiority over traditional methods. However, the paper suffers from clarity issues, including dense language and complex sentence structures that may hinder reader comprehension. Additionally, while the authors provide examples, these could be made more illustrative to effectively demonstrate the algorithm's applicability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by long sentences and inconsistent terminology usage, which can confuse readers. The quality of the presentation could improve through clearer definitions of technical terms and more structured organization. Although the paper introduces novel methodologies, the reproducibility is somewhat hindered by the lack of detailed explanations for mathematical notations and empirical results.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to online bipartite matching, offering valuable contributions to the field. However, significant improvements in clarity and presentation are necessary to enhance reader engagement and understanding. Addressing these issues would elevate the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4323902748938817,
    -1.796148410819579,
    -1.7502339099997997,
    -1.6243695933304132,
    -2.0509986814012335,
    -1.8866543101958126,
    -1.5929991734240576,
    -1.9968188776726807,
    -1.7710302813326542,
    -1.9195550492871194,
    -1.7936389643132804,
    -1.5013630972090173,
    -1.6427674207644696,
    -1.8335545980028551,
    -1.670551701622425,
    -1.697647146290479,
    -1.9729069793473406,
    -1.7497590109566212,
    -1.7595015095095152,
    -1.910690865544528,
    -1.9337650051863065,
    -1.8596375684323607,
    -1.8981643493546245,
    -1.660585337424142,
    -1.8825784959601377,
    -1.8914864249684495,
    -1.6972504399244437,
    -1.7096651222551176,
    -1.796709864285418
  ],
  "logp_cond": [
    [
      0.0,
      -2.2282896774830006,
      -2.2292449079202865,
      -2.247799804443035,
      -2.2376114727493444,
      -2.20121849347109,
      -2.279304004180692,
      -2.2364715753108775,
      -2.2391259738574956,
      -2.2659340755684494,
      -2.2264741107770183,
      -2.315223481409497,
      -2.2332688684363653,
      -2.2358683082649873,
      -2.241647370376588,
      -2.2139786500608345,
      -2.2457730755917997,
      -2.2236255261607374,
      -2.2414008310802465,
      -2.2502872369437723,
      -2.256260996355263,
      -2.255394122305991,
      -2.2780253421442977,
      -2.236331806978917,
      -2.256656928560364,
      -2.254007065654207,
      -2.248223908815194,
      -2.268017289031836,
      -2.27803812574142
    ],
    [
      -1.445385865867262,
      0.0,
      -1.2465115264897133,
      -1.2249734012833442,
      -1.1911300127431454,
      -1.33106366833444,
      -1.406628894095661,
      -1.2169056076436229,
      -1.2289117738731945,
      -1.3749930240091515,
      -1.2206254667348517,
      -1.4806316785131808,
      -1.2631538174499717,
      -1.2683878634584058,
      -1.2758749890763987,
      -1.2280434881533446,
      -1.3140129949689443,
      -1.3063612727286051,
      -1.2886251797059511,
      -1.3080172174101032,
      -1.3673194883283524,
      -1.452934034576376,
      -1.341496894294439,
      -1.2814526372537256,
      -1.3214559555909768,
      -1.3121019417881843,
      -1.2541783179860422,
      -1.3562364590165332,
      -1.4944356647476846
    ],
    [
      -1.4445920660240392,
      -1.3698315798940943,
      0.0,
      -1.3801950854766099,
      -1.3377998896417798,
      -1.3227112918045965,
      -1.463318571583107,
      -1.3458175205896434,
      -1.341187630622674,
      -1.3966697924187061,
      -1.3429154380815487,
      -1.5484929632290663,
      -1.3903866825643008,
      -1.3501437034804709,
      -1.312393845278408,
      -1.3255490725554735,
      -1.3809283594519384,
      -1.3776015899019438,
      -1.3972991918119841,
      -1.3454787629660045,
      -1.3874704481657432,
      -1.4256593820870433,
      -1.3788614887907686,
      -1.3929820976541805,
      -1.3764788832251118,
      -1.4080048706949668,
      -1.390109707701865,
      -1.4192562825362705,
      -1.499040601128547
    ],
    [
      -1.2991290631062864,
      -1.0646445973456373,
      -1.1083205099465936,
      0.0,
      -1.1019438519800642,
      -1.1588095720148999,
      -1.245577457026822,
      -1.169261707582629,
      -1.0901101588789317,
      -1.269362565514484,
      -1.0761999340528188,
      -1.3754727732783951,
      -1.0687523803482784,
      -1.0398343951791003,
      -1.1135465096620705,
      -1.0691318368753344,
      -1.1569490830122091,
      -1.1448747196575884,
      -1.1797192766568811,
      -1.143473010660224,
      -1.2059054984790465,
      -1.2399100066566355,
      -1.2115705753261514,
      -1.1316349950658435,
      -1.1919032815604778,
      -1.2629340151843078,
      -1.1409438053320435,
      -1.2032991544791913,
      -1.313948767955642
    ],
    [
      -1.7150671498104948,
      -1.5579974551774751,
      -1.6104815601267293,
      -1.6315445528549797,
      0.0,
      -1.5930795337807726,
      -1.714585606214494,
      -1.5472538679264332,
      -1.6266413069910888,
      -1.665930249098927,
      -1.6003414918973777,
      -1.775969544664827,
      -1.6025455988511637,
      -1.5777819557899537,
      -1.6089852379175031,
      -1.5953117233183232,
      -1.6547077077412906,
      -1.6080319180018123,
      -1.6492113792404623,
      -1.6419196096555286,
      -1.6927214863244904,
      -1.7526197146624485,
      -1.6699418475096142,
      -1.6045695668910425,
      -1.6359236989943777,
      -1.6560413361763686,
      -1.5848837306251957,
      -1.702443852014886,
      -1.7678032362126455
    ],
    [
      -1.6161430202543272,
      -1.4487548822472134,
      -1.3689214564772076,
      -1.4848445962470336,
      -1.4332864099583231,
      0.0,
      -1.5700017589244675,
      -1.4520459178574674,
      -1.4479250988697103,
      -1.5368024007382572,
      -1.4642557684124584,
      -1.6376447118688782,
      -1.4465468592041169,
      -1.4544069691831887,
      -1.4365973632260156,
      -1.429160785096813,
      -1.45281256936711,
      -1.4418586217363594,
      -1.4913320181966914,
      -1.4732079657572972,
      -1.5082373613360658,
      -1.5300006150721246,
      -1.4848817414393634,
      -1.5049618762173218,
      -1.4850882121945623,
      -1.5093664397272042,
      -1.4850241154181791,
      -1.5261825950401213,
      -1.5815884817956574
    ],
    [
      -1.3078481089135099,
      -1.1851262600970658,
      -1.2003734353726303,
      -1.1988552098924914,
      -1.2254237309272533,
      -1.1843151078393281,
      0.0,
      -1.1882962969928172,
      -1.1675244034175118,
      -1.2617347073661955,
      -1.1558662390238925,
      -1.2940066360735853,
      -1.1444227119799633,
      -1.227620335348352,
      -1.1928931991938119,
      -1.185906838068885,
      -1.2225208736674065,
      -1.1920411235166437,
      -1.1692245620412547,
      -1.2136355811877264,
      -1.2112583342358791,
      -1.2638330953110442,
      -1.2600876119812472,
      -1.2180924857628264,
      -1.207728168463348,
      -1.2829919245249732,
      -1.2420681863886038,
      -1.2655865042460106,
      -1.2873470343020363
    ],
    [
      -1.629177861144768,
      -1.427798013624367,
      -1.5035961201931998,
      -1.5389470083285042,
      -1.4557296491398117,
      -1.4719025257075602,
      -1.589708366296575,
      0.0,
      -1.496533957123484,
      -1.6183133944908963,
      -1.555577139239772,
      -1.6710079726733305,
      -1.494883974701681,
      -1.521560824229477,
      -1.49360859127386,
      -1.4795254860689095,
      -1.5376888627667618,
      -1.4861009612327893,
      -1.5784141668236535,
      -1.5050161651184255,
      -1.4806028065913057,
      -1.6348404693107488,
      -1.590773122785108,
      -1.5722810424847113,
      -1.551531922582847,
      -1.5277955841464534,
      -1.5037349846562333,
      -1.6164111332265205,
      -1.6720744814739081
    ],
    [
      -1.40666871801521,
      -1.228529054639908,
      -1.229915702129773,
      -1.2500820518693796,
      -1.294493332772109,
      -1.2863454307315603,
      -1.3664531108989721,
      -1.2345106808519934,
      0.0,
      -1.415384776713963,
      -1.2477738939414744,
      -1.5071824470600388,
      -1.2390849310190344,
      -1.2755575699967736,
      -1.2946948375872316,
      -1.2847322532043803,
      -1.2567127495428385,
      -1.281802451720197,
      -1.3168891983207793,
      -1.2638407963205396,
      -1.2819855063954,
      -1.397388884249783,
      -1.3322511190054735,
      -1.3037913756605348,
      -1.3200186125013165,
      -1.3654203691023161,
      -1.2904430672401475,
      -1.2874896850922337,
      -1.4348499107769555
    ],
    [
      -1.5920099492886737,
      -1.4694213035140242,
      -1.443291869340027,
      -1.5220948662407376,
      -1.4334161081223484,
      -1.4508667293291575,
      -1.562342534158827,
      -1.422855689460096,
      -1.5647971504802505,
      0.0,
      -1.476994309404793,
      -1.631785688483649,
      -1.5596948684004048,
      -1.4559701005421641,
      -1.435359465440917,
      -1.4883349235575172,
      -1.48537470951289,
      -1.5187034371040287,
      -1.4933094332357613,
      -1.4851756483465668,
      -1.5370031433134543,
      -1.5301357122824495,
      -1.4762255295539246,
      -1.450960474286772,
      -1.4258849367929813,
      -1.5181918757506832,
      -1.486660977648207,
      -1.5825233404662116,
      -1.603031469118656
    ],
    [
      -1.4263374284747201,
      -1.1865048238251987,
      -1.1732509565159985,
      -1.1263743230906824,
      -1.209756532494922,
      -1.2254886918224792,
      -1.3251153397173903,
      -1.2593346857592878,
      -1.1665906067657124,
      -1.3485004327498264,
      0.0,
      -1.5012044542408223,
      -1.2115993621787458,
      -1.1324964754705897,
      -1.2343381935818085,
      -1.2352294370704484,
      -1.2858728806490425,
      -1.2283356821129134,
      -1.2872892635964668,
      -1.2659804589313706,
      -1.3086505097232104,
      -1.4102099294843904,
      -1.341917881232602,
      -1.2112017136144047,
      -1.2545905357380325,
      -1.3574903279175938,
      -1.3110814030001652,
      -1.3010606041495028,
      -1.4195398296181243
    ],
    [
      -1.2108059162388216,
      -1.1012415583371042,
      -1.1694214402463923,
      -1.1568041766170796,
      -1.116324508651911,
      -1.1335275525711095,
      -1.1588816512669988,
      -1.099848150530818,
      -1.1717814788043428,
      -1.1339394427869465,
      -1.1706004198722026,
      0.0,
      -1.1832068080096978,
      -1.1736630568198452,
      -1.157798830525795,
      -1.1230772210642874,
      -1.139905213447984,
      -1.147792811091781,
      -1.132091902565467,
      -1.1377865989322824,
      -1.1609776624766526,
      -1.124942709221611,
      -1.1642573803492635,
      -1.1525965544315921,
      -1.135801274313771,
      -1.1044548861161265,
      -1.1372548186822131,
      -1.1434967270146625,
      -1.143129362699266
    ],
    [
      -1.2846854602104618,
      -1.1514262803345825,
      -1.1700815832095282,
      -1.1542624579074507,
      -1.2090883479330612,
      -1.159156086183262,
      -1.224428929784838,
      -1.1475715759733671,
      -1.1669972224424963,
      -1.302399513846422,
      -1.1460118637128391,
      -1.379659639735095,
      0.0,
      -1.1832063925426077,
      -1.1203760263240998,
      -1.1622567863109095,
      -1.2084490898051827,
      -1.1554047596249661,
      -1.2330106999269606,
      -1.1869807249279538,
      -1.1915497095252658,
      -1.2765567468964591,
      -1.2604515772648834,
      -1.2110849575761249,
      -1.2217003111848,
      -1.3073908716714442,
      -1.2300225711290593,
      -1.2626290146404677,
      -1.304105499648692
    ],
    [
      -1.4684097978499286,
      -1.2925100931452647,
      -1.2792711790407796,
      -1.2790748333015518,
      -1.218936497240173,
      -1.3020503940294783,
      -1.4698239962226798,
      -1.3376339258555134,
      -1.3060002841905534,
      -1.4429253891172884,
      -1.2442446083824674,
      -1.5687838927807538,
      -1.3044484772238087,
      0.0,
      -1.2176928444200659,
      -1.3533311845636755,
      -1.3528470217496875,
      -1.3304834608478329,
      -1.4139904886155379,
      -1.322802257017167,
      -1.3912442019299889,
      -1.4009044388757648,
      -1.3943360331755983,
      -1.3445151935753998,
      -1.3944952059390592,
      -1.4435909522908539,
      -1.354048073048755,
      -1.3951714360720016,
      -1.5095268828906465
    ],
    [
      -1.3341646767711934,
      -1.192280911198668,
      -1.0807550838097872,
      -1.1661857486868172,
      -1.1764654170359885,
      -1.1341741502683098,
      -1.2790970591047794,
      -1.1680255252596672,
      -1.1714125658980912,
      -1.252098114180541,
      -1.1542891959659163,
      -1.397921628198097,
      -1.1346450367800265,
      -1.1273293543874936,
      0.0,
      -1.1259596100563822,
      -1.2151887133337969,
      -1.1892129173708403,
      -1.2563663765429094,
      -1.145950796723427,
      -1.2247680227278746,
      -1.2323611195286759,
      -1.2024859522786098,
      -1.2145885476403986,
      -1.1812482088437104,
      -1.2222821130225898,
      -1.2047700150107545,
      -1.2082745633055385,
      -1.3223145230869613
    ],
    [
      -1.3795659768463593,
      -1.1686763037975485,
      -1.2118043375874976,
      -1.204402795310492,
      -1.184679340358341,
      -1.2282689711230186,
      -1.3380849386959268,
      -1.1853410440643473,
      -1.23447165356143,
      -1.3094062899759091,
      -1.247509982052368,
      -1.4315548401365823,
      -1.1981749905476573,
      -1.2983738171466837,
      -1.1989818927026512,
      0.0,
      -1.252413182238529,
      -1.2181619865059303,
      -1.276092817199616,
      -1.2958649972392153,
      -1.299308991363824,
      -1.3549095167083325,
      -1.3202279650523074,
      -1.2311804618768778,
      -1.314487071863202,
      -1.2849575326108473,
      -1.262188492303032,
      -1.3239523592190698,
      -1.4168679747980677
    ],
    [
      -1.5749820324967538,
      -1.5100615100603711,
      -1.3950249204454144,
      -1.4915146361644418,
      -1.468165368770643,
      -1.5045477562783478,
      -1.5147321000126395,
      -1.497670554941351,
      -1.4838045283284091,
      -1.6008974737818804,
      -1.5140580001772097,
      -1.6689307326516516,
      -1.4846445809013813,
      -1.4752388142986357,
      -1.4976493417222285,
      -1.5162601806923006,
      0.0,
      -1.5351542669695302,
      -1.5404111914307477,
      -1.4934341877927768,
      -1.5057603851548158,
      -1.6161536329065636,
      -1.487952704608345,
      -1.524028202322771,
      -1.4783816187224224,
      -1.5710801524248899,
      -1.489839535911281,
      -1.5720207709960676,
      -1.5895216978722784
    ],
    [
      -1.4275981470076613,
      -1.2523485929437383,
      -1.2096698395322325,
      -1.273712094017404,
      -1.254861715829094,
      -1.234911052459688,
      -1.3437627989772511,
      -1.2614686654926295,
      -1.2401151049231323,
      -1.4312451314775447,
      -1.2180099650780374,
      -1.495880225178695,
      -1.2337815572112074,
      -1.2826548345069342,
      -1.261900607395036,
      -1.2158367125090057,
      -1.3482275668723558,
      0.0,
      -1.3140632951706523,
      -1.2480591681925723,
      -1.2969303930744733,
      -1.3692756696551338,
      -1.360118338783637,
      -1.2589785335261554,
      -1.3150893530728196,
      -1.3961533672900708,
      -1.3226360248617028,
      -1.2753926494336076,
      -1.4223470648278025
    ],
    [
      -1.3634188198884343,
      -1.199674061532005,
      -1.302581810294531,
      -1.285133644357897,
      -1.2684570506935717,
      -1.255260226484419,
      -1.3248888225854618,
      -1.2134594301875945,
      -1.3019055624785483,
      -1.3066579358679022,
      -1.2693824005553227,
      -1.4304395378223933,
      -1.277619052937997,
      -1.3277771144603094,
      -1.313883245429524,
      -1.2671802997330484,
      -1.253805954640118,
      -1.2769840056927435,
      0.0,
      -1.2875146075709873,
      -1.2941060272039488,
      -1.3840448370398584,
      -1.3016720059629436,
      -1.27011339786803,
      -1.2710331603967597,
      -1.3189873629762217,
      -1.2893883485532278,
      -1.341320214557566,
      -1.384791522440152
    ],
    [
      -1.5158250897710552,
      -1.3706248413908844,
      -1.318252060247186,
      -1.4196227861565531,
      -1.426150452465536,
      -1.4027845762328948,
      -1.4807800628745782,
      -1.3831879047264863,
      -1.3587601467968127,
      -1.4612264691002594,
      -1.4182370771662427,
      -1.6137660371324638,
      -1.412717888778949,
      -1.3775277097040222,
      -1.3321017099598358,
      -1.4541412481109177,
      -1.4422791981346055,
      -1.3869503443750368,
      -1.4538318613530907,
      0.0,
      -1.4518649011781932,
      -1.5015755893174263,
      -1.4310155705635292,
      -1.470806038736988,
      -1.3927614657809984,
      -1.514888147973649,
      -1.4193068733314775,
      -1.4228780438912527,
      -1.5369423163400147
    ],
    [
      -1.6481067079587766,
      -1.5787384222499918,
      -1.5198340842617852,
      -1.5238792686565845,
      -1.6134435710003605,
      -1.5452298792888357,
      -1.577508425556022,
      -1.4433661309076,
      -1.4774577187279563,
      -1.6526630423535968,
      -1.4993108669644934,
      -1.7221872081201008,
      -1.5134949884769393,
      -1.5086171652079303,
      -1.5569691650237854,
      -1.6147448382537275,
      -1.5513295405824716,
      -1.5535115377492228,
      -1.6073352619738766,
      -1.5363561476981136,
      0.0,
      -1.622729652039243,
      -1.548308171247759,
      -1.529839194591032,
      -1.5816193693441791,
      -1.6056497073248572,
      -1.612498324835999,
      -1.5719417300424683,
      -1.6433361997097193
    ],
    [
      -1.524768927572848,
      -1.45913755638001,
      -1.4042836732676844,
      -1.463079694361381,
      -1.4660668295042465,
      -1.3828733430600881,
      -1.4904443083766987,
      -1.4347286269150006,
      -1.4334029531325427,
      -1.472530756977714,
      -1.4772705610018484,
      -1.5268312838581666,
      -1.4460278847826478,
      -1.4341305727834959,
      -1.3845377621035155,
      -1.4475593937487359,
      -1.4744691814080757,
      -1.4760525503788413,
      -1.466854067607119,
      -1.449733193425704,
      -1.4575334229263721,
      0.0,
      -1.4734273290774649,
      -1.4456893494494285,
      -1.4709395347763163,
      -1.4483539273820978,
      -1.4609349402036278,
      -1.4447391148845554,
      -1.4703949976072297
    ],
    [
      -1.6098133791555367,
      -1.5348892768088072,
      -1.4343933472289254,
      -1.5411416159113676,
      -1.5198949472410637,
      -1.4788984292427074,
      -1.578104568942808,
      -1.494033859206794,
      -1.5379895285117866,
      -1.4961986782849728,
      -1.51776303099594,
      -1.6636217824731447,
      -1.5216278819956413,
      -1.4932794656851638,
      -1.4523003201841431,
      -1.5358309353829205,
      -1.4473048369426262,
      -1.537236383020872,
      -1.5476661228959299,
      -1.4689606015152135,
      -1.5410220126537322,
      -1.5595242325368752,
      0.0,
      -1.5505500142725877,
      -1.3697268317301994,
      -1.4746941035691297,
      -1.536521881624536,
      -1.522015051654875,
      -1.604821227267705
    ],
    [
      -1.331835227915715,
      -1.1799077827509048,
      -1.2331492758372746,
      -1.20157800598971,
      -1.1357829124059957,
      -1.2098869339164775,
      -1.2853048406112824,
      -1.2341616793554917,
      -1.2350302702518867,
      -1.3030605441236005,
      -1.1743755480037201,
      -1.4065772786658561,
      -1.197308938035857,
      -1.1857968143835116,
      -1.254958147313777,
      -1.215019120708187,
      -1.2392110532350125,
      -1.1949883852924765,
      -1.2316047206721832,
      -1.2495037678731107,
      -1.251031899342605,
      -1.3166386239302237,
      -1.234241323285404,
      0.0,
      -1.2188290635548884,
      -1.3050450644595384,
      -1.2467883486044744,
      -1.2412822406842015,
      -1.3515509354191473
    ],
    [
      -1.5420866674376852,
      -1.4598493416471507,
      -1.4075103602741488,
      -1.4989282732184015,
      -1.448188515076064,
      -1.4311710618129327,
      -1.5283293418612622,
      -1.4050548584771518,
      -1.4897412370263274,
      -1.4697869164805155,
      -1.4409325468469867,
      -1.6228989712256732,
      -1.4712170072350956,
      -1.4993089291033115,
      -1.3957384257408705,
      -1.4855382092079934,
      -1.452380746504396,
      -1.4577312614600326,
      -1.505828723573316,
      -1.405634491433052,
      -1.4728137362773661,
      -1.5676824532168228,
      -1.4083568279693435,
      -1.4686824871461073,
      0.0,
      -1.491515135903131,
      -1.4882291178154283,
      -1.50905730619787,
      -1.6049812036257511
    ],
    [
      -1.6121415796948029,
      -1.431907860975013,
      -1.383392567133365,
      -1.5049245894297223,
      -1.416343041761446,
      -1.4398063483351133,
      -1.535781163970506,
      -1.3881104826627897,
      -1.4952522430746134,
      -1.445210037607637,
      -1.5001463446238834,
      -1.5514511203849595,
      -1.510357190845324,
      -1.4921655316804858,
      -1.389409617871331,
      -1.4003176809281563,
      -1.4676546035689777,
      -1.4985938233761527,
      -1.5109756418797042,
      -1.467741678795455,
      -1.4622270983192074,
      -1.4605417255557451,
      -1.439103313432092,
      -1.471848471208775,
      -1.4092985101562086,
      0.0,
      -1.4321190622945081,
      -1.5118336273077386,
      -1.5758339220154722
    ],
    [
      -1.3496487413581313,
      -1.1984311890303756,
      -1.2100477353743004,
      -1.2886918596147896,
      -1.1691117737435224,
      -1.219600289746715,
      -1.3169311837949693,
      -1.183509412318996,
      -1.2443852739963719,
      -1.2455586874569946,
      -1.2459063676705404,
      -1.3911598468842017,
      -1.2620785602152096,
      -1.2509884868999661,
      -1.2640475931305226,
      -1.2287112807344347,
      -1.2373914997440896,
      -1.2226668118761026,
      -1.3033025708174262,
      -1.2427433331685553,
      -1.3589836217757005,
      -1.3629693257244648,
      -1.2374678147115392,
      -1.257535633999819,
      -1.2243871386033527,
      -1.267148629871822,
      0.0,
      -1.3046745488983424,
      -1.409213365090419
    ],
    [
      -1.3610890720597038,
      -1.2414142854250958,
      -1.1997724687589513,
      -1.245850693481341,
      -1.2562966955007366,
      -1.189830317954919,
      -1.2801250868127998,
      -1.2174867596474683,
      -1.1466727179718321,
      -1.3200099948592365,
      -1.179625395078222,
      -1.3739454983966466,
      -1.193889639965291,
      -1.2078621673395873,
      -1.2161124934887597,
      -1.2701204837387297,
      -1.2240702817380649,
      -1.206058581286089,
      -1.2585779692403092,
      -1.2177089254624542,
      -1.2032270031827168,
      -1.2693593593565662,
      -1.204363082768058,
      -1.2043694397155156,
      -1.2236335422957003,
      -1.2741082485866753,
      -1.2524113337203997,
      0.0,
      -1.3048946028082313
    ],
    [
      -1.3953223373622134,
      -1.3836792200444021,
      -1.3396388459958246,
      -1.3743837409753101,
      -1.3725204224442349,
      -1.3227827138304717,
      -1.3910990493461406,
      -1.3301306665430326,
      -1.3523030167177585,
      -1.3986111023212808,
      -1.3878300117192832,
      -1.4187570160561291,
      -1.3567163411345828,
      -1.3454705317208324,
      -1.3628385475018572,
      -1.4141810431796298,
      -1.3183554084656735,
      -1.3477609613466015,
      -1.3551676450040102,
      -1.3365039424101848,
      -1.363230483340553,
      -1.232250270127263,
      -1.3479209509313765,
      -1.3492616108035165,
      -1.3197275402741044,
      -1.4265688464650697,
      -1.3637094557432594,
      -1.3834353199537754,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.20410059741088116,
      0.2031453669735952,
      0.18459047045084676,
      0.19477880214453736,
      0.2311717814227916,
      0.1530862707131897,
      0.19591869958300423,
      0.19326430103638614,
      0.16645619932543232,
      0.20591616411686342,
      0.11716679348438452,
      0.19912140645751641,
      0.19652196662889443,
      0.1907429045172937,
      0.2184116248330472,
      0.18661719930208198,
      0.20876474873314432,
      0.19098944381363525,
      0.1821030379501094,
      0.17612927853861882,
      0.1769961525878907,
      0.15436493274958396,
      0.19605846791496484,
      0.17573334633351756,
      0.17838320923967466,
      0.18416636607868764,
      0.16437298586204552,
      0.15435214915246176
    ],
    [
      0.35076254495231707,
      0.0,
      0.5496368843298658,
      0.5711750095362349,
      0.6050183980764336,
      0.4650847424851392,
      0.38951951672391805,
      0.5792428031759562,
      0.5672366369463846,
      0.42115538681042763,
      0.5755229440847274,
      0.3155167323063983,
      0.5329945933696074,
      0.5277605473611733,
      0.5202734217431804,
      0.5681049226662345,
      0.4821354158506348,
      0.48978713809097396,
      0.507523231113628,
      0.48813119340947586,
      0.4288289224912267,
      0.34321437624320317,
      0.45465151652514013,
      0.5146957735658535,
      0.47469245522860226,
      0.4840464690313948,
      0.5419700928335369,
      0.4399119518030459,
      0.30171274607189447
    ],
    [
      0.30564184397576044,
      0.38040233010570534,
      0.0,
      0.3700388245231898,
      0.4124340203580199,
      0.4275226181952032,
      0.2869153384166927,
      0.40441638941015623,
      0.4090462793771257,
      0.3535641175810935,
      0.407318471918251,
      0.2017409467707334,
      0.3598472274354989,
      0.4000902065193288,
      0.4378400647213916,
      0.42468483744432617,
      0.3693055505478613,
      0.37263232009785585,
      0.3529347181878155,
      0.4047551470337951,
      0.3627634618340565,
      0.3245745279127563,
      0.371372421209031,
      0.3572518123456192,
      0.37375502677468786,
      0.34222903930483284,
      0.36012420229793474,
      0.3309776274635292,
      0.2511933088712526
    ],
    [
      0.3252405302241268,
      0.5597249959847759,
      0.5160490833838196,
      0.0,
      0.522425741350349,
      0.46556002131551333,
      0.3787921363035911,
      0.45510788574778416,
      0.5342594344514815,
      0.3550070278159292,
      0.5481696592775944,
      0.24889682005201808,
      0.5556172129821348,
      0.5845351981513129,
      0.5108230836683427,
      0.5552377564550788,
      0.4674205103182041,
      0.47949487367282484,
      0.44465031667353205,
      0.4808965826701892,
      0.4184640948513667,
      0.38445958667377766,
      0.4127990180042618,
      0.49273459826456967,
      0.43246631176993544,
      0.3614355781461054,
      0.48342578799836966,
      0.42107043885122186,
      0.3104208253747711
    ],
    [
      0.33593153159073874,
      0.4930012262237584,
      0.44051712127450426,
      0.41945412854625386,
      0.0,
      0.45791914762046093,
      0.3364130751867396,
      0.5037448134748004,
      0.42435737441014476,
      0.3850684323023066,
      0.4506571895038558,
      0.2750291367364066,
      0.4484530825500699,
      0.4732167256112798,
      0.4420134434837304,
      0.4556869580829104,
      0.3962909736599429,
      0.4429667633994212,
      0.4017873021607712,
      0.4090790717457049,
      0.35827719507674316,
      0.29837896673878506,
      0.3810568338916194,
      0.44642911451019107,
      0.4150749824068558,
      0.3949573452248649,
      0.4661149507760378,
      0.34855482938634763,
      0.2831954451885881
    ],
    [
      0.27051128994148543,
      0.43789942794859926,
      0.517732853718605,
      0.40180971394877907,
      0.4533679002374895,
      0.0,
      0.3166525512713452,
      0.43460839233834525,
      0.4387292113261023,
      0.3498519094575554,
      0.42239854178335423,
      0.24900959832693448,
      0.4401074509916958,
      0.43224734101262396,
      0.45005694696979703,
      0.45749352509899954,
      0.43384174082870275,
      0.4447956884594533,
      0.3953222919991213,
      0.41344634443851547,
      0.37841694885974686,
      0.35665369512368805,
      0.4017725687564493,
      0.38169243397849084,
      0.4015660980012503,
      0.37728787046860845,
      0.4016301947776335,
      0.3604717151556913,
      0.3050658284001553
    ],
    [
      0.2851510645105477,
      0.4078729133269918,
      0.3926257380514273,
      0.3941439635315662,
      0.3675754424968043,
      0.4086840655847295,
      0.0,
      0.40470287643124037,
      0.42547477000654577,
      0.3312644660578621,
      0.4371329344001651,
      0.29899253735047226,
      0.44857646144409435,
      0.3653788380757057,
      0.40010597423024574,
      0.40709233535517253,
      0.37047829975665114,
      0.4009580499074139,
      0.4237746113828029,
      0.3793635922363312,
      0.38174083918817847,
      0.32916607811301346,
      0.3329115614428104,
      0.37490668766123125,
      0.38527100496070954,
      0.3100072488990844,
      0.3509309870354538,
      0.327412669178047,
      0.30565213912202127
    ],
    [
      0.3676410165279127,
      0.5690208640483136,
      0.49322275747948097,
      0.45787186934417656,
      0.541089228532869,
      0.5249163519651205,
      0.40711051137610577,
      0.0,
      0.5002849205491968,
      0.37850548318178445,
      0.4412417384329088,
      0.3258109049993503,
      0.5019349029709996,
      0.4752580534432038,
      0.5032102863988208,
      0.5172933916037712,
      0.459130014905919,
      0.5107179164398914,
      0.4184047108490272,
      0.49180271255425523,
      0.516216071081375,
      0.36197840836193196,
      0.40604575488757266,
      0.4245378351879694,
      0.44528695508983374,
      0.4690232935262273,
      0.4930838930164474,
      0.3804077444461602,
      0.3247443961987726
    ],
    [
      0.3643615633174442,
      0.5425012266927463,
      0.5411145792028811,
      0.5209482294632746,
      0.47653694856054507,
      0.4846848506010939,
      0.404577170433682,
      0.5365196004806607,
      0.0,
      0.35564550461869127,
      0.5232563873911797,
      0.2638478342726154,
      0.5319453503136198,
      0.49547271133588056,
      0.4763354437454226,
      0.48629802812827383,
      0.5143175317898157,
      0.48922782961245725,
      0.45414108301187484,
      0.5071894850121146,
      0.4890447749372542,
      0.37364139708287114,
      0.4387791623271806,
      0.46723890567211934,
      0.4510116688313377,
      0.40560991223033804,
      0.4805872140925067,
      0.48354059624042045,
      0.3361803705556987
    ],
    [
      0.3275450999984457,
      0.4501337457730952,
      0.47626317994709244,
      0.39746018304638175,
      0.48613894116477097,
      0.4686883199579619,
      0.35721251512829233,
      0.4966993598270233,
      0.3547578988068689,
      0.0,
      0.44256073988232636,
      0.2877693608034704,
      0.35986018088671456,
      0.46358494874495526,
      0.4841955838462024,
      0.4312201257296022,
      0.43418033977422943,
      0.40085161218309073,
      0.42624561605135813,
      0.4343794009405526,
      0.38255190597366506,
      0.3894193370046699,
      0.4433295197331948,
      0.4685945750003473,
      0.49367011249413806,
      0.40136317353643625,
      0.43289407163891247,
      0.3370317088209078,
      0.3165235801684634
    ],
    [
      0.3673015358385603,
      0.6071341404880817,
      0.6203880077972819,
      0.667264641222598,
      0.5838824318183584,
      0.5681502724908012,
      0.46852362459589014,
      0.5343042785539927,
      0.627048357547568,
      0.44513853156345395,
      0.0,
      0.29243451007245813,
      0.5820396021345347,
      0.6611424888426907,
      0.5593007707314719,
      0.558409527242832,
      0.5077660836642379,
      0.565303282200367,
      0.5063497007168136,
      0.5276585053819098,
      0.48498845459007,
      0.38342903482889,
      0.4517210830806784,
      0.5824372506988758,
      0.5390484285752479,
      0.4361486363956866,
      0.4825575613131152,
      0.49257836016377765,
      0.3740991346951561
    ],
    [
      0.29055718097019567,
      0.40012153887191304,
      0.331941656962625,
      0.3445589205919377,
      0.3850385885571064,
      0.3678355446379078,
      0.34248144594201846,
      0.4015149466781993,
      0.3295816184046745,
      0.3674236544220708,
      0.33076267733681464,
      0.0,
      0.3181562891993195,
      0.3277000403891721,
      0.3435642666832224,
      0.37828587614472986,
      0.3614578837610334,
      0.3535702861172363,
      0.3692711946435503,
      0.3635764982767349,
      0.3403854347323647,
      0.3764203879874062,
      0.33710571685975377,
      0.34876654277742514,
      0.3655618228952462,
      0.39690821109289076,
      0.36410827852680416,
      0.35786637019435474,
      0.35823373450975127
    ],
    [
      0.35808196055400776,
      0.4913411404298871,
      0.4726858375549414,
      0.48850496285701883,
      0.4336790728314084,
      0.48361133458120764,
      0.4183384909796315,
      0.49519584479110246,
      0.4757701983219733,
      0.34036790691804764,
      0.49675555705163044,
      0.2631077810293745,
      0.0,
      0.45956102822186184,
      0.5223913944403697,
      0.4805106344535601,
      0.43431833095928685,
      0.48736266113950344,
      0.409756720837509,
      0.45578669583651577,
      0.45121771123920373,
      0.36621067386801043,
      0.38231584349958614,
      0.4316824631883447,
      0.42106710957966964,
      0.3353765490930254,
      0.41274484963541025,
      0.38013840612400185,
      0.33866192111577753
    ],
    [
      0.3651448001529265,
      0.5410445048575905,
      0.5542834189620756,
      0.5544797647013033,
      0.6146181007626821,
      0.5315042039733768,
      0.36373060178017536,
      0.4959206721473417,
      0.5275543138123018,
      0.39062920888556674,
      0.5893099896203877,
      0.26477070522210133,
      0.5291061207790464,
      0.0,
      0.6158617535827893,
      0.4802234134391796,
      0.4807075762531676,
      0.5030711371550223,
      0.41956410938731725,
      0.5107523409856882,
      0.44231039607286626,
      0.4326501591270904,
      0.43921856482725685,
      0.4890394044274553,
      0.43905939206379596,
      0.38996364571200126,
      0.4795065249541002,
      0.4383831619308536,
      0.3240277151122086
    ],
    [
      0.3363870248512315,
      0.47827079042375686,
      0.5897966178126377,
      0.5043659529356077,
      0.49408628458643644,
      0.5363775513541151,
      0.3914546425176455,
      0.5025261763627578,
      0.49913913572433377,
      0.418453587441884,
      0.5162625056565087,
      0.2726300734243279,
      0.5359066648423985,
      0.5432223472349313,
      0.0,
      0.5445920915660427,
      0.4553629882886281,
      0.4813387842515846,
      0.41418532507951555,
      0.5246009048989979,
      0.4457836788945504,
      0.43819058209374906,
      0.4680657493438152,
      0.45596315398202636,
      0.48930349277871454,
      0.4482695885998351,
      0.4657816866116704,
      0.46227713831688644,
      0.34823717853546365
    ],
    [
      0.3180811694441197,
      0.5289708424929305,
      0.48584280870298135,
      0.49324435097998687,
      0.5129678059321379,
      0.4693781751674604,
      0.35956220759455215,
      0.5123061022261317,
      0.463175492729049,
      0.38824085631456984,
      0.4501371642381109,
      0.2660923061538967,
      0.49947215574282167,
      0.3992733291437953,
      0.49866525358782776,
      0.0,
      0.44523396405195004,
      0.47948515978454864,
      0.421554329090863,
      0.40178214905126364,
      0.3983381549266549,
      0.3427376295821465,
      0.37741918123817153,
      0.4664666844136012,
      0.38316007442727695,
      0.41268961367963164,
      0.435458653987447,
      0.37369478707140913,
      0.2807791714924113
    ],
    [
      0.39792494685058677,
      0.46284546928696946,
      0.5778820589019262,
      0.4813923431828988,
      0.5047416105766975,
      0.4683592230689928,
      0.45817487933470114,
      0.4752364244059897,
      0.48910245101893146,
      0.3720095055654602,
      0.4588489791701309,
      0.303976246695689,
      0.4882623984459593,
      0.4976681650487049,
      0.4752576376251121,
      0.45664679865504,
      0.0,
      0.43775271237781044,
      0.43249578791659293,
      0.4794727915545638,
      0.4671465941925248,
      0.35675334644077705,
      0.48495427473899566,
      0.4488787770245697,
      0.49452536062491825,
      0.40182682692245075,
      0.48306744343605956,
      0.400886208351273,
      0.38338528147506223
    ],
    [
      0.32216086394895993,
      0.4974104180128829,
      0.5400891714243887,
      0.4760469169392172,
      0.4948972951275272,
      0.5148479584969332,
      0.4059962119793701,
      0.48829034546399175,
      0.509643906033489,
      0.3185138794790765,
      0.5317490458785838,
      0.25387878577792633,
      0.5159774537454138,
      0.46710417644968705,
      0.48785840356158516,
      0.5339222984476155,
      0.40153144408426544,
      0.0,
      0.4356957157859689,
      0.5016998427640489,
      0.4528286178821479,
      0.3804833413014874,
      0.38964067217298415,
      0.4907804774304658,
      0.43466965788380163,
      0.35360564366655045,
      0.4271229860949184,
      0.4743663615230136,
      0.32741194612881874
    ],
    [
      0.39608268962108095,
      0.5598274479775103,
      0.4569196992149842,
      0.47436786515161833,
      0.4910444588159435,
      0.5042412830250962,
      0.4346126869240534,
      0.5460420793219207,
      0.45759594703096695,
      0.452843573641613,
      0.49011910895419253,
      0.3290619716871219,
      0.4818824565715183,
      0.4317243950492058,
      0.4456182640799913,
      0.4923212097764669,
      0.5056955548693973,
      0.4825175038167717,
      0.0,
      0.4719869019385279,
      0.4653954823055664,
      0.37545667246965686,
      0.4578295035465716,
      0.4893881116414853,
      0.4884683491127555,
      0.44051414653329357,
      0.4701131609562874,
      0.4181812949519492,
      0.3747099870693633
    ],
    [
      0.3948657757734728,
      0.5400660241536437,
      0.5924388052973419,
      0.4910680793879749,
      0.4845404130789921,
      0.5079062893116333,
      0.42991080266994985,
      0.5275029608180417,
      0.5519307187477154,
      0.4494643964442686,
      0.49245378837828535,
      0.29692482841206425,
      0.49797297676557895,
      0.5331631558405059,
      0.5785891555846923,
      0.45654961743361033,
      0.46841166740992257,
      0.5237405211694912,
      0.4568590041914373,
      0.0,
      0.4588259643663348,
      0.40911527622710175,
      0.4796752949809988,
      0.43988482680754015,
      0.5179293997635297,
      0.39580271757087915,
      0.4913839922130505,
      0.48781282165327533,
      0.3737485492045134
    ],
    [
      0.2856582972275299,
      0.3550265829363146,
      0.4139309209245212,
      0.40988573652972193,
      0.32032143418594594,
      0.38853512589747075,
      0.35625657963028434,
      0.49039887427870643,
      0.45630728645835017,
      0.2811019628327096,
      0.434454138221813,
      0.2115777970662056,
      0.42027001670936714,
      0.42514783997837613,
      0.3767958401625211,
      0.31902016693257895,
      0.3824354646038348,
      0.38025346743708366,
      0.3264297432124299,
      0.3974088574881929,
      0.0,
      0.31103535314706354,
      0.38545683393854735,
      0.40392581059527455,
      0.3521456358421273,
      0.3281152978614492,
      0.3212666803503075,
      0.36182327514383816,
      0.2904288054765871
    ],
    [
      0.33486864085951273,
      0.40050001205235075,
      0.4553538951646763,
      0.39655787407097964,
      0.3935707389281142,
      0.4767642253722726,
      0.369193260055662,
      0.42490894151736014,
      0.426234615299818,
      0.38710681145464676,
      0.38236700743051233,
      0.33280628457419414,
      0.41360968364971296,
      0.42550699564886485,
      0.4750998063288452,
      0.41207817468362484,
      0.385168387024285,
      0.3835850180535194,
      0.3927835008252418,
      0.40990437500665666,
      0.4021041455059886,
      0.0,
      0.38621023935489585,
      0.4139482189829322,
      0.3886980336560444,
      0.41128364105026294,
      0.3987026282287329,
      0.4148984535478053,
      0.38924257082513103
    ],
    [
      0.28835097019908784,
      0.36327507254581737,
      0.4637710021256991,
      0.35702273344325697,
      0.37826940211356086,
      0.4192659201119171,
      0.3200597804118166,
      0.4041304901478304,
      0.3601748208428379,
      0.40196567106965175,
      0.3804013183586845,
      0.23454256688147979,
      0.3765364673589833,
      0.40488488366946074,
      0.4458640291704814,
      0.36233341397170404,
      0.4508595124119983,
      0.36092796633375257,
      0.35049822645869466,
      0.42920374783941107,
      0.35714233670089235,
      0.3386401168177493,
      0.0,
      0.3476143350820369,
      0.5284375176244251,
      0.42347024578549486,
      0.36164246773008846,
      0.3761492976997496,
      0.2933431220869196
    ],
    [
      0.32875010950842687,
      0.48067755467323714,
      0.4274360615868673,
      0.45900733143443184,
      0.5248024250181462,
      0.45069840350766444,
      0.3752804968128596,
      0.42642365806865024,
      0.4255550671722552,
      0.35752479330054143,
      0.4862097894204218,
      0.2540080587582858,
      0.4632763993882849,
      0.47478852304063035,
      0.40562719011036497,
      0.44556621671595487,
      0.4213742841891295,
      0.4655969521316654,
      0.42898061675195875,
      0.41108156955103126,
      0.4095534380815369,
      0.34394671349391825,
      0.426344014138738,
      0.0,
      0.44175627386925354,
      0.35554027296460355,
      0.41379698881966753,
      0.4193030967399405,
      0.30903440200499466
    ],
    [
      0.3404918285224525,
      0.42272915431298697,
      0.47506813568598893,
      0.3836502227417362,
      0.43438998088407366,
      0.45140743414720497,
      0.3542491540988755,
      0.4775236374829859,
      0.3928372589338103,
      0.4127915794796222,
      0.441645949113151,
      0.25967952473446454,
      0.4113614887250421,
      0.3832695668568262,
      0.48684007021926723,
      0.3970402867521443,
      0.43019774945574163,
      0.4248472345001051,
      0.3767497723868216,
      0.47694400452708563,
      0.4097647596827716,
      0.31489604274331495,
      0.47422166799079424,
      0.41389600881403044,
      0.0,
      0.3910633600570068,
      0.3943493781447094,
      0.37352118976226767,
      0.27759729233438657
    ],
    [
      0.27934484527364667,
      0.45957856399343644,
      0.5080938578350847,
      0.38656183553872725,
      0.47514338320700356,
      0.4516800766333362,
      0.35570526099794364,
      0.5033759423056599,
      0.3962341818938362,
      0.44627638736081243,
      0.39134008034456613,
      0.3400353045834901,
      0.3811292341231256,
      0.3993208932879637,
      0.5020768070971184,
      0.4911687440402932,
      0.42383182139947184,
      0.3928926015922969,
      0.3805107830887453,
      0.4237447461729946,
      0.42925932664924216,
      0.4309446994127044,
      0.45238311153635746,
      0.4196379537596746,
      0.482187914812241,
      0.0,
      0.4593673626739414,
      0.3796527976607109,
      0.3156525029529773
    ],
    [
      0.3476016985663124,
      0.4988192508940681,
      0.48720270455014325,
      0.4085585803096541,
      0.5281386661809213,
      0.4776501501777286,
      0.38031925612947437,
      0.5137410276054477,
      0.4528651659280718,
      0.4516917524674491,
      0.4513440722539033,
      0.306090593040242,
      0.4351718797092341,
      0.4462619530244776,
      0.43320284679392107,
      0.468539159190009,
      0.4598589401803541,
      0.4745836280483411,
      0.3939478691070175,
      0.45450710675588835,
      0.33826681814874315,
      0.33428111419997886,
      0.45978262521290447,
      0.43971480592462475,
      0.472863301321091,
      0.4301018100526217,
      0.0,
      0.39257589102610124,
      0.28803707483402463
    ],
    [
      0.3485760501954138,
      0.4682508368300218,
      0.5098926534961663,
      0.46381442877377665,
      0.45336842675438094,
      0.5198348043001986,
      0.4295400354423178,
      0.49217836260764924,
      0.5629924042832855,
      0.38965512739588104,
      0.5300397271768955,
      0.335719623858471,
      0.5157754822898266,
      0.5018029549155303,
      0.49355262876635786,
      0.4395446385163879,
      0.4855948405170527,
      0.5036065409690287,
      0.4510871530148084,
      0.49195619679266334,
      0.5064381190724008,
      0.4403057628985514,
      0.5053020394870595,
      0.505295682539602,
      0.48603157995941726,
      0.4355568736684423,
      0.4572537885347179,
      0.0,
      0.40477051944688625
    ],
    [
      0.40138752692320456,
      0.4130306442410159,
      0.4570710182895934,
      0.4223261233101079,
      0.4241894418411831,
      0.47392715045494627,
      0.4056108149392774,
      0.46657919774238543,
      0.44440684756765947,
      0.3980987619641372,
      0.4088798525661348,
      0.3779528482292889,
      0.4399935231508352,
      0.4512393325645856,
      0.43387131678356083,
      0.38252882110578823,
      0.47835445581974456,
      0.4489489029388165,
      0.4415422192814078,
      0.46020592187523324,
      0.43347938094486493,
      0.5644595941581549,
      0.4487889133540415,
      0.44744825348190154,
      0.4769823240113136,
      0.37014101782034836,
      0.4330004085421586,
      0.41327454433164257,
      0.0
    ]
  ],
  "row_avgs": [
    0.18512230954839573,
    0.4817966559580932,
    0.3626918814511967,
    0.4537566110868922,
    0.4065581128844227,
    0.39730143120068634,
    0.3731195767763329,
    0.45377828526426417,
    0.4605198342840714,
    0.4158973263165417,
    0.5170195799016214,
    0.3554555931488021,
    0.42809082432613815,
    0.4716584178817024,
    0.4664582749432162,
    0.42372177047313375,
    0.4514098051031925,
    0.4438651370530401,
    0.46016292164481826,
    0.4760192079877088,
    0.3637647794667554,
    0.4029662921125944,
    0.37781347982119584,
    0.415426453616195,
    0.402965133324631,
    0.4198975364366929,
    0.42948999077259814,
    0.4688477600893997,
    0.43634711279404753
  ],
  "col_avgs": [
    0.33694301429712536,
    0.4647706184639029,
    0.4839427105946856,
    0.4421489663033306,
    0.46396626371865846,
    0.46415025092351,
    0.37318854708535915,
    0.4710378851069684,
    0.45341287909504163,
    0.37735058839830515,
    0.4541876972129269,
    0.27782394554656664,
    0.45301279152617696,
    0.45167316450327244,
    0.464129806736926,
    0.4507430212308921,
    0.4318528044884837,
    0.4424850464504812,
    0.4080012534650093,
    0.4422649901674447,
    0.4137736538150341,
    0.3670871080942978,
    0.41441137924389265,
    0.4342467487740437,
    0.43751512966756206,
    0.38859718707621577,
    0.4266483071892396,
    0.3932548472642953,
    0.3193014892287325
  ],
  "combined_avgs": [
    0.26103266192276053,
    0.47328363721099803,
    0.4233172960229411,
    0.4479527886951114,
    0.43526218830154056,
    0.43072584106209816,
    0.373154061930846,
    0.46240808518561627,
    0.45696635668955654,
    0.3966239573574234,
    0.4856036385572742,
    0.31663976934768434,
    0.44055180792615756,
    0.4616657911924874,
    0.4652940408400711,
    0.4372323958520129,
    0.4416313047958381,
    0.4431750917517606,
    0.4340820875549138,
    0.45914209907757675,
    0.38876921664089475,
    0.3850267001034461,
    0.3961124295325442,
    0.42483660119511935,
    0.4202401314960965,
    0.40424736175645437,
    0.42806914898091886,
    0.4310513036768475,
    0.37782430101139003
  ],
  "gppm": [
    619.213071303193,
    590.2183591978913,
    577.0721534875795,
    602.6103569562862,
    584.1729761866842,
    588.1139772976527,
    632.7559916569651,
    586.4932142816837,
    595.7865531223794,
    628.0217717069505,
    595.5229963921469,
    678.574448896615,
    596.8356474941766,
    596.4544800760979,
    592.7384571076776,
    594.4173264813281,
    604.8809735001292,
    600.9850126252238,
    616.0189173809455,
    600.0615770059488,
    611.1111870713609,
    633.5334164082757,
    613.1088018386077,
    606.1865252533254,
    601.0856237835063,
    623.0982587892937,
    609.1468605630314,
    626.0647535085976,
    659.3957561863901
  ],
  "gppm_normalized": [
    1.4496822234869846,
    1.3090660498704862,
    1.2747019905529673,
    1.339979543090676,
    1.2854279597103162,
    1.2986346469238323,
    1.4109340962199954,
    1.2932904688429618,
    1.3154737548052011,
    1.3854451025479089,
    1.3088979992316516,
    1.5147630879962684,
    1.3175083216037546,
    1.3159159145908395,
    1.3093883830339883,
    1.323449233038786,
    1.3316364167560375,
    1.3327549681829922,
    1.3609739765229822,
    1.321395416154995,
    1.3485008199855317,
    1.4044482304341623,
    1.3508720442948525,
    1.3362613010834092,
    1.318716286203411,
    1.3859306975628376,
    1.3411751968923091,
    1.3847310374141564,
    1.4564522231012482
  ],
  "token_counts": [
    1329,
    483,
    449,
    539,
    405,
    428,
    556,
    422,
    421,
    395,
    365,
    540,
    411,
    423,
    433,
    505,
    392,
    477,
    403,
    400,
    409,
    433,
    408,
    393,
    377,
    493,
    377,
    420,
    364,
    634,
    434,
    561,
    437,
    479,
    455,
    451,
    400,
    433,
    399,
    414,
    372,
    455,
    418,
    429,
    498,
    385,
    425,
    421,
    407,
    425,
    412,
    398,
    424,
    446,
    409,
    420,
    364,
    318
  ],
  "response_lengths": [
    3163,
    2452,
    3196,
    2548,
    2751,
    2656,
    2638,
    2234,
    2369,
    2255,
    2442,
    2148,
    2593,
    2458,
    2378,
    2802,
    2179,
    2430,
    2385,
    2305,
    2393,
    2427,
    2163,
    2487,
    2408,
    2242,
    2314,
    1961,
    1908
  ]
}