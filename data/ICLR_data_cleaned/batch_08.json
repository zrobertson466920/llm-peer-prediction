[
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWEAKLY-SUPERVISED NEURO-SYMBOLIC IMAGE MANIPULATION VIA MULTI-HOP COMPLEX INSTRUCTIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe are interested in image manipulation via natural language text – a task that is extremely useful for multiple AI applications but requires complex reasoning over multi-modal spaces. Recent work on neuro-symbolic approaches e.g., The Neuro Symbolic Concept Learner (NSCL) (Mao et al., 2019) has been quite effective for solving Visual Question Answering (VQA) as they offer better modularity, interpretability, and generalizability. We extend NSCL for the image manipulation task and propose a solution referred to as NEUROSIM. Previous work either requires supervised training data in the form of manipulated images or can only deal with very simple reasoning instructions over single object scenes. In contrast, NEUROSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NEUROSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides the manipulation. We design neural modules for manipulation, as well as novel loss functions that are capable of testing the correctness of manipulated object and scene graph representations via query networks trained merely on VQA data. An image decoder is trained to render the final image from the manipulated scene graph. Extensive experiments demonstrate that NEUROSIM, without using target images as supervision, is highly competitive with SOTA baselines that make use of supervised data for manipulation.\n\n1\n\nINTRODUCTION\n\nThe last decade has seen a significant growth in application of neural models to a variety of tasks including those in computer vision (Chen et al., 2017; Krizhevsky et al., 2012), NLP (Wu et al., 2016), robotics and speech (Yu & Deng, 2016). It has been observed that these models often lack interpretability (Fan et al., 2021), and may not always be well suited to handle complex reasoning tasks (Dai et al., 2019). On the other hand, classical AI systems can seamlessly perform complex reasoning in an interpretable manner due to their symbolic representation (Pham et al., 2007; Cai & Su, 2012). But these models are often found lacking in their ability to handle low level representations and be robust to noise. A natural question then arises: Can we design models which capture the best of both these paradigms? The answer lies in the recent development of Neuro-Symbolic models (Dong et al., 2019; Mao et al., 2019; Han et al., 2019) which combine the power of (purely) neural with (purely) symbolic representations. An interesting sub-class of these models work with a finite sized domain specification language (DSL) and make use of deep networks to learn neural representations of the concepts specified in the DSL. The learned representations are then used for performing downstream reasoning via learning of symbolic programs. This line of work was first popularized by Andreas et al. (2016); Hu et al. (2017); Johnson et al. (2017a), followed by Mao et al. (2019), who look at the task of Visual Question Answering (VQA), and other follow-up works such as learning meta-concepts (Han et al., 2019). Studies (Andreas et al., 2016; Hu et al., 2017; Mao et al., 2019) have shown that these models have several desirable properties such as modularity, interpretability, and improved generalizability.\n\nMotivated by the above, our aim is to build neuro-symbolic models for the task of weakly supervised manipulation of images comprising multiple objects, via complex multi-hop natural language instructions. Existing work includes weakly supervised approaches (Nam et al., 2018; Li et al., 2020) that require textual descriptions of images during training and are limited to very simple scenes (or\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ninstructions). Supervised approaches (Zhang et al., 2021; El-Nouby et al., 2019), though capable of handling multiple objects and complex multi-hop instructions, require explicit annotations in the form of target manipulated images; ref. Section 2 for a survey. We are interested in a weakly supervised solution that only makes use of data annotated for VQA, avoiding the high cost of getting supervised annotations, in the form of target manipulated images. Our key intuition is: this task can be solved by simply querying the manipulated representation without ever explicitly looking at the target image.\n\nFigure 1: The problem setup.\n\nOur solution builds on Neuro-Symbolic Concept Learner (NSCL) proposed by Mao et al. (2019) for solving VQA. We extend this work to incorporate the notion of manipulation operations such as change, add, and remove objects in a given image. As one of our main contributions, we design novel neural modules and a training strategy that just uses VQA annotations as weakly supervised data for the task of image manipulation. The neural modules are trained with the help of novel loss functions that measure the faithfulness of the manipulated scene and object representations by accessing a separate set of query networks, interchangeably referred to as quantization networks, trained just using VQA data. The manipulation takes place through interpretable programs created using primitive neural and symbolic operations from a Domain Specific Language (DSL). Separately, a network is trained to render the image from a scene graph representation using a combination of L1 and adversarial losses as done by Johnson et al. (2018). The entire pipeline is trained without any intermediate supervision. We refer to our system as Neuro-Symbolic Image Manipulator (NEUROSIM). Figure 1 shows an example of I/O pair for our approach.\n\nFor our experiment purposes, we extend CLEVR (Johnson et al., 2017b), a benchmark dataset for VQA, to incorporate manipulation instructions and create a dataset referred to as Complex Image Manipulation via Natural Language Instructions (CIM-NLI). We will release this dataset publicly post acceptance. Our evaluation on CIM-NLI dataset shows that, despite being weakly supervised. we are highly competitive or improve upon state-of-the-art supervised approaches (Zhang et al., 2021; El-Nouby et al., 2019) for this task, generalize well to scenes with more objects, and specifically perform well on instructions which involve multi-hop reasoning.\n\n2 RELATED WORK\n\nTable 1 categorizes the related work across three broad dimensions - problem setting, task complexity, and approach. The problem setting comprises of two sub-dimensions: i) supervision type - self, direct, or weak, ii) instruction format - text or UI-based. The task complexity comprises of following sub-dimensions: ii) scene complexity – single or multiple objects, ii) instruction complexity - zero or multi-hop instructions, iii) kinds of manipulations allowed - add, remove, or change. Finally, approach consists of the following sub-dimensions: i) model – neural or neuro-symbolic and ii) whether symbolic program is generated on the way or not.\n\nDong et al. (2017), TAGAN (Nam et al., 2018), and ManiGAN (Li et al., 2020) are close to us in terms of the problem setting. These manipulate the source image using a GAN-based encoder-decoder architecture. Their weak supervision differs from ours – We need VQA annotation, they need captions or textual descriptions. The complexity of their natural language instructions is restricted to 0-hop. Most of their experimentation is limited to single (salient) object scenes, and it is unclear how these strategies would perform with multi-object situations with intricate relationships. Lastly, while our approach requires only an explicit manipulation (delta) command during inference, existing approaches require partial target image description, and it is unclear how their method can be extended to the task where only the delta is given.\n\nIn terms of task complexity, the closest to us are approaches such as TIM-GAN (Zhang et al., 2021), GeNeVA (El-Nouby et al., 2019), which build an encoder decoder architecture and work with a latent representation of the image as well as the manipulation instruction. They require explicit annotations in terms of manipulated images during training. We argue that this can require a significant more\n\n2\n\nNeuro-symbolic Image Manipulator(NEUROSIM)Change the size of the thing behindthe largeballto bigO ßscene()O ßfilter(O, large)O ßfilter(O, sphere)O ßrelate(O, behind)O ßchange_size(O, large)Input: Source image IOutput: Manipulated imageI\"Input: Instruction Text TOutput: Manipulation program PUnder review as a conference paper at ICLR 2023\n\nPrior Work\n\nProblem Setting ST\n\nIF\n\nTask Complexity IC Operations\n\nSC\n\nApproach Model Program\n\nSS SIMSG DS PGIM DS GeNeVA DS TIM-GAN WS Dong et. al WS TAGAN WS ManiGAN NEUROSIM (ours) WS\n\nUI N/A Text# Text# Text# Text# Text# Text\n\nMO† N/A change, remove, add MO†* N/A change (image level) MO MH add MO† SO† SO† SO† MO MH change, remove, add\n\nZH change, remove, add ZH change ZH change ZH change\n\nN NS N\nN N\nN N\nNS\n\n✗ ✓\n✗ ✗\n✗ ✗\n✗ ✓\n\nTable 1: Comparison of Prior Work. Abbreviations (column titles) ST:= Supervision Type, IF:= Instruction Format, SC:= Scene Complexity, IC:=Instruction Complexity. Abbreviations (column values) SS:=Self Supervision, DS:=Direct Supervision, WS:=Weak Supervision, #: Human Written, MO:= Multiple Objects, MO∗:= Multiple Objects with Regular Patterns, SO:= Single Object, †: Natural Images, N/A:= Not applicable, MH:=Multi-Hop, ZH:=Zero-Hop, N:= Neural, NS:= Neuro-Symbolic, ✓:= Yes, ✗:= No. See Section 2 for more details.\n\nannotation effort, compared to our weak supervision setting, where we only need visual question answer annotations. Unlike us, these approaches work with purely neural models, and as shown in our experiments, their performance is heavily dependent on the amount of data available for training.\n\nIn terms of technique, the closest to our work are neuro-symbolic approaches for VQA such as NSVQA (Yi et al., 2018), NSCL (Mao et al., 2019), Neural Module Networks (Andreas et al., 2016) and its extensions (Hu et al., 2017; Johnson et al., 2017a). Clearly, while the modeling approach is similar and consists of constructing latent programs, the desired task are different in two cases. Our work extends the NSCL approach for the task of automated image manipulation.\n\nA related task is text guided image retrieval, where goal is to retrieve (not manipulate) an image from the database complying with the changes asked for in the input instruction (Vo et al., 2019; Chen et al., 2020). Another line of research (Jiang et al., 2021; Shi et al., 2021) deals with editing global features, such as brightness, contrast, etc., instead of object level manipulations like in our case. Recent works (Ramesh et al., 2022; Saharia et al., 2022) on text to image generation using diffusion models trained on massive (image, caption) data, are capable of generating photorealistic images given text. These also have the capability of editing images e.g. using text-diffs (Ramesh et al., 2022) but require captions for input images. Further, it is unclear how to extend this line of work to language guided complex image manipulation settings where multi-hop reasoning may be required; preliminary studies (Marcus et al., 2022) have highlighted their shortcomings in terms of compositional reasoning and dealing with relations.\n\n3 NEUROSIM: NEURO-SYMBOLIC IMAGE MANIPULATOR\n\n3.1 MOTIVATION AND ARCHITECTURE OVERVIEW\n\nThe key motivation behind our approach comes from the following hypothesis: consider a learner L (e.g., a neural network or the student in Fig 2) with sufficient capacity trying to achieve the task of manipulation over Images I. Further, let each image be represented in terms of its properties, or properties of its constituent parts (e.g. objects like apple, leaf, tree as shown in Fig 2), where each property comes from a finite set S e.g, attributes of objects in an image. Let the learner be provided with the prior knowledge (for e.g. through Question Answering as in Fig 2) about properties (e.g., color) and their possible values (e.g., red). Then, in order to learn the task of manipulation, it suffices to provide the learner with a query network, which given a manipulated image ̃I constructed by the learner via command C, can correctly answer questions (i.e. query) about the desired state of various properties of the constituents of the image\n\nFigure 2: Motivating example for our approach.\n\n3\n\nWhat is the color of the apple on the tree?Paint the red apples on the tree with blue colorTryLearning knowledge of concepts, attributes, relational-concepts eg color, red, apple, tree, on the via many QnA pairsRed ✔Green ✖Learning to manipulate an image via self correction using knowledge of concepts and attributes acquired before✖Are the apples blue? NoRetry✔Under review as a conference paper at ICLR 2023\n\n ̃I. The query network can be internal to the learner (e.g., the student in Fig 2 can query himself for checking the color of apples in the manipulated image). The learner can query repeatedly until it learns to perform the manipulation task correctly. Note that the learner does not have access to the supervised data corresponding to triplets of the form (Is, C, If ), where Is is the starting image, C is the manipulation command, and If is the resulting final image, for the task of manipulation. Inspired by this, we set out to test this hypothesis by building a model capable of manipulating images, without target images as supervision.\n\nFigure 3 captures a high level architecture of the proposed NEUROSIM pipeline. NEUROSIM allows manipulating images containing multiple objects, via complex natural language instructions. Similar to Mao et al. (2019), NEUROSIM assumes the availability of a domain-specific language (DSL) for parsing the instruction text T into an executable program P . NEUROSIM is capable of handling addition, removal, and change operations over image objects. It reasons over the image for locating where the manipulation needs to take place followed by carrying out the manipulation operation. The first three modules, namely i) visual representation network, ii) semantic parser, and iii) concept quantization network are suitably customized from the NSCL and trained as required for our purpose. In what follows, we describe the design and training mechanism of NEUROSIM.\n\nFigure 3: High level architecture of NEUROSIM.\n\n3.2 MODULES INHERITED FROM NSCL\n\n1] Visual Representation Network: Given input image I, this network converts it into a scene graph GI = (N, E). The nodes N of this scene graph are object embeddings and the edges E are embeddings capturing relationship between pair of objects (nodes). Node embeddings are obtained by passing the bounding box of each object (along with the full image) through a ResNet-34 (He et al., 2016b). Edge embeddings are obtained by concatenating the corresponding object embeddings.\n\n2] Semantic Parsing Module: The input to this module is a manipulation instruction text T in natural language. Output is a symbolic program P generated by parsing the input text. The symbolic programs are made of operators, that are part of our DSL (Specified in Appendix Section A).\n\n3] Concept Quantization Network: Any object in an image is defined by the set of visual attributes (A), and set of symbolic values (Sa) for each attribute a ∈ A. E.g., attributes can be shape, size, etc. Different symbolic values allowed for an attribute are also known as concepts. E.g., Scolor = {red, blue, green, . . .}. Each visual attribute a ∈ A is implemented via a separate neural network fa(·) which takes the object embedding as input and outputs the attribute value for the object in a continuous (not symbolic) space. Let fcolor : Rdobj −→ Rdattr represent a neural network for the color attribute and consider o ∈ Rdobj as the object embedding. Then, vcolor = fcolor(o) ∈ Rdattr is the embedding for the object o pertaining to the color attribute. Each symbolic concept s ∈ Sa for a particular attribute a (e.g., different colors) is also assigned a respective embedding in the same continuous space Rdattr. Such an embedding is denoted by cs. These concept embeddings are initialized at random, and later on fine tuned during training. An attribute embedding (e.g. vcolor) can be compared with the embeddings of all the concepts (e.g., cred, cblue, etc.) using cosine similarity, for the purpose of concept quantization of objects.\n\nTraining for VQA: As a first step, we train all the above three modules via a curriculum learning process (Mao et al., 2019). Semantic parser is trained jointly with the concept quantization networks for generating programs corresponding to the question texts coming from the VQA dataset. The corresponding output programs are composed of primitive operations coming from the DSL (e.g. filter, count, etc.) and does not include constructs related to manipulation operations. This trains the first three modules with high accuracy on the VQA task.\n\n4\n\n[2] Semantic Parsing ModuleO ßscene()O ßfilter(O, large)O ßfilter(O, sphere)O ßrelate(O, behind)O ßchange_size(O, large)Program P[1] Visual Representation NetworkScene graph GIfor source imageSource image IScene graph GI\"for target image[5] Rendering NetworkChange the size of the thing behindthe largeballto bigInstruction Text TManipulated imageI\"[3] Concept Quantization Network[4] Manipulation NetworkProgram ExecutorUnder review as a conference paper at ICLR 2023\n\n3.3 NOVEL MODULES AND TRAINING PROCEDURE FOR NEUROSIM\n\nNEUROSIM training starts with three sub-modules trained on the VQA task as described in Section 3.2. Next, we extend the original DSL to include three additional functional sub-modules within semantic parsing module, namely add, remove, and change. Refer to appendix section A for details on the DSL. We now reset the semantic parsing module and train it again from scratch for generating programs corresponding to image manipulation instruction text T . Such a program is subsequently used by the downstream pipeline to reason over the scene graph GI and manipulate the image. In this step, the semantic parser is trained using an off-policy program search based REINFORCE (Williams, 1992) algorithm. Unlike the training of semantic parser for the VQA task, in this step, we do not have any final answer like reward supervision for training. Hence, we resort to a weaker form of supervision. In particular, consider an input instruction text T and set of all possible manipulation program templates Pt from which one can create any actual program P that is executable over the scene graph of the input image. For a program P ∈ Pt, our reward is positive if this program P selects any object (or part of the scene graph) to be sent to the manipulation networks (change/add/remove). For e.g., consider the program change(filter(scene())), if after executing filter(scene()), we do not get even a single object selected, then we give a negative reward, signifying that this program cannot be correct, else we give a positive reward. Incorrect programs can also lead to object (objects) being selected for manipulation, which is why this is a weak supervision. See Appendix C for more details.\n\nOnce semantic parser is retrained, we clamp the first three modules and continue using them for the purpose of parsing instructions and converting images into their scene graph representations. Scene graphs are manipulated using our novel module called manipulation network which is describe next.\n\n4] Manipulation Network: This is our key module responsible for carrying out the manipulation operations. We allow three kinds of manipulation operations – add, remove, and change. Each of these operations are a composition of a quasi-symbolic and symbolic operation. A symbolic operation corresponds to a function that performs the required structural changes (i.e. addition/deletion of a node or an edge) in the scene graph GI against a given instruction. A quasi-symbolic operations is a dedicated neural network that takes the relevant part of GI as input and computes new representations of nodes and edges that are compatible with the changes described in the parsed instruction. (a) Change Network: For each visual attribute a ∈ A (e.g. shape, size, . . . ), we have a separate change neural network that takes the pair of (object embedding, embedding of the changed concept) as input and outputs the embedding of the changed object. This is the quasi-symbolic part for the change function, while the symbolic part is an identity mapping. For e.g., let gcolor : Rdobj+dattr −→ Rdobj represent the neural network that changes the color of an object. Consider o ∈ Rdobj as the object embedding and cred ∈ Rdattr as the concept embedding for the red color, then (cid:101)o = gcolor(o; cred) ∈ Rdobj represents the changed object embedding, whose color would be red. After applying the change neural network, we obtain the changed representation of the object (cid:101)o = ga(o; cs∗ a is the desired changed value for the attribute a. This network is trained using following losses.\n\n), where s∗\n\na\n\n(cid:88)\n\n∀s∈Sa\n\n(cid:88)\n\nla = −\n\nla = −\n\nIs=s∗\n\na\n\nlog [p(ha ((cid:101)o) = s)]\n\n(cid:88)\n\n∀a′∈A,a′̸=a\n\n∀s∈Sa′\n\np(ha′(o) = s) log [p(ha′((cid:101)o) = s)]\n\n(1)\n\n(2)\n\nwhere, ha(x) gives the concept value of the attribute a (in symbolic form s ∈ Sa) for the object x. The quantity p (ha(x) = s) denotes the probability that the concept value of the attribute a for the object x is equal to s and is given as follows p (ha(x) = s) = expdist(fa(x),cs)/(cid:80) (cid:101)s) where, dist(a, b) = (a⊤b − t2)/t1 is the shifted and scaled cosine similarity, t1, t2 being constants. The first loss term la penalizes the model if the (symbolic) value of the attribute a for the manipulated object is different from the desired value s∗ a in terms of probabilities. The second term la, on the other hand, penalizes the model if the values of any of the other attributes a′, deviate from their original values. Apart from these losses, we also include following additional losses.\n\nexpdist(fa(x),c\n\n(cid:101)s∈Sa\n\nlcycle = ∥o − ga((cid:101)o; cold)∥2; lconsistency = ∥o − ga(o; cold)∥2 [log D(o′) + log(1 − D (ga(o′; c)))] lobjGAN = −\n\n(cid:88)\n\no′∈O\n\n(3)\n\n(4)\n\nwhere cold is the original value of the attribute a of object o, before undergoing change. Intuitively the first loss term lcycle says that, changing an object and then changing it back should result in the same object. The second loss term lconsistency intuitively means that changing an object o that has value cold\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nfor attribute a, into a new object with the same value cold, should not result in any change. These additional losses prevent the change network from changing attributes which are not explicitly taken care of in earlier losses (1) and (2). For e.g., rotation or location attributes of the objects that are not part of our DSL. We also impose an adversarial loss lobjGAN to ensure that the new object embedding (cid:101)o is from the same distribution as real object embeddings. See Appendix C for more details. (b) Remove Network: The remove network takes the scene graph GI of the input image and removes the subgraph from GI that contains the nodes (and incident edges) corresponding to the object(s) that need to be removed, and returns a new scene graph G (cid:101)I which is reduced in size. The quasi-symbolic function for the remove network is identity. (c) Add Network: For adding a new object into the scene, add network requires the symbolic values of different attributes, say {sa1, sa2 , . . . , sak }, for the new object, e.g., {red, cylinder, . . .}. It also requires the spatial relation r (e.g. RightOf) of the new object with respect to an existing object in the scene. The add function works by first predicting the object (node) embedding (cid:101)onew for the object to be added, followed by predicting edge embeddings for new edges incident on the new node. New object embedding is obtained as follows: (cid:101)onew = gaddObj({csa1 }, orel, cr) where, orel is the object embedding of an existing object, relative to which new object’s position r is specified. After this, for each existing objects oi in the scene, an edge (cid:101)enew,i is predicted between the newly added object (cid:101)onew and existing object oi in following manner: (cid:101)enew,i = gaddEdge((cid:101)onew, oi). Functions gaddObj(·) and gaddEdge(·) are quasi-symbolic operations. Symbolic operations in add network comprises adding the above node and the incident edges into the scene graph.\n\n, · · · , csak\n\n, csa2\n\nThe add network is trained in a self-supervised manner. For this, we pick a training image and create it’s scene graph. Next, we randomly select an object o from this image and quantize it’s concepts, along with a relation with any other object oi in the same image. We then use our remove network to remove this object o from the scene. Finally, we use the quantized concepts and the relation that were gathered above and add this object o back into the scene graph using gaddObj(·) and gaddEdge(·). Let the embedding of the object after adding it back is (cid:101)onew. Training losses for this network are as follows:\n\nlconcepts = −\n\n(cid:88)k\n\nj=1\n\nlog(cid:0)p(haj ((cid:101)onew) = saj )(cid:1); lrelation = − log(p(hr((cid:101)onew, oi) = r))\n\nlobjSup = ∥o − (cid:101)onew∥2; ledgeSup =\n\n(cid:88)\n\n∥eold,i − (cid:101)enew,i∥2\n\ni∈O\n\nledgeGAN = −\n\n(cid:88)\n\n∀i∈O\n\n[log D({o; eold,i; oi}) + log(1 − D ({(cid:101)onew; (cid:101)enew,i; oi}))]\n\n(5)\n\n(6)\n\n(7)\n\nwhere saj is the required (symbolic) value of the attribute aj for the original object o, and r is the required relational concept. O is the set of the objects in the image, eold,i is the edge embedding for the edge between original object o and its neighboring object oi. Similarly, (cid:101)enew,i is the corresponding embedding of the same edge but after when we have (removed + added back) the original object. The loss terms lconcepts and lrelation ensure that the added object comprises desired values of attributes and relation, respectively. Since we had first removed and then added the object back, we already have the original edge and object representation, and hence we use them in loss terms given in equation 6. We use adversarial loss equation 7 for generating real (object, edge, object) triples and also a loss similar to equation 4 for generating real objects. For optimizing the generator in eq. equation 4 equation 7 modified GAN loss (Goodfellow et al., 2014) is used.\n\n3.4\n\nIMAGE RENDERING FROM SCENE GRAPH\n\n5] Rendering Network: Design and training methodology for this module closely follows Johnson et al. (2018). We take multiple images {I1, I2 · · · In} and generate their scene graph {GI1 , GI2 · · · GIn} using the visual representation network described earlier. Each of these scene graphs is then processed by a graph convolutional network and passed through a mask regression network followed by a box regression network to generate a coarse 2-dimensional structure (scene layout). A Cascaded Refinement Network (CRN) (Chen & Koltun, 2017) is then employed to generate an image from the the scene layout. A min-max adversarial training procedure is used to generate realistic images as formulated in GAN (Goodfellow et al., 2014), using two discriminators – i) A patch-based image discriminator that ensures the quality of overall image, and ii) An object discriminator that ensures the quality of object appearance.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4 EXPERIMENTS\n\nDatasets: Among the existing datasets, CSS (Vo et al., 2019) contains simple 0-hop instructions and is primarily designed for the text guided image retrieval task. CRIR (Chen et al., 2020) extends CSS to include multi-hop instructions but is not open source. Other datasets such as i-CLEVR (El-Nouby et al., 2019) and CoDraw are designed for iterative image editing (El-Nouby et al., 2019). i-CLEVR contains only \"add\" instructions and CoDraw doesn’t contain multi-hop instructions. Hence we created our own multi-object multi-hop instruction based image manipulation dataset, referred to as CIM-NLI. This dataset was generated with the help of CLEVR toolkit (Johnson et al., 2017b) – details of the generation process are described in the Appendix B. CIM-NLI consists of (Source image I, Instruction text T , Target image (cid:101)I ∗) triplets. The dataset contains a total of 18K, 5K, 5K unique images and 54K, 14K, 14K instructions in the train, validation and test splits respectively. Refer Appendix B for more details about the dataset splits.\n\nBaselines: Weakly supervised baselines (Li et al., 2020; Nam et al., 2018) for this task are designed for a problem setting different from ours – single salient object scenes, simple 0-hop instructions (Refer Section 2 for details). Further, they require paired images and their textual descriptions as annotations. We, therefore, do not compare with them in our experiments. Instead, we compare our model with purely supervised approaches such as TIM-GAN (Zhang et al., 2021) and GeNeVA (ElNouby et al., 2019). In order to make a fair and meaningful comparison between the two kinds (supervised and weakly-supervised) approaches, we carve out the following set-up. Assume the cost required to create one single annotated example for image manipulation task be αm while the corresponding cost for the VQA task be αv. Let α = αm/αv. Let βm be the number of annotated examples required by a supervised baseline for reaching a performance level of ηm on the image manipulation task. Similarly, let βv be the number of annotated VQA examples required to train NEUROSIM to reach the performance level of ηv. Let β = βm/βv. We are interested in figuring out the range of β for which performance of our system (ηv) is at least as good as the baseline (ηm). Correspondingly we can compute the ratio of the labelling effort required, i.e., α ∗ β, to reach these performance levels. If α ∗ β > 1, our system achieves the same or better performance, with lower annotation cost. See Appendix F, G for computational resources and hyperparameters respectively.\n\nEvaluation Metrics: For evaluation on image manipulation task, we use two metrics - i) FID, ii) Recall@k. FID (Heusel et al., 2017) measures the realism of the generated images. Recall@k measures the semantic similarity of gold manipulated image (cid:101)I ∗ and system produced manipulated image (cid:101)I. For computing Recall@k, we use (cid:101)I as a query and retrieve images from a corpus comprising the entire test set (gold manipulated images) and the source image I corresponding to (cid:101)I. similar to Zhang et al. (2021), the query image and the corpus images are embedded into a latent space through an autoencoder trained on CLEVR dataset. Cosine similarity is used for ranking retrieved images.\n\nβ = 0.054\n\nβ = 0.07\n\nβ = 0.1\n\nβ = 0.2\n\nβ = 0.54\n\nMethod\n\nFID R1 R3 FID R1 R3 FID R1 R3 FID R1 R3 FID R1 R3\n\nGeNeVA\n\n22.0 6.6 58.7\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\nTIM-GAN\n\n4.8 31.9 74.2\n\n4.4 32.6 80.0\n\n4.3 38.5 82.5\n\n4.9 47.4 86.4\n\n– 10.3 4.6 64.4 4.0 58.1 90.2\n\nNEUROSIM 13.8 45.3 65.5 13.7 45.8 66.7 14.0 45.6 66.7 14.1 45.6 67.9 13.8 45.5 66.7\n\nTable 2: Performance comparison of NEUROSIM with TIM-GAN and GeNeVA with varying β levels. The ‘-’ entries for GeNeVA were not computed due to excessive training time; it’s performance is low even when using full data. We always use 100K VQA examples (5K Images, 20 questions per image) for our weakly supervised training. R1, R3 correspond to Recall@1,3 respectively. FID: lower is better; Recall: higher is better.\n\n4.1 PERFORMANCE WITH VARYING DATASET SIZE\n\nTable 2 compares the performance of NEUROSIM system with TIM-GAN and GeNeVA with varying levels of β on CIM-NLI. Despite being weakly supervised, NEUROSIM performs significantly better than both the baselines for β ≤ 0.1 ( alternatively α ≥ 10) and very close to its closest competitor for β = 0.2 ( alternatively α = 5), using the R@1 performance metric. This clearly demonstrates the strength of our approach in learning to manipulate while only making use of VQA annotations. We hypothesize that, in most cases, NEUROSIM will be preferable since, we expect the cost of\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nannotating an output image for manipulation to be significantly higher than the cost of annotating a VQA example. FID scores for NEUROSIM could potentially be improved by a doing a joint training of VQA module along with image decoder loss, and is a direction for future work.\n\n4.2 PERFORMANCE WITH INCREASING REASONING HOPS\n\nTable 3 compares baselines with NEUROSIM for performance over instructions requiring 0-hop versus multi-hop (1 − 3 hops) reasoning. When dealing with multi-hop instructions, we see a massive drop of 14.8 and 7.8 points in the performance of TIM-GAN trained on 10% (5.4Kdata points) and Full (54Kdata points) CIM-NLI data respectively. NEUROSIM trained on 10% data, without output image supervision, sees a performance drop of only 1.5 points implying that it is much better at handling the complex reasoning involved.\n\nMethod\n\nGeNeVA (54K)\n\nGeNeVA (5.4K))\n\nTIM-GAN (54K)\n\nTIM-GAN (5.4K)\n\nNEUROSIM (5.4K)\n\nHops\n\nZH\n\nM H\n\n6.4 (+0.1) 6.3 8.5 9.9 (+1.4) 84.0 76.2 (-7.8) 56.4 41.6 (-14.8) 64.5 63.0 (-1.5)\n\nTable 3: R1 results for 0-hop (ZH) vs multi-hop (MH) instruction guided image manipulation.\n\n4.3 ZERO-SHOT GENERALIZATION TO LARGER SCENES\n\nWe developed another dataset called CIM-NLI-LARGE, consisting of scenes having 10 − 13 objects (See Appendix B for details). We study the combinatorial generalization ability of NEUROSIM and the baselines when the models are trained on CIM-NLI containing scenes with 3 − 8 objects only and evaluated on CIM-NLI-LARGE. Table 5 captures such a comparison. NEUROSIM does significantly better than TIM-GAN and GeNeVA trained on 10% (5.4Kdata points) of CIM-NLI data for e.g. it improves over TIM-GAN R1 score by 33.5 points. NEUROSIM nearly matches TIMGAN’s performance trained on full CIM-NLI data. This demonstrates the superior generalization capabilities of our weakly supervised model compared to supervised baselines.\n\nFigure 4: Visual comparison of NEUROSIM results with TIM-GAN and GeNeVA.\n\n4.4 QUALITATIVE ANALYSIS AND INTERPRETABILITY\n\nFigure 4 shows anecdotal examples for visually comparing NEUROSIM with baselines. Note, GeNeVA either performs the wrong operation on the image (row #1, 2, 4, and 5) or simply copies the input image to output without any modifications (row #3). TIM-GAN often makes semantic errors which show its lack of reasoning – for example, removing the wrong objects in row #3. Compared to baselines, NEUROSIM produces semantically more meaningful image manipulation. NEUROSIM can easily recover occluded objects (row #4). All models make rendering errors such as partial removal of objects, shape distortion (rows #2, 4, and 5). More results are in Section H of appendix.\n\n8\n\nTIM-GAN Input ImageInstruction NEUROSIM GeNeVARemovethelargeobjectinfrontofthegrayrubbercylinder.Removethemattecylinderleftofthesmallobjectthatisbehindthetinycube.Thereisarubberthinginfrontoftheredmatteball;changetheshapeofittocylinder.Thereisalargesphere,addasmallpurplematteballbehindit.Thereisalargecyanshinythingthatisontherightsideofthesmallpurplematteball;changetheshapeofittosphere.Ground Truth Under review as a conference paper at ICLR 2023\n\nNEUROSIM produces interpretable output programs, showing the steps taken by the model to edit the images. Some examples and errors are shown in Appendix J. This highlights the ease of detecting failures of NEUROSIM, which is not possible with neural baselines.\n\n4.5 HUMAN EVALUATION\n\nFor the human evaluation study, in each instance, we provided evaluators with four images (1) input image, (2) ground-truth image, (3) manipulated image generated by NEUROSIM (5.4K), and (4) manipulated image generated by TIM-GAN (54K). Images generated by the two systems are randomly shuffled to avoid any annotation bias. Evaluators were asked two simple binary (yes:1/no:0) questions about each system. The questions evaluated: (Q1) does the system perform the desired change mentioned in the input instruction, (Q2) does the system introduces any undesired changes other than the required ones. See Appendix Table 20 for the exact text of the questions. There were a total of 7 evaluators, and each was given the same set of 30 random image quadruples. Table 4 shows the average scores of evaluators across different questions. NEUROSIM performs much better on Q1 despite TIM-GAN using full annotation data, implying better semantic manipulation by NEUROSIM. TIM-GAN does significantly better on Q2 demonstrates its ability to generate better images. The average Fleiss’ kappa score (Fleiss et al., 2013) is 0.796, implying high inter-evaluator agreement.\n\nQn. NEUROSIM TIM-GAN\n\n5.4K\n\n0.43\n\n0.14\n\nQ1\n\nQ2\n\n54K\n\n0.31\n\n0.77\n\nMethod\n\nR1 R3\n\nGeNeVA 54K\n\nGeNeVA 5.4K\n\n5.0 65.8 8.2 64.6 66.3 92.4 TIM-GAN 54K TIM-GAN 5.4K 30.2 80.7 NEUROSIM 5.4K 63.7 89.1\n\nMethod\n\nR1\n\n0.2 Text-Only 34.1 Image-Only 39.5 Concat 34.8 TIRG NEUROSIM 85.8\n\nR3\n\n0.4 83.6 86.9 84.6 92.9\n\nTable 4: Average human evaluation scores.\n\nTable 5: Performance on generalization to Larger Scenes\n\nTable 6: Quality assessment of G\n(cid:101)I via image retrieval task.\n\n4.6 QUANTITATIVE ASSESSMENT OF MANIPULATED SCENE GRAPH G\n\n(cid:101)I\n\nWe strongly believe image rendering module of NEUROSIM pipeline and encoder modules used for computing Recall@k adds some amount of inefficiencies resulting in lower R1 and R3 scores for us. Therefore, we decide to assess the quality of manipulated scene graph G (cid:101)I that gets generated in our pipeline. For this, we consider the task of text guided image retrieval as proposed by Vo et al. (2019). In this task, an image from the database has to retrieved which would be the closest match to the desired manipulated image but no manipulated image needs to be generated. Therefore, we use our manipulated scene graph G (cid:101)I as the latent representation of the input instruction and image for image retrieval. We retrieve images from the database based on a novel graph edit distance between NEUROSIM generated G (cid:101)I of the desired manipulated images, and scene graphs of the images in the database. This distance is defined using the Hungarian algorithm (Kuhn, 1955) with a simple cost defined between any 2 nodes of the graph. See Appendix D for a detailed explanation. Table 6 captures the performance of NEUROSIM and other popular baselines for the image retrieval task. From this table, we observe that NEUROSIM significantly outperforms supervised learning baselines by a margin of about 50% without ever using output image supervision. This result demonstrates that NEUROSIM edits the scene graph in a meaningful way.\n\nRefer to Appendix D, K, for additional results and ablations respectively.\n\n5 CONCLUSION\n\nWe present an neuro-symbolic, interpretable approach NEUROSIM to solve image manipulation task using weak supervision of VQA annotations, building on existing work on neuro-symbolic VQA (Mao et al., 2019). Unlike previous approaches, ours is the first work that can handle multiobject scenes with complex instructions requiring multi-hop reasoning, and solve the task without any output image supervision. Our experiments on a newly created dataset of image manipulation demonstrates the potential of our approach compared to supervised baselines. Directions for future work include carefully understanding the nature of errors made by our symbolic programs, and have a human in the loop to provide feedback to the system for correction. Another direction would be experimenting with more complex and real image datasets; recent works on Neuro-symbolic VQA for real images Li et al. (2019) can be a good starting point.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n6 ETHICS STATEMENT\n\nAll the datasets used in this paper were synthetically generated and do not contain any personally identifiable information or offensive content. The ideas and techniques proposed in this paper are useful in designing interpretable natural language-guided tools for image editing, computer-aided design, and video games. One of the possible adverse impacts of AI-based image manipulation is the creation of deepfakes Vaccari & Chadwick (2020) (using deep learning to create fake images). To counter deepfakes, several researchers Dolhansky et al. (2020); Mirsky & Lee (2021) have also looked into the problem of detecting real vs. fake images.\n\n7 REPRODUCIBILITY STATEMENT\n\nCode for baselines in all our experiments are publicly available, as stated in Section 4. All the training details (e.g., data splits, data processing steps, hyperparameters) are provided in Section 4, Appendix B, and Appendix G. We use the CLEVR dataset (Johnson et al., 2017b) and CLEVR toolkit (code to generate the dataset) for creating the new datasets introduced in this work. These are publicly available to use. Data creation methodology has been explained in Appendix B. Code for NEUROSIM will be open-sourced post acceptance.\n\nREFERENCES\n\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural\n\nnetworks for question answering. In ACL, 2016.\n\nShaowei Cai and Kaile Su. Configuration checking with aspiration in local search for sat. In AAAI,\n\n2012.\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n\nLichang Chen, Guosheng Lin, Shijie Wang, and Qingyao Wu. Graph edit distance reward: Learning\n\nto edit scene graph. In ECCV, pp. 539–554, 2020.\n\nQifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded refinement networks.\n\nIn ICCV, pp. 1511–1520, 2017.\n\nBlender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation,\n\nStichting Blender Foundation, Amsterdam, 2018.\n\nWang-Zhou Dai, Qiuling Xu, Yang Yu, and Zhi-Hua Zhou. Bridging machine learning and logical\n\nreasoning by abductive learning. In NeurIPS, 2019.\n\nBrian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) dataset. arXiv preprint arXiv:2006.07397, 2020.\n\nHao Dong, Simiao Yu, Chao Wu, and Yike Guo. Semantic image synthesis via adversarial learning.\n\nIn ICCV, pp. 5706–5714, 2017.\n\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic\n\nmachines. In ICLR, 2019.\n\nAlaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, and Graham W Taylor. Tell, Draw, and Repeat: Generating and modifying images based on continual linguistic instruction. In ICCV, pp. 10304–10312, 2019.\n\nFeng-Lei Fan, Jinjun Xiong, Mengzhou Li, and Ge Wang. On interpretability of artificial neural networks: A survey. IEEE Transactions on Radiation and Plasma Medical Sciences, 5(6):741–760, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJoseph L Fleiss, Bruce Levin, and Myunghee Cho Paik. Statistical methods for rates and proportions.\n\njohn wiley & sons, 2013.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, volume 27, 2014.\n\nChi Han, Jiayuan Mao, Chuang Gan, Josh Tenenbaum, and Jiajun Wu. Visual concept-metaconcept\n\nlearning. In NeurIPS, 2019.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In CVPR, pp. 770–778, 2016a.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\n\nrecognition. In CVPR, pp. 770–778, 2016b.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017.\n\nRonghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to\n\nreason: End-to-end module networks for visual question answering. In ICCV, 2017.\n\nDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700–6709, 2019.\n\nWentao Jiang, Ning Xu, Jiayun Wang, Chen Gao, Jing Shi, Zhe Lin, and Si Liu. Language-guided\n\nglobal image editing via cross-modal cyclic mechanism. In ICCV, pp. 2115–2124, 2021.\n\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. In ICCV, 2017a.\n\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pp. 2901–2910, 2017b.\n\nJustin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In CVPR, June\n\n2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), NeurIPS, volume 25, 2012.\n\nH. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly,\n\n2(1-2):83–97, 1955.\n\nBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. Controllable text-to-image generation.\n\nIn NeurIPS, volume 32, 2019.\n\nBowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS Torr. Manigan: Text-guided image\n\nmanipulation. In CVPR, pp. 7880–7889, 2020.\n\nZhuowan Li, Elias Stengel-Eskin, Yixiao Zhang, Cihang Xie, Quan Hung Tran, Benjamin Van Durme, and Alan Yuille. Calibrating concepts and operations: Towards symbolic reasoning on real images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In ICLR, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nGary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e 2. arXiv\n\npreprint arXiv:2204.13807, 2022.\n\nYisroel Mirsky and Wenke Lee. The creation and detection of deepfakes: A survey. ACM Computing\n\nSurveys (CSUR), 54(1):1–41, 2021.\n\nSeonghyeon Nam, Yunji Kim, and Seon Joo Kim. Text-adaptive generative adversarial networks:\n\nmanipulating images with natural language. In NeurIPS, volume 31, 2018.\n\nDuc Nghia Pham, John Thornton, and Abdul Sattar. Building structure into local search for sat. In\n\nIJCAI, pp. 2359–2364, 2007.\n\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\n\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\n\ndetection with region proposal networks. NeurIPS, 28, 2015.\n\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n\nJing Shi, Ning Xu, Yihang Xu, Trung Bui, Franck Dernoncourt, and Chenliang Xu. Learning by\n\nplanning: Language-guided global image editing. In CVPR, pp. 13590–13599, 2021.\n\nCristian Vaccari and Andrew Chadwick. Deepfakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news. Social Media+ Society, 6(1), 2020.\n\nNam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text\n\nand image for image retrieval-an empirical odyssey. In CVPR, pp. 6439–6448, 2019.\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8(3):229–256, 1992.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n\nKexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. In NeurIPS, 2018.\n\nDong Yu and Li Deng. Automatic speech recognition, volume 1. Springer, 2016.\n\nTianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, and Irfan Essa. Text as neural operator: Image manipulation by text instruction. In ACM MM, pp. 1893–1902, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA DOMAIN SPECIFIC LANGUAGE (DSL)\n\nTable 7 captures the DSL used by our NEUROSIM pipeline. The first 5 constructs in this table are common with the DSL used in Mao et al. (2019). The last 3 operations (Change, Add, and Remove) were added by us to allow for the manipulation operations. Table 8 show the type system\n\nOperation Signature [Output ← Input])\n\nSemantics\n\nScene\n\nObjSet ← ()\n\nReturns all objects in the scene.\n\nFilter ObjSet ← (ObjSet, ObjConcept)\n\nRelate ObjSet ← (ObjSet, RelConcept, Obj)\n\nFilter out a set of objects from ObjSet that have concept (e.g. red) specified in ObjConcept.\n\nFilter out a set of objects from ObjSet that have concept specified relation concept (e.g. RightOf) with object Obj.\n\nQuery\n\nObjConcept ← (Obj, Attribute)\n\nReturns the Attribute value for the object Obj.\n\nExist\n\nBool ← (ObjSet)\n\nChecks if the set ObjSet is empty.\n\nChange Obj ← (Obj, Concept)\n\nAdd\n\nGraph ← (Graph, RelConcept, Obj, ConceptSet)\n\nRemove Graph ← (Graph, ObjSet)\n\nChanges the attribute value of the input object (Obj), corresponding to the input concept, to Concept\n\nAdds an object to the input graph, generating a new graph having the object with attribute values as ConceptSet, and present in relation RelConcept of the input Obj\n\nRemoves the input objects and their edges from the input graph to output a new graph\n\nTable 7: Extended Domain Specific Language (DSL) used by NEUROSIM.\n\nused by the DSL in this work. The first 5 types are inherited from Mao et al. (2019) while the last one is an extension of the type system for handling the inputs to the Add operator.\n\nType\n\nRemarks\n\nObjConcept Concepts for any given object, such as blue, cylinder, etc.\n\nAttribute Attributes for any given object, such as color, shape, etc.\n\nRelConcept Relational concepts for any given object pair, such as RightOf, LeftOf, etc.\n\nObject\n\nDepicts a single object\n\nObjectSet Depicts multiple objects\n\nConceptSet A set of elements of ObjConcept type\n\nTable 8: Extended type system for the DSL used by NEUROSIM.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB DATASET DETAILS\n\nB.1 CIM-NLI DATASET\n\nThis dataset was generated with the help of CLEVR toolkit (Johnson et al., 2017b) by using following recipe.\n\n1. First, we create a source image I and the corresponding scene data by using Blender (Community,\n\n2018) software.\n\n2. For each source image I created above, we generate multiple instruction texts T ’s using its scene data. These are generated using templates, similar to question templates proposed by Johnson et al. (2017b).\n\n3. For each such (I, T ) pair, we attach a corresponding symbolic program P (not used by NEU-\n\nROSIM though) as well as scene data for the corresponding changed image.\n\n4. Finally, for each (I, T ) pair, we generate the target gold image (cid:101)I ∗ using Blender software and its\n\nscene data from previous step.\n\nBelow are some of the important characteristics of the CIM-NLI dataset.\n\n• Each source image I comprises several objects and each object comprises four visual attributes -\n\ncolor, shape, size, and material.\n\n• Each instructions text T comprises one of the following three kinds of manipulation operations -\n\nadd, remove, and change.\n\n• An add instruction specifies color, shape, size, and material of the object that needs to be added. It also specifies a direct (or indirect) relation with one or more existing objects (called reference object(s)). The number of relations that are required to traverse for nailing down the target object is referred to as # of reasoning hops and we have allowed instructions with up to 3-hops reasoning. We do not generate any 0-hop instruction for add due to ambiguity of where to place the object inside the scene.\n\n• A change instruction first specifies zero or more attributes to uniquely identify the object that needs to be changed. It may also specify a direct (or indirect) relation with one or more existing reference objects. Lastly, it specifies the target values of an attribute for the identified object which needs to be changed.\n\n• A remove instruction specifies zero or more attributes of the object(s) to be removed. Additionally,\n\nit may specify a direct (or indirect) relation with one or more existing reference objects.\n\nTable 9 captures the fine grained statistics about the CIM-NLI dataset. Specifically, it further splits each of the train, validation, and test set across the instruction types - add, remove, and change.\n\nB.2 CIM-NLI-LARGE DATASET\n\nWe created another dataset called CIM-NLI-LARGE to test the generalization ability of NEUROSIM on images containing more number of objects than training images. CIM-NLI-LARGE tests the zero-shot transfer ability of both NEUROSIM and baselines on scenes containing more objects.\n\nEach image in CIM-NLI-LARGE dataset comprises of 10 − 13 objects as opposed to 3 − 8 objects in CIM-NLI dataset which was used to train NEUROSIM. The CIM-NLI-LARGE dataset consists of 1K unique input images. We have created 3 instructions for each image resulting in a total of 3K instructions. The number of add instructions is significantly less since there is very little free space available in the scene to add new objects. To create scenes with 12 and 13 objects, we made all objects as small size and the minimum distance between objects was reduced so that all objects could fit in the scene. Table 10 captures the statistics about this dataset.\n\nB.3 MULTI-HOP INSTRUCTIONS\n\nIn what follows, we have given examples of the instructions that require multi-hop reasoning to nail down the location/object to be manipulated in the image.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nOperation\n\nSplit\n\n# (I, T, (cid:101)I ∗)\n\n# reasoning hops\n\n# objects\n\nmin\n\nmean\n\nmax\n\nmin\n\nmean\n\nmax\n\nAdd\n\nRemove\n\nChange\n\ntrain\n\nvalid\n\ntest\n\ntrain\n\nvalid\n\ntest\n\ntrain\n\nvalid\n\ntest\n\n17827\n\n4459\n\n4464\n\n15999\n\n5000\n\n5000\n\n19990\n\n4996\n\n4998\n\n1\n\n1\n\n1\n\n0\n\n0\n\n0\n\n0\n\n0\n\n0\n\n2.00\n\n2.00\n\n2.00\n\n1.50\n\n1.50\n\n1.50\n\n1.50\n\n1.50\n\n1.50\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n3\n\n5.51\n\n5.50\n\n5.45\n\n5.50\n\n5.50\n\n5.48\n\n5.45\n\n5.56\n\n5.52\n\n8\n\n8\n\n8\n\n8\n\n8\n\n8\n\n8\n\n8\n\n8\n\nTable 9: Statistics of CIM-NLI dataset introduced in this paper.\n\nOperation\n\n# (I, T, (cid:101)I ∗)\n\n# reasoning hops\n\n# objects\n\nmin\n\nmean\n\nmax\n\nmin\n\nmean\n\nmax\n\nAdd\n\nRemove\n\nChange\n\n393\n\n524\n\n2083\n\n1\n\n0\n\n0\n\n2.0\n\n1.50\n\n1.51\n\n3\n\n3\n\n3\n\n10\n\n10\n\n10\n\n11.53\n\n11.48\n\n11.50\n\n13\n\n13\n\n13\n\nTable 10: Statistics of CIM-NLI-LARGE dataset.\n\n• Remove the tiny green rubber ball. (0-hop)\n\n• There is a block right of the tiny green rubber ball, remove it. (1-hop)\n\n• Remove the shiny cube left of the block in front of the gray thing. (2-hop)\n\n• Remove the small thing that is left of the brown matte object behind the tiny cylinder that is behind\n\nthe big yellow metal block. (3-hop)\n\nC MODEL DETAILS\n\nC.1 SEMANTIC PARSER\n\nC.1.1 DETAILS ON PARSING\n\nWe begin by extending the type system of Mao et al. (2019) and add ConceptSet because our add operation takes as input a set of concepts depicting attribute values of the new object being added (refer Table 8 for the details). Next, in a manner similar to Mao et al. (2019), we use a rule based system for extracting concept words from the input text. We, however, add an extra rule for extracting ConceptSet from the input sentence. Rest of the semantic parsing methodology remains the same as given in Mao et al. (2019), with the difference being that our training is weakly supervised (refer Section 3.3 of the main paper).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nC.1.2 TRAINING\n\nAs explained in Section 3.3 of the main paper, for training with weaker form of supervision, we use an off-policy program search based REINFORCE (Williams, 1992) algorithm for calculating the exact gradient. For this, we define a set of all possible program templates Pt. For a given input instruction text T , we create a set of all possible programs {PT } from Pt. For e.g. given a template {remove(relate(·, f ilter(·, scene())))}, this is filled in all possible ways, with concepts, conceptSet, attributes and relational concepts extracted from the input sentence to get programs for this particular template. All such programs created using all templates form the set PT . All PT are executed over the scene graph of the input image. A typical program structure in our work is of the form manip_op(reasoning()), where manip_op represents the manipulation operator, for example change, add, or remove; and reasoning() either selects objects for change or remove, or it selects a reference object for adding another object in relation to it. After a hyperparameter search for the reward (refer Section G of the appendix), we assign a reward of +8 if the reasoning() part of the program leads to an object being selected for change/remove instruction or a related object being selected for add instruction. If no such object is selected, we give a reward of +2. Reward values were decided on the basis of validation set accuracy. We find that with this training strategy, we achieve the validation set accuracy of 95.64%, where this accuracy is calculated based on whether a program lead to an object being selected or not. Note, this is a proxy to the actual accuracy. For finding the actual accuracy, we would need a validation set of (instruction, ground truth output program) pairs, but we do not use this supervised data for training or validation.\n\nC.2 MANIPULATION NETWORK\n\nIn what follows, we provide finer details of manipulation network components.\n\na\n\n), where s∗\n\nChange Network: As described in Section 3.3 of the main paper, we have a change neural network for each attribute. For changing the current attribute value of a given object o, we use the following neural network: (cid:101)o = ga(o; cs∗ a is the desired changed value for the attribute a. (cid:101)o is the new representation of the object. We model ga(·) by a single layer neural network without having any non-linearity. The input dimension of this neural network is (256 + 64) because we concatenate the object representation o ∈ R256 with the desired concept representation d ∈ R64. We pass this concatenated vector through ga(·) to get the revised representation of the object: (cid:101)o ∈ R256. The loss used to train the weights of the change network is a weighted sum of losses equation 1 to equation 4 given in the main paper. This leads to the overall loss function given below.\n\nLoverall_change = λ1 la + λ2 la + λ3 lcycle + λ4 lconsistency + λ5 lobjGAN\n\n(8)\n\nwhere, lobjGAN above is the modified GAN loss (Goodfellow et al., 2014). Here λ1 = 1, λ2 = 1/((num_attrs − 1) ∗ (num_concepts)), λ3 = λ4 = 103, and λ5 = 1/(num_objects). Here, (num_objects) is the number of objects in input image, (num_attrs) is the total number of attributes for each object, and (num_concepts) are the total number of concepts in the NSCL (Mao et al., 2019) framework.\n\nThe object discriminator is a neural network with input dimension 256 and a single 300 dimensional hidden layer with ReLU activation function. This discriminator is trained using standard GAN objective lobjGAN. See Fig 5a for an overview of the change operator\n\nRemove Network: The remove network is a symbolic operation as described in Section 3.3 of the main paper. That is, given an input set of objects, the remove operation deletes the subgraph of the scene graph that contains the nodes corresponding to removed objects and the edges incident on those nodes. See Fig 5c for an overview of the remove operator.\n\nAdd Network: The neural operation in the add operator comprises of predicting the object representation for the newly added object using a function gaddObj(·). This function is modeled as a single layer neural network without any activation. The input to this network is a concatenated vector [[csa1 ] represents the concatenation of all the concept vectors of the desired new objects. The vector orel is the representation of the object with whom the relation (i.e. position) of the new object has been specified and cr is the concept vector for\n\n], orel, cr], where [csa1\n\n, · · · , csak\n\n, · · · , csak\n\n, csa2\n\n, csa2\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Change operator overview.\n\n(b) Add operator overview.\n\n(c) Remove operator overview.\n\nFigure 5: Overview of new operators (change, add and remove) added to the DSL.\n\nthat relationship. The input dimension of gaddObj(·) is (k ∗ 64 + 256 + 64) and the output dimension is 256. For predicting representation of newly added edges in the scene graph, we us edge predictor gaddEdge(·). The input to this edge predictor function is the concatenated representation of the objects\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nwhich are linked by the edge. The input dimension of gaddEdge(·) is (256 + 256) and the output dimension is 256.\n\nThe loss used to train the add network weights is a weighted sum of losses equation 5 to equation 7 along with an object discriminator loss. The overall loss is given by the following expression.\n\nLoverall_add = λ1lconcepts + λ2lrelation + λ3lobjSup + λ4ledgeSup +\n\nλ5ledgeGAN + λ6lobjGAN\n\n(9)\n\nwhere, lobjGAN and ledgeGAN above denotes the modified GAN loss (Goodfellow et al., 2014). Here λ1 = λ2 = 1/(num_attrs), λ3 = λ4 = 103, λ5 = 1/(num_objects).\n\nThe object discriminator is a neural network with input dimension as 256 and a single 300 dimensional hidden layer with ReLU activation function. This discriminator is trained using the standard GAN objective lobjGAN. Note, lobjGAN has 2 parts – i) the loss for the generated (fake) object embedding using the add network, and ii) the loss for the real objects (all the unchanged object embeddings of the image). The former is unscaled but the latter one is scaled by a factor of 1/(num_objects).\n\nThe edge discriminator is a neural network with input dimension as (256 ∗ 3) and a single 300 dimensional hidden layer with ReLU activation function. As input to this discriminator network, we pass the concatenation of the two objects and the edge connecting them. This discriminator is trained using the standard GAN objective ledgeGAN. See Fig 5b for an overview of the add operator\n\nD ADDITIONAL RESULTS\n\nD.1 DETAILED PERFORMANCE FOR ZERO-SHOT GENERALIZATION ON LARGER SCENES\n\nTable 11 below is a detailed version of the table 5 in the main paper. This table compares the performance of NEUROSIM with baseline methods TIM-GAN and GeNeVA for the zero-shot generalization to larger scenes (with ≥ 10 objects), while the models were trained on images with 3 − 8 objects. Relative to the main paper’s table 5, this table offers separate performance numbers for each of the add, remove and change instructions.\n\nMethod\n\nGeNeVA\n\nGeNeVA\n\nTIMGAN\n\nTIMGAN\n\nTrain Data Size\n\n54K\n\n5.4K\n\n54K\n\n5.4K\n\nNEUROSIM 5.4K\n\nAdd\n\nChange\n\nRemove\n\nR1 R3\n\nR1 R3\n\nR1 R3\n\n0.5\n\n0.0\n\n64.6\n\n60.1\n\n4.9\n\n8.2\n\n69.9\n\n69.2\n\n9.0\n\n50.0\n\n14.3\n\n49.6\n\n12.5\n\n77.4\n\n73.4\n\n95.2\n\n78.2\n\n92.2\n\n1.0\n\n3.8\n\n70.0\n\n46.6\n\n32.1\n\n84.4\n\n44.7\n\n74.0\n\n68.2\n\n95.8\n\n90.7\n\n94.3\n\nTable 11: Detailed performance scores for NEUROSIM, TIM-GAN, and GeNeVA for zero-shot generalization to larger scenes (with ≥ 10 objects) from CIM-NLI-LARGE dataset, while models are trained on images with 3 − 8 objects. Table has separate performance numbers for add, remove, and change instructions. Along with each method, we have also written the number of data points from CIM-NLI dataset that were used for training. R1 and R3 correspond to Recall@1 and Recall@3, respectively.\n\nD.2\n\nIMAGE RETRIEVAL TASK\n\nA task that is closely related to the image manipulation task is the task of Text Guided Image Retrieval, proposed by Vo et al. (2019). Through this experiment, our is to demonstrate that NEUROSIM is highly effective in solving this task as well. In what follows, we provide details about this task, baselines, evaluation metric, how we adapted NEUROSIM for this task, and finally performance results in Table 12. This table is a detailed version of the Table 6 in the main paper.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTask Definition: Given an Image I, a text instruction T , and a database of images D, the task is to retrieve an image from the database that is semantically as close to the ground truth manipulated image as possible.\n\nNote, for each such (I, T ) pair, some image from the database, say (cid:101)I ∈ D, is assumed to be the ideal image that should ideally be retrieved at rank-1. This, so called desired gold retrieval image might even be an image which is the ideal manipulated version of the original images I in terms of satisfying the instruction T perfectly. Or, image (cid:101)I may not be such an ideal manipulated image but it still may be the image in whole corpus D that comes closest to the ideal manipulated image.\n\nIn practice, while measuring the performance of any such system for this task, the gold manipulated image for (I, T ) pair is typically inserted into the database D and such an image then serves as the desired gold retrieval image (cid:101)I.\n\nBaselines: Our baselines includes popular supervised learning systems designed for this task. The first baseline is TIRG proposed by Vo et al. (2019) where they combine image and text to get a joint embedding and train their model in a supervised manner using embedding of the desired retrieved image as supervision. For completeness, we also include comparison with other baselines – Concat, Image-Only, and Text-Only – that were introduced by Vo et al. (2019).\n\nA recent model proposed by Chen et al. (2020) uses symbolic scene graphs (instead of embeddings) to retrieve images from the database. Motivated by this, we also retrieve image via the scene graph that is generated by the manipulation module of NEUROSIM. However, unlike Chen et al. (2020), the nodes and edges in our scene graph have associated vectors and make a novel use of them while retrieving. We do not compare our performance with Chen et al. (2020) since it’s code is unavailable and we haven’t been able to reproduce their numbers on datasets used in their paper. Moreover, Chen et al. (2020) uses full supervision of the desired output image (which is converted to a symbolic scene graph), while we do not.\n\nEvaluation Metric: We use Recall@k (and report results for k = 1, 3) for evaluating the performance of text guided image retrieval algorithms which is standard in the literature.\n\nRetrieval using Scene Graphs: We use the scene graph generated by NEUROSIM as the latent representation to retrieve images from the database. We introduce a novel yet simple method to retrieve images using scene graph representation. For converting an image into the scene graph, we use the visual representation network of NEUROSIM. Given the scene graph G for the input image I and the manipulation instruction text T , NEUROSIM converts the scene graph into the changed scene graph G (cid:101)I as a query to retrieve images from the database D. For retrieval, we use the novel graph edit distance (GED) between G (cid:101)I and the scene graph representation of the database images. The scene graph for each database image is also obtained using the visual representation network of NEUROSIM. The graph edit distance is given below.\n\n(cid:101)I , as described in Section C in Appendix. Now, we use this graph G\n\nGED(G\n\n(cid:101)I , GD) =\n\n(cid:40)\n\n∞ minπ∈Π\n\n(cid:80)\n\n∀i∈{1,2,··· ,|N\n\n(cid:101)I |} c(ni, yi)\n\n|N (cid:101)I | ̸= |N otherwise.\n\n(cid:101)D|\n\n(cid:101)I , V\n\n(cid:101)I = (N\n\n(cid:101)I ) and GD = (ND, VD). ni and yi are the node embeddings of the query graph where, G G\n(cid:101)I and scene graph GD of an image from the database. c(a, b) is the cosine similarities between embeddings a and b. This GED is much simpler than that defined in Chen et al. (2020), since it does not need any hand designed cost for change, removal, or addition of nodes, or different attributes values. It can simply rely on the cosine similarities between node embeddings. We use the Hungarian algorithm (Kuhn, 1955) for calculating the optimal matching π of the nodes, among all possible matching Π. We use the negative of the cosine similarity scores between nodes to create the cost matrix for the Hungarian algorithm to process. This simple yet highly effective approach (See Table 6 in the main paper and Table 12 in the appendix), can be improved by more sophisticated techniques that include distance between edge embeddings and including notion of subgraphs in the GED. We leave this as future work. This result shows that our manipulation network edits the scene graph in a desirable manner, as per the input instruction.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nTrain Data Size\n\nAdd\n\nChange\n\nRemove\n\n1\n\n2\n\n3\n\n0\n\n1\n\n2\n\n3\n\n0\n\n1\n\n2\n\n3\n\nText-Only\n\n54K\n\n0.4\n\n0.3\n\n0.3\n\n0.1\n\n0.0\n\n0.1\n\n0.1\n\n0.1\n\n0.3\n\n0.3\n\n0.0\n\nImage-Only\n\n54K\n\n35.3 33.4 32.3\n\n20.1 23.2 16.9 19.8\n\n46.3 41.3 53.1 57.8\n\nConcat\n\nTIRG\n\n54K\n\n54K\n\n36.3 33.3 31.8\n\n37.3 40.4 34.2 37.9\n\n41.8 41.0 50.0 55.0\n\n35.6 31.8 33.5\n\n22.0 25.1 18.8 22.0\n\n46.6 42.7 52.5 56.1\n\nNEUROSIM 5.4K 96.2 95.3 95.3\n\n83.3 82.9 81.3 78.7\n\n79.6 77.4 86.4 82.2\n\nTable 12: Performance scores (Recall@1) on the Image Retrieval task, comparing NEUROSIM with TIM-GAN and GeNeVA with increase in reasoning hops, for add, remove, and change instructions. Along with each method, number of data points from CIM-NLI used for training are written.\n\nMethod\n\nInstruction\n\nR1 R3\n\nR1 R3\n\nR1 R3\n\nR1 R3\n\nR1 R3\n\nβ = 0.054\n\nβ = 0.07\n\nβ = 0.1\n\nβ = 0.2\n\nβ = 0.54\n\nGeNeVA\n\nadd\n\n0.0 57.3\n\nchange\n\n5.9 36.3\n\nremove\n\n13.2 82.3\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n–\n\n0.7 63.6\n\n4.1 39.4\n\n8.7 89.3\n\nadd\n\n1.9 70.7\n\n4.9 74.0\n\n8.6 76.7\n\n10.3 77.1\n\n13.1 78.6\n\nTIM-GAN\n\nchange\n\n41.0 72.1\n\n42.9 73.5\n\n49.8 77.3\n\n62.5 84.2\n\n78.3 92.3\n\nremove\n\n49.6 79.5\n\n47.0 91.9\n\n53.9 93.1\n\n65.3 96.8\n\n78.0 98.5\n\nadd\n\n4.9 30.9\n\n6.4 34.8\n\n5.7 34.7\n\n5.9 38.9\n\n5.6 35.0\n\nNEUROSIM\n\nchange\n\n57.2 79.4\n\n57.3 79.3\n\n57.2 79.3\n\n57.2 79.4\n\n57.1 79.3\n\nremove\n\n69.6 82.5\n\n69.5 82.5\n\n69.5 82.6\n\n69.5 82.5\n\n69.6 82.5\n\nTable 13: Detailed performance comparison of NEUROSIM with TIM-GAN (Zhang et al., 2021) and GeNeVA (El-Nouby et al., 2019) with varying β levels, split across add, remove and change instructions. The ’-’ entries for GeNeVA were not computed due to excessive training time; it’s performance is abysmal even when using full data. We always use 100K VQA examples (5K Images, 20 questions per image) for our weakly supervised training. R1 and R3 correspond to Recall@1 and 3, respectively. For Recall, higher the score is better.\n\nD.3 DETAILED MULTI-HOP REASONING PERFORMANCE\n\nTable 14 below provides a detailed split of the performance numbers reported in Table 3 of the main paper across i) number of hops (0 − 3 hops) and ii) type of instructions (add/remove/change). We observe that for change and remove instructions, NEUROSIM improves over TIM-GAN and GeNeVA trained on 5.4K CIM-NLI data points by a significant margin (∼ 20% on 3-hop change/remove instructions). However, NEUROSIM lags behind TIM-GAN when the entire CIM-NLI labelled data is used to train TIM-GAN. We also observe that all the models perform poorly on the add instructions, as compared to change and remove instructions.\n\nD.4 DETAILED PERFORMANCE FOR DIFFERENT COST RATIOS β\n\nTable 2 in Section 4 of the main paper showed the performance of NEUROSIM compared with TIM-GAN and GeNeVA for various values of β, where β is the ratio of the number of annotated\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nMethod\n\nGeNeVA\n\nGeNeVA\n\nTrain Data Size\n\n54K\n\n5.4K\n\nAdd\n\nChange\n\nRemove\n\n1\n\n2\n\n3\n\n0\n\n1\n\n2\n\n3\n\n0\n\n1\n\n2\n\n3\n\n1.1\n\n0.0\n\n0.5\n\n0.0\n\n0.5\n\n0.0\n\n3.6\n\n4.7\n\n4.2\n\n7.1\n\n4.7\n\n5.6\n\n3.9\n\n6.1\n\n9.0\n\n8.0\n\n8.3\n\n9.4\n\n12.3 11.3 15.5 13.5\n\nTIM-GAN\n\n54K\n\n7.6 16.1 15.7\n\n85.8 74.1 78.0 75.4\n\n82.2 68.3 81.9 79.7\n\nTIM-GAN\n\n5.4K\n\nNEUROSIM 5.4K\n\n1.4\n\n4.6\n\n2.3\n\n5.0\n\n1.9\n\n5.1\n\n54.5 36.4 38.7 34.5\n\n58.3 40.9 50.9 48.2\n\n59.5 57.9 55.8 55.7\n\n69.6 66.6 71.8 70.4\n\nTable 14: Performance scores (Recall@1) for NEUROSIM with TIM-GAN and GeNeVA with increase in reasoning hops, for add, remove, and change instructions. Along with each method, number of data points from CIM-NLI used for training are written.\n\n(with output image supervision) image manipulation examples required by the supervised baselines, to the number of annotated VQA examples required to train NEUROSIM. In Table 13, we show a detailed split of the performance, for the add, change, and remove operators, across the same values of β as taken before.\n\nWe find that for the change operator, NEUROSIM performs better than TIM-GAN by a margin of ∼ 8% (considering Recall@1) for β ≤ 0.1. For the remove operator, NEUROSIM performs better than TIM-GAN by a margin of ∼ 4% (considering Recall@1) for β ≤ 0.2. Overall, NEUROSIM performs similar to TIM-GAN, for β = 0.2, for remove and change operators. All models perform poorly on the add operator as compared to the change and remove operators. We find that having full output image supervision allows TIM-GAN to reconstruct (copy) the unchanged objects from the input to the output for all the operators. This results in a higher recall in general but it’s effect is most pronounced in the Recall@3. NEUROSIM, on the other hand, suffers from rendering errors which makes the overall recall score (especially Recall@3) lower. We believe that improving image rendering quality would significantly improve the performance of NEUROSIM and we leave this as future work.\n\nD.5 RESULTS ON DATASETS FROM DIFFERENT DOMAINS\n\nD.5.1 MINECRAFT DATASET\n\nDataset Creation: We create a new dataset having (Image, instruction) by building over the Minecraft dataset used in Yi et al. (2018). Specifically, we create zero and one hop remove instructions and one hop add instructions similar the creation of CIM-NLI. This dataset contains scenes and objects from the Minecraft video game and is used in prior works for testing Neuro-Symbolic VQA systems like NSCL Mao et al. (2019) and NS-VQA Yi et al. (2018). The setting of the Minecraft worlds dataset is significantly different from CLEVR in terms of concepts and attributes of objects and visual appearance.\n\nExperiment: We use the above dataset for testing the addition and removal of objects using NeuroSIM (See Fig 6). We train NeuroSIM’s decoder to generate images from scene graphs of the minecraft dataset. We assume access to a parser that gives us programs for an instruction. For removal, we use the same remove network as described above, while for addition, we assume access to the features of object to be added, which is added to the scene graph of the image and the decoder decodes the final image. See Figure 6 for a set of successful examples on the Minecraft dataset. We see that using our method, one can add and remove objects from the scene successfully, without using any output image as supervision during training. Though we have assumed the availability of parser in the above set-up, training it jointly with other modules should be straightforward, and can be achieved using our general approach described in Section 3 of the main paper.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Results for addition and removal of objects from images of the minecraft dataset\n\nD.5.2 REAL WORLD DATASET: GQA\n\nTo show that our proposed approach NEUROSIM can work on real-world images, we experimented with the GQA dataset (Hudson & Manning, 2019). This dataset was originally used for benchmarking the task of VQA, and contains real world scenes having multiple objects, with different concepts and attributes.\n\n1] Zero-Shot Domain Transfer\n\nOur first experiment was to check the performance of NEUROSIM (trained only on synthetic images from CLEVR dataset) on real-world images without any retraining/fine-tuning. For this, we handpicked a few real images from GQA dataset and performed following steps:\n\n1. Generated a scene graph for the image using our existing visual representation network\n\n(without retraining it).\n\n2. Next, we queried for the color of an object in the image using our query network.\n\n3. Next, we changed the color of the given object through our existing manipulation network.\n\n4. Finally, rendered the image using the representation of the changed object.\n\nFigure 7 shows probabilities obtained when we query the representation of real-world objects using our pretrained query networks before and after applying the pretrained change network to these object representations. From this zero-shot experiment, our query network and manipulation networks is able to disentangle attributes such as the object’s color and also change it. However, because the rendering module is never trained on real images, it struggles to generate the real images. It seems to map the object with the shapes it learned during CLEVR training. Training the image rendering module using graph-based representations on real images (e.g. object representations obtained using Faster RCNN) is likely to eliminate the above problem and is part of our future and ongoing work.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Examples of querying an object representation before and after the application of change operation by NEUROSIM trained only on synthetic images from CLEVR dataset. Example images are taken from GQA dataset Hudson & Manning (2019).\n\n2] Training on Real World Dataset\n\n• We trained our image rendering module (Section 3.4) from scratch on the GQA dataset. During training, the scene graph of an input image is constructed by extracting image objects’ representation vectors by exploiting pre-trained Faster RCNN Ren et al. (2015) and ResNet He et al. (2016a). This scene graph is fed as input to the rendering module. We want to emphasize that the rendering module is not trained fully to convergence due to a lack of computational resources and time during the discussion phase.\n\n• Next, we performed inference on the above trained image rendering module using unseen sample images from the dataset. Following ideas from Mao et al. (2019) and Li et al. (2021) we used a pre-trained ResNet classifier as our program parser, for selecting objects in a given scene as well as tagging their class labels (e.g. horse, elephant, etc.) and corresponding probabilities. This is similar to the concept quantization step described in the main paper. For reinforcing the interpretability benefits of our model, we have shown the output of remove operation on these examples in Fig 8 as well as the steps taken by the model to achieve this. For each example in this figure,\n\n– The leftmost image is the source image that needs to be manipulated. – The rightmost (bottom) image is obtained after rendering the scene graph of the source image. The purpose of this image is to show the baseline quality of the rendering module.\n\n– The rightmost (top) image is obtained after manipulation operation (removal of a single\n\nobject in this case) is performed by NEUROSIM.\n\nFig 8 shows that after applying the remove operation to an object in the given image, NEUROSIM is able to reconstruct the image without that object while keeping the rest of the scene intact (when compared with baseline rendered image). We believe a more comprehensive training of the image rendering module to convergence will result in better-quality visuals.\n\nWe have demonstrated the remove operation on real-world images through the above experiment. We believe this result is still significant, since ours is the the first work in this direction to achieve complex image manipulation through text. Performing the add and change operations on real-world images is future work.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Examples of the application of remove operation on real images. Please note that the programs are applied on the scene graphs. For better visualization, program steps have been shown on the full image.\n\nD.5.3 SUMMARY OF RESULTS\n\nWe have shown proof-of-concept experiments on two additional datasets: Minecraft (artificial), GQA (real). The goal of our experiments was to demonstrate that our technique has the potential to generalize beyond CLEVR based datasets, including on real images. Doing a more comprehensive set of experiments, such as training the parser on Minecraft along with other modules, as well as, working with change and add instruction on the real dataset, and possibly exploring other powerful decoding techniques (such as based on recent diffusion models Ramesh et al. (2022); Saharia et al. (2022)), is a direction for future work.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nE END-TO-END TRAINING\n\nThe main objective of this work is to make use of weakly supervised VQA data for the image manipulation task without using output image supervision. But a natural extension of our work is to use output image supervision as well, to improve the performance of NEUROSIM. We devised an experiment to compare how much performance boost can be obtained by utilizing ground truth output (manipulated) images as the supervision for different modules of NEUROSIM. This experiment demonstrates the value of end-to-end training for NEUROSIM and how it can exploit the supervised data. We refer to this variant as NEUROSIM(e2e). We begin with a pre-trained NEUROSIM model trained with VQA annotations and then fine-tune it using supervised manipulation data. The detailed results are given in Table 15. This experiment demonstrates that with a small amount of supervised data, the performance of NEUROSIM can be significantly improved (e.g., more than 9 points increase for the change instruction with only 5.4K supervision examples)\n\nInstruction\n\nModel\n\n# of CIM-NLI examples used for training\n\nChange\n\nAdd\n\nRemove\n\nGeNeVA\n\nTIM-GAN\n\nNEUROSIM\n\nNEUROSIM(e2e)\n\nGeNeVA\n\nTIM-GAN\n\nNEUROSIM\n\nNEUROSIM(e2e)\n\nGeNeVA\n\nTIM-GAN\n\nNEUROSIM\n\nNEUROSIM(e2e)\n\n5.4K\n\n5.9\n\n41.0\n\n57.2\n\n66.2\n\n0.0\n\n1.9\n\n4.9\n\n8.8\n\n13.2\n\n49.6\n\n69.6\n\n69.6\n\n7K\n\n-\n\n42.9\n\n57.3\n\n66.3\n\n-\n\n4.9\n\n6.4\n\n8.9\n\n-\n\n47.0\n\n69.5\n\n69.5\n\n10K\n\n20K\n\n-\n\n49.8\n\n57.2\n\n66.6\n\n-\n\n8.6\n\n5.7\n\n9.2\n\n-\n\n53.9\n\n69.5\n\n69.5\n\n-\n\n62.5\n\n57.2\n\n67.4\n\n-\n\n10.3\n\n5.9\n\n10.5\n\n-\n\n65.3\n\n69.5\n\n69.5\n\n54K\n\n4.1\n\n78.3\n\n57.1\n\n69.6\n\n0.7\n\n13.1\n\n5.6\n\n10.6\n\n8.7\n\n78\n\n69.6\n\n69.6\n\nTable 15: Performance comparison of NEUROSIM(e2e) with baselines. NEUROSIM(e2e) refers to NEUROSIM trained end-to-end by utilizing ground truth manipulated images as the supervision for NEUROSIM modules.\n\nGiven the significant increase in performance of NEUROSIM when using supervised data, we also test it’s generalization capability (Analogous to Section 4.2, 4.3), and quality of scene graph retrieval (Analogous to Section 4.5).\n\nFrom Table 16, we see that NEUROSIM(e2e) shows improved zero-shot generalization to larger scenes. Even when trained on just 5.4k CIM-NLI data, NEUROSIM(e2e) improves over TIM-GAN54k by 3.9 R@1 points. A 5.3 point improvement over TIM-GAN is observed when full CIM-NLI data is used.\n\nNext, we measure drop in performance with increasing reasoning hops. From Table 17, we see that NEUROSIM(e2e) achieves the lowest drop when compared to TIM-GAN. NEUROSIM(e2e) improves over weakly supervised NEUROSIM baseline by 6.6 R@1 points.\n\nFinally, we measure quality of scene graph via retrieval. From Table 18, we see that supervised training significantly improves the scene graph quality, thus improving retrieval performance. Supervised training improves retrieval by 7.3 R@1 points over weakly supervised NEUROSIM baseline.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nTrain Data Size\n\nR1\n\nR3\n\nTIM-GAN\n\nTIM-GAN\n\n5.4K 30.2\n\n80.7\n\n54K 66.3\n\n92.4\n\nNEUROSIM\n\n5.4K 63.7\n\n89.1\n\nNEUROSIM(e2e)\n\n5.4K 70.2\n\n92.6\n\nNEUROSIM(e2e)\n\n54K 71.6 91.7\n\nTable 16: Zero-shot generalization to larger scenes (Extension of Table 5 of main paper).\n\nMethod\n\nTIM-GAN\n\nTIM-GAN\n\nNEUROSIM\n\nTrain Data Size\n\n5.4K\n\n54K\n\n5.4K\n\nNEUROSIM(e2e)\n\n5.4K\n\nNEUROSIM(e2e)\n\n54K\n\nHops\n\nZH M H\n\nDrop in Performance\n\n56.4\n\n84.0\n\n64.5\n\n69.4\n\n71.1\n\n41.6\n\n76.2\n\n63.0\n\n67.3\n\n69.6\n\n-14.8\n\n-7.8\n\n-1.5\n\n-2.1\n\n-1.5\n\nTable 17: Performance with increasing reasoning hops (Extension of Table 3 of main paper).\n\nThese findings suggest that NEUROSIM(e2e) significantly outperforms other supervised approaches in almost all settings. One can fine-tune the image decoder and the visual representation network to further enhance the findings, which should greatly enhance the outcomes.\n\nF COMPUTATIONAL RESOURCES\n\nWe trained all our models and baselines on 1 Nvidia Volta V100 GPU with 32GB memory and 512GB system RAM. Our image decoder training takes about 4 days of training time. Training of the VQA task takes 5 − 7 days of training time and training the Manipulation networks take 4 − 5 hours of training time.\n\nModel\n\nR1\n\nR3\n\nText-Only\n\nImage-Only\n\nConcat\n\nTIRG\n\nNEUROSIM\n\n0.2\n\n34.1\n\n39.5\n\n34.8\n\n85.8\n\n0.4\n\n83.6\n\n86.9\n\n84.6\n\n92.9\n\nNEUROSIM(e2e)\n\n93.1\n\n96.7\n\nTable 18: Quality of scene graph measured via retrieval (Extension of Table 6 of main paper)\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nG HYPERPARAMETERS AND VALIDATION ACCURACIES\n\nG.1 TRAINING FOR VQA TASK\n\nThe hyperparameters for the VQA task are kept same as default values coming from the prior work (Mao et al., 2019). We refer the readers to Mao et al. (2019) for more details. We obtained a question answering accuracy of 99.3% after training on the VQA task.\n\nG.2 TRAINING SEMANTIC PARSER\n\nThe semantic parser is trained to parse instructions. Learning of this module happens using the REINFORCE algorithm as described in Section C of this appendix. During REINFORCE algorithm, we search for positive rewards from the set {7, 8, 10}, and negative rewards from the set {0, 2, 3}. We finally choose a positive reward of 8 and negative reward of 2. For making this decision, we first train the semantic parser for 20 epochs and then calculate its accuracy by running it on the quantized scenes from the validation set. For a particular output program, we say it is correct if it leads to an object being selected (see Section C of the appendix for more information) and this is how the accuracy of the semantic parser is calculated. This accuracy is a proxy for the real accuracy. An alternative is to use annotated ground truth programs for calculating accuracy and then selecting hyperparameters. However, we do not use ground truth programs. All other hyperparameters are kept the same as used by Mao et al. (2019) to train the parser on VQA task. We obtain a validation accuracy of 95.64% after training the semantic parser for manipulation instructions.\n\nG.3 TRAINING MANIPULATION NETWORKS\n\nThe architecture details of the manipulation network are present in Section C of this appendix. We use batch size of 32, learning rate of 10−3, and optimize using AdamW (Loshchilov & Hutter, 2017) with weight decay of 10−4. Rest of the hyperparameters are kept the same as used in Mao et al. (2019). During training, at every 5th epochs, we calculate the manipulation accuracy by using the query networks that were trained while training the NEUROSIM on VQA data. This serves as a proxy to the validation accuracy.\n\n• For the change network training, we use the query accuracy of whether the attribute that was suppose to change for a particular object, has changed correctly or not. Also, whether any other attribute has changed or not.\n\n• For the add network training, we use the query accuracy of whether the attributes of the added object are correct or not. Also, whether the added object is in a correct relation with reference object or not.\n\nWe obtained a validation accuracy (based on querying) of 95.9% for the add network and an accuracy of 99.1% for the change network.\n\nG.4\n\nIMAGE DECODER TRAINING\n\nThe architecture of the image decoder is similar to Johnson et al. (2018) but our input scene graph (having embeddings for nodes and edges) is directly processed by the graph neural network. We use a batch size of 16, learning rate of 10−5, and optimize using Adam (Kingma & Ba, 2014) optimizer. The rest of the hyperparameters are same as Johnson et al. (2018). We train the image decoder for a fixed set of 1000K iterations.\n\nH QUALITATIVE ANALYSIS\n\nFigures 9,10,11 compare the images generated by NEUROSIM, TIM-GAN, and GeNeVA on add, change and remove instructions respectively. NEUROSIM’s advantage lies in semantic correctness of manipulated images. For example, see Figure 9 row #3,4; Figure 10 row #2; 11 all images. In these images, NEUROSIM was able to achieve semantically correct changes, while TIM-GAN, GeNeVA faced problems like blurry, smudged objects while adding them to the scene, removing incorrect objects from the scene, or not changing/partially changing the object to be changed. Images generated\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Visual comparison of NEUROSIM with TIM-GAN and GeNeVA for the add operator. The red bounding boxes in the ground truth output image indicate the objects required to add to the input image.\n\nFigure 10: Visual comparison of NEUROSIM with TIM-GAN and GeNeVA for the change operator. The red bounding boxes in the input and ground truth output image indicate the objects required to be changed.\n\nby TIM-GAN are better in quality as compared to NEUROSIM. We believe the reason for this is that TIM-GAN, being fully supervised, only changes a small portion of the image and has learnt to copy a significant portion of the input image directly to the output. How ever this doesn’t insure the semantic correctness of TIM-GAN’s manipulation, as described above with examples where it makes errors. The images generated by NEUROSIM look slightly worse since the entire image is generated from object based embeddings in the scene graph. Improving neural image rendering from scene graphs can be a promising step to improve NEUROSIM.\n\n28\n\nTIM-GAN Input ImageInstruction NEUROSIM GeNeVAThere is a shiny thing that is on the right side of the shiny block, add a big gray metallic ball in front of it.There is a rubber thing behind the matte thing in front of the tiny rubber object, add a tiny blue shiny sphere behind it.Add a small gray rubber cylinder that is in front of the big cube.Ground Truth Add a large gray metallic cylinder that is in front of the small rubber object behind the tiny green matte cylinder.There is a purple shiny object in front of the purple metal ball, add a large red matte ball to the left of it.TIM-GAN Input ImageInstruction NEUROSIM GeNeVAThere is a rubber thing in front of the red matte ball; change the shape of it to cylinder. Change material of the rubber object in front of the small rubber thing that is left of the tiny gray matte sphere that is in front of the yellow block to shiny.There is a small matte thing; change the color of it to purple.Ground Truth There is a cylinder that is behind the small metallic cylinder; change the size of it to tiny.There is a tiny cylinder that is to the left of the small blue thing to the left of the big green metallic cylinder; change the material of it to matte.Under review as a conference paper at ICLR 2023\n\nFigure 11: Visual comparison of NEUROSIM with TIM-GAN and GeNeVA for the remove operator. The red bounding boxes in the input image indicate objects required to be removed.\n\nI ERRORS\n\nFigure 12: Types of errors in NEUROSIM.\n\nFigure 12 captures the images generated by our model where it has made error. The kind of errors that NEUROSIM makes can be broadly classified into three categories.\n\n• [Rendering Errors] This set includes images generated by our model which are semantically correct but suffer from rendering errors. The common rendering errors include malformed cubes, partial cubes, change in position of objects, and different lighting.\n\n• [Logical Errors] This set includes images generated by our model which have logical errors. That is, manipulation instruction has been interpreted incorrectly and a different manipulation has been performed. This happens mainly due to an incorrect parse of the input instruction into the program,\n\n29\n\nTIM-GAN Input ImageInstruction NEUROSIM GeNeVAThere is a large metal object left of the metallic object that is to the right of the large metallic thing in front of the sphere, remove it.Ground Truth Remove the large shiny object that is behind the big purple cylinder.There is a big sphere in front of the big ball behind the blue thing, remove it.There is a metallic thing in front of the small gray rubber thing, remove itThere is a shiny cube, remove it.Under review as a conference paper at ICLR 2023\n\nor manipulation network not trained to the perfection. For example, change network changing attributes which were supposed to remain unchanged.\n\n• [VQA Errors] The query networks are not ideal and have errors after they are trained on the VQA task. This in turn causes errors in supervision (obtained from query networks) while training the manipulation networks and leads to a less than optimally trained manipulation network. Also, during inference, object embeddings may not be perfect due to the imperfections in the visual representation network and that leads to incorrect rendering.\n\nJ\n\nINTERPRETABILITY OF NEUROSIM\n\nNEUROSIM allows for interpretable image manipulation through programs which are generated as an intermediate representation of the input instruction. This is one of the major strengths of NEUROSIM, since it allows humans to detect where NEUROSIM failed. This is not possible with purely neural models, that behave as a black-box. Knowing about the failure cases of NEUROSIM also means that it can be selectively trained to improve certain parts of the network (for eg individually training on change instructions to improve the change command, if the model is performing poorly on change instructions). We now assess the correctness of intermediate programs using randomly selected qualitative examples present in Figure 13. Since no wrong program was obtained in the randomly selected set, we find 2 more data points manually, to show some wrong examples.\n\nK ABLATIONS\n\nTable 19 shows the performance of NEUROSIM while certain loss terms are removed while learning of the networks. This depicts the importance of loss terms that we have considered. In particular we test the performance of the network by removing edge adversarial loss used by add network (row 2), object adversarial losses for both add and change networks (row 3, 5), self supervision losses used by add network (row 4), cyclic (row 6) and consistency (row 7) losses used by change network.\n\nLoss\n\nl\n\nl − ladd\n\nedgeGAN\n\nl − ladd\n\nobjGAN\n\nR1\n\nR3\n\n45.3 65.5\n\n43.7 66.0\n\n44.3 60.2\n\nl − ladd\n\nobjSup − ladd\n\nedgeSup 44.1 57.9\n\nl − lchange\n\nobjGAN\n\nl − lchange\n\ncycle\n\nl − lchange\n\nconsistency\n\n44.9 61.5\n\n36.5 51.1\n\n31.0 44.8\n\nTable 19: Ablations conducted by removing some loss terms. l is the total loss before any ablation. For each loss term being removed, the superscript denotes which network it belongs to (add or change). Ablations are conducted for the setting where β = 0.054 (see main paper Section 4 for the definition of β)\n\nL HUMAN EVALUATION\n\nTable 20 for the questions asked to human evaluators for the human evaluation study. See Section 4.5 of the main paper for more details.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 13: Qualitative examples of generated programs by NEUROSIM.\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\n[Change] Are all the attributes (color, shape, size, material, and relative position) of the changed object mentioned in the instructions identical between the ground truth image and the system-generated image?\n\nQuestion 1:\n\n[Add] Are all the attributes (color, shape, size, material, and relative position) of the added object mentioned in the instructions identical between the ground truth image and the system-generated image?\n\n[Remove] Are same objects removed in ground truth image and the systemgenerated image?\n\n[Change] Are all the attributes (color, shape, size, material, and relative position) of the remaining objects identical between the ground truth image and the system-generated image?\n\nQuestion 2:\n\n[Add] Are all the attributes (color, shape, size, material, and relative position) of the remaining objects identical between the ground truth image and the system-generated image?\n\n[Remove] Are all the attributes (color, shape, size, material, and relative position) of the remaining objects identical between the ground truth image and the system-generated image?\n\nTable 20: Questions asked to human evaluators for evaluating NEUROSIM and TIM-GAN. Note that there are some variations in the questions for Change, Add, and Remove instructions dues to different semantic nature of the instructions.\n\n32",
    "reference": "# Summary Of The Paper\n\nThis paper presents an neuro-symbolic, interpretable approach NEUROSIM to solve image manipulation task using weak supervision of VQA annotations, building on existing work on neuro-symbolic.\n\n# Strength And Weaknesses\n\nThis paper is the first work that can handle multiobject scenes with complex instructions requiring multi-hop reasoning, and solve the task without any output image supervision.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI have no other concerns.\n\n# Summary Of The Review\n\nThe overall contribution of this paper is significant.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nVARIATIONAL INFORMATION PURSUIT FOR INTERPRETABLE PREDICTIONS\n\nAditya Chattopadhyay, Kwan Ho Ryan Chan, Benjamin D. Haeffele, Donald Geman Johns Hopkins University, USA, {achatto1,kchan49,bhaeffele,geman}@jhu.edu\n\nRen ́e Vidal University of Pennsylvania, USA, vidalr@seas.upenn.edu\n\nABSTRACT\n\nThere is a growing interest in the machine learning community in developing predictive algorithms that are interpretable by design. To this end, recent work proposes to sequentially ask interpretable queries about data until a high confidence prediction can be made based on the answers obtained (the history). To promote short query-answer chains, a greedy procedure called Information Pursuit (IP) is used, which adaptively chooses queries in order of information gain. Generative models are employed to learn the distribution of query-answers and labels, which is in turn used to estimate the most informative query. However, learning and inference with a full generative model of the data is often intractable for complex tasks. In this work, we propose Variational Information Pursuit (V-IP), a variational characterization of IP which bypasses the need to learn generative models. V-IP is based on finding a query selection strategy and a classifier that minimize the expected cross-entropy between true and predicted labels. We prove that the IP strategy is the optimal solution to this problem. Therefore, instead of learning generative models, we can use our optimal strategy to directly pick the most informative query given any history. We then develop a practical algorithm by defining a finite-dimensional parameterization of our strategy and classifier using deep networks and train them end-to-end using our objective. Empirically, V-IP is 10-100x faster than IP on different Vision and NLP tasks with competitive performance. Moreover, V-IP finds much shorter query chains when compared to reinforcement learning which is typically used in sequential-decision-making problems. Finally, we demonstrate the utility of V-IP on challenging tasks like medical diagnosis where the performance is far superior to the generative modeling approach.\n\n1\n\nINTRODUCTION\n\nSuppose a doctor diagnoses a patient with a particular disease. One would want to know not only the disease but also an evidential explanation of the diagnosis in terms of clinical test results, physiological data, or symptoms experienced by the patient. For practical applications, machine learning methods require an emphasis not only on metrics such as generalization and scalability but also on criteria such as interpretability and transparency. With the advent of deep learning methods over traditionally interpretable methods such as decision trees or logistic regression, the ability to perform complex tasks such as large-scale image classification now often implies a sacrifice in interpretability. However, interpretability is important in unveiling potential biases for users with different backgrounds (Yu, 2018) or gaining users’ trust.\n\nMost of the prominent work in machine learning that addresses this question of interpretability is based on post hoc analysis of a trained deep network’s decisions (Simonyan et al., 2013; Ribeiro et al., 2016; Shrikumar et al., 2017; Zeiler & Fergus, 2014; Selvaraju et al., 2017; Smilkov et al., 2017; Chattopadhyay et al., 2019; Lundberg & Lee, 2017). These methods typically assign importance scores to different features used in a model’s decision by measuring the sensitivity of the model output to these features. However, explanations in terms of importance scores of raw features might not always be as desirable as a description of the reasoning process behind a model’s decision. Moreover, there are rarely any guarantees for the reliability of these post hoc explanations to faithfully represent the model’s decision-making process (Koh et al., 2020). Consequently, post hoc interpretability has been widely criticized (Adebayo et al., 2018; Kindermans et al., 2019; Rudin,\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n2019; Slack et al., 2020; Shah et al., 2021; Yang & Kim, 2019) and there is a need to shift towards ML algorithms that are interpretable by design.\n\nAn interesting framework for predicinterpretable making tions was recently introduced by Chattopadhyay et al. (2022). The authors propose the concept of an interpretable query set Q, a set of user-defined and taskspecific functions q : X → A, which map a data point in X to an answer in A, each having a clear interpretation to the enduser. For instance, a plausible query set for identifying bird species might involve querying beak shape, head colour, and other visual attributes of birds. Given a query set, their method sequentially asks queries about X until the answers obtained are sufficient for predicting the label/hypothesis Y with high confidence. Notably, as the final prediction is solely a function of this sequence of query-answer pairs, these pairs provide a complete explanation for the prediction. Figure 1 illustrates the framework on a bird classification task.\n\nFigure 1: Illustration of the framework on a bird classification task. The query set consists of questions about the presence or absence of different visual attributes of birds. Given an image xobs, a sequence of interpretable queries is asked about the image until a prediction can be made with high confidence. The choice of each query depends on the query-answers observed so far.\n\nTo obtain short explanations (short query-answer chains), the authors propose to use a greedy procedure called Information Pursuit (IP), which was first introduced in Geman & Jedynak (1996). Given any input xobs, IP sequentially chooses the query which has the largest mutual information about the label/hypothesis Y given the history of query-answers obtained so far. To compute this mutual information criteria, a generative model is first trained to learn the joint distribution between all query-answers q(X) and Y ; in particular, Variational Autoencoders (VAEs) (Kingma & Welling, 2013) are employed. This learnt VAE is then used to construct Markov Chain Monte Carlo (MCMC) estimates for mutual information via MCMC sampling. Unfortunately, the computational costs of MCMC sampling coupled with the challenges of learning accurate generative models that enable fast inference limit the application of this framework to simple tasks. As an example, classifying MNIST digits using 3 × 3 overlapping patches as queries1 with this approach would take weeks!\n\nIn this paper, we question the need to learn a full generative model between all query-answers q(X) and Y given that at each iteration IP is only interested in finding the most informative query given the history. More specifically, we present a variational charaterization of IP which is based on the observation that, given any history, the query q∗, whose answer minimizes the KL-divergence between the label distribution P (Y | X) and the posterior P (Y | q∗(X), history), will be the most informative query as required by IP. As a result, we propose to minimize this KL-divergence term in expectation (over randomization of histories) by optimizing over querier functions, which pick a query from Q given history, parameterized by deep networks. The optimal querier would then learn to directly pick the most informative query given any history, thus bypassing the need for explicitly computing mutual information using generative models. Through extensive experiments, we show that the proposed method is not only faster (since MCMC sampling methods are no longer needed for inference), but also achieves competitive performance when compared with the generative modeling approach and also outperforms other state-of-the-art sequential-decision-making methods.\n\nPaper Contributions. (1) We present a variational characterization of IP, termed Variational-IP or V-IP, and show that the solution to the V-IP objective is exactly the IP strategy. (2) We present a practical algorithm for optimizing this objective using deep networks. (3) Empirically, we show that V-IP achieves competitive performance with the generative modelling approach on various computer vision and NLP tasks with a much faster inference time. (4) Finally, we also compare our approach to Reinforcement Learning (RL) approaches used in sequential-decision making areas like Hard Attention (Mnih et al., 2014) and Symptom Checking (Peng et al., 2018), where the objective is to learn a policy which adaptively chooses a fixed number of queries, one at a time, such that an accurate prediction can be made. In all experiments, V-IP is superior to RL methods.\n\n1Each patch query asks about the pixel intensities observed in that patch for xobs.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nInterpretability in Machine Learning. These works can be broadly classified into two main categories: (i) post-hoc interpretability, and (ii) algorithms that are interpretable by design. A large number of papers in this area are devoted to post-hoc interpretability. However, as stated in the Introduction, the reliability of these methods have recently been called into question (Adebayo et al., 2018; Yang & Kim, 2019; Kindermans et al., 2019; Shah et al., 2021; Slack et al., 2020; Rudin, 2019; Koh et al., 2020; Subramanya et al., 2019). Consequently, recent works have focused on developing ML algorithms that are interpretable by design. Several of these works aim at learning deep networks via regularization such that it can be approximated by a decision tree (Wu et al., 2021) or locally by a linear network (Bohle et al., 2021; Alvarez Melis & Jaakkola, 2018). However, the framework of Chattopadhyay et al. (2022) produces predictions that are completely explained by interpretable query-chains and is not merely an approximation to an interpretable model like decision trees. Another line of work tries to learn latent semantic concepts or prototypes from data and subsequently base the final prediction on these learnt concepts (Sarkar et al., 2022; Nauta et al., 2021; Donnelly et al., 2022; Li et al., 2018; Yeh et al., 2020). However, there is no guarantee that these learnt concepts would be interpretable to the user or align with the user’s requirement. In sharp contrast, allowing the user to define an interpretable query set in (Chattopadhyay et al., 2022) guarantees by construction that the resulting query-chain explanations would be interpretable and useful.\n\nSequential Decision-Making. An alternative approach to learning short query-chains is to use methods for sequential decision learning. These algorithms can be used for making interpretable decisions by sequentially deciding “what to query next?” in order to predict Y as quickly as possible. Mnih et al. (2014) introduced a reinforcement-learning (RL) algorithm to sequentially observe an image through glimpses (small patches) and predict the label, and called their approach Hard Attention. Rangrej & Clark (2021) introduced a probabilistic model for Hard Attention which is similar to the IP algorithm. More specifically, they propose to learn a partial-VAE Ma et al. (2018) to directly learn the distribution of images given partially observed pixels. This VAE is then used to select glimpses in order of information gain, as in IP. In another work, Peng et al. (2018) introduced an RL-based framework to sequentially query patient symptoms for fast diagnosis. In §4, we compare V-IP with prior works in this area and show that in almost all cases our method requires a smaller number of queries to achieve the same level of accuracy. We conjecture that the superiority of V-IP over RL-based methods is because the V-IP optimization is not plagued by sparse rewards over long trajectories, for example, a positive reward for correct prediction after a large number of symptom queries as in Peng et al. (2018). Instead, Deep V-IP can be abstractly thought of as, given history, s, choosing a query q (the action) and receiving DKL(P (Y | x) || P (Y | q(x), s)) as an immediate reward. A more rigorous comparison of the two approaches would be an interesting future work.\n\n3 METHODS\n\n3.1 G-IP: INFORMATION PURSUIT VIA GENERATIVE MODELS AND MCMC\n\nLet X : Ω → X and Y : Ω → Y be random variables representing the input data and corresponding labels/output. We use capital letters for random variables and small letters for their realizations. Ω is the underlying sample space on which all random variables are defined. Let P (Y | X) denote the ground truth conditional distribution of Y given data X. Let Q be a set of task-specific, user-defined, interpretable functions of data, q : X → A, where q(x) ∈ A is the answer to query q ∈ Q evaluated at x ∈ X . We assume that Q is sufficient for solving the task, i.e., we assume that ∀(x, y) ∈ X × Y P (y | x) = P (y | {x′ ∈ X : q(x′) = q(x) ∀q ∈ Q}).\n\n(1)\n\nIn other words Q(X) := {q(X) : q ∈ Q} is a sufficient statistic for Y .\n\nThe Information Pursuit (IP) algorithm (Geman & Jedynak, 1996) proceeds as follows; given a data-point xobs, a sequence of most informative queries is selected as\n\nq1 = IP(∅) = arg max\n\nq∈Q\n\nI(q(X); Y );\n\n(2)\n\nqk+1 = IP({qi, qi(xobs)}1:k) = arg max\n\nq∈Q\n\nI(q(X); Y | q1:k(xobs)).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nHere qk+1 ∈ Q refers to the new query selected by IP at step k + 1, based on the history (denoted as q1:k(xobs))2, and qk+1(xobs) indicates the corresponding answer. The algorithm terminates after L queries, where L depends on the data point xobs, if all remaining queries are nearly uninformative, that is, ∀q ∈ Q I(q(X); Y | q1:L)) ≈ 0. The symbol I denotes mutual information. Evidently equation 2 requires estimating the query with maximum mutual information with Y based on history. One approach to carrying out IP is by first learning the distribution P (Q(X), Y ) from data using generative models and then using MCMC sampling to estimate the mutual information terms3. However, learning generative models for distributions with high-dimensional support is challenging, and performing multiple iterations of MCMC sampling can likewise be computationally demanding. To address this challenge, in the nest subsection we propose a variational characterization of IP that completely bypasses the need to learn and sample from complex generative models.\n\n3.2 V-IP: A VARIATIONAL CHARACTERIZATION OF INFORMATION PURSUIT\n\nWe begin this section by describing our variational characterization of IP. The proposed approach is motivated by the fact that generative models are only a means to an end; what we need is the function, that we call querier, that maps the histories observed, {qi, qi(xobs)}1:k, to the most informative next query qk+1 ∈ Q. It turns out that this most informative query is exactly the query q∗ whose answer will minimize the KL divergence between the conditional label distribution P (Y | X) and the posterior P (Y | q∗(X), {qi(xobs)}1:k). Based on this insight, is it possible to define an optimization problem to directly learn this querier function? This requires a few ingredients,\n\nFigure 2: Overview. Interpretable predictions using V-IP.\n\n• First, we need to learn a querier that, given any possible history one might encounter during IP, chooses the next most informative query. One possible strategy for this is to minimize the KL divergence objective in expectation over random histories of query-answer pairs.\n\n• The posterior P (Y | q∗(X), {qi(xobs)}1:k) depends on the data distribution and is typically unknown. Thus, we need to estimate this using probabilistic classifiers. A possible solution is to jointly optimize this expected KL divergence over both querier and classifier functions.\n\nThis leads to the following variational characterization of IP which will allow us to avoid generative models. Let K(x) be the set of all finite-length query-answer pairs of the form ({q1, q1(x)}, ..., {qm, qm(x)}), generated using queries from Q evaluated on any x ∈ X . We then define ̄K := ∪x∈X K(x) and denote elements of ̄K as “histories”. We define a classifier f : ̄K → PY as a function which maps arbitrary query-answer sequences to a distribution over Y. We define a querier g : ̄K → Q as a function which maps arbitrary query-answer sequences to a query q ∈ Q. The variational objective for IP is given by the following functional optimization problem, (cid:16)\n\n(cid:17)(cid:105)\n\n(cid:104)\n\nEX,S\n\nDKL\n\nP (Y | X) ∥ ˆP (Y | q(X), S)\n\nmin f,g\n\nwhere\n\nq := g(S) ∈ Q ˆP (Y | q(X), S) := f ({q, q(X)} ∪ S),\n\n(V-IP)\n\nand the minimum is taken over all possible mappings f (classifier) and g (querier). Here, S is a random set of query-answer pairs taking values in ̄K4. Given S = s and X = xobs, the querier g chooses a query q ∈ Q, evaluates it on xobs and passes the pair {q, q(xobs)} to the classifier. The classifier f then makes a prediction based on s appended with this additional pair {q, q(xobs)}.\n\n2Conditioning on q1:k(xobs) is to be understood as conditioning on the event {x′ ∈ X | {qi, qi(xobs)}1:k =\n\n{qi, qi(x′)}1:k}\n\n3Since mutual information requires computing expectation over density ratios which is still intractable de-\n\nspite having learnt a generative model.\n\n4Throughout this paper whenever we condition on S = s, we mean conditioning on the event of all data\n\nx′ ∈ X which share the same answers to queries as in s.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nLet (f ∗, g∗) be an optimal solution to V-IP. The querier g∗ will be the IP strategy with the requirement that the distribution of S, denoted as PS, in V-IP is chosen such that the histories observed while carrying out IP must have positive probability mass under PS. Thus, given a data-point xobs, I(q(X); Y );\n\nq1 = g∗(∅) = arg max\n\nq∈Q\n\nqk+1 = g∗({qi, qi(xobs)}1:k) = arg max\n\nq∈Q\n\nI(q(X); Y | q1:k(xobs)).\n\n(3)\n\nThe above sequential procedure is illustrated in Figure 2. As before {qi, qi(xobs)}1:k is referred to as the history observed after k queries and is a realization of S. This is formalized in the following proposition whose proof can be found in Appendix A. Proposition 1. Let (f ∗, g∗) be an optimal solution to V-IP. For any realization S = s such that P (S = s) > 0, define the optimization problem:\n\nmax ̃P ∈PY ,q∈Q\n\nI(q(X); Y | s) − EX|s\n\n(cid:104)\n\nDKL\n\n(cid:16)\n\nP (Y | q(X), s) ∥ ̃P (Y | q(X), s)\n\n(cid:17)(cid:105)\n\n.\n\n(4)\n\nThen there exists an optimal solution ( ̃P ∗ ̃P ∗ s (X)} ∪ s).\n\ns = f ∗({q∗\n\ns , q∗\n\ns , q∗\n\ns ) to the above objective such that q∗\n\ns = g∗(s) and\n\nThus, at the optima, the KL divergence term in equation 4 would be 0 and g∗ would pick the most informative query for any given subset of query-answer pairs S = s as is presented in equation 3.\n\nTheoretical guarantees aside, solving the optimization problem defined in V-IP is challenging since functional optimization over all possible classifier and querier mappings is intractable. In the following subsection, we present a practical algorithm for approximately solving this V-IP objective.\n\n3.3 V-IP WITH DEEP NETWORKS\n\nInstead of optimizing f and g over the intractable function space, we parameterize them using deep networks with weights θ and η respectively. Our practical version of V-IP, termed as Deep V-IP, is as follows,\n\nmin θ,η\n\nwhere\n\nEX,S[DKL(P (Y | X) ∥ Pθ(Y | qη(X), S)]\n\nqη := gη(S) Pθ(Y | qη(X), S) := fθ({qη, qη(X)} ∪ S).\n\n(Deep V-IP)\n\nNote that all we have done is replace arbitrary functions f and g in V-IP by deep networks parameterized by θ and η. To find good solutions there are two key constraints. First, the architecture for the classifier fθ and the querier gη functions need to be expressive enough to learn over an exponential (in |Q|) number of possible realizations of S. Second, we need to choose a sampling distribution PS for S. Notice, that for any reasonable-sized Q, there will be an exponentially large number Ideally, we would like to choose a PS that assigns positive mass of possible realizations of S. only to histories observed during the exact IP procedure, however this is like the “chicken-and-egg” dilemma. We now briefly discuss the architectures used for optimizing the Deep V-IP objective, followed by an exposition on the sampling distribution for S.\n\nArchitectures. The architecture for both the querier and classifier networks (described in more detail in Appendix C) are chosen in such a way that they can operate on arbitrary length query-answer pairs. There can be several choices for this. In this paper, we primarily use masking where the deep networks operate on fixed size inputs (the answers to all queries q ∈ Q evaluated on input x) with the unobserved query-answers masked out. We also experiment with set-based deep architectures proposed in Ma et al. (2018). We show by ablation studies in Appendix E that the masking-based architecture performs better. In practice, qη = argmax(gη(S)), where gη(S) ∈ R|Q| is the output of the querier network, which assigns a score to every query in Q and argmax computes an 1-hot indicator of the max element index of its input vector. To ensure differentiability through argmax we use the straight-through softmax estimator (Paulus et al., 2020) which is described in detail in Appendix D. Finally, Pθ(Y | qη(X), S) is the output of Softmax applied to the last layer of fθ.\n\nSampling of S. Choosing a sampling distribution that only has positive mass on histories observed during exact IP is like the “chicken-and-egg” dilemma. A simple alternative is to consider a distribution that assigns a positive mass to all possible sequences of query-answer pairs from ̄K. This\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nhowever would lead to slow convergence since the deep networks now have to learn to choose the most informative query given a large number query-answer pair subsets that would never be observed for any xobs ∈ X if one could do exact IP. To remedy this, we choose to adaptively bias our sampling distribution towards realizations of S one would observe if they carried out equation 3 using the current estimate for querier in place of g∗. More concretely, we optimize Deep V-IP by sequentially biasing the sampling distribution as follows: 1. Initial Random Sampling: We choose an initial distribution P 0\n\nS which ensures all elements of ̄K have positive mass. We first sample X ∼ PData. Then we sample k ∼ Uniform{0, 1, ..., |Q|} as the number of queries. Subsequently, k queries from Q are selected for X uniformly at random. is obtained by using the solution querier gηj to V-IP using P j S as the sampling distribution. In particular, we first sample X ∼ PData and k ∼ Uniform{0, 1, ..., |Q|} as before. Subsequently, we find the first k query-answer pairs for this sampled X using equation 3 with gηj as our querier.\n\n2. Subsequent Biased Sampling: The distribution P j+1\n\nS\n\nNotice that, the empty set ∅, corresponding to empty history, would have positive probability under any P j S and hence the querier would eventually learn to pick the most informative first query. Subsequent sequential optimization would aim at choosing the most informative second query and so on, assuming our architectures are expressive enough. In practice, we optimize with random sampling of S using stochastic gradients for numerous epochs. We then take the solution gη0 and fine-tune it with biased sampling strategies, each time optimizing using a single batch and consequently changing the sampling strategy according to the updated querier. Refer Appendix E for ablation studies on the effectiveness of the biased sampling strategy for S.\n\nStopping Criterion. There are two possible choices; (i) Fixed budget: Following prior work in sequential active testing (Ma et al., 2018; Rangrej & Clark, 2021), we stop asking queries after a fixed number of iterations. (ii) Variable query-lengths: Different data-points might need different number of queries to make confident predictions. For supervised learning tasks, where Y is “almost” a deterministic function of X, that is, maxY P (Y | X) ≈ 1, for any given xobs ∈ X , we terminate after L steps if maxY P (Y | q1:L(xobs)) ≥ 1 − ε, where ε is a hyperparameter. This is termed as the “MAP criterion”. For tasks where Y is more ambiguous and not a deterministic function of X, we choose to terminate once the posterior is “stable” for a pre-defined number of steps. This stability is measured by the difference between the two consecutive posterior entropies, H (cid:0)Y | q1:k(xobs)(cid:1) − H (cid:0)Y | q1:k+1(xobs)(cid:1) ≤ ε. This criterion, termed as the “stability criterion”, is an unbiased estimate of the mutual information-based stopping criteria used in Chattopadhyay et al. (2022).\n\nQualitative differences between Generative-IP and Variational-IP. We will refer to the generative approach for carrying out IP as described in Chattopadhyay et al. (2022) as Generative-IP or G-IP. The difference between G-IP and V-IP is similar in spirit to that of generative versus discriminative modelling in classification problems (Ng & Jordan, 2001). We conjecture that, when the data distribution agrees with the modelling assumptions made by the generative model, for example, conditional independence of query answers given Y , and the dataset size is “small,” then G-IP would obtain better results than V-IP since there are not enough datapoints for learning competitive querier and classifier networks. We thus expect the gains of V-IP to be most evident on datasets where learning a good generative model is difficult.\n\n4 EXPERIMENTS\n\nIn this section, through extensive experiments, we evaluate the effectiveness of the proposed method. We describe the query set used for each dataset in Table 1, with more details in Appendix C. The choice of query sets for each dataset was made to make our approach comparable with prior work. We also complement the results presented here with more examples in the Appendix. Code is available at https://github.com/ryanchankh/VariationalInformationPursuit.\n\n4.1\n\nINTERPRETABLE PREDICTIONS USING V-IP\n\nBasing predictions on an interpretable query set allows us to reason about the predictions in terms of the queries, which are compositions of elementary words, symbols or patterns. We will illustrate this by analyzing the query-answer chains uncovered by V-IP for different datasets. Figure 3a illustrates\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Descriptions of query set for different datasets.\n\nDataset\n\nQuery Set Q\n\nCUB-200 (Wah et al., 2011)\n\nindicator for the presence of different visual semantic attributes of birds\n\nHuffingtonNews (Misra, 2018)\n\nindicator for presence of words in article headline or description\n\nMNIST (LeCun et al., 1998), KMNIST (Clanuwat et al., 2018), Fashion-MNIST (Xiao et al., 2017b),\n\npixel intensities in 3 × 3 overlapping patches with stride 1\n\nCIFAR-{10,100} (Krizhevsky et al., 2009)\n\npixel intensities in 8 × 8 overlapping patches with stride 4\n\nSymCAT-200 (Peng et al., 2018) SymCAT-300 (Peng et al., 2018) SymCAT-400 (Peng et al., 2018)\n\nMuZhi (Wei et al., 2018) Dxy (Xu et al., 2019)\n\nindicator for the presence of different medical symptoms\n\nternary {Yes, No, Can’t Say} queries for the presence of different medical symptoms\n\nSize |Q|\n\n312\n\n1000\n\n676\n\n49\n\n328 349 355\n\n66 41\n\nthe decision-making process for V-IP on an image of a dog from the CIFAR-10 dataset. A priori the model’s belief is almost uniform for all the classes (second row, first column). The first query probes a patch near the centre of the image and observes the snout. Visually it looks similar to the left face of a cat, justifying the shift in the model’s belief to label “cat” with some mass on the label “dog”. Subsequent queries are aimed at distinguishing between these two possibilities. Finally, the model becomes more than 99% confident that it is a “dog” once it spots the left ear. Figure 3b shows the query-chain for a “genital herpes” diagnosis of a synthetic patient from the SymCAT-200 dataset. The y-axis shows the query asked at each iteration with green indicating a “Yes” answer and red indicating a “No”. Each row shows the model’s current belief in the patient’s disease. We begin with an initial symptom, “0: itching of skin”, provided by the patient. The subsequent queries ask about different conditions of the skin. The diseases shown are the top-10 most probable out of 200. All of these diseases have skin-related symptoms. After discovering the patient has painful urination (query 11), V-IP zooms into two possibilities “Balanitis” and “Genital herpes”. The subsequent queries rule out symptoms typically observed in patients with “Balanties” resulting in a 80% confidence in the herpes disease. For our final example, we elucidate the results for a bird image from the CUB-200 dataset in Figure 3c. The colour scheme for the y-axis is the same as in Figure 3b, with the exception that, unlike the patient case, we do not bootstrap V-IP with an initial positive attribute of the word. Instead, the first query about bill shape is the most-informative query about the label Y before any answer is observed. This is indicated with the grey “0:Init”. All the top10 most probable bird species in this context are seabirds, and have very similar visual characteristics to the true species, “Laysan Albatross”. After 14 queries V-IP figures out the true class with more than 99% confidence. Thus, in all three case-studies, we see that V-IP makes transparent decisions, interpretable in terms of queries specified by the user. A limitation of this framework however is finding a good query set that is interpretable and allows for highly accurate predictions with short explanations (short query-answer chains). We discuss this further in Appendix §G.\n\n4.2 QUANTITATIVE COMPARISON WITH PRIOR WORK\n\nTable 2: AUC values for test accuracy vs. explanation length curves for different datasets. To account for different query set sizes across datasets, we normalize the scores by |Q|. Refer to Table 4 in Appendix §F for extended results with standard deviations for other methods (averaged over 5 runs).\n\nBaselines. We compare V-IP primarily to the generative modelling approach for IP, namely G-IP. We also compare to Reinforcementin other arLearning methods prevalent eas of sequential-decision making like HardAttention (Mnih et al., 2014; Rangrej & Clark, 2021) or Symptom Checking (Peng et al., 2018), which can be adapted for our purposes. In particular, we compare with the RAM (Mnih et al., 2014) and RAM+(Li et al., 2017) algorithms. In both methods, a policy is learnt using deep networks to select queries based on previous query-answers for a fixed number of iterations such that the expected cumulative reward is maximized. A classifier network is also trained simultaneously with the policy network to make accurate predictions. In RAM, this reward is just the negative cross-entropy loss between true and predicted labels at the last step. In RAM+, this reward is the cumulative sum of the negative cross-entropy loss at each step. We also compare our method with the “Random” strategy where successive queries are\n\nCUB HuffingtonNews MNIST KMNIST Fashion-MNIST\n\n0.716 ± 0.008 0.680 ± 0.002 0.956 ± 0.002 0.911 ± 0.008 0.849 ± 0.010\n\nRandom RAM RAM+ G-IP\n\n0.736 0.691 0.964 0.872 0.831\n\n0.557 0.423 0.868 0.775 0.735\n\n0.695 0.431 0.920 0.841 0.804\n\n0.662 0.389 0.916 0.832 0.770\n\nV-IP (Ours)\n\nDataset\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: (a) V-IP on CIFAR-10. In column 1 row 1, we show the observed image xobs, and in column 1 row 2, we show the label distribution before V-IP observes any patches. In the subsequent columns, row 1 indicates the patches revealed (the history) so far and row 2 shows the corresponding posterior over the labels given this history. (b) V-IP on SymCAT-200. Each row in the heatmap shows the posterior of the disease labels given history. We show the top-10 most probable diseases out of 200. The y-axis indicates the corresponding symptom queried in each iteration by V-IP. We use the colour scheme that red denotes a “No” answer while green denotes a “Yes” answer. (c) V-IP on CUB-200. The observed image xobs is shown on the right. The heatmap shows the posterior, similar to (b).\n\nFigure 4: Tradeoff between accuracy and explanation length (avg. number of queries). The curves were generated by varying the ε parameter in the stopping criteria (see §3.3). For MNIST & HuffingtonNews we use the “MAP criterion”, whereas for CUB-200 we use the posterior “stability criterion”. Reported curves are averaged over 5 runs with shaded region denoting the standard deviation. More results in Appx. F.\n\nchosen randomly independent of the history observed so far. The predictions given history are still made using the V-IP classifier.\n\nWe first show results on the simple datasets used in Chattopadhyay et al. (2022) comparing with our own implementation of G-IP, RAM and RAM+. V-IP is competitive with G-IP in terms of performance but far more efficient in terms of speed of inference. In all datasets, V-IP outperforms the RL-based methods. Subsequently, we present results on more complex tasks like RGB image classification. On these datasets, V-IP achieves a higher accuracy given a fixed budget of queries compared with prior work.\n\nComparisons on Simple Tasks. Concise explanations are always preferred due to their simplicity. In Figure 4 we plot the trade-off between accuracy and explanation length obtained by various methods. V-IP is competitive with G-IP and obtains far shorter explanations than the RL-based methods to obtain the same test accuracy. This trade-off is quantified using the Area Under the Curve (AUC) metric in Table 2. Notice on the HuffingtonNews dataset the RL methods struggle\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nto perform better than even Random. This is potentially due to the fact that in these RL methods, the classifier is trained jointly with the policy which likely affects its performance when the actionspace is large (|Q| = 1000). On the other hand, the random strategy learns its classifier by training on random sequences of query-answer pairs. This agrees with findings in Rangrej & Clark (2021).\n\nWhile V-IP performs competitive with the generative approach the biggest gain is in terms of computational cost of inference. Once trained, inference in V-IP, that is computing the most-informative query, is akin to a forward pass through a deep network and is potentially O(1)5. On the other hand, the per-iteration cost in G-IP is O(N + |Q|m), where N is the number of MCMC iterations employed and m is the cardinality of the space q(X) × Y . As an example, on the same GPU server, G-IP takes about 47 seconds per iteration on MNIST whereas V-IP requires just 0.11s, an approximately 400× speedup! Note that the inference cost is the same for V-IP and the RL methods since all of them train a querier/policy function to choose the next query.\n\nComparisons on Complex Tasks. We now move on to more complex datasets where the gains of V-IP are more evident. First, we consider the task of natural image classification and show results on the CIFAR-{10, 100} datasets. For G-IP on these datasets, we refer to the Probabilistic HardAttn model introduced in Rangrej & Clark (2021) which proposes to learn a partial-VAE model (Ma et al., 2018) for images and then do inference using this model to compute the most informative query. Figures 5 a & b show the accuracy vs. number of queries curves for different methods. V-IP clearly outperforms all baselines on both datasets.\n\nNext, we consider the task of medical diagnosis by querying symptoms of the patient. We show results on the popular SymCAT dataset along with comparisons with prior work in Figure 5c. The plot clearly shows that V-IP achieves a much higher accuracy given a fixed budget of queries.\n\nFigure 5: Tradeoff between accuracy and number of queries asked (fixed budged stopping criterion) for the CIFAR datasets (a & b) and the SymCAT-200 dataset (c). Curves for baselines adapted from Rangrej & Clark (2021) for (a) & (b) and from He et al. (2022) for (c).\n\nTable 3: Test accuracy obtained on different symptom checking datasets after asking 20 queries.\n\nFor G-IP on this task, we refer to the BSODA framework introduced in He et al. (2022) which is again based on partial-VAEs. REFUEL (Peng et al., 2018) is a state-of-the-art RLbased method for this task akin to the RAM technique used in Hard-Attention literature. The classification accuracy for these different methods on all medical datasets are summarized in Table 3. Numbers for baselines are taken from Nesterov et al. (2022) since we used their released versions of these datasets6. As conjectured in §3.3, MuZhi and Dxy are small-scale datasets with about 500 training samples thus approaches based on generative models, like BSODA, are able to perform slightly better than V-IP.\n\n0.681 ± 0.004 0.599 ± 0.007 0.513 ± 0.009 0.706 ± 0.011 0.806 ± 0.019\n\nSymCAT-200 SymCAT-300 SymCAT-400 MuZhi Dxy\n\n0.548 0.482 0.438 0.718 0.757\n\n0.556 0.475 0.446 0.726 0.811\n\nBSODA REFUEL\n\nV-IP (Ours)\n\nDataset\n\n5 CONCLUSION\n\nIP was recently used to construct interpretable predictions by composing interpretable queries from a user-defined query set. The framework however required generative models which limited its application to simple tasks. Here, we have introduced a variational characterization of IP which does away with generative models and tries to directly optimize a KL-divergence based objective to find the most informative query, as required by IP, in each iteration.Through qualitative and quantitative experiments we show the effectiveness of the proposed method.\n\n5For simplicity we consider unit cost for any operation that was computed in a batch concurrently on a GPU. 6https://github.com/SympCheck/NeuralSymptomChecker\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis research was supported by the Army Research Office under the Multidisciplinary University Research Initiative contract W911NF-17-1-0304, the NSF grant 2031985 and by Simons Foundation Mathematical and Scientific Foundations of Deep Learning (MoDL) grant 135615. Moreover, the authors acknowledge support from the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE2139757. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. Advances in neural information processing systems, 31, 2018. pages 1, 3\n\nDavid Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining\n\nneural networks. Advances in neural information processing systems, 31, 2018. pages 3\n\nMoritz Bohle, Mario Fritz, and Bernt Schiele. Convolutional dynamic alignment networks for interpretable classifications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10029–10038, 2021. pages 3\n\nAditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, and Vineeth N Balasubramanian. Neural network attributions: A causal perspective. In International Conference on Machine Learning, pp. 981–990. PMLR, 2019. pages 1\n\nAditya Chattopadhyay, Stewart Slocum, Benjamin D Haeffele, Rene Vidal, and Donald Geman. Interpretable by design: Learning predictors by composing interpretable queries. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. pages 2, 3, 6, 8, 17, 18, 19, 24, 26\n\nTarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018. pages 7, 18, 26\n\nJon Donnelly, Alina Jade Barnett, and Chaofan Chen. Deformable protopnet: An interpretable image classifier using deformable prototypes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10265–10275, 2022. pages 3\n\nDonald Geman and Bruno Jedynak. An active testing model for tracking roads in satellite images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(1):1–14, 1996. pages 2, 3\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. pages 21\n\nWeijie He, Xiaohao Mao, Chao Ma, Yu Huang, Jos ́e Miguel Hern`andez-Lobato, and Ting Chen. Bsoda: A bipartite scalable framework for online disease diagnosis. In Proceedings of the ACM Web Conference 2022, pp. 2511–2521, 2022. pages 9, 20, 21\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks.\n\nIn Proceedings of the IEEE\n\nconference on computer vision and pattern recognition, pp. 7132–7141, 2018. pages 26\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. pages 21, 26\n\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Sch ̈utt, Sven D ̈ahne, Dumitru Erhan, and Been Kim. The (un) reliability of saliency methods. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267–280. Springer, 2019. pages 1, 3\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014. pages 17\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013. pages 2\n\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In International Conference on Machine Learning, pp. 5338–5348. PMLR, 2020. pages 1, 3, 26\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009. pages 7, 21\n\nMatthew Lavin. Analyzing documents with tf-idf, 2019. pages 18\n\nYann LeCun, L ́eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. pages 7, 18\n\nOscar Li, Hao Liu, Chaofan Chen, and Cynthia Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. pages 3\n\nZhichao Li, Yi Yang, Xiao Liu, Feng Zhou, Shilei Wen, and Wei Xu. Dynamic computational time for visual attention. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 1199–1209, 2017. pages 7\n\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\n\npreprint arXiv:1608.03983, 2016. pages 17\n\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances\n\nin neural information processing systems, 30, 2017. pages 1\n\nChao Ma, Sebastian Tschiatschek, Konstantina Palla, Jos ́e Miguel Hern ́andez-Lobato, Sebastian Nowozin, and Cheng Zhang. Eddi: Efficient dynamic discovery of high-value information with partial vae. arXiv preprint arXiv:1809.11142, 2018. pages 3, 5, 6, 9, 21\n\nRishabh Misra. News category dataset, 06 2018. pages 7\n\nVolodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. Advances\n\nin neural information processing systems, 27, 2014. pages 2, 3, 7\n\nMeike Nauta, Ron van Bree, and Christin Seifert. Neural prototype trees for interpretable finegrained image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14933–14943, 2021. pages 3\n\nAleksandr Nesterov, Bulat Ibragimov, Dmitriy Umerenkov, Artem Shelmanov, Galina Zubkova, and Vladimir Kokh. Neuralsympcheck: A symptom checking and disease diagnostic neural model with logic regularization. arXiv preprint arXiv:2206.00906, 2022. pages 9, 21\n\nAndrew Ng and Michael Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 14, 2001. pages 6\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. pages 17\n\nMax B Paulus, Chris J Maddison, and Andreas Krause. Rao-blackwellizing the straight-through\n\ngumbel-softmax gradient estimator. arXiv preprint arXiv:2010.04838, 2020. pages 5\n\nYu-Shao Peng, Kai-Fu Tang, Hsuan-Tien Lin, and Edward Chang. Refuel: Exploring sparse features in deep reinforcement learning for fast disease diagnosis. Advances in neural information processing systems, 31, 2018. pages 2, 3, 7, 9, 20\n\nSamrudhdhi B Rangrej and James J Clark. A probabilistic hard attention model for sequentially\n\nobserved scenes. arXiv preprint arXiv:2111.07534, 2021. pages 3, 6, 7, 9, 21\n\nSashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv\n\npreprint arXiv:1904.09237, 2019. pages 17\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135–1144, 2016. pages 1\n\nCynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019. pages 1, 3\n\nAnirban Sarkar, Deepak Vijaykeerthy, Anindya Sarkar, and Vineeth N Balasubramanian. A framework for learning ante-hoc explainable models via concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10286–10295, 2022. pages 3\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017. pages 17\n\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618–626, 2017. pages 1\n\nHarshay Shah, Prateek Jain, and Praneeth Netrapalli. Do input gradients highlight discriminative\n\nfeatures? Advances in Neural Information Processing Systems, 34, 2021. pages 2, 3\n\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International conference on machine learning, pp. 3145– 3153. PMLR, 2017. pages 1\n\nKaren Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. pages 1\n\nDylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180–186, 2020. pages 2, 3\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ́egas, and Martin Wattenberg. Smoothgrad:\n\nremoving noise by adding noise. arXiv preprint arXiv:1706.03825, 2017. pages 1\n\nAkshayvarun Subramanya, Vipin Pillai, and Hamed Pirsiavash. Fooling network interpretation in In Proceedings of the IEEE/CVF International Conference on Computer\n\nimage classification. Vision, pp. 2020–2029, 2019. pages 3\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. 2011. pages 7, 17\n\nZhongyu Wei, Qianlong Liu, Baolin Peng, Huaixiao Tou, Ting Chen, Xuan-Jing Huang, Kam-Fai Wong, and Xiang Dai. Task-oriented dialogue system for automatic diagnosis. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 201–207, 2018. pages 7, 20\n\nMike Wu, Sonali Parbhoo, Michael C Hughes, Volker Roth, and Finale Doshi-Velez. Optimizing for interpretability in deep neural networks with tree regularization. Journal of Artificial Intelligence Research, 72:1–37, 2021. pages 3\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: A mnist-like fashion product\n\ndatabase. In GitHub, 2017a. pages 26\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-\n\ning machine learning algorithms. 2017b. pages 7, 18\n\nLin Xu, Qixian Zhou, Ke Gong, Xiaodan Liang, Jianheng Tang, and Liang Lin. End-to-end knowledge-routed relational dialogue system for automatic diagnosis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 7346–7353, 2019. pages 7, 20\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nMengjiao Yang and Been Kim. Benchmarking attribution methods with relative feature importance.\n\narXiv preprint arXiv:1907.09701, 2019. pages 2, 3\n\nChih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. Advances in Neural Information Processing Systems, 33:20554–20565, 2020. pages 3\n\nBin Yu. Three principles of data science: predictability, computability, and stability (pcs). 2018.\n\npages 1\n\nFisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2403–2412, 2018. pages 21, 26\n\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks.\n\nIn\n\nEuropean conference on computer vision, pp. 818–833. Springer, 2014. pages 1\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA PROOF OF PROPOSITION 1\n\nBefore proceeding to the proof we will prove the following lemma, Lemma 1. Let Q be an user-defined query set and P(Y ) be the set of all possible distributions on Y . Then, for any realization S = s, the following holds true:\n\nmin ̃P ∈PY ,q∈Q\n\n(cid:104)\n\nEX|s\n\nDKL(P (Y | X) || ̃P (Y | q(X), s)\n\n(cid:105)\n\n≡ max\n\n ̃P ∈PY ,q∈Q\n\n(cid:104)\n\n(cid:105) I(q(X); Y | s) − EX|s[DKL(P (Y | q(X), s) || ̃P (Y | q(X), s))]\n\n(5)\n\nProof. Using information-theoretic properties of the KL-divergence we have the following set of equalities.\n\nmin ̃P ∈PY ,q∈Q\n\n(cid:104)\n\nEX|s\n\nDKL(P (Y | X) || ̃P (Y | q(X), s)\n\n(cid:105)\n\n= min\n\n ̃P ∈PY ,q∈Q\n\nEX|s\n\n(cid:104)\n\n(cid:105) DKL(P (Y | X, q(X), s) || ̃P (Y | q(X), s)\n\n(cid:34)\n\n(cid:34)\n\n(cid:88)\n\nY\n\n(cid:88)\n\n= min\n\n ̃P ∈PY ,q∈Q\n\nEX|s\n\n= min\n\n ̃P ∈PY ,q∈Q\n\nEX|s\n\nY (cid:20)\n\nlog\n\nEX,Y |s\n\n= min\n\n ̃P ∈PY ,q∈Q\n\n= min\n\n ̃P ∈PY ,q∈Q\n\nP (Y | X, q(X), s) log\n\n(cid:35)\n\nP (Y | X, q(X), s) ̃P (Y | q(X), s)\n\nP (Y | X, q(X), s) log\n\nP (X, Y | q(X), s) ̃P (Y | q(X), s)P (X | q(X), s)\n\nP (X, Y | q(X), s) P (Y | q(X), s)P (X | q(X), s)\n\n(cid:21)\n\n+ EX,Y |s\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nP (Y | q(X), s) ̃P (Y | q(X), s)\n\n(cid:35)\n\n(6)\n\nI(X; Y | q(X), s) + EX|s[DKL(P (Y | q(X), s) || ̃P (Y | q(X), s))]\n\nIn the first equality, assuming P (X = x, S = s) > 0.7, we used the fact that given any X = x, the label Y is independent of any query answer q(X) = q(x) and event {S = s}. Thus, P (Y | X = x) = P (Y | X = x, q(X) = q(x), S = s). In the fourth equality we multiplied the term inside the log by the identity P (Y |q(X),s) P (Y |q(X),s) , where P (Y | q(X), s) represents the true posterior of Y given the query answer q(X) and S = s.\n\nNow observe that for any fixed S = s and any q ∈ Q,\n\nI(X, q(X); Y | s) = I(X; Y | s) + I(q(X); Y | X, s)\n\n= I(X; Y | s)\n\nThe second equality is obtained by using the fact that q(X) is a function of X.\n\nDecomposing I(X, q(X); Y | s) another way,\n\nI(X, q(X); Y | s) = I(q(X); Y | s) + I(X; Y | q(X), s)\n\nFrom equation 7 and equation 8 we conclude that\n\nmin q∈Q\n\nI(Y ; X | q(X), s) ≡ min q∈Q\n\n−I(q(X); Y | s)\n\n(7)\n\n(8)\n\nSubstituting the RHS in the above result in equation 6 we obtain the desired result.\n\n7For any x′ ∈ X , if P (X = x′, S = s) = 0, then x′ would not contribute to the expectation in the first\n\nequation and so we do not need consider this case.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nProof of Proposition 1. Restating the objective from equation V-IP,\n\nmin f,g\n\nwhere\n\nEX,S\n\n(cid:104)\n\nDKL\n\n(cid:16)\n\nP (Y | X) ∥ ˆP (Y | q(X), S\n\n(cid:17)(cid:105)\n\nq := g(S) ∈ Q ˆP (Y | q(X), S) := f ({q, q(X)} ∪ S),\n\nNow, for any realization S = s, such that P (S = s) > 0, we have,\n\nmin ̃P ∈PY ,q∈Q\n\nEX|s\n\n(cid:104)\n\nDKL\n\n(cid:16)\n\nP (Y | X) || ̃Ps(Y | q(X), s)\n\n(cid:17)(cid:105)\n\n= EX|s\n\n(cid:104)\n\nDKL\n\n= EX|s\n\n(cid:104)\n\nDKL\n\n= EX|s\n\n(cid:104)\n\nDKL\n\n= EX|s\n\n(cid:104)\n\nDKL\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\n(cid:16)\n\nP (Y | X) || ̃P ∗\n\ns (Y | q∗\n\ns (X), s)\n\n(cid:17)(cid:105)\n\nP (Y | X) || ˆP (Y | ̃q(X), s)\n\n(cid:17)(cid:105)\n\n+ EX|s\n\nP (Y | X) || ˆP (Y | ̃q(X), s)\n\n(cid:17)(cid:105)\n\n− EX|s\n\n(cid:34)\n\n(cid:88)\n\nY\n\n(cid:34)\n\n(cid:88)\n\nY\n\nP (Y | X) log\n\nP (Y | X) log\n\n(cid:35)\n\n(cid:35)\n\nˆP (Y | ̃q(X), s) ̃P ∗ s (Y | q∗ s (X), s)\n\n ̃P ∗ s (Y | q∗ s (X), s) ˆP (Y | ̃q(X), s)\n\nP (Y | X) || ˆP (Y | ̃q(X), s)\n\n(cid:17)\n\n− DKL\n\n(cid:16) ̃P ∗\n\ns (Y | q∗\n\ns (X), s) || ˆP (Y | ̃q(X), s)\n\n(cid:17)(cid:105)\n\n≤ EX|s\n\n(cid:104)\n\nDKL\n\n(cid:16)\n\nP (Y | X) || ˆP (Y | ̃q(X), s)\n\n(cid:17)(cid:105)\n\n(9) In the first equality we used the definition of ( ̃P ∗ s , q∗ s ) as the solution to the minimization problem in the first equality. In the second equality, ̃q = g(s) for any querier g and ˆP (Y | ̃q(X), s) = f ({ ̃q, ̃q(X)} ∪ s}) for any classifier f . In the fourth equality we appealed to lemma 1 to conclude that ̃P ∗ s (X) and history s. The final step we used the non-negativity of the KL-divergence for getting rid of the second term.\n\ns (X), s), the true posterior over Y given answer q∗\n\ns (X), s) = P (Y | q∗\n\ns (Y | q∗\n\nSince the inequality in equation 9 holds ∀S = s, and mappings f and g, we conclude that q∗ and ̃P ∗ s , q∗ by using lemma 1 to characterize q∗\n\ns = g∗(s) s (X)} ∪ s) for any given S = s. Equation 4 in the proposition is then proved\n\ns = f ∗({q∗\n\ns and ̃P ∗ s .\n\nB TRAINING PROCEDURE\n\nConsider a mini-batch of N samples, {(xi, yi)}N\n\ni=1 from a training set.\n\nIn Deep V-IP objective, the KL-divergence is mathematically equivalent to the cross entropy loss. The mini-batch estimate of this loss can be expressed as:\n\nmin θ,η\n\n−\n\n1 N\n\nN (cid:88)\n\ni=1\n\nyi log ˆyi\n\nsubject to qη = argmax(gη(si))\n\nˆyi = fθ(si ∪ {qη, qη(xi)}),\n\n(10)\n\nwhere yi is the ground truth label corresponding to input xi. si is obtained by sampling for P J S as defined in §3.3. We optimize the above objective using Stochastic Gradient Descent (or its variants).\n\nTo optimize objective 10, for every sample xi in the batch, the sampled history si is fed into the querier network gη which outputs a score for every query q ∈ Q. The argmax(.) (see D regarding its differentiability) operator converges these scores into a |Q|-dimensional one-hot vector, with the non-zero entry at the location of the max. We then append this argmax query qη and it’s answer, (qη, qη(X)) to si. The updated history si ∪ (qη, qη(xi)) is then fed into the classifier fθ to obtain a softmax distribution over the labels, denoted as ˆyi.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nC EXPERIMENT DETAILS\n\nAll of our experiments are implemented in python using PyTorch (Paszke et al., 2019) version 1.12. Moreover, all training is done on one computing node with 64-core 2.10GHz Intel(R) Xeon(R) Gold 6130 CPU, 8 NVIDIA GeForce RTX 2080 GPUs (each with 10GB memory) and 377GB of RAM.\n\nGeneral Optimization Scheme. The following optimization scheme is used in all experiments for both Initial Random Sampling and Subsequent Biased Sampling, unless stated otherwise. We minimize the Deep V-IP objective using Adam (Kingma & Ba, 2014) as our optimizer, with learning rate lr=1e-4, betas=(0.9, 0.999), weight decay=0 and amdgrad=True (Reddi et al., 2019). We also use Cosine Annealing learning rate scheduler (Loshchilov & Hutter, 2016) with T max=50. We train our networks fθ and gη for 500 epochs using batch size 128. In both sampling stages, we linearly anneal temperature parameter τ , in our straight-through softmax estimator, from 1.0 to 0.2 over the 500 epochs.\n\nTraining Details for RAM and RAM+. For train the RL policy and classification network we use the popular PPO algorithm (Schulman et al., 2017) with entropy regularization (0.01 regularization parameter). We used in initial learning rate of 3e-5 and clip value 0.2. For a fair comparison the architectures for the policy8 and classification networks are kept the same for RAM, RAM+ and V-IP.\n\nC.1 SPECIES CLASSIFICATION ON CUB.\n\nDataset and Query Set. Caltech-UCSD Birds-200-201 (CUB-200) (Wah et al., 2011) is a dataset of 200 bird species, containing 11,788 images with 312 annotated binary features for different visual attributes of birds, such as the color or shape of the wing or the head of the bird. We construct the query set Q using these attributes. For example, given an image for a Blue-jay a possible query might be “is the back-colour blue?” and the answer “Yes”.\n\nThe query set construction and data preprocessing steps as same as Chattopadhyay et al. (2022): In the original dataset, each annotated attribution for each image is too noisy, containing often imprecise description of the bird. Hence, if a certain attribute is true/false for over 50% of the samples in a given category, then that attribute is true/false for all samples in the same category. We also train a CNN to answer each query using the training set annotations called the concept network. This concept network provides answers for training all the methods compared in §4, namely, RAM, RAM+, G-IP and V-IP. Last but not least, our query set Q consists of 312 queries that ask whether each of the binary attributes is 1 for present or −1 for absent.\n\nArchitecture and Training A diagram of the architectures is shown in Figure 6. Both fθ and gη have the same full-connected network architecture (except the last linear layer), but they do not share any parameters with each. We initialize each architecture randomly, and train using the optimization scheme mentioned in the beginning of this section. The input history is a |Q|-dimensional vector with unobserved answers masked out with zeros.\n\nUpdating the History. Let the history of query-answer pairs observed after k steps be denoted as Sk. Sk is represented by a masked vector of dimension 312, and qk+1 = argmax(gη(Sk))9 is a one-hot vector of the same dimension, denoting the next query. For a given observation xobs, we update Sk using qk+1 as follows:\n\n• We obtain the query-answer by performing a point-wise multiplication,\n\nqk+1(xobs) = qk+1 ⊙ xobs.\n\nthat\n\nis,\n\n• We update the history to Sk+1 by adding this query answer qk+1(xobs) to Sk.\n\nThe entire process can be denoted by Sk+1 = Sk + qk+1 ⊙ xobs.\n\n8This term is from the RL community. In our context, the policy network is exactly the querier network. 9recall gη is our querier function parameterized by a deep network with weights η.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 6: Classifier fθ (Left) and querier gη (Right) architectures used for species classification on CUB.\n\nC.2 TOPIC IDENTIFICATION ON THE HUFFINGTON POST NEWS CATEGORY DATASET.\n\nDataset and Query Set. The Huffington Post News Category dataset (HuffingtonNews) is a natural language dataset, containing news “headlines” and their short descriptions (continuation of the headline) extracted from Huffington Post news published between 2012 and 2018. We follow the same data-processing procedure as Chattopadhyay et al. (2022): Each data point is an extended headline formed by concatenation of the headline with their short descriptions. Moreover, we also remove redundant categories, including semantically ambiguous and HuffPost-specific words such as “Impact“ and “Worldpost.” We also remove categories with small number of articles, along with semantically equivalent category names, such as “Arts & Culture” versus ”Culture & Art.” After processing, there is a total of 10 categories in the dataset. In addition, only the top-1,000 words are kept according to their tf-idf scores (Lavin, 2019), along with semantically redundant words removed. For more details, please refer to Chattopadhyay et al. (2022).\n\nThe query set Q contains binary questions of whether one of the 1000 words exist in the headline. The query answer is 1 if the word in question is present and −1 if absent.\n\nArchitecture and Training. A diagram of the architectures is shown in Figure 7. Both the classifer fθ and the querier gη shares the same architecture except the last layer; However, they do not share any parameters. The inputs to fθ and gη are masked vectors, with masked values set to 0. To optimize the Deep V-IP objective, we randomly randomly initialize fθ and gη, and train using Adam optimizer and Cosine Annealing learning rate scheduler, with the settings mentioned in the beginning of the section. During Subsequent Adaptive Sampling, we train for 100 epochs using Stochastic Gradient Descent (SGD) instead of Adam, with learning rate lr=1e-4 and momentum=0.9, and Cosine Annealing learning rate scheduler, with T max=100. The input history is a |Q|-dimensional vector with unobserved answers masked out with zeros.\n\nUpdating the History. The method of updating the history is equivalent to that for CUB as mentioned in §C.1. The history is now a masked vector of dimension 1000, since there are 1000 queries in our query set for this dataset.\n\nC.3\n\nIMAGE CLASSIFICATION ON MNIST, KMNIST AND FASHION-MNIST.\n\nEach setting mentioned in the following is the same for MNIST, KMNIST and Fashing-MNIST unless stated otherwise.\n\nDataset and Query Set. MNIST (LeCun et al., 1998), KMNIST (Clanuwat et al., 2018), FashionMNIST (Xiao et al., 2017b) are gray-scale hand-written digit datasets, each containing 60,000 train-\n\n18\n\nLinear 2000History312 x 1 LayerNorm + ReLULinear 500LayerNorm + ReLULinear 200Class Logits200 x 1 Linear 2000History312 x 1 LayerNorm + ReLULinear 500LayerNorm + ReLULinear 312Query Scores 312 x 1 Published as a conference paper at ICLR 2023\n\nFigure 7: Classifier fθ (Left) and querier gη (Right) architectures used for topic identification on Huffington Post News Category dataset.\n\ning images and 10,000 testing images of size 28 × 28. We follow Chattopadhyay et al. (2022) for data pre-processing procedure and the design of our query sets of all three datasets.\n\nEach gray-scale image is converted into a binary image. For MNIST and KMNIST, we round values greater or equal than 0.5 up to 1 and round values below 0.5 down to -1. For Fashion-MNIST, we round values greater or equal up to 0.1 to 1 and round values below 0.1 down to -1.\n\nEach query set Q contains all overlapping 3 × 3 patches over the 28 × 28 pixel space, resulting in 676 queries. Each query answer indicates the 9 pixel intensities at the queried patch. The inputs to the classifier fθ and the querier gθ are masked images, with masked pixels zeroed out if they are not part of the current History.\n\nUpdating the History. The method for updating the history is equivalent to that for CUB as mentioned in §C.1 with some differences as we will describe next. For a given observation xobs, we update Sk using qk+1 as follows:\n\n• We reshape qk+1 from a vector of dimension 676 (the number of queries) to a 2D grid of\n\ndimension 26 × 26, denoted by ˆqk+1.\n\n• ˆqk+1 is then converted to a binary matrix with 1s at the location corresponding to the queried 3 × 3 patch and 0s everywhere else via a convolutation operation with a kernel of all 1s of size 3 × 3, stride 1 and padding 2.\n\n• We then obtain the query-answer by performing a hadamard product of the convolved output (in the previous step) with xobs. This results in a masked image, ˆqk+1(xobs), with the queried patch revealed and 0 everywhere else.\n\n• Finally, we update the history to Sk+1 by adding this query-answer to Sk. To account for pixels observed (unmsaked) in ˆqk+1(xobs) that overlap with the history Sk, we clip the values in Sk to lie between −1 and 1.\n\nThe entire process can be summarized as,\n\nSk+1 = Clip (cid:0)Sk + (Conv2D(ˆqk+1) ⊙ xobs), minval = −1, maxval = 1(cid:1)\n\n19\n\nLinear 2000History1000 x 1LayerNorm + ReLULinear 1000LayerNorm + ReLULinear 10Class Logits10 x 1 Linear 500LayerNorm + ReLULinear 2000History 1000 x 1LayerNorm + ReLULinear 1000LayerNorm + ReLULinear 1000Query Scores 1000 x 1Linear 500LayerNorm + ReLUPublished as a conference paper at ICLR 2023\n\n.\n\nArchitecture and Training. Refer to Figure 8 for a diagram of the architecture for the classifier fθ and the querier gη. Every Conv is a 2D Convolution with a 3 × 3 kernel, stride 1 and padding 1. Moreover, every MaxPool is a 2D max pooling operator with a 2 × 2 kernel. We initialize each architecture from random initialization, and train our networks using Adam as our optimizer and Cosine Annealing learning rate scheduler, with the same settings mentioned in the beginning of this section.\n\n(a) Classifier fθ\n\n(b) Querier gη\n\nFigure 8: Architectures used for image classification on MNIST, KMNIST, and Fashion-MNIST.\n\nC.4 MEDICAL DIAGNOSIS ON SYMCAT, MUZHI AND DXY\n\nDataset and Query Set. MuZhi (Wei et al., 2018) and Dxy (Xu et al., 2019) are two real-world medical datasets containing symptoms and diseases extracted from Chinese healthcare websites (https://muzhi.baidu.com/ and https://dxy.com/), where doctors provide online help based on patient’s self-report symptoms and online conversations. We follow the same data processing procedure as He et al. (2022). MuZhi has 66 symptoms (features) and 4 diseases (classes), including children’s bronchitis, children’s functional dyspepsia, infantile diarrhea infection, and upper respiratory infection. Dxy dataset contains 41 symptoms for 5 diseases: allergic rhinitis, upper respiratory infection, pneumonia, children hand-foot-mouth disease, and pediatric diarrhea. Last but not least, our query set Q consists of queries that correspond to asking questions about the presence of each symptom for the patient; the query-answer is either 1 for Yes, 0 for No and -1 for Can’t Say.\n\nSymCAT is a synthetic medical dataset generated from a symptom checking website called SymCAT introduced by Peng et al. (2018). The dataset has three versions; SymCAT-200 contains 328 symptoms and 200 disease, SymCAT-300 contains 349 symptoms and 300 classes, and SymCAT-400 con-\n\n20\n\nConv 32History28 x 28 x 1BatchNorm + ReLUConv 64BatchNorm + ReLU + MaxPoolConv 128BatchNorm + ReLUConv 256BatchNorm + ReLU + MaxPoolFlattenLinear 512BatchNorm + ReLULinear 10Class Logits10 x 1 Conv 32History 28 x 28 x 1BatchNorm + ReLUConv 64BatchNorm + ReLU + MaxPoolConv 128BatchNorm + ReLUConv 256BatchNorm + ReLU + MaxPoolUpsample + Conv 256ReLUUpsample + Conv 128BatchNorm + ReLUUpsample + Conv 64BatchNorm + ReLUUpsample + Conv 64ReLUUpsample + Conv 32BatchNorm + ReLUUpsample + Conv 1Query Scores26 x 26 Published as a conference paper at ICLR 2023\n\ntains 355 symptoms and 400 classes. We used publically avialable version of this dataset provided by Nesterov et al. (2022) at https://github.com/SympCheck/NeuralSymptomChecker. Our query set Q consists of queries that correspond to asking questions about the presence/absence of each symptom for the patient; the query-answer is either 1 for Yes and 0 for No.\n\nArchitecture and Training. A diagram of the architecture is shown in Figure 9. We used the set architecture proposed in Ma et al. (2018). We made this choice for a fair comparison with He et al. (2022) which also used to same architecture for their partial-VAEs. The input to the network is a concatenation of the query-answers {q(x j) : q ∈ Q}, trainable positional embeddings e j (red blocks), and bias terms b j (blue blocks). Each positional embedding is also multiplied by the query-answer. After the first linear layer, the intermediate embedding is multiplied by a queryanswer mask derived from the history, which each dimension has a value of 1 for query selected in the history and 0 otherwise. To optimize our objective, we randomly initialize fθ and gθ and train using the same optimizer and learning rate scheduler setting as mentioned in the beginning of the section. However, we train our algorithms for only 200 epochs, and linearly anneal the straightthrough softmax estimator’s temperature τ from 1.0 to 0.2 over the first 50 epochs.\n\nUpdating the History. Let the history of query-answer pairs observed after k steps be denoted as Sk. Since we used set architectures as our querier and classifier networks for these datasets, as proposed in Ma et al. (2018), Sk is represented as a set consisting of embeddings of query-answer pairs observed so far. The next query, qk+1 = argmax(gη(Sk)) is a one-hot vector of dimension equal to the size of the query set used. For a given observation xobs, we update Sk using qk+1 as follows:\n\n• Let M be a matrix of size |Q|×d where every row corresponds to a query-answer evaluated at xobs and d is the size the embeddings used for representing the query-answers. We obtain the answer corresponding to the selected query qk+1 by performing a matrix-vector product, that is, qk+1(xobs) = qT\n\nk+1M .\n\n• We update the history to Sk+1 by concatenating qk+1(xobs) to Sk.\n\nC.5\n\nIMAGE CLASSIFICATION ON CIFAR-10 AND CIFAR-100\n\nDataset and Query Set. CIFAR-{10,100} (Krizhevsky et al., 2009) are natural image datasets that contain {10, 100} different classes of objects. They contain 50,000 training images and 10,000 testing images. Each RGB image is of size 32 × 32. We design our query set Q following Rangrej & Clark (2021) consisting of all 8 × 8 overlapping patches with stride 4. This results in a query set size |Q| of 49. The inputs to the classifier fθ and the querier gη are full-sized 3 × 32 × 32 masked image, with masked pixels zeroed out if they are not part of the current History.\n\nArchitecture and Training. For CIFAR-10, the architectures used for the classifier fθ and querier gη are both Deep Layer Aggregation Network (DLA) (Yu et al., 2018). fθ and gη do not share any parameters with each other. An out-of-the-box implementation was used and can be found here: https://github.com/kuangliu/pytorch-cifar/blob/master/ models/dla.py. For CIFAR-100, the architectures used for the classifier fθ and querier gη are both DenseNet169 (Huang et al., 2017). fθ and gη do not share any parameters with each other. An out-of-the-box implementation was used and can be found here: https://github.com/ kuangliu/pytorch-cifar/blob/master/models/densenet.py. The only change made to the architectures is the last layer, which the dimensions depend on the number of classes or the size of the query set Q.\n\nDuring training, we follow standard data processing techniques as in He et al. (2016). In CIFAR10, we set batch size 128 for both initial and subsequent sampling stages. In CIFAR-100, we set batch size 64 for both initial and subsequent sampling stages. For both CIFAR-10 and CIFAR-100, we randomly initialize fθ and gη, and train them for 500 epochs during Initial Random Sampling using optimizer Adam and Cosine Annealing learning rate scheduler, with settings mentioned above. During Subsequent Adaptive Sampling, we optimize using Stochastic Gradient Descent (SGD), with learning rate lr=0.01 and momentum=0.9 and Cosine Annealing learning rate scheduler, with T max=50, for 100 epochs.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nFigure 9: Architecture used for Medical Diagnosis tasks on SymCAT, MuZhi and Dxy. The classifier fθ and querier gη share parameters in this architecture. |Q| is the size of the query set, which also corresponds to the number of features. C is the number of classes, depending on the dataset.\n\nUpdating the History. The method of updating the history is similar to that for MNIST, KMNIST, and Fashion-MNIST, as mentioned in § C.3. For a given observation xobs, we update the history Sk using qk+1 as follows:\n\n• We reshape qk+1 from a vector of dimension 49 (the number of queries) to a 2D grid of\n\ndimension 7 × 7, denoted by ˆqk+1.\n\n• ˆqk+1 is then converted to a binary matrix with 1s at the location corresponding to the queried 8 × 8 patch and 0s everywhere else via a 2D transposed convolutation operation with a kernel of all 1s of size 8 × 8, stride 4 and no padding.\n\n• We then obtain the query-answer by performing a hadamard product of the convolved output (in the previous step) with xobs. This results in a masked image, ˆqk+1(xobs), with the queried patch revealed and 0 everywhere else.\n\n22\n\nLinear 1024q|Q|(X)1 x 1q|Q|(X) * e|Q|400 x 1b|Q| 1 x 1q1(X)1 x 1q1(X) * e1400 x 1 b11 x 1Embedding1024 x |Q| MaxAggregate + ReLUAggregated Embedding1024 x 1 Linear 2000BatchNorm + ReLULinear 500Linear CLinear |Q|Class LogitsC x 1Query Scores |Q| x 1 BatchNorm + ReLUMask1024 x |Q| Published as a conference paper at ICLR 2023\n\n• Finally, we update the history to Sk+1 by adding this query-answer to Sk. Any pixel ij that is observed (unmasked) in ˆqk+1(xobs) and is also observed in the history Sk is handled by the transformation Sk+1[i, j] → Sk[i, j] if Sk+1[i, j] = 2Sk[i, j].\n\nThe entire process can be summarized as,\n\nS′\n\nk+1 = Sk + TransposedConv2D(ˆqk+1) ⊙ xobs.\n\n∀(i, j) ∈ {Number of pixels in image}\n\nSk+1[i, j] = Sk[i, j] Sk+1[i, j] = S′\n\nk+1[i, j] otherwise\n\nif S′\n\nk+1[i, j] = 2Sk[i, j]\n\n(11)\n\nD STRAIGHT-THROUGH SOFTMAX ESTIMATOR\n\nAs mentioned in §3.3, in the main text, we employ the straight-through softmax gradient estimator for differentiating through the argmax operation. We will now describe this estimator in detail. Consider the following optimization problem,\n\nf (argmax(θ))\n\nmin θ∈Rd\n\n(12)\n\nLet Z := argmax(θ). We will assume f is differentiable in its input. The straight-through softmax estimator of the gradient of f w.r.t θ is defined as,\n\n∇ST\n\nθ f :=\n\n∂f ∂Z\n\ndsoftmaxτ (θ) dθ\n\n,\n\n(cid:20)\n\nθ1 τ\n\nθ2 τ\n\n(cid:21)\n\nθd τ\n\nwhere softmaxτ (θ) :=\n\ne (cid:80)d tice that limτ →0 softmaxτ (θ) → argmax(θ). Thus, we replace the gradient of the argmax operation which is either 0 almost everywhere or doesn’t exist with a surrogate biased estimate.\n\nand τ is the temperature parameter. No-\n\ne (cid:80)d\n\ne (cid:80)d\n\ni=1 e\n\ni=1 e\n\ni=1 e\n\n. . .\n\nθi τ\n\nθi τ\n\nθi τ\n\nEquation 12 can then be optimized using the straight-through estimator by iteratively carrying out the following operation,\n\nwhere η is the learning rate.\n\nθ = θ − η∇ST\n\nθ f,\n\nIn our experiments we start with τ = 1.0 and linearly anneal it down to 0.2 over the course of training.\n\nE ABLATION STUDIES\n\nIn §3.3 we discussed two possible architectures for operating on histories of arbitrary sequence lengths; the set-based architectures (as in Figure 9) and the fixed-sized input masking based architectures (as in Figure 6). In Figure 10, we compare the two architectures on the same task of bird species identification using the same query set (see Table 1). We see that the fixed-sized input masking based architecture performs better in terms of avg. number of queries needed to get the same performance. Based on this observation, we use the latter architecture in all our experiments except on the medical datasets, where we used the set-based architecture for fair comparison with the BSODA method which uses a similar set-based architecture for their partial-VAEs.\n\nIn §3.3 we discussed that a sampling distribution PS that assigns a positive mass to every element in ̄K would make learning a good querier function challenging since the network has to learn over an exponentially (in size of Q) large number of histories. Instead we proposed to sequential bias the sampling according to our current estimate of the querier function. We validate the usefulness of this biased sampling strategy in Figure 11. In most datasets we observe that biasing the sampling distribution for S helps learn better querying strategies (in terms of accuracy vs. explanation length trade-offs).\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nFigure 10: Comparison of accuracy v/s explanation length (avg. number of queries) curves for the set-based and fixed-sized input masking based architectures on the CUB-200 dataset.\n\nNotice that in most datasets, biased sampling without the initial random sampling ultimately ends up learning a slightly better strategy (in terms of accuracy vs. explanation length trade-offs). However, since biased sampling requires multiple forward passes through our querier network to generate histories, it is much slower than the initial random sampling (IRS) scheme for PS. Thus, when computational resources are not a concern one could do away with the initial random sampling but under a computational budget, the random sampling allows for finding a quick solution which can then be finetuned using biased sampling. This finetuning will potentially require fewer epochs to get good performance than training using biased sampling from scratch.\n\nF EXTENDED RESULTS\n\nIn Figure 12, we show the trade-off between accuracy and explanation length (avg. number of queries) on KMNIST and Fashion-MNIST datasets as the stopping criterion ε is changed. In both these datasets, the “MAP criterion” is used as the stopping criterion. V-IP performs better than RLbased RAM and RAM+ on both these datasets. V-IP is competitive with G-IP eventually surpassing it in terms of accuracy for longer query-answer chains.\n\nIn addition, Table 4 shows extended results for AUC values for test accuracy versus explanation length curves for different datasets. A simplified table is shown in Table 2.\n\nTable 4: Extended results for Table 2. Every method, except G-IP, was repeated 5 times with a different seed. The high computational cost of G-IP (one run of inference on the MNIST test set takes a few weeks) makes it infeasible to repeat G-IP multiple times for computing standard-deviation values.\n\nDataset\n\nRandom\n\nRAM\n\nRAM+\n\n0.557 ± 0.002 CUB 0.423 ± 0.015 HuffingtonNews 0.868 ± 0.002 MNIST 0.775 ± 0.003 KMNIST Fashion-MNIST 0.735 ± 0.029\n\n0.662 ± 0.002 0.389 ± 0.010 0.916 ± 0.005 0.832 ± 0.003 0.770 ± 0.003\n\n0.695 ± 0.004 0.431 ± 0.003 0.920 ± 0.007 0.841 ± 0.002 0.804 ± 0.010\n\nG-IP\n\n0.736 0.691 0.964 0.872 0.831\n\nV-IP (Ours)\n\n0.716 ± 0.008 0.664 ± 0.002 0.956 ± 0.002 0.911 ± 0.008 0.849 ± 0.010\n\nG COMPARING V-IP WITH BLACK-BOX DEEP NETWORKS\n\nAn important aspect of the framework introduced by Chattopadhyay et al. (2022) is that the enduser defines queries that are interpretable to them. Given this set of queries, Q, V-IP learns to efficiently compose them into concise explanations for model predictions (in terms of query-answer chains). This begs the question, how much do we loose in terms of performance by constructing an interpretable Q? In Table 5, we report results studying this question.\n\n“Acc. w/ V-IP given ε” is the test accuracy obtained by V-IP after termination using our stopping criterion. “Acc. w/ V-IP given Q(X)” is the accuracy the classifier network (trained jointly with the\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n(a) MNIST\n\n(b) Fashion-MNIST\n\n(c) KMNIST\n\n(d) symCAT-200\n\n(e) symCAT-300\n\n(f) symCAT-400\n\n(g) CUB\n\n(h) HuffingtonNews\n\n(i) CIFAR-10\n\nFigure 11: Accuracy v/s Explanation length (avg. number of queries) curves for different datasets with and without the successive biased sampling for history S. We denote training with Initial Random Sampling only as IRS; training with Subsqeuent Biased Sampling only as SBS; and training with IRS, followed by subsequent fine-tuning with SBS as IRS + SBS.\n\nFigure 12: Tradeoff between accuracy and explanation length (average number of queries) for Fashion-MNIST (left) and KMNIST (right). Reported curves are averaged over 5 runs with shaded region denoting the standarddeviation.\n\nqueried network using the Deep V-IP objective) obtains when seeing all the query answers Q(X). In all data sets, V-IP learns to predict with short explanations (avg. number of queries) and a test accuracy in proximity to what can be achieved if all answers were observed (col 6).\n\n25\n\nPublished as a conference paper at ICLR 2023\n\n‘Acc. w/ Black-Box given Q(X)” reports the accuracy a black-box deep network obtains by training on feature vectors comprised of all query answers Q(X) using the standard cross-entropy loss. Columns 6 and 7 show that training a classifier network with the Deep V-IP objective results in only a minor loss in performance compared to training using the standard supervised classification cross-entropy loss.\n\n“Acc. w/ Black-Box given X” reports test accuracies obtained by training black-box deep networks on the whole input X to produce a single output (the classification label). Comparing these values with the accuracies reported in col 6 we see that basing predictions on an interpretable query set, almost always, results in a drop in accuracy. This is expected since interpretability can be seen as a constraint on learning. For example, there is a drop of about 15% for the HuffingtonNews dataset since our queries are about the presence/absence of words which completely ignores the linguistic structure present in documents. Similarly, for the binary image classification tasks (MNIST, KMNIST and Fashion-MNIST) the queries are binary patches which can be easily interpreted as edges, foregrounds and backgrounds. This binarization however results in a drop in performance, especially in Fashion-MNIST where it is harder to distinguish between some classes like coat and shirt without grayscale information.\n\nTable 5: Comparison of Test Performance between prediction using V-IP with stopping criterion ε, prediction using all queries, and prediction using a black-box trained model.\n\nDataset\n\nHuffingtonNews CUB-200 MNIST KMNIST Fashion-MNIST CIFAR-10 CIFAR-100\n\n|Q|\n\n1000 312 676 676 676 49 49\n\nStopping Criterion\n\n(ε)\n\nExplanation Length\n\nAcc. w/ V-IP given ε\n\nAcc. w/ V-IP given Q(X)\n\nAcc. w/ Black-Box given Q(X)\n\nAcc. w/ Black-Box given X\n\nMAP (0.99) Stability (0.001) MAP (0.99) MAP (0.99) MAP (0.99) Stability (0.127) Stability (1.438)\n\n195.89 18.82 6.34 20.34 19.42 9.79 12.82\n\n0.672 0.752 0.971 0.916 0.872 0.936 0.752\n\n0.712 0.763 0.991 0.958 0.876 0.946 0.788\n\n0.715 0.763 0.992 0.951 0.884 0.955 0.798\n\n0.865 10 0.827 11 0.998 12 0.988 13 0.967 14 0.955 15 0.798 16\n\nH ADDITIONAL QUERY-ANSWER CHAINS\n\nWe show additional trajectories for different task and datasets: CUB-200 (Figure 13), MNIST (Figure 14), Fashion-MNIST (Figure 15), KMNIST (Figure 16), HuffingtonNews (Figure 18), CIFAR10 (Figure 19) and CIFAR-100 (Figure 20). In every figure, we see that the correct predictions are explained by an interpretable sequence of query-answer pairs. The evolution of the posterior P (Y | q1:k(xobs)), as more and more queries are asked, gives insights into the model’s decisionmaking process as we see it’s belief shift among the possible labels for xobs. This adds an additional layer of transparency to the model.\n\n10Number taken from Chattopadhyay et al. (2022) where authors fine-tuned a Bert Large Uncased Trans-\n\nformer model to classify documents.\n\n11Number taken from Koh et al. (2020) which trained a CNN on raw images of birds from the CUB-200\n\ndataset\n\n12Number reported from Hu et al. (2018) 13Number reported from Clanuwat et al. (2018) 14Number reported from Xiao et al. (2017a) 15Trained a Deep Layer Aggregation model Yu et al. (2018) to classify CIFAR-10 images from scratch. 16Number reported fromHuang et al. (2017)\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nFigure 13: Examples for CUB-200.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 14: Examples for MNIST.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 15: Examples for Fashion-MNIST.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 16: Examples for KMNIST.\n\n30\n\nPublished as a conference paper at ICLR 2023\n\nFigure 17: Examples for SymCAT200\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nFigure 18: Examples for HuffingtonNews.\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nFigure 19: Examples for CIFAR10.\n\nFigure 20: Examples for CIFAR100.\n\n33",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a method called V-IP (Variational Information Pursuit) that does a multi-step prediction to improve interpretability instead of doing a one-pass prediction like other neural nets do. In each step, only a small sets of features (i.e. \"query set\" called in the paper) are revealed and the goal is to make a prediction using the minimun number of steps i.e. part of the feature sets. It can derive interpretability becasue the subsets of features causing a big increase of the prediction of the ground truth class between steps can be seen as important rationales of why model makes such prediction.\n\nPreviously, most method resort to using generative models to model the distributions between labels and subsets of features to pick which parts of features can maximally predict the target by resorting to MCMC sampling methods (the baseline called G-IP). Or others have proposed using reinforcement learning to sequentially select the feature sets that predict the correct target. The proposed method, V-IP, instead learns to greedily choose the subsets of features that maximize the downstream classifiers to predict the target y in each step as measured in the KL divergence, which can be seen as the mutual information of the current selected features in each step. Note V-IP can be seen as a RL method that has an immediate reward and the decay factor gamma set to 0. In the classification part, V-IP experiments with set-based and mask-based classifiers to predict and find the mask-based ones perform better. In a wide-variety of datasets including images, medical diagnosises, and texts data, the V-IP outperforms recent G-IP related methods and RL-based methods.\n\n# Strength And Weaknesses\n\n# Strength\n\n- The writing is clear and easy to follow. \n- The examples of interpretability shown in Fig. 3 are interesting to see.\n- I like the careful details of biased samplings and the set-based classifiers v.s. mask-based classifiers.\n- The experiments seem thorough, but some more experiments can be helpful. See below.\n\n# Weaknesses\n\n1. IMHO, the novelty side may be a little bit low since this method can be seen as a RL method with immediate reward of improving the classifiers predictions. Other inventions like initial random sampling and subsequent biased sampling are new in my opinions.\n2. G-IP seems to perform quite similarly or sometimes better than V-IP (Fig. 4) in larger datasets like CUB-200 with large query size 312. It seems contradictory to what authors state \"We thus expect the gains of V-IP to be most evident on large-scale datasets\". An ablation study that improves from small to large number of examples in a dataset may help verify what the authors claim.\n3. There is no consistent baselines across the datasets, which may make comparisons difficult. If it's easy to do, can authors also run the G-IP on the datasets in Table 3 and Figure 5 CIFAR-10/CIFAR-100 to see if the proposed method V-IP is indeed better than G-IP? Or authors can comment on why such comparisons are not easy due to code inaccessibility etc.\n4. I would love to see a more complete ablation study than the Supp. E (Fig. 11) that compare another version <1> No initial random sapling, and put a subset of results into a small table in the main text if possible.\n5. The metrics reported in the experimental section are mostly borrowed from other papers. Although it's great to directly compare to the SOTA numbers, there maybe subtle differences in the implementation leading to such result. For example, can authors please confirm fi those baselines BSODA, REFUEL in Table 3 and Figure 5 are using the same classifier architectures and training strategies the same as V-IP and the only difference lies in the strategy of selecting the features? If not, such number should not be directly compared, or some ablation studies are needed e.g. using BSODA's classifiers instead of the V-IP classifiers.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n# Clarity\n\nI enjoy reading the paper and it's quite smooth and clear. One little thins is maybe authors can illustrate that those queries are in fact the features in a dataset in Fig. 1. Although I understand the queries are more general, it makes me confused that if an oracle is needed to get the answer of these questions or it needs to be learned. I realize that those questions in Fig. 1 are in fact just a feature value in the dataset in the experiment sections so no oracle is needed.\n\n# Quality\nThe experiments are overall good. One important thing is can authors please add standard deviation on both Table 2, 3 and Figure 4, 5 to understand the significance of each method?\n\n# Originiality\nIMHO, I think the work has slightly lower originiality since these methods are similar to RL-based methods. \n1. It reminds me of an earlier work [1] that also uses RL to do per-instance feature selection. Can authors please comment on the relations?\n2. Can authors comment on in what scenarios the proposed greedy approach work better (gamma=0), and in what scenarios the RL-based approaches (gamma > 0) can be better? [1] seems to show that the RL-based approaches perform better.\n\n# Thoughts (may not be important)\n1. Can this method be further improved by combining with a generative approach such as partial VAE? For example, a way to improve V-IP is that for each feature selection doing an imputation for the rest of unselected features and use all the features to send to the classifier. I think it will improve in the image space where there is a high-degree of correlations. Do you think if this approach will improve the accuracy, and also the interpretability?\n\n[1] INVASE: Instance-wise Variable Selection using Neural Networks: https://openreview.net/forum?id=BJg_roAcK7\n\n# Summary Of The Review\n\nOverall I think the authors do a great job in writing and the examples presented are interesting to see. But given the crowded space of this problem (feature subsets selection), I am hoping to see more complete comparisons and other interesting questions as suggested above. The experimental number also lacks standard deviation which I believe is important. Although I find this work slightly less original, I don't mind such work being accepted as long as there are concrete evaluations and ablation studies to make readers learn something from it.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nIMAGES AS WEIGHT MATRICES: SEQUENTIAL IMAGE GENERATION THROUGH SYNAPTIC LEARNING RULES\n\nKazuki Irie1 J ̈urgen Schmidhuber1,2 1The Swiss AI Lab, IDSIA, USI & SUPSI, Lugano, Switzerland 2AI Initiative, KAUST, Thuwal, Saudi Arabia {kazuki, juergen}@idsia.ch\n\nABSTRACT\n\nWork on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of selfinvented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-ofthe-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step “denoising” of FPA-generated images to enhance their quality. Our code is public.1\n\n1\n\nINTRODUCTION\n\nA Fast Weight Programmer (Schmidhuber, 1991a; 1992) is a neural network (NN) that can learn to continually generate and rapidly modify the weight matrix (i.e., the program) of another NN in response to a stream of observations to solve the task at hand (reviewed in Sec. 2.1). At the heart of the weight generation process lies an expressive yet scalable parameterisation of update rules (or learning rules, or programming instructions) that iteratively modify the weight matrix to obtain any arbitrary weight patterns/programs suitable for solving the given task. Several recent works (Schlag et al., 2021a; Irie et al., 2021; 2022c;b) have demonstrated outer products with the delta rule (Widrow & Hoff, 1960; Schlag et al., 2021b) as an effective mechanism for weight generation. In particular, this has been shown to outperform the purely additive Hebbian update rule (Hebb, 1949) used in the Linear Transformers (Katharopoulos et al., 2020; Choromanski et al., 2021) in various settings including language modelling (Schlag et al., 2021a), time series prediction (Irie et al., 2022b), and reinforcement learning for playing video games (Irie et al., 2021). However, despite its intuitive equations—treating the fast weight matrix as a key/value associative memory—, the effective “actions” of these learning rules on the “contents” stored in the weight matrix still remain opaque, because in general the values stored in a weight matrix are not easily interpretable by humans.\n\nNow what if we let a fast weight programmer generate a “weight matrix” that corresponds to some human-interpretable data? While outer product-based pattern generation may have a good inductive bias for generating a weight matrix of a linear layer2, it can also be seen as a generic mechanism for iteratively generating any high dimensional data. So let us apply the same principle to generate and\n\n1https://github.com/IDSIA/fpainter 2In a linear layer, the weight matrix is multiplied with an input vector to produce an output vector. Consequently, assuming that the output vector is used to compute some scalar loss function, the gradient of the loss w.r.t. weights is expressed as an outer product (between the input and the gradient of the loss w.r.t. the output).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nincrementally refine natural images. We treat a colour image as three weight matrices representing synaptic connection weights of a fictive NN, and generate them iteratively through sequences of delta learning rules whose key/value patterns and learning rates are produced by an actual NN that we train. The resulting Fast Weight Painters (FPAs) learn to sequentially generate images, as sums of outer products, one rank at a time, through sequential applications of delta learning rules. Intuitively, the delta rule allows a painter to look into the currently generated image in a computationally efficient way, and to apply a change to the image at each painting step. We empirically observe that the delta rules largely improve the quality of the generated images compared to the purely additive outer product rules.\n\nWe train our FPAs in the framework of Generative Adversarial Networks (GAN; Goodfellow et al. (2014); Niemitalo (2010); Schmidhuber (1990); reviewed in Sec. 2.2). We evaluate our model on six standard image generation datasets (CelebA, LSUN-Church, Metfaces, AFHQ-Cat/Dog/Wild; all at the resolution of 64x64), and report both qualitative image quality as well as the commonly used Fr ́echet Inception Distance (FID) evaluation metric (Heusel et al., 2017). Performance is compared to the one of the state-of-the-art StyleGAN2 (Karras et al., 2020b;a) and the speed-optimised “lightweight” GAN (LightGAN; Liu et al. (2021)).\n\nWhile the performance still largely lags behind the one of StyleGAN2, we show that our generic models can generate images of respectable visual quality without any explicit inductive bias for image processing (e.g., no convolution is used in the generator). This confirms and illustrates that generic learning rules can effectively produce complex weight patterns that, in our case, yield natural images in various domains. Importantly, we can visualise each step of such weight generation in the human-interpretable image domain. This is a unique feature of our work since learning rules are typically not visually meaningful to humans in the standard weight generation scenario—see the example shown in Figure 1 (weight generation for few-shot image classification).\n\nClearly, our goal is not to achieve the best possible image generator (for that, much better convolutional architectures exist). Instead, we use natural images to visually illustrate the behaviour of an NN that learns to execute sequences of learning rules. Nevertheless, it is also interesting to see how a convolutional NN can further improve the quality of FPA-generated images. For this purpose, we conduct an additional study where we add to the FPA’s output, a now popular convolutional U-Net (Ronneberger et al., 2015; Salimans et al., 2017) used as the standard architecture (Ho et al., 2020; Song et al., 2021; Dhariwal & Nichol, 2021) for denoising diffusion models (Sohl-Dickstein et al., 2015). The image-to-image transforming U-Net learns (in this case) one-step “denoising” of FPA-generated images and effectively improves their quality.\n\nFigure 1: An illustration of the hardly human-interpretable standard weight generation process through sequences of delta rules in an FWP (DeltaNet) trained for 5-way 5-shot image classification on Mini-ImageNet (Vinyals et al., 2016; Ravi & Larochelle, 2017). The model is trained with the public code of Irie et al. (2022c) and achieves a test accuracy of 62.5%. The input to the model (shown at the bottom) is a sequence of images with their label (except for the last one to be predicted) processed from left to right, one image/label pair per step. The model has four layers with 16 heads each and a hidden layer size of 256. Each head generates a 16x16-dimensional fast weight matrix. Here we visualise weight generation of two heads: head ‘16’ in layer 1 and head ‘12’ in layer 4 as examples. In each case, the top row shows the rank-one update term (last term in Eq. 2), and the bottom row shows their cumulative sum, i.e., the fast weight matrix Wt of Eq. 2, generated at the corresponding step.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 BACKGROUND\n\nHere we review the two background concepts that are essential to describe our approach in Sec. 3: Fast Weight Programmers (Sec. 2.1) and Generative Adversarial Networks (Sec. 2.2).\n\n2.1 FAST WEIGHT PROGRAMMERS (FWPS)\n\nA Fast Weight Programmer (FWP; Schmidhuber (1991a; 1992)) is a general-purpose auto-regressive sequence-processing NN originally proposed as an alternative to the standard recurrent NN (RNN). The system consists of two networks: the slow net, or the programmer3 that learns (typically by gradient descent) to generate and rapidly modify a weight matrix (i.e., the program) of another net, the fast net, based on the input available at each time step. The context-sensitive fast weight matrix serves as short-term memory of this sequence processor. This concept has seen a recent revival due to its formal connection (Katharopoulos et al., 2020; Schlag et al., 2021a) to Transformers (Vaswani et al., 2017). In fact, Transformers with linearised attention (Katharopoulos et al., 2020) have a dual form (Aizerman et al., 1964; Irie et al., 2022a) that is an FWP with an outer product-based weight generation mechanism (Schmidhuber, 1991a; 1992). Let t, dkey, din, and dout denote positive integers. A notable example of such a model is the DeltaNet (Schlag et al., 2021a) that, at each time step t, transforms an input xt ∈ Rdin into an output yt ∈ Rdout while updating the fast weight matrix Wt−1 ∈ Rdout×dkey (starting from W0 = 0) as follows:\n\n[qt, kt, vt, βt] = Wslowxt\n\nWt = Wt−1 + σ(βt)(vt − Wt−1φ(kt)) ⊗ φ(kt)\n\nyt = Wtφ(qt)\n\n(1)\n\n(2)\n\n(3)\n\nwhere the slow net (Eq. 1; with a learnable weight matrix Wslow ∈ R(2∗dkey+dout+1)×din ) generates query qt ∈ Rdkey , key kt ∈ Rdkey, value vt ∈ Rdout vectors as well as a scalar βt ∈ R (to which we apply the sigmoid function σ). φ denotes an element-wise activation function whose output elements are positive and sum up to one (we use softmax) that is crucial for stability (Schlag et al., 2021a). Eq. 2 corresponds to the rank-one update of the fast weight matrix, from Wt−1 to Wt, through the delta learning rule (Widrow & Hoff, 1960) where the slow net-generated patterns, vt, φ(kt), and σ(βt), play the role of target, input, and learning rate of the learning rule respectively. We note that if we replace this Eq. 2 with a purely additive Hebbian learning rule and set the learning rate to 1, i.e., Wt = Wt−1 + vt ⊗ φ(kt), we fall back to the standard Linear Transformer (Katharopoulos et al., 2020). In fact, some previous works consider other learning rules in the context of linear Transformers/FWP, e.g., a gated update rule (Peng et al., 2021) and the Oja learning rule (Oja, 1982; Irie et al., 2022b). Our main focus is on the delta rule above, but we’ll also support this choice with an ablation study comparing it to the purely additive rule.\n\nAn example evolution of the fast weight matrix following delta rules of Eq. 2 is shown in Figure 1. As stated above, the visualisation itself does not provide useful information. The core idea of this work is to apply Eq. 2 to image generation, and conduct similar visualisation.\n\n2.2 GENERATIVE ADVERSARIAL NETWORKS\n\nThe framework of Generative Adversarial Networks (GANs; Goodfellow et al. (2014); Niemitalo (2010)) trains two neural networks G (as in generator) and D (as in discriminator), given some dataset R, by making them compete against each other. The GAN framework itself is a special instantiation (Schmidhuber, 2020) of the more general min-max game concept of Adversarial Artificial Curiosity (Schmidhuber, 1990; 1991b), and has applications across various modalities, e.g., to speech (Binkowski et al., 2020) and with limited success also to text (de Masson d’Autume et al., 2019), but here we focus on the standard image generation setting (Goodfellow et al., 2014). In what follows, let c, h, w, and d denote positive integers. Given an input vector z ∈ Rd whose elements are randomly sampled from the zero-mean unit-variance Gaussian distribution N (0, 1), G generates an image-like data G(z) ∈ Rc×h×w with a height of h, a width of w, and c channels (typically c = 3; a non-transparent colour image). D takes an image-like input X ∈ Rc×h×w and outputs a scalar\n\n3Sometimes we refer to the slow net as the “fast weight programmer” by generally referring to it as an NN whose output is a weight matrix, e.g., see the first sentence of the introduction. However, without the forward computation of the fast net, the slow net alone is not a general purpose sequence processing system.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nD(X) ∈ R between 0 and 1. The input X is either a sample from the real image dataset R or it is a “fake” sample generated by G. The training objective of D is to correctly classify these inputs as real (label ‘1’) or fake (label ‘0’), while G is trained to fool D, i.e., the error of D is the gain of G.4 The two networks are trained simultaneously with alternating parameter updates. The goal of this process is to obtain a G capable of generating “fake” images indistinguishable from the real ones. In practice, there are several ways of specifying the exact form of the objective function. We refer to the corresponding description (Sec. 3.3) and the experimental section for further details.\n\nOver the past decade, many papers have improved various aspects of GANs including the architecture (e.g., Karras et al. (2019; 2021)), loss function (e.g., Lim & Ye (2017)), data augmentation strategies (e.g., Zhao et al. (2020); Karras et al. (2020a), and even the evaluation metric (e.g., Heusel et al. (2017)) to achieve higher quality images at higher resolutions. In this work, our baselines are the state-of-the-art StyleGAN2 (Karras et al., 2020b;a) and a speed-optimised “light-weight” GAN (LightGAN; Liu et al. (2021)).\n\n3 FAST WEIGHT “PAINTERS” (FPAS)\n\nA Fast Weight Painter (FPA) is a generative model of images based on the weight generation process of outer product-based Fast Weight Programmers (FWP; Sec. 2.1; Eq. 2). Conceptually, such a model can be trained as a generator in the GAN framework (Sec. 2.2) or as a decoder in a Variational Auto-Encoder (VAE; Kingma & Welling (2014)). Here, we train it in the former setting that offers a rich set of accessible baselines. In the orginal FWPs, the slow net or the programmer’s goal is to generate useful programs/weights for the fast net to solve a given problem. In the proposed FPAs under the GAN framework, the “slow net” or the painter’s objective is to generate images that maximise the “fast net”/discriminator/critic’s prediction error.\n\nHere we describe the main architecture of our FPA (Sec. 3.1), its extension through the U-Net (Sec. 3.2), and other GAN specifications (Sec. 3.3).\n\n3.1 MAIN ARCHITECTURE\n\nLike a typical generator in the GAN framework, an FPA is an NN that transforms a randomly-sampled latent noise vector into an image. Its general idea is to use a sequence processing NN to decode the input vector for a fixed number of steps, where in each step, we generate a key/value vector pair and a learning rate that are used in a synaptic learning rule to generate a rank-one update to the currently generated image (starting from 0). The final image thus corresponds to the sum of these update terms. The number of decoding (or painting) steps is a hyper-parameter of the model or the training setup.\n\nIn what follows, let T , c, dkey, dvalue, dlatent, din, dhidden, denote positive integers. Here we first provide an abstract overview of the building blocks of the FPA, followed by specific descriptions of each block. Given a random input vector z ∈ Rdlatent , and a number of painting steps T , an FPA generates an image W ∈ Rc×dvalue×dkey with c channels, a height of dvalue, and a width of dkey, through the following sequence of operations:\n\n(x1, ..., xT ) = InputGenerator(z) (h1, ..., hT ) = SequenceProcessor(x1, ..., xT )\n\nWt = UpdateNet(Wt−1, ht) for t ∈ {1, ..., T } W = WT\n\n(4) (5)\n\n(6)\n\n(7)\n\nwhere for t ∈ {1, ..., T }, xt ∈ Rdin , ht ∈ Rdhidden, and Wt ∈ Rc×dvalue×dkey with W0 = 0.\n\nInputGenerator, SequenceProcessor, and UpdateNet denote the abstract blocks of operations described as follows:\n\nInput Generator. As its name indicates, the role of InputGenerator is to transform the latent vector z ∈ Rdlatent to a sequence of input vectors (x1, ..., xT ) with xt ∈ Rdin for t ∈ {1, ..., T }, for the subsequent sequence processing NN, SequenceProcessor. In practice, we consider two variants.\n\n4Despite the “adversarial” nature of this description, when G is trained by gradient descent, it is nothing but the gradient feedback of D through D(G(z)) that continually improves G. In this view, D is “collaborative”.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: An illustration of the Fast Weight Painter architecture. The example is taken from the last painting step of our best FPA trained on the AFHQ-Wild dataset.\n\nIn the first variant (v1), the input generator does not do anything: it feeds the same latent vector to SequenceProcessor at every step, i.e., xt = z for all t with din = dlatent. In the second variant (v2), InputGenerator is an NN that maps from an input vector of dimension dlatent to an output vector of size T ∗ din. The latter is split into T vectors xt (t ∈ {1, ..., T }) of size din each. An obvious advantage of the first approach is that it scales independently of T , and it may also be rolled out for an arbitrary number of steps. In most datasets, however, we found the second variant to perform better, yielding more stable GAN training.\n\nSequence Processor. SequenceProcessor further processes the input vector sequence produced by InputGenerator. In our preliminary experiments, we found that auto-regressive processing and in particular RNNs are a good choice for this component (e.g., we did not manage to train any models successfully when the standard Transformer was used instead). We use a multi-layer long short-term memory (LSTM; Hochreiter & Schmidhuber (1997)) RNN in all of our models. We also allocate a separate NN that maps the latent vector z to the initial hidden states of each RNN layer (omitted for clarity in Figure 2).\n\nImage Update Network. The actual painting finally emerges in the image update network, UpdateNet. The role of UpdateNet is to apply modifications to the current image Wt−1 (starting from W0 = 0) and obtain a new image Wt, through an application of the delta learning rule. At each time step t, the input ht ∈ Rdhidden is projected to key kt ∈ Rc∗dkey , value vt ∈ Rc∗dvalue, and learning rate βt ∈ Rc vectors by using a learnable weight matrix Wslow ∈ Rc∗(dkey+dvalue+1)×dhidden\n\n[kt, vt, βt] = Wslowht\n\n(8)\n\nSimilarly to multi-head attention in Transformers (Vaswani et al., 2017), the generated vectors are split into c sub-vectors, one for each channel, i.e., for each painting step t ∈ {1, ..., T }, we have [k1 t ] = vt for values, and [β1 t ] = βt for learning rates, where ki\n\nt , ..., vc t ∈ R for i ∈ {1, ..., c} denoting the channel index.\n\nt ] = kt for keys, [v1 t ∈ Rdvalue, and βi\n\nt , ..., kc t ∈ Rdkey , vi\n\nt , ..., βc\n\nFinally, for each step t ∈ {1, ..., T } and for each channel index i ∈ {1, ..., c}, the corresponding image channel W i\n\nt ∈ Rdvalue×dkey is updated through the delta update rule (same as in Eq. 2):\n\nW i\n\nt = W i\n\nt−1 + σ(βi\n\nt)(vi\n\nt − W i\n\nt−1φ(ki\n\nt)) ⊗ φ(ki t)\n\n(9)\n\nHere each channel of the image is effectively treated as a “weight matrix,” and “painting” is based on synaptic learning rules. The output image is finally obtained from the final step t = T as W = T ] ∈ Rc×dvalue×dkey (to which we apply tanh). We’ll visualise the iterative generation [W 1 process of Eq. 9 in the experimental section.\n\nT , ..., W c\n\nAn overview of this architecture is depicted in Figure 2.\n\n5\n\nInputGeneratorLatentVectorSequenceProcessorKeyValueLearningrateCurrentImageRank-1UpdateImageUpdate NetNewImageDeltaRulePublished as a conference paper at ICLR 2023\n\n3.2 OPTIONAL FINAL U-NET REFINEMENT STEP\n\nExperimentally, we find the process described above alone can generate reasonably fine looking images without any explicit inductive bias for images. However, the resulting evaluation scores are worse than those of the baseline methods based on convolutional architectures. This motivates us to further investigate how the quality of images generated by an FPA can be improved by an additional convolutional component. For this purpose, we use the image-to-image transforming U-Net architecture (Ronneberger et al., 2015; Salimans et al., 2017), the core architecture (Ho et al., 2020; Song et al., 2021; Dhariwal & Nichol, 2021) of the now popular denoising diffusion models (SohlDickstein et al., 2015). We apply the U-Net to the output of the FPA, i.e., after Eq. 7, it transforms the image W = WT ∈ Rc×dvalue×dkey into another image of the same size W ′ ∈ Rc×dvalue×dkey :\n\nW ′ = UNet(WT )\n\n(10)\n\nFrom the U-Net’s perspective, the output of the FPA is a “noisy” image. Its operation can be viewed as a one-step “denoising.” This process is depicted in Figure 3. In practice, we append this U-Net to the output of a pre-trained FPA, and train the resulting model in the GAN framework, while only updating parameters of the U-Net. We also discuss end-to-end training in Appendix C.1. However, in the main setting, all our U-Net models are trained with a pre-trained frozen FPA. In the experimental section, we’ll show that such a U-Net can effectively improve the quality of FPA-generated images.\n\nFigure 3: An illustration of the extra U-Net-based refinement. The example is generated by our best FPA/U-Net trained on the AFHQ-Wild dataset.\n\n3.3 DISCRIMINATOR ARCHITECTURE AND OTHER SPECIFICATIONS\n\nWe adopt the training configurations and the discriminator architecture of the LightGAN (Liu et al., 2021), i.e., we replace its generator by our FPA. In our preliminary study, we also tried the StyleGAN2 setting, however, we did not observe any obvious benefit. In the LightGAN framework, in addition to the main objective function of the hinge loss (Lim & Ye, 2017; Tran et al., 2017), the discriminator is regularised with an auxiliary image reconstruction loss at two different resolutions (for that two “simple” image decoders are added to the discriminator). For details, we refer to Appendix B.3.\n\n4 EXPERIMENTS\n\n4.1 BENCHMARKING ON THE STANDARD DATASETS\n\nWe start with evaluating how well the proposed FPA performs as an image generator without explicit inductive bias for images. For this, we train our model on six datasets: CelebA (Liu et al., 2015), LSUN Church (Yu et al., 2015), Animal Faces HQ (AFHQ) Cat/Dog/Wild (Choi et al., 2020), and MetFaces (Karras et al., 2020a), all at the resolution of 64x64. No data augmentation is used as we want to keep the comparison as simple as possible. For details of the datasets, we refer to Appendix B.1. As a first set of experiments, we set the number of generation steps for FPAs to T = 64 (we’ll vary this number in Sec. 4.3); such that the output images can be of full rank. We provide all hyper-parameters in Appendix B.2 and discuss training/generation speed in Appendix C.2. Following the standard practice, we compute the FID using 50 K sampled images and all real images (for further discussion on the FID computation, we refer to Appendix B.4). Table 1 shows the FID scores. We first observe that the state-of-the-art StyleGAN2 outperforms our FPA by a large margin. The performance gap is the smallest in the case of the small dataset, MetFaces, while for larger datasets, CelebA and LSUN-Church, the gap is large. At the same time, the reasonable FID values show that the FPA is quite successful. Qualitatively, we observe that the FPA can produce images with a respectable quality, even though we also observe that the StyleGAN2 tends to generate fine-looking\n\n6\n\nU-NetFast Weight Painter Published as a conference paper at ICLR 2023\n\nTable 1: FID scores. The resolution is 64x64 for all datasets. No data augmentation is used. The StyleGAN2 models are trained using the official public implementation.\n\nAFHQ\n\nLSUN\n\nModel\n\nCelebA MetFaces\n\nCat Dog Wild Church\n\nStyleGAN2 LightGAN\n\nFPA\n\n+ U-Net\n\n1.7 3.4\n\n18.3 3.7\n\n17.2 26.4\n\n36.3 24.5\n\n7.5 7.9\n\n17.1 6.8\n\n11.7 16.8\n\n45.3 19.9\n\n5.2 10.2\n\n20.2 9.4\n\n2.8 5.1\n\n42.8 5.2\n\nimages more consistently, and with a higher diversity. Figure 4 displays the curated output images generated by various models. By looking at these examples, it is hard to guess that they are generated as sums of outer products through sequences of synaptic learning rules. The visualisation confirms the FPA’s respectable performance.\n\nIn Table 1, we also observe that the extra convolutional U-Net (Sec. 3.2) largely improves the quality of the images generated by the FPA. Its performance is comparable to that of the LightGAN baseline across all datasets. For further discussion of the UNet’s effect, we refer to Appendix C.1.\n\nFigure 4: Curated image samples generated by different models specified above the images. Four images are shown for each dataset, from left to right: CelebA, MetFaces, AFHQ Cat, Dog, Wild, and LSUN Church. All at the resolution of 64x64.\n\n4.2 VISUALISING THE ITERATIVE GENERATION PROCESS\n\nHow are images like those shown in Figure 4 generated iteratively as sums of outer products by the FPA? Here we visualise this iterative generation process. Generally speaking, we found almost all examples interesting to visualise. We show a first set of examples in Figure 5. As we noted above, all FPAs in Table 1/Figure 4 use T = 64 painting steps. The first thing we observe is that for many steps, the “key” is almost one-hot (which is encouraged by the softmax), i.e., the part of the image is generated almost column-wise. For other parts, such as generation/refinement of the background (e.g., steps 57-64 in AFHQ-Wild or steps 1-16 in MetFaces), rank-1 update covers a large region of the image. Generally we can recognise the “intended action” of each learning rule step on the image (e.g., adding the extra colour in the background in steps 59-64 in LSUN Church, or drawing a draft of the animal face in steps 7-34 in AFHQ-Wild). We discuss many more examples in Appendix A.\n\n7\n\nStyleGAN2Fast Weight PainterFast Weight Painter + U-NetPublished as a conference paper at ICLR 2023\n\nFigure 5: Example painting steps for the FPAs of Table 1. All images have the resolution of 64x64. In each example, the generation steps are shown from left to right. The numbers below images indicate the step. Each row has two mini-rows showing two sequences of images. The top mini-row shows the rank-1 update generated at the corresponding step, while the bottom mini-row shows the cumulative sum thereof, i.e., the currently generated image (Eq. 9). For visualisation, the image at each step is normalised by the norm of the final image, and we apply tanh. We also note that the colour scaling is specific to each plot (in consequence, we observe some cases like the one we see in step 31 of LSUN-Church above where the effect of the rank-1 update is not visible once added to the image).\n\n4.3 ABLATION STUDIES\n\nTable 2: FID scores of FPAs for various numbers of painting steps. The resolution is 64x64 for all datasets. The numbers for T = 64 are copied from Table 1.\n\nVarying Number of Painting Steps. In all examples above, we train FPAs with a number of painting steps T = 64 such that the 64x64 output can be of full rank. Here we study FPAs with reduced numbers of steps. The task should remain feasible, as natural images typically keep looking good (at least to human eyes, to some extent) under low-rank approximations (Andrews & Patterson, 1976). We select the model configuration that achieves the best FID with T = 64, and train the same model with fewer steps {8, 16, 32}. Table 2 shows the FIDs. Globally, we find that fewer steps tend to greatly hurt performance. An exception is MetFaces (and AFHQ-Cat to some extent) where the degradation is much smaller. Figure 6 shows examples of 16-step generation of 64x64-resolution images for these two datasets, where we observe that FPAs find and exploit symmetries (for AFHQ-Cat) and other regularities (for MetFaces) as shortcuts for low rank/complexity image generation.\n\nSteps T MetFaces\n\n69.5 165.8 220.1\n\n31.9 220.5 215.2\n\n25.9 86.6 164.4\n\n43.6 53.8 80.1\n\nDog Wild\n\n32 16 8\n\nAFHQ\n\n17.1\n\n20.2\n\n36.3\n\n45.3\n\nCat\n\n64\n\nChoice of Learning Rules. As mentioned in Sec. 2.1, the delta rule is not the only way of parameterising the weight update (Eq. 9). However, we experimentally observe that both the purely additive rule and the Oja rule underperform the delta rule. This is in line with previous works on\n\n8\n\nAFHQ-WildLSUN-Church116824321168243233484056643348405664MetFaces116824323348406456Published as a conference paper at ICLR 2023\n\nFWPs (see introduction). The best FID we obtain on the CelebA using the purely additive rules is above 80, much worse than 18.3 obtained by the delta rule (Table 1). With the Oja rule, we did not manage to train any reasonable model on CelebA in our settings.\n\nFigure 6: Examples of 16-step generation of 64x64 images by the FPA.\n\n5 DISCUSSION\n\nLimitations. We have shown how the concept of FWPs can be applied to image generation, to visualise sequences of NN-controlled learning rules that produce natural images. While this contributes to the study of learning rules in general, we note that our visualisations are tailored to the image generation task: mapping a random vector to a sequence of images, where only the final image (their sum) is “evaluated.” In many FWP use cases, however, the generated WMs are queried/used at every time step to solve some sequence processing task. What the learning rules actually do in such scenarios remains opaque. Also, from the perspective of image generation methods, there are many aspects that we do not investigate here. For example, we directly use the convolutional LightGAN discriminator. However, since the FPA is non-convolutional, there may be alternative architectures with better feedback/gradient properties for training FPAs. Also, our experiments are limited to images with a resolution of 64x64. Increasing the resolution is typically not straightforward (Karras et al., 2018) but outside the scope of this work.\n\nDiffusion models. We use the GAN framework (Sec. 2.2) to train our FPA, and mention the alternative of using VAEs. At first glance, it also seems attractive to use rank-1 noise as an efficient alternative to the expensive U-Net of diffusion models (Sohl-Dickstein et al., 2015). Unfortunately, unlike in the standard Gaussian case (Feller, 1949), if the forward process is based on rank-1 noises (e.g., obtained as outer products of two random Gaussian noise vectors), we have no guarantee that the reversal process can be parameterised in the same way using an outer product. Nevertheless, generally speaking, an exchange of ideas between the fields of image and NN weight generation/processing may stimulate both research domains. An example is the use of discrete cosine transform (DCT) to parameterise a WM of an NN (e.g., Koutn ́ık et al. (2010a;b); van Steenkiste et al. (2016); Irie & Schmidhuber (2021)).\n\nOther Perspectives. We saw that the same generic computational mechanism can be used for both fast weight programming and image generation (painting). From a cognitive science perspective, it may be interesting to compare painting and programming as sequential processes.\n\n6 CONCLUSION\n\nWe apply the NN weight generation principles of Fast Weight Programmers (FWPs) to the task of image generation. The resulting Fast Weight Painters (FPAs) effectively learn to generate weight matrices looking like natural images, through the execution of sequences of NN-controlled learning rules applied to self-invented learning patterns. This allows us to visualise the iterative FWP process in six different image domains interpretable by humans. While this method is certainly not the best approach for image generation, our results clearly demonstrate/illustrate/visualise how an NN can learn to control sequences of synaptic weight learning rules in a goal-directed way to generate complex and meaningful weight patterns.\n\n9\n\nAFHQ-CatMetFaces116811681168116811681168Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nWe thank R ́obert Csord ́as for helpful suggestions on Figure 2. This research was partially funded by ERC Advanced grant no: 742870, project AlgoRNN, and by Swiss National Science Foundation grant no: 200021 192356, project NEUSYM. We are thankful for hardware donations from NVIDIA and IBM. The resources used for this work were partially provided by Swiss National Supercomputing Centre (CSCS) project s1145 and s1154.\n\nREFERENCES\n\nMark A. Aizerman, Emmanuil M. Braverman, and Lev I. Rozonoer. Theoretical foundations of potential function method in pattern recognition. Automation and Remote Control, 25(6):917–936, 1964.\n\nHarry C. Andrews and Claude L. Patterson. Singular value decompositions and digital image processing. IEEE Transactions on Acoustics, Speech, and Signal Processing, 24(1):26–53, 1976.\n\nMikolaj Binkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C. Cobo, and Karen Simonyan. High fidelity speech synthesis with adversarial networks. In Int. Conf. on Learning Representations (ICLR), Virtual only, April 2020.\n\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. StarGAN v2: Diverse image synthesis for multiple domains. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 8185–8194, Virtual only, June 2020.\n\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In Int. Conf. on Learning Representations (ICLR), Virtual only, 2021.\n\nCyprien de Masson d’Autume, Shakir Mohamed, Mihaela Rosca, and Jack W. Rae. Training language GANs from scratch. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 4302–4313, Vancouver, Canada, December 2019.\n\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 8780–8794, Virtual only, December 2021.\n\nWilliam Feller. On the theory of stochastic processes, with particular reference to applications. In Proc. Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pp. 403–433, 1949.\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 2672–2680, Montr ́eal, Canada, December 2014.\n\nDonald Olding Hebb. The organization of behavior; a neuropsycholocigal theory. A Wiley Book in\n\nClinical Psychology, 62:78, 1949.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 6626–6637, Long Beach, CA, USA, December 2017.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only, December 2020.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nKazuki Irie and J ̈urgen Schmidhuber. Training and generating neural networks in compressed weight\n\nspace. In ICLR Neural Compression Workshop, Virtual only, May 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nKazuki Irie, Imanol Schlag, R ́obert Csord ́as, and J ̈urgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only, December 2021.\n\nKazuki Irie, R ́obert Csord ́as, and J ̈urgen Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In Proc. Int. Conf. on Machine Learning (ICML), Baltimore, MD, USA, July 2022a.\n\nKazuki Irie, Francesco Faccio, and J ̈urgen Schmidhuber. Neural differential equations for learning to program neural nets through continuous learning rules. In Proc. Advances in Neural Information Processing Systems (NeurIPS), New Orleans, LA, USA, December 2022b.\n\nKazuki Irie, Imanol Schlag, R ́obert Csord ́as, and J ̈urgen Schmidhuber. A modern self-referential weight matrix that learns to modify itself. In Proc. Int. Conf. on Machine Learning (ICML), pp. 9660–9677, Baltimore, MA, USA, July 2022c.\n\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for In Int. Conf. on Learning Representations (ICLR),\n\nimproved quality, stability, and variation. Vancouver, Canada, April 2018.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 4401–4410, Long Beach, CA, USA, June 2019.\n\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training In Proc. Advances in Neural Information\n\ngenerative adversarial networks with limited data. Processing Systems (NeurIPS), Virtual only, December 2020a.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, June 2020b.\n\nTero Karras, Miika Aittala, Samuli Laine, Erik H ̈ark ̈onen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 852–863, Virtual only, December 2021.\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ̧ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Proc. Int. Conf. on Machine Learning (ICML), Virtual only, July 2020.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Int. Conf. on Learning\n\nRepresentations (ICLR), Banff, Canada, April 2014.\n\nJan Koutn ́ık, Faustino Gomez, and J ̈urgen Schmidhuber. Evolving neural networks in compressed weight space. In Proc. Conference on Genetic and Evolutionary Computation (GECCO), pp. 619–626, 2010a.\n\nJan Koutn ́ık, Faustino Gomez, and J ̈urgen Schmidhuber. Searching for minimal neural networks in fourier space. In Proc. Conf. on Artificial General Intelligence, Lugano, Switzerland, March 2010b.\n\nJae Hyun Lim and Jong Chul Ye. Geometric GAN. Preprint arXiv:1705.02894, 2017.\n\nBingchen Liu, Yizhe Zhu, Kunpeng Song, and Ahmed Elgammal. Towards faster and stabilized GAN training for high-fidelity few-shot image synthesis. In Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proc. IEEE Int. Conf. on Computer Vision (ICCV), pp. 3730–3738, Santiago, Chile, December 2015.\n\nOlli Niemitalo. A method for training artificial neural networks to generate missing data within a variable context. https://web.archive.org/web/20120312111546/http://yehar.com:80/blog/?p=167, Internet Archive, 2010.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nErkki Oja. Simplified neuron model as a principal component analyzer. Journal of mathematical\n\nbiology, 15(3):267–273, 1982.\n\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in GAN evaluation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 11410–11420, New Orleans, LA, USA, June 2022.\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. In Int. Conf. on Learning Representations (ICLR), Virtual only, 2021.\n\nSachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Int. Conf. on\n\nLearning Representations (ICLR), Toulon, France, April 2017.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical In Proc. Int. Conf. Medical Image Computing and Computer-Assisted\n\nimage segmentation. Intervention - MICCAI, pp. 234–241, Munich, Germany, October 2015.\n\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving the pixelCNN with discretized logistic mixture likelihood and other modifications. In Int. Conf. on Learning Representations (ICLR), Toulon, France, April 2017.\n\nImanol Schlag, Kazuki Irie, and J ̈urgen Schmidhuber. Linear Transformers are secretly fast weight\n\nprogrammers. In Proc. Int. Conf. on Machine Learning (ICML), Virtual only, July 2021a.\n\nImanol Schlag, Tsendsuren Munkhdalai, and J ̈urgen Schmidhuber. Learning associative inference using fast weight memory. In Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021b.\n\nJ ̈urgen Schmidhuber. Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments. Institut f ̈ur Informatik, Technische Universit ̈at M ̈unchen. Technical Report FKI-126, 90, 1990.\n\nJ ̈urgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Technical Report FKI-147-91, Institut f ̈ur Informatik, Technische Universit ̈at M ̈unchen, March 1991a.\n\nJ ̈urgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. Int. Conf. on simulation of adaptive behavior: From animals to animats, pp. 222–227, 1991b.\n\nJ ̈urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent\n\nnetworks. Neural Computation, 4(1):131–139, 1992.\n\nJ ̈urgen Schmidhuber. Generative adversarial networks are special cases of artificial curiosity (1990) and also closely related to predictability minimization (1991). Neural Networks, 127:58–66, 2020.\n\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proc. Int. Conf. on Machine Learning (ICML), pp. 2256–2265, Lille, France, July 2015.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021.\n\nDustin Tran, Rajesh Ranganath, and David M. Blei. Hierarchical implicit models and likelihood-free variational inference. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 5523–5533, Long Beach, CA, USA, December 2017.\n\nSjoerd van Steenkiste, Jan Koutn ́ık, Kurt Driessens, and J ̈urgen Schmidhuber. A wavelet-based encoding for neuroevolution. In Proc. Genetic and Evolutionary Computation Conference (GECCO), pp. 517–524, Denver, CO, USA, July 2016.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 5998–6008, Long Beach, CA, USA, December 2017.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nOriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 3630–3638, Barcelona, Spain, December 2016.\n\nBernard Widrow and Marcian E Hoff. Adaptive switching circuits. In Proc. IRE WESCON Convention\n\nRecord, pp. 96–104, Los Angeles, CA, USA, August 1960.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. Preprint arXiv:1506.03365, 2015.\n\nShengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han. Differentiable augmentation for dataefficient GAN training. In Proc. Advances in Neural Information Processing Systems (NeurIPS), Virtual only, December 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA MORE VISUALISATION EXAMPLES\n\nAs a continuation of Sec. 4.2, we provide further visualisations to illustrate the image generation process of FPAs.\n\nFigure 7 shows some interesting effects we observe on CelebA. In Example 1 (top), from step 1 to 16, the image is column-wise generated from the left. Then from step 18, the generation of the right part of the face starts from the right. Around step 40 and 46, we see that the separately generated left and right parts of the face do not fit each other. But then the FPA fixes this in three steps from step 46 to 48 that “harmonise” the image. Example 2 (bottom) shows another similar example.\n\nFigure 8 illustrates another typical behaviour we observe on both CelebA and AFHQ-Dog. In the CelebA example, we can see that the face is already generated around step 48. The rest of the steps are used to add extra hair. Similarly in the AFHQ-Dog example, step-48 image is already a fine-looking dog. An extra body is painted (rather unsuccessfully) from the right in the remaining steps.\n\nFigure 7: Example painting steps of the FPAs of Table 1 trained on CelebA at the resolution of 64x64. For details about the visualisation process, we refer to the caption of Figure 5.\n\nFigure 8: Example painting steps of the FPAs of Table 1 trained on CelebA and AFHQ-Dog at the resolution of 64x64. For details about the visualisation process, we refer to the caption of Figure 5 ( in particular, in step 25 of CelebA we observe an effect that is similar to step 31 of LSUN Church in Figure 5).\n\n14\n\n3348405664CelebA, example2116824323348405664CelebA, example 1116824321168243233484056643348405664CelebAAFHQ-Dog11682432Published as a conference paper at ICLR 2023\n\nB EXPERIMENTAL DETAILS\n\nB.1 DATASETS\n\nWe use six standard benchmark datasets for image generation: CelebA (Liu et al., 2015), LSUN Church (Yu et al., 2015), Animal Faces HQ (AFHQ) Cat/Dog/Wild (Choi et al., 2020), and MetFaces (Karras et al., 2020a). The number of images are 203 K for CelebA, 126 K for LSUN Church, 5 K for each subsets of AFHQ, and 1336 for the smallest MetFaces. For further information about the datasets, we refer to the original papers.\n\nB.2 HYPER-PARAMETERS\n\nWe conduct FPA tuning in terms of both hyper-parameters and model variations. Regarding the latter, we use three specifications: InputGenerator version v1 vs. v2 (see their descriptions in Sec. 3.1), input generator with or without tanh, and with or without a linear layer that maps the latent vector to the RNN initial states (otherwise the initial states are zeros). We also train models with/without applying tanh to the generated image at the FPA output. Using the notations introduced in Sec. 3.1, the hyper-parameter search is conducted across the following ranges: dlatent ∈ {128, 256, 512}, dhidden ∈ {1024, 2048, 4096}, and the number of RNN layers L ∈ {1, 2}. When InputGenerator is v2, we have din ∈ {8, 16, 32} and furthermore, we add an extra linear layer between the input generator and the RNN with a dimension d′ in ∈ {128, 256, 512}. The batch size and learning rate are fixed to 20 and 2e−4 respectively. We compute the FID score every 5 K training steps to monitor the performance. We take the hyper-parameters achieving the best FID as the best configuration. Table 3 summarises the corresponding results. For MetFaces, a more parameter-efficient v2 variant (with 20 M parameters) achieves a FID of 40.5 that is close to 36.3 (Table 1) achieved by the best model. However, as the parameter count does not matter for our study, we strictly take the best model.\n\nTable 3: Hyper-parameters of FPAs used for different datasets.\n\nHyper-Parameter\n\nCelebA MetFaces\n\nInputGenerator Input Gen. tanh Latent to RNN init\n\ndlatent din d′ in dhidden L\n\nGen. Param. Count (M)\n\nTotal Param Count (M)\n\nv2 No No\n\n128 8\n128 1024 2\n\n14\n\n26\n\nv1 -\nYes\n\n512 -\n- 4096 2\n\n220\n\n232\n\nAFHQ\n\nLSUN\n\nDog Wild Church\n\nv2 No No\n\n256 8\n128 1024 2\n\n14\n\n26\n\nv2 No No\n\n512 8\n128 1024 1\n\n6\n\n19\n\nv2 Yes Yes\n\n512 16 256 1024 2\n\n17\n\n29\n\nCat\n\nv2 No Yes\n\n256 8\n128 1024 1\n\n6\n\n18\n\nB.3 GAN DETAILS & IMPLEMENTATION\n\nAs stated in Sec. 3.3, our GAN setting is based on that of the LightGAN by Liu et al. (2021). For architectural details of the discriminator, we refer to the original paper. To be more specific, our code is based on the unofficial public LightGAN implementation https://github.com/ lucidrains/lightweight-gan. which is slightly different from the official implementation. In particular, the auxiliary reconstruction losses for the discriminator require 8x8 and 16x16 resolution images. In the original implementation, these are obtained directly from the intermediate layers of the generator, while in this unofficial implementation, they are obtained by scaling down the final output image of the generator by interpolation. The latter is the only compatible approach for FPAs since an FPA does not produce such intermediate images with small resolutions unlike the standard convolution based generators. For any further details, we refer to our public code. For the StyleGAN2\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nbaseline, we use the official implementation of StyleGAN3 (Karras et al., 2021) that also supports StyleGAN2: https://github.com/NVlabs/stylegan3.\n\nB.4 FID COMPUTATION\n\nWhile the Fr ́echet Inception Distance (FID; Heusel et al. (2017)) is a widely used metric for evaluating machine-generated images, it is sensitive to many details (Parmar et al., 2022). We also observe that it is crucial to use the same setting for all models consistently. We use the following procedure for all datasets and models (including the StyleGAN2 baseline). We store both the generated and resized real images in JPEG format, and use the pytorch-fid implementation of https: //github.com/mseitzer/pytorch-fid to compute the FID. While the usage of PNG is generally recommended by Parmar et al. (2022), since consistency is all we need for the purpose of our study, and JPEG is more convenient for frequently monitoring FID scores during training (as JPEG images are faster to store, and take less disk space), we opt for it here.\n\nNevertheless, in Table 4, we also report FID scores computed using the generated and resized real images saved in PNG format. These are computed using the models from Table 1, i.e., the best model checkpoint found based on JPEG-based FID scores during training. All numbers grow beyond those in Table 1 including those of the baselines, but the general trend does not change. The FID scores reported in all other tables are computed using the JPEG format.\n\nTable 4: FID scores using images in PNG format. The resolution is 64x64 for all datasets. No data augmentation is used.\n\nAFHQ\n\nLSUN\n\nModel\n\nCelebA MetFaces\n\nCat Dog Wild Church\n\nStyleGAN2 LightGAN\n\nFPA\n\n+ U-Net\n\n2.7 8.8\n\n33.6 6.9\n\n24.8 47.2\n\n73.7 43.1\n\n11.7 13.0\n\n23.4 9.8\n\n17.4 27.9\n\n71.6 30.5\n\n7.0 14.0\n\n28.6 12.6\n\n9.8 9.6\n\n64.7 8.5\n\nC EXTRA RESULTS & DISCUSSION\n\nC.1 DISCUSSION OF THE U-NET EXTENSION\n\nVisualising the U-Net’s Effect. In Sec. 4.1, we report the U-Net’s quality improvements of FPAgenerated images in terms of the FID metric. Here we further look into its effect on the actual images. Figure 9 displays some example images generated by FPA/U-Net models before and after the U-Net application. We observe that in general, the U-Net is successful at improving the quality of the FPA-generated images without completely ignoring the original images. However, we also observe examples where this is not the case, i.e., the U-Net generates almost completely new images as illustrated by the last row of MetFaces and AFHQ Dog examples.\n\nEnd-to-End Training. As described in Sec. 3.2, we train the FPA/U-Net model using pre-trained FPA parameters. We also tried end-to-end training from scratch, but observed that the U-Net starts learning before the FPA can generate fine-looking images. As a result, the resulting U-Net ignores the FPA output, hence the FPA does not receive any useful gradients for learning to generate meaningful images, and the FPA remains a noise generator. We also tried to train it by providing both the FPA and U-Net outputs to the discriminator (and by stopping the gradient flow from the U-Net to the FPA). This alleviates the problem of the FPA remaining a noise generator, but is still not good enough to make the U-Net properly act as a denoising/refinement component of the FPA.\n\nChoice of the Pre-Trained FPA. In general, we take the best performing standalone FPA as the pre-trained model to train a FPA/U-Net model (Sec. 3.2). In some cases, however, we find that the best standalone FPA model does not yield the best FPA/U-Net model. This is the case for AFHQ-Cat\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nand Dog as illustrated in Table 5. The FPA/U-Net model based on the best pre-trained FPA with an input generator of type v1 (Sec. 3.1) yields a slightly better FID than the one based on the best v2 variant, even though the trend is the opposite for the standalone FPA models. In Table 1 of the main text, we report the overall best FIDs.\n\nOur U-Net implementation is based on the diffusion model implementation of https://github. com/lucidrains/denoising-diffusion-pytorch. We use three down/up-sampling ResNet blocks with a total of 8 M parameters.\n\nFigure 9: Example images generated by the FPA/U-Net before (left) and after (right) the U-Net.\n\nTable 5: FID scores for the FPA/U-Net with various pre-trained FPA models. The resolution is 64x64 for all datasets. No data augmentation is used. v1/v2 indicates the type of the input generator described in Sec. 3.1.\n\nAFHQ-Cat AFHQ-Dog\n\nModel\n\nFPA\n\n+ U-Net\n\nv1\n\n30.8 6.8\n\nv2\n\n17.1 7.8\n\nv1\n\n49.3 19.9\n\nv2\n\n45.3 22.9\n\nC.2 TRAINING AND GENERATION SPEED\n\nWhile it is difficult to compare speed across different implementations, the generation speed of StyleGAN2 and the FPA are similar: about 55 images are generated every second for a batch size of one on a V100 GPU, while the LightGAN can generate 160 images per second in the same setting. The training/convergence speed of the FPA is similar to or better than that of the LightGAN on the relatively large datasets. For example, our FPA converges after about 150 K training steps on CelebA (vs. 285 K steps for the LightGAN), and 70 K steps on LSUN Church (similarly to the LightGAN) with a batch size of 20. The FPA/U-Net variant converges more slowly. For example on LSUN, it continues improving until 380 K steps in the same setting. On the small datasets, e.g., on AFHQ-Cat, the LightGAN converges faster (about 30 K steps) than the FPA (which continues improving until about 280 K steps) in the same setting. Any training run can be completed within one to three days on a single V100 GPU.\n\n17\n\nLSUN ChurchAFHQ WildMetFacesAFHQ Dog",
    "reference": "# Summary Of The Paper\n\nThe work visualizes image generation as sequential low-rank approximations to the weight matrices in a FWP(Fast Weight Programming) setup. The motivation is to not generate high fidelity images but to rather enable human interpretable visualizations of the weight updates in a Neural Network (NN). The authors have shown human comprehensible weight update sequences in six popular datasets.\n\n# Strength And Weaknesses\n\n*Novelty and Originality*\n\nThe work is interesting in it being one of the few works that draw similarities between images and weight matrices that are generated by another neural net. The recent of this type being the class of Implicit Neural Representations (INR) [1 ]. The work also is a a slightly novel extension to the existing literature on FWP to image generation. \n\n*Methodology*\n\nThe authors claims that image generation is not the best of the strengths of FPA, leaving the visualization of weight updates as the central purpose of the work. Given the poor generation quality of FPA, my question is if the  performance vs interpretability/explainability tradeoff presented in the work is worth a shot. Can the method be adapted to exercise control over generations by manipulating the low-rank updates, ref [4] or restrict the low-rank updates to find discriminating components, ref [5], instead of a rank based updation approach. \n\nWhy shouldn't  one compare FPA with recurrent painting techniques like Draw [2].\n\nThe authors posit that FPA does not include any explicit inductive bias as noted in page 2, \"*we show that our\ngeneric models can generate images of respectable visual quality without any explicit inductive\nbias for image processing (e.g., no convolution is used).*\" But the training involves discriminator of LightGAN [3] that uses convolutional layers. \n\nFrom Table 2, the number of train steps are directly proportional to the generation quality. Is it due to having a softmax over the key as discussed in Section 4.2 that forces slow update rules. In my understanding, and the authors' note, \"*The first thing we observe is that for many steps, the “key” is almost one-hot (which is encouraged by the softmax), i.e., the part of the image is generated almost column-wise.*\" the key is crucial in determining the attention or the generative patterns for the next iteration. Have the authors explored on other activations for key matrices. \n\nCould the authors explain why there is more drastic effect of trains steps on the AFHQ dataset when compared to MetFaces dataset (as shown in Table 2). Have they explored the possible effects of the rank of an image on the generation process. \n\nReferences:\n\n1. Skorokhodov, Ivan, Savva Ignatyev, and Mohamed Elhoseiny. \"Adversarial generation of continuous images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n2. Gregor, Karol, et al. \"Draw: A recurrent neural network for image generation.\" International conference on machine learning. PMLR, 2015.\n3. Liu, Bingchen, et al. \"Towards faster and stabilized gan training for high-fidelity few-shot image synthesis.\" International Conference on Learning Representations. 2020.\n4. Zhu, Jiapeng, et al. \"Low-rank subspaces in gans.\" Advances in Neural Information Processing Systems 34 (2021): 16648-16658.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n*Clarity*\n\nThe paper is mostly clear to read. But would appreciate a more vivid view of the FPA generator architecture and the role of a third NN to map z to initial hidden states of RNN layers of the slow NN.\n\n*Reproducibility*\n\nThe work builds upon the existing architecture and has given links to the versions of the implementations used in the work. The work should be fairly reproducible given the code for FPA generator is released\n\n*Novelty and Originality* (repeated from above)\n\nThe work is interesting in it being one of the few works that draw similarities between images and weight matrices that are generated by another neural net. The recent of this type being the class of Implicit Neural Representations (INR) [1 ]. The work also is a a slightly novel extension to the existing literature on FWP to image generation.\n\n# Summary Of The Review\n\nOverall, I appreciate the novelty in equating weight updation to image generation in FWP framework. The paper is clearly written and mostly easy to follow. My main concerns are with the limitations of the proposed approach in terms of its scalability w.r.t image size and to its applicability beyond the visualization of weight updates as discussed under the first point in the main review under methodology.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "NEURAL NETWORK APPROXIMATION OF LIPSCHITZ FUNCTIONS IN HIGH DIMENSIONS WITH APPLICATIONS TO INVERSE PROBLEMS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe remarkable successes of neural networks in a huge variety of inverse problems have fueled their adoption in disciplines ranging from medical imaging to seismic analysis over the past decade. However, the high dimensionality of such inverse problems has simultaneously left current theory, which predicts that networks should scale exponentially in the dimension of the problem, unable to explain why the seemingly small networks used in these settings work as well as they do in practice. To reduce this gap between theory and practice, a general method for bounding the complexity required for a neural network to approximate a Lipschitz function on a high-dimensional set with a low-complexity structure is provided herein. The approach is based on the observation that the existence of a linear Johnson-Lindenstrauss embedding A ∈ Rd×D of a given high-dimensional set S ⊂ RD into a low dimensional cube [−M, M ]d implies that for any Lipschitz function f : S → Rp, there exists a Lipschitz function g : [−M, M ]d → Rp such that g(Ax) = f (x) for all x ∈ S. Hence, if one has a neural network which approximates g : [−M, M ]d → Rp, then a layer can be added which implements the JL embedding A to obtain a neural network which approximates f : S → Rp. By pairing JL embedding results along with results on approximation of Lipschitz functions by neural networks, one then obtains results which bound the complexity required for a neural network to approximate Lipschitz functions on high dimensional sets. The end result is a general theoretical framework which can then be used to better explain the observed empirical successes of smaller networks in a wider variety of inverse problems than current theory allows.\n\n1\n\nINTRODUCTION\n\nAt present various network architectures (NN, CNN, ResNet) achieve state-of-the-art performance in a broad range of inverse problems, including matrix completion (Zheng et al., 2016; Monti et al., 2017; Dziugaite & Roy, 2015; He et al., 2017) image-deconvolution (Xu et al., 2014; Kupyn et al., 2018), low-dose CT-reconstitution (Nah et al., 2017), electric and magnetic inverse Problems (Coccorese et al., 1994) (seismic analysis, electromagnetic scattering). However, since these problems are very high dimensional, classical universal approximation theory for such networks provides very pessimistic estimates of the network sizes required to learn such inverse maps (i.e., as being much larger than what standard computers can store, much less train). As a result, a gap still exists between the widely observed successes of networks in practice and the network size bounds provided by current theory in many inverse problem applications. The purpose of this paper is to provide a refined bound on the size of networks in a wide range of such applications and to show that the network size is indeed affordable in many inverse problem settings. In particular, the bound developed herein depends on the model complexity of the domain of the forward map instead of the domain’s extrinsic input dimension, and therefore is much smaller in a wide variety of model settings.\n\nTo be more specific, recall in most inverse problems one aims to recover some signal x from its measurement y = F (x). Here y and x could both be high dimensional vectors, or even matrices and tensors, and F , which is called the forward map/operator, could either be linear or nonlinear with various regularity conditions depending on the application. In all cases, however, recovering x from y amounts to inverting F . In other words, one aims want to find the operator F −1, that sends every\n\n1\n\nmeasurement y back to the original signal x. Depending on the specific application of interest, there are various commonly considered forms of the forward map F . For example, F could be a linear map from high to low dimensions as in compressive sensing applications; F could be a convolution operator that computes the shifted local blurring of an image as in the image deblurring setting; F could be a mask that filters out the unobserved entries of the data as in the matrix completion application; or F could also be the source-to-solution map of a differential equation as in ODE/PDE based inverse problems.\n\nIn most of these applications, the inverse operator F −1 does not possess a closed-form expression. As a result, in order to approximate the inverse one commonly uses analytical approaches that involve solving, e.g., an optimization problem. Take the sparse recovery as an example. With the prior knowledge that the true signal x ∈ Rn is sparse, one can recover it from the under-determined measurements Rm (cid:51) y = Ax with m < n) by solving the optimization problem\n\nˆx = arg min\n\nz\n\n(cid:107)z(cid:107)0, Az = y\n\nThe inverse of the linear measurement map F (x) = y = Ax when restricted to the low-complexity domain of sparse vectors has an inverse, F −1(y), that is then the minimizer ˆx above.\n\nNote that traditional optimization-based approaches could be extremely slow for large-scale problems (e.g., for n large above). Alternatively, we can approximate the inverse operator by a neural network instead. Amortizing the initial cost of an expensive training stage, the network can later achieve unprecedented speed over time at the test stage leading to better total efficiency over its lifetime. To realize this goal, however, we need to first find a neural network architecture fθ, and train it to approximate F −1, so that the approximation error maxy (cid:107)fθ(y) − F −1(y)(cid:107) = (cid:107)fθ(y) − x(cid:107) is small. The purpose of this paper is to provide a unified way to give a meaningful estimation of the size of the network that one can use to set up the network in situations where the domain of F is low-complexity as is the case in, e.g., compressive sensing, low-rank matrix completion, deblurring with low-dimensional signal assumptions, etc..\n\n2 RELATED WORK\n\nThe expressive power of neural networks is important in applications as a means of both guiding network architecture design choices, as well as for providing confidence that good network solutions exist in general situations. As a result, numerous results about the approximation power has been established in recent years (Zhou, 2020; Petersen & Voigtlaender, 2020; Yarotsky, 2022; 2018; Lin & Jegelka, 2018). Most results concern the approximation of functions on RD, however, and yield network sizes that increase exponentially with the input dimension D. As a result, the high dimensionality of many inverse problems leads to bounds from most of the existing literature which are too large to explain the observed empirical success of neural approaches in such applications.\n\nA similar high-dimensional scaling issue arises in many image classification tasks as well. Motivated by this setting (Chen et al., 2019) refined previous approximation results for ReLU networks, and showed that input data that is close to a low-dimensional manifold leads to network sizes that only grow exponentially with respect to the intrinsic dimension of the manifold. However, this improved bound relies on the data fitting manifold assumption which is quite strong in the inverse problems setting. For example, even the “simple” sparse recovery problem does not have a domain/range that forms a manifold (note that the intersections of s-dimensional subspaces prevent from it being a manifold). Therefore, to study expressive power of networks on inverse problems needs to remove such strict manifold assumptions. Another mild issue with such manifolds results is that the number of neurons also depends on the curvature of the manifold in question which can be difficult to estimate. Furthermore, such curvature dependence is unavoidable for manifold results and needs to be incorporated into any valid bounds.1\n\nIn this paper, we provide another way to estimate the size of the network, by directly using the Guassian width of the data as a measure of its inherent complexity. Our result can therefore be\n\n1To see why, e.g., curvature dependence is unavoidable, consider any discrete training dataset in a compact ball. There always exists a 1-dimensional manifold, namely a curve, that goes through all the data points. Thus, the mere existence of the 1-dimensional manifold does not mean the data complexity is low. Curvature information and other manifold properties matter as well!\n\n2\n\nconsidered generalization of the manifold result discussed above in two ways. First, it applies to more arbitrary data sets with low complexities. And, it also applies to other types of networks besides just feedforward ReLu networks. Both types of generalization are then shown to be useful and applicable to various inverse problems.\n\n3 MAIN RESULTS\n\nWe begin by stating a few definitions. We say that a neural network (cid:15)-approximates a function f if the function implemented by the neural network (cid:98)f satisfies (cid:107) (cid:98)f (x) − f (x)(cid:107)∞ ≤ (cid:15) for all x in the domain of f . We say that a neural network architecture (cid:15)-approximates any function in a function class F if for any function f ∈ F, there exists a choice of edge weights such that the function (cid:98)f implemented by the neural network with that choice of edge weights satisfies (cid:107) (cid:98)f (x) − f (x)(cid:107)∞ ≤ (cid:15) for all x in the domain of f . Also, for any positive integers d < D, any set S ⊂ RD, and any constant ρ ∈ (0, 1), we say that a matrix A ∈ Rd×D is a ρ-JL (Johnson-Lindenstrauss) embedding of S if\n\n(1 − ρ)(cid:107)x − x(cid:48)(cid:107)2 ≤ (cid:107)Ax − Ax(cid:48)(cid:107)2 ≤ (1 + ρ)(cid:107)x − x(cid:48)(cid:107)2\n\nfor all x, x(cid:48) ∈ S.\n\nIf we furthermore have A(S) := {Ax : x ∈ S} ⊂ T , we say that A is a ρ-JL embedding of S into T . Intuitively, a ρ-JL embedding of S into Rd transforms S from a high-dimensional space to a low-dimensional space without significantly distorting distances between points.\n\nContributions: Existing universal approximation theorems for various types of neural networks are mainly stated for functions defined on an d-dimensional cube. Our main contribution is to generalize these results to functions defined on arbitrary JL-embedable sets, which possibly reside in very high dimensions. We then demonstrate how our result can be applied to inverse problems to obtain a reasonable estimate of the network size.\n\nSince our theory is to be applied to general inverse problems, for which we cannot assume anything more than Lipschitz continuous. Hence in this paper, we focus on the class of Lipschitz functions. More explicitly, we show that if there exists a ρ-JL embedding of a high-dimensional set S ⊂ RD into a low-dimensional cube [−M, M ]d, then we can use any neural network architecture which can (cid:15)-approximate L 1−ρ -Lipschitz functions on [−M, M ]d to construct a neural network architecture which can (cid:15)-approximate L-Lipschitz functions on S. To establish this, we show that if there exists ρ-JL embedding A ∈ Rd×D of S ⊂ RD into d-dimensions, then for any L-Lipschitz function f : 1−ρ -Lipschitz function g : [−M, M ]d → Rp (where M = supx∈S (cid:107)Ax(cid:107)∞) S → Rp, there exists a L such that g(Ax) = f (x) for all x ∈ S. Hence, if we have a neural network which can approximate g : [−M, M ]d → Rp, then we can compose it with a neural network which implements the JL embedding A to obtain a neural network which approximates f : S → Rp. By pairing JL embedding existence results along with results on approximation of Lipschitz functions by neural networks, we obtain results which bound the complexity required for a neural network to approximate Lipschitz functions on high dimensional sets.\n\nWe now state our main theorem. Theorem 1. Let d < D be positive integers, and let L, M > 0 and ρ ∈ (0, 1) be constants. Let S ⊂ RD be a bounded subset for which there exists a ρ-JL embedding A ∈ Rd×D of S into [−M, M ]d.\n\n1−ρ -Lipschitz function g : [−M, M ]d → Rp can be (cid:15)-approximated by a a) Suppose that any L feedforward neural network with at most N nodes, E edges, and L layers. Then, any L-Lipschitz function f : S → Rp can be (cid:15)-approximated by a feedforward neural network with at most N + D nodes, E + Dd edges, and L + 1 layers.\n\nb) Furthermore, if there exists a single feedforward neural network architecture with at most N nodes, E edges, and L layers that can (cid:15)-approximate any L 1−ρ -Lipschitz function g : [−M, M ]d → Rp, then there also exists another feedforward neural network architecture with at most N + D nodes, E +Dd edges, and L+1 layers that can (cid:15)-approximate any L-Lipschitz function f : S → Rp.\n\nc) Suppose that the ρ-JL embedding is of the form A = M D, where M is a partial circulant matrix, and D is a diagonal matrix with ±1 on its diagonal. Also, suppose that any L 1−ρ -Lipschitz\n\n3\n\nFigure 1: If there exists a ρ-JL embedding of S ⊂ RD into [−M, M ]d, then we can write the target function f : S → Rp as f = g ◦ JL where g : [−M, M ]d → Rp. So, we can then construct a neural network approximation of f by using a neural network approximation of g and adding a layer to implement the JL embedding.\n\nfunction g : [−M, M ]d → Rp can be (cid:15)-approximated by a convolutional neural network with at most N nodes, P parameters, and L layers. Then, any L-Lipschitz function f : S → Rp can be (cid:15)- approximated by a feedforward neural network with at most N +3D nodes, P +2D +d parameters, and L + 4 layers.\n\nd) Furthermore, if there exists a single convolutional neural network architecture with at most N nodes, P parameters, and L layers that can (cid:15)-approximate any L 1−ρ -Lipschitz function g : [−M, M ]d → Rp, then there also exists another convolutional neural network architecture with at most N + 2D nodes, P + 2Dd parameters, and L + 3 layers that can (cid:15)-approximate any LLipschitz function f : S → Rp. Remark 1. The theorem ensures that the network size for approximating f grows exponentially with the compressed dimension d instead of growing exponentially with the input dimension D. The task now reduces to making the compressed dimension d as small as possible while still ensuring that a ρ-JL embedding of S into [−M, M ]d exists. Remark 2. The theorem is quite general as parts a and b are not restricted to any particular type of network or activation function. In Section 3.3, we provide two corollaries of Theorem 1 that establish the expressive power of the feedforward and convolutional neural networks. Remark 3. If an inverse operator is Lipschitz continuous and there exists a ρ-JL embedding of the set of possible observations S into d dimensions, then the theorem gives us a bound on the complexity of a neural network architecture required to approximate the inverse operator.\n\n3.1\n\nJL EMBEDDINGS, AND COVERING NUMBERS AND GAUSSIAN WIDTH\n\nAs the existence of the JL map is a critical assumption of our theorem, in this section, we discuss the sufficient conditions for this assumption to hold. In addition, we also care about the structures of the JL maps, as they will end up being the first layer of the final neural network. For example, if the neural network is of convolution type, we need to make sure that a circulant JL matrix exists.\n\nExistence of ρ-JL maps: It is well-known that for finite sets S, the existence of a ρ-JL embedding can be guaranteed by the Johnson-Lindenstrauss Lemma. For sets S with infinite cardinally, the Johnson-Lindenstrauss lemma cannot be directly used. In the following proposition, we extend the Johnson-Lindenstrauss lemma from a finite set of n points to a general set S. Proposition 1. Let ρ ∈ (0, 1). For S ⊆ RD, define\n\nUS :=\n\n(cid:26) x − x(cid:48)\n\n(cid:107)x − x(cid:48)(cid:107)2\n\n: x, x(cid:48) ∈ S s.t. x (cid:54)= x(cid:48)\n\n(cid:27)\n\nto be the closure of the set of unit secants of S, and N (US , (cid:107) · (cid:107)2, δ) to be the covering number of US with δ-balls. Then, there exists a set S1 with |S1| = 2N (US , (cid:107) · (cid:107)2, δ) points such that if a matrix A ∈ Rd×D is a ρ-JL embedding of S1, then A is also a (ρ + 2(cid:107)A(cid:107)δ)-JL embedding of S.\n\n4\n\nThe proposition guarantees that whenever we have a JL-map for finite sets, we can extend it to a JL-map for infinite sets with similar level of complexity measured in terms of the covering numbers. There are many known JL-maps for finite sets that we can extend from, including sub-Gaussian matrix (Matouˇsek, 2008), Gaussian circulant matrices with random sign flip (Cheng & Zhang, 2014), etc. We present some of the related results here. Proposition 2 ((Matouˇsek, 2008)). Let x1, . . . , xn ∈ RD. Let ρ ∈ (0, 1 2 ) and β ∈ (0, 1). Let A ∈ Rd×D be a random matrix whose entries are i.i.d. from a subgaussian distribution with mean 0 and variance 1. Then, there exists a constant C > 0 depending only on the subgaussian distribution such that if d ≥ Cρ−2 log n A will be a ρ-JL embedding of {x1, . . . , xn} with probability at least 1 − β. Proposition 3 (Corollary 1.3 in (Cheng & Zhang, 2014)). Let x1, . . . , xn ∈ RD. Let ρ ∈ (0, 1 2 ), and let d = O(ρ−2 log1+α n) for some α > 0. Let A = 1√ M D where M ∈ Rd×D is a random Gaussian circulant matrix and D ∈ RD×D is a random Rademacher diagonal matrix. Then, with probability at least 2 3\n\n(cid:0)1 − (D + d)e− logα n(cid:1), A is a ρ-JL embedding of {x1, . . . , xn}.\n\nβ , then 1√\n\nd\n\nd\n\nNote that the α in the proposition can be set to be any positive number making the probability of failure less than 1.\n\nCombining the results of Propositions 2 and 3 with Proposition 1, we have the following existence result for the JL map of an arbitrary set S, Proposition 4. Let ρ ∈ (0, 1) be a constant. For S ⊆ RD, let N (US , (cid:107) · (cid:107)2, δ) to be the covering number with δ-balls of the unit secant US of S defined in Proposition 1. Then a) If D ≥ d (cid:38) ρ−2 log N (US , (cid:107) · (cid:107)2, embedding of S. ), then there exists a matrix A ∈ Rd×D in b) If D ≥ d (cid:38) ρ−2 log(4D + 4d) log N (US , (cid:107) · (cid:107)2, the form of M D and of size d × D that works as ρ-JL map for S, where M is a partial circulant matrix and D is a diagonal matrix with ±1 on its diagonal.\n\n), then there exists a matrix A ∈ Rd×D which is a ρ-JL\n\nρ √\n3D\n\nρ √\n3D\n\n4\n\n4\n\nThe above proposition characterizes the compressibility of a set S by a JL-mapping terms of the covering number. Alternatively, one can also characterize it using the Gaussian width. For example, in (Iwen et al., accepted. (See Arxiv 2110.04193)) it is shown using methods from (Vershynin, 2018) that if the set of unit secants of S has a low Gaussian width, then with high probability a subgaussian random matrix with provide a low-distortion linear embedding, and the dimension d required scales quadratically with the Gaussian width of the set of unit secants of S.\n\nProposition 5 (Corollary 2.1 in (Iwen et al., accepted. (See Arxiv 2110.04193))). Let ρ, β ∈ (0, 1) be constants. Let A ∈ Rd×D be a matrix whose rows aT d are independent, isotropic (E[aiaT\n\n1 , . . . , aT i ] = I), and subgaussian random vectors. Let S ⊂ RD, and Let\n\nω(US ) := E sup\n\nu∈US\n\n(cid:104)u, z(cid:105) ,\n\nz ∼ Normal(0, I)\n\nto be the Gaussian width of US . Then, there exists a constant C > 0 depending only on the distribution of the rows of A such that if\n\nd ≥\n\n(cid:16)\n\nC ρ2\n\nω(US ) +\n\n(cid:113)\n\n(cid:17)2\n\n,\n\nlog 2 β\n\nthen 1√ d\n\nA is a ρ-JL embedding of S with probability at least 1 − β.\n\nIn practice, one can use either the Gaussian width (Proposition 5) or the covering number (Proposition 4) to compute the lower bound of d, whichever is more convenient for a specific application.\n\n3.2 UNIVERSAL APPROXIMATOR NEURAL NETWORKS FOR LIPSCHITZ FUNCTIONS ON\n\nd-DIMENSIONAL CUBES\n\nIn Theorem 1, we showed that with the help of JL, approximation rate of neural networks for functions defined on an arbitrary set S can be derived from their approximation rates for functions\n\n5\n\ndefined on the cube [−M, M ]d. In this section, we review known results for the later, so that they can be used in combination of Theorem 1 to provide useful approximation results for network applications to various inverse problems. Specifically, we review two types of universal approximators for functions defined on the cube [−M, M ]d. One is the Feedforward ReLU network and the other is the Resnet type convolution neural network.\n\nFeedforward ReLU network: The fully connected feedforward neural network with ReLU activation is known to be a universal approximator of any Lipschitz function on the box [−M, M ]d. Moreover, for such networks, the non-asymptotic approximation error has also been established, allowing us to get an estimate of the network size. The proposition below is a variant of Proposition 1 in (Yarotsky, 2018), and the proof uses an approximating function that uses the same ideas as in (Yarotsky, 2018).\n\nProposition 6. Given constants L, M, (cid:15) > 0 and positive integers d and p, there exists a ReLU NN architecture with at most\n\n(p+C1)\n\n(cid:16)\n\n2\n\n(cid:108) LM\n\n√\n\nd\n\n(cid:109)\n\n(cid:15)\n\n(cid:17)d\n\n+ 1\n\nedges, C2\n\n(cid:16)\n\n2\n\n(cid:108) LM\n\n√\n\n(cid:109)\n\nd\n\n(cid:15)\n\n(cid:17)d\n\n+ 1\n\n+p nodes, and (cid:100)log2(d + 1)(cid:101)+2 layers\n\nthat can (cid:15)-approximate any L-Lipschitz function g : [−M, M ]d → Rp. Here, C1, C2 > 0 are universal constants. Also for each edge of the ReLU NN, the corresponding weight is either independent of g, or is of the form gi(x) for some fixed x ∈ [−M, M ]d and coordinate i = 1, . . . , p.\n\nConvolutional Neural Network: As many successful network applications on inverse problems results from the use of filters in the CNN architectures (Jin et al., 2017), we are particularly interested in the expressive power of CNN in approximating the Lipschitz functions. Currently known non-asymptotic results for CNN includes (Zhou, 2020; Petersen & Voigtlaender, 2020; Yarotsky, 2022), but they are established under stricter assumptions than merely Lipschitz continuous. On the other hand, the ResNet-based CNN with the following architecture has been shown to possess good convergence rate.\n\nCN N σ\n\nθ := F CW,b ◦ (Convσ\n\nωM,bM\n\n+ id) ◦ · · · ◦ (Convσ\n\nω1,b1\n\n+ id) ◦ P\n\n(1)\n\nm,...,ωLm\n\nm stored in ωM and Lm bias b1\n\nwhere σ is the activation function, each Convωm,bm is an convolution layer with Lm filters ω1 m stored in bm. The addition by the identity map, Convσ + id, makes it a residual block. F CW,b represents a fully connected layer appended to the final layer of the network. We see that the ResNet-based CNN is essentially a normal CNN with skip connections.\n\nm, ..., bLm\n\nωM,bM\n\nThe following asymptotic result is proved in (Oono & Suzuki, 2019). We note that the authors of (Oono & Suzuki, 2019) proved a more general result for β-H ̈older functions, but we state it for Lipschitz functions, i.e., β = 1. Proposition 7 (Corollary 4 from (Oono & Suzuki, 2019)). Let f : [−1, 1]d → R be a Lipschitz function. Then, for any K ∈ {2, ..., d}, there exists a CNN f (CNN) with O(N ) residual blocks, each of which has depth O(log N ) and O(1) channels, and whose filter size is at most K, such that (cid:107)f − f (CNN)(cid:107)∞ ≤ (cid:101)O(N −1/d).\n\n3.3 MAIN RESULTS\n\nWe can now combine Propositions 4 , 5, 6 and 7 with our Theorem 1 to obtain theorems bounding the required complexity of a feed-forward/convolutional neural network that can (cid:15)-approximate any L-Lipschitz function on arbitrary sets S ⊂ RD for which a ρ-JL embedding into [−M, M ]d exists. Theorem 2. Let d < D be positive integers, and let L > 0 and ρ ∈ (0, 1) be constants. Let S ⊂ RD be a bounded set and US be its set of unit secants. Suppose that\n\nd (cid:38) min\n\n(cid:110)\n\nρ−2 log N (US , (cid:107) · (cid:107)2,\n\n), ρ−2 (ω(US ))2(cid:111)\n\n,\n\nρ √\n3D\n\n4\n\nwhere N (US , (cid:107) · (cid:107)2, there exists a ReLU neural network architecture with at most\n\n4\n\nρ √\n3D\n\n) is the covering number and ω(US ) is the Gaussian width of US . Then,\n\n(p + C1)\n\n(cid:16)\n\n2\n\n√\n\n(cid:108) LM\n\nd (1−ρ)(cid:15)\n\n(cid:109)\n\n(cid:17)d\n\n+ 1\n\n+ Dd edges,\n\n6\n\n(cid:16)\n\n2\n\nC2\n\n√\n\n(cid:108) LM\n\nd (1−ρ)(cid:15)\n\n(cid:109)\n\n(cid:17)d\n\n+ 1\n\n+ p + D nodes,\n\nthat can (cid:15)-approximate any L-Lipschitz function f : S → Rp, where M = supx∈S (cid:107)Ax(cid:107)∞.\n\nand (cid:100)log2(d + 1)(cid:101) + 3 layers\n\nOur Theorem 3 is a variant on our Theorem 2, but for convolutional neural networks. Theorem 3. Let d < D be positive integers, and let L > 0 and ρ ∈ (0, 1) be constants. Let S ⊂ RD be a bounded set and US be its set of unit secants. Suppose that\n\nd (cid:38) ρ−2 log(4D + 4d) log N (US , (cid:107) · (cid:107)2,\n\nρ √\n3D\n\n4\n\n).\n\nThen, for any L-Lipschitz function f : S → Rp, there exists a ResNet type CNN f (CNN) in the form of (1) with O(N ) residual blocks, each of which has a depth O(log N ) and O(1) channels, and whose filter size is at most K such that (cid:107)f − f (CNN)(cid:107)∞ ≤ (cid:101)O(N −1/d).\n\n4 APPLICATIONS TO INVERSE PROBLEMS\n\nNow we focus on inverse problems and demonstrate how the main theorems can be used to provide a reasonable estimate of the size of the neural networks needed to solve some classical inverse problems in signal processing. The problems we consider here are sparse recovery, blind deconvolution, and matrix completion.\n\nIn all the inverse problems, we want to recover some signal x ∈ S from its forward measurement y = F (x), where the forward map F is assumed to be known. The minimal assumption we have to impose on F is the invertibility.\n\nAssumption 1 (invertibility of the forward map): Let S be the domain of the forward map F , and Y = F (S) be the range. Assume that the inverse operator F −1 : Y → S exists and is Lipschitz continuous with constant L,\n\n(cid:107)F −1(y1) − F −1(y2)(cid:107) ≤ L(cid:107)y1 − y2(cid:107),\n\nfor all\n\ny1, y2 ∈ Y.\n\nFor any inverse problems satisfying Assumption 1, Theorem 1, 2 and 3 provide ways to estimate the size of the universal approximator networks for the inverse map. When applying the theorems to each problem, we need to estimate the covering number of UY first.\n\nDepending on the problem, one may estimate the covering number either numerically or theoretically. If the domain Y of the inverse map is irregular and discrete, then it may be easier to compute the covering number numerically. If the domain has a nice mathematical structure, then we may be able to estimate it theoretically. Below are three examples of the theoretical estimation. From them, we see that it is quite common for inverse problems to have a small intrinsic complexity, with which Theorems 2 and 3 can significantly reduce the required size of the network from the previously known results.\n\nWe emphasize that the covering number that the theorems use is the one of the unit secant of Y, which can be much larger than the covering number of Y itself.\n\nSparse recovery: Sparsity is now one of the most commonly used priors in inverse problems as signals in many real applications possess certain level sparsity in some appropriate domain. For simplicity, we consider the strictly sparse signals. Let ΣN s be the set of s−sparse vectors of length N . Assume a sparse vector is measured linearly y = Ax ≡ F (x), the inverse problem amounts to recovering x from y. Now that we want to use a network to approximate the inverse map F −1 : AΣN s , and estimate the size of the network through the theorems, we need to estimate the covering number of the unit secant UAΣN\n\ns ≡ Y ∈ y → x ∈ ΣN\n\n.\n\ns\n\nProposition 8. Let UAΣN\n\ns\n\ndenote the set of unit secants of AΣN\n\ns . Then, we have\n\nlog N (UAΣN\n\ns\n\n, (cid:107) · (cid:107), δ) (cid:46) s log\n\nN δ\n\n.\n\n7\n\nProof. By definition, the unit secant of Y = AΣN\n\ns is defined as\n\nUY =\n\n(cid:26) y1 − y2 (cid:107)y1 − y2(cid:107)\n\n,\n\ny1, y2 ∈ AΣN\n\ns\n\n(cid:27)\n\nwhich contains all unit vectors that are linear combinations of 2s columns of A. Let T with |T | = 2s be a fixed support set, the covering number of span(AT ) ∩ Sm−1 is ( 3 δ )2s, so the covering number of UY is at most N s( 3\n\nδ )2s.\n\nIf the inverse of F exists, such as in the case when A is a Restricted-Isometry-Property matrix, then by Theorem 2 and 3, there exist neural networks of fully connected type or of CNN type with O((cid:15)−s log N ) number of weights, that can do the sparse recovery up to an error of (cid:15).\n\nBlind deconvolution: Blind deconvolution concerns the recovery of a signal x from its blurry measurements\n\ny = k ⊗ x\n\n(2)\n\nwhen the kernel k is also unknown. Here ⊗ denotes the convolution operation.\n\nBlind-deconvolution is an ill-posed problem due to the existence of a scaling ambiguity between x and k, namely, if (k, x) is a solution, then (αk, 1 α x) with α (cid:54)= 0 is also a solution. To resolve this issue, we focus on recovering the outer product xkT , where x and k here are both columns vectors. The recovery of the outer product xkT from the convolution y = k ⊗ x can be well-posed in various settings (Lee et al., 2015; Ahmed et al., 2013). For example, (Ahmed et al., 2013) showed that if we assume x = Φu and k = Ψv, where Φ ∈ RN,n(n < N ) is i.i.d. Gaussian matrix and Ψ ∈ RN,m(m < N ) is a matrix of small coherence, then for large enough N , the outer-product xkT can be stably recovered from y in the following sense. For any two signal-kernel pairs (x, k), ( ̃x, ̃k) and their corresponding convolutions y, ̃y, we have\n\n(cid:107)xkT − ̃x ̃kT (cid:107) ≤ L(cid:107)y − ̃y(cid:107) (3) with some L. When using a neural network to approximate the inverse map by F −1 : y → xkT , we need to estimate the covering number of the unit secant cone of Y = {y = x ⊗ k, x ∈ ΦΣN s , k ∈ spanΨ}, which is done in the following proposition. Proposition 9. Suppose the inverse map F −1 : y → xkT is Lipschitz continuous with Lipschitz constant L, then for Y = {y = x ⊗ k, x ∈ span(Φ), k ∈ spanΨ}, the logarithm of the covering number of the set of unit secants of Y is bounded by\n\nlog N (UY , (cid:107) · (cid:107)2, δ) (cid:46) max{m, n} log\n\n3L δ\n\n.\n\nCombining this proposition with Theorem 2 and 3, we obtain that there exist neural networks of full connected type or of CNN type having about O((cid:15)− max{m,n} log(L(n+m))) number of weights, that can solve the blind-deconvolution problem up to an error of (cid:15).\n\nMatrix completion: Matrix Completion is a central task in machine learning where we want to recover a matrix from its partially observed entries. It arises from a number of applications including image super resolution (Shi et al., 2013; Cao et al., 2014), image/video denoising (Ji et al., 2010), recommender systems (Zheng et al., 2016; Monti et al., 2017), and gene-expression prediction (Kapur et al., 2016), etc.. Recently neural network models have achieved state-of-the-art performance (Zheng et al., 2016; Monti et al., 2017; Dziugaite & Roy, 2015; He et al., 2017), but a general existence result in the non-asymptotic regime is still missing.\n\nIn this setting, the measurements Y = PΩX consists of a set of observed entries of the unknown low-rank matrix X, where Ω is the index set of the observed entries and PΩ is the mask that sets all but entries in Ω to 0. Assuming M n,m is the set of n × m matrices with rank at most r and X ∈ M n,m . If the mask is random, and the left and right eigenvectors U, V of X are incoherent, in the sense that\n\nr\n\nr\n\nmax 1≤i≤n\n\n(cid:107)U T ei(cid:107) ≤\n\n, max 1≤i≤m\n\n(cid:107)V T ei(cid:107) ≤\n\n(cid:114) μ0r n\n\n(cid:114) μ0r m\n\n,\n\n(4)\n\nmax 1≤i≤n,1≤j≤m\n\n(cid:107)(U V T )i,j(cid:107) ≤\n\n(cid:114) μ1r nm\n\n8\n\nthen it is known (e.g. (Candes & Plan, 2010)) that provided that the number of observations\n\n|Ω| (cid:38) μ0r max{m, n} log2 max{m, n}, then with overwhelming probability, the inverse map F −1 : Y → X exists and is Lipschitz continuous. Let us denote the set of low-rank matrices satisfying (4) to be C. To estimate the complexity of the inverse map, we compute the covering number of UY for Y = {Y = PΩX : X ∈ M m,n ∩ C}. Proposition 10. Suppose the mask is chosen so that the inverse map F −1 : Y = PΩX → X is Lipschitz continuous with Lipschitz constant L, then for Y = {PΩX : X ∈ M m,n ∩ C}, the logarithm of the covering number of the set of unit secants of Y is bounded by\n\nr\n\nr\n\nlog N (UY , (cid:107) · (cid:107)2, δ) (cid:46) r(m + n) log\n\n(cid:19)\n\n.\n\n(cid:18) L δ\n\nCombining this proposition with Theorem 2 and 3, we obtain that there exist neural networks of full connected type or of CNN type having about O((cid:15)−r(m+n) log(Lnm)) number of weights, that can solve the blind-deconvolution problem up to an error of (cid:15).\n\n5 CONCLUSION AND DISCUSSION\n\nThe main message of this paper is that when neural networks are used to approximate Lipschitz continuous functions, the size of the network only needs to grow exponentially with respect to the intrinsic complexity of the input set measured using either the Gaussian width or the covering numbers. Therefore, it is more optimistic than the previous estimate that requires the size of the network to grow exponentially with respect to the input dimension.\n\nSimilar results were derived previously in (Chen et al., 2019) in a more restrictive setting, namely, the input set is assumed to be close to a smooth manifold with a small curvature, and the network type is restricted to the feedforward ReLU networks. In this paper, by utilizing the JL map, we are able to state the result in a very general setting, that does not pose any structural requirement on the inputs set other than that they have a small complexity. In addition, our result holds for many different types of networks – although we only stated it for feedfoward neural networks and the ResNet type of convolutional neural networks, the same idea naturally applies to other types of networks as long as an associated JL-map exists.\n\nThe estimate we provided for the network size ultimately depends on the complexity of the input set, measured by either the covering number or the Gaussian width of its set of unit secants. The computation of these quantities varies case by case, and in some cases might be rather difficult. This is a possible limitation of the proposed method. Because if the estimation of the input complexity is not tight enough, we may again get a pessimistic bound. Having said that, for most of the classical inverse problems, the covering number and the Gaussian width are not too difficult to calculate. As we demonstrated in Section 4, there are many known properties of them that one can use to facilitate the calculation. And when a training dataset is given, one can even compute the covering number numerically with off-the-shelf algorithms.\n\nFinally, although the applications of neural networks to inverse problems are seeing its success. There are much more failed attempts with unclear reasons. One common explanation is that the size of the network in use is not large enough for the targeted applications. Since inverse problems models usually have a much higher intrinsic dimensionality than say image classification models, the required network sizes might also be much larger. The classical universal approximation theorems only guarantees small errors when the network size approaches infinity, therefore is not very helpful in the non-asymptotic regime where we have to choose the network size, which is now known to be critical to good performances. We hope the presented result can give more insight on this matter.\n\nREFERENCES\n\nAhmed, Ali, Recht, Benjamin, & Romberg, Justin. 2013. Blind deconvolution using convex pro-\n\ngramming. IEEE Transactions on Information Theory, 60(3), 1711–1732.\n\nArora, Raman, Basu, Amitabh, Mianjy, Poorya, & Mukherjee, Anirbit. 2016. Understanding deep\n\nneural networks with rectified linear units. arXiv preprint arXiv:1611.01491.\n\n9\n\nAzagra, Daniel, Le Gruyer, Erwan, & Mudarra, Carlos. 2021. Kirszbraun’s theorem via an explicit\n\nformula. Canadian Mathematical Bulletin, 64(1), 142–153.\n\nCandes, Emmanuel J, & Plan, Yaniv. 2010. Matrix completion with noise. Proceedings of the IEEE,\n\n98(6), 925–936.\n\nCandes, Emmanuel J, & Plan, Yaniv. 2011. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57(4), 2342–2359.\n\nCao, Feilong, Cai, Miaomiao, & Tan, Yuanpeng. 2014. Image interpolation via low-rank matrix IEEE Transactions on Circuits and Systems for Video Technology,\n\ncompletion and recovery. 25(8), 1261–1270.\n\nChen, Minshuo, Jiang, Haoming, Liao, Wenjing, & Zhao, Tuo. 2019. Efficient approximation of deep relu networks for functions on low dimensional manifolds. Advances in neural information processing systems, 32.\n\nCheng, Lizhi, & Zhang, Hui. 2014. New bounds for circulant Johnson-Lindenstrauss embeddings.\n\nCommunications in Mathematical Sciences, 12(4), 695–705.\n\nCoccorese, Enzo, Martone, Raffaele, & Morabito, F Carlo. 1994. A neural network approach for the solution of electric and magnetic inverse problems. IEEE transactions on magnetics, 30(5), 2829–2839.\n\nDziugaite, Gintare Karolina, & Roy, Daniel M. 2015. Neural network matrix factorization. arXiv\n\npreprint arXiv:1511.06443.\n\nHe, Xiangnan, Liao, Lizi, Zhang, Hanwang, Nie, Liqiang, Hu, Xia, & Chua, Tat-Seng. 2017. Neural collaborative filtering. Pages 173–182 of: Proceedings of the 26th international conference on world wide web.\n\nIwen, M.A., Schmidt, Benjamin, & Tavakoli, Arman. accepted. (See Arxiv 2110.04193). On Fast Johnson-Lindenstrauss Embeddings of Compact Submanifolds of RN with Boundary. Discrete & Computational Geometry.\n\nJi, Hui, Liu, Chaoqiang, Shen, Zuowei, & Xu, Yuhong. 2010. Robust video denoising using low rank matrix completion. Pages 1791–1798 of: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE.\n\nJin, Kyong Hwan, McCann, Michael T, Froustey, Emmanuel, & Unser, Michael. 2017. Deep convolutional neural network for inverse problems in imaging. IEEE Transactions on Image Processing, 26(9), 4509–4522.\n\nKapur, Arnav, Marwah, Kshitij, & Alterovitz, Gil. 2016. Gene expression prediction using low-rank\n\nmatrix completion. BMC bioinformatics, 17(1), 1–13.\n\nKupyn, Orest, Budzan, Volodymyr, Mykhailych, Mykola, Mishkin, Dmytro, & Matas, Jiˇr ́ı. 2018. Deblurgan: Blind motion deblurring using conditional adversarial networks. Pages 8183–8192 of: Proceedings of the IEEE conference on computer vision and pattern recognition.\n\nLee, Kiryung, Li, Yanjun, Junge, Marius, & Bresler, Yoram. 2015. Stability in blind deconvolution of sparse signals and reconstruction by alternating minimization. Pages 158–162 of: 2015 International Conference on Sampling Theory and Applications (SampTA). IEEE.\n\nLin, Hongzhou, & Jegelka, Stefanie. 2018. Resnet with one-neuron hidden layers is a universal\n\napproximator. Advances in neural information processing systems, 31.\n\nMatouˇsek, Jiˇr ́ı. 2008. On variants of the Johnson–Lindenstrauss lemma. Random Structures &\n\nAlgorithms, 33(2), 142–156.\n\nMonti, Federico, Bronstein, Michael, & Bresson, Xavier. 2017. Geometric matrix completion with recurrent multi-graph neural networks. Advances in neural information processing systems, 30.\n\n10\n\nNah, Seungjun, Hyun Kim, Tae, & Mu Lee, Kyoung. 2017. Deep multi-scale convolutional neural network for dynamic scene deblurring. Pages 3883–3891 of: Proceedings of the IEEE conference on computer vision and pattern recognition.\n\nOono, Kenta, & Suzuki, Taiji. 2019. Approximation and non-parametric estimation of ResNettype convolutional neural networks. Pages 4922–4931 of: International Conference on Machine Learning. PMLR.\n\nPetersen, Philipp, & Voigtlaender, Felix. 2020. Equivalence of approximation by convolutional neural networks and fully-connected networks. Proceedings of the American Mathematical Society, 148(4), 1567–1581.\n\nSchwartz, Jacob T. 1969. Nonlinear functional analysis. Vol. 4. CRC Press.\n\nShi, Feng, Cheng, Jian, Wang, Li, Yap, Pew-Thian, & Shen, Dinggang. 2013. Low-rank total variation for image super-resolution. Pages 155–162 of: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer.\n\nVershynin, Roman. 2018. High-dimensional probability: An introduction with applications in data\n\nscience. Vol. 47. Cambridge university press.\n\nWedin, Per- ̊Ake. 1972. Perturbation bounds in connection with singular value decomposition. BIT\n\nNumerical Mathematics, 12(1), 99–111.\n\nXu, Li, Ren, Jimmy S, Liu, Ce, & Jia, Jiaya. 2014. Deep convolutional neural network for image\n\ndeconvolution. Advances in neural information processing systems, 27.\n\nYarotsky, Dmitry. 2018. Optimal approximation of continuous functions by very deep ReLU net-\n\nworks. Pages 639–649 of: Conference on learning theory. PMLR.\n\nYarotsky, Dmitry. 2022. Universal approximations of invariant maps by neural networks. Construc-\n\ntive Approximation, 55(1), 407–474.\n\nZheng, Yin, Tang, Bangsheng, Ding, Wenkui, & Zhou, Hanning. 2016. A neural autoregressive approach to collaborative filtering. Pages 764–773 of: International Conference on Machine Learning. PMLR.\n\nZhou, Ding-Xuan. 2020. Universality of deep convolutional neural networks. Applied and compu-\n\ntational harmonic analysis, 48(2), 787–794.\n\n11",
    "reference": "# Summary Of The Paper\n\nAuthors in this work study the approximation of high-dimensional Lipschitz functions and characterize the size of the NN required for an arbitrarily good approximation. They are able to say if the data $x \\in R^{D}$ is $\\rho$-JL embedable in a low-dim manifold $R^d$, then there exists an apporpriate deep network (with say ReLU activation) that can approximate any $L/(1-\\rho)$-Lipschitz function $g: [-M,M]^d \\rightarrow \\mathbb{R}^p$, then there exists a deep network with only a small constant increase in depth, and the size of the network that increases exponentially in $d$ instead of $D$ (the input dimension). They show this by an existence of a JL-embedding for general sets in $\\mathbb{R}^D$, in terms of it's Gaussian width (or covering numbers). They use these results to obtain the required size of NN to get an $\\varepsilon$-approximation of inverse maps that are Lipschitz continuous.\n\n# Strength And Weaknesses\n\nStrengths:\nUnderstanding training w.r.t the low-dimensional embeddings are important and this work has a good characterization of the size of the network required for Lipschitz continuous functions in high dimensions. \n\nPrior works such as (Chen et al. 2019) characterize the number of neurons that depend on the curvature of the low-dimensional manifold in question which can be difficult to estimate. This work sort of alleviates, with a less abstract notion, which are Gaussian widths of the set or the covering number, which seem to be more amenable to estimation from data.\n\nThe size of the network grows exponentially in $d$ rather than $D$, the input dimension and the results offer for tradeoffs in practice.\n\nWeakness/Clarifications:\nAre there any lower bounds known for the size of the networks? Could the authors comment on the tightness of the size of the networks.\n\nIt would have been great to see experiments on some high dimensional datasets as there are usually gaps in approximation characterizations and its training and generalization qualities. \n\nIt would be nice to get some bounds on the samples required to for a reasonable approximation, when you estimate the GW or the covering number of the set. Or at least a some discussion pertaining to this.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is written clearly and well motivated and the techniques are novel. \n\nMinor comments:\nThe citation (Iwen et al. accepted (See Arxiv)) could be just changed to it's arxiv link.\nMinor typo: Guassian->Gaussian in the last line of page 2.\n\n# Summary Of The Review\n\nOverall it is an interesting work on the size of NNs required for approximation of Lipschitz continuous functions in high-dimensions by embedding them in a low-dimensional manifold with complexity parameters that are practical to estimate, as compared to prior work. It would have been great to see some experiments to see this in action.\n\nUPDATE: After the response and the other reviews, it feels like many clarifications were not convincing and thus I decrease my score to 6.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nNEURAL BREGMAN DIVERGENCES FOR DISTANCE LEARNING\n\nFred Lu∗†, Edward Raff∗†, & Francis Ferraro∗ ∗University of Maryland, Baltimore County {lu_fred,raff_edward}@bah.com, ferraro@umbc.edu\n\n† Booz Allen Hamilton\n\nABSTRACT\n\nMany metric learning tasks, such as triplet learning, nearest neighbor retrieval, and visualization, are treated primarily as embedding tasks where the ultimate metric is some variant of the Euclidean distance (e.g., cosine or Mahalanobis), and the algorithm must learn to embed points into the pre-chosen space. The study of non-Euclidean geometries is often not explored, which we believe is due to a lack of tools for learning non-Euclidean measures of distance. Recent work has shown that Bregman divergences can be learned from data, opening a promising approach to learning asymmetric distances. We propose a new approach to learning arbitrary Bergman divergences in a differentiable manner via input convex neural networks and show that it overcomes significant limitations of previous works. We also demonstrate that our method more faithfully learns divergences over a set of both new and previously studied tasks, including asymmetric regression, ranking, and clustering. Our tests further extend to known asymmetric, but non-Bregman tasks, where our method still performs competitively despite misspecification, showing the general utility of our approach for asymmetric learning.\n\n1\n\nINTRODUCTION\n\nLearning a task-relevant metric among samples is a common application of machine learning, with use in retrieval, clustering, and ranking. A classic example of retrieval is in visual recognition where, given an object image, the system tries to identify the class based on an existing labeled dataset. To do this, the model can learn a measure of similarity between pairs of images, assigning small distances between images of the same object type. Given the broad successes of deep learning, there has been a recent surge of interest in deep metric learning—using neural networks to automatically learn these similarities (Hoffer & Ailon, 2015; Huang et al., 2016; Zhang et al., 2020).\n\nThe traditional approach to deep metric learning learns an embedding function over the input space so that a simple distance measure between pairs of embeddings corresponds to task-relevant spatial relations between the inputs. The embedding function f is computed by a neural network, which is learned to encode those spatial relations. For example, we can use the basic Euclidean distance metric to measure the distance between two samples x and y as (cid:107)2. This distance is critical in two ways. First, it is used to define the loss functions, such as triplet or contrastive loss, to dictate how this distance should be used to capture task-relevant properties of the input space. Second, since f is trained to optimize the loss function, the distance influences the learned embedding f .\n\nf (x)\n\nf (y)\n\n−\n\n(cid:107)\n\nThis approach has limitations. When the underlying reference distance is asymmetric or does not follow the triangle inequality, a standard metric cannot accurately capture the data. An important example is clustering over probability distributions, where the standard k-means approach with Euclidean distance is sub-optimal, leading to alternatives being used like the KL-divergence (Banerjee et al., 2005). Other cases include textual entailment and learning graph distances which disobey the triangle inequality.\n\nRecent work has shown interest in learning an appropriate distance from the data instead of predetermining the final metric between embeddings (Cilingir et al., 2020; Pitis et al., 2020). A natural class of distances that include common measures such as the squared Euclidean distance are the Bregman divergences (Bregman, 1967). They are parametrized by a strictly convex function φ and measure the distance between two points x and y as the first-order Taylor approximation error of the\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nMethod\n\nNBD PBDL Deep-div\n\nφ representation\n\nLearning Approach\n\nComplexity\n\nJoint Learning\n\nφ(x) = ICNN(x)\n\nGradient Descent\n\nφ(x) = maxi(b(cid:62) φ(x) = maxi(b(cid:62)\n\ni x + zi) Linear Programming i x + zi)\n\nGradient Descent\n\n( )\nθ |\n| O\n(n3) O\nθ (\n|\n\n|\n\n+ K)\n\nO\n\nYes No Yes\n\nTable 1: Comparison of our Bregman learning approach NBD with prior methods. NBD simultaneously has better representational power and computational efficiency.\n\nfunction originating from y at x. The current best approach in Bregman learning approximates φ using the maximum of affine hyperplanes (Siahkamari et al., 2020; Cilingir et al., 2020).\n\nIn this work we address significant limitations of previous approaches and present our solution, Neural Bregman Divergences (NBD). NBD is the first non max-affine approach to learn a deep Bregman divergence, avoiding key limitations of prior works. We instead directly model the generating function φ(x), and then use φ(x) to implement the full divergence Dφ. To demonstrate efficacy, we leverage prior and propose several new benchmarks of asymmetry organized into three types of information they provide: 1) quality of learning a Bregman divergence directly, 2) ability to learn a Bregman divergence and a feature extractor jointly, and 3) effectiveness in asymmetric tasks where the ground truth is known to be non-Bregman. This set of tests shows how NBD is far more efficacious in representing actual Bregman divergences than prior works, while simultaneously performing better in non-Bregman learning tasks.\n\nThe rest of our paper is organized as follows. In §2 we show how to implement NBD using an Input Convex Neural Network and compare to related work, with further related work in §3. In §4 we demonstrate experiments where the underlying goal is to learn a known Bregman measure, and then to jointly learn a Bregman measure with an embedding of the data from which the measure is computed. Then §5 studies the performance of our method on asymmetric tasks where the underlying metric is not Bregman, to show more general utility where prior Bregman methods fail. Finally we conclude in §6.\n\n2 NEURAL BREGMAN DIVERGENCE LEARNING\n\nA Bregman divergence computes the divergence between two points x and y from a space by taking the first-order Taylor approximation of a generating function φ. This generating function is . A proper and informative φ defined over is incredibly important: different φ can capture different properties of the spaces over which they are defined. Our aim in this paper is to learn Bregman divergences by providing a neural method for learning informative functions φ.\n\nand can be thought of as (re-)encoding points from\n\nX\n\nX\n\nX\n\nDefinition 2.1. Let x, y φ :\n\n, where R, the Bregman divergence parametrized by φ is\n\nX ⊆\n\n∈ X\n\nRd. Given a continuously differentiable, strictly convex\n\nX →\n\n, (cid:104)·\n\n·(cid:105)\n\nwhere\n\nrepresents the dot product and\n\nDφ(x, y) = φ(x)\n\nφ(y)\n\nφ(y), x\n\ny\n\n,\n\n(1)\n\n− φ(y) is the gradient of φ evaluated at y.\n\n− (cid:104)∇\n\n−\n\n(cid:105)\n\n∇\n\nA properly defined φ can capture critical, inherent properties of the underlying space. By learning φ via Eq. (1), we aim to automatically learn these properties. For example, Bregman divergences is the D-dimensional simplex representing D-dimensional can capture asymmetrical relations: if yields the KL divergence, Dφ(x, y) = discrete probability distributions then φ(x) = (cid:80) d xd log xd 2\n2), then 2\nDφ(x, y) = 2. Focusing on the hypothesis space of Bregman divergences is valuable due to (cid:107) the fact that many core machine learning measures, including squared Euclidean, Kullback-Leibler, and Ikura-Saito divergences, are special cases of Bregman divergences. While special cases of the Bregman divergence are used today, and many general results have been proven over the space of Bregman measures, less progress has been made in learning Bregman divergences.\n\n= Rd and φ is the squared L2 norm (φ(y) =\n\n. On the other hand, if x\n\nx, log x\n\n−\n\nX\n\nX\n\nyd\n\n(cid:107)\n\n(cid:107)\n\n(cid:107)\n\ny\n\ny\n\n(cid:104)\n\n(cid:105)\n\n2.1 EXISTING BREGMAN LEARNING APPROACHES PBDL. Recent works have proposed ways to empirically learn the Bregman divergence that best represents a dataset by focusing on a max-affine representation of φ (Siahkamari et al., 2020;\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2022). Given a set of K affine hyperplanes of the form b(cid:62) i x + zi, the convex function φ can be given a lower-bound approximation as φ(x) = maxi(b(cid:62) i x + zi). Given a dataset of m distance K\nconstraints between pairs of samples D(xa, xp) 1 can be learned using convex optimization to minimize an objective function, e.g. a variant of the triplet . With some rewriting of the loss objective, the problem can be solved with techniques such as ADMM. We have modified some of the presentation for simplicity; refer to the original works for detail.\n\ntr(xa, xp, xn) = (cid:80)m\n\nD(xa, xn), the parameters\n\n0, 1 + D(xaj , xpj )\n\nD(xaj , xnj )\n\nj=1 max\n\nbi, zi\n\n≤\n\n−\n\nL\n\n{\n\n{\n\n}\n\n}\n\nWhile effective on smaller datasets at learning a suitable divergence for ranking and clustering, this approach has limitations when scaling up to the dataset sizes used in deep learning. It requires a fixed (n3) set of triplets for training which limits the data size (whereas on-demand mining can generate triplets). Even though ADMM can be distributed (and the 2-block ADMM from Siahkamari et al. (2022) is an order of magnitude faster than the original), it does not scale as well as batch gradient descent methods to large data. Moreover, it cannot be directly used for deep learning tasks, as we cannot simultaneously learn an embedding with the divergence.\n\nO\n\nDeep-div. Following the max-affine approach, Cilingir et al. (2020) proposed the first deep learning approach to learn φ, where the base neural network is followed by a layer of K max-affine components. Each max-affine component is a linear layer (or a shallow network), trained via backpropagation. Recall that φ(x) := maxi(b(cid:62) i x+zi for each i. To simplify the computation of dφ(x, y), they make use of the following property.\n\ni x+zi) and let φi(x) = b(cid:62)\n\nFact 2.1.1. Let i and j be the corresponding max-affine components for φ(x) and φ(y). Then the Bregman divergence between two points x and y is Dφ(x, y) = φi(x)\n\nφj(x).\n\n−\n\nThus for a given x, y, the components i, j are updated by backpropagation. Observe that no matter how large K is, φ is neither continuously differentiable nor strictly convex. As a result, this structure is not reliably learned using gradient descent. We provide an example to illustrate this, which is confirmed in our experiments (e.g. Table 4).\n\nExample 2.2. Consider a Bregman regression problem learning D(x, y) from known targets d and loss function\n\nd)2. The gradient with respect to each parameter b is\n\n(x, y, d; φ) = (Dφ(x, y)\n\nL\n\n−\n\nb\n\n∇\n\nL\n\n(x, y, d; φ) = 2(Dφ(x, y)\n\nd)\n\n−\n\n· ∇\n\nbDφ(x, y)\n\nFor any max-affine slopes bi, bj, x respectively, while the others are 0. If we are also learning an embedding f (x), then replace x with f (x) above, and furthermore for any embedding parameter θ, we have\n\nbDφ(x, y) are x and\n\nθDφ(x, y) = (bi\n\nθf (x).\n\nbj)\n\n∇\n\n−\n\n∇\n\n−\n\n· ∇\n\nFrom this we infer that when i = j no learning occurs. Furthermore, if any max-affine hyperplane l is (nearly) dominated by the others over the domain of the inputs (that is, φl(x) < φ(x) for (almost) all x), that component never (rarely) gets updated, resulting in essentially ‘dead’ components.\n\nAnother implication is that dφ(x, y) = 0 for any x, y located on the same maximal component. By linearity there are at most K such regions, thus in classification or clustering with K classes the runtime of Deep-div is forced to increase with K. Even so, this resolution is low for more fine-grained tasks such as regression unless K is very large. In practice, training this method on deep metric learning or regression shows issues which corroborate our analysis.\n\n2.2 REPRESENTING Dφ VIA φ DIRECTLY\n\nO\n\nNext we describe our new method, where we directly learn φ with a continuous convex neural network, which has a number of advantages over the piecewise approach. Whereas each pass of (K) loop over the max-affine components, slowing training and inference, Deep-div requires an our approach does not incur additional complexity beyond an additional backpropagation step. Our approach is suited for both classification and regression tasks because it gives much finer resolution to φ, while max-affine approaches incur irreducible error when the target is smooth. This is further improved by selecting an activation function to make the learned φ strictly convex, continuously differentiable, unlike the max-affine approximation. Empirically, we demonstrate that our method converges consistently and more efficiently to lower error solutions.\n\nTo represent φ, we adopt the Input Convex Neural Network (ICNN) (Amos et al., 2017). The ICNN composes linear layers with non-negative weights W + and affine functions with unconstrained ). The composition of these three weights U with convex non-decreasing activation functions g( ·\n\n3\n\nPublished as a conference paper at ICLR 2023\n\ncomponents for the ith layer of an ICNN is given by Eq. (2), with zi the i’th layer’s input and zi+1 the output,\n\n(2) By construction, the resulting neural network satisfies convexity. Chen et al. (2019) and Pitis et al. (2020) have shown under specific conditions that ICNNs universally approximate convex functions. Furthermore, with hidden layers, the representational power of an ICNN is exponentially greater than that of a max-affine function ( Chen et al. (2019) Thm. 2), giving much finer resolution over φ.\n\ni zi + Uiz0 + bi\n\nzi+1 = g (cid:0)W +\n\n(cid:1) .\n\nPrior works on the ICNN have only tried piecewise linear activation functions such as the ReLU ) = max(x, 0); we instead use the Softplus activation g(x) = log(1 + exp(x)) which variants for g( ·\nlends the network smoothness and strict convexity. This is an important design choice as learning φ(y) involves the second derivatives, which for any piecewise activation is zero. This causes term, restricting the capacity to learn. We further discuss\n\nφ(y), x\n\ny\n\n∇ vanishing gradients in the its representational capacity in F.1.\n\n(cid:104)∇\n\n−\n\n(cid:105)\n\n∇\n\nEfficient Computation. In order to backpropagate a loss through the φ(y) term in 1, we use double φ(y) would backpropagation as in Drucker & Le Cun (1991). Normally computing the gradient of involve constructing the Hessian, with a resulting quadratic increase in computation and memory use. Double backpropagation uses automatic differentiation to efficiently compute gradients with respect to the inputs with the “Jacobian vector product” (Frostig et al., 2021), so that can be computed in the cost of evaluating φ(y) one additional time. Since there are already three calls to φ, this is only a 25% increase in computational overhead to backpropagate through Eq. (1), provided we have a learnable representation of φ. This functionality has been implemented in the PyTorch API (Paszke et al., 2017). Furthermore, we can vectorize the Bregman divergence over a pairwise matrix, making our method significantly more computationally efficient than prior approaches. In (K) max-affine computation of Deep-div cuts our training time in half particular, avoiding the (Table 8). We show empirical timings, along with discussion, in Appendix A.\n\nφ(y), x\n\n(cid:104)∇\n\n∇\n\nO\n\n−\n\ny\n\n(cid:105)\n\n2.3\n\nJOINT TRAINING\n\nThe original feature space is rarely ideal for computing the distance measures between samples. Classical metric learning generally attempts to apply a linear transformation to the feature space in order to apply a fixed distance function D( ) such as Euclidean distance (Jain et al., 2012; Kulis ·\n· et al., 2012). In deep metric learning, a neural network fθ is used to embed the samples into a latent space where the distance function is more useful (Musgrave et al., 2020). In our approach, instead of fixing the distance function, we also learn a Bregman divergence as the measure:\n\n,\n\nDφ(fθ(x), fθ(y)) = φ(fθ(x))\n\nwith\n\n∇\n\nφ evaluated at ̃y = fθ(y).\n\nNote we now have two sets of parameters to learn: those associated with φ and the encoder (θ). During training, they are simultaneously learned through gradient descent, which involves double-backpropagation as described earlier. We summarize this process in Alg. 1. The metric model accepts two samples as input and estimates the divergence between them. When the target divergence value is available, the metric can be trained using a regression loss function such as mean square error. Otherwise, an implicit comparison such as triplet or contrastive loss can be used.\n\n3 OTHER RELATED WORK\n\nφ(fθ(y))\n\n−\n\n− (cid:104)∇\n\nφ( ̃y), fθ(x)\n\nfθ(y) (cid:105)\n\n−\n\n(3)\n\nAlgorithm 1 Neural Bregman Divergence (NBD). Given data pairs (ai, bi), our approach learns (1) fθ to featurize ai and bi; (2) φ to compute a Bregman divergence value ˆy between the featurized data points. The computed Bregman divergence is trained via loss function (cid:96) to be close to a target divergence value yi. If a target divergence value isn’t available, an implicit loss function can be used.\n\nRequire: Dataset of pairs and target distance, Loss function\n\n,\n\nR\n\n→\n\n) : R ·\n← ←\n\narbitrary neural network as a feature extractor a ICNN network parameterized as by Eq. (2)\n\n(cid:96)( ·\n1: fθ 2: φ 3: for each data tuple (ai, bi) with label yi in dataset do 4: 5: 6: 7: 8: 9:\n\nx y\nrhs −\nˆy −\n− (cid:96) (ˆy, yi) . backward() update parameters of φ and θ\n\n(cid:46) Use double backprop (cid:46) Empirical Bregman (cid:46) Compute gradients\n\n(cid:46) Perform feature extraction\n\nφ(y), x φ(y)\n\nfθ(ai) fθ(bi)\n\n← (cid:104)∇ φ(x)\n\ny (cid:105) rhs\n\n← ←\n\n←\n\n10: return Jointly trained feature extractor fθ and learned\n\nBregman Divergence φ\n\nIn classic metric learning methods, a linear or kernel transform on the ambient feature space is used, combined with a standard distance\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nExponential\n\nGaussian\n\nMultinomial\n\nModel\n\nPurity\n\nRand Index\n\nPurity\n\nRand Index\n\nPurity\n\nRand Index\n\nNBD Deep-div Euclidean Mahalanobis PBDL\n\n0.735 0.08 0.665 0.12 0.365 0.02 0.452 0.05 0.718 0.08\n\n0.830 0.03 0.788 0.08 0.615 0.02 0.697 0.02 0.830 0.04\n\n0.913 0.05 0.867 0.12 0.782 0.11 0.908 0.06 0.806 0.14\n\n0.938 0.03 0.910 0.07 0.869 0.05 0.935 0.03 0.874 0.09\n\n0.921 0.02 0.876 0.08 0.846 0.09 0.894 0.06 0.833 0.08\n\n0.939 0.01 0.919 0.04 0.900 0.05 0.926 0.03 0.895 0.04\n\nTable 2: We cluster data generated from a mixture of exponential, Gaussian, and multinomial distributions. Learning the metric from data is superior to using a standard metric such as Euclidean. Our approach NBD furthermore outperforms all other divergence learning methods. Means and standard deviations are reported over 10 runs.\n\nfunction such as Euclidean or cosine distance. The linear case is equivalent to Mahalanobis distance learning. Information on such approaches are in (Xing et al., 2002; Kulis et al., 2012; Jain et al., 2012; Kulis et al., 2009). Bregman divergences generalize many standard distance measures and can further introduce useful properties such as asymmetry. They have classically been used in machine learning for clustering, by modifying the distance metric used in common algorithms such as kmeans (Banerjee et al., 2005; Wu et al., 2009). One of the first methods to learn a Bregman divergence fits a non-parametric kernel to give a local Mahalanobis metric. The coefficients for the data points are fitted using subgradient descent (Wu et al., 2009). We described the more recent approaches earlier.\n\n−\n\nRecently Pitis et al. (2020) approached asymmetric distance learning by fitting a norm N with modified neural networks which satisfy norm properties and using the induced distance metric y). They introduce two versions that we include as baselines: one (Deepnorm) parametrizes N (x N with a modified ICNN that satisfies properties such as non-negativity and subadditivity. The second (Widenorm) computes a nonlinear transformation of a set of Mahalanobis norms. By construction, these metrics allow for asymmetry but still satisfy the triangle inequality. On the other hand, the Bregman divergence does not necessarily obey the triangle inequality. This is appealing for situations where the triangle inequality may be too restrictive.\n\n4 BREGMAN EXPERIMENTS\n\nWe conduct several experiments that validate our approach as an effective means of learning divergences across a number of tasks. Over 44 comparisons, NBD outperforms prior Bregman learning approaches in all but three. In the first section §4.1 we demonstrate that NBD effectively learns standard Bregman retrieval and clustering benchmarks, outperforming the previous Bregman methods PBDL and Deepdiv. In addition, we construct a Bregman regression task in §4.2 where the labels are known divergences over raw feature vectors, so that the only learning task is that of the divergence itself. Finally in §4.3 we investigate the ability of our method to learn the ground truth divergence while simultaneously learning to extract a needed representation, training a sub-network’s parameters θ and our divergence φ jointly. This is typified by the “BregMNIST” benchmark, which combines learning the MNIST digits with the only supervisory signal being the ground truth divergence between the digit values. Refer to the Appendix for detailed training protocols and data generation procedures.\n\nDataset Model\n\nMAP\n\nAUC\n\nPurity\n\nRand\n\nabalone\n\nbalance scale\n\ncar\n\niris\n\nDeep-div Euclidean Mahalanobis NBD PBDL\n\nDeep-div Euclidean Mahalanobis NBD PBDL\n\nDeep-div Euclidean Mahalanobis NBD PBDL\n\nDeep-div Euclidean Mahalanobis NBD PBDL\n\n0.281 0.301 0.310 0.316 0.307\n\n0.804 0.611 0.822 0.887 0.836\n\n0.787 0.681 0.787 0.820 0.798\n\n0.945 0.827 0.946 0.957 0.943\n\n0.645 0.666 0.677 0.682 0.659\n\n0.859 0.666 0.854 0.915 0.855\n\n0.757 0.589 0.752 0.803 0.775\n\n0.967 0.897 0.973 0.977 0.967\n\n0.377 0.422 0.419 0.432 0.386\n\n0.869 0.633 0.851 0.898 0.872\n\n0.852 0.704 0.778 0.860 0.854\n\n0.811 0.820 0.884 0.909 0.889\n\n0.660 0.750 0.750 0.750 0.735\n\n0.828 0.568 0.761 0.872 0.814\n\n0.750 0.523 0.654 0.758 0.750\n\n0.820 0.828 0.879 0.902 0.888\n\nTable 3: On real datasets, learned Bregman divergences outperform Euclidean or Mahalanobis metrics for downstream ranking (MAP, AUC) and clustering (Purity, Rand Index). NBD beats prior Bregman learning approaches on most datasets. See Appendix Table 10 for standard deviations and more datasets.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4.1 BREGMAN RANKING AND CLUSTERING\n\nOur first task expands the distributional clustering experiments in Banerjee et al. (2005); Cilingir et al. (2020). The datasets consist of mixtures of N = 1000 points in R10 from five clusters, where the multivariate distribution given cluster identity is non-isotropic Gaussian, exponential, or multinomial. Given a distance metric, we apply a generalized k-means algorithm to cluster the data points. While standard metrics, such as L2-distance and KL-divergence, may be ideal for specific forms of data (e.g. isotropic Gaussian and simplex data, respectively), our goal is to learn an appropriate metric directly from a separate labeled training set. In particular, because Bregman divergences are uniquely associated with each member of the exponential family (Banerjee et al., 2005), our method is especially suited for clustering data from a wide range of distributions which may not be known ahead of time. To learn the metric from data, we apply triplet mining, including all triplets with non-zero loss (Hoffer & Ailon, 2015). We use the same method to train all models except for the Euclidean baseline, which requires no training, and PBDL where we directly use the authors’ code.\n\nAs shown in Table 2, our method NBD gives improved clustering over all distributions compared to all baselines. In particular, standard k-means with Euclidean distance is clearly inadequate. While the Mahalanobis baseline shows significant improvement, it is only comparable to NBD in the Gaussian case, where a matrix can be learned to scale the clusters to be isotropic. This task indicates the importance of learning flexible divergences from data.\n\nAfter demonstrating success in distributional clustering, we now apply our method to ranking and clustering real data (Table 3), as first shown in Siahkamari et al. (2020). For the ranking tasks, the test set is treated as queries for which the learned model retrieves items from the training set in order of increasing divergence. The ranking is scored using mean average precision (MAP) and area under ROC curve (AUC). Our method again outperforms the other Bregman learning methods in the large majority of datasets and metrics. We emphasize that these are standard experiments from recent work, on which our method proves superior.\n\n4.2 DIVERGENCE REGRESSION\n\nTruth\n\nEuclidean\n\nMahalanobis\n\nx log x\n\nKL\n\nCorrelation\n\nNone Med High None Med\n\nHigh None Med High None Med High\n\nNBD Deep-div Deepnorm Widenorm Mahalanobis\n\n0.17 7.78 3.56 3.56 0.00\n\n0.15 7.81 3.97 3.99 0.03\n\n0.16 7.84 4.15 4.12 0.05\n\n0.16 17.92 7.70 7.73 0.02\n\n0.18 12.26 5.97 6.01 0.04\n\n0.20 14.15 7.66 7.60 0.09\n\n0.52 2.59 1.59 1.49 1.45\n\n0.54 2.67 1.74 1.48 1.67\n\n0.57 2.70 1.79 1.48 1.72\n\n0.19 0.44 0.30 0.30 0.23\n\n0.19 0.50 0.28 0.28 0.22\n\n0.19 0.51 0.28 0.28 0.22\n\nTable 4: Regression test MAE when unused distractor features are correlated (None/Med/High) with the true/used features. Best results in bold, second best in italics. NBD performs best on asymmetric regression, and second-best to Mahalanobis on symmetric regression, where a Mahalanobis distance is expected to fit perfectly.\n\nAs a confirmation that our method can faithfully represent Bregman divergences, we use simulated data to demonstrate that our method efficiently learns divergences between pairs of inputs. We generate pairs of 20-dim. vectors from a Normal distribution, with 10 informative features used to compute the target divergence and 10 distractors. To be more challenging and realistic, we add various levels of correlations among all features to make the informative features harder to separate.\n\nThe following target divergences are used: (1) squared Euclidean distance (symmetric); (2) squared Mahalanobis distance (symmetric); (3) φ(x) = x log x (asymmetric); (4) KL-divergence (asymmetric). In this task we compare our NBD with Deep-div and Mahalanobis, but we did not find a regression option for PBDL in the authors’ code. Instead we add Deepnorm and Widenorm metrics from Pitis et al. (2020) as alternative baselines which do not learn Bregman divergences.\n\nThe results of these experiments are in Table 4, with loss curves shown in Appendix Fig. 4. In the symmetric cases of the Euclidean and Mahalanobis ground-truth, our NBD method performs nearly as well as using a Mahalanobis distance itself. This shows that our method is not losing any representational capacity in being able to represent these standard measures. This is notably not true for the prior approaches for asymmetric learning: Deepnorm, Widenorm, and Deep-div.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: MSE (y-axis) after epochs of training (x-axis) on asymmetric BregMNIST (left) and BregCIFAR (right) with true φ(x) = x log x. NBD performs best in both tasks.\n\nNotably, unlike in the clustering tasks, the piecewise representation of φ in Deep-div is unable to accurately represent Bregman regression targets, as discussed earlier in §2.1. In Fig. 4c and Fig. 4d two asymmetric divergences are used, and our NBD approach performs better than all existing options. Because these experiments isolate purely the issue of learning the divergence itself, we have strong evidence that our approach is the most faithful to learning a known divergence from a supervisory signal. Note that the Mahalanobis distance performed second best under all noise levels, meaning the prior asymmetric methods were in fact less accurate at learning asymmetric measures than a purely symmetric model.\n\n4.3 CO-LEARNING AN EMBEDDING WITH A DIVERGENCE\n\nHaving shown that our method outperforms the prior Bregman learning approaches on shallow clustering, classification, and regression, we introduce a more challenging task, BregMNIST, where a neural embedding must be learned along with the divergence metric. The dataset consists of paired MNIST images, with the target distance being a Bregman divergence between the digits shown in the images. Example pairs are displayed in Fig. 1 for the asymmetrical Bregman divergence parametrized by φ(x) = (x + 1) log(x + 1).\n\nWe also make a harder version by substituting MNIST with CIFAR10 with the same divergence labels. In both cases the relation of features to class label is arbitrary (that is, we impose an ordinal relation among labels that does not exist in the data), meaning that the embedding function must learn to effectively map image classes to the correct number used to compute the divergence, while the metric head must also learn to compute the target Bregman divergence. The results of the experiments (Fig. 2) mirror our results in §4.2. For both BregMNIST and BregCIFAR NBD performs best, while prior methods of learning asymmetric measures perform worse than the Mahalanobis distance.\n\n5 NON-BREGMAN LEARNING\n\nFigure 1: Demonstration of the BregMNIST task. Nodes with the same color indicate weight sharing. Each image is embedded by a CNN, and the ground-truth divergence is computed from the digit values of the input images. The embeddings of each image are given to NBD, and the loss is computed from NBD’s output and the true divergence. The CNN and NBD are learned jointly.\n\nWe have shown that our NBD method is the most effective among all available options when the underlying ground-truth is from the class of Bregman divergences. In this section we will now explore the effectiveness of our approach on tasks that are known to be non-Euclidean, but not necessarily representable by a Bregman divergence. The purpose of these experiments is to show that NBD does not depend on the underlying representation being a proper divergence in order to still be reasonably effective, and that it is still more effective then the prior Deep-div approach to Bregman learning. This is also of practical relevance to applications: just as the Euclidean metric was used for convenient properties and simplicity, without belief that the underlying system was truly Euclidean, our NBD\n\n7\n\n020401001010204056NBDDeep-divDeepnormWidenormMahalanobisCNNCNNNBD‘(ˆyNBD,y)abD(a,b)=4log(cid:18)46(cid:19)−(4−6)≈0.378Published as a conference paper at ICLR 2023\n\nmay be valuable for developing more flexible methods that inherit the mathematical convenience of Bregman divergences. These tasks probe the efficacy of the closest Bregman approximation of the underlying divergence, so we expect that our method will not surpass SOTA when the task is sufficiently non-Bregman.\n\n5.1 DEEP METRIC LEARNING\n\nWe extend the ranking experiments from §4.1 and the architecture of Fig. 1 to the deep metric learning setting where an embedding is learned alongside the divergence. We use a ResNet-18 as the base feature extractor and apply batch triplet mining to learn Eq. 3 by minimizing the triplet loss.\n\nCIFAR10\n\nSTL10\n\nSVHN\n\nNBD Deep-div Deepnorm Widenorm Euclidean\n\n95.6 93.3 95.0 95.6 95.0\n\n95.0 92.3 95.0 95.1 95.0\n\n96.9 96.0 96.9 96.9 96.9\n\nTable 5: MAP@10 on deep metric learning, replacing the standard Euclidean distance with a metric co-learned with the embedding.\n\nMost metrics perform comparably in this experiment, although Deep-div is consistently outperformed by the others. We observed that Deep-div has higher variance, and depending on the initialization could learn well or not at all. This may be due to ’dead‘ affine components discussed earlier. Fixing the Euclidean distance still appears as effective as learning the final metric here. We hypothesize this is because the embedding space of image datasets is well-behaved enough for a standard distance to accurately cluster images. In the following experiments we will investigate tasks where, even after applying an arbitrary feature extractor, a standard distance measure is no longer sufficient.\n\n5.2 APPROXIMATE SEMANTIC DISTANCE\n\nThe next task involves learning symmetric distances that do not follow the triangle inequality. We group the CIFAR10 classes into two categories: man-made and natural. Within each category we select an arbitrary exemplar class (car and deer in our experiment). We then assign proxy distances between classes to reflect semantic similarity: 0.5 within the same class, 2 between any non-exemplar class and its category exemplar, and 8 between non-exemplar classes within a category. Pairs from different categories are not compared. Besides disobeying the triangle inequality, the distance values do not reflect a known divergence and can be changed.\n\nLike BregCIFAR, we present pairs of images to the model, which simultaneously adjusts a neural embedding and learn a divergence function such that inter-class distances in the embedding space match the target values. This task is harder than the previous ones because it is not sufficient to learn a separable embedding for each class; the embeddings must additionally be arranged appropriately in a non-Euclidean space. The results in Table 6 indicate our method effectively learns distances that do not follow the triangle inequality. The Deep-div approach does second-best here due to the small space of valid outputs. The other approaches by limitation adhere to the triangle inequality and do not perform as well.\n\nMetric\n\nSame Unseen\n\nNBD Deep-div Deepnorm Widenorm Mahalanobis\n\n0.04 0.10 1.23 1.39 2.00\n\n3.52 4.13 4.18 4.50 4.56\n\nTable 6: MSE for CIFAR10 category semantic distance after 200 epochs. Our NBD performs the best on seen and unseen images.\n\n5.3 OVERLAP DISTANCE\n\nThe overlap distance task presents pairs of the same image or different images, but with different crops taken out. A horizontal and vertical cut are chosen uniformly at random from each image. When the crops are based on the same image, the asymmetrical divergence measure between images X and Y is the percent intersection area: D(X, Y ) = 1 . Otherwise the divergence is 1. We use the INRIA Holidays dataset (see Appendix G). The results can be found in Fig. 3, where we see NBD performs the second best of all options.\n\n|X∩Y | |X|\n\n−\n\nFigure 3: MSE (y-axis) for predicting the overlap D(X, Y ) between two image embeddings learned jointly with the underlying CNN.\n\n8\n\n0204060801000.10.20.3testMSENBDDeepnormWidenormDeep-divMahalanobisPublished as a conference paper at ICLR 2023\n\n3d\n\n3dd\n\noctagon\n\ntaxi\n\ntraffic\n\nMethod\n\nTrain\n\nTest\n\nTrain\n\nTest\n\nTrain\n\nTest\n\nTrain\n\nTest\n\nTrain\n\nTest\n\nNBD Deepnorm Mahalanobis Deep-div Widenorm Bregman-sqrt Bregman-GS\n\n4.34 4.97 4.45 695.97 4.49 5.94 4.50\n\n33.49 22.44 30.90 930.57 27.92 27.59 30.51\n\n19.91 34.40 24.99 589.13 25.76 27.70 23.78\n\n337.59 275.99 267.18 806.14 253.65 266.25 266.91\n\n4.67 4.81 6.82 879.94 5.17 8.62 7.26\n\n25.32 15.19 44.30 1046.08 23.46 40.18 43.13\n\n3.11 1.52 1.31 489.80 1.18 1.57 1.17\n\n66.27 20.31 18.32 625.16 16.20 19.02 16.71\n\n2.53 1.76 1.47 399.14 1.44 1.63 1.55\n\n12.03 5.27 5.60 618.94 5.21 5.23 5.49\n\nTable 7: Results of estimating distances on the shortest-path task. Triangle-inequality preserving deep and wide-norm are expected to perform best. Our NBD performs significantly better than previous Bregman learning approach Deep-div, and can be competitive with the triangle-inequality preserving methods. The gap between train and test loss shows the impact of triangle inequality helping to avoid over-fitting the observed sub-graph used for training. Dataset and experiment details in Appendix.\n\nWe observe that Widenorm performs better on this task, especially during the initial learning process, due to the fact that it is permitted to violate the positive definiteness property of norms: D(x, x) > 0. Thus the method learns to map a difference of zero between embeddings to some intermediate distance with lower MSE. This can be problematic in use cases where the definiteness is important.\n\n5.4 SHORTEST PATH LENGTH\n\nOur final task involves estimating the shortest path on a graph from one embedded node to another based on their distances to and from a set of landmark nodes. This task inherently favors the Widenorm and Deepnorm methods because they maintain the triangle inequality (i.e., no shortcuts allowed in shortest path), and so are expected to perform better than NBD. We reproduce the experimental setup of Pitis et al. (2020) closely with details in Appendix E.\n\nThe results for each method are shown in §5.4, which largely match our expectations. The triangleinequality preserving measures usually perform best, given the nature of the problem: any violation of the triangle inequality means the distance measure is “taking a shortcut” through the graph search space, and thus must be under-estimating the true distance to the target node. NBD and Deep-div, by being restricted to the space of Bregman divergences, have no constraint that prevents violating the triangle-inequality, and thus often under-estimate the true distances. Comparing the train and test losses help to further assess this behavior, as the training pairs can be overfit to an extent. We see that NBD effectively learns the asymmetric relationships between seen points despite underestimating the distance to new points.\n\nφ(y)\n\nφ(x)\n\n2 (cid:107)∇\n\n2 + 1 2\n(cid:107)\n\nTo further explore the degree to which the properties underlying Bregman divergences affect shortest path length learning, we introduce two extensions to NBD. The first is a soft modification encouraging the triangle inequality to be obeyed (Bregman-sqrt). The second has a hard constraint guaranteeing the triangle inequality (Bregman-GS) and is defined as Dgsb −\n2 y\n2 (Acharyya et al., 2013). Results are included in §5.4, with detail on the (cid:107) extensions in the Appendix. There is an inherent tradeoff between the two extensions as Bregmansqrt can be asymmetric but still does not require satisfying the triangle inequality, while Bregman-GS is symmetric but always satisfies the triangle inequality. We see that these two modifications to NBD are highly competitive with Deepnorm and Widenorm. Furthermore, the relative performance of each provides an indication of whether asymmetry or triangle inequality is more crucial to modeling a given dataset. These methods highlight that even when a given task is highly non-Bregman, NBD can be readily extended to relax or strengthen various assumptions to better model the data.\n\nφ (x, y) = Dφ(x, y) + Dφ(y, x) + 1\n\n− ∇\n\n2 (cid:107)\n\nx\n\n6 CONCLUSION\n\nTo enable future asymmetric modeling research, we have developed the Neural Bregman Divergence (NBD). NBD jointly learns a Bregman measure and a feature extracting neural network. We show that NBD learns divergences directly or indirectly when trained jointly with a network, and that NBD still learns effectively when the underlying metric is not a divergence, allowing effective use of our tool across a wide spectrum but retaining the nice properties of Bregman divergences.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nWe would like to thank the anonymous reviewers for their comments, questions, and suggestions. This material is based in part upon work supported by the National Science Foundation under Grant No. IIS-2024878, with some computation provided by the UMBC HPCF, supported by the National Science Foundation under Grant No. CNS-1920079. This material is also based on research that is in part supported by the Army Research Laboratory, Grant No. W911NF2120076, and by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either express or implied, of the Air Force Research Laboratory (AFRL), DARPA, or the U.S. Government.\n\nREFERENCES\n\nSreangsu Acharyya, Arindam Banerjee, and Daniel Boley. Bregman divergences and triangle\n\ninequality. In ICDM, pp. 476–484, 2013.\n\nBrandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In ICML, 2017.\n\nArindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering\n\nwith bregman divergences. Journal of machine learning research, 6(10), 2005.\n\nL.M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics, 7(3):200–217, jan 1967. ISSN 00415553. doi: 10.1016/0041-5553(67) 90040-7.\n\nBenjamin Charlier, Jean Feydy, Joan Alexis Glaunès, François-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1–6, 2021.\n\nPengwen Chen, Yunmei Chen, Murali Rao, et al. Metrics defined by bregman divergences. Commu-\n\nnications in Mathematical Sciences, 6(4):915–926, 2008.\n\nYize Chen, Yuanyuan Shi, and Baosen Zhang. Optimal control via neural networks: A convex\n\napproach. In ICLR, 2019.\n\nHatice Kubra Cilingir, Rachel Manzelli, and Brian Kulis. Deep divergence learning. In ICML, 2020.\n\nHarris Drucker and Y. Le Cun. Double backpropagation increasing generalization performance. In IJCNN, pp. 145–150. IEEE, 1991. ISBN 0-7803-0164-1. doi: 10.1109/IJCNN.1991.155328.\n\nRoy Frostig, Matthew Johnson, Dougal Maclaurin, Adam Paszke, and Alexey Radul. Decomposing\n\nreverse-mode automatic differentiation. In POPL, 2021.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nElad Hoffer and Nir Ailon. Deep metric learning using triplet network. In International workshop on\n\nsimilarity-based pattern recognition, pp. 84–92. Springer, 2015.\n\nChen Huang, Chen Change Loy, and Xiaoou Tang. Local similarity-aware deep feature embedding.\n\nIn NeurIPS, 2016.\n\nPrateek Jain, Brian Kulis, Jason V Davis, and Inderjit S Dhillon. Metric and kernel learning using a\n\nlinear transformation. The Journal of Machine Learning Research, 13:519–547, 2012.\n\nHervé Jegou, Douze Matthijs, and Cordelia Schmid. Hamming embedding and weak geometry\n\nconsistency for large scale image search–extended version–. 2008.\n\nBrian Kulis, Mátyás A Sustik, and Inderjit S Dhillon. Low-rank kernel learning with bregman matrix\n\ndivergences. Journal of Machine Learning Research, 10(2), 2009.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nBrian Kulis et al. Metric learning: A survey. Foundations and trends in machine learning, 5(4):\n\n287–364, 2012.\n\nKevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. In ECCV, 2020.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nSilviu Pitis, Harris Chan, Kiarash Jamali, and Jimmy Ba. An inductive bias for distances: Neural\n\nnets that respect the triangle inequality. arXiv preprint arXiv:2002.05825, 2020.\n\nAli Siahkamari, Xide Xia, Venkatesh Saligrama, David Castañón, and Brian Kulis. Learning to\n\napproximate a bregman divergence. In NeurIPS, volume 33, pp. 3603–3612, 2020.\n\nAli Siahkamari, Durmus Alp Emre Acar, Christopher Liao, Kelly L Geyer, Venkatesh Saligrama, and Brian Kulis. Faster algorithms for learning convex functions. In International Conference on Machine Learning, pp. 20176–20194. PMLR, 2022.\n\nLei Wu, Rong Jin, Steven CH Hoi, Jianke Zhu, and Nenghai Yu. Learning bregman distance functions\n\nand its application for semi-supervised clustering. In NeurIPS, 2009.\n\nEric P Xing, Andrew Y Ng, Michael I Jordan, and Stuart Russell. Distance metric learning with application to clustering with side-information. In NeurIPS, volume 15, pp. 12. Citeseer, 2002.\n\nDingyi Zhang, Yingming Li, and Zhongfei Zhang. Deep metric learning with spherical embedding.\n\nIn NeurIPS, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA COMPUTATIONAL DISCUSSION\n\nWe measure timing information of NBD as well as the benchmarks used in our experiments for a divergence learning task as in §4.2. The training time (forward and backward passes), inference time (forward pass), and pairwise distance matrix computation are collected. While the pairwise distance is not directly used in our experiments, it is commonly computed in metric learning applications such as clustering and classification. The data dimension size N D used here is characteristic of the size of many metric learning experiments, where the N N pairwise distance matrix can be stored on GPU memory, but the N D tensor with embedding dimension D does not fit: the fast but N\nnaive approach of flattening the matrix and passing as a N\n\nN -length batch does not work.\n\n×\n\n×\n\n×\n\n×\n\n×\n\nThe Mahalanobis method is naturally the fastest method and serves as a reasonable runtime lower bound. This is because the distance can be expressed a simple composition of a norm with a linear layer. The squared Euclidean norm can be simplified as D(x, y) = , which (cid:105) can be efficiently computed. We can similarly compute the Bregman divergences. For example, the squared Euclidean distance is equivalently written as D(x, y) = ,\n(cid:105) which is its Bregman divergence formulation. Though not necessary for our current experiments, the pairwise distances for larger batch sizes for the Mahalanobis and NBD can be readily implemented in PyKeOps Charlier et al. (2021). Thus the longer computational time for NBD can likely be attributed to the increased cost of double backpropagation and the convex metric architecture.\n\nx, y 2\n(cid:104)\n\n2 − y\n\n2 − (cid:107)\n\n2 − (cid:104)\n\n2y, x\n\n2 +\n\n2 (cid:107)\n\n2 (cid:107)\n\n2 (cid:107)\n\n2 (cid:107)\n\n−\n\nx\n\nx\n\n(cid:107)\n\n(cid:107)\n\n(cid:107)\n\ny\n\ny\n\nMethod\n\nTraining\n\nInference\n\nPairwise distance\n\nNBD Deep-div Deepnorm Widenorm Mahalanobis\n\n0.73 (0.08) 1.63 (0.11) 0.81 (0.07) 0.52 (0.04) 0.46 (0.03)\n\n0.10 (0.03) 0.12 (0.02) 0.09 (0.02) 0.08 (0.02) 0.08 (0.02)\n\n0.52 (0.06) 0.59 (0.03) 4.53 (0.03) 2.61 (0.03) 0.40 (0.03)\n\nTable 8: Timing information for a divergence learning task as in §4.2, with embedding dimension 20 and a batch size of 1000, comparing the methods used in our experiments. We compute the per-epoch training time (forward and backward passes), inference time (forward pass), and pairwise distance matrix computation. Results are averaged over 30 runs, with standard deviation in parentheses.\n\nOn the other hand, the Deepnorm cannot be vectorized in such manner, so pairwise distances need to be computed on the order of O(N 2) runtime, for example by further splitting the tensor into smaller mini-batches. While the Widenorm is composed of Mahalanobis metrics (set to 32 in our experiments) that can be vectorized, in our experiments the memory requirement was still too high, also requiring looping over sub-batches. We use sub-batch size 200 in this analysis. We note that the loop can be alternatively performed over the Mahalanobis components in Widenorm, but this would still be slower than the standard Mahalanobis and NBD methods. Finally, the Deep-div method efficiently computes pairwise distances, but its forward pass requires the input to be passed through a set of K affine sub-networks via looping, increasing the computational time.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nB FIGURES FOR BREGMAN REGRESSION TASK\n\n(a) Euclidean target\n\n(b) Mahalanobis target\n\n(c) x log x target\n\n(d) KL-div. target\n\nFigure 4: Results when vectors with 10 real features and 10 distractor features are used to compute different specific Bregman divergences. Mean absolute error is on the y-axis and number of training epochs on the x-axis. The shaded region around each plot shows the standard deviation of the results. Note in all cases our NBD has very low variance while effectively learning the target divergence.\n\n13\n\n020406080100051015testMAENBDDeepnormWidenormDeep-divMahalanobis02040608010001020testMAE02040608010012345testMAE510152025300123testMAEPublished as a conference paper at ICLR 2023\n\nC DATA GENERATION DETAILS\n\nDistributional clustering. We sample 1000 points uniformly into 5 clusters, each with 10 feature dimensions. To generate non-isotropic Gaussians we sample means uniformly in the hyper-box within 4, 4] for each coordinate. The variances are a random PSD matrix (scikit-learn make_spd_matrix) [\n− added with σ2I where σ2 = 5. The reason for these values are because we aimed to have each mixture task be similarly difficult (clusters not perfectly separable but also not too challenging). For the multinomial task, we sampled 100 counts into the 10 feature dimensions. Each cluster’s underlying probability distribution was sampled from Dirichlet([10, 10, . . . , 10]). Finally, the exponential case are iid samples for each feature. The underlying cluster rates are sampled uniformly between [0.1, 10].\n\nRegression noise features. To add correlation among features (both informative and distractors), we generate covariance matrices with controlled condition number κ while keeping the marginal (0, 1). For the medium correlation task κ < 100, while for distributions of each feature as xi high correlation κ is between 250 and 500.\n\n∼ N\n\n50000 pairs were generated with 20 features.\n\nD EXPERIMENTAL PROCEDURE\n\nOverview. Our experiments fall into two categories: regression (e.g. sections 4.2, 4.3, 5.2, 5.3, 5.4) and clustering/ranking (e.g. sections 4.1, 5.1). For the first category a scalar divergence target is assumed. Square error loss is used, and an input pair of samples is drawn from an underlying dataset or randomly generated. From the input pair, the true divergence target is computed. For example (N 2) possible pairs. Pairs are drawn simultaneously as if the dataset has N samples, there are batches with specified batch size, and an epoch (unless otherwise stated) is defined as the number of pairs equal to the size of the original dataset (so an epoch is the usual length, but not all data has necessarily been seen).\n\nO\n\nFor the second category, a divergence target does not exist, only the relative ranking of anchor-positive being less distance than anchor-negative. The triplet loss is used here. In all such metric learning tasks, we fit models using triplet mining, with margin 0.2, and Adam optimizer. All triplets with non-zero loss are mined from a batch. For more detail on triplet loss and mining see Musgrave et al. (2020). (B3) triplets. We iterate over the dataset in batches. The Thus for a batch size of B there are up to O\nexception is PBDL which uses the authors’ original code with pre-generated tuples.\n\nDistributional clustering. We used batch size 128, 200 epochs, 1e-3 learning rate for all models. Here, and in all subsequent experiments, to train PBDL we used the authors’ provided Python code, which uses the alternating direction method of multipliers (ADMM) technique. (They also provide Matlab code using Gurobi.)\n\nBregman ranking. Since Deep-div and NBD are deep learning approaches, we use Adam to optimize this problem instead of convex optimization solvers. To ensure convergence, we tune the learning rate and number of epochs using gridsearch over a validation set separated from the training data. We do the same for the Mahalanobis approach. A typical example of the parameters is batch size 256, 250 epochs, learning rate 1e-3.\n\nRegression. We used 100 epochs of training with learning rate 1e-3, batch size 1000.\n\nDeep regression experiments. For the remaining experiments which involve co-learning an embedding, we use default hyperparameter settings to keep methods comparable, such as Adam optimizer, learning rate 1e-3, batch size 128, embedding dimension 128, and 200 epochs. By deep regression, we refer to tasks that have a continuous target, such as BregMNIST, overlap distance, and shortest path.\n\nFor the MNIST/CIFAR tasks the embedding network consists of two/four convolutional layers respectively followed by two fully-connected layers (more specific details follow).For the semantic distance CIFAR task, we used a pretrained ResNet20 as the embedding without freezing any layers for faster learning He et al. (2016). Results were robust to the embedding model chosen.\n\nWe replicated each training and reported means and standard deviations. For the Bregman benchmark tasks we trained 20x, while for the deep learning/graph learning tasks we trained 5x. Learning curves\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nDataset Asymmetric\n\nDimension\n\nEdge weights\n\nDetails\n\n3d taxi\n\n3dd\n\ntraffic\n\noctagon\n\nNo No\n\nYes\n\nYes\n\nYes\n\n50x50x50 cubic grid 25x25x25x25 (two objects on 2d grid)\n\n50x50x50 cubic grid\n\n100x100 2d grid\n\n100x100 2d grid, diagonals connected\n\nuniform uniform\n\nuniform\n\n0.01, 0.02, . . . , 1.00 0.01, 0.02, . . . , 1.00\n\n0.01, 0.02, . . . , 1.00\n\n{ {\n\n{\n\n} }\n\n}\n\nforward and reverse sampled from Normal\n\nwith mean from uniform\n\n0.01, 0.02, . . . , 1.00\n\nforward and reverse sampled from Normal\n\nwith mean from uniform\n\n0.01, 0.02, . . . , 1.00\n\n{\n\n{\n\n}\n\n}\n\nedges wrap around no wrap only one edge in each dimension is available\n\nno wrap\n\nno wrap\n\nTable 9: Details of the shortest-path datasets, the number of dimensions in the graph, and how the edge weights in the graph are computed. These tasks were originally proposed by Pitis et al. (2020) and favor asymmetric methods that maintain the triangle inequality.\n\nin the figures show mean and 95% confidence interval for the loss over each epoch. We used Quadro RTX 6000 GPUs to train our models.\n\nDeep metric learning. Finally, this refers to the triplet loss experiment with co-learning of embedding and metric. For this we use the following settings: learning rate 1e-4, Adam optimizer, batch size 64, embedding dimension 32, 30 epochs. For all datasets that are 32x32, we resized to 224x224 and used a pretrained ResNet18 as the base embedding model, which is then finetuned during training. We used smaller learning rate and embedding/batch dimensions in keeping with the standard metric learning protocol in Musgrave et al. (2020) which we found gave more stable results.\n\nWe note that in Cilingir et al. (2020) they ran their Deep-div on a similar task and reported some results better and some worse than our results with Deep-div. We note that there are differences in protocol, where they extensively tuned the training procedure with hyperparmater optimization, whereas we selected default values to ensure robustness. However, we used a larger base extractor with pretrained weights whereas they used a custom CNN. As a result differences in performance can be expected. While our reported metric is MAP@10 and theirs was nearest neighbor accuracy, we found both measures to be very close across our experiments.\n\nE SHORTEST PATH DETAILS\n\nPitis et al. (2020) introduced learning shortest path length in graphs as a task. The problem consists of learning d(x, y) where x, y ). For each node, the predictive features consist of shortest distances from the node to a set of 32 landmark nodes (and vice versa for asymmetric graphs). As this task requires predicting the distance from one node to another, maintaining the triangle inequality has inherent advantages and is the correct inductive bias for the task. We still find the task useful to elucidate the difference between NBD and the prior Deep-div.\n\nare a pair of nodes in a large weighted graph G = (\n\n∈ V\n\nV\n\nE\n\n,\n\nWe reproduce the experimental setup of Pitis et al. (2020) closely, collecting a 150K random subset of pairs from the graph as the dataset, with true distances computed using A∗ search. While the original work normalized distances to mean 50, we found that such large regression outputs were difficult to learn. Instead we normalize to mean 1, which results in faster convergence of all methods. A 50K/10K (0, 0.2), and train-test split was used. The features are standardized with added noise sampled from 96 normal-distributed distractor features were included. For additional details refer to Appendix E of Pitis et al. (2020). However, their experimental detail and code was sufficient to only reproduce three of the graph datasets (3d, taxi, 3dd). Therefore, we develop two additional asymmetric graphs (traffic and octagon). The details of all the shortest-path graphs we use are provided in Table 9. Models were trained for 50 epochs at learning rate 5e-5 and 50 epochs at 5e-6.\n\nN\n\nE.1 EXTENSIONS OF BREGMAN DIVERGENCE FOR THE TRIANGLE INEQUALITY\n\nFor the first we draw from mathematical literature demonstrating that metrics can be induced from the square root of certain symmetrized Bregman divergences, depending on constraints on φ Chen et al. (2008). We learn the square root of the Bregman divergence to provide a soft inductive bias (as an illustrative example, Euclidean distance is a metric but squared Euclidean distance is not).\n\nFor the second we introduce a modification of the Bregman divergence known as the Generalized Symmetrized Bregman divergence. As shown by Acharyya et al. (2013), the square root of such a\n\n15\n\nPublished as a conference paper at ICLR 2023\n\ndivergence is guaranteed to satisfy the triangle inequality. This divergence is defined as Dgsb Dφ(x, y) + Dφ(y, x) + 1 φ(x)\n\nφ(y)\n\nx\n\ny\n\n2\n\nφ (x, y) =\n\n2 (cid:107)\n\n−\n\n(cid:107)\n\n2\n\n2 + 1\n\n2 (cid:107)∇\n\n− ∇\n\n2.\n\n(cid:107)\n\nThere is an inherent tradeoff between the two extensions as Bregman-sqrt can be asymmetric but still does not require satisfying the triangle inequality, while Bregman-GS is symmetric but always satisfies the triangle inequality.\n\nF ADDITIONAL HYPERPARAMETER DETAILS\n\nWe followed the hyperparameter specifications for the Deepnorm and Widenorm results as stated in Pitis et al. (2020). The Widenorm used 32 components with size 32, concave activation size 5, and max-average reduction. For the Deepnorm we used the neural metric version which gave the strongest performance in their paper: 3 layers with dimension 128, MaxReLU pairwise activations, concave activation size 5, and max-average reduction. We adapt their PyTorch code from https: //github.com/spitis/deepnorms. In the graph distance task, results show the same learning pattern (and relative performances between models) but the overall error magnitudes that we obtain are rather different than their reported results.\n\nWe re-implemented the Deep-div method following the description in Cilingir et al. (2020). The number of affine sub-networks stated in their paper varied but was generally set to low values such as 10, for the purpose of matching the number of classes in their classification tasks. In their appendix they experiment with increasing numbers of sub-networks and find best results at 50. For this reason we set 50 for our experiments. Following their paper, we use small FNNs for the max-affine components.\n\nOur NBD uses a 2 hidden layer FICNN with width 128 for φ. We found that our results are robust to the depth and width of the FICNN.\n\nF.1 ACTIVATION FOR FICNN\n\nWhile developing theory using the softplus directly is harder because of the non-linearity, we believe that the approximation error using softplus activations is upper bounded by the ReLU approximation error. This is because the softplus can be made arbitrarily close to ReLU by making α large in the operation gα(x) = log(1 + exp(αx))/α. As the α value could be learned in the affine components of the neural network, a ICNN with ReLU can be closely expressed as an ICNN with softplus. So we can simply substitute gα(x) for ReLU(x) in the original proofs, and therefore our optimal error is less than or equal to the ReLU error. (In particular when the true function has curvature then it will be strictly better than ReLU).\n\nG OVERLAP DETAILS\n\nWe use the INRIA Holidays dataset Jegou et al. (2008), which contains over 800 high-quality vacation pictures contributed by the original authors. Many consecutive-numbered images (e.g. 129800.jpg, 129801.jpg, 129802.jpg) are retakes of the same scene, which would interfere with assigning zero overlap to different images. To address this we only use images ending in 00.jpg, which are all different scenes. This leaves 300 images, which we resize to 72x72 then apply a 64x64 center-crop. The training set consists of 10, 000 pairs sampled with random crops each epoch from the first 200 of the images, while the test set is a fixed set of 10, 000 pairs with crops drawn from the last 100. We set a 25% chance for a given pair to come from different images and receive a divergence of 1. All models were trained with batch size 128, Adam optimizer with learning rate 5e-4, and embedding dimension of 128. The embedding network consists of four 3x3 convolutional layers (32, 64, 128, 256 filters respectively) with 2x2 max pooling layers followed by two linear layers with hidden dimension 256.\n\nWe compute overlap as the percent of non-intersecting area from the crops. We show this in Fig. 5.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Demonstration of how the overlap distance is computed in our setup. Ground-truth distance is the intersection of the shared images divided by the area of the first image.\n\nH BREGMAN RANKING AND CLUSTERING STANDARD DEVIATIONS\n\nFull version of Table 3. Standard deviations in small font, means in regular font.\n\nDataset\n\nModel\n\nMAP\n\nAUC\n\nPurity\n\nRand Index\n\nabalone\n\nbalance-scale\n\ncar\n\niris\n\ntransfusion\n\nwine\n\nDeep-div Euclidean Mahalanobis NBD PBDL Deep-div Euclidean Mahalanobis NBD PBDL Deep-div Euclidean Mahalanobis NBD PBDL Deep-div Euclidean Mahalanobis NBD PBDL Deep-div Euclidean Mahalanobis NBD PBDL Deep-div Euclidean Mahalanobis NBD PBDL\n\n0.281 0.01 0.301 0.01 0.310 0.01 0.316 0.01 0.307 0.01 0.804 0.03 0.611 0.01 0.822 0.01 0.887 0.01 0.836 0.02 0.787 0.01 0.681 0.00 0.787 0.01 0.820 0.01 0.798 0.01 0.945 0.03 0.827 0.02 0.946 0.03 0.957 0.02 0.943 0.03 0.648 0.01 0.666 0.01 0.680 0.01 0.695 0.01 0.637 0.01 0.983 0.02 0.844 0.02 0.949 0.02 0.969 0.02 0.978 0.02\n\n0.645 0.02 0.666 0.01 0.677 0.01 0.682 0.01 0.659 0.01 0.859 0.02 0.666 0.01 0.854 0.01 0.915 0.01 0.855 0.02 0.757 0.01 0.589 0.00 0.752 0.01 0.803 0.01 0.775 0.01 0.967 0.02 0.897 0.01 0.973 0.01 0.977 0.01 0.967 0.02 0.525 0.02 0.536 0.01 0.570 0.01 0.603 0.01 0.504 0.01 0.987 0.01 0.884 0.02 0.970 0.01 0.980 0.01 0.982 0.01\n\n0.377 0.02 0.422 0.03 0.419 0.02 0.432 0.03 0.386 0.02 0.869 0.02 0.633 0.06 0.851 0.06 0.898 0.02 0.872 0.02 0.852 0.04 0.704 0.02 0.778 0.02 0.860 0.01 0.854 0.01 0.811 0.16 0.820 0.07 0.884 0.12 0.909 0.10 0.889 0.14 0.756 0.03 0.748 0.03 0.750 0.03 0.756 0.03 0.748 0.03 0.953 0.08 0.902 0.07 0.944 0.10 0.960 0.05 0.820 0.14\n\n0.660 0.04 0.750 0.01 0.750 0.01 0.750 0.01 0.735 0.02 0.828 0.03 0.568 0.04 0.761 0.05 0.872 0.03 0.814 0.03 0.750 0.04 0.523 0.03 0.654 0.03 0.758 0.02 0.750 0.02 0.820 0.16 0.828 0.05 0.879 0.11 0.902 0.10 0.888 0.13 0.621 0.04 0.563 0.04 0.543 0.05 0.600 0.04 0.622 0.03 0.947 0.08 0.887 0.06 0.940 0.09 0.948 0.06 0.823 0.12\n\nTable 10: Across several real datasets, a learned Bregman divergence is superior to Euclidean or Mahalanobis metrics for downstream ranking (MAP, AUC) and clustering (Purity, Rand Index) tasks. Furthermore, our approach NBD consistently outperforms the prior Bregman learning approaches, Deep-div and PBDL, on most datasets. MAP = mean average precision, AUC = area under curve\n\n17\n\nCNNCNNCNNCNNNBD‘(ˆyNBD,y)abD(a,b)1−0.24=0.76=yD(b,a)ba1−1=0.0=yNBD‘(ˆyNBD,y)Published as a conference paper at ICLR 2023\n\nI FIGURE (BREGMNIST) INCLUDING SYMMETRIC CASE\n\n(a) squared Euclidean distance.\n\n(b) φ(x) = (x + 1) log(x + 1)\n\nFigure 6: MSE (y-axis) after epochs of training (x-axis), where NBD performs best in the symmetric (left) and asymmetric (center, right) Bregman learning tasks. We see Mahalanobis performs well in the symmetric task (left) since it is correctly specified for the ground truth, but performs relatively poorly in the asymmetric case (right). Deep-div is unable to learn effectively in either (not shown on left because the error was too high).\n\n18\n\n0102030405050100NBDDeepnormWidenormMahalanobis01020304050100101NBDDeepnormWidenormDeep-divMahalanobis",
    "reference": "# Summary Of The Paper\n\nThis paper provides a method for learning Bergman divergences using input convex neural networks as the convex generating function. This eliminates the shortcomings of previous work on the topic including non convexity of the neural network for deep divergence learning and high computational complexity of linear Bregman divergence learning. They further use CNN's as feature extractors and implement joint training. They experiment with various tasks such as regression, ranking and clustering and outperform the competing methods. Further they extend the Bregman divergence learning in two directions, one where the divergence satisfies both triangle inequality and symmetry and another where only triangle inequality is satisfied.\n\n# Strength And Weaknesses\n\nThe paper has many contributions and experiments. The idea of using input convex neural networks for learning Bergman divergences is new. The extensions to satisfy the triangle inequality and symmetry are also new. They have done extensive experimentation with different tasks and have compared to a significant number of competing methods. Further they have defined interesting divergence regression tasks such as BregMNIST and BregCifar.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe overall quality of the writing is good. There are many sections in the paper which looks misleading at first but has a good flow in general.\n\n# Summary Of The Review\n\nI think the paper has done enough work and has good quality for publication in the conference. Accept.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEXCESS RISK OF TWO-LAYER RELU NEURAL NETWORKS IN TEACHER-STUDENT SETTINGS AND ITS SUPERIORITY TO KERNEL METHODS\n\nShunta Akiyama Graduate School of Information Science and Technology, The University of Tokyo, Japan shunta akiyama@mist.i-tokyo.ac.jp\n\nTaiji Suzuki Graduate School of Information Science and Technology, The University of Tokyo, Japan Center for Advanced Intelligence Project, RIKEN, Japan taiji@mist.i.u-tokyo.ac.jp\n\nABSTRACT\n\nWhile deep learning has outperformed other methods for various tasks, theoretical frameworks that explain its reason have not been fully established. We investigate the excess risk of two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs. Especially, we consider the student network that has the same width as the teacher network and is trained in two phases: first by noisy gradient descent and then by the vanilla gradient descent. Our result shows that the student network provably reaches a near-global optimal solution and outperforms any kernel methods estimator (more generally, linear estimators), including neural tangent kernel approach, random feature model, and other kernel methods, in a sense of the minimax optimal rate. The key concept inducing this superiority is the nonconvexity of the neural network models. Even though the loss landscape is highly non-convex, the student network adaptively learns the teacher neurons.\n\n1\n\nINTRODUCTION\n\nExplaining why deep learning empirically outperforms other methods has been one of the most significant issues. In particular, from the theoretical viewpoint, it is important to reveal the mechanism of how deep learning trained by an optimization method such as gradient descent can achieve superior generalization performance. To this end, we focus on the excess risk of two-layer ReLU neural networks in a nonparametric regression problem and compare its rate to that of kernel methods. One of the difficulties in showing generalization abilities of deep learning is the non-convexity of the associated optimization problem Li et al. (2018), which may let the solution stacked in a bad local minimum. To alleviate the non-convexity of neural network optimization, recent studies focus on over-parameterization as the promising approaches. Indeed, it is fully exploited by (i) Neural Tangent Kernel (NTK) (Jacot et al., 2018; Allen-Zhu et al., 2019; Arora et al., 2019; Du et al., 2019; Weinan et al., 2020; Zou et al., 2020) and (ii) mean field analysis (Nitanda & Suzuki, 2017; Chizat & Bach, 2018; Mei et al., 2019; Tzen & Raginsky, 2020; Chizat, 2021; Suzuki & Akiyama, 2021).\n\nIn the setting of NTK, a relatively large-scale initialization is considered. Then the gradient descent related to parameters of neural networks can be reduced to the convex optimization in RKHS, which is easier to analyze. However, in this regime, it is hard to explain the superiority of deep learning because the estimation ability of the obtained estimator is reduced to that of the corresponding kernel. From this perspective, recent works focus on the “beyond kernel” type analysis Allen-Zhu & Li (2019); Bai & Lee (2020); Li et al. (2020); Chen et al. (2020); Refinetti et al. (2021); Abbe et al. (2022). Although their analysis shows the superiority of deep learning to kernel methods in each setting, in terms of the sample size (n), all derived bounds are essentially Ω(1/ n). This bound is known to be sub-optimal for regression problems Caponnetto & De Vito (2007).\n\n√\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nIn the mean field analysis setting, a kind of continuous limit of the neural network is considered, and its convergence to some specific target functions has been analyzed. This regime is more suitable in terms of a “beyond kernel” perspective, but it essentially deals with a continuous limit and hence is difficult to control the discretization error when considering a teacher network with a finite width. Indeed, the optimization complexity has been exploited recently in some research, but it still requires an exponential time complexity in the worst case (Mei et al., 2018b; Hu et al., 2019; Nitanda et al., 2021a). This problem is mainly due to the lack of landscape analysis that requires closer exploitation of the problem structure. For example, we may consider the teacher student setting where the true function is represented as a neural network. This allows us to use the landscape analysis in the optimization analysis and give a more precise analysis of the statistical performance. In particular, we can obtain a more precise characterization of the excess risk (e.g., Suzuki & Akiyama (2021)).\n\nMore recently, some studies have focused on the feature learning ability of neural networks (Abbe et al., 2021; 2022; Chizat & Bach, 2020; Ba et al., 2022; Nguyen, 2021). Among them, Abbe et al. (2021) considers estimation of the function with staircase property and multi-dimensional Boolean inputs and shows that neural networks can learn that structure through stochastic gradient descent. Moreover, Abbe et al. (2022) studies a similar setting and shows that in a high-dimensional setting, two-layer neural networks with sufficiently smooth activation can outperform the kernel method. However, obtained bound is still O(1/ n) and requires a higher smoothness for activation as the dimensionality of the Boolean inputs increases.\n\n√\n\nThe teacher-student setting is one of the most common settings for theoretical studies, e.g., (Tian, 2017; Safran & Shamir, 2018; Goldt et al., 2019; Zhang et al., 2019; Safran et al., 2021; Tian, 2020; Yehudai & Shamir, 2020; Suzuki & Akiyama, 2021; Zhou et al., 2021; Akiyama & Suzuki, 2021) to name a few. Zhong et al. (2017) studies the case where the teacher and student have the same width, shows that the strong convexity holds around the parameters of the teacher network and proposes a special tensor method for initialization to achieve the global convergence to the global optimal. However, its global convergence is guaranteed only for a special initialization which excludes a pure gradient descent method. Safran & Shamir (2018) empirically shows that gradient descent is likely to converge to non-global optimal local minima, even if we prepare a student that has the same size as the teacher. More recently, Yehudai & Shamir (2020) shows that even in the simplest case where the teacher and student have the width one, there exist distributions and activation functions in which gradient descent fails to learn. Safran et al. (2021) shows the strong convexity around the parameters of the teacher network in the case where the teacher and student have the same width for Gaussian inputs. They also study the effect of over-parameterization and show that overparameterization will change the spurious local minima into the saddle points. However, it should be noted that this does not imply that gradient descent can reach the global optima. Akiyama & Suzuki (2021) shows that the gradient descent with a sparse regularization can achieve the global optimal solution for an over-parameterized student network. Thanks to the sparse regularization, the global optimal solution can exactly recover the teacher network. However, this research requires a highly over-parameterized network. Indeed, it requires an exponentially large number of widths in terms of the dimensionality and the sample size. Moreover, they impose quite strong assumptions such that there is no observation noise and the parameter of each neuron in the teacher network should be orthogonal to each other.\n\nThe superiority of deep learning against kernel methods has also been discussed in the nonparametric statistics literature. They show the minimax optimality of deep learning in terms of excess risk. Especially a line of research (Schmidt-Hieber, 2020; Suzuki, 2018; Hayakawa & Suzuki, 2020; Suzuki & Nitanda, 2021; Suzuki & Akiyama, 2021) shows that deep learning achieves faster rates of convergence than linear estimators in several settings. Here, the linear estimators are a general class of estimators that includes kernel ridge regression, k-NN regression, and Nadaraya-Watson estimator. Among them, Suzuki & Akiyama (2021) treats a tractable optimization algorithm in a teacher-student setting, but they require an exponential computational complexity and smooth activation function, which does not include ReLU.\n\nIn this paper, we consider a gradient descent with two phases, a noisy gradient descent first and a vanilla gradient descent next. Our analysis shows that through this method, the student network recovers the teacher network in a polynomial order computational complexity (with respet to the sample size) without using an exponentially wide network, even though we do not need the strong assumptions such as the no-existence of noise and orthogonality. Moreover, we evaluate the excess risk of the trained network and show that the trained network can outperform any linear estimators,\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nincluding kernel methods, in terms of its dependence on the sample size. More specifically, our contributions can be summarized as follows:\n\n• We show that by two-phase gradient descent, the student network, which has the same width as the teacher network, provably reaches the near-optimal solution. Moreover, we conduct a refined analysis of the excess risk and provide the upper bound for the excess risk of the student network, which is much faster than that obtained by the generalization bound analysis with the Rademacher complexity argument. Throughout this paper, our analysis does not require the highly over-parameterization and any special initialization schemes.\n\n• We provide a comparison of the excess risk between the student network and linear estimators and show that while the linear estimators much suffer from the curse of dimensionality, the student network less suffers from that. Particularly, in high dimensional settings, the convergence rate of the excess risk of any linear estimators becomes close to O(n−1/2), which coincides with the classical bound derived by the Rademacher complexity argument.\n\n• The lower bound of the excess risk derived in this paper is valid for any linear estimator. The analysis is considerably general because the class of linear estimators includes kernel ridge regression with any kernel. This generality implies that the derived upper bound cannot be derived by the argument that uses a fixed kernel, including Neural Tangent Kernel.\n\n2 PROBLEM SETTINGS\n\nNotations For m ∈ N, let [m] := {1, . . . , m}. For x ∈ Rd, (cid:107)x(cid:107) denotes its Euclidean norm. We denote the inner product between x, y ∈ Rd by (cid:104)x, y(cid:105) = (cid:80)d j=1 xiyi. Sd−1 denotes the unit sphere in Rd. For a matrix W , we denote its operator and Frobenius norm by (cid:107)W (cid:107)2 and (cid:107)W (cid:107)F , respectively. Here, we introduce the problem setting and the model that we consider in this paper. We focus on a regression problem where we observe n training examples Dn = (xi, yi)n i=1 generated by the following model for an unknown measurable function f ◦ : Rd → R:\n\nyi = f ◦(xi) + (cid:15)i,\n\nwhere (xi)n i=1 is independently identically distributed sequence from PX that is the uniform distribution over Ω = Sd−1, and (cid:15)i are i.i.d. random variables satisfying E[(cid:15)i] = 0, E[(cid:15)2 i ] = v2, and |(cid:15)i| ≤ U a.s.. Our goal is to estimate the true function f ◦ through the training data. To this end, we consider the square loss (cid:96)(y, f (x)) = (y − f (x))2 and define the expected risk and the empirical risk as L(f ) := EX,Y [(cid:96)(Y, f (X)] and (cid:98)L(f ) := 1 n (cid:96)(yi, f (xi)), respectively. In this paper, we measure the performance of an estimator (cid:98)f by the excess risk L( (cid:98)f ) − L(f ). Since\n\ninf f :measurable\n\ninf L(f ) = L(f ◦) = 0, we can check that the excess risk coincides with (cid:107) (cid:98)f − f ◦(cid:107) L2(PX ), the L2distance between (cid:98)f and f ◦. We remark that the excess risk is different from the generalization gap L( (cid:98)f ) − (cid:98)L( (cid:98)f ). Indeed, when considering the convergence rate with respect to n, the generalization n) Wainwright (2019). On the other hand, the excess gap typically converges to zero with O(1/ √\nrisk can converge with the rate faster than O(1/\n\nn), which is known as fast learning rate.\n\n√\n\n2\n\n2.1 MODEL OF TRUE FUNCTIONS\n\nTo evaluate the excess risk, we introduce a function class in which the true function f ◦ is included. In this paper, we focus on the teacher-student setting with two-layer ReLU neural networks, in which the true function (called teacher) is given by\n\nfa◦,W ◦ (x) =\n\nm (cid:88)\n\nj=1\n\nj σ((cid:104)w◦ a◦\n\nj , x(cid:105)),\n\nwhere σ(u) = max{u, 0} is the ReLU activation, m is the width of the teacher model satisfying j ∈ Rd for j ∈ [m] are its parameters. We impose several conditions for the m ≤ d, and a◦ m) ∈ Rd×m and σ1 ≥ σ2 ≥ · · · ≥ σm parameters of the teacher networks. Let W ◦ = (w◦\n\nj ∈ R, w◦\n\n2 · · · w◦\n\n1 w◦\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nbe the singular values of W ◦. First, we assume that a◦ j ∈ {±1} for any j ∈ [m]. Note that by 1homogeneity of the ReLU activation1, this condition does not restrict the generality of the teacher networks. Moreover, we assume that there exists σmin > 0 such that σm > σmin. If σm = 0, there exists an example in which fa◦,W ◦ has multiple representations. Indeed, Zhou et al. (2021) shows that in the case a◦ j , x(cid:105)) = (cid:80)m j=1 σ((cid:104)−w◦ j , x(cid:105)). Hence, throughout this paper, we focus on the estimation problem in which the\n\nj = 0, it holds that fa◦,W ◦ = (cid:80)m\n\nj = 1 for all j ∈ [m] and (cid:80) w◦\n\nj=1 σ((cid:104)w◦\n\ntrue function is included in the following class:\n\nF ◦ := {fa◦,W ◦ | a◦ ∈ {±1}m, (cid:107)W ◦(cid:107)2 ≤ 1, σm > σmin}.\n\n(1)\n\nThis class represents the two-layer neural networks with the ReLU activation whose width is at most the dimensionality of the inputs. The constraint (cid:107)W ◦(cid:107)2 ≤ 1 is assumed only for the analytical simplicity and can be extended to any positive constants.\n\n3 ESTIMATORS\n\nIn this section, we introduce the classes of estimators: linear estimators and neural networks (student networks) trained by two-phase gradient descent. The linear estimator is introduced as a generalization of the kernel method. We will show separation between any linear estimator and neural networks by giving a suboptimal rate of the excess risk for the linear estimators (Theorem 4.1), which simultaneously gives separation between the kernel methods and the neural network approach. A detailed comparison of the excess risk of these estimators will be conducted in Section 4.\n\n3.1 LINEAR ESTIMATORS\n\nGiven observation (x1, y1), . . . , (xn, yn), an estimator (cid:98)f is called linear if it is represented by\n\n(cid:98)f (x) =\n\nn (cid:88)\n\ni=1\n\nyiφi(x1, . . . , xn, x),\n\nwhere (φi)n i=1 is a sequence of measurable and L2(PX )-integrable functions. The most important example in this study is the kernel ridge regression estimator. We note that the kernel ridge estimator i,j=1 ∈ Rn×n, k(x) = is given by (cid:98)f (x) = Y T(KX + λI)−1k(x), where KX = (k(xi, xj))n [k(x, x1), . . . , k(x, xn)]T ∈ Rn and Y = [y1, . . . , yn]T ∈ Rn for a kernel function k : Rd × Rd → R, which is linear to the output observation Y . Since this form is involved in the definition of linear estimators, the kernel ridge regression with any kernel function can be seen as one of the linear estimators. The choice of φi is arbitrary, and thus the choice of the kernel function is also arbitrary. Therefore, we may choose the best kernel function before we observe the data. However, as we will show in Theorem 4.1, it suffers from a suboptimal rate. Other examples include the k-NN estimator and the Nadaraya-Watson estimator. Thus our analysis gives a suboptimality of not only the kernel method but also these well-known linear estimators. Suzuki (2018); Hayakawa & Suzuki (2020) utilized such an argument to show the superiority of deep learning but did not present any tractable optimization algorithm.\n\n3.2 STUDENT NETWORKS TRAINED BY TWO-PHASE GRADIENT DESCENT\n\nWe prepare the neural network trained through the observation data (called student), defined by\n\nf (x; θ) =\n\nm (cid:88)\n\nj=1\n\najσ((cid:104)wj, x(cid:105)),\n\nwhere θ = ((a1, w1), . . . (am, wm)) ∈ R(d+1)m =: Θ. We assume that the student and teacher networks have the same width. Based on this formulation, we aim to train the parameter θ that will be provably close to that of the teacher network. To this end, we introduce the training algorithm, two-phase gradient descent, which we consider in this paper.\n\n1σ((cid:104)w, x(cid:105)) = (cid:107)w(cid:107)σ((cid:104)w/(cid:107)w(cid:107), x(cid:105)) for any w ∈ Rd/{0} and x ∈ Rd.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nPhase I: noisy gradient descent For r ∈ R, let ̄r := R · tanh (r|r|/2R) be a clipping of r, where R > 1 is a fixed constant. In the first phase, we conduct a noisy gradient descent with the weight decay regularization. The objective function used to train the student network is given as follows:\n\n(cid:98)Rλ(θ) :=\n\n1 2n\n\nn (cid:88)\n\ni=1\n\n(yi − f (xi; ̄θ))2 + λ\n\n|aj|2 + (cid:107)wj(cid:107)2(cid:17)\n\n,\n\nm (cid:88)\n\n(cid:16)\n\nj=1\n\nwhere ̄θ is the element-wise clipping of θ and λ > 0 is a regularization parameter. The parameter clipping ensures the bounded objective value and smoothness of the expected risk around the origin, which will be helpful in our analysis. Then, the parameters of the student network are updated by\n\nθ(k+1) = θ(k) − η(1)∇ (cid:98)Rλ\n\n(cid:115)\n\n(cid:16)\n\nθ(k)(cid:17)\n\n+\n\n2η(1) β\n\nζ (k),\n\nwhere η(1) > 0 is a step-size, (cid:8)ζ (k)(cid:9)∞ k=1 are independently identically distributed noises from the standard normal distribution, and β > 0 is a constant called inverse temperature. This type of noisy gradient descent is called gradient Langevin dynamics. It is known that by letting β be large, we can ensure that the smooth objective function will decrease. On the other hand, because of the non-smoothness of the ReLU activation, the objective function (cid:98)Rλ is also non-smooth. Hence it is difficult to guarantee the small objective value. To overcome this problem, we evaluate the expected one instead in the theoretical analysis, which is given by\n\nRλ(θ) :=\n\n1 2\n\n(cid:104)(cid:0)fa◦,W ◦ (x) − f (x; ̄θ)(cid:1)2(cid:105)\n\nEx\n\n+ λ\n\n|aj|2 + (cid:107)wj(cid:107)2(cid:17)\n\n.\n\nm (cid:88)\n\n(cid:16)\n\nj=1\n\nWe can ensure a small Rλ(θ) after a sufficient number of iterations (see Section 4.2 for the detail).\n\nPhase II: vanilla gradient descent After phase I, we can ensure that for each node of the student network, there is a node of the teacher network that is relatively close to each other. Then we move to conduct the vanilla gradient descent to estimate the parameters of the teacher more precisely. Before conducting the gradient descent, we rescale the parameters as follows:\n\nj ← sgn( ̄a(k)), w(k) a(k)\n\nj ←\n\n(cid:12) (cid:12)\n\n(cid:12) ̄a(k)(cid:12)\n\n(cid:12) (cid:12) ̄wj\n\n(k),\n\n∀j ∈ [m].\n\nWe note this transformation does not change the output of the student network thanks to the 1homogeneity of the ReLU activation. After that, we update the parameters of the first layer by\n\nW (k+1) = W (k) − η(2)∇W (cid:98)R\n\n(cid:16)\n\nW (k)(cid:17)\n\n,\n\nwhere η(2) > 0 is a step-size different from η(1) and (cid:98)R(W ) := 1 i=1(yi − f (xi; θ))2. In this phase, we no longer need to update the parameters of both layers. Moreover, the regularization term and the gradient noise added in phase I are also unnecessary. These simplifications of the optimization algorithm are based on the strong convexity of (cid:98)R(W ) around W ◦, the parameters of the teacher network. The analysis for this local convergence property is based on that of Zhang et al. (2019), and eventually, we can evaluate the excess risk of the student network.\n\n2n\n\n(cid:80)n\n\nThe overall training algorithm can be seen in Algorithm 1. In summary, we characterize the role of each phase as follows: in phase I, the student network explore the parameter space globally and finds the parameters that are relatively close to that of teachers, and in phase II, the vanilla gradient descent for the first layer outputs more precise parameters, as we analyze in Section 4.2. Remark 3.1. Akiyama & Suzuki (2021) also considered the convergence of the gradient descent in a teacher-student model. They considered a sparse regularization, (cid:80)m j=1 |aj|(cid:107)wj(cid:107), for the ReLU activation while we consider the L2-regularization given by (cid:80)m j=1(|aj|2 + (cid:107)wj(cid:107)2). These two regularizations are essentially the same since the minimum of the later regularization under the constraint of |aj|(cid:107)wj(cid:107) = const. is given by 2 (cid:80)m j=1 |aj|(cid:107)wj(cid:107) by the arithmetic-geometric mean relation. On the other hand, Akiyama & Suzuki (2021) consider a vanilla gradient descent instead of the noisy gradient descent. This makes it difficult to reach the local region around the optimal solution, and their analysis required an exponentially large width to find the region. We may use a narrow network in this paper with the same width as the teacher network. This is due to the ability of the gradient Langevin dynamics to explore the entire space and find the near global optimal solution.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Two-Phase Gradient Descent Input: max iteration k(1) and k(2), stepsize parameter η(1), η(2) > 0, regularization parameter\n\nλ > 0, inverse temperature β > 0.\n\n1: Initialization: θ (0) ∼ ρ0. 2: for k = 1, 2, . . . , k(1) do ζ (k) ∼ N (0, Im(d+1)) 3: θ(k+1) = θ(k) − η(1)∇ (cid:98)Rλ\n\n4: 5: end for 6: Reparameterization: a(k) 7: for k = k(1) + 1, k(1) + 2, . . . , k(2) do 8: W (k+1) = W (k) − η(2)∇W (cid:98)R(cid:0)W (k)(cid:1) 9: end for\n\nj = sgn( ̄a(k)\n\nj\n\n(cid:0)θ(k)(cid:1) +\n\n(cid:113) 2η(1)\n\nβ ζ (k)\n\n), w(k)\n\nj =\n\n(cid:12) (cid:12)\n\n(cid:12) ̄a(k)\n\nj\n\n(cid:12) (cid:12)\n\n(cid:12) ̄w(k)\n\nj\n\n4 EXCESS RISK ANALYSIS AND ITS COMPARISON\n\nThis section provides the excess risk bounds for linear estimators and the deep learning estimator (the trained student network). More precisely, we give its lower bound for linear estimators and upper bound for the student network. As a consequence, it will be provided that the student network achieves a faster learning rate and less hurt from a curse of dimensionality than linear estimators.\n\n4.1 MINIMAX LOWER BOUND FOR LINEAR ESTIMATORS\n\nHere, we analyze the excess risk of linear estimators and introduce its lower bound. More specifically, we consider the minimax excess risk over the class of linear estimators given as follows:\n\nRlin(F ◦) = inf\n\n(cid:98)f :linear\n\nsup f ◦∈F ◦\n\nEDn [(cid:107) (cid:98)f − f ◦(cid:107)2\n\nL2(PX )],\n\nwhere the infimum is taken over all linear estimators, and the expectation is taken for the training data. This expresses the infimum of worst-case error over the class of linear estimators to estimate a function class F ◦. Namely, any class of linear estimators cannot achieve a faster excess risk than Rlin(F ◦). Based on this concept, we provide our result about the excess risk bound for linear estimators. Under the definition of F ◦ by Eq. (1), we can obtain the lower bound as follows: Theorem 4.1. For arbitrary small κ > 0, we have that\n\nRlin(F ◦) (cid:38) n− d+2\n\n2d+2 n−κ.\n\nThe proof can be seen in Appendix A. This theorem implies that under d ≥ 2, the convergence rate of excess risk is at least slower than n− 2+2 2d+2 → −1/2 as d → ∞, the convergence rate of excess risk will be close to n−1/2 in high dimensional settings, which coincides with the generalization bounds derived by the Rademacher complexity argument. Hence, we can conclude that the linear estimators suffer from the curse of dimensionality.\n\n2·2+2 = n−2/3. Moreover, since − d+2\n\nThe key strategy to show this theorem is the following “convex-hull argument” given as follows:\n\nRlin(F ◦) = Rlin(conv(F ◦)),\n\nwhere conv(F ◦) := {(cid:80)N j=1 λj = 1} and conv(·) is the closure of conv(·) in L2(PX ). By combining this argument with the minimax optimal rate analysis exploited in Zhang et al. (2002) for linear estimators, we obtain the rate in Theorem 4.1.\n\nj=1 λjfj | N ∈ N, fj ∈ F ◦, λj ≥ 0, (cid:80)N\n\nThis equality implies that the linear estimators cannot distinguish the original class F ◦ and its convex hull conv(F ◦). Therefore, if the function class F ◦ is highly non-convex, then the linear estimators result in a much slower convergence rate since conv(F ◦) will be much larger than that of the original class F ◦. Indeed, we can show that the convex hull of the teacher network class is considerably larger than the original function class, which causes the curse of dimensionality. For example, the mean of two teacher networks with a width m can be a network with width 2m, which shows that conv(F ◦) can consist of much wider networks. See Appendix A for more details.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.2 EXCESS RISK OF THE NEURAL NETWORKS\n\nHere, we give an upper bound of the excess risk of the student network trained by Algorithm 1. The main result is shown in Theorem 4.6, which states that the student network can achieve the excess risk with O(n−1). This consequence is obtained by three-step analysis. First, (1) we provide a convergence guarantee for phase I and phase II in Algorithm 1. We first show that by phase I, the value of Rλ(θ(k)) will be sufficiently small (see Proposition 4.3). Then, (2) we can show that the parameters of the student network and the teacher networks are close to each other by Proposition 4.4. By using the strong convexity around the parameters of the teacher network, the convergence of phase II is ensured. By combining these result, (3) we get the excess risk bound as Theorem 4.6.\n\n(1) Convergence in phase I: First, we provide a convergence result and theoretical strategy of the proof for phase I. Since the ReLU activation is non-smooth, the loss function (cid:98)Rλ(·) is also nonsmooth. Therefore it is difficult to ensure the convergence of the gradient Langevin dynamics. To overcome this problem, we evaluate the value of Rλ(·) instead by considering the update θ(k+1) = θ(k) − η(1)∇Rλ β ζ (k), and bound the residual due to using the gradient of (cid:98)Rλ(·). This update can be interpreted as the discretization of the following stochastic differential equation:\n\n(cid:0)θ(k)(cid:1) +\n\n(cid:113) 2η(1)\n\ndθ = −β∇Rλ(θ)dt +\n\n2dBt,\n\n√\n\nwhere (Bt)t≥0 is the standard Brownian motion in Θ(= R(d+1)m). It is known that this process has a unique invariant distribution π∞ that satisfies dπ∞ dθ (θ) ∝ exp(−βRλ(θ)). Intuitively, as β → ∞, this invariant measure concentrates around the minimizer of Rλ. Hence, by letting β sufficiently large, obtaining a near-optimal solution will be guaranteed.\n\nSuch a technique for optimization is guaranteed in recent works (Raginsky et al., 2017; Erdogdu et al., 2018). However, as we stated above, they require a smooth objective function. Therefore we cannot use the same technique here directly. To overcome this difficulty, we evaluate the difference between ∇ (cid:98)Rλ and ∇Rλ as follows: Lemma 4.2. There exists a constant C > 0 such that with probability at least 1 − δ, it holds that\n\nVgrad := sup\n\nθ\n\n(cid:13) (cid:13) (cid:13)∇Rλ(θ) − ∇ (cid:98)Rλ(θ)\n\n(cid:114)\n\n(cid:13) (cid:13) ≤ CR3m (cid:13)\n\nd log(mdn/δ) n\n\n.\n\nThis lemma implies that with high probability, the difference between ∇ (cid:98)Rλ and ∇Rλ will vanish as n → ∞. Thanks to this lemma, we can connect the dynamics of the non-smooth objective with that of the smooth objective and import the convergence analysis developed so far in the smooth objective. In particular, we utilize the technique developed by Vempala & Wibisono (2019) (see Appendix C for more details). We should note that our result extends the existing one Vempala & Wibisono (2019) in the sense that it gives the convergence for the non-differential objective function (cid:98)Rλ(·). As a consequence, we obtain the following convergence result as for phase I. Proposition 4.3. Let R∗ λ be the minimum value of Rλ in Θ, q be a density function of π∞ (i.e., q(θ) ∝ exp(−βRλ(θ))) and Hq(p) := (cid:82) R p(θ) log p(θ ) q(θ ) dx be the KL-divergence. There exists a constant c, C > 0 and the log-Sobolev constant α (defined in Lemma C.4) such that with step-size 0 < η(1) < c\n\niteration, the output θ(k) satisfies\n\nβR3m3d , after k(1) ≥ β\n\nδλα\n\nE[Rλ(θ(k))] − R∗\n\nλ ≤ C\n\nαη(1) log 2Hq(ρ0) (cid:34)\n\nδ\n\n(λ + m) exp(cid:0)m2β(cid:1)\n\n(cid:114)\n\nδ +\n\n1 3nλ\n\n+\n\nd 2β\n\nlog\n\n(cid:18) m3dβ λ\n\n(cid:19)(cid:35)\n\nwith probability at least 1 − δ, where the expectation is taken over the initialization and Gaussian random variables added in the algorithm.\n\nTherefore, we can see that phase I optimization can find a near optimal solution with a polynomial time complexity (with respect to n) even though the objective function is non-smooth. It also may be considered to use the gradient Langevin dynamics to reach the global optimal solution by using higher β. However, it requires increasing the inverse temperature β exponentially related to n and other parameters, which leads to exponential computational complexity. To overcome this difficulty,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nwe utilize the local landscape of the objective function. We can show the objective function will be strongly convex around the teacher parameters and we do not need to use the gradient noise and any regularization. Indeed, we can show that the vanilla gradient descent can reach the global optimal solution in phase II, as shown in the following.\n\n(2) Convergence in phase II: Next, we prove the convergence guarantee of phase II and provide an upper bound of the excess risk. The convergence result is based on the fact that when Rλ(θ) is small enough (guaranteed in Proposition 4.3), the parameters of the student network will be close to those of the teacher network, as the following proposition: Proposition 4.4. There exists a threshold (cid:15)0 = poly(m−1, σmin) such that by letting λ ≤ (cid:15)0/m, if (cid:15) = Rλ(θ)−R∗ λ ≤ (cid:15)0, it holds that for every j ∈ [m], there exists kj ∈ [m] such that sgn(akj ) = a◦ (cid:12) and (cid:13) (cid:12)akj (cid:13)\n\n(cid:13) (cid:13) ≤ cσm/κ3m3.\n\n(cid:12) (cid:12)wkj − w◦\n\nj\n\nj\n\nThe proof of this proposition can be seen in Appendix D. We utilize the technique in Zhou et al. (2021), which give the same results to the cases when the activation is the absolute value function. In this proposition, we compare the parameters of the teacher network with the normalized student parameters. This normalization is needed because of the 1-homogeneity of the ReLU activation. (cid:13) (cid:12) The inequality (cid:13) (cid:13) ≤ cσm/κ3m3 ensures the closeness of parameters in the sense of (cid:12)akj (cid:13) the direction and the amplitude. Combining this with the equality sgn(akj ) = a◦ j , we can conclude the closeness and move to ensure the local convergence. Thanks to this closeness and local strong convexity, we can ensure the convergence in phase II as follows: Lemma 4.5. Let κ := σ1/σm and ̃σ := ((cid:81)m tion 4.4 holds. Then there exists absolute constants c1, c2, c3, c4, c5 such that under\n\nm. Suppose that the condition is Proposi-\n\n(cid:12) (cid:12)wkj − w◦\n\nj=1 σj)/σm\n\nj\n\nn ≥\n\nc1κ10m9d σm\n\nlog\n\n(cid:19)\n\n(cid:18) κmd σm\n\n· (cid:0)(cid:107)W ∗(cid:107)2\n\nF + v2(cid:1),\n\nthe output of the gradient descent with step-size η ≤ 1\n\nc2κm2 satisfies\n\n(cid:107) (cid:98)f − f ◦(cid:107)2\n\nL2(PX )\n\n(cid:46) ̃σ2σ−4\n\nminm5 log n\n\nn\n\n(cid:16)\n\n1 −\n\n+ c4\n\n(cid:17)k\n\nc3η ̃σκ2\n\n·\n\nσm κ3m2\n\nafter k iterations with probability at least 1 − c5d−10.\n\n(3) Unified risk bound: By combining (1) and (2), we obtain a unified result as follows: Theorem 4.6. There exists (cid:15)0 = poly(m−1, σmin) and constants C and C (cid:48) > 0 such that for any b m2(cid:1) where σb := b(cid:15)0, let k(1) = Cλ−2β−1 exp(cid:0)m2β(cid:1) and 0 < b < 1, under n ≥ λσ−3 k(2) = k(1) + log(cid:0)C (cid:48)nη(2)−2(cid:1), then the output of Algorithm 1 with λ = σbd−1, β = Ω(σ−1 b d), η(1) = O(λσ3\n\nb m2(cid:1)) and η(2) = O(σminm−2) satisfies\n\nb exp(cid:0)σ−1\n\nexp(cid:0)σ−1\n\nb\n\n(cid:107) (cid:98)f − f ◦(cid:107)2\n\nL2(PX )\n\n(cid:46) ̃σ2σ−4\n\nminm5 log n\n\nn\n\nwith probability at least 1 − b − d−10, where ̃σ = ((cid:81)m\n\nj=1 σj)/σm m.\n\nThe proof of this theorem also can be seen in Appendix D. This theorem implies that for fixed m, the excess risk of the student networks is bounded by\n\nEDn [(cid:107) (cid:98)f − f ◦(cid:107)\n\nL2(PX )] (cid:46) n−1.\n\n2\n\nAs compared to the lower bound derived for linear estimators in Theorem 4.1, we get the faster rate n−1 in terms of the sample size. Moreover, the dependence of the excess risk on the dimensionality d does not appear explicitly. Therefore we can conclude that the student network less suffers from the curse of dimensionality than linear estimators. As we pointed out in the previous subsection, the convex hull argument causes the curse of dimensionality for linear estimators since they only prepare a fixed basis. On the other hand, the student network can “find” the basis of the teacher network via noisy gradient descent in phase I and eventually avoid the curse of dimensionality.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Convergence of the training loss and test loss.\n\nRemark 4.7. Akiyama & Suzuki (2021) establishes the local convergence theory for the student wider than the teacher. However, it cannot apply here since they only consider the teacher whose parameters are orthogonal to each other. Suzuki & Akiyama (2021) also showed the benefit of the neural network and showed the superiority of deep learning in a teacher-student setting where the teacher has infinite width. They assume that the teacher has decaying importance; that is, the teacher can be written as f ◦(x) = (cid:80)∞ (cid:46) j−b (with an exponent a, b > 0) for a bounded smooth activation σ. Our analysis does not assume the decay of importance, and the activation function is the non-differential ReLU function. Moreover, Suzuki & Akiyama (2021) considers a pure gradient Langevin dynamics instead of the two-stage algorithm, which results in the exponential computational complexity in contrast to our analysis.\n\nj , x(cid:105)) where a◦\n\n(cid:46) j−a and w◦\n\nj σ((cid:104)w◦\n\nj=1 a◦\n\nj\n\nj\n\n5 NUMERICAL EXPERIMENT\n\nj = −1 for 26 ≤ j ≤ 50 and (w◦\n\nIn this section, we conduct a numerical experiment to justify our theoretical results. We apply Algorithm 1 to the settings d = m = 50. For the teacher network, we employ a◦ j = 1 for 1 ≤ j ≤ 25, a◦ 50) = I50 as its parameters. The parameters of the student network are initialized by θ(0) ∼ N (0, Im(d+1)). We use the sample with n = 1000 as the training data. Hyperparameters are set to η(1) = η(2) = 0.01, β = 100, λ = 0.01, k(1) max = 1000 and k(2) max = 2000. Figure 1 shows the experimental result. The orange line represents the training loss without the regularization term. The blue line represents the test loss. Since we can compute the generalization error analytically (see Appendix B), we utilize its value as the test loss.\n\n1, . . . , w◦\n\nWe can see that in phase I, both the training and test losses decrease first and then fall flat. At the beginning of phase II, we can observe that both the training and test losses decrease linearly. This reflects the strong convexity around the parameters of the teacher network, as we stated in the convergence guarantee of phase II. While the training loss keeps going up and down, the curve of the test loss is relatively smooth. This difference is due to the smoothness of the generalization loss (or Rλ), which we use in the convergence analysis in phase I. The test loss does not keep decreasing and converges to a constant. The existence of the sample noise causes this phenomenon: even if the parameters of the student coincide with that of the teacher, its training loss will not be zero. Thus we can say that the numerical experiment is consistent with our theoretical results.\n\n6 CONCLUSION\n\nIn this paper, we focus on the nonparametric regression problem, in which a true function is given by a two-layer neural network with the ReLU activation, and evaluate the excess risks of linear estimators and neural networks trained by two-phase gradient descent. Our analysis revealed that while any linear estimator suffers from the curse of dimensionality, deep learning can avoid it and outperform linear estimators, which include the neural tangent kernel approach, random feature model, and other kernel methods. Essentially, the non-convexity of the model induces this difference.\n\n9\n\nphaseI(noisy gradient descent)phaseII(vanilla gradient descent)Published as a conference paper at ICLR 2023\n\nREFERENCES\n\nEmmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. In Advances in Neural Information Processing Systems, volume 34, pp. 26989–27002. Curran Associates, Inc., 2021.\n\nEmmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. arXiv preprint arXiv:2202.08658, 2022.\n\nShunta Akiyama and Taiji Suzuki. On learnability via gradient method for two-layer relu neural networks in teacher-student setting. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 152–162. PMLR, 2021.\n\nZeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? In\n\nAdvances in Neural Information Processing Systems, pp. 9017–9028, 2019.\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.\n\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019.\n\nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Highdimensional asymptotics of feature learning: How one gradient step improves the representation. arXiv preprint arXiv:2205.01445, 2022.\n\nYu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of In International Conference on Learning Representations, 2020. URL\n\nwide neural networks. https://openreview.net/forum?id=rkllGyBFPH.\n\nD. Bakry and M. ́Emery. Diffusions hypercontractives. In S ́eminaire de Probabilit ́es XIX 1983/84,\n\npp. 177–206. Springer, 1985.\n\nAlon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with Gaussian inputs. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 605–614. PMLR, 2017.\n\nT Tony Cai, Jianqing Fan, and Tiefeng Jiang. Distributions of angles in random packing on spheres.\n\nJournal of Machine Learning Research, 14:1837–1864, 2013.\n\nAndrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.\n\nFoundations of Computational Mathematics, 7(3):331–368, 2007.\n\nTowards understanding hierarchical\n\nMinshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard representaIn Advances in Neural Information Processing Systems, volume 33, pp. 22134– URL https://proceedings.neurips.cc/paper/2020/file/\n\nSocher. tions. 22145, 2020. fb647ca6672b0930e9d00dc384d8b16f-Paper.pdf.\n\nlearning: Benefits of neural\n\nLenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent. Mathe-\n\nmatical Programming, pp. 1–46, 2021.\n\nLenaic Chizat and Francis Bach. On the global convergence of gradient descent for overIn Advances in Neural Information Processing\n\nparameterized models using optimal transport. Systems, volume 31, pp. 3036–3046, 2018.\n\nLenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on Learning Theory, pp. 1305–1338. PMLR, 2020.\n\nYoungmin Cho and Lawrence Saul. Kernel methods for deep learning.\n\nIn Advances in Neural\n\nInformation Processing Systems, volume 22, 2009.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International Conference on Machine Learning, pp. 1675– 1685, 2019.\n\nMurat A Erdogdu, Lester Mackey, and Ohad Shamir. Global non-convex optimization with discretized diffusions. In Advances in Neural Information Processing Systems 31, pp. 9671–9680, 2018.\n\nSebastian Goldt, Madhu Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborov ́a. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup. In Advances in Neural Information Processing Systems, pp. 6981–6991, 2019.\n\nSatoshi Hayakawa and Taiji Suzuki. On the minimax optimality and superiority of deep neural\n\nnetwork learning over sparse parameter spaces. Neural Networks, 123:343–361, 2020.\n\nRichard Holley and Daniel W Stroock. Logarithmic sobolev inequalities and stochastic ising models.\n\nJournal of Statistical Physics, 46:1159–1194, 1987.\n\nKaitong Hu, Zhenjie Ren, David Siska, and Lukasz Szpruch. Mean-field langevin dynamics and\n\nenergy landscape of neural networks. arXiv preprint arXiv:1905.07769, 2019.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, pp. 8571– 8580, 2018.\n\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landIn Advances in Neural Information Processing Systems, pp. 6389–6399,\n\nscape of neural nets. 2018.\n\nYuanzhi Li, Tengyu Ma, and Hongyang R. Zhang. Learning over-parametrized two-layer neural networks beyond NTK. In Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research, pp. 2613–2682. PMLR, 2020.\n\nSong Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses.\n\nThe Annals of Statistics, 46(6A):2747–2774, 2018a. ISSN 00905364, 21688966.\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018b.\n\nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pp. 2388– 2464. PMLR, 2019.\n\nPhan-Minh Nguyen. Analysis of feature learning in weight-tied autoencoders via the mean field\n\nlens. arXiv preprint arXiv:2102.08373, 2021.\n\nAtsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for infinite ensembles. arXiv\n\npreprint arXiv:1712.05438, 2017.\n\nAtsushi Nitanda, Denny Wu, and Taiji Suzuki. Particle dual averaging: Optimization of mean field neural networks with global convergence rate analysis. In Advances in Neural Information Processing Systems, volume 34. Curran Associates, Inc., 2021a. to appear.\n\nAtsushi Nitanda, Denny Wu, and Taiji Suzuki. Particle dual averaging: Optimization of mean field neural network with global convergence rate analysis. Advances in Neural Information Processing Systems, 34, 2021b.\n\nMaxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pp. 1674–1703. PMLR, 2017.\n\nMaria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov ́a. Classifying highdimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In International Conference on Machine Learning, pp. 8936–8947. PMLR, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nItay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural net-\n\nworks. In International Conference on Machine Learning, pp. 4433–4441. PMLR, 2018.\n\nItay M Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the optimization landscape of shallow relu neural networks. In Conference on Learning Theory, pp. 3889–3934. PMLR, 2021.\n\nJohannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU acti-\n\nvation function. The Annals of Statistics, 48(4):1875–1897, 2020.\n\nTaiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality. In International Conference on Learning Representations, 2018.\n\nTaiji Suzuki and Shunta Akiyama. Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods. In International Conference on Learning Representations, 2021.\n\nTaiji Suzuki and Atsushi Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space. In Advances in Neural Information Processing Systems, volume 34, pp. 3609–3621, 2021.\n\nYuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 3404–3413, 2017.\n\nYuandong Tian. Student specialization in deep rectified networks with finite width and input dimension. In Proceedings of the 37th International Conference on Machine Learning, volume 119, pp. 9470–9480. PMLR, 2020.\n\nBelinda Tzen and Maxim Raginsky. A mean-field theory of lazy training in two-layer neuarXiv preprint\n\nral nets: entropic regularization and controlled McKean-Vlasov dynamics. arXiv:2002.01987, 2020.\n\nSantosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. Advances in neural information processing systems, 32:8094–8106, 2019.\n\nMartin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/ 9781108627771.\n\nE Weinan, Chao Ma, and Lei Wu. A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics. Science China Mathematics, pp. 1–24, 2020.\n\nGilad Yehudai and Ohad Shamir. Learning a single neuron with gradient methods. In Proceedings\n\nof the 33rd Conference on Learning Theory, volume 125, pp. 3756–3786, 2020.\n\nShuanglin Zhang, Man-Yu Wong, and Zhongguo Zheng. Wavelet threshold estimation of a regres-\n\nsion function with random design. Journal of multivariate analysis, 80(2):256–284, 2002.\n\nXiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu networks via gradient descent. In Proceedings of Machine Learning Research, volume 89, pp. 1524–1534. PMLR, 2019.\n\nKai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In International conference on machine learning, pp. 4140–4149. PMLR, 2017.\n\nMo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-\n\nlayer neural network. In Conference on Learning Theory, pp. 4577–4632. PMLR, 2021.\n\nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-\n\nparameterized deep ReLU networks. Machine Learning, 109(3):467–492, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA PROOF OF THEOREM 4.1\n\nFirst, we introduce the formal statement of the “convex hull argmunent” we stated in Section 4.1.\n\nProposition A.1 (Hayakawa & Suzuki (2020)). The minimax optimal rate of linear estimators on a target function class F ◦ is the same as that on the convex hull of F ◦:\n\nRlin(F ◦) = Rlin(conv(F ◦)),\n\nwhere conv(F ◦) := {(cid:80)N closure of conv(·) in L2(PX ).\n\nj=1 λjfj | N ∈ N, fj ∈ F ◦, λj ≥ 0, (cid:80)N\n\nj=1 λj = 1} and conv(·) is the\n\nFor the proof, we use this convex hull argument and the minimax optimal rate analysis for linear estimators developed by Zhang et al. (2002). They essentially showed the following statement in their Theorem 1. Note that they consider the class of linear estimators on the Euclidean space, but we can apply the same argument for the class of linear estimators on Sd−1. Proposition A.2 (Theorem 1 of Zhang et al. (2002)). Let μ be uniform measure on Sd−1 satisfying μ(Sd−1) = 1. Suppose that the space Ω has even partition A such that |A| = 2K for an integer K ∈ N, each A ∈ A has measure α12−K ≤ μ(A) ≤ α22−K for constants α1, α2 > 0, and A is indeed a partition of Ω, i.e., ∪A∈AA = Ω, A ∩ A(cid:48) = ∅ for A, A(cid:48) ∈ A and A (cid:54)= A(cid:48). Then, if K is chosen as n−γ1 ≤ 2−K ≤ n−γ2 for constants γ1, γ2 > 0 that are independent of n, then there exists an event E such that, for a constant C (cid:48) > 0,\n\nP (E) ≥ 1 − o(1) and |{xi | xi ∈ A (i ∈ {1, . . . , n})}| ≤ C (cid:48)α2n2−K (∀A ∈ A).\n\nMoreover, suppose that, for a class F ◦ of functions on Ω, there exists ∆ > 0 that satisfies the following conditions:\n\n1. There exists F > 0 such that, for any A ∈ A, there exists g ∈ F ◦ that satisfies g(x) ≥\n\n1\n\n2 ∆F for all x ∈ A,\n\n2. There exists K (cid:48) and C (cid:48)(cid:48) > 0 such that 1 n\n\nthe event E.\n\n(cid:80)n\n\ni=1 g(xi)2 ≤ C (cid:48)(cid:48)∆22−K(cid:48)\n\nfor any g ∈ F ◦ on\n\nThen, there exists a constant F1 such that at least one of the following inequalities holds:\n\n2K(cid:48) n\n\n≤ Rlin(F ◦),\n\n∆22−K ≤ Rlin(F ◦),\n\nF 2 4F1C (cid:48)(cid:48) F 3 32\n\nfor sufficiently large n. Lemma A.3. Let 0 < ∆ ≤ 1/2 and let g : Sd−1 → R be a function defined by\n\ng(x) =\n\n1 d − 1\n\nd (cid:88)\n\nj=2\n\n(cid:20)\n\n−σ(xj) +\n\n1 2\n\nσ(xj + 2∆ · x1) +\n\nσ(xj − 2∆ · x1)\n\n(cid:21) .\n\n1 2\n\nThen it holds that g(x) ≥ ∆/2 for x ∈ B∞ (1, 0, . . . , 0) ∈ Sd−1 and B∞\n\n∆ (e1) and g(x) = 0 for x /∈ B∞ r (e1) := (cid:8)x ∈ Sd−1 | (cid:107)x − e1(cid:107)∞ ≤ r(cid:9) for r > 0.\n\n2∆(e1), where e1 :=\n\nProof. Let gj(x) = −σ(xj) + 1 Then, we have x1 ≥ 1 − ∆ and |xj| ≤ ∆ for any j ∈ {2, . . . , d}. If 0 ≤ xj ≤ ∆, it holds that\n\n2 σ(xj − 2∆ · x1). First we suppose that x ∈ B∞ ∆ .\n\n2 σ(xj + 2∆ · x1) + 1\n\ngj(x) = −σ(xj) +\n\n1 2\n\nσ(xj + 2∆ · x1) +\n\nMoreover, if −∆ ≤ xj ≤ 0, we get\n\ngj(x) = −σ(xj) +\n\n1 2\n\nσ(xj + 2∆ · x1) +\n\n1 2\n\n1 2\n\nσ(xj − 2∆ · x1) =\n\nσ(xj − 2∆ · x1) =\n\n1 2\n\n1 2\n\n(2∆ · x1 − xj) ≥ ∆(1 − ∆) ≥\n\n(xj + 2∆ · x1) ≥ ∆(1 − ∆) ≥\n\n1 2\n\n1 2\n\n∆.\n\n∆.\n\nHence, we get the first assertion by g(x) = 1\n\nd−1\n\n(cid:80)d\n\nj=2 gj(x) ≥ ∆ 2 .\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nNext we suppose x /∈ B∞ 2∆(e1). Then, it holds that xj ≥ 2∆ ≥ 2∆x1 for any j ∈ {2, . . . , d}. Hence it holds that |xj/x1| ≥ 2∆, and we obtain that sgn(xj + 2∆ · x1) = sgn(xj) = sgn(xj − 2∆ · x1) ∈ {±1}. We can check gj(x) = 0 for each case, and hence it holds that g(x) = 0. Thus we get the second assertion.\n\nproof of Theorem 4.1. Let us consider the covering of Sd−1 by spherical caps, i.e., Br(x) ∩ Sd−1 for some x ∈ Sd−1 with radius r satisfying r ∈ (0, 1). It is known that there is a covering A with |A| ∼ r−d (ignoring logarithm terms). Then, by letting r ∼ 2−K/d, there exists a covering A satisfying |A| = 2K.\n\nFor each A ∈ A, we define a function gA by the same manner as in Lemma A.3, i.e., for A ∈ A written by Br(xA) ∩ Sd−1 with xA ∈ Sd−1, we consider the orthogonal basis including xA and define gA with regrading xA as e1. Define F ◦ A := {gA/2 | A ∈ A}. It is not difficult to check that F ◦\n\nA ∈ conv(F ◦). Then by Proposition A.1, it holds that\n\nRlin(F ◦) = Rlin(conv(F ◦)) ≥ Rlin(F ◦\n\nA),\n\nwhere the inequality follows from F ◦ A ∈ conv(F ◦). Hence, it suffices to give the lower bound A and K = K (cid:48). Applying for the right hand side. Now, we apply Proposition A.2 with F ◦ = F ◦ Lemma A.3 with ∆ = 2−K/d In the event E which we introduce in Proposition A.2, there exists a constant C (cid:48) such that |{xi | xi ∈ A (i ∈ {1, . . . , n})}| ≤ C (cid:48)α2n2−K for all A ∈ A. Therefore, we obtain that\n\n1 n\n\nn (cid:88)\n\ni=1\n\ngA(xi)2 (cid:46) 1\n\nn\n\nn2−K · ∆2 = 2−K∆2\n\nTherefore, Proposition A.2 gives Rlin(F ◦ we get the assertion.\n\nA) (cid:38) min{ 2K\n\nn , 2−(1+2/d)K}. By letting 2K ∼ nd/2(d+1),\n\nB EXPLICIT FORM OF THE OBJECTIVE FUNCTION AND ITS GRADIENT\n\nIn this section, we derive the explicit form of Rλ(·) and its gradient, which we utilize in our analysis (especially that of the convergence in phase I). First, for w, v ∈ Rd/{0}, we have that\n\nEx∼PX [σ((cid:104)w, x(cid:105))σ((cid:104)v, x(cid:105))] =\n\nE ̃x∼N (0,Id)[σ((cid:104)w, x(cid:105))σ((cid:104)v, x(cid:105))] E ̃x∼N (0,Id)[(cid:107) ̃x(cid:107)2]\n\n=\n\nsin φ(w, v) + (π − φ(w, v)) cos φ(w, v) 2πd\n\n(cid:107)w(cid:107)(cid:107)v(cid:107),\n\n(2)\n\nwhere φ(w, v) := arccos((cid:104)w, v(cid:105)/(cid:107)w(cid:107)(cid:107)v(cid:107)). The second equality follows from E ̃x∼N (0,Id)[(cid:107) ̃x(cid:107)2] = d and\n\nE ̃x∼N (0,Id)[σ((cid:104)w, x(cid:105))σ((cid:104)v, x(cid:105))] =\n\nsin φ(w, v) + (π − φ(w, v)) cos φ(w, v) 2π\n\n(cid:107)w(cid:107)(cid:107)v(cid:107)\n\n(see Cho & Saul (2009) or Safran & Shamir (2018)). Moreover, the first equality follows from that fact that for ̃x ∼ N (0, Id), r2 := (cid:107) ̃x(cid:107)2 and φ := ̃x/(cid:107) ̃x(cid:107) are random variables that independently follow the Chi-squared distribution and the uniform distribution on Sd−1 respectively, and therefore,\n\nE ̃x∼N (0,Id)[σ((cid:104)w, x(cid:105))σ((cid:104)v, x(cid:105))] = E ̃x∼N (0,Id)\n\n(cid:2)r2σ((cid:104)w, φ(cid:105))σ((cid:104)v, φ(cid:105))(cid:3) = Ex∼PX [σ((cid:104)w, x(cid:105))σ((cid:104)v, x(cid:105))] · E ̃x∼N (0,Id)[(cid:107) ̃x(cid:107)2].\n\nBy using Eq. (2), we get\n\nRλ(θ) =\n\n=\n\n1 2\n1 2\n\n(cid:104)(cid:0)fa◦,W ◦ (x) − f (x; ̄θ)(cid:1)2(cid:105)\n\nEx\n\n+ λ(cid:107)θ(cid:107)2\n\n(cid:104)\n\n(fa◦,W ◦ (x))2(cid:105)\n\nEx\n\n−\n\nm (cid:88)\n\ni,j=1\n\n ̄aia◦\n\nj I( ̄wi, w◦\n\nj ) +\n\n1 2\n\nm (cid:88)\n\ni,j=1\n\n ̄ai ̄ajI( ̄wi, ̄wj) + λ(cid:107)θ(cid:107)2,\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nwhere ̄w is the element-wise clipping of w ∈ Rd and\n\nI(w, v) =\n\nsin φ(w, v) + (π − φ(w, v)) cos φ(w, v) 2πd\n\n(cid:107)w(cid:107)(cid:107)v(cid:107).\n\ndr = |r|/cosh2(r|r|/2R). Then, since Next, we move to derive the gradient of Rλ(·). Note that d ̄r er + e−r ≥ 2 + |r| for r ∈ R, we have that cosh(r|r|/2R) ≥ 1 + r2/4R, and hence | d ̄r dr | ≤ (1+r2/4R)2 ≤ min{|r|, 16R2|r|/r4} ≤ 4R. Moreover, through a straightforward calculation, we can show that d ̄r\n\ndr is 1-Lipschitz (in other words, the mapping r (cid:55)→ ̄r is 1-smooth).\n\n|r|\n\nUsing this, each component of the gradient of Rλ(·) can be written as follows:\n\n∇aj Rλ(θ) =\n\nm (cid:88)\n\ni=1\n\n ̄aiI( ̄wi, ̄wj) ·\n\nd ̄aj daj\n\n∇wj Rλ(θ) = −\n\nm (cid:88)\n\ni=1\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\ni=1\n\nd ̄wj dwj\n\n+\n\nm (cid:88)\n\ni=1\n\nm (cid:88)\n\n−\n\ni I(w◦ a◦\n\ni , ̄wj) ·\n\nd ̄aj daj\n\n+ 2λaj\n\n(3)\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\nd ̄wj dwj\n\n+ 2λwj,\n\n(4)\n\nwhere (cid:12) denotes the Hadamard product and\n\nJ(w, v) =\n\n(cid:107)v(cid:107)(cid:107)w(cid:107)−1 sin φ(w, v)w + (π − φ(w, v))v 2πd\n\n,\n\nwhich is the gradient of I(w, v) with respect to w (see Brutzkus & Globerson (2017) or Safran & Shamir (2018)).\n\nC PROOF OF PROPOSITION 4.3\n\nThis section provides the convergence guarantee for phase I. Our objective is to give the proof of Proposition 4.3. To this end, we first introduce the theory around the gradient Langevin dynamics exploited in Vempala & Wibisono (2019).\n\nC.1 A BRIEF NOTE ON THE GRADIENT LANGEVIN DYNAMICS\n\nIn their analysis, the following notion of the log-Sobolev inequality plays the essential role, which defined as follows:\n\nDefinition C.1. A probability distribution with a density function q satisfies the log-Sobolev inequality (LSI) if there exists a constant α > 0 such that for all smooth function g, it holds that\n\nEq[g2 log g2] − Eq[g2] log Eq[g2] ≤\n\n2 α\n\nEq[(cid:107)∇g(cid:107)2].\n\nα is called a log-Sobolev constant.\n\nIt is known that the LSI is equivalent to the following inequality:\n\nHq(p) ≤\n\n1 2α\n\nJq(p) (∀p ∈ P),\n\n(5)\n\nR p(θ) log p(θ )\n\nq(θ ) dx is the KL divergence, Jq(p) := (cid:82)\n\nwhere Hq(p) := (cid:82) relative Fisher information, and P is the set of all probability density functions. Now we consider the sampling from the probability distribution q over Rd. We assume that − log q(·) : Rd → R is differentiable. One of the well-known and promising approaches is updating the parameter θ(0) sampled from an initial distribution ρ0 as follows:\n\n(cid:13)∇ log p(θ )\n\ndθ is the\n\nR p(θ)\n\nq(θ )\n\n(cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nθ(k+1) = θ(k) − η∇(− log q)(θ(k)) + (cid:112)2ηζ (k),\n\n(6)\n\nwhere η > 0 is a constant and ζ (k) ∼ N (0, Id) is an independent standard Gaussian random variable. Vempala & Wibisono (2019) shows that if the LSI holds and − log q has a smoothness, the sufficient number of updates (6) actually achieves the sampling from q, in a sense that the KL divergence between the distribution of θ(k) and q will be small.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTheorem C.2 ((Vempala & Wibisono, 2019, Theorem 1)). Suppose that a probability measure with a density function q satisfies the LSI and − log q is L-smooth. Then for any θ(0) ∼ p0 with Hq(p0), the sequence (θ(k))∞\n\nk=0 with step-size 0 < η < α\n\n4L2 satisfies\n\nHq(pt) ≤ exp(−αηk)Hq(p0) +\n\n8ηdL2 α\n\n.\n\nHence for any δ > 0, the output of the update (6) with step-size η ≤ α Hq(pt) < δ after k ≥ 1\n\niterations.\n\nαη log 2Hq(pt)\n\nδ\n\n4L2 min{1, δ\n\n4d } achieves\n\nC.2 PROOF OF LEMMA 4.2\n\nThe goal of this section is to prove Proposition 4.3, the convergence of gradient Langevin dynamics. As we stated in Section 4.2, we consider the value of Rλ(·) instead of (cid:98)Rλ(·), and ensure its value will decrease enough. To this end, we first prove Lemma 4.2, which evaluates the difference between ∇Rλ(·) and ∇ (cid:98)Rλ(·).\n\nproof of Lemma 4.2. The proof of Lemma 4.2 is basically based on that of Theorem 1 in Mei et al. (2018a) and Lemma 5.3 in Zhang et al. (2019). For the notational simplicity we denote m(d + 1) =: D. Let N(cid:15) be the (cid:15)-covering number of B distance. Let Θ(cid:15) = (cid:8)θ 1, . . . , ̄θ N (cid:16) 3\nlog N = D log\n\nwith respect to the (cid:96)2- (cid:9) be a corresponding (cid:15)-cover with |Θ(cid:15)| = N . It is known that\n\nis sufficient to ensure the existence of such covering.\n\nDR/(cid:15)\n\nDR\n\n√\n\n√\n\n0,\n\n(cid:17)\n\n(cid:16)\n\n(cid:17)\n\nFirst we note that ∇Rλ(θ) − ∇ (cid:98)Rλ(θ) = 1 θ ∈ Θ, let j(θ) ∈ arg min\n\ni=1 ∇(cid:96)(yi, f (xi; ̄θ)) − ∇E[(cid:96)(y, f (x; ̄θ))]. For each √\n0, , we consider the\n\n(cid:13) (cid:13) and (cid:98)θ := ̄θ j(θ ). For θ ∈ B\n\n(cid:13) (cid:13) ̄θ − ̄θ j\n\nDR\n\n(cid:16)\n\n(cid:17)\n\nn\n\n(cid:80)n\n\nfollowing decomposition:\n\nj∈[N ]\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇E[(cid:96)(y, f (x; ̄θ))] =\n\n1 n\n\n(cid:32)\n\n+\n\nn (cid:88)\n\n(cid:105) (cid:104) ∇(cid:96)(yi, f (xi; ̄θ)) − ∇(cid:96)(yi, f (xi; (cid:98)θ))\n\ni=1\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; (cid:98)θ)) − ∇E[(cid:96)(y, f (x; (cid:98)θ))]\n\n(cid:33)\n\n(cid:16)\n\n+\n\n∇E[(cid:96)(y, f (x; (cid:98)θ))] − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:17)\n\n.\n\nThis gives that\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\n+\n\n+\n\n16\n\n1 n\n\n1 n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ni=1\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\n(cid:104)\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇(cid:96)(yi, f (xi; (cid:98)θ))\n\n(cid:105)\n\n∇(cid:96)(yi, f (xi; (cid:98)θ)) − ∇E[(cid:96)(y, f (x; (cid:98)θ))]\n\n(cid:13)∇E[(cid:96)(y, f (x; (cid:98)θ))] − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) (cid:13),\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nPublished as a conference paper at ICLR 2023\n\nand hence it holds that \n\nP\n\n sup θ ∈B(0,\n\n√\n\nDR) \n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\n≥ t\n\n\n\n≤ P\n\n sup θ ∈B(0,\n\n√\n\n(cid:124)\n\n\n\n+ P\n\n sup θ ∈B(0,\n\n√\n\n(cid:124)\n\n\n\n+ P\n\n sup θ ∈B(0,\n\n√\n\n(cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n1 n\n\nn (cid:88)\n\n(cid:104)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇(cid:96)(yi, f (xi; (cid:98)θ))\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≥\n\n(cid:123)(cid:122) (I)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; (cid:98)θ)) − ∇E[(cid:96)(y, f (x; (cid:98)θ))]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≥\n\n(cid:123)(cid:122) (II)\n\n\n\n\n\nt 3\n\n(cid:125)\n\n\n\n\n\nt 3\n\n(cid:125)\n\n(cid:13) (cid:13)\n\nDR)\n\n(cid:13)∇E[(cid:96)(y, f (x; (cid:98)θ))] − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:123)(cid:122) (III)\n\n(cid:13) (cid:13) (cid:13) ≥\n\n\n\n\n\nt 3\n\n(cid:125)\n\nfor any t > 0. Then we evaluate the each term of the RHS.\n\nUpper bound on (I): Since ∇(cid:96)(yi, f (xi; ̄θ)) = 2(cid:0)f (xi; ̄θ) − yi\n\n(cid:1)∇f (xi; ̄θ), it holds that\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇(cid:96)(yi, f (xi; (cid:98)θ)) (cid:17) f (xi; ̄θ) − f (xi; (cid:98)θ)\n\n= 2\n\n(cid:16)\n\n∇f (xi; ̄θ) − 2(f (xi; (cid:98)θ) − yi)(∇f (xi; (cid:98)θ) − ∇f (xi; ̄θ)).\n\nTherefore, we have that\n\n\n\nP\n\n sup θ ∈B(0,\n\n√\n\nDR)\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n(cid:104) ∇(cid:96)(yi, f (xi; ̄θ)) − ∇(cid:96)(yi, f (xi; (cid:98)θ))\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≥\n\n\n\n\n\nt 3\n\n≤ P\n\n sup θ ∈B(0,\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n2 n\n\n\n\n+ P\n\n sup θ ∈B(0,\n\n√\n\nDR)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2 n\n\ni=1\n\nn (cid:88)\n\n(cid:104)\n\ni=1\n\nn (cid:88)\n\n(cid:104)(cid:16)\n\nf (xi; ̄θ) − f (xi; (cid:98)θ)\n\n(cid:17)\n\n(cid:105) ∇f (xi; ̄θ)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≥\n\n\n\n\n\nt 6\n\n(f (xi; (cid:98)θ) − yi)(∇f (xi; (cid:98)θ) − ∇f (xi; ̄θ))\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≥\n\n\n\n,\n\nt 6\n\nSince the mapping (cid:98)θ (cid:55)→ f (x; (cid:98)θ) is 2R-Lipschitz and (cid:13) (cid:13) ≤ 2mR for any θ ∈ Θ, it holds that the first term must be zero as long as t ≥ 4mR2(cid:15). As for the second term, since |f (x; ̄θ) − yi| ≤ mR2 + U + 1 for any xi, yi and θ ∈ Θ, it holds that (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:105) (f (xi; (cid:98)θ) − yi)(∇f (xi; (cid:98)θ) − ∇f (xi; ̄θ))\n\n(cid:13)∇f (x; ̄θ)(cid:13)\n\n sup θ ∈B(0,\n\n(I) = P\n\nn (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n2 n\n\nt 6\n\n\n\n\n\n\n\n≥\n\ni=1\n\n(cid:104)\n\n√\n\n\n\n≤ P\n\n sup θ ∈B(0,\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n2 n\n\nn (cid:88)\n\n(cid:104)\n\ni=1\n\n∇f (xi; (cid:98)θ) − ∇f (xi; ̄θ)\n\n≥\n\nt 6(mR2 + U + 1)\n\n\n\n.\n\n(cid:105)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nHence, we move to evaluate\n\nsup √\n\nθ ∈B(0,\n\nDR)\n\n(cid:13) (cid:13) (cid:13)\n\n2 n\n\n(cid:104)\n\n(cid:80)n\n\ni=1\n\n∇f (xi; (cid:98)θ) − ∇f (xi; ̄θ)\n\n(cid:105)(cid:13) (cid:13) (cid:13). To this end, we con-\n\nsider the decomposition (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\n2 n\n\n(cid:105) ∇f (xi; (cid:98)θ) − ∇f (xi; ̄θ)\n\n(cid:104)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\nm (cid:88)\n\nj=1\n\n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2 n\n\nn (cid:88)\n\ni=1\n\n(cid:105) (cid:104) ∇aj f (xi; (cid:98)θ) − ∇aj f (xi; ̄θ)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2 n\n\nn (cid:88)\n\ni=1\n\n(cid:105) (cid:104) ∇wj f (xi; (cid:98)θ) − ∇wj f (xi; ̄θ)\n\n(cid:33) ,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nwhere\n\n∇aj f (xi; ̄θ) = σ((cid:104) ̄wj, xi(cid:105))\n\nd ̄aj daj\n\n,\n\n∇wj f (xi; ̄θ) = ̄aj1l{(cid:104) ̄wj, xi(cid:105) ≥ 0}xi (cid:12)\n\nd ̄wj dwj\n\n.\n\nThis decomposition implies that\n\n\n\n(I) ≤ P\n\n max\n\nj∈[m]\n\nsup √\n\nθ ∈B(0,\n\n\n\n+ P\n\n max\n\nj∈[m]\n\nsup √\n\nθ ∈B(0,\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nDR)\n\n2 n\n\n2 n\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\ni=1\n\n(cid:105) (cid:104) ∇aj f (xi; (cid:98)θ) − ∇aj f (xi; ̄θ)\n\n≥\n\nt 12m(mR2 + U + 1)\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:105) (cid:104) ∇wj f (xi; (cid:98)θ) − ∇wj f (xi; ̄θ)\n\n≥\n\nt 12m(mR2 + U + 1)\n\n\n\n.\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nFor each term, it holds that\n\n(cid:13) (cid:13)\n\n(cid:13)∇aj f (xi; (cid:98)θ) − ∇aj f (xi; ̄θ)\n\n(cid:13) (cid:13) (cid:13) ≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(σ((cid:104) ̄wj, xi(cid:105)) − σ((cid:104) ˆwj, xi(cid:105)))\n\n≤ (cid:107) ̄wj − ˆwj(cid:107)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nd ̄aj daj\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nd ̄aj daj\n\n−\n\nd ̄aj daj (cid:12) dˆaj (cid:12) (cid:12) daj (cid:12)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nσ((cid:104) ˆwj, xi(cid:105))\n\n(cid:18) d ̄aj daj\n\n−\n\ndˆaj daj\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)\n\nand\n\n≤ 4R(cid:107) ̄wj − ˆwj(cid:107) + 2| ̄aj − ˆaj| ≤ 4R(cid:15) + (cid:15)\n\n(cid:13) (cid:13)\n\n(cid:13)∇wj f (xi; (cid:98)θ) − ∇wj f (xi; ̄θ)\n\n(cid:13) (cid:13) (cid:13) ≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n ̄aj(1l{(cid:104) ̄wj, xi(cid:105) ≥ 0} − 1l{(cid:104) ˆwj, xi(cid:105) ≥ 0})xi (cid:12)\n\n+\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n( ̄aj − ˆaj)1l{(cid:104) ˆwj, xi(cid:105) ≥ 0}xi (cid:12)\n\nˆaj1l{(cid:104) ̄wj, xi(cid:105) ≥ 0}xi (cid:12)\n\n(cid:18) d ̄wj dwj\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nd ̄wj dwj\n\nd ̄wj dwj (cid:13) (cid:13) (cid:13) (cid:13) d ˆwj dwj\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ R\n\n≤ R\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(1l{(cid:104) ̄wj, xi(cid:105) ≥ 0} − 1l{(cid:104) ˆwj, xi(cid:105) ≥ 0})xi (cid:12)\n\nd ̄wj dwj\n\n+ 4R(cid:107) ̄aj − ˆaj(cid:107) + 4R(cid:107) ̄wj − ˆwj(cid:107)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(1l{(cid:104) ̄wj, xi(cid:105) ≥ 0} − 1l{(cid:104) ˆwj, xi(cid:105) ≥ 0})xi (cid:12)\n\nd ̄wj dwj\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 8R(cid:15).\n\nThe first term can be bounded by (cid:13) (cid:13) (cid:13) (cid:13)\n\n(1l{(cid:104) ̄wj, xi(cid:105) ≥ 0} − 1l{(cid:104) ˆwj, xi(cid:105) ≥ 0})xi (cid:12)\n\nd ̄wj dwj\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ (cid:107)(1l{(cid:104) ̄wj, xi(cid:105) ≥ 0} − 1l{(cid:104) ˆwj, xi(cid:105) ≥ 0})xi(cid:107) · (cid:107) ˆwj(cid:107)\n\n≤ 1l{|(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} · (cid:107) ˆwj(cid:107),\n\nwhere the last inequality follows from |(cid:104) ̄wj, xi(cid:105) − (cid:104) ˆwj, xi(cid:105)| ≤ (cid:107) ̄wj − ˆwj(cid:107) · (cid:107)xi(cid:107) ≤ (cid:15). Therefore, we obtain that \n\n\n\n(I) ≤ P\n\n max\n\nj∈[m]\n\nsup √\n\nθ ∈B(0,\n\nDR)\n\n#{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} · (cid:107) ˆwj(cid:107) n\n\n≥\n\nt 24mR(mR2 + U + 1)\n\n\n\n(cid:32)\n\n= P\n\nmax (cid:98)θ ∈Θ(cid:15),j∈[m]\n\n#{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} · (cid:107) ˆwj(cid:107) n\n\n≥\n\nt 24mR(mR2 + U + 1)\n\n(cid:33)\n\nas long as\n\n24mR(mR2+U +1) ≥ max{4R(cid:15), (cid:15), 8R(cid:15)} = 8R(cid:15). We have that\n\nt\n\n(cid:18) #{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} · (cid:107) ˆwj(cid:107) n\n\nP\n\n≥\n\nt 24mR(mR2 + U + 1)\n\n(cid:19)\n\n= P\n\n(cid:18) #{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} n\n\n≥\n\nt 24mR(mR2 + U + 1)(cid:107) ˆwj(cid:107)\n\n(cid:19)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nwhen ˆwj (cid:54)= 0. If ˆwj = 0, the LHS must be zero as long as t > 0. Lemma 12 in Cai et al. (2013) shows that for each j and i, the angle between ˆwj and xi is distributed with density function\n\nh(φ) =\n\n1 √\nπ\n\nΓ(cid:0) d (cid:1) Γ(cid:0) d−1\n\n2\n\n2\n\n(cid:1) · (sin φ)d−2 :\n\nφ ∈ [0, π].\n\nSince (cid:12)\n\n2 − φ(cid:12) (cid:12) π\n\n(cid:12) ≤ ∆ implies |cos φ| = (cid:12)\n\n(cid:12)sin(cid:0) π\n\n2 − φ(cid:1)(cid:12)\n\n(cid:12) ≤ ∆ for any ∆ > 0 and h(φ) ≤ 1√\n\n2 ) Γ( d 2 ) Γ( d−1\n\nfor\n\nπ\n\nany φ ∈ [0, π], it holds that\n\nP(|(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)) ≤ P\n\n(cid:18)(cid:12) (cid:12) (cid:12)\n\nπ 2\n\n− φij\n\n(cid:12) (cid:12) (cid:12) ≤\n\n(cid:15) (cid:107) ˆwj(cid:107)\n\n(cid:19)\n\n≤\n\n2(cid:15) (cid:107) ˆwj(cid:107)\n\n1 √\nπ\n\nΓ(cid:0) d (cid:1) Γ(cid:0) d−1\n\n2\n\n2\n\n√\n\n2 d(cid:15) √\nπ(cid:107) ˆwj(cid:107)\n\n,\n\n(cid:1) ≤\n\nwhere φij is the angle between ˆwj and xi. Therefore, #{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} follows the Binomial distribution B(n, p) with p ≤ 2 π(cid:107) ˆwj (cid:107) . Since a random variables that follows the Binomial distribution is bounded and especially sub-Gaussian Wainwright (2019), it holds that\n\nd(cid:15)\n\n√\n\n√\n\n(cid:32)\n\nP\n\n#{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} n\n\n≥ s +\n\n(cid:33)\n\n√\n\n2 d(cid:15) √\nπ(cid:107) ˆwj(cid:107)\n\n≤ P\n\n(cid:18) #{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} n\n\n(cid:19)\n\n≥ s + p\n\n≤ exp(cid:0)−2ns2(cid:1).\n\nfor an arbitrarily s > 0. By taking uniform bound, we obtain that √\n\n(cid:32)\n\nP\n\nmax (cid:98)θ ∈Θ(cid:15),j∈[m]\n\n#{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} n\n\n≥ s +\n\nd(cid:15) 2\n√ π(cid:107) ˆwj(cid:107)\n\n(cid:33)\n\n≤ N exp(cid:0)−2ns2(cid:1)\n\nHence, as long as (cid:15) ≤\n\n√\n\nπ\n\n96mR(mR2+U +1)2\n\n√\n\nt (verified later in this proof), by letting s =\n\nd\n\n48mR(mR2+U +1)(cid:107) ˆwj (cid:107) , we obtain that\n\nt\n\n(cid:32)\n\nP\n\nmax (cid:98)θ ∈Θ(cid:15),j∈[m]\n\n#{i ∈ [n] | |(cid:104) ˆwj, xi(cid:105)| ≤ (cid:15)} n\n\n≥ s +\n\n√\n\n(cid:33)\n\nd(cid:15) π(cid:107) ˆwj(cid:107)\n\n(cid:32)\n\n(cid:18)\n\n≤ mN exp\n\n−2n\n\nt 24mR(mR2 + U + 1)(cid:107) ˆwj(cid:107)\n\n(cid:19)2(cid:33)\n\n(cid:32)\n\n≤ mN exp\n\n−\n\n(cid:18)\n\n2n dR2\n\nt 24mR(mR2 + U + 1)\n\n(cid:19)2(cid:33) ,\n\nwhere the last inequality follows from (cid:107) ˆwj(cid:107)2 ≤ dR2. As a result, the term (I) can be bounded by\n\n(cid:32)\n\n(I) ≤ mN exp\n\n−\n\n(cid:18)\n\n2n dR2\n\nt 24mR(mR2 + U + 1)\n\n(cid:19)2(cid:33) .\n\nUpper bound on (II): First, we observe that the term (II) is equivalent to (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇(cid:96)(yi, f (xi; ̄θ j)) − ∇E[(cid:96)(y, f (x; ̄θ j))]\n\nmax j∈[N ]\n\nn (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\n(cid:32)\n\nP\n\ni=1\n\n(cid:33) .\n\nt 3\n\n≥\n\nFor each j ∈ [N ], a straightforward calculation gives that (cid:107)∇(cid:96)(yi, f (xi; θ j))(cid:107) ≤ 2R(mR2 + 1), and hence the vector ∇(cid:96)(yi, f (xi; θ j)) is sub-Gaussian with a parameter R(mR2 + 1), i.e., it holds that\n\n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nP\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ j)) − ∇E[(cid:96)(y, f (x; ̄θ j))]\n\n(cid:33)\n\n≥\n\nt 3\n\n≤ 2e− nt2\n\n18G2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nwith G = R(mR2 + 1) for arbitrary t > 0. By taking uniform bound, we obtain\n\n(cid:32)\n\nP\n\nmax j∈[N ]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ j)) − ∇E[(cid:96)(y, f (x; ̄θ j))]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:33)\n\n≥\n\nt 3\n\n≤ 2N e− t2\n\n18G2 .\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nUpper bound on (III): The goal is obtaining (III) = 0 for a sufficiently small (cid:15). Particularly, we assume that (cid:15) < 1 here. To this end, we aim to show\n\n(cid:13) (cid:13)\n\n(cid:13)∇E[(cid:96)(y, f (x; (cid:98)θ))] − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) ≤ cL(cid:48)(cid:15)1/2 (cid:13)\n\n(7)\n\nwith a constant c > 0 and L = O(m2R3). First we consider the case where the absolute value of the each component in ̄θ is bounded by 1/2. By Lemma C.5, it holds that\n\n(cid:13) (cid:13)\n\n(cid:13)∇E[(cid:96)(y, f (x; (cid:98)θ))] − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) ≤ L(cid:48)(cid:107)θ − θ j(θ )(cid:107) = L(cid:48) · (cid:13)\n\n(cid:16)\n\n(cid:107)θ − θ j(θ )(cid:107)2(cid:17) 1\n\n2\n\nfor any θ ∈ Θ with L(cid:48) = O(m2R3). Moreover, a straightforward calculation shows that a mapping r (cid:55)→ 2R tanh−1(r/R) (the inverse mapping of r (cid:55)→ R tanh(r/2R)) is 8-Lipschitz in [0, 1/2], we have (cid:107)θ − θ j(θ )(cid:107)2 ≤ 8(cid:107) ̄θ − (cid:98)θ(cid:107) ≤ 8(cid:15). Therefore, we obtain that\n\n(cid:13) (cid:13)\n\n(cid:13)∇E[(cid:96)(y, f (x; (cid:98)θ))] − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) ≤ L(cid:48)(8(cid:15))1/2, (cid:13)\n\ni.e., Eq. (7) with c = 8. Assume that there is a component of θ whose absolute value is greater than 1/2. First, suppose that a component of ̄wj is greater than 1/2 for j ∈ [m]. We consider the decomposition (cid:13) (cid:13) (cid:13)∇wj (cid:13) (cid:13) (cid:13)∇ ˆwj\n\nE[(cid:96)(y, f (x; (cid:98)θ))] − ∇wj\n\nE[(cid:96)(y, f (x; (cid:98)θ))] − ∇ ̄wj\n\nE[(cid:96)(y, f (x; ̄θ))](cid:13) (cid:13) ·\n\nE[(cid:96)(y, f (x; ̄θ))]\n\nE[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13)∇ ̄wj\n\n(cid:13) (cid:13) (cid:13) ·\n\n+ (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n−\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nd ˆwj dwj\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nd ˆwj dwj\n\nd ̄wj dwj\n\n(cid:13) (cid:13) .\n(cid:13) (cid:13)\n\n√\n\nd ˆwj dwj\n\n(cid:13) (cid:13) (cid:13) ≤ 4\n\ndR, the first term is at most O(mR3) · (cid:15). Since (cid:13)\n\n(cid:55)→ E[(cid:96)(y, f (x; ̄θ))] is L(cid:48)(cid:48) smooth Since (cid:107) ̄wj(cid:107) > 1/2, we can check that the mapping ˆwj with L(cid:48)(cid:48) = O(mR2d−1/2) according to its Hessian (see Safran & Shamir (2018)). Since (cid:13) (cid:13) (cid:13) ≤ 2R(mR2 + (cid:13) the second term is at most O(mR3) · (cid:15). Hence we get that 1) and r (cid:55)→ ̄r is 1-smooth, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ≤ O(mR3) · (cid:15). In the case |aj| > 1/2 for j ∈ [m], (cid:13)∇wj (cid:13) E[(cid:96)(y, f (x; ̄θ))] (cid:13) the same bound also holds with (cid:13). By using these bound instead of Lemma E.5 and (cid:15) < 1, we obtain the same bound Eq. (7) in this case. Eq. (7) implies (III) = 0 as long as t\n\nE[(cid:96)(y, f (x; (cid:98)θ))] − ∇wj\n\nE[(cid:96)(y, f (x; (cid:98)θ))] − ∇aj\n\n(cid:13)E[(cid:96)(y, f (x; ̄θ))](cid:13)\n\n3 ≥ cL(cid:48)(cid:15)1/2, which gives the assertion.\n\nE[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) (cid:13)∇aj\n\nCombining (I)–(III): Combining these bounds, we get that\n\n\n\nP\n\n sup θ ∈B(0,\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 n\n\nn (cid:88)\n\ni=1\n\n∇(cid:96)(yi, f (xi; ̄θ)) − ∇E[(cid:96)(y, f (x; ̄θ))]\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\n≥ t\n\n\n\n2n dR2 √\n\n(cid:18)\n\nt 24mR(mR2 + U + 1) (cid:33)\n\n(cid:19)2(cid:33)\n\n+ 2N e− t2\n\n18G2 + 0\n\nDR) (cid:32)\n\n≤ mN exp\n\n−\n\n(cid:32)\n\n= exp\n\nD log\n\n3\n\nDR (cid:15)\n\n(cid:34)\n\n(cid:32)\n\n·\n\nm exp\n\n−\n\n(cid:18)\n\n2n dR2\n\nt 24mR(mR2 + U + 1)\n\n(cid:19)2(cid:33)\n\n(cid:18)\n\n+ 2 exp\n\n−\n\nnt2 18R2(mR2 + U + 1)2\n\n(cid:19)(cid:35)\n\nas long as t ≥ C0 max{mR2(cid:15), mR(mR2 + U )(cid:15), L(cid:48)(cid:15)1/2} holds with a constant C0 > 0. By letting t = C1L(cid:48)(cid:15)1/2 and (cid:15) = C2\n\nnm2 with constants C1 > 0 and C2 > 0 , we obtain the conclusion.\n\nd log δ\n\nC.3 PROOF OF THE CONVERGENCE IN PHASE I\n\nBased on the results so far, we move to the proof of Proposition 4.3. The proof is conducted in twostep. First, we evaluate the “distance” between the π∞ and the distribution of θ(k). Moreover, it is\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nensured that the function value Rλ(θ), where θ is sampled from π∞, will be small for a sufficiently large β. Combining these two facts, we can guarantee that the function value Rλ(θ(k)) also will be small, which concludes Proposition 4.3. The following proposition ensures the convergence of the marginal distribution of θ(k) to the invariant measure π∞: Proposition C.3. Suppose that the probability measure π∞ satisfies the LSI with a constant α and Rλ(·) is L-smooth with L > 1. Let q be a density function of π∞ (i.e., q(θ) ∝ exp(−βRλ(θ))) with β > 2. For any θ(0) ∼ ρ0 with Hq(ρ0) < +∞, the sequence (θ(k))∞ k=0 with step-size 0 < η(1) < 4βL2 satisfies\n\nα\n\n(cid:18)\n\nHq(ρk) ≤ exp\n\n−\n\n(cid:19)\n\nk\n\nHq(ρ0) +\n\nαη(1) β\n\n16βη(1)DL2 α\n\n+\n\n32βV 2\n\ngrad\n\n3α\n\n,\n\nwhere D := m(d + 1), ρk is the density function of the marginal distribution of θ(k), and Vgrad is a constant introduced in Lemma 4.2. In particular, for any δ > 0, the output of phase I with step-size η(1) ≤ δα\n\nafter k ≥ β\n\n32βL2D achieves Hq(ρk) < δ +\n\nαη(1) log 2Hq(ρ0)\n\n32βV 2 3α\n\niterations.\n\ngrad\n\nδ\n\ngrad\n\n32βV 2 3α\n\n. Since V 2\n\nAs we stated in Section 4.2, our result extends the existing one Vempala & Wibisono (2019) in the sense that it gives the convergence for the non-differential objective function (cid:98)Rλ(·). Indeed, this (cid:46) n−1 by Lemma 4.2, we can ensure difference appears in the last term, that this error diverges to zero as the sample size n increases. To apply this result to ensure the convergence of the phase I, we just need to check that the invariant measure π∞ satisfies the LSI and Rλ is smooth, and we clarify them as follows: Lemma C.4 (log-Sobolev inequality). The invariant measure π∞ satisfies the LSI with a constant α = 2βλ exp(cid:0)−8βm2R4(cid:1). Lemma C.5 (smoothness). Rλ(·) is L-smooth, i.e., for any θ, θ (cid:48) ∈ Θ, (cid:13) L(cid:13)\n\n(cid:13)∇Rλ(θ) − ∇Rλ(θ (cid:48))(cid:13)\n\n(cid:13) holds with L = O(m2R3 + λ).\n\n(cid:13)θ − θ (cid:48)(cid:13)\n\n(cid:13) ≤\n\ngrad\n\nThe proof of these lemmas can be seen in Appendix E.\n\nRemark C.6. In Lemma C.4, the LSI constant α depends exponentially on m, which results in the exponential dependency of phase I convergence on m. This is caused by the fact that the sup-norm of the student network depends on m (see the proof of Lemma C.4). Therefore, we can indeed remove this dependency by considering the following settings: (1) utilizing the mean field network (multiplying 1/m to the output of the student), (2) wjs are directed to different directions to some extent, and hence the sup-norm can be bounded.\n\nTo ensure Proposition C.3, we first show the following lemma, which evaluates the each step of the gradient Langevin dynamics.\n\nLemma C.7. Suppose that π∞ satisfies the LSI with a constant α and Rλ(·) is L-smooth with L > 1, and β > 2. Then for any θ(0) ∼ ρ0 with Hq(ρ0) < +∞, if 0 < η < α\n\n4βL2 ,it holds that\n\nHq(ρk+1) ≤ e−αη/βHq(ρk) + 12η2DL2 + 8ηV 2\n\ngrad,\n\nwhere ρk is the density function of the marginal distribution of θ(k) and Vgrad is the constant defined in Lemma 4.2.\n\nProof. The proof of Lemma C.7 is basically based on that of Lemma 3 in Vempala & Wibisono (2019). For notational simplicity suppose k = 0 and let θ 0 = θ(0). The one step of the gradient Langevin dynamics\n\nθ(1) = θ(0) − η∇ (cid:98)Rλ\n\n(cid:16)\n\nθ(0)(cid:17)\n\n+\n\n(cid:114) 2η β\n\nζ (0)\n\ncan be seen as an output at time ηβ−1 of the following SDE: √\n\ndθ t = −β∇ (cid:98)Rλ(θ 0)dt +\n\n2dBt,\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nwhere {Bt}t≥0 is the standard Brownian motion in Θ (= R(d+1)×m). As Vempala & Wibisono (2019), it holds that\n\n∂ρt|0(θt|θ 0) ∂t\n\n(cid:16)\n\n= ∇ ·\n\nρt|0(θt|θ0)β∇ (cid:98)Rλ(θ0)\n\n(cid:17)\n\n+ ∆ρt|0(θt|θ0),\n\nand therefore, d\ndt\n\nHq(ρt) = −Jq(ρt) + β · Eρ0t\n\n(cid:20)(cid:28)\n\n∇Rλ(θt) − ∇ (cid:98)Rλ(θ0), ∇ log\n\n(cid:29)(cid:21) ,\n\nρt(θt) q(θt)\n\nwhere ρt|0(·|θ 0) the conditional density, and ρt0 is the density of the joint distribution of θ 0 and θ t.\n\nThen we evaluate the second term. The inner product in this term can be bounded by (cid:28)\n\n(cid:29)\n\n∇Rλ(θt) − ∇ (cid:98)Rλ(θ0), ∇ log\n\nρt(θt) q(θ t)\n\n≤\n\n≤ 2\n\n(cid:13) (cid:13) (cid:13)∇Rλ(θt) − ∇ (cid:98)Rλ(θ0) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13)∇Rλ(θt) − ∇Rλ(θ0) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n∇ log\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n1 4\n\nρt(θt) q(θ t)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n+ 2\n\n(cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13)∇Rλ(θ0) − ∇ (cid:98)Rλ(θ0) (cid:13)\n\n+\n\n1 4\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ log\n\nρt(θt) q(θ t)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n.\n\nIn the above bound, we use (cid:104)a, b(cid:105) ≤ a2 + b2/4 for a, b ∈ RD in the first inequality and (cid:107)a − b(cid:107)2 ≤ 2(cid:107)a(cid:107)2 + 2(cid:107)b(cid:107)2 for a, b ∈ RD in the second inequality. Therefore, by using Lemma 4.2, we get that\n\n(cid:20)(cid:28)\n\nEρ0t\n\n∇Rλ(θt) − ∇Rλ(θ0), ∇ log\n\n(cid:29)(cid:21)\n\nρt(θt) q(θ t)\n\n≤ 2V 2\n\ngrad + 2Eρ0t\n\n= 2V 2\n\ngrad + 2Eρ0t\n\n(cid:20)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∇Rλ(θt) − ∇Rλ(θ0) (cid:13)\n\n2(cid:21)\n\n(cid:20)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∇Rλ(θt) − ∇Rλ(θ0) (cid:13)\n\n2(cid:21)\n\n+\n\n+\n\n1 4\n\n1 4\n\nEρ0t\n\n(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ log\n\nρt(θt) q(θ)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nJq(ρt).\n\nThen the second term is bounded by\n\nEρ0t\n\n(cid:20)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∇Rλ(θt) − ∇Rλ(θ0) (cid:13)\n\n2(cid:21)\n\n≤ L2Eρ0t\n\n= L2Eρ0t\n\n(cid:104)\n\n(cid:107)θ t − θ 0(cid:107)2(cid:105) (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:20)(cid:13) (cid:13) (cid:13)∇Rλ(θ 0) + (∇ (cid:98)Rλ(θ 0) − ∇Rλ(θ 0))\n\n−t∇ (cid:98)Rλ(θ 0) +\n\n(cid:114) 2t β\n\nζ (0)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n2(cid:35)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n= t2L2Eρ0t\n\n+ L2Eρ0t\n\n(cid:34)(cid:13) (cid:114) 2t (cid:13) (cid:13) β\n(cid:13)\n\nζ (0)\n\n2(cid:35)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ 2t2L2(cid:16)\n\nEρ0t\n\n(cid:104)\n\n(cid:107)∇Rλ(θ 0)(cid:107)2(cid:105)\n\n+ V 2\n\ngrad\n\n(cid:17)\n\n+ L2 2t\n\nβ\n\nD\n\n≤\n\n(cid:18) 4t2L4 α\n\n1 β\n\nHq(ρ0) + 2t2L3D\n\n(cid:19)\n\n+ 2η2L2V 2\n\ngrad + tL2D.\n\nIn the last inequality, we use Lemma 10 in Vempala & Wibisono (2019) and β > 2. Thus we obtain\n\nJq(ρt) +\n\nHq(ρ0) + 4βt2L3D + 2βtL2D + (2βη2L2 + 2)V 2\n\ngrad\n\nd dt\n\nHq(ρt) ≤ −\n\n≤ −\n\n3 4\n3α 2\n\n8βt2L4 α\n8βt2L4 α\n\nHq(ρt) +\n\nHq(ρ0) + 6βtL2D + 4βV 2\n\ngrad\n\nsince the LSI (Eq. (5)) holds and tL ≤ ηL ≤ 1. Multiplying both sides by e3αt/2 and integrating them from t = 0 to t = ηβ−1, we get\n\ne3αη/2βHq(ρη) − Hq(ρ0) ≤\n\n2(e3αη/2β − 1) 3α (cid:18) 8η2L4 α\n\n≤ 2η\n\n(cid:18) 4βη2L4 α\n\nHq(ρ0) + 6βηDL2 + 4βV 2\n\ngrad\n\n(cid:19)\n\nHq(ρ0) + 6ηDL2 + 4V 2\n\ngrad\n\n(cid:19)\n\n,\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nwhere we use the inequality ea ≤ 1 + 2a for a ∈ [0, 1] and 3αη/2β ≤ 1 (derived from the assumption of η). Rearranging this inequality, we have\n\nHq(ρη) ≤ e−3αη/2β\n\n(cid:18)\n\n1 +\n\n16η3L4 α\n\n(cid:19)\n\nHq(ρ0) + e−3αη/2β(cid:0)12η2DL2 + 8V 2\n\ngradη(cid:1)\n\n≤ e−αη/βHq(ρ0) + 12η2DL2 + 8ηV 2\n\ngrad,\n\nwhere the last inequality follows from 1 + 16η3L4 ρ0 by ρk and ρη by ρk+1, we get the conclusion.\n\nα ≤ 1 + αη\n\n16β2 ≤ 1 + αη\n\n2β ≤ eαη/2β. By replacing\n\nproof of Proposition C.3. By Lemma C.7, it holds that\n\nHq(ρk) ≤ e−αηk/βHq(ρ0) + (cid:0)12η2DL2 + 8ηV 2\n\ngrad\n\nk (cid:88)\n\n(cid:1)\n\nk(cid:48)=1\n\ne−αηk(cid:48)/β\n\n12η2DL2 + 8ηV 2\n\ngrad\n\n≤ e−αηk/βHq(ρ0) +\n\n≤ e−αηk/βHq(ρ0) +\n\n3α where, the last inequality follows from L > 1 (derived from Lemma C.5) and 1 − e−c ≥ 3 c ∈ [0, 1\n\n1 − e−αη/β\n\n4 . Thus we get the assertion.\n\n4 ] and αη\n\n4L2 < 1\n\nβ < 1\n\n4 c for\n\n16βηDL2 α\n\n+\n\n32βV 2\n\ngrad\n\n,\n\nproof of Proposition 4.3. By the Otto-Villani theorem, it holds that W2(ρk, q)2 ≤ 2 fore, Proposition C.3 implies that after k ≥ β\n\niteration, it holds that\n\nα Hq(ρk). There-\n\nW2(ρk, q) ≤\n\nThen we obtain that\n\nδ\n\nαη log 2Hq(ρ0) (cid:118) (cid:117) (cid:117) (cid:116)\n\nδ +\n\n(cid:32)\n\n2 α\n\n32βV 2\n\ngrad\n\n(cid:33)\n\n3α\n\nE[Rλ(θ(k))] − R∗\n\nλ ≤\n\n(cid:16)\n\nE[Rλ(θ(k))] − Eπ∞ [Rλ(θ)]\n\n(cid:17)\n\n+ (Eπ∞ [Rλ(θ)] − R∗ λ)\n\n≤ C(λ + m)\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n(cid:32)\n\n2 α\n\nδ +\n\n32βV 2\n\ngrad\n\n(cid:33)\n\n3α\n\n+\n\nD 2β\n\nlog\n\n(cid:18) eL M\n\n(cid:18) bβ D\n\n(cid:19)(cid:19)\n\n+ 1\n\n,\n\nwhere we use Lemma E.1 and Lemma E.2 for the inequality. By specifying α, L, M and b by applying Lemma C.4, Lemma C.5, and Lemma E.3, we get the conslusion.\n\nD PROOF OF THEOREM 4.6\n\nThe objective of this section is to prove Theorem 4.6. First, by the noisy gradient descent, the objective value decreases enough, and we can ensure that for each node of the teacher network, there exists a node of the student network that is “close” to each other. Then we can prove the local convergence property based on the strong convexity around the parameters of the teacher network.\n\nThe proof of the local convergence relies on that of Zhang et al. (2019). They consider the setting where the parameters of the second layer are all positive, i.e., aj = a◦ j = 1 for all j ∈ [m] and provide the following proposition: Proposition D.1 (Theorem 4.2 of Zhang et al. (2019)). Let f ◦ : x (cid:55)→ (cid:80)m j , x(cid:105)) be a teacher m) ∈ Rd×m, κ = σ1/σm is the condition number network with parameters W ◦ = (w◦ of W ◦, and σ = ((cid:81)m i=1 are sampled from N (0, Id), and the outputs (yi)n i=1 are generated from the teacher network. Suppose that the initial estimator W (0) satisfies (cid:107)W (0) − W ◦(cid:107)F ≤ cσm/κ3m2, where c > 0 is a small enough absolute constant. Then there exists absolute constants c1, c2, c3, c4, and c5 such that under\n\n2 · · · w◦ m. Assume the inputs (xi)n\n\nj=1 σj)/σm\n\nj=1 σ((cid:104)w◦\n\n1 w◦\n\nn ≥\n\nc1κ10m9d σm\n\nlog\n\n(cid:19)\n\n(cid:18) κmd σm\n\n· (cid:0)(cid:107)W ∗(cid:107)2\n\nF + v2(cid:1),\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nthe output of the gradient descent with step-size η ≤ 1\n\nc2κm2 satisfies\n\n(cid:107)W (k) − W ◦(cid:107)2\n\nF ≤\n\n(cid:16)\n\n1 −\n\n(cid:17)k\n\nc3η σκ2\n\n(cid:107)W (0) − W ◦(cid:107)2\n\nF +\n\nc4σ2κ4m5d log n n\n\n· (cid:0)(cid:107)W ◦(cid:107)2\n\nF + v2(cid:1)\n\nwith probability at least 1 − c5d−10.\n\nTheir proof can also be applied to the setting in this paper, i.e., aj, a◦ node j and a student node kj are close to each other, it holds that a◦ holds that E[Rλ(θ(k))] − R∗\n\nλ ≤ b(cid:15)0, we obtain that\n\nj ∈ {±1} holds, and if a teacher j = akj . In Proposition 4.3, if it\n\nwith probability at least 1 − b, by using the Markov inequality. In the rest of this section, we assume that (8) is satisfied. In this case, if Proposition 4.4 is ensured, we can apply Proposition D.1 and Theorem 4.6 is proved. We give its proof in the rest of this section.\n\nRλ(θ(k)) − R∗\n\nλ ≤ (cid:15)0\n\n(8)\n\nproof of Proposition 4.4. Let θ ◦ = ((a◦ Rλ(θ) − R∗ λ ≤ (cid:15)0, it holds that (fa◦,W ◦ (x) − f (x; θ))2(cid:105)\n\n+ λ\n\nEx\n\nm (cid:88)\n\n(cid:104)\n\n1 2\n\n1, w◦\n\n1), . . . , (a◦\n\nm, w◦\n\nm)). Then by Rλ(θ) − Rλ(θ ◦) ≤\n\n(cid:16)\n\n|aj|2 + (cid:107)wj(cid:107)2(cid:17)\n\n≤ λ\n\nm (cid:88)\n\nj=1\n\n(cid:16)\n\n|a◦\n\nj |2 + (cid:13)\n\n(cid:13)w◦\n\nj\n\n2(cid:17)\n\n(cid:13) (cid:13)\n\n+ (cid:15)0,\n\nj=1\n\nj=1 (cid:13) 2\n(cid:13)\n\n(cid:13) (cid:13)w◦\n\nj\n\nand therefore,\n\n(cid:104)\n\n(fa◦,W ◦ (x) − f (x; θ))2(cid:105)\n\nEx\n\n≤\n\n1 2\n\n≤\n\n(cid:15)0 m\n\n(cid:15)0 m\n\nm (cid:88)\n\n(cid:16)\n\n|a◦\n\nj |2 + (cid:13)\n\n(cid:13)w◦\n\nj\n\n2(cid:17)\n\n(cid:13) (cid:13)\n\n+ (cid:15)0\n\nj=1\n\nm (cid:88)\n\n(cid:0)1 + (cid:107)W ◦(cid:107)2\n\nF\n\n(cid:1) + (cid:15)0 ≤ 3(cid:15)0,\n\n(9)\n\nwhere we use |a◦ move to evaluate the LHS. Since σ(u) = u+|u|\n\nj |2 = 1 for all j ∈ [m] and (cid:80)m\n\nj=1\n\nfor u ∈ R, it holds that\n\n2\n\n= (cid:107)W ◦(cid:107)2\n\nF ≤ m(cid:107)W ◦(cid:107)2\n\n2 ≤ m. Then we\n\nf (x; θ) =\n\nm (cid:88)\n\nj=1\n\najσ((cid:104)wj, x(cid:105)) =\n\n1 2\n\nm (cid:88)\n\nj=1\n\naj(|(cid:104)wj, x(cid:105)| + (cid:104)wj, x(cid:105)),\n\nfa◦,W ◦ (x) =\n\nm (cid:88)\n\nj=1\n\najσ((cid:104)wj, x(cid:105)) =\n\n1 2\n\nm (cid:88)\n\nj=1\n\na◦\n\nj\n\n(cid:0)(cid:12) (cid:12)(cid:104)w◦\n\nj , x(cid:105)(cid:12)\n\n(cid:12) + (cid:104)w◦\n\nj , x(cid:105)(cid:1)\n\nHence we have that (cid:104)\n\n(fa◦,W ◦ (x) − f (x; θ))2(cid:105)\n\n1 2\n\nEx\n\n=\n\n=\n\n1 8\n\n1 8\n\nEx\n\nEx\n\n\n\n\n\n \n\n\n\nm (cid:88)\n\nj=1\n\n\n\n\n\n \n\n\n\nm (cid:88)\n\nj=1\n\n(cid:0)(cid:12) (cid:12)(cid:104)w◦\n\nj , x(cid:105)(cid:12)\n\n(cid:12) + (cid:104)w◦\n\nj , x(cid:105)(cid:1) −\n\n2\n\n\n\naj(|(cid:104)wj, x(cid:105)| + (cid:104)wj, x(cid:105))\n\n\n\n \n\nm (cid:88)\n\nj=1\n\n(cid:12) (cid:12)(cid:104)w◦\n\nj , x(cid:105)(cid:12)\n\n(cid:12) −\n\n2\n\n\n\naj|(cid:104)wj, x(cid:105)|\n\n\n\n  +\n\n1 8\n\nEx\n\nm (cid:88)\n\nj=1\n\n(cid:42) m (cid:88)\n\n\n\n\n\nj=1\n\nj w◦ a◦\n\nj −\n\nm (cid:88)\n\nj=1\n\n(cid:43)2 ,\n\najwj, x\n\na◦\n\nj\n\na◦\n\nj\n\nwhere the last equality follows from Ex [|(cid:104)w1, x(cid:105)|(cid:104)w2, x(cid:105)] = 0 for all w1, w2 ∈ Rd, which follows from the fact that the distribution PX is symmetric. Then Eq. (9) gives that\n\n\n\n\n\n \n\n\n\nm (cid:88)\n\nj=1\n\n(cid:42) m (cid:88)\n\n\n\n\n\nj=1\n\nEx\n\nEx\n\na◦\n\nj\n\n(cid:12) (cid:12)(cid:104)w◦\n\nj , x(cid:105)(cid:12)\n\n(cid:12) −\n\n2\n\n\n\naj|(cid:104)wj, x(cid:105)|\n\n\n\n  ≤ 24(cid:15)0,\n\nm (cid:88)\n\nj=1\n\nj w◦ a◦\n\nj −\n\nm (cid:88)\n\nj=1\n\n(cid:43)2\n\najwj, x\n\n ≤ 24(cid:15)0.\n\n24\n\n(10)\n\n(11)\n\nPublished as a conference paper at ICLR 2023\n\nj , wk(cid:105)|/(cid:13)\n\nThe analysis based on Eq. (10), the error analysis of student networks with the absolute value activation, is conducted in Zhou et al. (2021). Here we import Lemma D.2 from their technique. They focus on the setting where a◦ j = 1 for all j ∈ [m], but we can apply it here. Then we get that for every j ∈ [m], there exists kj ∈ [m] and a constant C > 0 such that min (cid:15)1/3 and (cid:13) arccos(cid:0)|(cid:104)w◦ We simply denote kj by j. Since Zhou et al. (2021) uses the absolute value for the activation, it (cid:13) (cid:13)(cid:107)wj(cid:107)(cid:1) > π/2 (i.e., w◦ may hold that arccos(cid:0)(cid:104)w◦ j and wk have “opposite” directions). 1, . . . , a◦ From now on, we omit such cases by Eq. (11). Let a = (a1, . . . , am) = (a◦ m) and W∆ = (w◦ j and wj by φj. Then, Eq. (11) can be rewritten as\n\nm − wm). And we denote the angle between w◦\n\n(cid:13) (cid:13)(cid:107)wk(cid:107)(cid:1) ≤ Cmσ−5/3\n\n(cid:13) (cid:13) ≤ poly(m, σ−1\n\n1 − w1, . . . , w◦\n\n(cid:12) (cid:12)wkj − w◦\n\nj , wj(cid:105)/(cid:13)\n\nmin)(cid:15)3/8.\n\n(cid:12) (cid:12)akj (cid:13)\n\n(cid:13)w◦\n\n(cid:13)w◦\n\nj\n\nj\n\nj\n\nEx∼PX\n\n(cid:2)(aTW∆x)2(cid:3) ≤ 24(cid:15)0.\n\nLet ̃x ∼ N (0, Id), since r2 := (cid:107) ̃x(cid:107)2 and φ := ̃x/(cid:107) ̃x(cid:107) are random variables that independently follow the Chi-squared distribution and the uniform distribution on Sd−1 respectively. Hence it holds that\n\nEx∼PX\n\n(cid:2)(aTW∆x)2(cid:3) =\n\nE ̃x∼N (0,Id)\n\n(cid:2)(aTW∆ ̃x)2(cid:3)\n\nE ̃x∼N (0,Id)(cid:107) ̃x(cid:107)2\n\n=\n\nE\n\nr∼N (0,(cid:107)(cid:104)a,W∆(cid:105)(cid:107)2)\n\n(cid:2)r2(cid:3)\n\nd\n\n=\n\n(cid:107)(cid:104)a, W∆(cid:105)(cid:107)2 d\n\n≤ 24(cid:15)0.\n\nThis implies (cid:107)(cid:104)a, W∆(cid:105)(cid:107)2 ≤ 24(cid:15)0d. Since w◦ (cid:13) (cid:13) (cid:13)(cid:104)w◦ (cid:13) = sin φj, we have that\n\nj , wj(cid:105)w◦\n\nj − wj\n\nj − wj = (1 − (cid:104)w◦\n\nj , wj(cid:105))w◦\n\nj + ((cid:104)w◦\n\nj , wj(cid:105)w◦\n\nj − wj) and\n\n(cid:16)\n\n(cid:104)a, W∆(cid:105) =\n\n(1 − (cid:104)w◦\n\n1, w1(cid:105))a1, . . . , (1 − (cid:104)w◦\n\nm, wm(cid:105))am\n\n(cid:17)T\n\nW ◦\n\n(cid:18)\n\n+\n\n(cid:104)a, W∆(cid:105) −\n\n(cid:16)\n\n(1 − (cid:104)w◦\n\n1, w1(cid:105))a1, . . . , (1 − (cid:104)w◦\n\nm, wm(cid:105))am\n\n(cid:19)\n\n(cid:17)T\n\nW ◦\n\n(cid:16)\n\n=\n\n(1 − (cid:104)w◦\n\n1, w1(cid:105))a1, . . . , (1 − (cid:104)w◦\n\nm, wm(cid:105))am\n\n(cid:17)T\n\nW ◦\n\n(cid:68)\n\na, (cid:0)(cid:104)w◦\n\n1, w1(cid:105)w◦\n\n1 − w1, . . . , (cid:104)w◦\n\nm, wm(cid:105)w◦\n\nm − wm\n\n(cid:1)(cid:69)\n\n+\n\nand the second term is at most O(m3/2σ−5/3 j=1(1 − (cid:104)w◦ σmin 0, which gives the assertion.\n\nj , wj(cid:105))2. Hence, by letting (cid:15) = o(d−1m−3/2σ8\n\nmin (cid:15)1/3). As for the first term, it holds that it is at least j , wj(cid:105) >\n\nmin), it must hold that (cid:104)w◦\n\n(cid:80)m\n\nj1\n\n(cid:13) (cid:13)\n\n(cid:13)w◦\n\nj=1 |(cid:104)w◦ , w◦ j2\n\nj , x(cid:105)| is a teacher network with parameters w◦ (cid:105)/(cid:13)\n\nLemma D.2 (Lemma 9 and Lemma 10 in Zhou et al. (2021)). Assume that x ∼ N (0, Id) and f ◦ : x (cid:55)→ (cid:80)m m ∈ Rd satisfying (cid:13) arccos(cid:0)(cid:104)w◦ (cid:13) ≤ wmax for all j ∈ min j1,j2 [m]. Then there exists a threshold (cid:15)0 = poly(∆, m−1, w−1 (cid:98)f : x (cid:55)→ (cid:80)m kj ∈ [m] and a constant C > 0 such that arccos(cid:0)|(cid:104)w◦ (cid:12) and (cid:13) (cid:12)akj (cid:13)\n\nmax.wmin) such that if a student network j=1 |(cid:104)wj, x(cid:105)| satisfies Ex[(f ◦− (cid:98)f )2] ≤ (cid:15) ≤ (cid:15)0, it holds that for every j ∈ [m], there exists min (cid:15)1/3\n\n(cid:1) ≥ ∆ for ∆ > 0 and 0 < wmin ≤ (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) ≤ poly(m, ∆−1, wmax)(cid:15)3/8.\n\n(cid:13) (cid:13)(cid:107)wk(cid:107)(cid:1) ≤ Cmwmaxw−5/3\n\n1, . . . , w◦ (cid:13)w◦\n\n(cid:12) (cid:12)wkj − w◦\n\nj , wk(cid:105)|/(cid:13)\n\n(cid:13) (cid:13)w◦\n\n(cid:13)w◦\n\nj1\n\nj2\n\nj\n\nj\n\nj\n\nE AUXILIARY LEMMAS\n\nE.1 EVALUATION OF THE INVARIANT MEASURE\n\nThis subsection provides lemmas about the evaluation of the function value sampled from the invariant measure β. These are utilized in the proof of Proposition 4.3 (see Appendix C). First, we introduce two results from Raginsky et al. (2017), and then we prove the dissipativity, which is imposed as an assumption in these results. Lemma E.1 (Proposition 11 in Raginsky et al. (2017)). Suppose that f : Θ → R satisfies the following conditions:\n\n• f is L-smooth.\n\n25\n\nPublished as a conference paper at ICLR 2023\n\n• f is (M, b)-dissipative, i.e., it holds that (cid:104)θ, ∇f (θ)(cid:105) ≥ M (cid:107)θ(cid:107)2 − b for any θ ∈ Θ.\n\nThen, for any β ≥ 2/M , it holds that\n\nEθ ∼π∞[f (θ)] − min\n\nθ∈Θ\n\nf (θ) ≤\n\nd 2β\n\nlog\n\n(cid:18) eL M\n\n(cid:18) bβ d\n\n(cid:19)(cid:19)\n\n+ 1\n\nLemma E.2 (Lemma 2 and Lemma 6 in Raginsky et al. (2017)). Let μ1, μ2 be two probability measures on Θ with finite second moments, and let f : Θ → R be a (M, b)-dissipative function satisfying (cid:107)∇f (0)(cid:107) ≤ B for B ≥ 0. Then, it holds that\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:90)\n\nΘ\n\ngdμ1 −\n\n(cid:90)\n\nΘ\n\ngdμ2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ (M σ + B)W2(μ1, μ2),\n\nwhere σ2 := max{(cid:82)\n\nΘ (cid:107)θ(cid:107)2dμ1, (cid:82)\n\nΘ (cid:107)θ(cid:107)2dμ2}.\n\nLemma E.3 (dissipativity). Rλ(·) is (M, b)-dissipative with M = 2λ and b = 8m2R3.\n\nProof. By a straightforward calculation, we have that\n\naj∇aj Rλ(θ) +\n\nm (cid:88)\n\nj=1\n\n(cid:104)wj, ∇wj Rλ(θ)(cid:105)\n\n(cid:104)θ, ∇Rλ(θ)(cid:105) =\n\n=\n\nm (cid:88)\n\nj=1\n\nm (cid:88)\n\nj=1\n\n(cid:34) m (cid:88)\n\naj\n\n ̄aiI( ̄wi, ̄wj) ·\n\nd ̄aj daj\n\n−\n\nm (cid:88)\n\ni=1\n\ni I(w◦ a◦\n\ni , ̄wj) ·\n\n(cid:35)\n\n+ 2λaj\n\nd ̄aj daj\n\ni=1 (cid:42)\n\nm (cid:88)\n\n+\n\nm (cid:88)\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\nwj, −\n\nd ̄wj dwj\n\n+\n\nm (cid:88)\n\ni=1\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\n(cid:43)\n\nd ̄wj dwj\n\n+ 2λwj\n\nj=1\n\n= 2λ(cid:107)θ(cid:107)2 +\n\nm (cid:88)\n\ni=1 (cid:34) m (cid:88)\n\naj\n\nj=1\n\ni=1\n\n ̄aiI( ̄wi, ̄wj) ·\n\nd ̄aj daj\n\n−\n\nm (cid:88)\n\ni=1\n\ni I(w◦ a◦\n\ni , ̄wj) ·\n\n(cid:35)\n\nd ̄aj daj\n\n(cid:42)\n\nwj, −\n\n+\n\nm (cid:88)\n\nj=1\n\nm (cid:88)\n\ni=1\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\nd ̄wj dwj\n\n+\n\nm (cid:88)\n\ni=1\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\n(cid:43) .\n\nd ̄wj dwj\n\nAs for the second term and the third term, since |I(w, v)| ≤ (cid:107)w(cid:107)(cid:107)v(cid:107)/2d and (cid:107)J(w, v)(cid:107) ≤ (cid:107)v(cid:107)/2d for any w, v ∈ Rd, we have that\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nm (cid:88)\n\nj=1\n\n(cid:34) m (cid:88)\n\naj\n\n ̄aiI( ̄wi, ̄wj) ·\n\ni=1\n\nm (cid:88)\n\nj=1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\naj\n\nd ̄aj daj\n\n(cid:34) m (cid:88)\n\ni=1\n\nd ̄aj daj\n\n−\n\nm (cid:88)\n\ni=1\n\ni I(w◦ a◦\n\ni , ̄wj) ·\n\nd ̄aj daj\n\n ̄aiI( ̄wi, ̄wj) −\n\nm (cid:88)\n\ni=1\n\ni I(w◦ a◦\n\ni , ̄wj)\n\n≤ 4m2R3,\n\n(cid:35)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:35)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nand\n\n(cid:12) (cid:12) m\n(cid:88) (cid:12) (cid:12) (cid:12) (cid:12)\n\nj=1\n\n(cid:42)\n\nwj, −\n\nm (cid:88)\n\ni=1\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\nd ̄wj dwj\n\n+\n\nm (cid:88)\n\ni=1\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\nd ̄wj dwj\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\nm (cid:88)\n\nj=1\n\n(cid:42)\n\nwj, −\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nm (cid:88)\n\ni=1\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\nd ̄wj dwj\n\n+\n\nm (cid:88)\n\ni=1\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\nd ̄wj dwj\n\n(cid:43)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ 4m2R3.\n\nCombining these inequality, we get that\n\n(cid:104)θ, ∇Rλ(θ)(cid:105) ≥ 2λ(cid:107)θ(cid:107)2 − 8m2R3,\n\nwhich gives the conclusion.\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nE.2 PROOF OF LEMMA C.4\n\nIn this subsection, we give a proof to Lemma C.4, the LSI for the invariant measure π∞. The key notion is that Rλ can be decomposed to the bounded term (L2-distance) and the strongly convex term (regularization term). Combining this fact with the following lemma, we can ensure the LSI. Lemma E.4 (Holley & Stroock (1987); Nitanda et al. (2021b)). Let a probability measure on Θ with a density function q satisfy the LSI with a constant α. For a function f : Θ → R that satisfies |f (θ)| ≤ B for any θ ∈ Θ, a probability measure defined by\n\nQ(θ)dθ :=\n\nexp(f (θ))q(θ) Eq[exp(f (θ))q(θ)]\n\ndθ\n\nsatisfies the LSI with a constant α exp(−4B).\n\nproof of Lemma C.4. First, we note that\n\nexp(−βRλ(θ))dθ = exp\n\n(cid:16)\n\n−βλ(cid:107)θ(cid:107)2(cid:17)\n\n(cid:18)\n\n· exp\n\n−\n\nβ 2\n\n(cid:104)(cid:0)fa◦,W ◦ (x) − f (x; ̄θ)(cid:1)2(cid:105)(cid:19)\n\ndθ.\n\nEx\n\nSince the function θ (cid:55)→ βλ(cid:107)θ(cid:107)2 is 2βλ-strongly convex, a measure with density exp dθ satisfies the LSI with a constant βλ Bakry & ́Emery (1985). Moreover, by a straightforward calcu- (cid:104)(cid:0)fa◦,W ◦ (x) − f (x; ̄θ)(cid:1)2(cid:105) lation shows that β ≤ 2βm2R4, Lemma E.4 implies that π∞ satisfies the LSI with a constant 2βλ exp(cid:0)−8βm2R4(cid:1), which gives the conclusion.\n\nEx\n\n2\n\n(cid:16)\n\n−βλ(cid:107)θ(cid:107)2(cid:17)\n\nE.3 PROOF OF LEMMA C.5\n\n, i.e., Rλ(θ) := L(θ) + λ(cid:107)θ(cid:107)2. In this subsection we write L(θ) := 1 Since θ (cid:55)→ λ(cid:107)θ(cid:107)2 is 2λ-smooth, it is sufficient to show that L(·) is L(cid:48)-smooth with L(cid:48) = O(m2R3) for proving Lemma C.5. To this end, let θ, θ (cid:48) ∈ Θ. We consider the decomposition\n\n(cid:104)(cid:0)fa◦,W ◦ (x) − f (x; ̄θ)(cid:1)2(cid:105)\n\nEx\n\n2\n\n(cid:13)∇L(θ) − ∇L(θ (cid:48))(cid:13) (cid:13)\n\n(cid:13) =\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\nm (cid:88)\n\nj=1\n\n(cid:16)(cid:12) (cid:12)∇aj L(θ) − ∇aj L(θ (cid:48))(cid:12) (cid:12)\n\n2\n\n+ (cid:13)\n\n(cid:13)∇wj L(θ) − ∇wj L(θ (cid:48))(cid:13) (cid:13)\n\n2(cid:17)\n\nm (cid:88)\n\n(cid:12)∇aj L(θ) − ∇aj L(θ (cid:48))(cid:12) (cid:0)(cid:12)\n\n(cid:12) + (cid:13)\n\n(cid:13)∇wj L(θ) − ∇wj L(θ (cid:48))(cid:13) (cid:1), (cid:13)\n\n≤\n\nwhere\n\n∇aj L(θ) − ∇aj L(θ (cid:48)) =\n\nj=1\n\nm (cid:88)\n\n(cid:18)\n\ni=1\n\n ̄aiI( ̄wi, ̄wj) ·\n\nd ̄aj daj\n\n− ̄a(cid:48)\n\niI( ̄w(cid:48)\n\ni, ̄w(cid:48)\n\nj) ·\n\n(cid:19)\n\nd ̄a(cid:48) j\ndaj\n\nm (cid:88)\n\n(cid:18)\n\n−\n\ni=1\n\ni I(w◦ a◦\n\ni , ̄wj) ·\n\nd ̄aj daj\n\n− a◦\n\ni I(w◦\n\ni , ̄w(cid:48)\n\nj) ·\n\n∇wj L(θ) − ∇wj L(θ (cid:48)) = −\n\nm (cid:88)\n\ni=1\n\n(cid:18)\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\nd ̄wj dwj\n\n− ̄a(cid:48)\n\nia◦\n\nj J( ̄w(cid:48)\n\ni, w◦\n\nj ) (cid:12)\n\n(cid:19) ,\n\nd ̄a(cid:48) j\ndaj\n\n(cid:19)\n\nd ̄w(cid:48) j\ndwj\n\n+\n\n1 2\n\nm (cid:88)\n\ni=1\n\n(cid:18)\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\nd ̄wj dwj\n\n− ̄a(cid:48)\n\ni ̄a(cid:48)\n\njJ( ̄w(cid:48)\n\ni, ̄w(cid:48)\n\nj) (cid:12)\n\n(cid:19)\n\n.\n\nd ̄w(cid:48) j\ndwj\n\n(see Eq. (3) and Eq. (4)). The following lemma gives an upper bound for each term. Lemma E.5. For any θ, θ (cid:48) ∈ Θ and j ∈ [m], it holds that\n\n(cid:12)∇aj L(θ) − ∇aj L(θ (cid:48))(cid:12) (cid:12)\n\n(cid:12) ≤ m\n\n(cid:32)\n\n5R3 2\n\n+\n\n√\n\n2\n\ndR d\n\n(cid:33)\n\n(cid:0)(cid:12) (cid:12)aj − a(cid:48)\n\nj\n\n(cid:12) + (cid:13) (cid:12)\n\n(cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:1) + 2R3 (cid:13)\n\nm (cid:88)\n\ni=1\n\n(cid:107) ̄wi − ̄w(cid:48)\n\ni(cid:107)\n\n(cid:13)∇wj L(θ) − ∇wj L(θ (cid:48))(cid:13) (cid:13)\n\n(cid:13) ≤ m\n\n(cid:18) 2R d\n\n(cid:12) + (cid:13) (cid:12)\n\n(cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:1) + 2R3 (cid:13)\n\n+ 2R3\n\n(cid:19)\n\n(cid:0)(cid:12) (cid:12)aj − a(cid:48)\n\nj\n\n27\n\nm (cid:88)\n\n(|ai − a(cid:48)\n\ni| + (cid:107)wi − w(cid:48)\n\ni(cid:107)).\n\ni=1\n\nPublished as a conference paper at ICLR 2023\n\nProof. The proof is based on the straightforward calculation. As for the first inequality, for every i ∈ [m], it holds that\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n ̄aiI( ̄wi, ̄wj) ·\n\nd ̄aj daj\n\n− ̄a(cid:48)\n\niI( ̄w(cid:48)\n\ni, ̄w(cid:48)\n\nj) ·\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n( ̄aiI( ̄wi, ̄wj) − ̄a(cid:48)\n\niI( ̄wi, ̄wj)) ·\n\nd ̄a(cid:48) j\ndaj (cid:12) d ̄aj (cid:12) (cid:12) daj (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:0) ̄a(cid:48)\n\niI( ̄wi, ̄wj) − ̄a(cid:48)\n\niI( ̄wi, ̄w(cid:48)\n\nj)(cid:1) ·\n\niI( ̄wi, ̄w(cid:48)\n\nj) − ̄a(cid:48)\n\niI( ̄w(cid:48)\n\ni, ̄w(cid:48)\n\nj)(cid:1) ·\n\nd ̄aj daj\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n+\n\n(cid:12) (cid:12) iI( ̄w(cid:48) ̄a(cid:48) (cid:12) (cid:12)\n\ni, ̄w(cid:48)\n\nj) ·\n\n(cid:18) d ̄aj daj\n\n−\n\n· 4R + (cid:13)\n\n(cid:13) ̄wj − ̄w(cid:48)\n\nj\n\n(cid:13) (cid:13)\n\ndR2 2d\n\n4R + (cid:107) ̄wi − ̄w(cid:48)\n\ni(cid:107)\n\ndR2 2d\n\n4R + R\n\n(cid:0)(cid:12) (cid:12)aj − a(cid:48)\n\n(cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:1) + 2R3(cid:107) ̄wi − ̄w(cid:48) (cid:13)\n\ni(cid:107),\n\n(cid:0) ̄a(cid:48)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:107) ̄wi(cid:107)(cid:107) ̄wj(cid:107) 2d (cid:12) + (cid:13) (cid:12)\n\nj\n\n+\n\n≤ | ̄aj − ̄a(cid:48) j|\n\n≤\n\n5R3 2\n\nand\n\nd ̄aj daj\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:19)(cid:12) d ̄a(cid:48) (cid:12) j\n(cid:12) daj (cid:12) (cid:107) ̄wi(cid:107)(cid:107) ̄wj(cid:107) 2d\n\n· | ̄aj − ˆaj|\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\ni I(w◦ a◦\n\ni , ̄wj) ·\n\nd ̄aj daj\n\n− a◦\n\ni I(w◦\n\ni , ̄w(cid:48)\n\nj) ·\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n≤\n\n1 2d\n\n(cid:0)a◦\n\ni I(w◦\n\ni , ̄wj) − a◦\n\ni , ̄w(cid:48)\n\nj)(cid:1) ·\n\n+\n\ni I(w◦ a◦\n\ni , ̄w(cid:48)\n\nj) ·\n\n(cid:18) d ̄aj daj\n\n−\n\nd ̄a(cid:48) j\ndaj\n\n(cid:19)(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:13) (cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:13)4R +\n\n| ̄aj − ˆaj| ≤\n\n(cid:0)(cid:12) (cid:12)aj − a(cid:48)\n\nj\n\n(cid:12) + (cid:13) (cid:12)\n\n(cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:1), (cid:13)\n\ni I(w◦ √\n\ndR 2d\n\n(cid:12) d ̄a(cid:48) (cid:12) j\n(cid:12) daj (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nd ̄aj daj\n\n(cid:12) (cid:12) (cid:12) (cid:12) √\n\n2\n\ndR d\n\nwhere we use (cid:13) assertion. As for the second inequality, for every i ∈ [m], it holds that\n\n(cid:13) (cid:13) ≤ (cid:107)W o(cid:107) ≤ 1 for any j ∈ [m]. Then the triangle inequality gives the first\n\n(cid:13)w◦\n\nj\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n ̄ai ̄ajJ( ̄wi, ̄wj) (cid:12)\n\nd ̄wj dwj\n\n− ̄a(cid:48)\n\ni ̄a(cid:48)\n\njJ( ̄w(cid:48)\n\ni, ̄w(cid:48)\n\nj) (cid:12)\n\n≤\n\n(cid:13) (cid:13) (cid:0) ̄ai ̄ajJ( ̄wi, ̄wj) − ̄a(cid:48) (cid:13) (cid:13)\n\ni ̄a(cid:48)\n\njJ( ̄wi, ̄wj)(cid:1) (cid:12)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\nd ̄w(cid:48) j\ndwj (cid:13) (cid:13) (cid:13) (cid:13)\n\nd ̄wj dwj\n\n+\n\ni ̄a(cid:48)\n\njJ( ̄wi, ̄w(cid:48)\n\nj) − ̄a(cid:48)\n\ni ̄a(cid:48)\n\njJ( ̄w(cid:48)\n\njJ( ̄w(cid:48)\n\ni, ̄w(cid:48)\n\nj) (cid:12)\n\n√\n\n· 4\n\ndR + R|ai − a(cid:48) i|\n\ndR + R2\n\n· (cid:13) (cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:13) · 4\n\ndR\n\ni ̄a(cid:48)\n\njJ( ̄wi, ̄wj) − ̄a(cid:48)\n\ni ̄a(cid:48)\n\njJ( ̄wi, ̄w(cid:48)\n\nj)(cid:1) (cid:12)\n\nd ̄wj dwj d ̄w(cid:48) j\ndwj\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)\n\n−\n\n(cid:18) d ̄wj dwj √\n\n(cid:13) (cid:13) (cid:0) ̄a(cid:48) (cid:13) (cid:13)\n\nd ̄wj dwj √\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ni ̄a(cid:48) ̄a(cid:48) √\n\nd 2d\n\nj)(cid:1) (cid:12)\n\ni, ̄w(cid:48) √\n\ndR 2d\n\n· 4 √\n\n(cid:13) (cid:13) (cid:0) ̄a(cid:48) (cid:13) (cid:13) √\n\n≤ R(cid:12)\n\n(cid:12)aj − a(cid:48)\n\nj\n\n(cid:12) (cid:12)\n\n+ R2\n\n≤ 2R3(cid:0)(cid:12)\n\n(cid:12)aj − a(cid:48)\n\ndR 2d √\nd 2d (cid:12) (cid:12) + (cid:13)\n\nj\n\n√\n\n· (cid:107)wi − w(cid:48)\n\n(cid:13)wj − w(cid:48)\n\nj\n\ndR + 2R2\n\ni(cid:107) · 4 (cid:13) (cid:1) + 2R3(|ai − a(cid:48) (cid:13)\n\n· (cid:13) (cid:13)wj − w(cid:48)\n\ndR 2d i| + (cid:107)wi − w(cid:48)\n\ni(cid:107)).\n\nj\n\n(cid:13) (cid:13)\n\nand\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n ̄aia◦\n\nj J( ̄wi, w◦\n\nj ) (cid:12)\n\nd ̄wj dwj\n\n− ̄a(cid:48)\n\nia◦\n\nj J( ̄w(cid:48)\n\ni, w◦\n\nj ) (cid:12)\n\n≤\n\n(cid:13) (cid:13) (cid:0) ̄aia◦ (cid:13) (cid:13)\n\nj J( ̄wi, w◦\n\nj ) − ̄a(cid:48)\n\nia◦\n\nj J( ̄wi, w◦\n\nj )(cid:1) (cid:12)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nia◦ ̄a(cid:48)\n\nj J( ̄w(cid:48)\n\ni, w◦\n\nj ) (cid:12)\n\n(cid:18) d ̄wj dwj\n\n−\n\n≤\n\n1 2d\n\n(cid:12) (cid:12)aj − a(cid:48)\n\nj\n\n(cid:12) (cid:12)4R + R\n\n1 2d\n\n· (cid:13) (cid:13)wj − w(cid:48)\n\nj\n\n(cid:13) (cid:13) +\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:0) ̄a(cid:48) (cid:13) (cid:13)\n\nd ̄w(cid:48) j\ndwj (cid:13) (cid:13) (cid:13) (cid:13) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) · (cid:13) (cid:13)wj − w(cid:48)\n\nj\n\nd ̄wj dwj d ̄w(cid:48) j\ndwj R\n2d\n\nAgain by using the triangle inequality, we obtain the conclusion.\n\n28\n\nia◦\n\nj J( ̄wi, w◦\n\nj ) − ̄a(cid:48)\n\nia◦\n\nj J( ̄w(cid:48)\n\ni, w◦\n\nj )(cid:1) (cid:12)\n\nd ̄wj dwj\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) =\n\n2R d\n\n(cid:0)(cid:12) (cid:12)aj − a(cid:48)\n\nj\n\n(cid:12) + (cid:13) (cid:12)\n\n(cid:13)wj − w(cid:48)\n\nj\n\n(cid:1)\n\n(cid:13) (cid:13)\n\nPublished as a conference paper at ICLR 2023\n\nproof of Lemma C.5. By using Lemma E.5,\n\n(cid:13)∇L(θ) − ∇L(θ (cid:48))(cid:13) (cid:13) (cid:13)\n\nm (cid:88)\n\n≤\n\n(cid:12)∇aj L(θ) − ∇aj L(θ (cid:48))(cid:12) (cid:0)(cid:12)\n\n(cid:12) + (cid:13)\n\n(cid:13)∇wj L(θ) − ∇wj L(θ (cid:48))(cid:13) (cid:1) (cid:13)\n\nj=1 (cid:34)\n\n≤ m\n\n(cid:34)\n\n≤ m\n\n5R3 2\n\n5R3 2\n\n2\n\n2\n\n+\n\n+\n\n√\n\ndR d\n√\n\ndR d\n\n+ 2R3 +\n\n+ 2R3 +\n\n2R d\n\n2R d\n\n+\n\n+\n\n1 2\n\n1 2\n\n· 2R3 + 2R3\n\n(cid:35) m (cid:88)\n\nj=1\n\n(cid:0)(cid:12) (cid:12)aj − a(cid:48)\n\nj\n\n(cid:12) + (cid:13) (cid:12)\n\n(cid:13)wj − w(cid:48)\n\nj\n\n(cid:1)\n\n(cid:13) (cid:13)\n\n(cid:35)√\n\n· 2R3 + 2R3\n\n2m(cid:13)\n\n(cid:13)θ − θ (cid:48)(cid:13)\n\n(cid:13) = L(cid:48)(cid:13)\n\n(cid:13)θ − θ (cid:48)(cid:13) (cid:13)\n\nholds with L(cid:48) = O(m2R3). Combining this with the fact that the mapping θ (cid:55)→ λ(cid:107)θ(cid:107)2 is 2λ-smooth and the triangle inequality, we obtain that Rλ(·) is L-smooth with L = O(m2R3 + λ), which gives the conclusion.\n\n29",
    "reference": "# Summary Of The Paper\n\nThis paper studies the excess risk of learning neural networks in the teacher-student setting, as well as the theoretical superiority of neural networks over other linear (or kernel) methods. It provides an excess risk bound of $\\frac{\\log n}{n}$ convergence rate (with polynomial dependency on other parameters) for the neural networks, and also a minimax lower bound of $n^{-\\frac{d+2}{2d+2}-o(1)}$ for all kernel methods to learn the same target function class.\n\n# Strength And Weaknesses\n\nStrengths:\n1. This paper is clearly and rigorously written. All the notations are well explained and most of the technical difficulties are introduced naturally. It is very delightful to read as the paper managed to convey these highly technical statements with such clarity and fine elaboration.\n2. The story in this paper is complete, in that this paper almost resolves most of the questions in its setting. They have the convergence guarantee with well-explained optimization algorithms and analysis, and also a generalization guarantee as excess risk upper bound. For the lower bound, they proved the LB for all linear classes (kernels) that their generalization over the teacher class is poor compared to student neural nets. \n3. The problem this paper deals with is a long-pursued one in the deep learning theory community. The results in this paper go beyond the previous works and are proven for a setting with almost minimal assumptions over the target function class (for example, it goes beyond the orthogonality condition on which many previous results are based, and it goes beyond the local convergence result of Zhou et al. 2021). This is highly non-trivial progress in this direction. The results of this paper also provide some solid technical contributions.\n\nWeaknesses:\n1. The two-stage optimization process, which uses two different optimization algorithms, seems to be the remaining problem to solve. It would be better if the analysis can be applied to settings without this technical workaround.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is written clearly and with high quality. The novelty of its results lies in that it improved the previous analysis to settings where assumptions are more natural.\n\n# Summary Of The Review\n\nThis paper presents a solid contribution to the area of deep learning theory, although it has not resolved all the problems under its setting, it still manages to improve the previous analysis and give an almost complete story of learning in the student-teacher setting beyond kernel methods.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "UnderreviewasaconferencepaperatICLR2023ACCELERATINGSPIKINGNEURALNETWORKTRAININGUSINGTHEd-BLOCKMODELAnonymousauthorsPaperunderdouble-blindreviewABSTRACTThereisagrowinginterestinusingspikingneuralnetworks(SNNs)tostudythebraininsilicoandinemulatingthemonneuromorphiccomputersduetotheirlowerenergyconsumptioncomparedtoartificialneuralnetworks(ANNs).Sig-nificantprogresshasbeenmadeindirectlytrainingSNNstoperformonaparwithANNsintermsofaccuracy.However,thesemethodsareslowduetotheirsequentialnatureandrequirecarefulnetworkregularisationtoavoidoverfit-ting.WeproposeanewSNNmodel,thed-blockmodel,withstochasticab-soluterefractoryperiodsandrecurrentconductancelatencies,whichreducesthenumberofsequentialcomputationsusingfastvectorisedoperations.Ourmodelobtainsacceleratedtrainingspeedsandstate-of-the-artperformanceacrossvariousneuromorphicdatasetswithouttheneedforanyregularisationandusingfewerspikescomparedtostandardSNNs.1INTRODUCTIONArtificialneuralNetworks(ANNs)areubiquitousinachievingstate-of-the-artperformanceacrossvariousdomains,suchasimagerecognition(Heetal.,2016),naturallanguageprocess-ing(NLP)(Brownetal.,2020)andcomputergames(Silveretal.,2017;Vinyalsetal.,2019).Thesenetworkshavealsoprovenusefulforstudyingthebrainduetotheirarchitecturalsimilarities(Richardsetal.,2019)andhavefurtheradvancedourunderstandingofthecomputationalpro-cessesunderlyingthevisualandauditorysystem(Harperetal.,2016;Singeretal.,2018;Cadenaetal.,2019;Francl&McDermott,2022;Yamins&DiCarlo,2016).However,ANNshavebeencriti-cisedfortheirsubstantialenergydemandsresultingfromtheircontinuedexponentialgrowthinsize(Strubelletal.,2019;Schwartzetal.,2020),asexemplifiedbytheGPTlanguagemodelsscal-ingfrom110millionto1.5billionto175billionparameterstodeliverever-improvingadvancesacrossvariousNLPtasks(Radfordetal.,2018;2019;Brownetal.,2020).Furthermore,theappli-cabilityofANNstoneuroscienceisconfinedbytheiractivationfunction,asthebrainemploysspikesratherthancontinuous-valuedoutputsusedbyANNunits.Spikingneuralnetworks(SNNs)areatypeofbinaryneuralnetwork(Figure1a),whichover-comethesechallengesastheyconsumedrasticallylessenergythanANNswhendeployedonneuromorphichardware(Wunderlichetal.,2019)andtheirbiologicalrealismmakesthemafavourablemodelforstudyingthebraininsilico(Vogelsetal.,2011;Denève&Machens,2016).However,SNNtrainingremainsachallengingproblemduetothenon-differentiablebinaryac-tivationfunctionemployedbythespikingneurons.Thishashistoricallyresultedinsolutionsthatimposeconstraintsontheneurons,suchasratecodes(O’Connoretal.,2013;Esseretal.,2015;Rueckaueretal.,2016),oronlyallowingneuronstospikeatmostonce(Bohteetal.,2002;Mostafa,2017;Comsaetal.,2020).Arecentproposalknownassurrogategradienttrainingcanovercometheselimitationsandhasbeenshowntoimprovetrainingonchallengingdatasetsusingmodelsofincreasingbiologicalrealism(Eshraghianetal.,2021).Surrogategradienttrainingreplacestheundefinedderivateoftheneuron’sactivationfunctionwithasurrogatefunctionandusesbackpropagationthroughtime(BPPT)fortraining,sinceSNNsareaparticularformofrecurrentneuralnetwork(RNN)(Neftcietal.,2019).SNNsthusexperiencemanyshortcomingsassociatedwithRNNs,suchastheirnotablyslowtrainingtimesresultingfromtheirsequentialnature(Kuchaiev&Ginsburg,2017;Vaswanietal.,2017).Furthermore,SNNsrequiremultipleregularisationtermstoavoid1UnderreviewasaconferencepaperatICLR2023abFigure1:a.Left:Thestandardleakyintegrateandfire(LIF)model.Right:Inputandoutputactivityoftheneuron(bottompanel:Inputraster,lowermiddlepanel:InputcurrentI,uppermiddlepanel:MembranepotentialVwiththedottedlinerepresentingthefiringthreshold,andtoppanel:OutputspikesS).b.Left:Ourd-blockmodel(hered=3blockswithedgesdenotedbyverticaldottedredlines).Right:Inputandoutputactivityoftheneuron(sameasintheLIFmodelbutwithachangeintheuppermiddlepanel,wherethed-blockcomputesmembranepotentialswithoutreset ̃V).overfittingandtoobtainsparsespikingactivity(Zenke&Vogels,2021;Perez-Nieves&Goodman,2021),wherethelatterisimportantinobtainingenergy-efficientcomputationsonneuromorphichardware(Pandaetal.,2020).Inthiswork,weaddresstheseshortcomingsbyproposinganewSNNmodelwithstochasticab-soluterefractoryperiodsandrecurrentconductancelatencies.Theabsoluterefractoryperiodisthebriefperiodafteraspikeduringwhichaneuroncannotspikeagain;itisafeatureofthebiologynottypicallyincorporatedintostandardSNNs.Inarecentstudy,Tayloretal.(2022)pro-posedamodel-whichwerefertoasablock-foracceleratedtrainingofSNNsinwhichindividualneuronsspikeatmostonce.Weextendthismodeltobeingmulti-spikeandrecurrent,byrecur-rentlyconnectingandconcatenatingtheseblocksacrosstime,andhencerefertoourmodelasthed-blockmodel,aseachneuroncanspikeatmostdtimeswhenemployingdblocks(Figure1b).Ourcontributionsaresummarisedasfollows.1.WeproposeanewSNNmodelwhichusesfewersequentialoperationscomparedtocon-ventionalSNNsandishenceforthmoreparallelisableonGPUs.Weexperimentallyval-idateourmodeltoobtainacceleratedtrainingspeedsonsyntheticbenchmarks(upto34×speedup)andvariousneuromorphicdatasets(achievingupto9×speedup).2.OurmodeltheoreticallyconsumeslessenergythanconventionalSNNswhenem-ployedonneuromorphichardware,asitemitsfewerspikesthanstandardSNNsdur-inginferenceacrossmultipleneuromorphicdatasets.Furthermore,itdoesnotincludeanytrainingregularisation,thusavoidingtheneedfortime-consumingandenergy-demandinghyperparametersearchestofindsuitableregularisationvalues.3.Notablyweobtainstate-of-the-artresultsonchallengingneuromorphicdatasetswithan86.20%and68.16%accuracyontheSpikingHeidelbergDataset(SHD)andSpikingSpeechCommands(SSC)datasetrespectively,raisingperformanceby∼3%and∼8%overpreviouslypublishedresultsusingstandardSNNs.2BACKGROUNDANDRELATEDWORK2.1STANDARDSPIKINGMODELAspikingneuralnetwork(SNN)isatypeofrecurrentneuralnetworkinwhichneuronsoutputbinarysignalsknownasspikes.Aneuroniinlayerl(consistingofRN(l)neurons)emitsaspikeS(l)i[t]=f(V(l)i[t])∈{0,1}attimetifitsmembranepotentialV(l)i[t]reachesfiringthresholdVth.f(V(l)i[t])=!1,ifV(l)i[t]>Vth0,otherwise(1)2UnderreviewasaconferencepaperatICLR2023Theleakyintegrateandfire(LIF)modeldescribestheevolutionofthemembranepotentialV(l)i(t)forrestingpotentialVrest∈R,membranetimeconstantτ∈RandinputresistanceR∈R(Gerstneretal.,2014).τdV(l)i(t)dt=−V(l)i(t)+Vrest+RI(l)i(t)(2)Withoutlossofgeneralitythemodelisnormalised(V(l)i(t)∈[0,1]byVrest=0,Vth=1,R=1)anddiscretised(Tayloretal.,2022),suchthatforsimulationtimestepst∈{1,...,T}:V(l)i[t+1]=βV(l)i[t]+(1−β)\"b(l)i+N(l−1)#j=1W(l)ijS(l−1)j[t+1]+N(l)#j=1Wrec(l)ijS(l)j[t]$%&’(InputcurrentI(l)i[t+1]−S(l)i[t]%&’(Spikereset(3)Themembranepotentialischargedbytheconstantbiascurrentsourceb(l)i,andtheinputS(l−1)[t]∈RN(l−1)andoutputspikesS(l)[t−1]∈RN(l)throughfeedforwardW(l)∈RN(l)×N(l−1)andrecurrentconnectivityWrec(l)∈RN(l)×N(l)respectively.Ateverytimestepthemembranepo-tentialdissipatesbyafactor0≤β=exp(−∆tτ)≤1(forsimulationtimeresolution∆t∈R).ThemembranepotentialisatrestVrest=0intheabsenceofanyinputcurrentandaspikeS(l)i[t]=1isoutputtedifthepotentialrisesabovethefiringthresholdVth=1(afterwhichthepotentialisreducedbackclosetorestingstate).2.2TRAININGTECHNIQUESTodate,SNNtrainingremainschallenging.Thesuccessofbackpropagation(Rumelhartetal.,1986)inANNsdoesnotnaturallytranslatetoSNNsduetotheirnon-differentiablenature.Vari-ousmethodshavebeenproposedtocircumventthisissue,however,theseeitherfailtoproperlyutilisethetemporaldynamicsofneuronsorresultinslowtrainingtimes.Shadowtraining:ANNtoSNNconversionInsteadoftrainingaSNN,shadowtrainingconvertsanalreadytrainedANNtoaSNNsuchthatthefiringratesoftheSNNneuronsapproximatetheactivationsoftheANNunits(O’Connoretal.,2013;Esseretal.,2015;Rueckaueretal.,2016;2017).AlthoughthismethodpermitsdeepSNNstoperformwellonchallenginglarge-scaledatasetslikeImagenet(Dengetal.,2009),itenduresvariousshortcomings.Firstly,itrequireslongsimulationdurationstoobtainreasonablepredictionaccuracies,whichhavebeenarguedtolargelydiminishtheenergyefficienciesofSNNsemulatedonneuromorphichardware(David-son&Furber,2021).Secondly,convertedSNNsperformworsethanANNsduetotheconversionprocess.Thirdly,althoughthisperformancegapcanbereducedbycouplingANNandSNNtrain-ing(Wuetal.,2021a;b;Kheradpishehetal.,2021),themodellingapplicabilitytoneuroscienceislimitedduetotheimposedrate-code(whereasthebrainmightinsteadprocessstimuliusingatemporalcode(Guyonneauetal.,2004;Cariani,1999)).DirectSNNtrainingAlthoughthestandarduseofbackpropagationisprohibited,alternativeapproacheshavebeenproposedtoestimatethegradientsofnetworkweights.Perturbationlearningrandomlyperturbsnetworkweightstoapproximategradients,yetthismethodrequiresmanytrialstoaverageoutnoiseandscalespoorlywithgrowingnetworkarchitectures(Williams,1992;Seung,2003).Latencylearningcalculatesthegradientsatthetimeofspikingas,unlikespikes,timeiscontinuous(Bohteetal.,2002;Mostafa,2017;Comsaetal.,2020;Kheradpisheh&Masquelier,2020).However,thismethodenforcesthemodellingconstraintthatneuronsspikeatmostonceandisaffectedbythedeadneuronproblem,wherealackofspikeactivityisdetri-mentaltolearning.Surrogategradientlearningcircumventstheseissuesbyreplacingtheunde-finedderivativeofthespikefunctionwithasurrogatefunction(Esseretal.,2016;Hunsberger&Eliasmith,2015;Zenke&Ganguli,2018;Leeetal.,2016).Thismethodallowsnetworkstolearntemporaldynamicssinceitpassesgradientsthroughtime(Bellecetal.,2018;Shrestha&Orchard,2018;Neftcietal.,2019),andhasthusbecomethestatusquofordirectlytrainingSNNs.How-ever,learningisveryslowasthenetworkissequentiallysimulatedateverypointintime(whichweovercomebyprocessingmultipletimestepsatonce).3UnderreviewasaconferencepaperatICLR2023block-2block-1block-3abFigure2:Computationalgraphs.a.Standardmodel.ThemembranepotentialsV(1)[t]arerecur-rentlydependentonthepriorvaluesV(1)[t−1]andchargedbyinputspikesS(0)[t]andoutputspikesS(1)[t]throughfeedforwardandrecurrentweights(denotedbydottedredlines)respec-tively.OutputspikesaregeneratedbypassingthemembranepotentialsV(1)[t]throughthespikeactivationfunction.b.Ourd-blockmodel.Inputspikesareprocessedbyd(hered=3)equallengthblocks,whereeveryblockisasingle-spikeSNN(wherewehaveadoptedthefastTayloretal.(2022)model).Withinlayerlandblockn,inputspikesS(l−1)[t]chargethemembranepo-tentialswithoutreset ̃V(l,n)[t],whicharemappedtospikeoutputsS(l)[t]usingafastgroupingofoperationsm(l,n)[t](seesection3.1).Recurrentconnections(dottedredlines)transportoutputspikesfromoneblocktoanother.AcceleratedsurrogategradienttrainingSomeworkhasaddressedtheslowtrainingtimesofSNNsusingsurrogategradients.Perez-Nieves&Goodman(2021)managedtoobtainfastertrain-ingspeedsbydevelopingasparsebackpropalgorithm,whichonlypassesgradientsthroughthemembranepotentialsthatareclosetothefiringthreshold.Althoughtheymanagetoacceler-atethebackwardpassupto70×,thesespeedupsdependonthedevelopmentofcustomCUDAkernels,whichonlysupportsimplefeedforwardarchitectures(whereasweacceleratetrainingofSNNswithrecurrentconnectivityandtrainableneuraltimeconstants).Otherworkshaveman-agedtospeeduptrainingbyremovingthebackwardpassandperformingalllearningonline(Bellecetal.,2020;Murray,2019).Theseapproaches,however,resultininferiorperformanceaccuraciesonvariousdatasetsincomparisontostandardsurrogategradientdescent.Lastly,acorelimitationofallofthesespeedupapproachesistheprerequisiteofsequentiallysimulatingthenetworkateverypointintime(whereaswelessenthisbottleneck).3FASTERTRAININGWITHTHEd-BLOCKMODELWeproposeanewandfastalgorithmicimplementationforaSNN-calledd-block-withstochas-ticabsoluterefractoryperiodsandstochasticrecurrentconductancelatencies.Wedevelopourmodelbyextendingtheacceleratedsingle-spikeSNNbyTayloretal.(2022)frombeingsingle-spikeandfeedforward,tobeingmulti-spikeandrecurrent.3.1THESINGLE-SPIKEBLOCKMODELInthestandardSNN,theinputspikesS(0)[t]chargethemembranepotentialsV(1)[t]andout-putspikesS(1)[t]areemittedateverypointintimet(Figure2a).Thissequentialprocessingishoweverslow.InarecentstudyTayloretal.(2022)proposedafasterSNNmodel-withtheas-sumptionofsingle-spikeoutputsandfeedforwardconnectivity-whichavoidsthissequentialcomputationbyinsteadcomputingalloutputspikesS(1)[t]simultaneouslyacrosstime.Thismodel,whichwerefertoasablock(orangeboxinFigure2b),quicklytransformsinputspikestooutputspikesusingthreesteps:1.TimeseriesofpresynapticspikesS(l−1)j1aremappedtoatimeseriesofinputcurrentsI(l)iusingatensormultiplicationI(l)i[t]=)N(l−1)j=1W(l)ijS(l−1)j[t].1Boldfacevariablesdenotesarraysasopposedtoscalarvalues.4UnderreviewasaconferencepaperatICLR20232.InsteadofcomputingmembranepotentialsV(1)[t](asdoneinthestandardmodel),theblockcalculatesmembranepotentialswithoutreset ̃V(1)[t](byexcludingtheresetterminEquation3),whicharecalculatedusingafastconvolution ̃V(l)i[t]=βtV(l)i[0]+(1−β)*I(l)i⊛β+[t](whereβ=[β0,β1,···,βT−1]).3.CorrectoutputspikesS(l)iareobtainedusingthefollowingoperations(whichwegroupasminFigure2).(1)Erroneousoutputspikes ̃S(l)iareobtainedbypassingtheno-resetmembranepotentials ̃V(l)ithroughthespikefunctionf.(2)Theseerroneousoutputspikesaretransformedintoalatentencodingz(l)i=)tk=1 ̃S(l)i[k](t−k+1),whereeveryel-ementthereinencodesanorderingofthespikes.(3)CorrectoutputspikesareobtainedthroughtransformationS(l)i=g(z(l)i),whereg(z(l)i)[t]=!1,ifz(l)i[t]=10,otherwise(4)3.2EXTENDINGTHESINGLE-SPIKEBLOCKTOMULTI-SPIKEBLOCKSTheblockmodelonlyemitsasinglespikeoverthesimulationperiodT.Toextendthemodeltousemultiplespikes,wedividethesimulationperiodTintodchunksofequallengthT/dandprocesseachchunkbyablockusingthesameneuralparameters(i.e.weightsharingthesynapticconnectivityandmembranetimeconstants;Figure2b).Iftheinputdataisnotdivisiblebyd,wesplitthedataintodchunksoflength⌊T/d⌋andafinalchunkoflengthT−d·⌊T/d⌋.Inthismulti-blockextension,whichwerefertoasthed-blockmodel,neuronscanspikeuptodtimesoverthesimulationperiodT.EveryneuronisgovernedbyastochasticabsoluterefractoryperiodasaneuroncanspikeatmostoncewithinablockoflengthT/dandwillthereforeneedtowaitbetween1andT/dtimestepsbeforeitcanspikeagaininthenextblock.Fromhereonforthweindexvariablesoflayerlinblocknwithsuperscript(l,n)(e.g.theno-resetmembranepotentialvalueattimetofneuroniinlayerlandblocknisdenotedas ̃V(l,n)i[t]).3.3INCLUDINGRECURRENTCONNECTIONSTheblockmodeldoesnotincludeanyrecurrentconnectivity.Nowthatwehaveextendedthemodeltobemulti-spike,wecanincluderecurrentconnectivitybyconnectingtheoutputspikesofblock-nasinputtothenextblock-(n+1)(seeredlinesinFigure2).Weachievethisbycre-atingabinaryvectorofspikestates(i.e.spikeornospike)foreachneuronwithinablockandfeedingtheseasinputtothesameneuronsinthenextblock.Specifically,theoutputspikesS(l,n)∈RN(l)×TinlayerlandblocknareflattenedalongtimemaxtS(l,n)[t]∈RN(l)(todenoteiftheneuronshavespikedwithintheblock’sduration)beforetransportingthemtothenextblockn+1viatherecurrentconnectivityWrec(l).2DifferentfromthestandardSNN,theseconnectionshaveavariableconductionlatencyequivalentindurationtotherefractoryperiodsoftheafferentneuronsinlayerlandblock-n.Thestarting(no-reset)membranepotentialofneuroniinlayerlandblock-(n+1)isthencomputedfromtheflattenedoutputspikesofblock-nandthefirstinputspikestoblock-(n+1). ̃V(l,n+1)i[1]=\"b(l)i+N(l−1)#j=1W(l)ijS(l−1,n+1)j[1]$%&’(Feedforwardcurrent+N(l)#j=1Wrec(l)ijmaxtS(l,n)[t]%&’(Recurrentcurrent(5)3.4TRAININGTHED-BLOCKMODELThed-blockmodelistrainedusingbackpropogationwithsurrogategradients(Neftcietal.,2019),wherewereplacetheundefinedderivativeofthespikefunction(Equation1)withafastsigmoidfunctiondfsur(V)dV=(βsur|V|+1)−2(Zenke&Ganguli,2018),whichhasbeenshowntoworkwell2Forsimplicitywehaveomittedthebatchdimension,whichisincludedinourimplementationandusedinourexperiments.5UnderreviewasaconferencepaperatICLR2023Figure3:Trainingspeedupofourd-blockmodeloverthestandardSNNforfeedforwardandrecurrentnetworks.a.Feedforwardnetworktrainingspeedupasafunctionofthenumberofblocksdandsimulationstepst(forfixedhiddenneuronsn=100andbatchsizeb=128).b.Feedforwardnetworktrainingspeedupasafunctionofthenumberofblocksdandhiddenneu-ronsn(forfixedsimulationstepst=512andbatchsizeb=128).c.Sameasa.butforrecurrentnetworks.d.Sameasb.butforrecurrentnetworks.inpractice(Zenke&Vogels,2021)(herehyperparameterβsurdefinestheslopeofthegradient).Exacttrainingdetailsforgeneratingpredictions,thelossfunction,optimisationproceduresandhyperparameterscanbefoundintheAppendix.3.5THEORETICALSPEEDUPANDPERFORMANCEADVANTAGESOurmodelismoreparallelisablethanthestandardSNNmodel.Thecomputationalcomplex-ityO(NT)ofasingleneuronwithinthestandardmodelissmallerthanthatofourmodelO(dN⌊T/d⌋2)3,forNpresynapticneurons,Tsimulationtimestepsanddnumberofblocks.However,thesequentialcomplexityofourmodelisO(d)incomparisontotheO(T)complexityofthestandardmodel.Thus,choosingdsmallerthanTandusingGPUs-designedtoexecutemanyoperationsinparallel-leadstotheoreticalfastertrainingspeedswithinourmodel.Wehypothesisethedesignofourmodeltobemorerobusttooverfittingas-dependingonthechosennumberofblocksd-ourmodelemitslessspikes,thusactingasaformofimplicitregular-isationonthespikecode.Inaddition,thestochasticabsoluterefractoryperiodsandstochasticrecurrentconductancelatenciesactasaformofnoiseregularisation.Lastly,ourmodelobtainsamoresalientflowofgradientthroughtimeduetotherecurrentconductancelatenciesactingasaformskipconnection(Srivastavaetal.,2015;Heetal.,2016),butthroughtime.4EXPERIMENTSANDRESULTSWeevaluatethetrainingspeedupandperformanceofourmodelonrealdatasetsincomparisontopriorwork.WeusedPyTorch(Paszkeetal.,2017)forallimplementationswithbenchmarksandtrainingconductedonaclusterofNVIDIATeslaV100GPUs.4.1SPEEDUPBENCHMARKSWebenchmarkedthetimerequiredtocompleteasinglesurrogategradienttrainingstepinourd-blockmodelandthestandardSNNforadifferentnumberofblocksd,hiddenunitsn,simulationstepstandbatchsizesbonasyntheticspikedataset(seeAppendix).3ThecomputationalcomplexityO(dN⌊T/d⌋2)ofourmodelcomesfromsimulatingdblocksoflength⌊T/d⌋,whereakerneloflength⌊T/d⌋isconvolvedovereachoftheNpresynapticspiketrains.6UnderreviewasaconferencepaperatICLR2023RobustspeedupforadifferentnumberofsimulationstepsConsideringonlyfeedforwardSNNs(Figure3a-b),wefindourd-blockmodeltotrainfasterforagrowingnumberofsimu-lationsteps,withamaximumtrainingspeedupof34×ford=15blocksandt=2048simulationstepsinasinglehiddenlayermodelof100feedforwardneurons(Figure3a).Thesespeedupsarerobustoverdifferentnumbersofblocks(e.g.weobtainspeedupsof21×,21×and18×usingd=10,d=20andd=30blocksrespectivelyovert=1024simulationsteps),yetstarttodeclinewhenthenumberofblocksapproachesthenumberofsimulationsteps(e.g.weobtainaslower\"speedup\"of0.4×whenusingd=30blocksandt=32simulationsteps).4Speedupsarefurtherraisedwhenusingsmallerbatchsizes(withbatchsizesb=32andb=64obtainingamaximumspeedupof40×and37×respectively)orfixedmembranetimeconstants(obtainingamaximumspeedupof44×;SeeAppendix).RobustspeedupsusingdifferentnumberofhiddenunitsAlsoforfeedforwardSNNs,wefindourd-blockmodeltomaintainrobusttrainingspeedupsoverlargernumberofhiddenunits(e.g.obtaininga13×speedupwhenusingn=200,n=400orn=600hiddenunitsford=10blocksandt=512simulationsteps;Figure3b).Again,thesespeedupsaremorepronouncedwhenusingsmallerbatchsizes(obtaininga15×speedupwhenusingn=200,n=400orn=600hiddenunitsford=10blockswithbatchsizeb=32)orfixedmembranetimeconstants(obtaininga16×speedupwhenusingn=200,n=400orn=600hiddenunitsford=10blocks;seeAppendix).FastertrainingspeedsusingrecurrentconnectivityOurrecurrentlyconnectedmodelobtainsfastertrainingoverthestandardrecurrentSNN.Thesespeedupsareslightlygreaterthanthesubstantialspeedupsachievedwhenbothmodelsonlyemployfeedforwardconnectivity.Again,intherecurrentcasethetrainingspeedsincreasewiththenumberofsimulationsteps(withamaximumspeedupof35×ford=15blocksandt=2048simulationsteps;Figure3c)andarerobustoverlargernumbersofhiddenunits(withan18×,17×and17×speedupwhenusingn=200,n=400andn=600hiddenunitsrespectivelyford=10blocksandt=512simulationsteps;Figure3d).Asinthefeedforwardmodel,trainingspeedupsarefurtheramplifiedintherecurrentmodelwhenusingsmallerbatchsizes(withbatchsizesb=32andb=64obtainingamaximumspeedupof41×and39×respectively)orfixedmembranetimeconstants(obtainingamaximumspeedupof44×;SeeAppendix).4.2PERFORMANCEONREALDATASETSWetestedtheapplicabilityofourmodelondifferentneuromorphicdatasetsofincreasingdiffi-culty.Weconfinedourselvestoneuromorphicdatasets-ratherthanimagedatasets-asthesedatasetsarespecificallycraftedforSNNs,andbestutilisetheirtemporalandneuraldynamics(i.e.leakandspiketiming).ThesimplestdatasetistheN-MNISTdataset(Orchardetal.,2015),inwhichtheMNISTimagedataset(LeCun,1998)isconvertedintospikesusinganeuromorphicvisionsensor.MorechallengingaretheSpikingHeidelbergDataset(SHD)andSpikingSpeechCommands(SSC)datasets,inwhichspokendigitsandcommandsareconvertedintospikesus-ingamodelofauditorybushycellsinthecochlearnucleus(Crameretal.,2020).Thesechalleng-ingtemporaldatasetsprovideagoodbasetoevaluateourmodelasweareabletostudytheeffectofusingmultipleblocksandrecurrentconnectivity,whereasotherdatasetslikeimagesarelesssuitedastheyarereadilysolvedusingsingle-spikeandnon-recurrentSNNs(Zhouetal.,2021).State-of-the-artresultsonneuromorphicdatasetsWeobtaincompetitiveaccuraciesacrossthedifferentneuromorphicdatasets,reachinganaccuracyof98.04%,86.20%and68.16%ontheN-MNIST,SHDandSSCdatasetrespectively(Table1).Notably,weimproveperformanceonthechallengingSHDandSSCdatasetby∼3%and∼8%respectivelyoverpriorpublishedresultsusingstandardrecurrentlyconnectedLIFnetworks.Improvingperformanceusingrecurrence,moreblocksandadditionallayersWetrainedmultiplemodels(comprisingasinglehiddenlayerof128neurons)acrossthedifferentdatasetsto4Ourd-blockmodelisidenticaltothestandardSNNmodelwhenthenumberofblocksdisequaltothenumberofsimulationstepst,yetourmodelperformsmorecomputationalworkpertimestepandhencetrainingslowsdownasdtendstotinthelimit.7UnderreviewasaconferencepaperatICLR2023Figure4:Analysisofourd-blockmodelonchallengingneuromorphicdatasets.Weuseasinglerecurrentlyconnectedhiddenlayernetworkof128neuronsandreportresultsforthreerepeatrunsofthemodelforwhichthemeanands.d.areplotted.a.Accuracyasafunctionofthenumberofblocksdusingfeedforwardandrecurrentconnectivity.b.Accuracywiththespikeresetbeingattachedordetachedfromthecomputationalgraphduringtraining.c.Accuracyasafunctionofanincreasingnumberofhiddenlayers.d.TrainingspeedupofourmodelvsthestandardSNNasafunctionofthenumberofblocksd.e.ReductioninspikesduringinferenceofourmodelvsthestandardSNNasafunctionofblocksd.Table1:Performancecomparisontoexistingliterature(†denotesdataaugmentation,βdenotestrainabletimeconstantsandwereportourresultsinboldasanaverageofthreerepeatruns).DatasetModelArchitectureNeuronmodelAccuracy(%)N-MNISTourmodel400-10(d=30)recurrentLIFβ98.04±0.6Shrestha&Orchard(2018)500-500-10feedforwardLIF98.89±0.06Leeetal.(2016)800-10feedforwardLIF98.66Wuetal.(2018)400-400-10feedforwardLIF98.78SHDourmodel256-256-20(d=30)recurrentLIFβ86.20±0.33Crameretal.(2020)†1024-20recurrentLIF83.2±1.30Perez-Nievesetal.(2021)128-20recurrentLIFβ82.7±0.80Eshraghian&Lu(2022)3000-20recurrentLIF82.27±0.27Zenke&Vogels(2021)256-20recurrentLIF82.0±0.02Crameretal.(2022)186-20recurrentLIF76.2±1.3Crameretal.(2020)128-20recurrentLIF71.4±1.90SSCourmodel256-256-256-35(d=30)recurrentLIFβ68.16±0.28Perez-Nievesetal.(2021)128-20recurrentLIFβ60.1±0.7Crameretal.(2020)128-20recurrentLIF50.9±1.18UnderreviewasaconferencepaperatICLR2023investigatetheeffectofincludingrecurrenceandincreasingthenumberofblocks.Wefindthatincludingrecurrentconnectionsinourmodelimprovespredictionaccuraciesacrossalldatasets,especiallyonthemorechallengingSHDandSSCdatasets(Figure4a).Increasingthenumberofblocksalsoimprovesperformance(Figure4a).However,performancestartstodropforalargenumberofblocks(d=40acrossalldatasets),likelyduetooverfittingaswedonotincludeanyregularisation.Wealsoinvestigatetheeffectofdetachingtheflowofgradientsthroughtherecur-rentconnections,asdoingsohasbeenshowntoimprovetraininginstandardrecurrentSNNs(Zenke&Vogels,2021).Weobservenodegradationinperformancewhendetachingthespikeresettermsfromthecomputationalgraphsduringtrainingandfindthatitimprovestheperfor-manceontheSSCdataset(Figure4b).Lastly,weinvestigatetheeffectofusingadditionalhiddenlayersandfindthatadditionallayersimproveaccuraciesontheSHDandSCCdatasets,butde-gradeperformanceontheN-MNISTdataset(Figure4c).AcceleratedtrainingWefindthatourmodeltrainsfasterthanthestandardSNN,withourmodelobtainingamaximumtrainingspeedupof∼5×,∼7×,and∼9×ontheN-MNIST,SHD,andSSCdatasets,respectively,ford=10blocks(Figure4d).Thedifferenceinspeedupsisduetothedifferenttemporallengthsandinputandoutputdimensionsofthedatasets.IncreasedspikesparsityduringinferenceOurmodelusesfewerspikesthanstandardSNNsduringinference,witha>50%spikecountreductionacrossalldatasetswhenusingd=30blocksanda>80%reductionwhenusingd=1block(Figure4e).OurmodelthustheoreticallyrequireslessenergythanstandardSNNsifemulatedonneuromorphichardware,astheenergyconsump-tionscalesapproximatelyproportionallywiththenumberofemittedspikes(Pandaetal.,2020).However,wefindthatsparsitystartstodeclineforagrowingnumberofblocks.AlthoughweobtainfavourablesparsityfortheSHD(59%spikecountreduction)andSSC(50%spikecountreduction)datasets,wefindourmodeltoobtainworsesparsityontheN-MNISTdataset(41%spikecountincrease)whenusingd=40blocks.Thissuggeststhatsparsityadvantagesarede-pendentonthenumberofblocksandthedataset.5DISCUSSIONSurrogategradientdescenthasbeenasubstantialalgorithmicdevelopmentfordirectlytrainingSNNs(Eshraghianetal.,2021).However,giventherelationofSNNstoRNNs(Neftcietal.,2019),SNNshavebeenslowtotrainduetotheirsequentialnature,precludingefficientparallelisationonGPUs(Kuchaiev&Ginsburg,2017;Vaswanietal.,2017).InarecentstudyTayloretal.(2022)proposedamodelforacceleratingthetrainingofsingle-spikeSNNs.However,duetothesingle-spikeconstraint,thismodelislessapplicabletochallengingneuromorphicdatasets(comparedtoarecurrentlyconnectedmulti-spikeSNNs).Inthisworkweaddresstheseshortcomingsbyex-tendingtheirmodeltobemulti-spikeandrecurrent,andexperimentallyvalidatethisnewmodeltoachievefastertrainingspeedsonbothsyntheticbenchmarks(upto34×speedup)andonvar-iousneuromorphicdatasets(upto∼9×speedup)comparedtothestandardmulti-spikeSNN.Furthermore,ourmodelachievesstate-of-the-artresultsonthechallengingSHDandSSCneu-romorphicdatasets,withanaccuracyof86.20%and68.16%,respectively,raisingperformanceby∼3%and∼8%overpriorpublishedresultsusingstandardLIFnetworks.Wearealsoabletodosowithoutanytrainingregularisation,whereasotherworksemployvariousregularisationconstraintstoavoidoverfittingandtoobtainanefficientspikecode(Zenke&Vogels,2021;Perez-Nieves&Goodman,2021).Evenso,wefindourmodeltoemitfewerspikesduringinferenceincomparisontostandardSNNs,thustheoreticallyloweringenergyrequirementsrelativetothestandardSNNwhenemulatingourmodelonneuromorphichardware(Pandaetal.,2020).Ourworkcanbeimprovedinvariousavenues.Fastertrainingspeedscouldbeobtainedbyem-ployingasparsebackpropimplementationforourmodel,whichhasshowntobeeffectiveinstandardSNNs(Perez-Nieves&Goodman,2021).Here,backpropismodifiedtoperformlesscomputationbytakingadvantageofthesparsespikingnatureofSNNs.Theaccuracyofourmodelcouldbeimprovedbyusingdifferentsurrogategradients(Zenke&Vogels,2021),includ-ingadditionalmodellingmechanismssuchastemporalattention(Yaoetal.,2021)oradaptivefiringthresholds(Yinetal.,2021),orbyusingblocksofvaryinglengths(wherewehavelimitedourselvestoblocksofthesamelength).9UnderreviewasaconferencepaperatICLR20236REPRODUCIBILITYSTATEMENTWeoutlinethetheoreticalconstructionofourmodelinsection3.Animplementationofourmodelandinstructionstoreplicatetheexperimentsandresultsofthispapercanbefoundathttps://github.com/webstorms/DBlock,wherewepublishourcodeundertheBSD3-ClauseLi-cence.Lastly,trainingdetailsareprovidedintheAppendix.REFERENCESGuillaumeBellec,DarjanSalaj,AnandSubramoney,RobertLegenstein,andWolfgangMaass.Longshort-termmemoryandlearning-to-learninnetworksofspikingneurons.Advancesinneuralinformationprocessingsystems,31,2018.GuillaumeBellec,FranzScherr,AnandSubramoney,EliasHajek,DarjanSalaj,RobertLegenstein,andWolfgangMaass.Asolutiontothelearningdilemmaforrecurrentnetworksofspikingneurons.Naturecommunications,11(1):1–15,2020.SanderMBohte,JoostNKok,andHanLaPoutre.Error-backpropagationintemporallyencodednetworksofspikingneurons.Neurocomputing,48(1-4):17–37,2002.TomBBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.arXivpreprintarXiv:2005.14165,2020.SantiagoACadena,GeorgeHDenfield,EdgarYWalker,LeonAGatys,AndreasSTolias,MatthiasBethge,andAlexanderSEcker.Deepconvolutionalmodelsimprovepredictionsofmacaquev1responsestonaturalimages.PLoScomputationalbiology,15(4):e1006897,2019.PeterCariani.Temporalcodingofperiodicitypitchintheauditorysystem:anoverview.Neuralplasticity,6(4):147–172,1999.IuliaMComsa,KrzysztofPotempa,LucaVersari,ThomasFischbacher,AndreaGesmundo,andJyrkiAlakuijala.Temporalcodinginspikingneuralnetworkswithalphasynapticfunction.InICASSP2020-2020IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP),pp.8529–8533.IEEE,2020.BenjaminCramer,YannikStradmann,JohannesSchemmel,andFriedemannZenke.Theheidel-bergspikingdatasetsforthesystematicevaluationofspikingneuralnetworks.IEEETransac-tionsonNeuralNetworksandLearningSystems,2020.BenjaminCramer,SebastianBillaudelle,SimeonKanya,AronLeibfried,AndreasGrübl,VitaliKarasenko,ChristianPehle,KorbinianSchreiber,YannikStradmann,JohannesWeis,etal.Sur-rogategradientsforanalogneuromorphiccomputing.ProceedingsoftheNationalAcademyofSciences,119(4):e2109194119,2022.SimonDavidsonandSteveBFurber.Comparisonofartificialandspikingneuralnetworksondigitalhardware.FrontiersinNeuroscience,15:345,2021.SophieDenèveandChristianKMachens.Efficientcodesandbalancednetworks.Natureneuro-science,19(3):375–382,2016.JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehier-archicalimagedatabase.In2009IEEEconferenceoncomputervisionandpatternrecognition,pp.248–255.Ieee,2009.JasonKEshraghianandWeiDLu.Thefinelinebetweendeadneuronsandsparsityinbinarizedspikingneuralnetworks.arXivpreprintarXiv:2201.11915,2022.JasonKEshraghian,MaxWard,EmreNeftci,XinxinWang,GregorLenz,GirishDwivedi,Mo-hammedBennamoun,DooSeokJeong,andWeiDLu.Trainingspikingneuralnetworksusinglessonsfromdeeplearning.arXivpreprintarXiv:2109.12894,2021.10UnderreviewasaconferencepaperatICLR2023SteveKEsser,RathinakumarAppuswamy,PaulMerolla,JohnVArthur,andDharmendraSModha.Backpropagationforenergy-efficientneuromorphiccomputing.Advancesinneuralinformationprocessingsystems,28,2015.StevenK.Esser,PaulA.Merolla,JohnV.Arthur,AndrewS.Cassidy,RathinakumarAppuswamy,AlexanderAndreopoulos,DavidJ.Berg,JeffreyL.McKinstry,TimothyMelano,DavisR.Barch,CarmelodiNolfo,PallabDatta,ArnonAmir,BrianTaba,MyronD.Flickner,andDharmen-draS.Modha.Convolutionalnetworksforfast,energy-efficientneuromorphiccomputing.ProceedingsoftheNationalAcademyofSciences,113(41):11441–11446,2016.doi:10.1073/pnas.1604850113.AndrewFranclandJoshHMcDermott.Deepneuralnetworkmodelsofsoundlocalizationrevealhowperceptionisadaptedtoreal-worldenvironments.NatureHumanBehaviour,6(1):111–133,2022.WulframGerstner,WernerMKistler,RichardNaud,andLiamPaninski.Neuronaldynamics:Fromsingleneuronstonetworksandmodelsofcognition.CambridgeUniversityPress,2014.RudyGuyonneau,RufinVanRullen,andSimonJThorpe.Temporalcodesandsparserepresen-tations:akeytounderstandingrapidprocessinginthevisualsystem.JournalofPhysiology-Paris,98(4-6):487–497,2004.NicolSHarper,OliverSchoppe,BenDBWillmore,ZhanfengCui,JanWHSchnupp,andAndrewJKing.Networkreceptivefieldmodelingrevealsextensiveintegrationandmulti-featureselec-tivityinauditorycorticalneurons.PLoScomputationalbiology,12(11):e1005113,2016.KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimagerecog-nition.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.770–778,2016.EricHunsbergerandChrisEliasmith.Spikingdeepnetworkswithlifneurons.arXivpreprintarXiv:1510.08829,2015.SaeedRezaKheradpishehandTimothéeMasquelier.Temporalbackpropagationforspikingneuralnetworkswithonespikeperneuron.InternationalJournalofNeuralSystems,30(06):2050027,2020.SaeedRezaKheradpisheh,MaryamMirsadeghi,andTimothéeMasquelier.Spikingneuralnet-workstrainedviaproxy.arXivpreprintarXiv:2109.13208,2021.DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,2014.OleksiiKuchaievandBorisGinsburg.Factorizationtricksforlstmnetworks.arXivpreprintarXiv:1703.10722,2017.YannLeCun.Themnistdatabaseofhandwrittendigits.http://yann.lecun.com/exdb/mnist/,1998.JunHaengLee,TobiDelbruck,andMichaelPfeiffer.Trainingdeepspikingneuralnetworksusingbackpropagation.Frontiersinneuroscience,10:508,2016.HeshamMostafa.Supervisedlearningbasedontemporalcodinginspikingneuralnetworks.IEEEtransactionsonneuralnetworksandlearningsystems,29(7):3227–3235,2017.JamesMMurray.Localonlinelearninginrecurrentnetworkswithrandomfeedback.Elife,8:e43299,2019.EmreONeftci,HeshamMostafa,andFriedemannZenke.Surrogategradientlearninginspikingneuralnetworks:Bringingthepowerofgradient-basedoptimizationtospikingneuralnet-works.IEEESignalProcessingMagazine,36(6):51–63,2019.PeterO’Connor,DanielNeil,Shih-ChiiLiu,TobiDelbruck,andMichaelPfeiffer.Real-timeclassi-ficationandsensorfusionwithaspikingdeepbeliefnetwork.Frontiersinneuroscience,7:178,2013.11UnderreviewasaconferencepaperatICLR2023GarrickOrchard,AjinkyaJayawant,GregoryKCohen,andNitishThakor.Convertingstaticimagedatasetstospikingneuromorphicdatasetsusingsaccades.Frontiersinneuroscience,9:437,2015.PriyadarshiniPanda,SaiAparnaAketi,andKaushikRoy.Towardscalable,efficient,andaccuratedeepspikingneuralnetworkswithbackwardresidualconnections,stochasticsoftmax,andhybridization.FrontiersinNeuroscience,14:653,2020.AdamPaszke,SamGross,SoumithChintala,GregoryChanan,EdwardYang,ZacharyDeVito,ZemingLin,AlbanDesmaison,LucaAntiga,andAdamLerer.Automaticdifferentiationinpytorch.2017.NicolasPerez-NievesandDanGoodman.Sparsespikinggradientdescent.AdvancesinNeuralInformationProcessingSystems,34:11795–11808,2021.NicolasPerez-Nieves,VincentCHLeung,PierLuigiDragotti,andDanFMGoodman.Neuralheterogeneitypromotesrobustlearning.Naturecommunications,12(1):1–9,2021.AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improvinglanguageun-derstandingbygenerativepre-training.2018.AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog,1(8):9,2019.BlakeARichards,TimothyPLillicrap,PhilippeBeaudoin,YoshuaBengio,RafalBogacz,AmeliaChristensen,ClaudiaClopath,RuiPonteCosta,ArchydeBerker,SuryaGanguli,etal.Adeeplearningframeworkforneuroscience.Natureneuroscience,22(11):1761–1770,2019.BodoRueckauer,Iulia-AlexandraLungu,YuhuangHu,andMichaelPfeiffer.Theoryandtoolsfortheconversionofanalogtospikingconvolutionalneuralnetworks.arXivpreprintarXiv:1612.04052,2016.BodoRueckauer,Iulia-AlexandraLungu,YuhuangHu,MichaelPfeiffer,andShih-ChiiLiu.Con-versionofcontinuous-valueddeepnetworkstoefficientevent-drivennetworksforimageclas-sification.Frontiersinneuroscience,11:682,2017.DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams.Learningrepresentationsbyback-propagatingerrors.nature,323(6088):533–536,1986.RoySchwartz,JesseDodge,NoahASmith,andOrenEtzioni.Greenai.CommunicationsoftheACM,63(12):54–63,2020.HSebastianSeung.Learninginspikingneuralnetworksbyreinforcementofstochasticsynaptictransmission.Neuron,40(6):1063–1073,2003.SumitBShresthaandGarrickOrchard.Slayer:Spikelayererrorreassignmentintime.Advancesinneuralinformationprocessingsystems,31,2018.DavidSilver,JulianSchrittwieser,KarenSimonyan,IoannisAntonoglou,AjaHuang,ArthurGuez,ThomasHubert,LucasBaker,MatthewLai,AdrianBolton,etal.Masteringthegameofgowithouthumanknowledge.nature,550(7676):354–359,2017.YosefSinger,YayoiTeramoto,BenDBWillmore,JanWHSchnupp,AndrewJKing,andNicolSHarper.Sensorycortexisoptimizedforpredictionoffutureinput.elife,7:e31557,2018.RupeshKumarSrivastava,KlausGreff,andJürgenSchmidhuber.Highwaynetworks.arXivpreprintarXiv:1505.00387,2015.EmmaStrubell,AnanyaGanesh,andAndrewMcCallum.Energyandpolicyconsiderationsfordeeplearninginnlp.arXivpreprintarXiv:1906.02243,2019.LukeTaylor,AndrewKing,andNicolHarper.Acceleratingspikingneuralnetworktraining.arXivpreprintarXiv:2205.15286,2022.12UnderreviewasaconferencepaperatICLR2023AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.Advancesinneuralinforma-tionprocessingsystems,30,2017.OriolVinyals,IgorBabuschkin,WojciechMCzarnecki,MichaëlMathieu,AndrewDudzik,Juny-oungChung,DavidHChoi,RichardPowell,TimoEwalds,PetkoGeorgiev,etal.Grandmasterlevelinstarcraftiiusingmulti-agentreinforcementlearning.Nature,575(7782):350–354,2019.TimPVogels,HenningSprekeler,FriedemannZenke,ClaudiaClopath,andWulframGerstner.Inhibitoryplasticitybalancesexcitationandinhibitioninsensorypathwaysandmemorynet-works.Science,334(6062):1569–1573,2011.RonaldJWilliams.Simplestatisticalgradient-followingalgorithmsforconnectionistreinforce-mentlearning.Machinelearning,8(3):229–256,1992.JibinWu,YansongChua,MaluZhang,GuoqiLi,HaizhouLi,andKayChenTan.Atandemlearn-ingruleforeffectivetrainingandrapidinferenceofdeepspikingneuralnetworks.IEEETrans-actionsonNeuralNetworksandLearningSystems,2021a.JibinWu,ChenglinXu,XiaoHan,DaquanZhou,MaluZhang,HaizhouLi,andKayChenTan.Progressivetandemlearningforpatternrecognitionwithdeepspikingneuralnetworks.IEEETransactionsonPatternAnalysisandMachineIntelligence,2021b.YujieWu,LeiDeng,GuoqiLi,JunZhu,andLupingShi.Spatio-temporalbackpropagationfortraininghigh-performancespikingneuralnetworks.Frontiersinneuroscience,12:331,2018.TimoWunderlich,AkosFKungl,EricMüller,AndreasHartel,YannikStradmann,SyedAhmedAamir,AndreasGrübl,ArthurHeimbrecht,KorbinianSchreiber,DavidStöckel,etal.Demon-stratingadvantagesofneuromorphiccomputation:apilotstudy.Frontiersinneuroscience,13:260,2019.DanielLKYaminsandJamesJDiCarlo.Usinggoal-drivendeeplearningmodelstounderstandsensorycortex.Natureneuroscience,19(3):356–365,2016.ManYao,HuanhuanGao,GuangsheZhao,DinghengWang,YihanLin,ZhaoxuYang,andGuoqiLi.Temporal-wiseattentionspikingneuralnetworksforeventstreamsclassification.InPro-ceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pp.10221–10230,2021.BojianYin,FedericoCorradi,andSanderMBohté.Accurateandefficienttime-domainclassi-ficationwithadaptivespikingrecurrentneuralnetworks.NatureMachineIntelligence,3(10):905–913,2021.FriedemannZenkeandSuryaGanguli.Superspike:Supervisedlearninginmultilayerspikingneuralnetworks.Neuralcomputation,30(6):1514–1541,2018.FriedemannZenkeandTimPVogels.Theremarkablerobustnessofsurrogategradientlearningforinstillingcomplexfunctioninspikingneuralnetworks.NeuralComputation,33(4):899–925,2021.ShiboZhou,XiaohuaLi,YingChen,SanjeevTChandrasekaran,andArindamSanyal.Temporal-codeddeepspikingneuralnetworkwitheasytrainingandrobustperformance.InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,pp.11143–11151,2021.AAPPENDIXA.1SYNTHETICSPIKEDATASETFORTHESPEEDBENCHMARKSAsoutlinedin(Tayloretal.,2022),weconstructedbinaryinputspikesofshapeB×N×T(Bbeingthebatchsize,NthenumberofinputneuronsandTthenumberofsimulationsteps),suchthateverybatchdimensionbhadafiringraterb∼U(umin,umax)uniformlysampled(withumin=0Hzandumax=200Hz).ForeverybatchdimensionwegeneratedarandombinaryspikematrixofshapeN×T,suchthateveryinputneuroninthematrixhadanexpectedfiringrateofrbHz.13UnderreviewasaconferencepaperatICLR2023A.2TRAININGDETAILSANDHYPERPARAMETERSA.2.1READOUTNEURONSEverynetworkhadanoutputlayerofreadoutneurons(containingthesamenumberofneuronsasthenumberofclasseswithinthedatasettrainedon),whereweremovedthespikeandresetmechanism(asdoneinZenke&Vogels(2021)).Theoutputofreadoutneuroncinresponsetoinputsamplecwastakentobethesummatedmembranepotentialovertimeob,c=)tVLb,c[t].A.2.2BETACLIPPINGToenforcecorrectneurondynamicsweclippedthevaluesofβ(l)iintotherange[0,1].β(l)i=!1,ifβ(l)i>10,ifβ(l)i<0(6)A.2.3WEIGHTINITIALISATIONAllnetworkconnectivityweightsweresampledfromauniformdistributionU(−(N−1,(N−1)whereNisthenumberofafferentconnections.Allbiaseswereinitialisedas0.Allneuronsinthehiddenlayerswereinitialisedwithamembranetimeconstantτ=10msandτ=20msforthereadoutneurons.A.2.4SUPERVISEDTRAININGLOSSWetrainedallnetworkstominimiseacross-entropyloss(withBandCbeingthenumberofbatchsamplesanddatasetclassesrespectively)L=−1BB#b=1C#c=1yb,clog(pb,c)(7)Variableyb,c∈{0,1}Cistheonehottargetvectorandpb,carethenetworkpredictionprobabilities,whichwereobtainedbypassingthereadoutneuronoutputsob,cthroughthesoftmaxfunction.pb,c=expob,c)Ck=1expob,k(8)A.2.5SURROGATEGRADIENTWeusedthefastsigmoidfunctionasoursurrogategradient(Zenke&Ganguli,2018),whichhasbeenshowntoworkwellinpractice(Zenke&Vogels,2021).Herehyperparameterβsur(whichwesetto10inallexperiments)definestheslopeofthegradient.dfsur(V)dV=(βsur|V|+1)−2(9)A.2.6TRAININGPROCEDUREWeusedtheAdamoptimiser(withdefaultparameters)(Kingma&Ba,2014)foralltrainingstart-ingwithaninitiallearningrateof0.001,whichwasdecayedbyafactorof10everytimethenum-berofepochsreachedanewmilestone.Amodelwassavedifitmanagedtolowerthetrainingerrorattheendofeachepoch.A.2.7TRAININGHYPERPARAMETERSANDEXTENDEDSPEEDUPRESULTS14UnderreviewasaconferencepaperatICLR2023Table2:Datasetandcorrespondingtrainingparametersforbestperformingmodels.N-MNISTSHDSSCDataset(train/test)60k/10k8156/226475466/20382Inputneurons1156700700Datasetclasses102035Epochs506080Learningrate0.0010.0010.001BatchsizeB128128128SimulationstepsT300500500Timeresolution∆t(ms)122Milestones(30)(30)(30,60)acebdfFigure5:Trainingspeedupofourd-blockmodeloverthestandardSNNforfeedforwardandrecurrentnetworksasafunctionofthenumberofblocksdandsimulationstepst(forfixedhiddenneuronsn=100).a.-b.Batchsize32.c.-d.Batchsize64.e.-f.Batchsize128butusingfixedmembranetimeconstants.Leftcolumn:Feedforwardnetworksandrightcolumn:Recurrentnetworks.15UnderreviewasaconferencepaperatICLR2023acebdfFigure6:Trainingspeedupofourd-blockmodeloverthestandardSNNforfeedforwardandrecurrentnetworksasafunctionofthenumberofblocksdandhiddenneuronsn(forfixedsim-ulationstepst=512).a.-b.Batchsize32.c.-d.Batchsize64.e.-f.Batchsize128butusingfixedmembranetimeconstants.Leftcolumn:Feedforwardnetworksandrightcolumn:Recurrentnetworks.16",
    "reference": "# Summary Of The Paper\n\nThis work extends the 1-block model in Taylor et al. (2022) to the d-block model. Compared with the LIF model, the d-block model achieves accelerated computing on GPU by using fewer sequential operations.\n\n# Strength And Weaknesses\n\nStrength:\n\nThe proposed model indeed enables accelerated computing on GPU, and achieves sota results on some benchmarks.\n\nWeaknesses:\n\n1. This work is quite a naive extension of Taylor et al. (2022). If the authors of this work and Taylor et al. (2022) are the same, I highly recommend the authors to combine the two works into one.\n\n2. The authors need to describe the training method explicitly in the main content. The proposed model can be treated as a modified LIF model, which is irrelevant to the training methods. From appendix A.3.5, which is not detailed, I guess that the existing surrogate gradient method is adopted. The accelerated training on GPU is due to the model's parallel computing nature, not due to some novel training method. The authors should make it clear.\n\n3. Can the d-block model be implemented on neuromorphic hardware in an event-driven manner? First, the 1-block model is equivalent to the single-spike LIF model, so I do not worry about it. But for the d-block model, the spikes from the first 3 time steps are used in the 4th time step. Is it implementable? will it be implementable on future neuromorphic hardware?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The authors do not describe the used training methods. Audiences need to guess what they do.\n\nQuality: The model achieves good performance and training efficiency. But it will be useless if it cannot be implemented on neuromorphic chips.\n\nNovelty: This work is a naive extension of Taylor et al. (2022).\n\nReproducibility: Good.\n\n# Summary Of The Review\n\nIf the authors want me to raise the score, they should convince me about the novelty and the implementability of the model.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nSCALE-INVARIANT BAYESIAN NEURAL NETWORKS WITH CONNECTIVITY TANGENT KERNEL\n\nSung-Yub Kim1, Sihwan Park1, Kyungsu Kim3,4,5∗, Eunho Yang1,2∗ Korea Advanced Institute of Science and Technology (KAIST)1, AITRICS2, Samsung Medical AI Research Center3, Sungkyunkwan University School of Medicine4, Massachusetts General Hospital and Harvard Medical School5 sungyub.kim@kaist.ac.kr, sihwan.park@mli.kaist.ac.kr kskim.doc@gmail.com, eunhoy@kaist.ac.kr\n\nABSTRACT\n\nStudying the loss landscapes of neural networks is critical to identifying generalizations and avoiding overconfident predictions. Flatness, which measures the perturbation resilience of pre-trained parameters for loss values, is widely acknowledged as an essential predictor of generalization. While the concept of flatness has been formalized as a PAC-Bayes bound, it has been observed that the generalization bounds can vary arbitrarily depending on the scale of the model parameters. Despite previous attempts to address this issue, generalization bounds remain vulnerable to function-preserving scaling transformations or are limited to impractical network structures. In this paper, we introduce new PAC-Bayes prior and posterior distributions invariant to scaling transformations, achieved through the decomposition of perturbations into scale and connectivity components. In this way, this approach expands the range of networks to which the resulting generalization bound can be applied, including those with practical transformations such as weight decay with batch normalization. Moreover, we demonstrate that scale-dependency issues of flatness can adversely affect the uncertainty calibration of Laplace approximation, and we propose a solution using our invariant posterior. Our proposed invariant posterior allows for effective measurement of flatness and calibration with low complexity while remaining invariant to practical parameter transformations, also applying it as a reliable predictor of neural network generalization.\n\n1\n\nINTRODUCTION\n\nNeural networks (NNs) have succeeded tremendously, but understanding their generalization mechanism in real-world scenarios remains challenging (Kendall & Gal, 2017; Ovadia et al., 2019). Although it is widely recognized that NNs naturally generalize well and avoid overfitting, the underlying reasons are not well understood (Neyshabur et al., 2015b; Zhang et al., 2017; Arora et al., 2018). Recent studies on the loss landscapes of NNs attempt to address these issues. For example, Hochreiter & Schmidhuber (1995) proposed the flat minima (FM) hypothesis, which states that loss stability for parameter perturbations positively correlates with network generalizability, as empirically demonstrated by Jiang et al. (2020).\n\nHowever, the FM hypothesis still has limitations. According to Dinh et al. (2017), rescaling two successive layers can arbitrarily degrade a flatness measure while maintaining the generalizability of NNs. Meanwhile, Li et al. (2018) argued that weight decay (WD) leads to a contradiction of the FM hypothesis in practice: Although WD sharpens pre-trained NNs (i.e., decreased loss resilience), it generally improves the generalization. In short, they suggest that transformations on network parameters (e.g., re-scaling layers and WD) may lead to contradictions to the FM hypothesis. A thorough discussion on this can be found in Appendix E.\n\nTo resolve this contradiction, we investigate PAC-Bayesian prior and posterior distributions to derive a new scale-invariant generalization bound. As a result, our bound guarantees invariance for a general\n\n∗Correspondence to\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nclass of function-preserving scale transformations with a broad class of networks. Specifically, our bound is more general than existing works (Tsuzuku et al., 2020; Kwon et al., 2021), both in terms of transformations (e.g., activation-wise rescaling (Neyshabur et al., 2015a) and WD with batch normalization (BN; Ioffe & Szegedy (2015))) that guarantee invariance and in terms of NN architectures. Therefore, our bound ensures no FM contradiction for the first time, which should not occur in practical NNs, including ResNet (He et al., 2016) and Transformer (Vaswani et al., 2017).\n\nOur generalization bound is derived from scale invariances of prior and posterior distributions, guaranteeing not only its scale invariance but also the scale invariance of its substance, the KullbackLeibler (KL) divergence-based kernel. We call this kernel an empirical Connectivity Tangent Kernel (CTK), as a modification of empirical Neural Tangent Kernel (Jacot et al., 2018) with the scaleinvariance property. Moreover, we define a new sharpness metric as the trace of CTK, named Connectivity Sharpness (CS). We show via empirical studies that CS predicts NN generalization performance better than existing sharpness measures (Liang et al., 2019; Neyshabur et al., 2017).\n\nIn Bayesian NN regimes, we connect the contradictions of the FM hypothesis with the issue of amplifying predictive uncertainty. Then, we alleviate this issue by using a Bayesian NN based on the posterior distribution of our PAC-Bayesian analysis. We name this Bayesian NN as Connectivity Laplace (CL), as it can be seen as a variation of Laplace approximation (LA; MacKay (1992)) using a different Jacobian. Specifically, we demonstrate the major pitfalls of WD with BN in LA and show how to remedy this issue using CL.1 We summarize our contributions as follows:\n\n• Our novel PAC-Bayes generalization bound guarantees invariance for general function-preserving scale transformations with a broad class of networks (Sec. 2.2 and 2.3). We empirically verify this bound gives non-vacuous results for ResNet with 11M parameters (Sec. 2.4).\n\n• Based on our bound, we propose a low-complexity sharpness metric CS (Sec. 2.5), which empiri-\n\ncally shows a stronger correlation with generalization error than other metrics (Sec. 4.1).\n\n• To prevent overconfident predictions, we show how our scale-invariant Bayesian NN can be used\n\nto solve pitfalls of WD with BNs, proving its practicality (Sec. 3 and 4.2).\n\n2 PAC-BAYES BOUND WITH SCALE-INVARIANCE\n\nThis section introduces a data-dependent PAC-Bayes generalization bound without scale-dependency issues. To this end, we introduce our setup in Sec. 2.1, construct the scale-invariant PAC-Bayes prior and posterior in Sec. 2.2, and present the detailed bound in Sec. 2.3. Then, we demonstrate the effectiveness of this bound for ResNet-18 with CIFAR in Sec. 2.4. An efficient proxy of this bound without complex hyper-parameter optimization is provided in Sec. 2.5.\n\n2.1 BACKGROUND\n\nSetup and Definitions. We consider a Neural Network (NN), f (·, ·) : RD × RP → RK, given input x ∈ RD and network parameter θ ∈ RP . Hereafter, for simplicity, we consider vectors as single-column matrices unless otherwise stated. We use the output of NN f (x, θ) as a prediction for input x. Let S := {(xn, yn)}N n=1 denote the independently and identically distributed (i.i.d.) training data drawn from true data distribution D, where xn ∈ RD and yn ∈ RK are input and output representations of n-th training instance, respectively. For simplicity, we denote concatenated input and output of all instances as X ∈ RN D and Y ∈ RN K, respectively, and f (X , θ) ∈ RN K as a concatenation of {f (xn, θ)}N n=1. Given a prior distribution of network parameters p(θ) and a likelihood function p(S|θ) := (cid:81)N n=1 p(yn|f (xn, θ)), Bayesian inference defines posterior distribution of network parameter θ as p(θ|S) := exp(−L(S, θ))/Z(S), where L(S, θ) := − log p(θ) − (cid:80)N n=1 log p(yn|xn, θ) is training loss and Z(S) := (cid:82) p(θ)p(S|θ)dθ is the normalizing factor. For example, the likelihood function for a regression task will be Gaussian: p(y|x, θ) = N (y|f (x, θ), σ2IK) where σ is (homoscedastic) observation noise scale. For a classification task, we treat it as a one-hot regression task following Lee et al. (2019a); He et al. (2020). While we adopt this modification for theoretical tractability, Hui & Belkin (2021) showed this modification offers good performance competitive to the cross-entropy loss. Details on this modification are given in Appendix C.\n\n1https://github.com/sungyubkim/connectivity-tangent-kernel\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nLaplace approximation. In general, the exact computation for the Bayesian posterior of a network parameter is intractable. The Laplace approximation (LA; MacKay (1992)) is proposed to approximate the posterior distribution with a Gaussian distribution defined as pLA(ψ|S) ∼ θL(S, θ∗))−1) where θ∗ ∈ RP is a pre-trained parameter with training loss and N (ψ|θ∗, (∇2 ∇2\n\nθL(S, θ∗) ∈ RP ×P is a Hessian matrix of loss function w.r.t. parameter at θ∗.\n\nRecent works on LA replace the Hessian matrix with (Generalized) Gauss-Newton matrix to make computation easier (Khan et al., 2019; Immer et al., 2021). With this approximation, the LA posterior of the regression problem can be represented as\n\npLA(ψ|S) ∼ N (ψ|θ∗, (IP /α2 (cid:124) (cid:123)(cid:122) (cid:125) Damping\n\n+ J⊤ (cid:124)\n\n)−1) θ Jθ/σ2 (cid:125) (cid:123)(cid:122) Curvature\n\n(1)\n\nwhere α, σ > 0, IP ∈ RP ×P is a identity matrix, and Jθ ∈ RN K×P is a concatenation of Jθ(x, θ∗) ∈ RK×P (Jacobian of f w.r.t. θ at input x and parameter θ∗) along training input X . Inference with LA requires a further sub-curvature approximation for modern NN architectures (e.g., ResNet (He et al., 2016) and Transformer (Vaswani et al., 2017)) because of the prohibitively large covariance matrix. This approximation includes diagonal, Kronecker-factored approximate curvature (KFAC), last-layer, and subnetwork approximation (Ritter et al., 2018; Kristiadi et al., 2020; Daxberger et al., 2021). Meanwhile, it is well known that proper selection of prior scale α is needed to balance the dilemma between overconfidence and underfitting in LA.\n\nPAC-Bayes bound with data-dependent prior. We consider a PAC-Bayes generalization error bound of classification task used in McAllester (1999); Perez-Ortiz et al. (2021) (especially equation (7) of Perez-Ortiz et al. (2021)). Let P be a PAC-Bayes prior distribution over RP independent of training dataset S, and err(·, ·) : RK×K → [0, 1] be an error function defined separately from the loss function. For any constant δ ∈ (0, 1] and λ > 0, and any PAC-Bayes posterior distribution Q over\n\nRP , the following holds with probability at least 1 − δ: errD(Q) ≤ errS (Q) + where errD(Q) := E(x,y)∼D,θ∼Q[err(f (x, θ), y)], errS (Q) := E(x,y)∼S,θ∼Q[err(f (x, θ), y)], and N denotes the cardinality of S. That is, errD(Q) and errS (Q) are generalization error and empirical error, respectively. The only restriction on P here is that it cannot depend on the dataset S.\n\n2N\n\nN /δ)\n\nKL[Q∥P]+log(2\n\n(cid:113)\n\n√\n\nFollowing the recent discussion in Perez-Ortiz et al. (2021), one can construct data-dependent PACBayes bounds by (i) randomly partitioning dataset S into SQ and SP so that they are independent, (ii) pre-training a PAC-Bayes prior distribution PD only dependent of SP (i.e., PD belongs to a PAC-Bayes prior due to the independence of SQ), (iii) fine-tuning a PAC-Bayes posterior distribution Q dependent of entire dataset S, and (iv) computing empirical error errSQ (Q) with target subset SQ (not entire dataset S). In summary, we modify the aforementioned original PAC-Bayes bound through a data-dependent prior PD as\n\nerrD(Q) ≤ errSQ (Q) +\n\n(cid:115)\n\nKL[Q∥PD] + log(2(cid:112)NQ/δ) 2NQ\n\n(2)\n\nwhere NQ is the cardinality of SQ. We denote sets of input and output of partitioned datasets (SP, SQ) by XP, YP, XQ, YQ for simplicity.\n\n2.2 SCALE-INVARIANT PRIOR AND POSTERIOR FROM LINEARIZATION W.R.T. CONNECTIVITY\n\nOur goal is to construct scale-invariant PD and Q. To this end, we assume a pre-trained parameter θ∗ ∈ RP with the prior dataset SP. This parameter can be attained with standard NN optimization procedures (e.g., stochastic gradient descent (SGD) with momentum). Then, we consider a linearized NN at the pre-trained parameter with an auxiliary variable c ∈ RP as\n\nθ∗ (x, c) := f (x, θ∗) + Jθ(x, θ∗)diag(θ∗)c glin\n\n(3)\n\nwhere diag is a vector-to-matrix diagonal operator. Note that equation 3 is the first-order Taylor approximation of NN with perturbation θ∗ ⊙ c given input x and parameter θ∗: gpert θ∗ (x, c) := f (x, θ∗ + θ∗ ⊙ c) ≈ glin θ∗ (x, c), where ⊙ denotes element-wise multiplication of two vectors. Here we express the perturbation in parameter space as θ∗ ⊙ c instead of a single variable such as δ ∈ RP . By decomposing the scale and connectivity of perturbation, this linearization design matches the\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nscale of perturbation (i.e., θ∗ ⊙ c) to the scale of θ∗ in a component-wise manner. Note that a similar decomposition was proposed in pruning at initialization (Lee et al., 2019c;b) to measure the importance of each connection independently of its weight. However, they only consider this form to predict the effect of each connection before pre-training. Based on equation 3, we define a data-dependent prior (PD) over connectivity as\n\nPθ∗ (c) := N (c | 0P , α2IP ).\n\n(4)\n\nThis distribution can be translated to a distribution over parameter by considering the distribution of perturbed parameter (ψ := θ∗ + θ∗ ⊙ c): Pθ∗ (ψ) := N (ψ | θ∗, α2diag(θ∗)2). We now define the PAC-Bayes posterior over connectivity Q(c) as follows:\n\nQθ∗ (c) := N (c|μQ, ΣQ) ,\n\nΣQJ⊤\n\nc (Y − f (X , θ∗))\n\nμQ :=\n\nΣQ :=\n\nσ2 J⊤ c Jc σ2\n\n(cid:18) IP\n\nα2 +\n\n=\n\nΣQdiag(θ∗)J⊤\n\nθ (Y − f (X , θ∗)) σ2 diag(θ∗)J⊤ θ Jθdiag(θ∗) σ2\n\n(cid:19)−1\n\n(cid:19)−1\n\n=\n\n(cid:18) IP\n\nα2 +\n\n,\n\n(5)\n\n(6)\n\n(7)\n\nwhere Jc ∈ N K × P is a concatenation of Jc(x, 0P ) := Jθ(x, θ∗)diag(θ∗) ∈ RK×P (i.e., Jacobian of perturbed NN gpert θ∗ (x, c) w.r.t. c at input x and connectivity 0P ) along training input X . Indeed, Qθ∗ is the posterior of Bayesian linear regression w.r.t. connectivity c. We refer to Appendix D for detailed derivations. Again, it is equivalent to the posterior distribution over parameter Qθ∗ (ψ) = N (cid:0)ψ|θ∗ + θ∗ ⊙ μQ, (diag(θ∗)−2/α2 + J⊤ θ Jθ/σ2(cid:1)−1 ) where diag(θ∗)−2 := (diag(θ∗)−1)2 by assuming that all components of θ∗ are non-zero. This assumption can be easily satisfied by considering the prior and posterior distributions of non-zero components of NNs only. Although we choose this restriction for theoretical tractability, future works can relax it to achieve diverse predictions by considering the distribution of zero coordinates.\n\nIn summary, a data-dependent PAC-Bayes bound can be computed with our PAC-Bayes distributions. The validity of this data-dependent PAC-Bayes bound is ensured as follows: our PAC-Bayes prior depends on the SP through θ∗, but independent to the SQ that measures the errors. Note that here a two-phase training (i.e., pre-training with SP and fine-tuning with S) explained in Sec. 2.1 is used to attain our PAC-Bayes posterior. Similar ideas of two-phase training with linearization were proposed in the context of transfer learning in Achille et al. (2021); Maddox et al. (2021). In transfer learning, there is a distribution shift between SP and SQ. Therefore, SP cannot be used for their fine-tuning phase in contrast to our PAC-Bayes posterior.\n\nNow we provide an invariance property of our prior and posterior distributions w.r.t. functionpreserving scale transformations as follows: The main intuition behind this proposition is that Jacobian w.r.t. connectivity is invariant to the function-preserving scaling transformation, i.e., Jθ(x, T (θ∗))diag(T (θ∗)) = Jθ(x, θ∗)diag(θ∗). Representative cases of T in Proposition 2.1 are presented in Appendix E to highlight theoretical implications; these include the case of WD applied to the general network, including BN. Proposition 2.1 (Scale-invariance of PAC-Bayes prior and posterior). Let T : RP → RP is a invertible diagonal linear transformation such that f (x, T (ψ)) = f (x, ψ) , ∀x ∈ RD, ∀ψ ∈ RP . Then, both PAC-Bayes prior and posterior are invariant under T :\n\nPT (θ∗)(c) d= Pθ∗ (c), QT (θ∗)(c) d= Qθ∗ (c).\n\nFurthermore, generalization and empirical errors are also invariant to T .\n\n2.3 RESULTING PAC-BAYES BOUND\n\nNow we plug in our prior and posterior into the modified PAC-Bayes generalization error bound in equation 2. As a result, we obtain a novel generalization error bound, named PAC-Bayes-CTK, which is guaranteed to be invariant to scale transformations (hence without the contradiction of FM hypothesis mentioned in Sec. 1).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nTheorem 2.2 (PAC-Bayes-CTK and its invariance). Let us assume pre-trained parameter θ∗ with data SP. By applying Pθ∗ and Qθ∗ to data-dependent PAC-Bayes bound (equation 2), we get\n\nerrD(Qθ∗ ) ≤ errSQ(Qθ∗ ) +\n\n(cid:122)\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:117) (cid:117) (cid:117) (cid:117) (cid:116)\n\nKL divergence (cid:125)(cid:124)\n\nμ⊤ Q μQ 4α2NQ (cid:124) (cid:123)(cid:122) (cid:125) (average) perturbation\n\n+\n\nP (cid:88)\n\ni=1 (cid:124)\n\n(cid:123)(cid:122) sharpness\n\n(cid:125)\n\n(cid:123) h (βi) 4NQ\n\nlog(2(cid:112)NQ/δ) 2NQ\n\n+\n\n(8)\n\nwhere {βi}P σ2 J⊤ is invariant to T for the function-preserving scale transformation by Proposition 2.1.\n\nc Jc)−1 and h(x) := x − log(x) − 1. This upper bound\n\ni=1 are eigenvalues of (IP + α2\n\nNote that recent works on FM contradiction focus only on the scale-invariance of sharpness metrics: Indeed, their generalization bounds are not invariant to scale transformations due to the scaledependent terms (equation (34) in Tsuzuku et al. (2020) and equation (6) in Kwon et al. (2021)). Specifically, these terms are proportional to the norm of pre-trained parameters. On the other hand, the generalization bound in Petzka et al. (2021) (Theorem 11 in their paper) only holds for single-layer NNs, whereas our bound has no restrictions for network structure. As a result, our PAC-Bayes bound is the first scale-invariant PAC-Bayes bound to the best of our knowledge.\n\nThe following corollary explains why we name PAC-Bayes bound in Theorem 2.2 PAC-Bayes-CTK. Corollary 2.3 (Relation between CTK and PAC-Bayes-CTK). Let us define empirical Connectivity Tangent Kernel (CTK) of S as Cθ∗ θ ∈ RN K×N K by removing below term. Note that empirical CTK has T (≤ N K) non-zero eigenvalues of {λi}T i=1, then the followings hold for {β}P i=1 in Theorem 2.2: (i) βi = σ2/(σ2 + α2λi) < 1 for i = 1, . . . , T and (ii) βi = 1 for i = T + 1, . . . , P . Since h(1) = 0, this means P − T terms of summation in the sharpness part of PAC-Bayes-CTK vanish to 0. Furthermore, this sharpness term of PAC-Bayes-CTK is a monotonically increasing function for each eigenvalue of empirical CTK.\n\nc = Jθdiag(θ∗)2J⊤\n\nX := JcJ⊤\n\nCorollary 2.3 clarifies why (cid:80)P i=1 h(βi)/4NQ in Theorem 2.2 is called the sharpness term of PACBayes-CTK: A sharp pre-trained parameter would have large CTK eigenvalues (since eigenvalues of CTK measure the sensitivity of output w.r.t. connectivity), increasing the sharpness term and the generalization gap. Finally, Proposition 2.4 shows that empirical CTK is also scale-invariant. Proposition 2.4 (Scale-invariance of empirical CTK). Let T : RP → RP be an function-preserving scale transformation in Proposition 2.1. Then empirical CTK at parameter ψ is invariant under T :\n\nxy\n\n(9)\n\nCT (ψ)\n\n:= Cψ\n\nxy , ∀x, y ∈ RD, ∀ψ ∈ RP . Remark 2.5 (Connections to empirical NTK). The empirical CTK Cψ xy resembles the existing empirxy := Jθ(x, ψ)Jθ(y, ψ)⊤ ∈ ical Neural Tangent Kernel (NTK) at parameter ψ (Jacot et al., 2018): Θψ RK×K. Note that the deterministic NTK in Jacot et al. (2018) is the infinite-width limiting kernel at initialized parameters, while empirical NTK can be defined on any (finite-width) NNs. We focus on empirical kernels for finite pre-trained parameters throughout the paper, and we leave deterministic kernels defined for future studies. Comparing empirical kernels, the main difference between empirical CTK and the existing empirical NTK is in the definition of Jacobian. In CTK, Jacobian is computed w.r.t. connectivity c while the empirical NTK uses Jacobian w.r.t. parameters θ. Therefore, another PAC-Bayes bound can be derived from the linearization of f lin θ∗ (x, δ) := f (x, θ∗) + Jθ(x, θ∗)δ. As this bound is related to the eigenvalues of Θθ∗ X , we call this bound PAC-Bayes-NTK and provide derivations in Appendix B. Note that PAC-Bayes-NTK is scale-variant as ΘT (ψ) xy in general.\n\n̸= Θψ\n\nxy\n\n2.4 COMPUTING APPROXIMATE BOUND IN REAL WORLD PROBLEMS\n\nTo verify that PAC-Bayes bound in Theorem 2.2 is non-vacuous, we compute it for real-world problems. We use CIFAR-10 and 100 datasets (Krizhevsky, 2009), where the 50K training instances are randomly partitioned into SP of cardinality 45K and SQ of cardinality 5K. We refer to Appendix H for detailed experimental settings.\n\nTo compute equation 8, one needs (i) μQ-based perturbation term, (ii) Cθ∗ X -based sharpness term, and (iii) samples from PAC-Bayes posterior Qθ∗ . μQ in equation 6 can be obtained by minimizing\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison between PAC-Bayes-CTK and PAC-Bayes-NTK for ResNet-18\n\nCIFAR-10\n\nParameter scale Trace (×10−4) Perturbation Sharpness KL Test err. (×102)\n\nBound (×102)\n\nCIFAR-100\n\nParameter scale Trace (×10−4) Perturbation Sharpness KL Test err. (×102)\n\nBound (×102)\n\nPAC-Bayes-CTK\n\nPAC-Bayes-NTK\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n1.91 ± 0.04 6.26 ± 0.15 28.92 ± 0.21 17.59 ± 0.17 4.82 ± 0.12\n\n1.91 ± 0.04 5.72 ± 0.09 28.96 ± 0.22 17.34 ± 0.15 4.78 ± 0.11\n\n1.91 ± 0.04 5.77 ± 0.08 28.95 ± 0.22 17.36 ± 0.15 4.78 ± 0.11\n\n1.91 ± 0.04 5.84 ± 0.05 28.95 ± 0.20 17.39 ± 0.13 4.77 ± 0.12\n\n8793.18 ± 227.31 636.55 ± 12.16 728.80 ± 2.69 682.68 ± 4.74 13.00 ± 0.53\n\n2590.97 ± 62.10 564.38 ± 7.56 602.17 ± 2.77 583.27 ± 2.88 8.26 ± 0.17\n\n1107.58 ± 20.64 438.21 ± 10.73 502.32 ± 2.32 470.27 ± 4.32 6.94 ± 0.09\n\n766.26 ± 11.80 288.24 ± 6.38 441.91 ± 2.08 365.07 ± 2.96 6.25 ± 0.08\n\n9.21 ± 0.04\n\n9.21 ± 0.02\n\n9.21 ± 0.02\n\n9.21 ± 0.03\n\n39.00 ± 0.61\n\n32.07 ± 0,25\n\n28.24 ± 0.06\n\n24.84 ± 0.11\n\nPAC-Bayes-CTK\n\nPAC-Bayes-NTK\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n2.33 ± 0.37 14.54 ± 0.25 42.52 ± 5.26 28.53 ± 2.51 21.78 ± 0.14\n\n2.34 ± 0.37 14.32 ± 0.24 42.53 ± 5.26 28.42 ± 2.52 21.82 ± 00.18\n\n2.33 ± 0.37 14.08 ± 0.20 42.52 ± 5.27 28.30 ± 2.55 21.84 ± 0.19\n\n2.34 ± 0.37 13.84 ± 0.14 42.53 ± 5.26 28.19 ± 2.56 21.86 ± 0.21\n\n5830.55 ± 532.26 620.18 ± 6.83 694.45 ± 8.67 657.31 ± 7.21 43.39 ± 0.64\n\n1913.90 ± 244.05 569.16 ± 6.94 580.20 ± 12.22 574.68 ± 8.95 37.06 ± 0.26\n\n1089.53 ± 104.06 459.16 ± 2.86 519.78 ± 7.89 489.47 ± 3.57 32.32 ± 0.32\n\n955.05 ± 66.16 329.29 ± 3.34 504.74 ± 6.10 417.02 ± 3.98 28.37 ± 0.13\n\n27.74 ± 0.37\n\n27.76 ± 0.40\n\n27.75 ± 0.42\n\n27.75 ± 0.42\n\n68.44 ± 0.82\n\n59.96 ± 0.19\n\n51.90 ± 0.04\n\n44.80 ± 0.25\n\n2N ∥Y − f (X , θ∗) − Jcc∥2 + σ2\n\narg minc∈RP L(c) = 1 2α2N c⊤c by first-order optimality condition. Note that this problem is a convex optimization problem w.r.t. c, since c is the parameter of the linear regression problem. We use Adam optimizer (Kingma & Ba, 2014) with a fixed learning rate 1e-4 to solve this. For the sharpness term, we apply the Lanczos algorithm to approximate the eigenspectrum of Cθ∗ X following Ghorbani et al. (2019). We use 100 Lanczos iterations based on their setting. Lastly, we estimate empirical and test errors with 8 samples of CL/LL implementation of the Randomize-Then-Optimize (RTO) framework (Bardsley et al., 2014; Matthews et al., 2017). The pseudo-code and computational complexity of RTO implementation can be found in Appendix F.\n\nTable 1 provides the bounds and related terms of PAC-Bayes-CTK (Theorem 2.2) and NTK (Theorem B.1). First, we found that our estimated PAC-Bayes-CTK and NTK are non-vacuous (i.e., estimated bounds are better than guessing at random) for ResNet-18 with 11M parameters. Note that deriving non-vacuous bound is challenging in PAC-Bayes analysis: only a few PAC-Bayes works (Dziugaite & Roy, 2017; Zhou et al., 2018; Perez-Ortiz et al., 2021) verified the non-vacuous property of their bounds, and other PAC-Bayes works (Foret et al., 2020; Tsuzuku et al., 2020) did not. To check the invariance property of PAC-Bayes-CTK, we scale the scale-invariant parameters in ResNet-18 (i.e., parameters preceding BN layers) for fixed constants {0.5, 1.0, 2.0, 4.0}. Due to BN layers, these transformations do not affect the function represented by NN, and the error bounds should be preserved for scale-invariant bounds. Table 1 shows that PAC-Bayes-CTK bound is stable to these transformations. On the other hand, PAC-Bayes-NTK bound is very sensitive to the parameter scale.\n\n2.5 CONNECTIVITY SHARPNESS AND ITS EFFICIENT COMPUTATION\n\nNow, we focus on the fact that the trace of CTK is also invariant to the parameter scale by Proposition 2.4. Unlike PAC-Bayes-CTK and NTK, traces of CTK and NTK do not require onerous hyperparameter selection of δ, α, σ. Therefore, we simply define CS(θ∗) := tr(Cθ∗ X ) as a practical sharpness measure at θ∗, named Connectivity Sharpness (CS) to detour the complex computation of PAC-Bayes-CTK. This metric can be easily applied to find NNs with better generalization, similar to other sharpness metrics (e.g., trace of Hessian), as shown in Jiang et al. (2020). We evaluate the detecting performance of CS in Sec. 4.1. The following corollary shows how CS can explain the generalization performance of NNs, conceptually. Corollary 2.6 (Connectivity sharpness, Informal). Let us assume CTK and KL divergence terms of PAC-Bayes-CTK as defined in Theorem 2.2. Then, if CS vanishes to zero or infinity, the KL divergence term of PAC-Bayes-CTK also does so.\n\nAs traces of a matrix can be efficiently estimated by Hutchinson’s method (Hutchinson, 1989), one can compute the CS without explicitly computing the entire CTK. We refer to Appendix F for detailed procedures of computing CS. As CS is invariant to function-preserving scale transformations by Proposition 2.4, it does not contradict the FM hypothesis.\n\n3 BAYESIAN NNS WITH SCALE-INVARIANCE\n\nIn this section, we discuss a practical implication of our posterior distribution (equation 5) used in the PAC-Bayes analysis. To this end, we first interpret our PAC-Bayes posterior as a modified result of\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Correlation analysis of sharpness measures with generalization gap. We refer Sec. 4.1 for the details of sharpness measures (row) and correlation metrics for sharpness-generalization relationship (column).\n\nτ (rank corr.)\n\nnetwork depth network width mini-batch size learning rate weight decay\n\nΨ (avg.)\n\nK (cond. MI)\n\ntr(H)\n\n0.706\n\n0.764 0.687 0.976 0.966 -0.031\n\n0.672\n\n0.320\n\ntr(F)\n\n0.679\n\n0.652 0.922 0.810 0.713 -0.103\n\n0.599\n\n0.243\n\ntr(Θθ∗\n\n)\n\nSO\n\nPO\n\nSM\n\nPM\n\nAS\n\nFR\n\nCS\n\n0.703\n\n0.978 0.330 0.988 1.000 0.402\n\n0.739\n\n0.352\n\n0.490\n\n0.436\n\n0.473\n\n0.636\n\n0.755\n\n0.649\n\n0.837\n\n-0.358 -0.533 0.859 0.829 0.647\n\n0.289\n\n0.039\n\n-0.719 -0.575 0.893 0.874 0.711\n\n0.774 0.495 0.909 0.057 0.168\n\n0.545 0.564 0.750 0.621 0.211\n\n0.756 0.827 0.829 0.794 0.710\n\n0.771 0.921 0.685 0.565 0.373\n\n0.978 0.978 0.905 0.897 0.742\n\n0.237\n\n0.481\n\n0.538\n\n0.783\n\n0.663\n\n0.900\n\n0.041\n\n0.049\n\n0.376\n\n0.483\n\n0.288\n\n0.539\n\nLA (MacKay, 1992). Then, we demonstrate that this modification improves existing LA when WD is applied to NNs with normalization layers (Proposition 3.1). One can view the parameter space version of Qθ∗ as a modified version of LA posterior (equation 1) by (i) substituting parameter-dependent damping (diag(θ∗)−2) for isotropic damping and (ii) adding perturbation θ∗ ⊙ μQ to the mean of Gaussian distribution. Here, we focus on the effect of replacing the damping term of LA in batch-normalized NNs in the presence of WD. We refer to Antoran et al. (2021; 2022) for the discussion on the effect of adding perturbation to the LA with linearized NNs.\n\nThe main difference between the covariance terms of LA in equation 1 and equation 7 is the definition of Jacobian (i.e., parameter or connectivity) similar to the difference between empirical CTK and θ Jθ/σ2(cid:1)−1 NTK in Remark 2.5. Therefore, we name pCL(ψ|S) ∼ N (ψ|θ∗, (cid:0)diag(θ∗)−2/α2 + J⊤ )\nas Connectivity Laplace (CL) approximated posterior.\n\nTo compare CL posteriors against existing LAs, we explain how WD with BN can produce unexpected side effects of amplifying uncertainty. This side effect can be quantified if we consider linearized NN for LA, called Linearized Laplace (LL; Foong et al. (2019)). Assuming σ2 ≪ α2, the predictive distribution of LL and CL are\n\nθ∗ (x, ψ)|pLA(ψ|S) ∼ N (f (x, θ∗), α2Θθ∗ f lin θ∗ (x, ψ)|pCL(ψ|S) ∼ N (f (x, θ∗), α2Cθ∗ f lin\n\nxx − α2Θθ∗ xx − α2Cθ∗\n\nxX Θθ∗−1 xX Cθ∗−1\n\nX Θθ∗ X x) X Cθ∗\n\n(11) for any input x ∈ Rd where X in subscript means concatenation. We refer to Appendix G for the detailed derivations. The following proposition illustrates how WD with BN can increase the prediction uncertainty of equation 10. Proposition 3.1 (Uncertainty amplifying effect for LL). Let us assume that Wγ : RP → RP is a WD on scale-invariant parameters (e.g., parameters preceding BN layers) by multiplying γ < 1 and all the non-scale-invariant parameters are fixed. Then, the predictive uncertainty of LL is amplified by 1/γ2 > 1 while the predictive uncertainty of CTK is preserved as\n\nX x)\n\n(10)\n\nVarψ∼pLA(ψ|S)(f lin Varψ∼pCL(ψ|S)(f lin\n\nWγ (θ∗)(x, ψ)) = Varψ∼pLA(ψ|S)(f lin Wγ (θ∗)(x, ψ)) = Varψ∼pCL(ψ|S)(f lin\n\nθ∗ (x, ψ))\n\nθ∗ (x, ψ))/γ2\n\nwhere Var(·) is variance of random variable.\n\nSince the primal regularization effect of WD actually occurs when combined with BN as experimentally shown in Zhang et al. (2019), Proposition 3.1 describes a real-world issue. Recently, Antoran et al. (2021; 2022) observed similar pitfalls in Proposition 3.1. However, their solution requires a more complicated hyper-parameter search: independent prior selection for each normalized parameter group. On the other hand, CL does not increase the hyper-parameter to be optimized compared to LL. We believe this difference will make CL more attractive to practitioners.\n\n4 EXPERIMENTS\n\nHere we describe experiments demonstrating (i) the effectiveness of Connectivity Sharpness (CS) as a generalization measurement metric and (ii) the usefulness of Connectivity Laplace (CL) as a general-purpose Bayesian NN: With CS and CL, we can resolve the contradiction in the FM hypothesis concerning the generalization of NNs and attain stable calibration performance for various ranges of prior scales.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 3: Test negative log-likelihood on two UCI variants (Hernández-Lobato & Adams, 2015; Foong et al., 2019). We marked the best method among the four in bold and marked the best method among LL/CL in italics.\n\nOriginal (Hernández-Lobato & Adams, 2015)\n\nGAP variants (Foong et al., 2019)\n\nDeep Ensemble\n\nMCDO\n\nLL\n\nCL\n\nDeep Ensemble\n\nMCDO\n\nLL\n\nCL\n\nboston_housing concrete_strength energy_efficiency kin8nm naval_propulsion power_plant protein_structure wine yacht_hydrodynamics\n\n2.90 ± 0.03 3.06 ± 0.01 0.74 ± 0.01 -1.07 ± 0.00 -4.83 ± 0.00 2.81 ± 0.00 2.89 ± 0.00 1.21 ± 0.00 1.26 ± 0.04\n\n2.63 ± 0.01 3.20 ± 0.00 1.92 ± 0.01 -0.80 ± 0.01 -3.85 ± 0.00 2.91 ± 0.00 2.96 ± 0.00 0.96 ± 0.01 2.17 ± 0.06\n\n2.85 ± 0.01 3.22 ± 0.01 2.12 ± 0.01 -0.90 ± 0.00 -4.57 ± 0.00 2.91 ± 0.00 2.91 ± 0.00 1.24 ± 0.01 1.20 ± 0.04\n\n2.88 ± 0.02 3.11 ± 0.02 0.83 ± 0.01 -1.07 ± 0.00 -4.76 ± 0.00 2.81 ± 0.00 2.89 ± 0.00 1.27 ± 0.01 1.25 ± 0.04\n\n2.71 ± 0.01 4.03 ± 0.07 0.77 ± 0.01 -0.94 ± 0.00 -2.22 ± 0.33 2.91 ± 0.00 3.11 ± 0.00 1.48 ± 0.01 1.71 ± 0.03\n\n2.68 ± 0.01 3.42 ± 0.00 1.78 ± 0.01 -0.71 ± 0.00 -3.36 ± 0.01 2.97 ± 0.00 3.07 ± 0.00 1.03 ± 0.00 3.06 ± 0.02\n\n2.74 ± 0.01 3.47 ± 0.01 2.02 ± 0.01 -0.87 ± 0.00 -3.66 ± 0.11 2.98 ± 0.00 3.07 ± 0.00 1.45 ± 0.01 1.78 ± 0.02\n\n2.75 ± 0.01 4.03 ± 0.02 0.90 ± 0.02 -0.93 ± 0.00 -3.80 ± 0.07 2.91 ± 0.00 3.13 ± 0.00 1.43 ± 0.00 1.74 ± 0.01\n\n4.1 CONNECTIVITY SHARPNESS AS A GENERALIZATION MEASUREMENT METRIC\n\nBased on the CIFAR-10 dataset, we evaluate three correlation metrics to determine whether CS is more correlated with generalization performance than existing sharpness measures: (a) Kendall’s rank-correlation coefficient (τ ; Kendall (1938)) (b) granulated Kendall’s coefficient and their average (Ψ; Jiang et al. (2020)) (c) conditional independence test (K; Jiang et al. (2020)). In all correlation metrics, a higher value indicates a stronger relationship between sharpness and generalization.\n\nWe compare CS to the following baseline sharpness measures: trace of Hessian (tr(H); Keskar et al. (2017)), trace of empirical Fisher (tr(F); Jastrzebski et al. (2021)), trace of empirical NTK at θ∗, Fisher-Rao (FR; Liang et al. (2019)) metric, Adaptive Sharpness (AS; Kwon et al. (2021)), and four PAC-Bayes bound based measures: Sharpness-Orig. (SO), PAC-Bayes-Orig. (PO), Sharpness-Mag. (SM), and PAC-Bayes-Mag. (PM), which are eq. (52), (49), (62), (61) in Jiang et al. (2020). We compute granulated Kendall’s correlation by using five hyper-parameters (network depth, network width, learning rate, weight decay, and mini-batch size) and three options for each. Thus, we train models with 35 = 243 different training configurations. We vary the depth and width of NN based on VGG-13 (Simonyan & Zisserman, 2015). Further experimental details can be found in Appendix H.\n\nIn Table 2, CS shows the best results for τ , Ψ, and K compared to all other sharpness measures. Additionally, granulated Kendall of CS is higher than other sharpness measures for 3 out of 5 hyperparameters and competitive with other sharpness measures for the remaining hyperparameters. The main difference between our CS and other sharpness measures is in the correlation with weight decay/network width: We found that SO and PM can capture the correlation with weight decay, and hypothesize that this is due to the weight norm term of SO/PO. As this weight norm term would interfere in capturing the sharpness-generalization correlation related to the number of parameters (i.e., width/depth), SO/PO fail to capture correlation with network width in Table 2. On the other hand, CS/AS do not suffer from such a problem. Also, it is notable that FR weakly captures this correlation despite its invariant property. For network width, we found that sharpness measures except for CS, tr(F), AS/FR fail to capture a strong correlation. In summary, only CS/AS detect clear correlations with all hyperparameters; among them, CS captures clearer correlations.\n\n4.2 CONNECTIVITY LAPLACE AS AN EFFICIENT GENERAL-PURPOSE BAYESIAN NN\n\nWe evaluate CL’s effectiveness as a general-purpose Bayesian NN using the UCI and CIFAR datasets. We refer to Appendix H for detailed experimental settings.\n\nUCI regression We implement full-curvature versions of LL and CL and evaluate these to the 9 UCI regression datasets (Hernández-Lobato & Adams, 2015) and its GAP-variants (Foong et al., 2019) to compare calibration performance on in-between uncertainty. We measure test NLL for LL/CL and 2 baselines (Deep Ensemble (Lakshminarayanan et al., 2017) and Monte-Carlo DropOut (MCDO; Gal & Ghahramani (2016))). Eight ensemble members are used in Deep Ensemble, and 32 MC samples are used in LL, CL, and MCDO. Table 3 shows that CL performs better than LL on 6 of 9 datasets. Even though LL produces better calibration results on 3 of the datasets for both settings, the performance gaps between LL and CL are not as severe as on the other 6 datasets.\n\nImage classification. We evaluate the uncertainty calibration performance of CL on CIFAR-10 and 100. As baseline methods, we consider Deterministic network, Monte-Carlo Dropout (MCDO; (Gal & Ghahramani, 2016)), Monte-Carlo Batch Normalization (MCBN; (Teye et al., 2018)), Deep Ensemble (Lakshminarayanan et al., 2017), Batch Ensemble (Wen et al., 2020), and LL (Khan et al.,\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Uncertainty calibration results on CIFAR-100 (Krizhevsky, 2009) for ResNet-18 (He et al., 2016)\n\nNLL (↓)\n\nECE (↓)\n\nBrier. (↓)\n\nAUC (↑)\n\nCIFAR-100\n\nDeterministic MCDO MCBN Batch Ensemble Deep Ensemble Linearized Laplace\n\n1.5370 ± 0.0117 1.4264 ± 0.0110 1.4689 ± 0.0106 1.4029 ± 0.0031 1.0110 1.1673 ± 0.0093\n\n0.1115 ± 0.0017 0.0651 ± 0.0008 0.0998 ± 0.0016 0.0842 ± 0.0005 0.0507 0.0532 ± 0.0010\n\n0.3889 ± 0.0031 0.3925 ± 0.0020 0.3750 ± 0.0028 0.3582 ± 0.0010 0.2740 0.3597 ± 0.0020\n\n- 0.6907 ± 0.0121 0.7982 ± 0.0210 0.7887 ± 0.0115 0.7802 0.8066 ± 0.0120\n\nConnectivity Laplace (Ours)\n\n1.1307 ± 0.0042\n\n0.0524 ± 0.0019\n\n0.3319 ± 0.0005\n\n0.8423 ± 0.0204\n\n(a) NLL\n\n(b) ECE\n\n(c) Brier Score\n\nFigure 1: Sensitivity to α. Expected calibration error (ECE), Negative Log-likelihood (NLL), and Brier score results on corrupted CIFAR-100 for ResNet-18. Showing the mean (line) and standard deviation (shaded area) across four different seeds.\n\n2019). We use the Randomize-Then-Optimize (RTO) implementation of LL/CL in Appendix F. We measure negative log-likelihood (NLL), expected calibration error (ECE; Guo et al. (2017)), and Brier score (Brier.) for ensemble predictions. We also measure the area under receiver operating curve (AUC) for OOD detection, where we set the SVHN (Netzer et al., 2011) dataset as an OOD.\n\nTable 4 shows uncertainty calibration results on CIFAR-100. We refer to Appendix J for results on other settings, including CIFAR-10 and VGGNet. Except for Deep Ensemble, CL shows better results than baselines for all uncertainty calibration metrics. Deep Ensemble performs best in 3 out of 4 metrics, but each ensemble member requires full training. LL and CL, however, require only post-hoc training on pre-trained NNs. Particularly noteworthy is that CL presents competitive results with Deep Ensemble, even with much smaller computations.\n\nRobustness to the selection of prior scale. Figure1 shows the uncertainty calibration results over various α values for LL, CL, and Deterministic (Det.) baseline. As mentioned in previous works (Ritter et al., 2018; Kristiadi et al., 2020), the uncertainty calibration results of LL are extremely sensitive to the selection of α. Especially, LL shows severe under-fitting for large α (i.e., small damping) regime. On the other hand, CL shows stable performance in the various ranges of α.\n\n5 CONCLUSION\n\nIn this work, we proposed a new approach to enhance the robustness of generalization bound using PAC-Bayes prior and posterior distributions. By separating scales and connectivities, our approach achieved invariance to function-preserving scale transformations, which is not addressed by existing generalization error bounds. As a result, our method successfully resolved the contradiction in the FM hypothesis caused by general scale transformation. In addition, our posterior distribution for PAC-Bayes analysis improved the Laplace approximation without significant drawbacks when dealing with weight decay with BN. To further improve our understanding of NN generalization effects, future research could explore extending prior and posterior distributions beyond Gaussian distributions to more task-specific distributions. This could help bridge the gap between theory and practice.\n\n9\n\n1031021011001011.01.21.41.61.82.02.22.4NLLLLCLDet.1031021011001010.0500.0750.1000.1250.1500.1750.200ECELLCLDet.1031021011001010.30.40.50.60.7BrierLLCLDet.Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThis work was supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075 / No.20170-01779) and the National Research Foundation of Korea (NRF) grants (No.2018R1A5A1059921) funded by the Korea government (MSIT). This work was also supported by Samsung Electronics Co., Ltd (No.IO201214-08133-01).\n\nREFERENCES\n\nAlessandro Achille, Aditya Golatkar, Avinash Ravichandran, Marzia Polito, and Stefano Soatto. Lqf: Linear quadratic fine-tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15729–15739, 2021.\n\nJavier Antoran, James Urquhart Allingham, David Janz, Erik Daxberger, Eric Nalisnick, and José Miguel Hernández-Lobato. Linearised laplace inference in networks with normalisation layers and the neural g-prior. In Fourth Symposium on Advances in Approximate Bayesian Inference, 2021.\n\nJavier Antoran, David Janz, James U Allingham, Erik Daxberger, Riccardo Rb Barbano, Eric Nalisnick, and José Miguel Hernández-Lobato. Adapting the linearised laplace model evidence for modern deep learning. In International Conference on Machine Learning, pp. 796–821. PMLR, 2022.\n\nSanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 254–263. PMLR, 10–15 Jul 2018.\n\nJohnathan M Bardsley, Antti Solonen, Heikki Haario, and Marko Laine. Randomize-then-optimize: A method for sampling from posterior distributions in nonlinear inverse problems. SIAM Journal on Scientific Computing, 36(4):A1895–A1910, 2014.\n\nChristopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. github, 2018.\n\nErik Daxberger, Eric Nalisnick, James U Allingham, Javier Antoran, and Jose Miguel HernandezLobato. Bayesian deep learning via subnetwork inference. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 2510–2521. PMLR, 18–24 Jul 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nLaurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1019–1028. PMLR, 06–11 Aug 2017.\n\nGintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Proceedings of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI), 2017.\n\nAndrew YK Foong, Yingzhen Li, José Miguel Hernández-Lobato, and Richard E Turner. between’uncertainty in bayesian neural networks. arXiv preprint arXiv:1906.11537, 2019.\n\n’in-\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization\n\nfor efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059. PMLR, 2016.\n\nBehrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density. In International Conference on Machine Learning, pp. 2232–2241. PMLR, 2019.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1321–1330. PMLR, 06–11 Aug 2017.\n\nBobby He, Balaji Lakshminarayanan, and Yee Whye Teh. Bayesian deep ensembles via the neural tangent kernel. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1010–1022. Curran Associates, Inc., 2020.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nJosé Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In International conference on machine learning, pp. 1861–1869. PMLR, 2015.\n\nSepp Hochreiter and Jürgen Schmidhuber. Simplifying neural nets by discovering flat minima. In G. Tesauro, D. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing Systems, volume 7. MIT Press, 1995.\n\nLike Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs crossentropy in classification tasks. In International Conference on Learning Representations, 2021.\n\nMichael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):1059–1076, 1989.\n\nAlexander Immer, Maciej Korzepa, and Matthias Bauer. Improving predictions of bayesian neural\n\nnets via local linearization. In AISTATS, pp. 703–711, 2021.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448–456, Lille, France, 07–09 Jul 2015. PMLR.\n\nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nStanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo B Kerg, Huan Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof J Geras. Catastrophic fisher explosion: Early phase fisher matrix impacts generalization. In International Conference on Machine Learning, pp. 4772–4784. PMLR, 2021.\n\nYiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning Representations, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAlex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nMaurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81–93, 1938.\n\nNitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th International Conference on Learning Representations, ICLR 2017, 2017.\n\nMohammad Emtiyaz E Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa. Approximate inference turns deep networks into gaussian processes. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAgustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being bayesian, even just a bit, fixes overconfidence in relu networks. In International conference on machine learning, pp. 5436–5446. PMLR, 2020.\n\nA Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of\n\nToronto, 2009.\n\nJungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpnessarXiv preprint\n\naware minimization for scale-invariant learning of deep neural networks. arXiv:2102.11600, 2021.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019a.\n\nJaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 15156–15172. Curran Associates, Inc., 2020.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, and Philip HS Torr. A signal propagation perspective for pruning neural networks at initialization. arXiv preprint arXiv:1906.06307, 2019b.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learning Representations, 2019c.\n\nHao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nTengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 888–896. PMLR, 2019.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nEkaterina Lobacheva, Maxim Kodryan, Nadezhda Chirkova, Andrey Malinin, and Dmitry P Vetrov. On the periodic behavior of neural network training with batch normalization and weight decay. Advances in Neural Information Processing Systems, 34:21545–21556, 2021.\n\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\n\npreprint arXiv:1608.03983, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\n\nence on Learning Representations, 2019.\n\nDavid J. C. MacKay. A practical bayesian framework for backpropagation networks. Neural Comput.,\n\n4(3):448–472, may 1992. ISSN 0899-7667. doi: 10.1162/neco.1992.4.3.448.\n\nWesley Maddox, Shuai Tang, Pablo Moreno, Andrew Gordon Wilson, and Andreas Damianou. Fast adaptation with linearized neural networks. In International Conference on Artificial Intelligence and Statistics, pp. 2737–2745. PMLR, 2021.\n\nAlexander G de G Matthews, Jiri Hron, Richard E Turner, and Zoubin Ghahramani. Sample-thenIn NIPS Workshop on Advances in\n\noptimize posterior sampling for bayesian linear models. Approximate Bayesian Inference, 2017.\n\nDavid A McAllester. Pac-bayesian model averaging. In Proceedings of the twelfth annual conference\n\non Computational learning theory, pp. 164–170, 1999.\n\nRaphael A Meyer, Cameron Musco, Christopher Musco, and David P Woodruff. Hutch++: Optimal stochastic trace estimation. In Symposium on Simplicity in Algorithms (SOSA), pp. 142–155. SIAM, 2021.\n\nKevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011.\n\nBehnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015a.\n\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the\n\nrole of implicit regularization in deep learning. In ICLR (Workshop), 2015b.\n\nBehnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-\n\nization in deep learning. arXiv preprint arXiv:1706.08947, 2017.\n\nRoman Novak, Jascha Sohl-Dickstein, and Samuel S Schoenholz. Fast finite width neural tangent\n\nkernel. In International Conference on Machine Learning, pp. 17018–17044. PMLR, 2022.\n\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchéBuc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nMaria Perez-Ortiz, Omar Risvaplata, John Shawe-Taylor, and Csaba Szepesvári. Tighter risk certifi-\n\ncates for neural networks. Journal of Machine Learning Research, 22(227):1–40, 2021.\n\nHenning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley. Relative\n\nflatness and generalization. Advances in Neural Information Processing Systems, 34, 2021.\n\nJie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nHippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural\n\nnetworks. In International Conference on Learning Representations, 2018.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. In International Conference on Learning Representations, 2015.\n\nMattias Teye, Hossein Azizpour, and Kevin Smith. Bayesian uncertainty estimation for batch normalized deep networks. In International Conference on Machine Learning, pp. 4907–4916. PMLR, 2018.\n\nYusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using PAC-Bayesian analysis. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9636–9647. PMLR, 13–18 Jul 2020.\n\nJoost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep deterministic neural network. In Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9690–9700. PMLR, 13–18 Jul 2020.\n\nTwan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint\n\narXiv:1706.05350, 2017.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz In Advances in neural information\n\nKaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 5998–6008, 2017.\n\nYeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient\n\nensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020.\n\nM.A. Woodbury. Inverting Modified Matrices. Memorandum Report / Statistical Research Group,\n\nPrinceton. Statistical Research Group, 1950.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\n\ndeep learning requires rethinking generalization, 2017.\n\nGuodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay\n\nregularization. In International Conference on Learning Representations, 2019.\n\nWenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz. Non-vacuous generalization bounds at the imagenet scale: a pac-bayesian compression approach. arXiv preprint arXiv:1804.05862, 2018.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA NOTATIONS\n\nTable 5: Notations used in the main paper\n\nxn ∈ RD, yn ∈ RK ≜ training inputs/outputs\n\nθ, ψ ∈ RP ≜ parameter of NNs\n\nf (x, θ) ≜ output of NN given x, θ θ∗ (x, δ) ≜ f (x, θ∗) + Jθ(x, θ∗)δ (linearization of NN w.r.t. parameter) f lin θ∗ (x, c) ≜ f (x, θ∗) + Jθ(x, θ∗)diag(θ∗)c (linearization of NN w.r.t. connectivity) glin (x, δ), gpert θ∗ (x, c) ≜ Perturbed NN w.r.t. parameter/connectivity\n\nf pert\n\nθ∗\n\n0d ≜ zero vector with dimension d Id ≜ identity matrix with dimension d × d\n\ntr(A) ≜ a trace (i.e., sum of diagonal elements) of matrix A\n\ndiag(v) ≜ a diagonal matrix whose diagonal elements correspond to v\n\nT : RP → RP ≜ a function-preserving scale transformation\n\nD ≜ (true) data distribution S ≜ i.i.d. sampled training dataset from S\n\nSP, SQ ≜ randomly partitioned prior/posterior datasets from S\n\nN, NP, NQ ≜ cardinality of S, SP, SQ\n\nX , Y/XP, YP/XQ, YQ ≜ concatenated inputs and outputs of all instances of S/SP/SQ.\n\nJθ(x, θ∗), Jc(x, θ∗) ∈ RK×P ≜ Jacobian of outputs w.r.t parameter/connectivity given x, θ∗\n\nf (X , θ) ∈ RN K×P ≜ concatenated outputs of NN along X\n\nJθ, Jc ∈ RN K×P ≜ concatenated Jacobian w.r.t. parameter/connectivity along X\n\nPθ∗ (c), Qθ∗ (c) ≜ our PAC-Bayes prior/posterior\n\nerrD(Q), errS (Q) ≜ generalization/empirical error of PAC-Bayes posterior\n\nα ≜ standard deviation of our PAC-Bayes prior σ ≜ scale (standard deviation) of observational noise μQ ∈ RP , ΣQ ∈ RP ×P ≜ mean and covariance of our PAC-Bayes posterior\n\nΘψ X , Cψ\n\nxx′, Cψ\n\nxx′ ∈ R ≜ empirical NTK/CTK of NN given ψ and input pair x, x′ ∈ RD\n\nX ∈ RN ×N ≜ empirical NTK/CTK of NN for training inputs (X ) given ψ\n\nΘψ\n\n{λi}P {βi}P\n\ni=1\n\ni=1\n\n≜ eigenvalues of empirical CTK ≜ eigenvalues of (IP + α2\n\nσ2 J⊤\n\nc Jc)−1\n\nh(x) ≜ x − log(x) − 1 (non-negative convex function w.r.t. βi. See Fig. 2)\n\n(h ◦ s)(x) ≜ non-negative concave function w.r.t. λi. See Fig. 2)\n\nB PROOFS\n\nB.1 PROOF OF PROPOSITION 2.1\n\nProof. Since the prior Pθ∗ (c) is independent to the parameter scale, Pθ∗ (c) d= PT (θ∗)(c) is trivial. For Jacobian w.r.t. parameters, we have\n\nJT (θ)(x, T (ψ)) =\n\n∂f (x, T (ψ)) ∂T (θ)\n\n=\n\n∂f (x, ψ) ∂T (θ)\n\n=\n\n∂f (x, ψ) ∂θ\n\n∂θ ∂T (θ)\n\n= Jθ(x, ψ)T −1\n\nThen, the Jacobian of NN w.r.t. connectivity at T (ψ) holds\n\nJθ(x, T (ψ))diag(T (ψ)) = Jθ(x, ψ)T −1T diag(ψ)\n\n= Jθ(x, ψ)diag(ψ)\n\n(12)\n\n(13)\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nwhere the first equality holds from the above one and the fact that T is a diagonal linear transformation. Therefore, the covariance of posterior is invariant to T .\n\n(cid:18) IP\n\ndiag(T (θ∗))J⊤\n\nθ (X , T (θ∗))Jθ(X , T (θ∗))diag(T (θ∗))\n\n(cid:19)−1\n\nα2 + (cid:18) IP\n\n=\n\nα2 +\n\nσ2\n\ndiag(θ∗)J⊤\n\nθ (X , θ∗)Jθ(X , θ∗)diag(θ∗)\n\n(cid:19)−1\n\nσ2\n\n=\n\n(cid:18) IP\n\nα2 +\n\nθ Jθdiag(θ∗) diag(θ∗)J⊤ σ2\n\n(cid:19)−1\n\nMoreover, the mean of posterior is also invariant to T .\n\nΣQdiag(T (θ∗))J⊤\n\nθ (X , T (θ∗)) (Y − f (X , T (θ∗)))\n\nΣQdiag(T (θ∗))J⊤\n\nθ (X , T (θ∗)) (Y − f (X , θ∗))\n\nσ2\n\nσ2\n\nΣQdiag(θ∗)J⊤\n\nθ (X , θ∗) (Y − f (X , θ∗))\n\nσ2\n\nΣQdiag(θ∗)J⊤\n\nθ (Y − f (X , θ∗)) σ2\n\n=\n\n=\n\n=\n\nTherefore, equation 6 and equation 7 are invariant to function-preserving scale transformations. The remaining part of the proposition is related to the definition of function-preserving scale transformation T . For generalization error, the following holds\n\nerrD(QT (θ∗)) = E(x,y)∼D,ψ∼QT (θ∗ ) [err(f (x, ψ), y)]\n\nθ∗ (x, c), y)]\n\n= E(x,y)∼D,c∼QT (θ∗ )[err(gpert = E(x,y)∼D,c∼Qθ∗ [err(gpert = E(x,y)∼D,ψ∼Qθ∗ [err(f (x, ψ), y)] = errD(Qθ∗ )\n\nθ∗ (x, c), y)]\n\nWLOG, this proof can be extended to the empirical error errSQ .\n\nB.2 PROOF OF THEOREM 2.2\n\nProof. (Construction of KL divergence) To construct PAC-Bayes-CTK, we need to arrange KL divergence between posterior and prior as follows:\n\nKL[Q∥P] =\n\n=\n\n=\n\n=\n\ntr(Σ−1\n\n1 2\n1 2\n1 2\nμ⊤ Q μQ 2α2 (cid:124) (cid:123)(cid:122) (cid:125) perturbation\n\n(cid:0)tr (cid:0)Σ−1\n\nP (ΣQ + (μQ − μP)(μQ − μP)⊤)(cid:1) + log |ΣP| − log |ΣQ| − P (cid:1)\n\nP (μQ − μP)(μQ − μP)⊤)) +\n\n(cid:0)tr(Σ−1\n\nP ΣQ) + log |ΣP| − log |ΣQ| − P (cid:1)\n\n(μQ − μP)⊤Σ−1\n\nP (μQ − μP) +\n\nP ΣQ) − log |Σ−1\n\nP ΣQ| − P (cid:1)\n\n1 2\n(cid:0)tr(Σ−1\n\n1 2\n\n+\n\n1 2\n(cid:124)\n\n(cid:0)tr(Σ−1\n\nP ΣQ) − log |Σ−1 (cid:123)(cid:122) sharpness\n\nP ΣQ| − p(cid:1) (cid:125)\n\nwhere the first equality uses the KL divergence between two Gaussian distributions, the third equality uses trace property (tr(AB) = tr(BA) and tr(a) = a for scalar a), and the last equality uses the definition of PAC-Bayes prior (Pθ∗ (c) = N (c|0P , α2IP )). For sharpness term, we first compute the Σ−1\n\nP ΣQ term as\n\nΣ−1\n\nP ΣQ =\n\n(cid:18)\n\nIP +\n\nα2 σ2 J⊤\n\nc Jc\n\n(cid:19)−1\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nSince α2, σ2 > 0 and J⊤ of {βi}P eigenvalues, we have\n\nP ΣQ have non-zero eigenvalues i=1. Since a trace is the sum of eigenvalues and the log-determinant is the sum of the log of\n\nc Jc is positive semi-definite, the matrix Σ−1\n\nKL[Q∥P] =\n\nμ⊤ Q μQ 2α2 +\n\n1 2\n\nP (cid:88)\n\ni=1\n\n(βi − log(βi) − 1) =\n\nμ⊤ Q μQ 2α2 +\n\n1 2\n\nP (cid:88)\n\ni=1\n\nh(βi)\n\nwhere h(x) = x − log(x) − 1. By plugging this KL divergence into the equation 2, we get equation 8.\n\n(Eigenvalues of Σ−1 P ΣQ) To show the scale-invariance of PAC-Bayes-CTK, it is sufficient to show that the KL divergence between posterior and prior is scale-invariant: log(2(cid:112)NQ/δ)/2NQ is independent to KL PAC-Bayes prior/posterior. We already show the invariance property of empirical/generalization error term in Proposition 2.1.\n\nTo show the invariance property of KL divergence, let us write a singular value decomposition of Jacobian w.r.t. connectivity Jc ∈ RN K×P as Jc = U ΣV ⊤, where U ∈ RN K×N K and V ∈ RP ×P are orthogonal matrices and Σ ∈ RN K×P is a rectangular diagonal matrix with descending order for singular values. Then, the following holds for Σ−1 P ΣQ\n\nΣ−1\n\nP ΣQ =\n\n(cid:18)\n\nIP +\n\n(cid:18)\n\n=\n\nIP +\n\n(cid:19)−1\n\nc Jc\n\nα2 σ2 J⊤ α2 σ2 V Σ⊤ΣV ⊤\n\n(cid:19)−1\n\n(cid:18)\n\n= V\n\nIP +\n\n(cid:19)−1\n\nα2 σ2 Λ\n\nV ⊤\n\nwhere Λ = Σ⊤Σ ∈ RP ×P is a diagonal matrix with λi := Λii = 0 for i ≥ N K. Therefore, eigenvalues of Σ−1 . Now, we consider Connectivity Tangent Kernel (CTK) as defined in equation 2.3:\n\n1+α2λi/σ2 = σ2\n\nP ΣQ are\n\nσ2+α2λi\n\n1\n\nCθ∗\n\nX := JcJ⊤\n\nc = Jθdiag(θ∗)2J⊤\n\nθ ∈ RN K×N K.\n\nSimilar to J⊤\n\nc Jc, CTK can be expressed as follows\n\nCθ∗\n\nX = JcJ⊤\n\nc = U ΣV ⊤V Σ⊤U ⊤ = U ΣΣ⊤U ⊤ = U Λ′U ⊤\n\nwhere Λ′ = ΣΣ⊤ ∈ RN K×N K. As the smallest (P − N K) eigenvalues of Λ = Σ⊤Σ are just zeros, Λ′ is just a reduced diagonal matrix of Λ with these eigenvalues removed. As a result, {λi}N K i=1 are eigenvalues of CTK.\n\n(Scale invariance of CTK) The scale-invariance property of CTK is a simple application of equation 13:\n\nCT (ψ)\n\nxy = JT (θ)(x, T (ψ))diag(T (ψ)2)JT (θ)(y, T (ψ))⊤\n\n= Jθ(x, ψ)T −1T diag(ψ)diag(ψ)T T −1Jθ(x, ψ)⊤ = Jθ(x, ψ)diag(ψ)diag(ψ)Jθ(x, ψ)⊤ xy , ∀x, y ∈ RD, ∀ψ ∈ RP . = Cψ\n\nTherefore, CTK is invariant to any function-preserving scale transformation T and so do its eigenvalues. This guarantees the invariance of Σ−1 P ΣQ and its eigenvalues. In summary, we showed the scale-invariance property of the sharpness term of KL divergence. Now all that remains is to show the invariance of the perturbation term. However, this is already proved in the proof of Proposition 2.1. Therefore, we show PAC-Bayes-CTK is invariant to any function-preserving scale transformation.\n\nB.3 PROOF OF COROLLARY 2.3\n\nProof. In proof of Theorem 2.2, we showed that eigenvalues of Σ−1\n\nP ΣQ can be represented as\n\nσ2 σ2 + α2λi\n\n17\n\nPublished as a conference paper at ICLR 2023\n\n(a) y = h(x)\n\n(b) y = (h ◦ s)(x) where σ = α = 1\n\nFigure 2: Functions used in proofs\n\ni=1 are eigenvalues of Λ = Σ⊤Σ. By SVD of Jacobian w.r.t. connectivity, λi = Λii = where {λi}P Σ2 ii ≥ 0. Therefore, eigenvalues of CTK are squares of singular values of Jc and so λi ≥ 0, ∀i. As a result, βi ≤ 1 for all i = 1, . . . , P for eigenvalues {βi}P P ΣQ and equality holds for λi = 0 (i.e., i ≥ N K). Now all that remains is to show that the sharpness term of PAC-Bayes-CTK is a monotonically increasing function on each eigenvalue of CTK. To show this, we first keep in mind that\n\ni=1 of Σ−1\n\ns(x) :=\n\nσ2 σ2 + α2x\n\nis a monotonically decreasing function for x ≥ 0 and h(x) := x − log(x) − 1 is a monotonically decreasing function for x ∈ (0, 1]. Since the sharpness term of KL divergence is\n\nP (cid:88)\n\ni=1\n\nh(βi) 4NQ\n\n=\n\nP (cid:88)\n\ni=1\n\n(h ◦ s)(λi) 4NQ\n\nThis is a monotonically increasing function for x ≥ 0 since s(x) ≤ 1 for x ≥ 0. For your information, we plot y = h(x) and y = (h ◦ s)(x) in Figure 2.\n\nB.4 PROOF OF PROPOSITION 2.4\n\nWe refer to Scale invariance of CTK part of the proof of Theorem 2.2. This is a direct application of the scale-invariance property of Jacobian w.r.t. connectivity.\n\nB.5 PROOF OF COROLLARY 2.6\n\nProof. Since CS is a trace of CTK, it is a sum of the eigenvalues of CTK. As shown in the proof of Corollary 2.3, eigenvalues of CTK are square of singular values of Jacobian w.r.t. connectivity c. Therefore, the eigenvalues of CTK are non-negative and vanish to zero if CS vanishes to zero.\n\nP (cid:88)\n\ni=1\n\nλi = 0 ⇒ λi = 0 ⇒ βi = s(λi) = 1 ⇒ h(βi) = 0,\n\n∀i = 1, . . . , P\n\nThis means the sharpness term of KL divergence vanishes to zero. Furthermore, singular values of Jacobian w.r.t. c also vanish to zero in this case. Therefore, μQ vanishes to zero, also. Similarly, if CS diverges to infinity, this means (at least) one of the eigenvalues of CTK diverges to infinity. In this case, the following holds\n\nλi → ∞ ⇒ βi = s(λi) → 0 ⇒ h(βi) → ∞,\n\n∀i = 1, . . . , P\n\nTherefore, the KL divergence term of PAC-Bayes-CTK also diverges to infinity.\n\n18\n\n0123456789x0.02.55.07.510.012.515.017.5h(x)Plotting of y=h(x)020406080100x0.00.51.01.52.02.53.03.5(hs)(x)Plotting of y=(hs)(x)Published as a conference paper at ICLR 2023\n\nB.6 PROOF OF PROPOSITION 3.1\n\nProof. By assumption, we fixed all non-scale invariant parameters. This means we exclude these parameters in the sampling procedure of CL and LL. In terms of predictive distribution, this can be translated as\n\nθ∗ (x, ψ)|pLA(ψ|S) ∼ N (f (x, θ∗), α2 ˆΘθ∗ f lin θ∗ (x, ψ)|pCL(ψ|S) ∼ N (f (x, θ∗), α2 ˆCθ∗ f lin xx′ := (cid:80)\n\nxx − α2 ˆΘθ∗ xx − α2 ˆCθ∗\n\nxX\n\nX x)\n\nX\n\nˆΘθ∗−1 ˆCθ∗−1\n\nˆΘθ∗ ˆCθ∗\n\nX x)\n\nxX\n\nX ∂f (x′,ψ) ∂θi\n\ni∈P\n\n∂f (x,ψ) ∂θi\n\nxx′ := (cid:80) where ˆΘψ (ψi)2 for scale-invariant parameter set P. Thereby, we mask the gradient of the non-scale-invariant parameters as zero. Therefore, this can be arranged as follows xx′ = Jθ(x, ψ)diag(1P )Jθ(x, ψ)⊤,\n\nxx′ = Jθ(x, ψ)diag(ψ)diag(1P )diag(ψ)Jθ(x, ψ)⊤\n\n∂f (x,ψ) ∂θi\n\nand ˆCψ\n\n∂f (x′,ψ) ∂θi\n\nˆCψ\n\nˆΘψ\n\ni∈P\n\nwhere 1P ∈ RP is a masking vector (i.e., one for included components and zero for excluded components). Then, the WD for scale-invariant parameters can be represented as\n\nWγ(ψ)i =\n\n(cid:26)γψi, ψi,\n\nif ψi ∈ P. if ψi ̸∈ P.\n\nTherefore, we get\n\nˆΘWγ (ψ)\n\nxx′\n\nfor empirical NTK and\n\n= Jθ(x, Wγ(ψ))diag(1P )Jθ(x, Wγ(ψ)))⊤ γ diag(1P )W −1 = Jθ(x, ψ)W −1 = Jθ(x, ψ)diag(1P /γ2)Jθ(x, ψ)⊤ = 1/γ2Jθ(x, ψ)diag(1P )Jθ(x, ψ)⊤ = 1/γ2 ˆΘψ\n\nγ Jθ(x, ψ)⊤\n\nxx′\n\nˆCWγ (ψ)\n\nxx′\n\n= Jθ(x, Wγ(ψ))diag(Wγ(ψ))diag(1P )diag(Wγ(ψ))Jθ(x, Wγ(ψ)))⊤ γ Wγdiag(ψ)diag(1P )diag(ψ)WγW −1 = Jθ(x, ψ)W −1 = Jθ(x, ψ)diag(ψ)diag(1P )diag(ψ)Jθ(x, ψ)⊤ = ˆCψ\n\nγ Jθ(x, ψ)⊤\n\nxx′\n\nfor empirical CTK. Therefore, we get\n\nWγ (θ∗)(x, ψ)|pLA(ψ|S) ∼ N (f (x, θ∗), α2/γ2 ˆΘθ∗ f lin Wγ (θ∗)(x, ψ)|pCL(ψ|S) ∼ N (f (x, θ∗), α2 ˆCθ∗ f lin\n\nxx − α2/γ2 ˆΘθ∗ ˆCθ∗−1\n\nxx − α2 ˆCθ∗\n\nxX\n\nX\n\nxX\n\nX\n\nˆΘθ∗−1 ˆCθ∗\n\nX x)\n\nˆΘθ∗\n\nX x)\n\nThis gives us\n\nVarψ∼pLA(ψ|S)(f lin Varψ∼pCL(ψ|S)(f lin\n\nWγ (θ∗)(x, ψ)) = Varψ∼pLA(ψ|S)(f lin Wγ (θ∗)(x, ψ)) = Varψ∼pCL(ψ|S)(f lin\n\nθ∗ (x, ψ))\n\nθ∗ (x, ψ))/γ2\n\nB.7 DERIVATION OF PAC-BAYES-NTK\n\nTheorem B.1 (PAC-Bayes-NTK). Let us assume pre-trained parameter θ∗ with data SP. Let us assume PAC-Bayes prior and posterior as\n\nP′\n\nQ′\n\nθ∗(δ) := N (δ|0P , α2IP ) θ∗ (δ) := N (δ|μQ′, ΣQ′) ΣQ′J⊤\n\nμQ′ :=\n\nθ (Y − f (X , θ∗))\n\nΣQ′ :=\n\nσ2 J⊤ θ Jθ σ2\n\n(cid:18) IP\n\nα2 +\n\n(cid:19)−1\n\n19\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\nPublished as a conference paper at ICLR 2023\n\nBy applying P′\n\nθ∗ , Q′\n\nθ∗ to data-dependent PAC-Bayes bound (equation 2), we get\n\nerrD(Q′\n\nθ∗ ) ≤ errSQ′ (Q′\n\nθ∗ ) +\n\n(cid:122)\n\n(cid:118) (cid:117) (cid:117) (cid:117) (cid:117) (cid:117) (cid:117) (cid:117) (cid:116)\n\nKL divergence (cid:125)(cid:124)\n\nμ⊤ Q′μQ′ 4α2NQ′ (cid:124) (cid:123)(cid:122) (cid:125) (average) perturbation\n\n+\n\nP (cid:88)\n\ni=1 (cid:124)\n\n(cid:123)(cid:122) sharpness\n\n(cid:125)\n\n(cid:123) h (β′ i) 4NQ′\n\nlog(2(cid:112)NQ′/δ) 2NQ′\n\n+\n\n(18)\n\nwhere {β′ is not scale-invariant in general.\n\ni=1 are eigenvalues of (IP + α2\n\ni}P\n\nσ2 J⊤\n\nθ Jθ)−1 and h(x) := x − log(x) − 1. This upper bound\n\nProof. The main difference between PAC-Bayes-CTK and PAC-Bayes-NTK is the definition of Jacobian: PAC-Bayes-CTK uses Jacobian w.r.t connectivity and PAC-Bayes-NTK uses Jacobian w.r.t. parameter. Therefore, Construction of KL divergence of proof of Theorem 2.2 is preserved except\n\nΣ−1\n\nP′ ΣQ′ = (IP +\n\nα2 σ2 J⊤\n\nθ Jθ)−1\n\nand β′\n\ni are eigenvalues of Σ−1\n\nP′ ΣQ′. Note that these eigenvalues satisfy\n\nβ′\n\ni =\n\nσ2 σ2 + α2λ′ i\n\nwhere λ′\n\ni are eigenvalues of J⊤\n\nθ Jθ.\n\nRemark B.2 (Function-preserving scale transformation to NTK). In contrast to the CTK, the scale invariance property is not applicable to the NTK due to the Jacobian w.r.t. parameter:\n\nJθ(x, T (ψ)) =\n\n∂ ∂T (ψ)\n\nf (x, T (ψ)) =\n\n∂ ∂T (ψ)\n\nf (x, ψ) = Jθ(x, ψ)T −1\n\nIf we assume all parameters are scale-invariant (or equivalently masking the Jacobian for all nonscale-invariant parameters as in the proof of Proposition 3.1), the scale of NTK is proportional to the inverse scale of parameters.\n\nC DETAILS OF SQUARED LOSS FOR CLASSIFICATION TASKS\n\nFor the classification tasks in Sec. 4.2, we use the squared loss instead of the cross-entropy loss since our theoretical results are built on the squared loss. Here, we describe how we use the squared loss to mimic the cross-entropy loss. There are several works (Lee et al., 2020; Hui & Belkin, 2021) that utilized the squared loss for the classification task instead of the cross-entropy loss. Specifically, Lee et al. (2020) used\n\nL(S, θ) =\n\n1 2N K\n\n(cid:88)\n\n∥f (xi, θ) − yi∥2\n\n(xi,yi)∈S\n\nwhere C is the number of classes, and Hui & Belkin (2021) used\n\nl((x, c), θ) =\n\n1 2K\n\n k(fc(x, θ) − M )2 +\n\nK (cid:88)\n\n\n\n\n\nfi(x, θ)2\n\ni=1,i̸=c\n\nfor single data loss, where l((x, c), θ) is sample loss given input x, target c and parameter θ, fi(x, θ) ∈ R is the i-th component of f (x, θ) ∈ RK, k and M are dataset-specific hyper-parameters.\n\nThese works used the mean for reducing the vector-valued loss into a scalar loss. However, this can be problematic when the number of classes is large. When the number of classes increases, the denominator of the mean (the number of classes) increases while the target value remains 1 (one-hot label). As a result, the scale of a gradient for the target class becomes smaller. To avoid such an\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nunfavorable effect, we use the sum for reducing vector-valued loss into a scalar loss instead of taking the mean, i.e.,\n\nl((x, c), θ) =\n\n1 2\n\n (fc(x, θ) − 1)2 +\n\nK (cid:88)\n\n\n\n\n\nfi(x, θ)2\n\ni=1,i̸=c\n\nThis analysis is consistent with the hyper-parameter selection in Hui & Belkin (2021). They used larger k and M as the number of classes increases (e.g., k = 1, M = 1 for CIFAR-10 (Krizhevsky, 2009), but k = 15, M = 30 for ImageNet (Deng et al., 2009)) which results in manually compensating the scale of the gradient to the target class label.\n\nD DERIVATION OF PAC-BAYES POSTERIOR\n\nDerivation of Qθ∗ (c) For Bayesian linear regression, we compute the posterior of β ∈ RP\n\nyi = xiβ + εi,\n\nfor i = 1 . . . , M\n\nwhere εi ∼ N (0, σ2) is i.i.d. sampled and the prior of β is given as β ∼ N (0P , α2IP ). By concatenating this, we get\n\nwhere y ∈ RM , X ∈ RM ×p are concatenation of yi, xi, respectively and ε ∼ N (0M , σ2IM ). It is well known (Bishop, 2006; Murphy, 2012) that the posterior of β for this problem is\n\ny = Xβ + ε\n\nβ ∼ N (β|μ, Σ)\n\nμ :=\n\nΣ :=\n\nΣX⊤y σ2 (cid:18) IP\n\nα2 +\n\n(cid:19)−1\n\n.\n\nX⊤X σ2\n\nSimilarly, we define the Bayesian linear regression problem as\n\nyi = f (xi, θ∗) + Jθ(xi, θ∗)diag(θ∗)c + εi,\n\nfor i = 1 . . . , N K\n\nwhere M = N K and the regression coefficient is β = c in this case. Thus, we treat yi − f (xi, θ∗) as a target and Jθ(xi, θ∗)diag(θ∗) as an input of linear regression problem. By concatenating this, we get\n\nY = f (X , θ∗) + Jcc + ε ⇒ (Y − f (X , θ∗)) = Jcc + ε.\n\nBy plugging this into the posterior of the Bayesian linear regression problem, we get\n\nQθ∗ (c) := N (c|μQ, ΣQ)\n\nΣQJ⊤\n\nc (Y − f (X , θ∗))\n\nμQ :=\n\nΣQ :=\n\nσ2 J⊤ c Jc σ2\n\n(cid:18) IP\n\nα2 +\n\n=\n\nΣQdiag(θ∗)J⊤\n\nθ (Y − f (X , θ∗)) σ2 diag(θ∗)J⊤ θ Jθdiag(θ∗) σ2\n\n(cid:19)−1\n\n(cid:19)−1\n\n=\n\n(cid:18) IP\n\nα2 +\n\nDerivation of Qθ∗ (ψ) We define perturbed parameter ψ as follows\n\nψ := θ∗ + θ∗ ⊙ c.\n\nSince ψ is affine to c, we get the distribution of ψ as\n\nQθ∗ (ψ) := N (ψ|μψ\n\nQ, Σψ Q) Q := θ∗ + θ∗ ⊙ μQ\n\nμψ\n\nΣψ\n\nQ := diag(θ∗)ΣQdiag(θ∗) =\n\n(cid:18) diag(θ∗)−2 α2\n\n+\n\nJ⊤ θ Jθ σ2\n\n(cid:19)−1\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nE REPRESENTATIVE CASES OF FUNCTION-PRESERVING SCALING\n\nTRANSFORMATIONS\n\nActivation-wise rescaling transformation (Tsuzuku et al., 2020; Neyshabur et al., 2015a) For NNs with positive homogeneous (e.g., ReLU) activations, following holds for ∀x ∈ Rd, γ > 0: f (x, θ) = f (x, Rγ,l,k(θ)), where rescale transformation Rγ,l,k(·)2 is defined as\n\n(Rγ,l,k(θ))i =\n\n \n\n\n\nγ · θi θi/γ θi\n\n, if θi ∈ {param. subset connecting as input edges to k-th activation at l-th layer} , if θi ∈ {param. subset connecting as output edges to k-th activation at l-th layer} , for θi in the other cases\n\n(19)\n\nNote that Rγ,l,k(·) is a finer-grained rescaling transformation than layer-wise rescaling transformation (i.e., common γ for all activations in layer l) discussed in Dinh et al. (2017). Dinh et al. (2017) showed that even layer-wise rescaling transformations can sharpen pre-trained solutions in terms of the trace of Hessian (i.e., contradicting the FM hypothesis). This contradiction also occurs in previous PAC-Bayes bounds (Tsuzuku et al., 2020; Kwon et al., 2021) due to the scale-dependent term.\n\nFor example, let us assume a linear NN as\n\nf (x, (θ1, θ2, θ3, θ4)) = (θ3, θ4)⊤\n\n(cid:19)\n\n(cid:18)θ1 θ2\n\nx = θ1θ3x + θ2θ4x\n\nThen, the Jacobian of this NN would be\n\nJθ(x, θ) =\n\n(cid:19)\n\n(cid:18)θ3x θ4x θ1x θ2x\n\nFrom above discussion, (0.5, 1, 2, 1) = R2,1,1 ((1, 1, 1, 1)) would change the Jacobian while maintaining the predictions. To show this,\n\nf (x, (0.5, 1, 2, 1)) = f (x, (1, 1, 1, 1)) = x + x = 2x\n\nfor all x ∈ R. However,\n\nJθ(x, (0.5, 1, 2, 1)) =\n\n(cid:19)\n\n(cid:18) 2x\n\nx 0.5x x\n\n(cid:19)\n\n(cid:18)x x x x\n\n̸=\n\n= Jθ(x, (1, 1, 1, 1))\n\nin general. Therefore, we verified that the activation-wise rescaling transformation R2,1,1 is a valid function-preserving scale transformation. WD with BN layers (Ioffe & Szegedy, 2015) For parameters W ∈ Rnl×nl−1 preceding BN layer,\n\nBN((diag(γ)W )u) = BN(W u)\n\n(20)\n\nfor an input u ∈ Rnl−1 and a positive vector γ ∈ Rnl on these parameters preserve function represented by NNs for ∀x ∈ Rd, γ ∈ Rnl f (x, Sγ,l,k(θ)), where scaling transformation Sγ,l,k(·) is defined for i = 1, . . . , P\n\n+ . This implies that scaling transformations + : f (x, θ) =\n\n(Sγ,l,k(θ))i =\n\n(cid:26)γk · θi θi\n\n, if θi ∈ {param. subset connecting as input edges to k-th activation at l-th layer} , for θi in the other cases\n\n(21)\n\nNote that the WD (Loshchilov & Hutter, 2019; Zhang et al., 2019) can be implemented as a realization of Sγ,l,k(·) (e.g., γ = 0.9995 for all activations preceding BN layers). Therefore, thanks to Proposition 2.1 and Theorem 2.4, our CTK-based bound is invariant to WD applied to parameters before BN layers. We also refer to (Van Laarhoven, 2017; Lobacheva et al., 2021) for the optimization perspective of WD with BN. For example, let us assume a BN layer with single activation as BN(θx) where x, θ ∈ R and batch of input is given as 0, 0, 2, 2. Then, the Jacobian of NN would be\n\nJθ(x, θ) =\n\nx |θ|\n\n2For a simple two layer linear NN f (x) := W2σ(W1x) with weight matrix W1, W2, the first case of equation 19 corresponds to k-th row of W1 and the second case of equation 19 corresponds to k-th column of W2.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nas f (x, θ) = θx |θ| and the denominator is detached from backpropagation computation for autodifferentiation packages (e.g., TensorFlow, Pytorch, and JAX). From the above discussion, 0.9995 = S0.9995,1,1(1) would change the Jacobian while maintaining the predictions. To show this,\n\nfor all inputs. On the other hand,\n\nf (x, 1) = f (x, 0.9995) = x\n\nx 0.9995 Therefore, we showed that the WD with BN layer S0.9995,1,1 is a valid function-preserving scale transformation.\n\nJθ(x, 0.9995) =\n\n̸= x = Jθ(x, 1).\n\nF IMPLEMENTATIONS\n\nF.1 RTO IMPLEMENTATION OF CONNECTIVITY LAPLACE\n\nAlgorithm 1 Connectivity Laplace RTO Require: training data S, pre-trained parameter θ∗, number of samples M , prior scale α, observa-\n\ntional noise scale σ for m = 1, . . . , M do\n\nSample parameter noise cm for t = 1, . . . , T do\n\n0 ∼ N (c0|0P , α2IP ) and label noise εm ∼ N (ε|0N K, σ2IN K)\n\nθ∗ (XB, cm) = f (XB, θ∗) + Jθ(XB, θ∗)diag(θ∗)cm for mini-batch input XB.\n\nRandomly draw a mini-batch B from S. Define glin Define L(cm) = 1 2 + 1 Compute backpropagation of L(cm) w.r.t. connectivity cm. Update cm with SGD optimizer.\n\n2|B|σ2 ∥YB + εm\n\nθ∗ (XB, cm)∥2\n\nB − glin\n\n2|B|α2 ∥cm − cm\n\n0 ∥2 2.\n\nend for\n\nend for return Samples from Connectivity Laplace {cm}M\n\nm=1.\n\nTo estimate the empirical/generalization bound in Sec. 2.4 and calibrate uncertainty in Sec. 4.2, we need to sample c from the posterior Qθ∗ (c). For this, we sample perturbations δ in connectivity space (cid:18) IP\n\n(cid:19)−1(cid:33)\n\n(cid:32)\n\nδ ∼ N\n\nδ|0P ,\n\nα2 +\n\nJ⊤ c Jc σ2\n\nso that c = μQ + δ for equation 6. To sample this, we provide a novel approach to sample from LA/CL without curvature approximation. To this end, we consider the following optimization problem\n\narg min\n\nc\n\nL(c) := arg min\n\nc\n\n1\n\n2N σ2 ∥Y − f (X , θ∗) − Jcc + ε∥2 +\n\n1\n\n2N α2 ∥c − c0∥2\n\n2\n\nwhere ε ∼ N (ε|0N K, σ2IN K) and c0 ∼ N (c0|0P , α2IP ). By first-order optimality condition, we have\n\nN ∇cL(c) = −\n\nJ⊤\n\nc (Y − f (X , θ∗) − Jcc∗ + ε) σ2\n\n+\n\nc∗ − c0\n\nα2 = 0P .\n\nBy arranging this w.r.t. optimizer c∗, we get\n\nc∗ =\n\n=\n\n=\n\n(cid:18)\n\n(cid:18)\n\nJ⊤\n\nc Jc +\n\nJ⊤\n\nc Jc +\n\nσ2 α2 IP σ2 α2 IP\n\n(cid:19)−1 (cid:18)\n\nJ⊤\n\nc (Y − f (X , θ∗)) +\n\n(cid:19)−1\n\nJ⊤\n\nc (Y − f (X , θ∗)) +\n\n(cid:18)\n\n(cid:19)\n\nσ2 α2 c0 + Jcε σ2 α2 IP\n\nc Jc +\n\nJ⊤\n\n(cid:19)−1 (cid:18) σ2\n\nα2 c0 + Jcε\n\n(cid:19)\n\n(cid:18) IP\n\nα2 +\n\nJ⊤ c Jc σ2\n\n(cid:19)−1\n\nJ⊤\n\nc (Y − f (X , θ∗)) σ2\n\n+\n\n(cid:18) IP\n\nα2 +\n\nJ⊤ c Jc σ2\n\n(cid:19)−1 (cid:18) c0\n\nα2 +\n\n(cid:124)\n\n(cid:123)(cid:122) Deterministic\n\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) Stochastic\n\nJ⊤ c ε σ2\n\n(cid:19)\n\n(cid:125)\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nSince both ε and c0 are sampled from independent Gaussian distributions, we have J⊤ c ε σ2\n\nJ⊤ c Jc σ2\n\nIP α2 +\n\nα2 +\n\n(cid:18) c0\n\nz|0P ,\n\n∼ N\n\nz :=\n\n(cid:19)\n\n(cid:19)\n\n(cid:18)\n\nTherefore, optimal solution of randomized optimization problem arg minc L(c) is\n\n(cid:32)\n\nc ∼ N\n\nc\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:18) IP\n\nα2 +\n\nJ⊤ c Jc σ2\n\n(cid:19)−1\n\nJ⊤\n\nc (Y − f (X , θ∗)) σ2\n\n,\n\n(cid:18) IP\n\nα2 +\n\nJ⊤ c Jc σ2\n\n(cid:19)−1(cid:33)\n\n= N (c|μQ, ΣQ)\n\nSimilarly, sampling from CL can be implemented as the following optimization problem.\n\narg min\n\nc\n\nL(c) := arg min\n\nc\n\n1\n\n2N σ2 ∥Jcc − ε∥2 +\n\n1\n\n2N α2 ∥c − c0∥2\n\n2\n\nwhere ε ∼ N (ε|0N K, σ2IN K) and c0 ∼ N (c0|0P , α2IP ). Since we sample the noise of data/perturbation and optimize the perturbation, this can be interpreted as a Randomize-ThenOptimize implementation of Laplace approximation and Connectivity Laplace (Bardsley et al., 2014; Matthews et al., 2017). In Algorithm 1, we provide a pseudo-code for the RTO implementation of CL. Note that both time and memory complexity of computing linearized NN for mini-batch B is comparable to a forward propagation as shown in Novak et al. (2022) with jax.jvp function in JAX (Bradbury et al., 2018). Therefore, the time/memory complexity of mini-batch JVP would be O(|B|LW 2)/O(|B|W + LW 2 + N K) for MLPs with width W and depth L.\n\nF.2 COMPUTING CONNECTIVITY SHARPNESS\n\nAlgorithm 2 Hutchison’s method for computing Connectivity sharpness Require: training data S, pre-trained parameter θ∗, number of samples M\n\nxB = 0 for m = 1, . . . , M do\n\nSample zm ∼ N (ε|0N K, σ2IN K) for t = 1, . . . , T do\n\nSequentially draw a mini-batch B from S. Compute vector-Jacobian product vm Compute xB = xB + ∥vm\n\nB = zm\n\nB ∥2\n\n2/T\n\nB Jθ(X , θ∗)diag(θ∗).\n\nend for\n\nend for return Estimated Connectivity Sharpness xB\n\nIt is well known that empirical NTK or Jacobian is intractable in the modern architecture of NNs (e.g., ResNet (He et al., 2016) or BERT (Devlin et al., 2018)). Therefore, one might wonder how Connectivity Sharpness (CS) can be computed for these architectures. One can compute CS with Hutchison’s method (Hutchinson, 1989; Meyer et al., 2021) as it is defined as a trace of empirical CTK. According to Hutchison’s method, trace of a matrix A ∈ Rm×m is\n\ntr(A) = tr(AIp) = tr(AEz[zz⊤]) = Ez[tr(Azz⊤)] = Ez[tr(z⊤Az)] = Ez[z⊤Az] where z ∈ Rm is a random variable with cov(z) = Im (e.g., standard normal distribution or Rademacher distribution). Since A = Cθ∗ c ∈ RN K in our case, we further use miniX = JcJ⊤ B ∈ RM K from Rademacher distribution for batch approximation to compute z⊤Az: (i) Sample zm B ∈ RP with vector-Jacobian mini-batch M with size M and (ii) compute vm product of JAX (Bradbury et al., 2018) (or it can simply computed using standard backprop) and (iii) compute xm B for all mini-batch in the training dataset is a Monte-Carlo approximation of CS with sample size 1. Empirically, we found that this approximation is sufficiently stable to capture the correlation between sharpness and generalization as shown in Sec. 4.1. In Algorithm 2, we provide a pseudo-code for the implementation. Note that both time and memory complexity of computing vm B is comparable to a backward propagation as shown in Novak et al. (2022) with jax.vjp function in JAX (Bradbury et al., 2018). Therefore, the time/memory complexity of mini-batch VJP would be O(|B|LW 2)/O(|B|LW + LW 2 + N KW ) for MLPs with width W and depth L.\n\n2. Then, the sum of xm\n\nB := Jc(XB, 0p)⊤zm\n\nB = ∥vm\n\nB ∥2\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nG PREDICTIVE UNCERTAINTY OF CONNECTIVITY/LINEARIZED LAPLACE\n\nIn this section, we derive predictive uncertainty of Linearized Laplace (LL) and Connectivity Laplace (CL). By matrix inversion lemma (Woodbury, 1950), the weight covariance of LL is\n\n(Ip/α2 + Jθ(X , θ∗)⊤Jθ(X , θ∗)/σ2)−1 = α2Ip − α2Jθ(X , θ∗)⊤(\n\nσ2\n\nα2 IN k + Θθ∗\n\nX X )−1Jθ(X , θ∗).\n\nTherefore, if σ2/α2 → 0, then the weight covariance of LL converges to\n\nα2Ip − α2Jθ(X , θ∗)⊤Θθ∗−1\n\nX X Jθ(X , θ∗).\n\nWith this weight covariance and linearized NN, the predictive uncertainty of LL is\n\nθ∗ (x, θ)|pLA(θ|S) ∼ N (f (x, θ∗), α2Θθ∗ f lin\n\nxx − α2Θθ∗\n\nxX Θθ∗−1\n\nX X Θθ∗\n\nX x).\n\nSimilarly, the predictive uncertainty of CL is\n\nθ∗ (x, θ)|Qθ∗ (θ) ∼ N (f (x, θ∗), α2Cθ∗ f lin\n\nxx − α2Cθ∗\n\nxX Cθ∗−1\n\nX X Cθ∗\n\nX x).\n\nH DETAILED EXPERIMENTAL SETTINGS\n\nH.1 BOUND ESTIMATION\n\nWe pre-train ResNet-18 (He et al., 2016) with a mini-batch size of 1K on SP with SGD of initial learning rate 0.4 and momentum 0.9. We use cosine annealing for learning rate scheduling (Loshchilov & Hutter, 2016) with a warmup for the initial 10% training step. We fix δ = 0.1, α = 0.1, and σ = 1.0 to compute equation 8. These values are chosen so that the PAC-Bayes-CTK and NTK bounds fall within the non-vacuous range. We use 3 random seeds to compute the standard errors. Additional results on few pre-training data with NP = 5, 000 and NQ = 45, 000 are presented in Appendix I.\n\nH.2 CONNECTIVITY SHARPNESS\n\nTable 6: Configuration of hyper-parameter\n\nnetwork depth network width learning rate WD mini-batch size\n\n1, 2, 3 32, 64, 128 0.1, 0.032, 0.001 0.0, 1e-4, 5e-4 256, 1024, 4096\n\nTo verify that the CS has a better correlation with generalization performance compared to existing sharpness measures, we evaluate the three metrics: (a) Kendall’s rank-correlation coefficient τ (Kendall, 1938) which considers the consistency of a sharpness measure with generalization gap (i.e., if one has higher sharpness, then so has higher generalization gap) (b) granulated Kendall’s coefficient (Jiang et al., 2020) which examines Kendall’s rank-correlation coefficient w.r.t. individual hyper-parameters to separately evaluate the effect of each hyper-parameter to generalization gap (c) conditional independence test (Jiang et al., 2020) which captures the causal relationship between measure and generalization.\n\nThree metrics are compared with the following baselines: trace of Hessian (tr(H); (Keskar et al., 2017)), trace of Fisher information matrix (tr(F); (Jastrzebski et al., 2021)), trace of empirical NTK at θ∗ (tr(Θθ∗ )), and four PAC-Bayes bound based measures, sharpness-orig (SO), PAC-Bayes-orig (PO), 1/α′ sharpness mag (SM), and 1/σ′ PAC-Bayes mag (PM), which are eq. (52), (49), (62), (61) in Jiang et al. (2020).\n\nFor the granulated Kendall’s coefficient, we use 5 hyper-parameters: network depth, network width, learning rate, WD and mini-batch size, along with 3 options for each hyper-parameters as in Table 6. We use the VGG-13 as a base model and we adjust the depth and width of each conv block. We add\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nTable 7: Example of network configuration with respect to the depth 1, width 128 in (Simonyan & Zisserman, 2015)-style.\n\nConvNet Configuration\n\ninput (224 × 224 RGB image)\n\nConv3-128 BN ReLU\n\nMaxPool\n\nConv3-256 BN ReLU\n\nMaxPool\n\nConv3-512 BN ReLU\n\nMaxPool\n\nConv3-1024 BN ReLU\n\nMaxPool\n\nConv3-1024 BN ReLU\n\nMaxPool\n\nFC-4096 ReLU\n\nFC-4096 ReLU\n\nFC-1000\n\nBN layers after the convolution layer for each block. Specifically, the number of convolution layers of each conv block is the depth and the number of channels of convolution layers of the first conv block is the width. For the subsequent conv blocks, we follow the original VGG width multipliers (×2, ×4, ×8). An example with depth 1 and width 128 is depicted in Table 7.\n\nWe use an SGD optimizer with momentum of 0.9. We train each model for 200 epochs and use cosine learning rate scheduler (Loshchilov & Hutter, 2016) with 30% of initial epochs as warm-up epochs. The standard data augmentations (padding, random crop, random horizontal flip, and normalization) for CIFAR-10 are used for training data. For the analysis, we only use models with above 99% training accuracy following Jiang et al. (2020). As a result, we use 200 out of 243 trained models for our correlation analysis. For every experiment, we use 8 NVIDIA RTX 3090 GPUs.\n\nH.3 BNN EXPERIMENTS\n\nUCI regression We train MLP with a single hidden layer. We fix σ = 1 and choose α from {0.01, 0.1, 1, 10, 100} using log-likelihood of validation dataset since the optimal α varies for each regression dataset. We use 8 random seeds to compute the average and standard error of the test negative log-likelihoods.\n\nImage classification task We pre-train models for 200 epochs CIFAR-10/100 dataset (Krizhevsky, 2009) with ResNet-18(He et al., 2016) as mentioned in Section 2.4. We choose ensemble size M as 8\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nexcept Deep Ensemble (Lakshminarayanan et al., 2017) and Batch Ensemble (Wen et al., 2020). We use 4 ensemble members for Deep Ensemble and Batch Ensemble due to computational cost. We used 4 random seeds to compute the standard errors except for Deep Ensemble, which ensembles the NNs from 4 different random seeds.\n\nFor evaluation, we define a prediction of single-member as the one-hot representation of network output with label smoothing. We select the label smoothing coefficient as 0.01 for CIFAR-10 and 0.1 for CIFAR-100. We define ensemble prediction as an averaged prediction of single-member predictions. For OOD detection, we use the variance of prediction in output space, which is competitive to recent OOD detection methods (Ren et al., 2019; Van Amersfoort et al., 2020). We use 0.01 for σ and select the best α with cross-validation. For every experiment, we used 8 NVIDIA RTX 3090 GPUs.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nI ADDITIONAL RESULTS ON BOUND ESTIMATION\n\nTable 8: Results for experiments on PAC-Bayes-CTK/NTK estimation with NP = 5, 000 and NQ = 45, 000. We use 4 random seeds to compute error bars.\n\nCIFAR-10\n\nParameter scale Trace (×10−4) Perturbation Sharpness KL Test err. (×102)\n\nBound (×102)\n\nCIFAR-100\n\nParameter scale Trace (×10−4) Perturbation Sharpness KL Test err. (×102)\n\nBound (×102)\n\nPAC-Bayes-CTK\n\nPAC-Bayes-NTK\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n1.30 ± 0.13 335.02 ± 9.57 13.48 ± 0.63 174.25 ± 5.03 23.75 ± 0.17\n\n1.31 ± 0.13 338.85 ± 9.39 13.49 ± 0.63 176.17 ± 4.94 23.87 ± 0.19\n\n1.31 ± 0.13 337.21 ± 9.47 13.49 ± 0.63 175.35 ± 4.99 23.94 ± 0.22\n\n1.31 ± 0.13 335.87 ± 8.39 13.49 ± 0.63 174.68 ± 4.46 23.94 ± 0.22\n\n541.36 ± 32.58 202.53 ± 3.14 64.70 ± 0.55 133.61 ± 1.48 31.33 ± 0.30\n\n139.50 ± 9.06 315.99 ± 2.75 52.55 ± 0.58 184.27 ± 1.24 27.99 ± 0.20\n\n54.50 ± 0.63 447.97 ± 3.75 44.25 ± 0.11 246.11 ± 1.86 26.15 ± 0.22\n\n48.86 ± 0.90 549.35 ± 7.64 42.30 ± 0.20 295.83 ± 3.82 25.64 ± 0.20\n\n27.71 ± 0.15\n\n27.82 ± 0.17\n\n27.84 ± 0.17\n\n27.85 ± 0.17\n\n33.59 ± 0.25\n\n30.59 ± 0.10\n\n29.66 ± 0.14\n\n29.98 ± 0.18\n\nPAC-Bayes-CTK\n\nPAC-Bayes-NTK\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n0.5\n\n1.0\n\n2.0\n\n4.0\n\n2.22 ± 0.14 447.86 ± 12.50 17.11 ± 0.40 232.49 ± 6.18 65.07 ± 0.33\n\n2.22 ± 0.14 447.86 ± 8.47 17.10 ± 0.40 232.48 ± 4.16 65.15 ± 0.38\n\n2.22 ± 0.14 445.76 ± 7.75 17.11 ± 0.40 231.43 ± 3.79 65.21 ± 0.39\n\n2.22 ± 0.14 446.95 ± 8.08 17.11 ± 0.40 232.03 ± 3.94 65.22 ± 0.39\n\n655.76 ± 23.48 399.66 ± 10.44 66.36 ± 0.27 233.01 ± 5.26 69.92 ± 0.50\n\n167.02 ± 5.78 622.17 ± 27.92 54.13 ± 0.25 338.15 ± 13.96 69.03 ± 0.38\n\n57.01 ± 3.92 790.63 ± 46.70 44.47 ± 0.59 417.55 ± 23.06 68.37 ± 0.49\n\n50.57 ± 4.71 823.06 ± 55.58 42.64 ± 0.79 432.85 ± 27.41 68.63 ± 0.50\n\n70.98 ± 0.35\n\n71.04 ± 0.38\n\n71.07 ± 0.38\n\n71.10 ± 0.39\n\n71.10 ± 0.27\n\n69.71 ± 0.59\n\n71.44 ± 0.62\n\n73.61 ± 0.49\n\nJ ADDITIONAL RESULTS ON IMAGE CLASSIFICATION\n\nTable 9: Uncertainty calibration results on CIFAR-10 (Krizhevsky, 2009) for ResNet-18 (He et al., 2016).\n\nNLL (↓)\n\nECE (↓)\n\nBrier. (↓)\n\nAUC (↑)\n\nCIFAR-10\n\nDeterministic MCDO MCBN Batch Ensemble Deep Ensemble Linearized Laplace\n\n0.3135 ± 0.0088 0.2845 ± 0.0148 0.2922 ± 0.0074 0.2740 ± 0.0030 0.1753 0.2077 ± 0.0032\n\n0.0350 ± 0.0014 0.0157 ± 0.0012 0.0325 ± 0.0010 0.0286 ± 0.0005 0.0067 0.0180 ± 0.0009\n\n0.0875 ± 0.0026 0.1056 ± 0.0062 0.0838 ± 0.0022 0.0814 ± 0.0009 0.0594 0.0816 ± 0.0010\n\n- 0.9172 ± 0.0074 0.9431 ± 0.0033 0.9376 ± 0.0036 0.8527 0.8991 ± 0.0198\n\nConnectivity Laplace (Ours)\n\n0.2089 ± 0.0023\n\n0.0120 ± 0.0019\n\n0.0720 ± 0.0011\n\n0.9705 ± 0.0098\n\nTable 10: Uncertainty calibration results on CIFAR-10 (Krizhevsky, 2009) for VGG-13 (Simonyan & Zisserman, 2015).\n\nNLL (↓)\n\nECE (↓)\n\nBrier. (↓)\n\nAUC (↑)\n\nCIFAR-10\n\nDeterministic MCDO MCBN Batch Ensemble Deep Ensemble Linearized Laplace\n\n0.4086 ± 0.0018 0.3889 ± 0.0049 0.3852 ± 0.0012 0.3544 ± 0.0036 0.2243 0.3366 ± 0.0013\n\n0.0490 ± 0.0003 0.0465 ± 0.0009 0.0462 ± 0.0002 0.0399 ± 0.0009 0.0121 0.0398 ± 0.0004\n\n0.1147 ± 0.0005 0.1106 ± 0.0015 0.1108 ± 0.0003 0.1064 ± 0.0012 0.0776 0.1035 ± 0.0003\n\n- 0.7765 ± 0.0221 0.9051 ± 0.0065 0.9067 ± 0.0030 0.7706 0.8883 ± 0.0017\n\nConnectivity Laplace (Ours)\n\n0.2674 ± 0.0028\n\n0.0234 ± 0.0011\n\n0.0946 ± 0.0010\n\n0.9002 ± 0.0033\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nTable 11: Uncertainty calibration results on CIFAR-100 (Krizhevsky, 2009) for VGG-13 (Simonyan & Zisserman, 2015).\n\nNLL (↓)\n\nECE (↓)\n\nBrier. (↓)\n\nAUC (↑)\n\nCIFAR-100\n\nDeterministic MCDO MCBN Batch Ensemble Deep Ensemble Linearized Laplace\n\n1.8286 ± 0.0066 1.7439 ± 0.0089 1.7491 ± 0.0075 1.6142 ± 0.0101 1.2006 1.5806 ± 0.0054\n\n0.1544 ± 0.0010 0.1363 ± 0.0008 0.1399 ± 0.0010 0.1077 ± 0.0020 0.0456 0.1036 ± 0.0004\n\n0.4661 ± 0.0018 0.4456 ± 0.0017 0.4488 ± 0.0015 0.4143 ± 0.0027 0.3228 0.4127 ± 0.0010\n\n- 0.6424 ± 0.0099 0.7039 ± 0.0197 0.7232 ± 0.0021 0.6929 0.6893 ± 0.0221\n\nConnectivity Laplace (Ours)\n\n1.4073 ± 0.0039\n\n0.0703 ± 0.0028\n\n0.3827 ± 0.0012\n\n0.7254 ± 0.0136\n\nK TRANSPOSED TABLE 3\n\nTable 12: Transposed Table\n\nOriginal\n\nGAP variants\n\nDeep Ensemble MCDO LL CL Deep Ensemble MCDO LL CL\n\nboston_housing 2.90 ± 0.03 2.63 ± 0.01 2.85 ± 0.01 2.88 ± 0.02 2.71 ± 0.01 2.68 ± 0.01 2.74 ± 0.01 2.75 ± 0.01\n\nconcrete_strength 3.06 ± 0.01 3.20 ± 0.00 3.22 ± 0.01 3.11 ± 0.02 4.03 ± 0.07 3.42 ± 0.00 3.47 ± 0.01 4.03 ± 0.02\n\nenergy_efficiency 0.74 ± 0.01 1.92 ± 0.01 2.12 ± 0.01 0.83 ± 0.01 0.77 ± 0.01 1.78 ± 0.01 2.02 ± 0.01 0.90 ± 0.02\n\nkin8nm -1.07 ± 0.00 -0.80 ± 0.01 -0.90 ± 0.00 -1.07 ± 0.00 -0.94 ± 0.00 -0.71 ± 0.00 -0.87 ± 0.00 -0.93 ± 0.00\n\nnaval_propulsion -4.83 ± 0.00 -3.85 ± 0.00 -4.57 ± 0.00 -4.76 ± 0.00 -2.22 ± 0.33 -3.36 ± 0.01 -3.66 ± 0.11 -3.80 ± 0.07\n\npower_plant 2.81 ± 0.00 2.91 ± 0.00 2.91 ± 0.00 2.81 ± 0.00 2.91 ± 0.00 2.97 ± 0.00 2.98 ± 0.00 2.91 ± 0.00\n\nprotein_structure wine\n\n2.89 ± 0.00 2.96 ± 0.00 2.91 ± 0.00 2.89 ± 0.00 3.11 ± 0.00 3.07 ± 0.00 3.07 ± 0.00 3.13 ± 0.00\n\n1.21 ± 0.00 0.96 ± 0.01 1.24 ± 0.01 1.27 ± 0.01 1.48 ± 0.01 1.03 ± 0.00 1.45 ± 0.01 1.43 ± 0.00\n\nyacht_hydrodynamics 1.26 ± 0.04 2.17 ± 0.06 1.20 ± 0.04 1.25 ± 0.04 1.71 ± 0.03 3.06 ± 0.02 1.78 ± 0.02 1.74 ± 0.01\n\nOriginal\n\nGAP variants\n\nDeep Ensemble MCDO LL\n\nCL Deep Ensemble MCDO LL\n\nCL\n\nTable 13: First 5 columns of transposed table\n\nboston_housing 2.90 ± 0.03 2.63 ± 0.01 2.85 ± 0.01\n\nconcrete_strength 3.06 ± 0.01 3.20 ± 0.00 3.22 ± 0.01\n\nenergy_efficiency 0.74 ± 0.01 1.92 ± 0.01 2.12 ± 0.01\n\n2.88 ± 0.02 2.71 ± 0.01 2.68 ± 0.01 2.74 ± 0.01\n\n2.75 ± 0.01\n\n3.11 ± 0.02 4.03 ± 0.07 3.42 ± 0.00 3.47 ± 0.01\n\n4.03 ± 0.02\n\n0.83 ± 0.01 0.77 ± 0.01 1.78 ± 0.01 2.02 ± 0.01\n\nkin8nm -1.07 ± 0.00 -0.80 ± 0.01 -0.90 ± 0.00\n\n-1.07 ± 0.00 -0.94 ± 0.00 -0.71 ± 0.00 -0.87 ± 0.00\n\nnaval_propulsion -4.83 ± 0.00 -3.85 ± 0.00 -4.57 ± 0.00\n\n-4.76 ± 0.00 -2.22 ± 0.33 -3.36 ± 0.01 -3.66 ± 0.11\n\n0.90 ± 0.02\n\n-0.93 ± 0.00\n\n-3.80 ± 0.07\n\nTable 14: Last 4 columns of transposed table\n\nOriginal\n\nGAP variants\n\nDeep Ensemble MCDO LL\n\nCL Deep Ensemble MCDO LL\n\npower_plant 2.81 ± 0.00 2.91 ± 0.00 2.91 ± 0.00\n\n2.81 ± 0.00 2.91 ± 0.00 2.97 ± 0.00 2.98 ± 0.00\n\nprotein_structure wine\n\n2.89 ± 0.00 2.96 ± 0.00 2.91 ± 0.00\n\n2.89 ± 0.00 3.11 ± 0.00 3.07 ± 0.00 3.07 ± 0.00\n\n1.21 ± 0.00 0.96 ± 0.01 1.24 ± 0.01\n\n1.27 ± 0.01 1.48 ± 0.01 1.03 ± 0.00 1.45 ± 0.01\n\nCL\n\n2.91 ± 0.00\n\n3.13 ± 0.00\n\n1.43 ± 0.00\n\nyacht_hydrodynamics 1.26 ± 0.04 2.17 ± 0.06 1.20 ± 0.04\n\n1.25 ± 0.04 1.71 ± 0.03 3.06 ± 0.02 1.78 ± 0.02\n\n1.74 ± 0.01\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nL EIGENSPECTRUMS OF EMPIRICAL CTK AND NTK\n\n(a) CTK (CIFAR-10)\n\n(b) CTK (CIFAR-100)\n\n(c) NTK (CIFAR-100)\n\n(d) NTK (CIFAR-100)\n\nFigure 3: Eigenspectrums of CTK and NTK for CIFAR datasets\n\nIn this section, we follow Algorithm 1 of Ghorbani et al. (2019) to visualize the eigenvalue densities of empirical CTK and NTK. We use 100 Lanczos iterations (Appendix F) with 4 realizations and fix the bandwidth of the RBF kernel as the difference between the maximum and minimum eigenvalue following the implementation of Ghorbani et al. (2019). We present the results in Figure 3.\n\nFigure 3 shows that empirical CTKs have positively skewed (i.e., right-tailed) eigenspectrums with modes close to 0. In other words, many of the non-zero eigenvalues of CTK are close to zero. Therefore, the corresponding h(βi) for these eigenvalues are also close to zero, as shown in Corollary 2.3 (See Fig. 2b for the visualization). As with empirical CTKs, the empirical NTKs are positively skewed, but their eigenvalue scales are much larger than CTKs. In summary, our empirical study demonstrates that although an empirical CTK can have up to NK non-zero eigenvalues, only a few eigenvalues are critical to the scale of (cid:80)P\n\ni=1 h(βi).\n\n30\n\n05001000150020000.0000.0010.0020.0030.0040.005Eigenspectrum of CTK for CIFAR-10020040060080010000.00000.00050.00100.00150.00200.00250.00300.0035Eigenspectrum of CTK for CIFAR-1000.00.20.40.60.81.01.21e60.00.20.40.60.81.01.21.41.61e5Eigenspectrum of NTK for CIFAR-1001000002000003000004000005000006000000.00.51.01.52.01e5Eigenspectrum of NTK for CIFAR-100",
    "reference": "# Summary Of The Paper\n\nThe paper proposes scaling-invariant prior and posterior distributions for PAC Bayes generalization bounds. It supports these empirically and, based on these, introduces a practical uncertainty estimation method that compares competitively with the linearized Laplace approximation.\n\n# Strength And Weaknesses\n\nStrengths:\n* The range of contributions is quite broad, from a PAC Bayes bound to a practical approximate inference method.\n* The paper is overall well-structured.\n* The bounds seem to be empirically reasonably tight.\n* The uncertainty estimation method performs competitively.\n\nWeaknesses:\n* I found it quite difficult to follow what the paper is doing in detail and would not be confident that I would be able to implement the paper just based on the manuscript. In particular, I don't really understand which dataset split is used for calculating which quantities.\n* There are a lot of grammatical mistakes in the manuscript (as far as I can tell as a non-native English speaker), to the extent that it made the paper difficult to read for me (I'd usually just mention this as a minor comment, but in this instance I find that it is to a degree that it negatively impacts the already lacking clarity of the paper).\n* There is no discussion of computational cost. It seems like the method needs to evaluate a lot (100 Lanczos iteration; inside of an optimization problem) of Jacobian-vector products, which is not cheap (especially in case each product requires a pass over the training set, although I'm not sure if this is the case).\n\n**************************************************\nPOST REBUTTAL UPDATE\n\nThe authors have extensively updated the paper, addressing the concerns around presentation. I therefore increase my score.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity** Good in terms of the overall flow and structure, but quite underwhelming when it comes to the details. There is just too much notation that is introduced inline throughout the paper, without any aids for the reader, such as a notation table or an algorithm box with pseudocode that recaps things concisely.\n\n**Quality** Overall I'm willing to give the paper the benefit of the doubt and assume that it is good work with interesting empirical and theoretical contributions (although I will caveat this with saying that I did not check the proofs on the PAC Bayes bounds).\n\n**Novelty** The ideas in this work are new.\n\n**Reproducibility** Hyperparameters are vaguely described as a table of ranges in the appendix, however I did not find any final values. Given the further lack of clarity on the method itself, I doubt I would be able to reproduce any of the results in the paper.\n\n# Summary Of The Review\n\nAn in principle interesting paper with glaring weaknesses in its presentation, which all things considered make me lean towards a rejection.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nNEURAL DISCRETE REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDesigning effective action spaces for complex environments is a fundamental and challenging problem in reinforcement learning (RL). Some recent works have revealed that naive RL algorithms utilizing well-designed handcrafted discrete action spaces can achieve promising results even when dealing with highdimensional continuous or hybrid decision-making problems. However, elaborately designing such action spaces requires comprehensive domain knowledge. In this paper, we systemically analyze the advantages of discretization for different action spaces and then propose a unified framework, Neural Discrete Reinforcement Learning (NDRL), to automatically learn how to effectively discretize almost arbitrary action spaces. Specifically, we propose the Action Discretization Variational AutoEncoder (AD-VAE), an action representation learning method that can learn compact latent action spaces while maintain the essential properties of original environments, such as boundary actions and the relationship between different action dimensions. Moreover, we uncover a key issue that parallel optimization of the AD-VAE and online RL agents is often unstable. To address it, we further design several techniques to adapt RL agents to learned action representations, including latent action remapping and ensemble Q-learning. Quantitative experiments and visualization results demonstrate the efficiency and stability of our proposed framework for complex action spaces in various environments.\n\n1\n\nINTRODUCTION\n\nRecent advances in Reinforcement Learning have yielded many promising research achievements Vinyals et al. (2019); Berner et al. (2019); Schrittwieser et al. (2019). However, the complexity of action spaces still prevents us from directly utilizing advanced RL algorithms to real-world scenarios, such as high-dimensional continuous control in robot manipulation Lillicrap et al. (2016) and structured hybrid action decision-making in strategy games Kanervisto et al. (2022). Complex action spaces lead to extensive challenges in designs of policy optimization Xiong et al. (2018b), efficiency of exploration Seyde et al. (2021b) and behaviour stability of learned agents Bester et al. (2019).\n\nTo handle these issues, some existing work first elaborately design particular reinforcement learning methods in original complex action spaces. Specifically, deterministic policy gradient methods Lillicrap et al. (2016); Fujimoto et al. (2018) are designed to handle continuous control problems. And Xiong et al. (2018b); Fan et al. (2019b) propose some techniques to extract the relationship between different action dimensions, which is important in hybrid action spaces. However, these designs often suffer from low exploration efficiency and unstable training due to the infinite action spaces and interference between different sub-actions Bester et al. (2019), respectively. Action space shaping Kanervisto et al. (2020) is another way to tackle these problems. Particularly, many RL applications Kanervisto et al. (2022); Wei et al. (2022) design specific action discretization mechanisms to simplify the decision-making spaces, leading to the promising performance improvement, but it requires intensive investigations about the corresponding environments. Moreover, the combination of many manually discretized sub-actions will result in the exponential explosion of action numbers, which is incompatible with large action spaces. Recently, some works propose to learn abstract action representations to boost RL training. HyAR Li et al. (2021) designs a special training scheme with VAE Kingma & Welling (2014) to map the original hybrid action space to a continuous latent action space. Some other methods Dadashi et al. (2022); Shafiullah et al. (2022); Jiang et al. (2022) build prior sets of discrete actions to from expert demonstrations, and then deploy RL agents on this fixed discrete action sets. To preserve the necessary attributes of environments, all the above discretiza-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ntion techniques require related domain knowledge to discard redundant information about actions, which means that they are unsuitable for different environments with arbitrary action spaces.\n\nIn this paper, we focus on how to learn a unified discrete action representations from scratch without any domain knowledge. Based on previous analysis and our investigations (as shown in Figure 1), we summarize the following advantages of discretization for the complexity of the action space:\n\n• Unified action discretization provides a powerful and general approach to dealing with reinforcement learning in complex action spaces. It is equivalent to split the entire pipeline into two parts: (1). representation learning and (2). decision-making. The former focus on intrinsic properties and data distributions of the action space, then transform various action spaces into standard discrete action sets, while the latter only needs to solve core decision-making problems.\n\n• Effective discretization can improve sample efficiency by reducing the overhead in repeating sub-optimal, useless, and semantically similar actions. RL agent can just explore and exploit the necessary subsets of the original action space during training.\n\nThen, we introduce Neural Discrete Reinforcement Learning (NDRL) framework. Specifically, inspired by VQ-VAE van den Oord et al. (2017), we propose a action representation method called Action Discretization Variational Auto-Encoder (AD-VAE) to learn latent discrete action space from the original environment, and conduct RL on the learned space utilizing any classical RL techniques about the discrete action. It is essential to capture the intrinsic properties of the original action space, which is beneficial to learn a compact latent action space while keeping necessary information of the original action space. Therefore, we design a state-conditioned action encoder and decoder, and utilize graph neural network (Kipf & Welling, 2016) and soft-argmax operation Luvizon et al. (2019) to improve the capability of AD-VAE for the relationships between different action dimensions and boundary action values. Furthermore, we find a core issue of parallel optimization of AD-VAE and RL agents: the online updates of AD-VAE may lead to semantic changes of latent actions (i.e. the non-stationary of decision spaces), resulting in severe data staleness and Q-value over-estimation. To solve this problem, we introduce action remapping and ensemble Q-learning. Concretely, we apply the classic DQN as an instance to our framework, named Action Discretization Q-learning (ADQ), which can be deployed for most complex action spaces. Compared with pioneer works (Chandak et al., 2019a; Zhou et al., 2020; Dadashi et al., 2022), to our best knowledge, our proposed framework is the first online RL paradigm capable of employing in discrete action spaces learned from different continuous and hybrid decision-making environments.\n\nTo demonstrate the efficiency and stability of our NDRL framework and AD-VAE method, we evaluate it on the classic continuous control benchmark MuJoCo Todorov et al. (2012), showing that ADQ can achieve excellent performance operating in high-dimensional continuous space even with a small number of actions. To evaluate the generality, we test it on the hybrid action environments Gym Hybrid thomashirtz (2021), HardMove from HyAR and GoBigger Zhang (2021). The results show that ADQ outperforms current state-of-the-art hybrid action algorithms in both sample efficiency and final performance. Besides, we also conduct a series of ablation study experiments and interpret more details about NDRL by visualization on the latent space.\n\n2 RELATED WORK\n\nAction Discretization Discretization and continuity are like the relationship between 0 and 1 in the binary world. All things, including time and space, are continuous, but for the convenience of cognition, we will discretize all of them. Only then can we have measures, such as the concepts of hours, minutes and meters. In RL, learning directly on a high-dimensional continuous action space may present difficulties in exploration due to the uncountable set of actions. In addition, (Bjorck et al., 2021) argues that the nonlinear function saturation caused by unstable network parameterization will cause the well-known high variance problem. The most straightforward solution is discretization, however, this usually suffers from the curse of dimensionality. To alleviate this problem, many assumptions about the action space have been proposed. For example, (Tang & Agrawal, 2020) verifies the feasibility of discretizing the action space in on-policy optimization by utilizing the factorized distribution across action dimensions. In (Dadashi et al., 2022), the authors proposed to circumvent the curse of dimensionality problem by learning a set of plausible discrete actions from expert demonstrations. We argue that this algorithm can naturally be seen as a special case of\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nour NDRL framework. (Seyde et al., 2021a) explored the effect of extreme actions on continuous action control, which also inspired the design of AD-VAE for maintaining boundary actions.\n\nHybrid Action Space Many real-world problems may have hybrid action spaces. For example, in the GoBigger game, we need to select an action type first, and then give its corresponding continuous control arguments. The simplest idea is to map it onto a unified homogeneous action space, like discretizing continuous actions or making discrete actions continuous, however this may create scalability issues with the curse of dimensionality. Going a step further, recent work proposes various hand-designed network structures to learn directly on the original hybrid action space. For instance, Parameterized Action DDPG (Hausknecht & Stone, 2016) uses a modified DDPG actorcritic structure and HPPO (Fan et al., 2019a) proposes different types of heads for different action types. PDQN (Xiong et al., 2018a) and MPDQN (Bester et al., 2019) use a hybrid structure of DQN and DDPG, explicitly modeling the dependencies between continuous and discrete sub-actions.\n\nAction Representation Learning The concept of the latent space is widely used in various elements of reinforcement learning, such as latent state and dynamics. But in action space, (Chandak et al., 2019b) proposes action representation learning in a large action space, leveraging the structure in the space of actions and showing its importance for enhancing generalization over large action sets in real-world large-scale applications. (Li et al., 2021) propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous arguments via an embedding table and conditional Variational Auto-Encoder (VAE).\n\n3 BACKGROUND Markov Decision Process In reinforcement learning, we model a decision-making problem as a Markov Decision Process (MDP) M=(S, A, P, R, γ, ρ0), where S and A represent the state space and the action space, P is the transition function: S × A → S, R is the expected reward function: S × A → R, γ ∈ [0, 1) is the discounted factor, and ρ0 is the initial state distribution. The objective of RL is to learn a policy π : S → A to maximize the expected discounted return J(π) = Eπ,ρ0,P,R[(cid:80)∞ t=0 γtrt], where the expectation is taken with respect to the trajectory distribution induced by π and environment dynamics.\n\nIn decision-making problems, at each time t, the agent receives a state and Hybrid Action Space carries out an action, which can be divided into 3 types: discrete, continuous and hybrid action. Here we give a general formalization. A hybrid action a contains N decision nodes (sub-actions). At each decision node i and time step t, the agent needs to give a proto-action at,i with two attributes including type and range of values; the type of value at,i,type indicates whether it is continuous or discrete, and the range of value at,i,range indicates its corresponding executable action set. Thus, we use an ordered tuple like a = (at,1, at,2, ..., at,N ) to describe these basic nodes. Furthermore, the relations between decision nodes can be defined by a adjacency matrix Ar,t in graph theory, called action relation matrix. The value of the elements in the matrix is {0, 1}. If the element Ai,j in row i and column j is equal to 1, it means that there is a directed edge from decision node i to j. If equal to 0, there is no dependency between them. In many real-world problems, the dependencies between the proto-action always are invariant, that is, the action relation matrix is independent of t. Generally, hybrid action space can be defined as a tuple:\n\nA = ({at,i,type, at,i,range | i ∈ [1, ..., N ]}, Ar)\n\n(1)\n\nThe Parameterized Action Space defined in (Masson et al., 2016) is a special instance of our definition, specifically, which is equivalent to action a containing 2 decision nodes and at,0,type = 0, at,0,range = K, at,1,type = 1, at,1,range = X. There is only a directed edge from decision node 1 to decision node 2, formally, the adjacency matrix Ar is:\n\n(cid:18) 0 0\n\n(cid:19)\n\n1 0\n\n(2)\n\nAction Transformed MDP Here we augment a MDP with action transformation, which can be defined as M=(S, A, P, R, γ, ρ0, T ), where T denotes the transformation operator on action space, such as action discretization . Denote the transformed action as k, we can describe T as:\n\nThe other elements are consistent with the original definition of MDP. Through this transformation, we can learn an RL agent more efficiently on this new, often reduced, latent action space.\n\nT : k = T (s, a) a = T−1(s, k)\n\n(3)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Top: Visualization analysis about different action spaces in the same LunarLander (Brockman et al., 2016) environment. ExpertCluster is discrete action obtained by clustering on TD3 expert data). HandCrafted is obtained by the threshold of spaceship engine. Vertical engine only enables when x is bigger than 0; And if y is smaller than -0.5, the left booster will fire, and if y is bigger than 0.5, the right booster will fire. Bottom: (left) Episode return of three algorithms on LunarLander: TD3 (original continuous action space), DQN + expert cluster, ADQ (discrete action learned by AD-VAE from scratch) ; (right) The ratio of some semantically same actions (no operation), more no-op actions means greater redundancy during training, i.e., lower is better.\n\nVector Quantised Variational AutoEncoders Motivated by vector quantization (VQ) and Variational AutoEncoder, VQ-VAE van den Oord et al. (2017)) is designed to learn a discrete latent representations to represent the original data distribution (e.g., text, image) in a unsupervised manner. VQ-VAE mainly comprises of an encoder eφ, an decoder dψ, and a learnable code table Vε. The learnable code table maintains a set of embeddings {ek}K−1 k=0 . Firstly, the encoder takes the data x as input, and outputs an embedding vector ze = f (x). Then using the embedding vector ze to query the nearest (usually in Euclidean distance) code vector zd in the code table and outputs an latent index k simultaneously. Thirdly, the decoder uses the code vector ed as its input to produce reconstructions ˆx. The whole objective is to minimize the following loss function\n\nL = Ld(ˆx, x) + ∥sg [eφ(x)] − ze∥2\n\n2 + β ∥eφ(x) − sg[ze]∥2\n\n2\n\nzd = ek, where k = argminj ∥ze − ej∥2\n\n(4)\n\n(5)\n\nWhere sg(·) is the stop gradient function. The first term of loss function is to reconstruct error in certain distance metric, the second item and the third term is embedding loss and commitment loss respectively. Please refer to van den Oord et al. (2017) for more details.\n\n4 NEURAL DISCRETE REINFORCEMENT LEARNING\n\n4.1 MOTIVATION\n\nFirst, to motivate our proposed framework, we further discuss the advantages of reinforcement learning in discrete action spaces from the following 3 aspects: unity, efficiency and stability.\n\n4\n\n0M0.25M0.5M0.75M1M1.25M1.5MEnv Steps2001000100200300ReturnTD3DQN + expert clusterADQ0M0.25M0.5M0.75M1M1.25M1.5MEnv Steps0.00.20.40.60.81.0Noop Action RatioTD3DQN + expert clusterADQUnder review as a conference paper at ICLR 2023\n\n4.1.1 UNITY\n\nIn practice, researchers need to first transform the original decision-making problem into a standard MDP form. Due to the different types of target action spaces, it is inevitable to utilize different techniques for the corresponding action spaces, which brings non-negligible learning and tuning costs beyond core RL optimization. But when we dive deeper into this problem, we find there are obvious redundancies in most complex action space, e.g., only a few discrete actions/samples can perform well in multi-dimensional continuous control Seyde et al. (2021b); Hubert et al. (2021). In Figure 1(a), we also illustrate the entropy of different action spaces to show the effectiveness of discretization. Therefore, learning compact action representations instead of repeating some dirty work in the raw action space is a natural and powerful choice. Moreover, the decoupling of action representation learning and RL allows researchers to concentrate on only one of the topics.\n\n4.1.2 EFFICIENCY\n\nFurthermore, we verify the efficiency of action discretization in online RL training. As shown in Figure 1(a), we find the well-designed discrete action space shows lower entropy and more compact representation. We also conduct a simple experiment to test the performance and the ratio of useless action like excessive no operation in 1(b). Based on these observations, we design AD-VAE to automatically learn the latent discrete action space from the original action space. On one hand, this model can learn to approximate the necessary parts and ignore meaningless parts of the original space, which is beneficial to improve exploration efficiency. On the other hand, some works Jiang et al. (2022) show that the marginal distribution of each action dimension is often multi-modal, thus using a discrete categorical distribution is more suitable than the simple regression in TD3.\n\n4.1.3 STABILITY\n\nHowever, previous methods Dadashi et al. (2022); Jiang et al. (2022) only succeed in deploying action discretization on imitation learning and offline RL settings, which means that it needs to first learn a latent space and then apply decision-making algorithms on the fixed discrete action space. We tried to directly utilize AD-VAE on online settings but obtain unstable episode returns. Compared to training on the frozen discrete action space, we find some abnormal indicators including unusually high Q-values and obvious fluctuations in gradient scale and variance. Due to parallel optimization about AD-VAE and RL agents, it would be a hazardous non-stationary MDP if the latent action space changes too much. To figure out this problem, we propose action remapping and ensemble Q-learning. Together with AD-VAE, these techniques form the entire NDRL framework.\n\n4.2 NDRL FRAMEWORK\n\n4.2.1 OVERVIEW\n\nMotivated by the above analysis, we propose a framework that combines online RL training with learnable action discretization on complex action spaces, named Neural Discrete Reinforcement Learning (NDRL). At a high-level, this framework is a “meta-algorithm” that splits decision-making in arbitrary complex action spaces into two parts: the part of representation learning mapping the original action space to a new discrete action space and the reinforcement learning part built on learned discrete representations. Based on this design, any reinforcement learning designed for discrete space (e.g. DQN, PPO) can be potentially applied to different complex action spaces. The overview of NDRL framework is described in Figure 1, and we will introduce the data collecting phase and network training phase respectively:\n\nCollecting Phase: This phase describes how to collect data used in the training of AD-VAE and RL agents. Given the current state st, RL agents first select corresponding discrete latent action kt, then utilizing AD-VAE decoder to transform it back to the original action space. Moreover, NDRL also deploys some extra randomization operations (e.g. epsilon greedy in DQN) on the decoded action to maintain sufficient exploration. The final action at interacts with the environment and it returns reward rt and next state st+1. All necessary data will be packed into a transition and put into buffers.\n\nTraining Phase: The training phase is to execute two training pipelines with different data buffers in parallel. (1) For action representation learning, NDRL follows the main training scheme of VQVAE to reconstruct primitive actions with state conditions. (2) For RL training, it first remaps the\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of NDRL framework. Collect Phase: Given the current state st, the RL agent gives latent discrete action kt, and utilizes AD-VAE code table and decoder to obtain the raw action at, then interacts with the environment. Train Phase: First remap the sampled transition (st, at, rt, st+1) with latest AD-VAE encoder and table to kt, then deploy the normal RL training. Note the AD-VAE is also trained with state condition in parallel. Black round means continuous action space, while grey round means discrete, the whole tree structure means hybrid action.\n\noriginal action in the sampled transition with AD-VAE encoder to obtain the latest latent action kt, then conduct normal policy optimization on transformed data.\n\nBesides, for the better initialization of off-policy RL algorithms, we also design a pretrain stage at the beginning of the entire algorithm. The full pseudo-code of NDRL is provided in Algorithm 1. If there are some expert demonstrations, a promising set of discrete action candidates can be learned from it. Otherwise, we can collect data with random policy to train the AD-VAE and learn some basic properties of the original action space for subsequent parallel learning. After pretrain stage, data collecting and two parts of training can be executed asynchronously, so the computational cost of NDRL can be easily optimized and show the same efficiency as other methods.\n\n4.2.2 AD-VAE\n\nIn this section, we first analyze the problem of directly using the original VQ-VAE, then introduce the specific design about our proposed Action Discretization Variational AutoEncoders (AD-VAE).\n\nModeling Intrinsic Properties of Action Spaces In online RL training, there are both necessary and redundant subsets of the original action space. For instance, Seyde et al. (2021b) pointed out that in some continuous action tasks, the optimal action may be at some extreme boundary values (e.g. -1 and 1), which is a common phenomenon in several physical simulation environments. Also, Bester et al. (2019) revealed that effective combinations between different action parts is significant for the optimization in hybrid action space. Therefore, it is wise to pay more attention to those actions that are more beneficial to the optimal policy, and ignore some useless even harmful actions. Otherwise, trivial action reconstruction with the same weights and distance metrics can only obtain some over-smooth actions. Besides, we also can take advantages of the value function to focus on those actions with higher future return, saving the cost of agent exploration and exploitation.\n\nInformation Completion in AD-VAE We first introduce the technique of information completion in AD-VAE. In some complex environments, the set of optimal actions could be large and vary in different training stages. If we reconstruct actions without state information, AD-VAE must maintain a large discrete action sets and RL agents needs to learn decisions on a great number of actions, which can easily make it overwhelming and cause training instability. On the contrary, properly use of state-conditioned input in both encoder and decoder of AD-VAE can increase the representational\n\n6\n\nNeural Discrete Reinforcement Learning FrameworkCollecting PhaseTrainingPhaseAgentAD-VAE Decoder Normal RL InferenceNormal RL TrainingAgentLatent Action Remappingi......AD-VAE Code Tablee0e2e1eK-1AD-VAE TrainingRL TrainingAD-VAE EncoderAD-VAE Code TableAD-VAE DecoderAction Representation LearningHybrid Action SpaceDiscrete Action Space......AD-VAE EncoderAD-VAE Code TableDataAction Discretization (AD-VAE) RL AgentProcessUnder review as a conference paper at ICLR 2023\n\npower and diversity, and reduce the burden of action reconstruction and RL training. Moreover, the relationship between different action parts, i.e. the action relation matrix mentioned in Section 3, can assist AD-VAE to learn more efficiently and reasonably, so we can represent this information as the connections of nodes with a graph neural network. Besides, AD-VAE is not designed to learn the entire action space but to properly model the subsets required by current RL optimization, thus we customize sampling method and training scheme for AD-VAE, including sampling a mixture of stale data in replay buffer and latest collected data as a training mini-batch, validating the reconstruction error of actions to indicate the update frequency.\n\nContinuous Action Regression in AD-VAE Another important intrinsic property of original action spaces is some special action values, such as the extreme actions mentioned in Seyde et al. (2021b) or thresholds of engine dynamics in LunarLander. It is critical for action representation networks to restore these continuous value accurately. Therefore, in AD-VAE, we adopt the softargmax operation to automatically reconstruct these special actions. Assuming the range of original action is [Amin, Amax], and it is divided into N + 1 bins on average, the predicted action is:\n\nˆa =\n\nN (cid:88)\n\nj=0\n\nsj ∗ pj (si) , sj = Amin + j ∗ (Amax − Amin)/N\n\n(6)\n\nWhen evaluation, we directly output the corresponding support value if the probability of the support is greater than a threshold (e.g. 0.9). In some environments like Hopper/Halfcheetah, we find this design can help a simple DQN agent achieve a comparable performance with TD3.\n\nOther parts of AD-VAE follow the design of VQ-VAE, the whole training procedure is to minimize the loss function described in Equation 4.\n\n4.2.3 ADAPTING RL TO LATENT ACTION SPACES\n\nIn this section, we continue to analyze why and how to adapt online RL to latent discrete action spaces, and then illustrate an instance of our NDRL framework on the value-based RL algorithm DQN, Action Discretization Q-learning (ADQ).\n\nSemantic Inconsistency In online RL, Double DQN (van Hasselt et al., 2016) pointed out that Q-value over-estimation problems caused by the function approximation error and the max operator in the bootstrap target often lead to performance deterioration. Furthermore, this problem may be exacerbated in NDRL. On one hand, latent action stored in replay buffer will be stale and biased due to updates of AD-VAE. On the other hand, since AD-VAE is dynamically and simultaneously updated together with RL agents, for a particular latent action, the corresponding action in the original action space could often change, which is more likely to lead to the overestimation of Q-value.\n\nLatent Action Remapping Similar to the reanalyze operation in MuZero Schrittwieser et al. (2019), we design a latent action remapping operation to solve the problem of stale data. In the collected mini-batch {st, at, kold , rt, st+1}, the latent action is determined by the old version of AD-VAE. When updating RL agents, we remap the original action to the corresponding latent action via the latest action encoder eφ: kt = eψ(st, at), and then executes RL training on the remapped samples {st, sg[knew\n\n], rt, st+1} (sg means the stop gradient operation).\n\nt\n\nt\n\nEnsemble Q-learning To further alleviate the Semantic Inconsistency problem, inspired by previous work Anschel et al. (2017); An et al. (2021), we propose an ensemble Q-learning method for more stable Q-value updates for latent action space, reducing the uncertainty of approximation error and over-estimation, which greatly improves the stability of parallel optimization of AD-VAE and RL agents. Specifically, we utilize a shared state encoder and N ensemble Q-value heads, i.e., the penultimate layer of the Q network is connected to N linear layers and outputs N Q-value, then we adjust the update equation of Double DQN as follows:\n\nLi =\n\n(cid:104) Q(st, at; θk) − [rt + γminkQ(st+1, at+1; ˆθk)]\n\n(cid:105)2\n\nat+1 = argmax\n\n1 N\n\n(cid:88)\n\nk\n\nQ(st+1, at+1; θk)\n\n(7)\n\n(8)\n\nWhere Li means the loss function of i-th Q head. Detailed experiments can be found in Section 5.3.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: Training curves of ADQ against other baseline algorithms in environments with complex action spaces. Top: In four continuous action environments of MuJoCo, ADQ significantly outperforms DQN with manually discretized action space, and is basically comparable to the classic TD3 algorithm. Bottom: In four hybrid action environments in Gym-Hybrid, HardMove and GoBigger, ADQ outperforms the baseline MPDQN and HPPO in both performance and stability. Curves and shadings denote the mean and standard deviation over 5 seeds.\n\nFigure 4: Visualization of the latent action space of LunarLander games. Details is in 5.2.\n\n5 EXPERIMENTS\n\nFor the evaluation of our NDRL framework, we ask and answer the following questions: 1) Is it efficient and stable to employ online RL training on discretization action spaces for different decision problems, especially in high-dimensional continuous and hybrid action spaces? (Section 5.1); 2) How do we interpret the training of the latent action space? Can we further verify some observations mentioned in introduction parts? (Section 5.2); 3) How do various designs improve the NDRL framework, including AD-VAE and other RL adaption techniques? (Section 5.3).\n\n5.1 MAIN RESULTS\n\nIn this section, we investigate the performance and efficiency of NDRL in various continuous and hybrid action environments against previous algorithms designed specifically for these action spaces. Firstly, we evaluate our methods on MuJoCo, a classic continuous control benchmark, including two high-dimensional continuous domains (Ant and Humanoid with 8 and 17 dimensions respectively). Note we also add a few redundant dimensions in original action spaces. We set up two comparison groups for our ADQ, one is the popular continuous action space algorithm TD3, and the other is naive DQN deployed in the manually discretized action space, i.e., equally dividing the original continuous action into 3 bins at each dimension and using their Cartesian product to obtain handcrafted discrete actions. In Figure 3, ADQ can acquire comparable results to TD3 and show a obvious improvement over naive DQN in all four domains on the top. Secondly, we leverage GymHybrid, HardMove and GoBigger to verify the effectiveness of ADQ in more complex hybrid action spaces. This requires to deal with the relationship between different actions parts (e.g., the value of action arguments depend on the choice of action type) and more complex environment dynamics (GoBigger). At the bottom of Figure 3, we compare ADQ with two types of hybrid action space algorithms, MPDQN and HPPO, suggesting that ADQ can automatically learn the intrinsic properties of discrete action type and continuous action arguments, and performs excellent performance and solid stability. All the detailed settings are shown in Appendix ?? and ?? respectively.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Ablating results of ADQ in two types of environments (continuous and hybrid) over 5 seeds. When we remove any one of the proposed techniques, the performance of ADQ will drop significantly, which verifies the effectiveness of our proposed techniques. ADQ w/o AR is especially important for improving ADQ performance in environments such as HalfCheetah-v3.\n\n5.2 VISUALIZATION ANALYSIS OF LATENT ACTION SPACE\n\nFurthermore, we demonstrate the learned latent action representation in LunarLander environment for 2-dimensional continuous control. Figure 4 first shows the status of the spaceship when it is about to land (left), i.e., launching both horizontal and vertical engines to control speed and position, then uniformly samples points in original continuous space and transforms them with AD-VAE encoder to find their nearest discrete indexes (middle). Also, we directly send the corresponding embeddings in code table to decoder to acquire their counterparts in the raw space (right). We can observe that the learned latent space is similar to the intrinsic mechanisms of this environment, highlighted by red line: only using one discrete action to represent less important actions in this state like no-op, mapping several different actions to the bottom right corner.\n\n5.3 ABLATION STUDIES\n\nWe also empirically evaluate the specific impacts of our proposed AD-VAE and RL adaption techniques on two example environments respectively: HalfCheetah-v3 (continuous), and HardMovev0-n10 (hybrid). The ablation results are shown in Figure 5. Concretely, we have the following five ablation variants in total, and their brief descriptions are as follows:\n\nADQ w/o remapping: ADQ variant that does not remap latent actions during RL training.\n\nADQ w/o ensemble: A variant of ADQ that doesn’t utilizing the Ensemble Q-Learning technique.\n\nADQ w/o warmup: ADQ variant that starts training without any warmup pretraining.\n\nADQ w/o AR: ADQ variant with traditional VQ-VAE reconstruction head.\n\nADQ w/o AR + hand-crafted action: Built on ADQ w/o AR, this variant adds manually selected boundary actions (i.e. the Bernoulli extreme actions) to latent discrete action spaces for RL training.\n\nFigure 5 show that when we remove either of the proposed techniques, the performance of ADQ drops significantly in both two environments, verifying the effectiveness of our proposed techniques. Due to the semantic inconsistency problem, the ADQ w/o ensemble agent suffers from severe overestimation issues and finally show poor performance. The ADQ w/o remapping agent meets the same problem, but in some cases the over-estimation problem can be partially alleviated by Ensemble Q-learning technique. The ADQ w/o warmup agent shows much slow learning progress due to lack of good starting points. Note that in environments with boundary optimal actions such as HalfCheetah-v3, ADQ w/o AR is especially important for improving ADQ performance, even better than the variant using extra hand-crafted actions. Other ablation results like pretraining on expert demonstrations and the sensity of hyper-parameters can also be found in Appendix.\n\n6 CONCLUSIONS AND LIMITATIONS Starting from comprehensive analysis for action discretization, we introduce a general and efficient paradigm named Neural Discrete Reinforcement Learning, including our proposed AD-VAE and RL adaption techniques. We empirically evaluate the efficiency and stability of our framework. Although our method achieve superior performance in different benchmark environments, there are still some challenging action spaces in multi-agent games, such as variable-length actions in episodes. Besides, combining latent discrete actions with MCTS is also a valuable attempt. We will continue to pursue ultimate solution for action space shaping in future work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nGaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 7436–7447, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 3d3d286a8d153a4a58156d0e02d8570c-Abstract.html.\n\nOron Anschel, Nir Baram, and Nahum Shimkin. Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 176–185. PMLR, 2017. URL http://proceedings.mlr.press/v70/anschel17a.html.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal J ́ozefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond ́e de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. CoRR, abs/1912.06680, 2019. URL http://arxiv.org/abs/1912.06680.\n\nCraig J. Bester, Steven D. James, and George Dimitri Konidaris. Multi-pass q-networks for deep reinforcement learning with parameterised action spaces. CoRR, abs/1905.04388, 2019. URL http://arxiv.org/abs/1905.04388.\n\nJohan Bjorck, Carla P Gomes, and Kilian Q Weinberger. Is high variance unavoidable in rl? a case\n\nstudy in continuous control. arXiv preprint arXiv:2110.11222, 2021.\n\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\n\nWojciech Zaremba. Openai gym, 2016.\n\nY. Chandak, G. Theocharous, J. Kostas, S. M. Jordan, and P. S. Thomas. Learning action represen-\n\ntations for reinforcement learning. In ICML, volume 97, pp. 941–950, 2019a.\n\nYash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learning action representations for reinforcement learning. In International conference on machine learning, pp. 941–950. PMLR, 2019b.\n\nRobert Dadashi, L ́eonard Hussenot, Damien Vincent, Sertan Girgin, Anton Raichuk, Matthieu Geist, and Olivier Pietquin. Continuous control with action quantization from demonstrations. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 4537–4557. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/dadashi22a.html.\n\nZ. Fan, R. Su, W. Zhang, and Y. Yu. Hybrid actor-critic reinforcement learning in parameterized\n\naction space. In IJCAI, pp. 2279–2285, 2019a.\n\nZhou Fan, Rui Su, Weinan Zhang, and Yong Yu. Hybrid actor-critic reinforcement learning in parameterized action space. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 2279–2285. ijcai.org, 2019b. doi: 10.24963/ijcai.2019/316. URL https://doi.org/10. 24963/ijcai.2019/316.\n\nS. Fujimoto, H. v. Hoof, and D. Meger. Addressing function approximation error in actor-critic\n\nmethods. In ICML, volume 80, pp. 1582–1591, 2018.\n\nM. Hausknecht and P. Stone. Deep reinforcement learning in parameterized action space. ICLR,\n\n2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nThomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 4476–4486. PMLR, 2021. URL http://proceedings.mlr.press/v139/ hubert21a.html.\n\nZhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rockt ̈aschel, Edward Grefenstette, and Yuandong Tian. Efficient planning in a compact latent action space. CoRR, abs/2208.10291, 2022. doi: 10.48550/arXiv.2208.10291. URL https://doi.org/10. 48550/arXiv.2208.10291.\n\nAnssi Kanervisto, Christian Scheller, and Ville Hautam ̈aki. Action space shaping in deep reinIn IEEE Conference on Games, CoG 2020, Osaka, Japan, August 24forcement learning. 27, 2020, pp. 479–486. IEEE, 2020. doi: 10.1109/CoG47356.2020.9231687. URL https: //doi.org/10.1109/CoG47356.2020.9231687.\n\nAnssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang, Haicheng Chen, Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, Franc ̧ois Fleuret, Alexander Nikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons learned. CoRR, abs/2202.10583, 2022. URL https: //arxiv.org/abs/2202.10583.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/ abs/1312.6114.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. arXiv preprint arXiv:1609.02907, 2016.\n\nBoyan Li, Hongyao Tang, Yan Zheng, Jianye Hao, Pengyi Li, Zhen Wang, Zhaopeng Meng, and Li Wang. Hyar: Addressing discrete-continuous action reinforcement learning via hybrid action representation. arXiv preprint arXiv:2109.05490, 2021.\n\nTimothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http: //arxiv.org/abs/1509.02971.\n\nDiogo C. Luvizon, Hedi Tabia, and David Picard. Human pose regression by combining indirect part detection and contextual information. Comput. Graph., 85:15–22, 2019. doi: 10.1016/j.cag. 2019.09.002. URL https://doi.org/10.1016/j.cag.2019.09.002.\n\nW. Masson, P. Ranchod, and G. D. Konidaris. Reinforcement learning with parameterized actions.\n\nIn AAAI, pp. 1934–1940, 2016.\n\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. CoRR, abs/1911.08265, 2019. URL http://arxiv.org/abs/1911.08265.\n\nTim Seyde, Igor Gilitschenski, Wilko Schwarting, Bartolomeo Stellato, Martin Riedmiller, Markus Wulfmeier, and Daniela Rus. Is bang-bang control all you need? solving continuous control with bernoulli policies. Advances in Neural Information Processing Systems, 34:27209–27221, 2021a.\n\nTim Seyde,\n\nIgor Gilitschenski, Wilko Schwarting, Bartolomeo Stellato, Martin A. Riedsolvmiller, Markus Wulfmeier, and Daniela Rus. ing continuous control with bernoulli policies. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances\n\nIs bang-bang control all you need?\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nin Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 27209– URL https://proceedings.neurips.cc/paper/2021/hash/ 27221, 2021b. e46be61f0050f9cc3a98d5d2192cb0eb-Abstract.html.\n\nNur Muhammad (Mahi) Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning k modes with one stone. CoRR, abs/2206.11251, 2022. doi: 10.48550/arXiv.2206.11251. URL https://doi.org/10.48550/arXiv.2206.11251.\n\nYunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization. In Proceedings of the aaai conference on artificial intelligence, volume 34, pp. 5981–5988, 2020.\n\nthomashirtz. Gym hybrid. https://github.com/thomashirtz/gym-hybrid, 2021.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\nA ̈aron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.\n\nNeural discrete repreIn Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. sentation learning. (eds.), Advances Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett in Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6306–6315, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html.\n\nInformation Processing Systems 30:\n\nAnnual Conference on Neural\n\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qIn Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the Thirtieth learning. AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp. 2094–2100. AAAI Press, 2016. URL http://www.aaai.org/ocs/index.php/AAAI/ AAAI16/paper/view/12389.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Micha ̈el Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, R ́emi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, C ̧ aglar G ̈ulc ̧ehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario W ̈unsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft II using multi-agent reinforcement learning. Nat., 575(7782):350–354, 2019. doi: 10.1038/s41586-019-1724-z. URL https://doi.org/10.1038/s41586-019-1724-z.\n\nHua Wei, Jingxiao Chen, Xiyang Ji, Hongyang Qin, Minwen Deng, Siqin Li, Liang Wang, Weinan Zhang, Yong Yu, Lin Liu, Lanxiao Huang, Deheng Ye, Qiang Fu, and Wei Yang. Honor of kings arena: an environment for generalization in competitive reinforcement learning. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2022.\n\nJ. Xiong, Q. Wang, Z. Yang, P. Sun, L. Han, Y. Zheng, H. Fu, T. Zhang, J. Liu, and H. Liu. Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space. CoRR, abs/1810.06394, 2018a.\n\nJiechao Xiong, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong Zhang, Ji Liu, and Han Liu. Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space. CoRR, abs/1810.06394, 2018b. URL http:// arxiv.org/abs/1810.06394.\n\nMing Zhang. Gobigger: A scalable platform for cooperative-competitive multi-agent reinforcement\n\nlearning. https://github.com/opendilab/GoBigger, 2021.\n\nW. Zhou, S. Bajracharya, and D. Held. PLAS: latent action space for offline reinforcement learning.\n\nCoRR, abs/2011.07213, 2020.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe authors propose NDRL, which is a class of methods to automatically discretize action spaces.  They identify issues with prior work and with their methods, and propose improvements to address these issues.  They analyze their methods empirically.\n\n# Strength And Weaknesses\n\n**Edits:**\n- introduction, “prior sets of discrete actions to from expert demonstrations”: grammar issue?\n- introduction: “utilize graph neural network” should be “utilize a graph neural network”\n- introduction: “and soft-argmax operation” should be “and a soft-argmax operation”\n- “Generally, hybrid action space can be defined as a tuple:” add “a” before a “hybrid”\n- “Then using the embedding vector z^e…”: grammar, what noun is doing the “using”?\n- “The full pseudo-code of NDRL is provided in Algorithm 1”: this is in the supplementary material, so the reference to the pseudo-code should say “in supplementary material Section …”, to avoid making the reader look futilely around the main body of the paper for the algorithm.\n- bottom of page 8: “All the detailed settings are shown in Appendix ?? and ?? respectively.”\n\n**Strengths:**\n- The ablation studies are nicely done and convincing.\n- The authors appear to have chosen their hyperparameters in a principled manner: based on prior work, with little (potentially-unfair-to-baselines) tuning.  When they did tune, they explored those hyperparameters thoroughly, which further strengthens this work (Section A.5).\n\n**Small clarity weaknesses:**\n- Section 3, Hybrid Action Space, first ~5 sentences (through “to describe these basic nodes.”): This is a little hard to follow.  Upon several rereads of this part, I think I fully understand, but a more rigorous approach to defining this notation would make this part of the paper clearer and easier to read.\n- psi, phi, and several other symbols were not defined.  Don’t make the reader guess or infer these definitions.\n\n**Larger weaknesses (clarity and other):**\n- Section 3, Vector Quantised Variational AutoEncoders: I found this difficult to read.  See the question below and the relevant edit above for ideas to make this part clearer.\n- ”In Figure 1(a), we also illustrate the entropy of different action spaces to show the effectiveness of discretization”: A minor issue is there is no 1a subfigure.  A more significant issue: is the entropy being shown, as indicated in the quote above?  How so?  Is H entropy?  Was this defined?  What is x in this figure?  It also seems to be undefined.\n- On a similar note, Figure 1 and its caption are very confusing.  In addition to the issues mentioned above:\n    - x and y are undefined. (The x here does not seem to be the same as the also-undefined x which appears in the top part of the figure)\n    - The sentences “HandCrafted is obtained…right booster will fire” do not make sense to me.  (These sentences seem to be describing a hand-crafted policy, but that does not seem to fit this context, which is talking about action spaces.)\n- Section 4.2.1: The paper completely lost me here; the clarity needs improvement.  Some examples of things I am confused about from this section:\n    - Comparing Figure 2 to Algorithm 1, the phases and terminology are different.  Is the “Collecting Phase” the same thing as “Stage 1”?  If so, another issue is that Figure 2 implies that data is only collected at this stage, but Algorithm 1 talks about using that data to train during this stage.\n    - Figure 2: Shouldn’t there be an arrow from the k vector to the code table?\n    - It is not clear (from the figure, the caption, or the text) where the collected data is going or how it is being used in the training phase (or even if it is being used in the training phase).\n    - “All necessary data will be packed into a transition and put into buffers.”  I am not sure what a transition means in this context (maybe a {s, a, s’, r} tuple?), or where the buffers go, or how the data is subdivided into different buffers\n    - It’s not clear to me how the decoder, the hybrid action space, and the selected action (a_t) interact.  (Although this could be related to the clarity issues from Section 3, rather than a problem with Section 4.2.1.) \n    - Does the collecting phase happen once (many episodes of data), and then the training phase happen after?  Or does the algorithm loop back and forth between the phases? (And if so, how often?  Once per episodes?  Once per timestep?  Some other interval?)  The answers to these questions are not clear from Figure 2 and Section 4.2.1.  Also, while Algorithm 1 may give answers to these questions, the relationships between the components of Algorithm 1 and the components of Figure 2 is not clear (discussed more above).\n- Claim: “ADQ significantly outperforms DQN with manually discretized action space”. In RL, 5 seeds is not usually sufficient to make a claim like this, due to the large variance between runs.  This claim is not convincing, nor are several other claims in the paper based on a similarly-small number of seeds.\n\n**Question:** “the decoder uses the code vector e^d”: should this be z^d?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nWhile the authors’ ideas make sense at a high level, I struggled to follow the details of their proposed algorithms (see specific clarity issues above).  This negatively affected the quality and reproducibility of the paper in my view.\n\n# Summary Of The Review\n\nThe authors study an important and interesting problem; however, the clarity of the paper could be improved.  There are also some empirical problems.  However, the empirical contribution overall was strong.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMUTUAL INFORMATION REGULARIZED OFFLINE REINFORCEMENT LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOffline reinforcement learning (RL) aims at learning an effective policy from offline datasets without active interactions with the environment. The major challenge of offline RL is the distribution shift that appears when out-of-distribution actions are queried, which makes the policy improvement direction biased by extrapolation errors. Most existing methods address this problem by penalizing the policy for deviating from the behavior policy during policy improvement or making conservative updates for value functions during policy evaluation. In this work, we propose a novel MISA framework to approach offline RL from the perspective of Mutual Information between States and Actions in the dataset by directly constraining the policy improvement direction. Intuitively, mutual information measures the mutual dependence of actions and states, which reflects how a behavior agent reacts to certain environment states during data collection. To effectively utilize this information to facilitate policy learning, MISA constructs lower bounds of mutual information parameterized by the policy and Q-values. We show that optimizing this lower bound is equivalent to maximizing the likelihood of a one-step improved policy on the offline dataset. In this way, we constrain the policy improvement direction to lie in the data manifold. The resulting algorithm simultaneously augments the policy evaluation and improvement by adding a mutual information regularization. MISA is a general offline RL framework that unifies conservative Q-learning (CQL) and behavior regularization methods (e.g., TD3+BC) as special cases. Our experiments show that MISA performs significantly better than existing methods and achieves new state-of-the-art on various tasks of the D4RL benchmark.\n\n1\n\nINTRODUCTION\n\nReinforcement learning (RL) has made remarkable achievements for solving sequential decisionmaking problems, ranging from game playing (Mnih et al., 2013; Silver et al., 2017; Berner et al., 2019) to robot control (Levine et al., 2016; Kahn et al., 2018; Savva et al., 2019). However, its success heavily relies on 1) an environment to interact with for data collection and 2) an online algorithm to improve the agent based only on its own trial-and-error experiences. These make RL algorithms incapable in real-world safety-sensitive scenarios where interactions with the environment are dangerous or prohibitively expensive, such as in autonomous driving and robot manipulation with human autonomy (Levine et al., 2020; Kumar et al., 2020). Therefore, offline RL is proposed to study the problem of learning decision-making agents from experiences that are previously collected from other agents when interacting with the environment is costly or not allowed.\n\nThough much demanded, extending RL algorithms to offline datasets is challenged by the distributional shift between the data-collecting policy and the learning policy. Specifically, a typical RL algorithm alternates between evaluating the Q values of a policy and improving the policy to have better cumulative return under the current value estimation. When it comes to the offline setting, policy improvement often involves querying out-of-distribution (OOD) state-action pairs that have never appeared in the dataset, for which the Q values are over-estimated due to extrapolation error of neural networks. As a result, the policy improvement direction is erroneously affected, eventually leading to catastrophic explosion of value estimations as well as policy collapse after error accumulation. Existing methods (Kumar et al., 2020; Wang et al., 2020; Fujimoto & Gu, 2021; Yu et al., 2021) tackle this problem by either forcing the learned policy to stay close to the behavior\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\npolicy (Fujimoto et al., 2019; Wu et al., 2019; Fujimoto & Gu, 2021) or generating low value estimations for OOD actions (Nachum et al., 2017; Kumar et al., 2020; Yu et al., 2021). Though these methods are effective at alleviating the distributional shift problem of the learning policy, the improved policy is unconstrained and might still deviate from the data distribution. A natural question thus arises: can we directly constrain the policy improvement direction to lie in the data manifold?\n\nIn this paper, we step back and consider the offline dataset from a new perspective, i.e., the Mutual Information between States and Actions (MISA). By viewing state and action as two random variables, the mutual information represents the reduction of uncertainty of actions given certain states, a.k.a., information gain in information theory (Nowozin, 2012). Therefore, mutual information is an appealing metric to sufficiently acquire knowledge from a dataset and characterize a behavior policy. We for the first time introduce it into offline RL as an regularization that directly constrains the policy improvement direction. Specifically, to allow practical optimizations of state-action mutual information estimation, we introduce the MISA lower bound of state-action pairs, which connects mutual information with RL by treating a parameterized policy as a variational distribution and the Q-values as the energy functions. We show that this lower bound can be interpreted as the likelihood of a non-parametric policy on the offline dataset, which actually represents the one-step improvement of the current policy based on the current value estimation. Maximizing MISA lower bound is equivalent to directly regularizing the policy improvement within the dataset manifold. However, the constructed lower bound involves integration over a self-normalized energy-based distribution, whose gradient estimation is intractable. To alleviate this dilemma, Markov Chain Monte Carlo (MCMC) estimation is adopted to produce an unbiased gradient estimation for MISA lower bound.\n\nTheoretically, MISA is a general framework for offline RL that unifies several existing offline RL paradigms including behavior regularization and conservative learning. As examples, we show that TD3+BC (Fujimoto & Gu, 2021) and CQL (Kumar et al., 2020) are degenerated cases of MISA. In our experiments, we demonstrate that MISA achieves significantly better performance on various environments of the D4RL (Fu et al., 2020) benchmark than the state-of-the-art methods. Additional ablation studies, visualizations, and limitations are discussed to better understand the proposed method. Our code will be released upon publication.\n\n2 RELATED WORKS\n\nOffline Reinforcement Learning The most critical challenge for extending an off-policy RL algorithm to an offline setup is the distribution shift between the behavior policy, i.e., the policy for data collection, and the learning policy. To tackle this challenge, most of the offline RL algorithms consider a conservative learning framework. They either regularize the learning policy to stay close to the behavior policy (Fujimoto et al., 2019; Wu et al., 2019; Fujimoto & Gu, 2021; Siegel et al., 2020; Wang et al., 2020), or force Q values to be low for OOD state-action pairs (Nachum et al., 2017; Kumar et al., 2020; Yu et al., 2021). For example, TD3+BC (Fujimoto & Gu, 2021) adds an additional behavior cloning (BC) signal along with the TD3 (Fujimoto et al., 2018), which encourages the policy to stay in the data manifold; CQL (Kumar et al., 2020), from the Q-value perspective, penalizes the OOD state-action pairs for generating high Q-value estimations and learns a lower bound of the true value function. However, their policy improvement direction is unconstrained and might deviate from the data distribution. On the other hand, SARSA-style updates (Sutton & Barto, 2018) are considered to only query in-distribution state-action pairs (Peng et al., 2019; Kostrikov et al., 2022). Nevertheless, without explicitly querying Bellman’s optimality equation, they limit the policy from producing unseen actions. Our proposed MISA follows the conservative framework and directly regularizes the policy improvement direction to lie within the data manifold with mutual information, which more fully exploits the dataset information while learning a conservative policy.\n\nMutual Information Estimation. Mutual information is a fundamental quantity in information theory, statistics, and machine learning. However, direct computation of mutual information is intractable as it involves computing a log partition function of a high dimensional variable. Thus, how to estimate the mutual information I(x, z) between random variables X and Z, accurately and efficiently, is a critical issue. One straightforward lower bound for mutual information estimation is Barber-Agakov bound (Barber & Agakov, 2004), which introduces an additional variational distribution q(z | x) to approximate the unknown posterior p(z | x). Instead of using an explicit “decoder” q(z | x), we can use unnormalized distributions for the variational family\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nq(z | x) (Donsker & Varadhan, 1975; Belghazi et al., 2018; Oord et al., 2018), i.e., approximate the distribution as q(z | x) = p(z)ef (x,z) Ep(z)[ef (x,z)] , where f (x, z) is an arbitrary critic function. As an example, InfoNCE (Oord et al., 2018) has been widely used in representation learning literature (Oord et al., 2018; He et al., 2020; Chen et al., 2020). To further improve the mutual information estimation, a combination of normalized and unnormalized variational distribution family can be considered (Brekelmans et al., 2022; Poole et al., 2019). Our MISA connects mutual information estimation with RL by parameterizing a tractable lower bound with a policy network as a variational distribution and the Q values as critics. In this way, MISA explicitly regularizes the policy improvement direction to lie in the data manifold and produces strong empirical performance.\n\n3 PRELIMINARIES\n\nReinforcement Learning We consider a Markov Decision Process (MDP) denoted as a tuple M = (S, A, p0(s), p(s′ | s, a), r(s, a), γ), where S is the state space, A is the action space, p0(s) is the initial state distribution, p(s′ | s, a) is the transition function, r(s, a) is the reward function, and γ is the discount factor. The target of a learning agent is to find a policy π∗(a | s) that maximizes the accumulative reward by interacting with the environment\n\nπ∗ = arg max\n\nπ\n\nEπ\n\n(cid:34) ∞ (cid:88)\n\nt=0\n\nγtr(st, at) | s0 ∼ p0(s), at ∼ π(a | st)\n\n.\n\n(1)\n\n(cid:35)\n\nQ-learning is a set of off-policy RL algorithms that utilize the optimal Bellman’s optimality operator B∗Q(s, a) = r(s, a) + γEs′∼p(s′|s,a)[maxa′ Q(s′, a′)] to learn a Q function. Differently, Bellman’s expectation operator BπQ(s, a) = r(s, a) + γEs′∼p(s′|s,a);a′∼π(·|s′)[Q(s′, a′)] gives an actor-critic framework that alternates between policy evaluation and policy improvement. Consider a value network Qφ(s, a) parameterized by φ and a policy network πθ(a|s) parameterzied by θ. Let μπ(s) denote the stationary distribution induced with policy π, which is also called occupancy measure (Schulman et al., 2015). Given the current policy, the policy evaluation aims to learn a Q network that can accurately predict its values minimizing Eμπθ (s)πθ(a|s)[(Qφ(s, a) − Bπθ Qφ(s, a))2]. Policy improvement focuses on learning the optimal policy that maximizes Eμπ(s)π(a|s)[Qφ(s, a)]. In practical implementations, the Bellman operator is usually replaced with its sample-based version ˆB, and the expectation over μπ(s)π(a|s) is approximated by an online replay buffer or an offline dataset D.\n\nNevertheless, as it is unavoidable to query the OOD actions when performing the maximization over actions, an inaccurate over-estimation of Q value will be selected and the error will accumulate during the Bellman’s update. Conservative RL methods, in turn, aim to perform “conservative” updates of the value / policy function during optimization by constraining the updates on only the in-distribution samples, which eventually minimizes the negative impact of OOD actions.\n\nKL Divergence Given two probability distributions p(x) and q(x) on the same probability space, the KL divergence (i.e., relative entropy) from q to p is given by DKL(p||q) = Ep(x) 0. The minimum value is achieved when the two densities are identical. We consider two dual representations that result in tractable estimators for the KL divergence. Lemma 3.1 (f -divergence representation (Nowozin et al., 2016)). The KL divergence admits the following lower bound:\n\n(cid:104) log p(x)\n\nq(x)\n\n≥\n\n(cid:105)\n\nDKL(p||q) ≥ sup T ∈F\n\nEp(x) [T (x)] − Eq(x)[eT (x)−1],\n\n(2)\n\nwhere the supremum is taken over a function family F satisifying the intergrability constraints.\n\nLemma 3.2 (Donsker-Varadhan representation (Nguyen et al., 2010)). The KL divergence has the lower bound:\n\nDKL(p||q) ≥ sup T ∈F\n\nEp(x) [T (x)] − log(Eq(x)[eT (x)]),\n\n(3)\n\nwhere the supremum is taken over a function family F satisifying the intergrability constraints.\n\nThe above two bounds are tight for sufficiently large families F.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n4 MUTUAL INFORMATION REGULARIZED OFFLINE RL\n\nIn this paper, we propose to think the offline RL problem from the perspective of mutual information and develop a novel framework (MISA) by estimating the Mutual Information between States and Actions of a given offline dataset. We show that MISA is a general framework which unifies multiple existing offline RL algorithms as special cases, including standard behavior cloning, TD3+BC (Fujimoto & Gu, 2021), and CQL (Kumar et al., 2020).\n\n4.1 MUTUAL INFORMATION REGULARIZATION\n\nConsider the state S and action A as two random variables. Let p(S,A)(s, a) denote the joint distribution of state-action pairs, and pS(s), pA(a) be the marginal distributions. The subscripts are omitted in the following for simplicity. The mutual information between S and A is defined with:\n\nI(S; A) = Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\np(s, a) p(s)p(a)\n\n= Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\np(a | s) p(a)\n\n= H(A) − H(A | S),\n\n(4)\n\nwhere H is Shannon entropy, and H(A|S) is conditional entropy of A given S. The higher mutual information between S and A means the lower uncertainty in A given state S. This coincides with the observation that the actions selected by a well-performing agent are usually coupled with certain states. Therefore, given a joint distribution of state-action pairs induced from a (sub-optimal) behavior agent, it is natural to learn a policy that can recover the dependence between states and actions produced by the behavior agent. By regularizing the agent with I(S; A) estimation, we encourage the agent to 1) perform policy update within the dataset distribution and 2) avoid being over-conservative and make sufficient use of the dataset information.\n\nLet πβ(a|s) represent a behavior policy and pβ(s, a) be the joint distribution of state-action pairs induced by πβ. Calculating the mutual information is often intractable as accessing to pβ(s, a) is infeasible. Fortunately, in the problem of offline reinforcement learning, a dataset D = {(st, at, rt, st+1)} of transitions is given by drawing samples independently from pβ(s, a). This dataset can thus be seen as a sample-based empirical joint distribution pD(s, a) for pβ. Let I(θ, φ) denote a mutual information lower bound that relies on parameterized functions with parameters θ and φ1, which are usually the policy network and Q network in the context of RL. We defer the derivation of such bounds in Sec. 4.2. Based on the above motivation, we aim at learning a policy that can approximate the mutual information of the dataset while being optimized to get the best possible cumulative return. We focus on the actor-critic framework, and formulate the offline RL problem with mutual information reguralization as follows:\n\nEs,a,s′∼D\n\n(cid:20) 1 2\n\n(Qφ(s, a) − Bπθ Qφ(s, a))2\n\n(cid:21)\n\nEs∼D,a∼πθ(a|s) [Qφ(s, a)] + α2 ˆID(θ, φ),\n\nmin φ\n\nmax θ\n\n− α1 ˆID(θ, φ),\n\n(Policy Evaluation)\n\n(Policy Improvement)\n\n(5)\n\n(6)\n\nwhere α1 and α2 are the coefficients to balance RL objective and mutual information objective, and ˆID(θ, φ) denotes the sample-based version of I(θ, φ) estimated from dataset D.\n\n4.2 STATE-ACTION MUTUAL INFORMATION ESTIMATION\n\nIn this section, we develop practical solutions to approximate the mutual information I(S; A) from samples of the joint distribution. We use the learning policy πθ(a|s) as a variational variable and Eqn. 4 can be rewritten as:\n\nI(S; A) = Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s)p(a|s) p(a)πθ(a|s)\n\n= Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s) p(a)\n\n+ DKL (p(s, a)||p(s)πθ(a|s)) ,\n\nwhere p(s)πθ(a|s) is an induced joint distribution. Let IBA ≜ Ep(s,a) I(S; A) ≥ IBA as the KL divergence is always non-negative. This is exactly the Barber-Agakov (BA) lower bound developed by (Barber & Agakov, 2004).\n\n. We have\n\np(a)\n\n(cid:104) log πθ(a|s)\n\n(cid:105)\n\n(7)\n\n1Note some lower bounds might only have one parameterized function.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nTo obtain tighter bounds, we turn to KL dual representations of DKL (p(s, a)||p(s)πθ(a|s)) in Eqn. 7. To this end, we choose F to be a set of parameterized functions Tφ : S × A → R, φ ∈ Φ, which can be seen as an energy function.\n\nWith the f -divergence dual representation, we derive MISA-f as\n\nIMISA-f ≜ Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s) p(a)\n\n+ Ep(s,a) [Tφ(s, a)] − Ep(s)πθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n.\n\n(8)\n\nThe IMISA-f bound is tight when p(a|s) ∝ πθ(a|s)eTφ(s,a)−1. Similarly, using the DV representation in Theorem 3.2, we can have another bound IMISA-DV ≤ I(S; A), as shown below: (cid:21) (cid:104)\n\n(cid:20)\n\nIMISA-DV ≜ Ep(s,a)\n\nlog\n\n+ Ep(s,a) [Tφ(s, a)] − log Ep(s)πθ(a|s)\n\neTφ(s,a)(cid:105)\n\n,\n\n(9)\n\nπθ(a|s) p(a)\n\nwhich is tight when p(a|s) = 1\n\nZ p(s)πθ(a|s)eTφ(s,a), where Z = Ep(s)πθ(a|s)\n\n(cid:2)eTφ(s,a)(cid:3).\n\nWe observe that the KL term in Eqn. 7 can be rewritten as:\n\nDKL (p(s, a)||p(s)πθ(a|s)) = Ep(s)\n\n(cid:20)\n\nEp(a|s)\n\n(cid:20)\n\nlog\n\n(cid:21)(cid:21)\n\np(a|s) πθ(a|s)\n\n= Ep(s) [DKL(p(a|s)||πθ(a|s))] .\n\nApplying the DV representation of DKL(p(a|s)||πθ(a|s)), we can have a new lower bound IMISA:\n\nIMISA ≜ Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s) p(a)\n\n+ Ep(s,a) [Tφ(s, a)] − Ep(s) log Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)(cid:105)\n\n.\n\n(10)\n\nThe bound is tight when p(a|s) = 1\n\nZ(s) πθ(a|s)eTφ(s,a), where Z(s) = Eπθ(a|s)[eTφ(s,a)].\n\nTheorem 4.1. Given the joint distribution of state s and action a, the lower bounds of mutual information I(S; A) defined in Eqn. 8-10 have the following relations:\n\nI(S; A) ≥ IMISA ≥ IMISA-DV ≥ IMISA-f .\n\n(11)\n\nThe proof is deferred to the appendix due to space limit.\n\nAlgorithm 1 Mutual Information Regularized Offline RL Input: Initialize Q network Qφ, policy network πθ, dataset D, hyperparameters α1 and α2. for t ∈ {1, . . . , MAX STEP} do\n\nTrain the Q network by gradient descent with objective JQ(φ) in Eqn. 12: φ := φ − ηQ∇φJQ(φ) Improve policy network by gradient ascent with object Jπ(θ) in Eqn. 13: θ := θ + ηπ∇θEs∼D,a∼πθ(a|s)[Qφ(s, a)] + α2∇θIMISA\n\nend Output: The well-trained πθ.\n\n4.3\n\nINTEGRATION WITH OFFLINE REINFORCEMENT LEARNING\n\nWe now describe how our MISA lower bound is integrated into the above framework (Eqn. 12-13) to give a practical offline RL algorithm. We propose to use a Q network Qφ(s, a) as the energy function Tφ(s, a) , and use pD(s, a) as the joint distribution in Eqn. 10. Then we have the following objective to learn a Q-network during policy evaluation:\n\nJQ(φ) = J B\n\nQ(φ) − γ1\n\n(cid:16)\n\nEs,a∼D [Qφ(s, a)] − Es∼D 2 (Qφ(s, a) − Bπθ Qφ(s, a))2(cid:105) (cid:104) 1\n\nwhere J B\n\nQ(φ) = Es,a,s′∼D\n\n(cid:104)\n\nlog Eπθ(a|s)\n\n(cid:104)\n\neQφ(s,a)(cid:105)(cid:105)(cid:17)\n\n,\n\n(12)\n\nrepresents the TD error. For policy im-\n\nporvement, note that the entropy term H(a) in Eqn. 10 can be omitted as it is a constant given dataset D. Thus, we have the below objective to maximize:\n\nJπ(θ) = Es∼D,a∼πθ(a|s) [Qφ(s, a)]+γ2\n\n(cid:16)\n\nEs,a∼D[log πθ(a|s)] − Es∼D\n\n(cid:104)\n\nlog Eπθ(a|s)\n\n(cid:104)\n\neQφ(s,a)(cid:105)(cid:105)(cid:17)\n\n.\n\n(13) The formulations for other regularizers (e.g., IMISA-DV and IMISA-f ) can be derived similarly. A detailed description of the MISA algorithm for offline RL can be found in Algo. 1.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nIntuitive Explanation on the Mutual Information Regularizer. By rearranging the terms in Eqn. 10, MISA can be written as:\n\n(cid:34)\n\nIMISA = Es,a∼D\n\nlog\n\nπθ(a | s)eQφ(s,a)\n\nEπθ(a′|s)\n\n(cid:2)eQφ(s,a′)(cid:3)\n\n(cid:35)\n\n,\n\n(14)\n\nwhere the log term can be seen as the log probability of a one-step improved policy. More specifically, for policy improvement with KL divergence reguralization: maxπ Es∼D,a∼π[Qφ(s, a)] + θ,φ ∝ πθ(a|s)eQφ(s,a) (Abdolmaleki et al., 2018; DKL(π||πθ), the optimal solution is given by π∗ θ,φ as IMISA = Es,a∼D[log π∗ Peng et al., 2019). Therefore, IMISA is rewritten with π∗ θ,φ(a|s)], and maximizing it means maximizing the log likelihood of the dataset using the improved policy. In other words, instead of directly fitting the policy on the dataset, which is short-sighted, this objective considers the optimization direction of the policy improvement step. Given the current policy and policy evaluation results, it first computes the analytic improved policy, and then forces the dataset likelihood to be maximized using the improved policy. In this way, even if an out-of-distribution state-action pair get an overestimated q value, IMISA is going to suppress this value and make sure in-distribution data have relatively higher value estimation.\n\nUnbiased Gradient Estimation For policy improvement with Eqn. 13, differentiating through (cid:2)eQφ(s,a))(cid:3). For a Gaussian a sampling distribution πθ(a | s) is required for Es∼D log Eπθ(a|s) policy πθ(a | s) = N (μθ, σθ), one could consider the reparameterization trick (Kingma & Welling, (cid:2)eQφ(s,μθ+ε∗σθ)(cid:3). However, this introduces 2014) and convert the objective as Es∼D log Eε∼N (0,I) high variance in offline reinforcement learning setups because we condition the policy improvement directly on the Q values of the out-of-distribution actions, which eventually gives a noisy policy. Hence, we aim to minimize the influence of Q values for policy improvement.\n\nDifferentiating Eqn. 10 with respect to policy parameters θ, we have\n\n∂IMISA ∂θ\n\n= Es,a∼D\n\n(cid:20) log πθ(a | s) ∂θ\n\n(cid:21)\n\n− Es∼D,a∼pθ,φ(a|s)\n\n(cid:20) log πθ(a | s) ∂θ\n\n(cid:21)\n\n(15)\n\nwhere pθ,φ(a | s) = πθ(a|s)eQφ(s,a)) Eπθ (a|s)[eQφ (s,a))] derivation. By optimizing Eqn. 15, we obtain an unbiased gradient estimation of the MISA objective with respect to the policy parameters, while minimizing the negative effects of the Q values of OOD actions. To sample from pθ,φ(a | s), one can consider Markov-Chain Monte-Carlo (MCMC) methods, e.g., Hamiltonian Monte Carlo (Betancourt, 2017).\n\nis a self-normalized distribution. See appendix A.2 for a\n\n4.4 CONNECTIONS TO EXISTING OFFLINE RL METHODS\n\nWe show that some existing offline RL methods can be viewed as special cases of MISA framework.\n\nBehavior Cloning and BC Regularized RL We first show that behavior cloning is a form of mutual information regularizer. As shown by Eqn. 7, IBA ≜ Es,a∼D of mutual information. Since H(a) is a consistent given datasets, maximizing IBA is equivalent to maximizing Es,a∼D [log πθ(a|s)], which is exactly the objective for behavior cloning.\n\ngives a lower bound\n\nlog πθ(a|s)\n\np(a)\n\n(cid:104)\n\n(cid:105)\n\nAs for TD3+BC (Fujimoto & Gu, 2021), the policy evaluation is unchanged, while the policy improvement objective is augmented by an MSE regularization term, i.e., Es∼D[Q(s, πθ(s))] − (cid:2)(πθ(s) − a)2(cid:3), where λ is a hyperparameter. Maximizing the negative MSE term is equivγEs,a∼D alent to maximizing Es,a∼D [log pπθ (a|s)], where pπθ = Ce− 1 is a Gaussian distribution, and C is a constant. This is a special case of Eqn. 13 when we remove the last log-mean-exp term.\n\n2 (πθ(s)−a)2\n\nConservation Q Learning CQL (Kumar et al., 2020) was proposed to alleviate the overestimation issue of Q learning by making conservative updates to the Q values during policy evaluation. The policy improvement is kept unchanged compared to standard Q learning. We focus on the entropy-regularized policy evaluation of CQL as below:\n\nmin φ\n\nJ B\n\nQ(φ) − γ1Es∼D\n\nEa∼πD(a|s)[Qφ(s, a)] − log\n\n(cid:34)\n\n(cid:35)\n\neQφ(s,a)\n\n,\n\n(cid:88)\n\na\n\n(16)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nBC\n\n10%BC\n\nDT\n\nAWAC\n\nOneStep RL\n\nTD3+BC\n\nCQL\n\nIQL\n\nMISA\n\nhalfcheetah-medium-v2 hopper-medium-v2 walker2d-medium-v2 halfcheetah-medium-replay-v2 hopper-medium-replay-v2 walker2d-medium-replay-v2 halfcheetah-medium-expert-v2 hopper-medium-expert-v2 walker2d-medium-expert-v2\n\ngym-locomotion-v2 (total)\n\nkitchen-complete-v0 kitchen-partial-v0 kitchen-mixed-v0\n\nkitchen-v0 (total)\n\npen-human-v0 hammer-human-v0 door-human-v0 relocate-human-v0 pen-cloned-v0 hammer-cloned-v0 door-cloned-v0 relocate-cloned-v0\n\n42.6 52.9 75.3 36.6 18.1 26 55.2 52.5 107.5\n\n466.7\n\n65 38 51.5\n\n154.5\n\n63.9 1.2 2\n0.1 37 0.6 0\n-0.3\n\nadroit-v0 (human+cloned)\n\n104.5\n\n42.5 56.9 75 40.6 75.9 62.5 92.9 110.9 109\n\n666.2\n\n42.6 67.6 74 36.6 82.7 66.6 86.8 107.6 108.1\n\n672.6\n\n43.5 57 72.4 40.5 37.2 27 42.8 55.8 74.5\n\n450.7\n\n- -\n-\n\n-\n\n- -\n- -\n- -\n- -\n\n0\n\n- -\n-\n\n-\n\n- -\n- -\n- -\n- -\n\n0\n\n- -\n-\n\n-\n\n- -\n- -\n- -\n- -\n\n0\n\nantmaze-umaze-v0 antmaze-umaze-diverse-v0 antmaze-medium-play-v0 antmaze-medium-diverse-v0 antmaze-large-play-v0 antmaze-large-diverse-v0\n\n54.6 45.6 0\n0 0\n0\n\n62.8 50.2 5.4 9.8 0\n6\n\n59.2 53 0\n0 0\n0\n\n56.7 49.3 0\n0.7 0\n1\n\n48.4 59.6 81.8 38.1 97.5 49.5 93.4 103.3 113\n\n684.6\n\n- -\n-\n\n-\n\n- -\n- -\n60 2.1 0.4 -0.1\n\n62.4\n\n64.3 60.7 0.3 0\n0 0\n\n48.3 59.3 83.7 44.6 60.9 81.8 90.7 98 110.1\n\n677.4\n\n- -\n-\n\n-\n\n- -\n- -\n- -\n- -\n\n0\n\n78.6 71.4 10.6 3\n0.2 0\n\n44 58.5 72.5 45.5 95 77.2 91.6 105.4 108.8\n\n47.4 66.3 78.3 44.2 94.7 73.9 86.7 91.5 109.6\n\n47.4 67.1 84.1 45.6 98.6 86.2 94.7 109.8 109.4\n\n698.5\n\n692.6\n\n742.9\n\n43.8 49.8 51\n\n62.5 46.3 51\n\n70.2 45.7 56.6\n\n144.6\n\n159.8\n\n172.5\n\n37.5 4.4 9.9 0.2 39.2 2.1 0.4 -0.1\n\n93.6\n\n74 84 61.2 53.7 15.8 14.9\n\n71.5 1.4 4.3 0.1 37.3 2.1 1.6 -0.2\n\n88.1 8.1 5.2 0.1 58.6 2.2 0.5 -0.1\n\n118.1\n\n162.7\n\n87.5 62.2 71.2 70 39.6 47.5\n\n378\n\n92.3 89.1 63 62.8 17.5 23.4\n\n348.1\n\nantmaze-v0 (total)\n\n100.2\n\n134.2\n\n112.2\n\n107.7\n\n125.3\n\n163.8\n\n303.6\n\nTable 1: Average normalized score on the D4RL benchmark. Results of baselines are taken directly from (Kostrikov et al., 2022).\n\nwhere we highlight the main difference between it and our MISA policy evaluation (Eqn. 12) in blue. Let πU(a|s) denote a uniform distribution of actions and |A| is the number of actions. The log-sum-exp term can be written as log Ea∼πU(a|s)[Qφ(s, a)] + log |A|. Substituting it into Eqn. 16 and discarding the constant log |A|, we recover the the formulation in Eqn. 12. Therefore, CQL is actually doing mutual information regularization during policy evaluation. The key difference is that it is not using the current policy network as the variational distribution. Instead, a manually designed distribution is used in CQL. However, a uniform policy is usually suboptimal in environments with continuous actions. CQL thus constructs a mixed variational policy by drawing samples drawn from the current policy network, a uniform distribution and the dataset. In our formulation, the variational distribution will be optimized to give a better mutual information estimation. This might explain why MISA is able to give better performance than CQL.\n\n5 EXPERIMENTS\n\nWe perform extensive experiments on various tasks of the D4RL benchmark (Fu et al., 2020) to demonstrate effectiveness of the proposed method MISA. To provide better understandings of MISA, we provide additional ablation studies, visualizations, and discussions on the limitations.\n\n5.1 OFFLINE REINFORCEMENT LEARNING ON D4RL BENCHMARKS\n\nExperiment Setups. For all D4RL environments, we follow the network architectures of CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2022), where a neural network of 2 encoding layers of size 256 is used, followed by an output layer. We use ELU activation function (Clevert et al., 2015) (cid:2)eTψ(s,a)(cid:3), and SAC (Haarnoja et al., 2018) as the base RL algorithm. When approximating Eπθ(a|s) we use 50 Monte-Carlo samples. In addition, for unbiased gradient estimation with MCMC samples, we use a burn-in steps of 5. For all tasks, we average the mean returns over 10 evaluation trajectories and 5 random seeds. Detailed setups and hyper-parameters are in the appendix.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nDataset\n\nhalfcheetah-medium-v2 hopper-medium-v2 walker2d-medium-v2 halfcheetah-medium-replay-v2 hopper-medium-replay-v2 walker2d-medium-replay-v2 halfcheetah-medium-expert-v2 hopper-medium-expert-v2 walker2d-medium-expert-v2\n\nk=5\n\n47.1 62.2 83.3 45.4 79.9 83.7 94.5 105.7 109.2\n\nk=20\n\n46.9 65.3 83.9 45.3 88.4 86.9 92.8 102.7 109.4\n\nBI=1\n\nno BA\n\n47.2 61.8 81.8 45.2 72.9 82.8 92.8 93.4 109.3\n\n49.1 64.4 83.8 46.5 100.3 86.1 87.1 89.6 108.1\n\nBA\n\n56.3 1.2 7.5 52.4 56.4 51.1 26.8 1.3 1.4\n\ngym-locomotion-v2 (total)\n\n711\n\n721.6\n\n687.2\n\n715\n\n254.4\n\nMISA-f\n\nMISA-DV\n\nMISA-biased\n\nMISA\n\n43.5 60.5 73.2 39.8 34.8 34.9 57.6 57.7 102.7\n\n504.7\n\n45.5 61.6 82.8 43.8 45.9 81.4 92.4 111.5 108.8\n\n673.7\n\n48.4 65.7 84.2 46.9 98.1 80.6 84.6 103.2 109.2\n\n720.9\n\n47.4 67.1 84.1 45.6 98.6 86.2 94.7 109.8 109.4\n\n742.9\n\nTable 2: Ablation studies on gym-locomotion-v2. k denotes the number of Monte-Carlo samples (cid:2)eTψ(s,a)(cid:3), BI represents the burnin-steps for MCMC simulation, and BA for estimating Eπθ(a|s) denotes the use of Barber-Agakov Bound. In addition, MISA-x denotes different variants of MISA.\n\nGym Locomotion Tasks. We first evaluate MISA on the standard MuJoCo-style continuous control tasks, reported as gym-locomotion-v2 in Table 1. We observe that MISA improves the performance of baselines by a large margin. Specifically, MISA is less sensitive to the characteristics of data distributions. The medium datasets include trajectories collected by an SAC agent trained to reach 1/3 of the performance of an expert; the medium-replay datasets contain all data samples of the replay buffer during the training of the medium SAC agent, which covers the noisy exploration process of the medium agent. We can observe that prior methods are generally sensitive to the noisy sub-optimal data in medium and medium-replay environments, while MISA outperforms them by a large margin. In particular, MISA achieves near-expert performance on walker2d-mediumreplay with only sub-optimal trajectories. This indicates that by regularizing the policy and Qvalues within the mutual information of the dataset, we can fully exploit the data and perform safe and accurate policy improvement during RL. Moreover, on medium-expert environments, where the datasets are mixtures of medium agents and experts, MISA successfully captures the multi-modality of the datasets and allows further improvements of the policy over baselines.\n\nAdroit Tasks. According to (Fujimoto & Gu, 2021), adroit tasks require strong policy regularization to overcome the extrapolation error, because the datasets are either generated by human (adroithuman-v0), which would show a narrow policy distribution, or a mixture of human demonstrations and a behavior cloning policy (adroit-cloned-v0). We observe that MISA provides a stronger regularization and significantly outperforms the baselines on adroit domains.\n\nKitchen Tasks. An episode of Kitchen environment consists of multiple sub-tasks that can be mixed in an arbitrary order. We observe that MISA outperforms baselines on both kitchen-complete-v0 and kitchen-mixed-v0, while achieving slightly worse performance on kitchen-partial-v0. Specifically, on kitchen-mixed, the result is consistent with our assumption that by better regularizing the policy, MISA guarantees a safer and in-distribution policy improvement step in offline RL.\n\nAntmaze Tasks. On the challenging AntMaze domain with sparse delayed reward, we observe that MISA generally outperforms CQL and achieves the best performance on umaze environments. However, MISA performs worse than IQL on the challenging large environments. Multi-step value update is often necessary for learning a robust value estimation in these scenarios (Kostrikov et al., 2022) while MISA adopts a single-step SAC for the base RL algorithm.\n\n5.2 ABLATION STUDIES\n\nTo better understand MISA, we conduct extensive ablation studies on each component (Table 2).\n\nMISA requires careful Monte-Carlo approximation. Firstly, we vary the number (k) of Monte-Carlo (cid:2)eTψ(s,a)(cid:3) and reduce the burn-in steps of MCMC sampling samples for approximating Eπθ(a|s) process. Both operations would introduce additional Monte-Carlo approximation errors to MISA. Comparing k = 5, k = 20, and MISA (k = 50), the performance increases monotonically; comparing MISA (BI=5) with BI=1, we can observe a sharp performance drop. We then conclude that MISA requires careful Monte-Carlo approximation for good performance.\n\nAccurately estimating the mutual information I(S; A) is critical for offline RL. In Table 2, BA stands for the Barber-Adakov Bound, while no BA stands for removing the Barber-Agakov term in Eqn. 10, which gives an inaccurate estimation (neither an upper bound nor a lower bound) to I(S; A). We\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nobserve a clear performance drop when comparing them with MISA. In addition, as discussed in Sect. 4.2, considering the tightness of the various lower bound, we have BA ≤ MISA-f ≤ MISADV ≤ MISA. Empirically, in Table 2, we observe that the overall performance of these four variants is consistent with the tightness of the bounds. This suggests that accurately estimating the I(S; A) is crucial to offline RL and tighter bounds often give better performance.\n\nUnbiased gradient estimations improves performance of MISA. Lastly, we study the importance of unbiased estimation discussed in Sect. 4.3. MISA-biased ignores the bias correction term in Eqn. 15. Although MISA-biased outperforms the baselines, it still performs worse than MISA. This suggests that by correcting the gradient estimation with additional MCMC samples, MISA achieves a better regularized policy learning in offline RL.\n\n5.3 VISUALIZATION OF EMBEDDING\n\nIn Fig. 1, we visualize the embeddings before the output layer of Q-value networks, given different mutual information bounds (BA and MISA). We select a subset from walker2dmedium-v2 dataset to study the division of low reward (blue) and high reward (red) (s, a) pairs. We color each point by the reward r(s, a). As discussed in Sect. 4.2, BA gives a lowest bound for mutual information estimation and MISA produces the tightest bound. In Fig. 1, we observe a consistent result. The embeddings of BA converge to a set of regular curves and fail to cluster the high r(s, a), because Q-values have converged to indistinguishably high values (3 × 1012) for all (s, a) pairs. In contrast, MISA successfully learns to cluster the (s, a) pairs with a high reward into a cluster. From this perspective, we claim that regularizing the mutual information encourages learning a robust representation in offline RL scenarios.\n\nFigure 1: tSNE of the Q-value network embeddings of walker2d-medium-v2 dataset, where red color denote high reward and blue color denote low reward.\n\n(b) MISA\n\n(a) BA\n\n5.4 LIMITATIONS\n\nAlthough MISA achieves great performance on several benchmarks as reported in Table 1, we have made the assumption that high mutual information comes from the stationary policy of a well-behaving agent. This will prevent MISA from being applied to tasks with extremely low-quality data, e.g., a random policy whose I(S; A) is near zero. We validate this limitation by running the locomotion-randomv2 datasets of D4RL benchmark. The results are presented in Table 3. We observe that on datasets generated by random policies, MISA achieves worse performance than both CQL and IQL.\n\nhalfcheetah-random-v2 hopper-random-v2 walker2d-random-v2\n\nTable 3: Results on locomotion-random-v2\n\n35.4 10.8 7\n\n20.5 7.8 8.9\n\nIQL MISA\n\n8.6 7.3 2.2\n\nDataset\n\nCQL\n\n6 CONCLUSIONS\n\nWe present the MISA framework for offline reinforcement learning by directly regularizing policy improvement and policy evaluation with the mutual information between state-action pairs of the dataset. MISA connects mutual information estimation with RL by constructing tractable lower bounds, treating the learning policy as a variational distribution and Q values as energy functions. The resulting tractable lower bound resembles a non-parametric energy-based distribution, which can be interpreted as the likelihood of a one-step improved policy given current value estimation. In this way, MISA can constrain the policy improvement within the dataset manifold. In our experiments, MISA significantly outperforms the state-of-the-art methods on D4RL benchmark. However, MISA assumes a high correspondence between states and actions, which might fail on uncorrelated data generated by a random policy. We leave it for future study.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nAs discussed in the paper, MISA framework provides a simple yet effective approach to policy learning from offline datasets. Although the results presented in this paper only consider simulated environments, given the generality of MISA, it could be potentially effective on learning real-robot policies in more complex environments. We should be cautious about the misuse of the method proposed. Depending on the specific application scenarios, it might be harmful to domestic privacy and safety.\n\nREPRODUCIBILITY STATEMENT\n\nIn this paper, all experiments are averaged over 5 random seeds for the stability and reliability of the results. Our code is attached in the supplementary materials. We promise to clean and release the code upon publication.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAbbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, R ́emi Munos, Nicolas Heess, and Martin A. Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learning Representations, 2018.\n\nDavid Barber and Felix Agakov. The IM algorithm: a variational approach to information maxi-\n\nmization. Advances in neural information processing systems, 16(320):201, 2004.\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon Hjelm, and Aaron C. Courville. Mutual information neural estimation. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, 2018.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nMichael Betancourt. A conceptual introduction to hamiltonian monte carlo.\n\narXiv preprint\n\narXiv:1701.02434, 2017.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/google/jax.\n\nRob Brekelmans, Sicong Huang, Marzyeh Ghassemi, Greg Ver Steeg, Roger Baker Grosse, and Improving mutual information estimation with annealed and energy-based\n\nAlireza Makhzani. bounds. In International Conference on Learning Representations, 2022.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.\n\nAndrzej Cichocki and Shun-ichi Amari. Families of alpha-beta-and gamma-divergences: Flexible\n\nand robust measures of similarities. Entropy, 12(6):1532–1568, 2010.\n\nDjork-Arn ́e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\n\nlearning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\n\nMonroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time, i. Communications on Pure and Applied Mathematics, 28(1):1–47, 1975.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\n\nAdvances in Neural Information Processing Systems, 2021.\n\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-\n\ncritic methods. In International conference on machine learning, 2018.\n\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\n\nexploration. In International conference on machine learning, 2019.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, 2018.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Conference on Computer Vision and Pattern Recognition, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL http://github.com/google/flax.\n\nGregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, and Sergey Levine. Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation. In International Conference on Robotics and Automation, 2018.\n\nDiederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and\n\nYann LeCun (eds.), International Conference on Learning Representations, 2014.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-\n\nlearning. In International Conference on Learning Representations, 2022.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\n\nreinforcement learning. Advances in Neural Information Processing Systems, 2020.\n\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-\n\nmotor policies. The Journal of Machine Learning Research, 2016.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-\n\nrial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. Advances in neural information processing systems, 2017.\n\nXuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 2010.\n\nSebastian Nowozin. Improved information gain estimates for decision tree induction. arXiv preprint\n\narXiv:1206.4620, 2012.\n\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f -gan: Training generative neural samplers using variational divergence minimization. Advances in neural information processing systems, 29, 2016.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\n\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational In International Conference on Machine Learning, pp. 5171–\n\nbounds of mutual information. 5180. PMLR, 2019.\n\nManolis Savva, Jitendra Malik, Devi Parikh, Dhruv Batra, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, and Vladlen Koltun. Habitat: A platform for embodied ai research. In International Conference on Computer Vision, ICCV, 2019.\n\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\n\npolicy optimization. In International conference on machine learning, 2015.\n\nNoah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: In International Conference on Behavior modelling priors for offline reinforcement learning. Learning Representations, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 2017.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nZiyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances in Neural Information Processing Systems, 2020.\n\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\n\narXiv preprint arXiv:1911.11361, 2019.\n\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. Advances in Neural Information Processing Systems, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS AND DERIVATIONS\n\nA.1 PROOF FOR THEOREM 4.1\n\nWe first show IMISA, IMISA-DV and IMISA-f are lower bounds for mutual information I(S, A).\n\nLet μθ,φ(a|s) ≜ 1\n\nZ(s) πθ(a|s)eTφ(s,a), where Z(s) = Eπθ(a|s)[eTφ(s,a)], IMISA can be written as:\n\nIMISA ≜ Ep(s,a)\n\n= Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s) p(a)\n\n+ Ep(s,a) [Tφ(s, a)] − Ep(s) log Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)(cid:105)\n\n(cid:21)\n\np(a|s) p(a)\n\n− Ep(s,a)[log p(a|s)]\n\n(17)\n\n+ Ep(s,a)[log πθ(a|s)] + Ep(s,a) [Tφ(s, a)] − Ep(s)[log Z(s)]\n\n= I(S, A) − Ep(s) [DKL(p(a|s)||μθ,φ(a|s))] ≤ I(S, A).\n\nThe above inequality holds as the KL divergence is always non-negative.\n\nSimilarly, let μθ,φ(s, a) ≜ 1 can be written as:\n\nZ p(s)πθ(a|s)eTφ(s,a), where Z(s) = Ep(s)πθ(a|s)[eTφ(s,a)], IMISA-DV\n\nIMISA-DV ≜ Ep(s,a)\n\n= Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s) p(a)\n\n+ Ep(s,a) [Tφ(s, a)] − log Ep(s)πθ(a|s)\n\n(cid:104)\n\neTφ(s,a)(cid:105)\n\n(cid:21)\n\np(a|s) p(a)\n\n− Ep(s,a)[log p(a|s)]\n\n(18)\n\n+ Ep(s,a)[log πθ(a|s)] + Ep(s,a) [Tφ(s, a)] − log Z\n\n= I(S, A) − DKL(p(s, a)||μθ,φ(s, a)) ≤ I(S, A).\n\nThe above inequality holds as the KL divergence is always non-negative.\n\nConsider the generalized KL-divergence (Cichocki & Amari, 2010; Brekelmans et al., 2022) between two un-normalized distributions ̃p(x) and ̃q(x) defined by\n\nDGKL( ̃p(x)|| ̃q(x)) =\n\n(cid:90)\n\n ̃p(x) log\n\n ̃p(x) ̃q(x)\n\n− ̃p(x) + ̃q(x)dx,\n\n(19)\n\nwhich is always non-negative and reduces to KL divergence when ̃p and ̃q are normalized. Let ̃μθ,φ(a|s) ≜ πθ(a|s)eTφ(s,a)−1 denote an un-normalized policy. We can rewrite IMISA-f as\n\nIMISA-f ≜ Ep(s,a)\n\n= Ep(s,a)\n\n(cid:20)\n\nlog\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nπθ(a|s) p(a)\n\n+ Ep(s,a) [Tφ(s, a)] − Ep(s)πθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n(cid:21)\n\np(a|s) p(a)\n\n− Ep(s,a)[log p(a|s)]\n\n+ Ep(s,a)[log πθ(a|s)] + Ep(s,a) [Tφ(s, a) − 1] + 1 − Ep(s)πθ(a|s)\n\n= I(S, A) − Ep(s) [DGKL(p(a|s)|| ̃μθ,φ(a|s))] ≤ I(S, A).\n\n(20)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\nSo far, we have proven that IMISA, IMISA-DV and IMISA-f mutual information lower bounds. Then we are going to prove their relations by starting fromt he relation between IMISA and IMISA-DV.\n\nIMISA − IMISA-DV = DKL(p(s, a)||μθ,φ(s, a)) − Ep(s) [DKL(p(a|s)||μθ,φ(a|s))]\n\n= Ep(s)Ep(a|s)\n\n= Ep(s)Ep(a|s)\n\n(cid:20)\n\nlog\n\n(cid:20)\n\np(s, a) p(a|s)\n\n− log\n\n(cid:21)\n\nμθ,φ(s, a) μθ,φ(a|s) (cid:21)\n\np(s)Z(s)\n\n(21)\n\n1 Z\n\nlog p(s) − log\n\n= Ep(s)\n\n(cid:20)\n\nlog p(s) − log\n\n(cid:18)\n\n= DKL\n\np(s)||\n\n1 Z\n\n(cid:21)\n\np(s)Z(s)\n\n1 Z\n\n(cid:19)\n\np(s)Z(s)\n\n≥ 0,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nZ p(s)Z(s) is a self-normalized distribution as Z = Ep(s)[Z(s)]. Therefore, we have IMISA ≥\n\nwhere 1 IMISA-DV.\n\nSimilarly, the relation between IMISA-DV and IMISA-f is given by:\n\nIMISA-DV − IMISA-f = Ep(s) [DGKL(p(a|s)|| ̃μθ,φ(a|s))] − DKL(p(s, a)||μθ,φ(s, a))\n\n= Ep(s)Ep(a|s)\n\n= Ep(s)Ep(a|s)\n\n= Ep(s)Ep(a|s)\n\n= Ep(s,a)\n\n(cid:104)\n\nlog\n\n= Eμθ,φ(s,a)\n\np(a|s) p(s, a)\n\n− log\n\n(cid:20)\n\nlog\n\n(cid:20)\n\n− log p(s) − log\n\n(cid:20)\n\nlog\n\nμθ,φ(s, a) p(s) ̃μθ,φ(a|s)\n\n(cid:21)\n\n ̃μθ,φ(a|s) μθ,φ(s, a) (cid:21) ̃μθ,φ(a|s) μθ,φ(s, a) (cid:21)\n\n− 1 + Ep(s)Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n− 1 + Ep(s)Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n− Eμθ,φ(s,a)[1] + Ep(s)Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n(cid:105)\n\ne Z\n\n(cid:104)\n\nlog\n\n− Eμθ,φ(s,a)[1] + Ep(s)Eπθ(a|s) (cid:105) e\nZ\n\n− Eμθ,φ(s,a)[1] + Ep(s)Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n= Eμθ,φ(s,a)\n\n(cid:20)\n\nlog\n\n(cid:21)\n\nμθ,φ(s, a) p(s) ̃μθ,φ(a|s)\n\n− Eμθ,φ(s,a)[1] + Ep(s)Eπθ(a|s)\n\n(cid:104)\n\neTφ(s,a)−1(cid:105)\n\n= DGKL (μθ,φ(s, a)||p(s) ̃μθ,φ(a|s)) ≥ 0,\n\n(22) where p(s) ̃μθ,φ(a|s) is an unnormalized joint distribution. Therefore, we have I(S, A) ≥ IMISA ≥ IMISA-DV ≥ IMISA-f .\n\nA.2 DERIVATION OF MISA GRADIENTS\n\nWe detail how the unbiased gradient is derived in Sec.4.3.\n\n∂IMISA ∂θ\n\n= Es,a∼D\n\n= Es,a∼D\n\n= Es,a∼D\n\n(cid:20) log πθ(a | s) ∂θ\n\n(cid:21)\n\n(cid:20) log πθ(a | s) ∂θ (cid:20) log πθ(a | s) ∂θ\n\n(cid:21)\n\n(cid:21)\n\n− Es∼D\n\n(cid:34)\n\n(cid:34)\n\n∂ log Eπθ(a|s)[eQφ(s,a)] ∂θ\n\n(cid:35)\n\n− Es∼D\n\nEπθ(a|s)\n\n− Es∼D,a∼pθ,φ(a|s)\n\n(cid:34)\n\neQφ(s,a)\n\n(cid:2)eQφ(s,a)(cid:3) (cid:21)\n\nEπθ(a|s) (cid:20) log πθ(a | s) ∂θ\n\n(cid:35)(cid:35)\n\nlog πθ(a | s) ∂θ\n\n(23)\n\n(24)\n\nfor Eqn. 23, we use the log-derivative trick.\n\nB ADDITIONAL EXPERIMENTAL DETAILS\n\nFor gym-locomotion-v2, kitchen-v0, and adroit-v0 environments, we average the results over 10 evaluation episodes and 5 random seeds. Following (Kostrikov et al., 2022), we evaluate the antmaze-v0 environments for 100 episodes instead. To stabilize the training of our agents in antmaze-v0 environments, we follow (Kumar et al., 2020) and normalize the reward by r′ = (r − 0.5) ∗ 4.\n\nIn addition, for a fair comparison with baseline methods, we use the same network structure as used in CQL (Kumar et al., 2020), where a network with embedding layers of sizes (256, 256, 256) is used for antmaze-v0 environments, and embedding layers of sizes (256, 256) is used for other tasks. ELU activation is used after each layer (Clevert et al., 2015). We use a learning rate of 1 × 10−4 for both the policy network and Q-value network with a cosine learning rate scheduler. To sample from the non-parametric distribution pθ,φ(a | s) = πθ(a|s)eQφ(s,a) Eπθ (a|s)[eQφ(s,a)] Carlo algorithm. As MCMC sampling is slow, we trade-off its accuracy with efficiency by choosing moderately small iteration configurations. Specifically, we set the MCMC burn-in steps to 5, number of leapfrog steps to 2, and MCMC step size to 1.\n\n, we use Hamiltonian Monte\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFor practical implementations, we follow the CQL-Lagrange (Kumar et al., 2020) implementation by constraining the Q-value update by a “budget” variable τ and rewrite Eqn. 12 as\n\nmin Q\n\nmax γ1≥0\n\nγ1\n\n(cid:16)\n\nEs∼D\n\n(cid:104) log Eπθ(a|s)\n\n(cid:104)\n\neQφ(s,a)(cid:105)(cid:105)\n\n− Es,a∼D [Qφ(s, a)] − τ\n\n(cid:17)\n\n− J B\n\nQ(φ).\n\n(25)\n\nEqn. 25 implies that if the expected value of Q-value difference is less than the threshold τ , γ1 will adjust to close to 0; if the Q-value difference is higher than the threshold τ , γ1 will be larger and penalize Q-values harder. We set τ = 10 for antmaze-v0 environments and τ = 3 for adroit-v0 and kitchen-v0 environments. For gym-locomotion-v2 tasks, we disable this function and direction optimize Eqn. 12, because these tasks have a relatively short horizon and dense reward, and further constraining the Q values is less necessary. Our code is implemented in JAX (Bradbury et al., 2018) with Flax (Heek et al., 2020) neural networks library.\n\n16",
    "reference": "# Summary Of The Paper\n\nThe authors present MISA, a framework for offline RL based on mutual information based regularization over states and actions. The authors motivate their approach step by step and demonstrate how TD3+Behavior cloning and CQL are related to their proposed approach. Finally, the authors provide empirical results relative to key baselines on the D4RL benchmark while also providing ablation studies and analysis.\n\n# Strength And Weaknesses\n\nStrengths: \n- The authors present a clear buildup to their approach in math and justify each step.\n- The authors contextualize their approach pretty well with respect to prior art on a theoretical level.\n- The approach gets pretty good performance relative to baselines on the D4RL benchmark. \n\nWeaknesses: \n- The novelty is not terribly high in comparison to past work on offline RL and mutual information estimation. \n- The introduction of the mutual information regularization in equations 5 and 6 struck me as quite intuitive in nature and not really originating from first principles.\n\nBoth: \n- I do really appreciate that the authors clearly highlight limitations of their approach when doing learning from data that is using far from an expert policy. However, on the other hand, it is quite a significant limitation relative to baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing quality is pretty good overall and I really appreciate the theoretical discourse of this paper. The novelty is not very high, but I do appreciate that the authors also clearly present their contribution relative to prior art. The authors have provided their code in the supplemental material.\n\n# Summary Of The Review\n\nMy biggest concerns about this paper are the relatively low amount of novelty and the fact that the overall motivation is a bit intuitive in nature. However, I lean towards acceptance because I do really appreciate the theoretical discourse and experiments. I feel that in light of this context the authors have provided, the paper is fleshed out enough to make a contribution to the conference, particularly because of its focus on such an important area. \n\nUpdate After Author Response: \n\nI have read through the other reviews and responses to each review. I have kept my score unchanged, but I can't really serve as a strong advocate for this paper either. It is quite outside my area of expertise, so I feel that my confidence is quite low about what the right thing to do here is as I sympathize with all of the points made.\n\nI agree that the novelty is not huge here, but also hear the authors that it is a significant contribution beyond the past literature. However, the results not being too significant could indeed be a potential issue for adoption of this method. Moreover, I think the authors are too quick to discount the importance of not performing well on data from a uniform policy. It is certainly not common for offline RL benchmarks, but if the data compiled by our behavior policy is already \"expert\" level then offline RL is only improving things at the margins. In typical off-policy RL we start with a uniform policy and while I acknowledge that these are certainly different settings, I think offline RL should be considered harder than off-policy RL (due to lack of access to the underlying behavior policy and lack of access to a simulator during training) and not a strictly easier setting (akin to some slightly more ambitious form of imitation learning).\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nINEQUALITY PHENOMENON IN l∞-ADVERSARIAL TRAINING, AND ITS UNREALIZED THREATS\n\nRanjie Duan1, Yuefeng Chen1, Yao Zhu1,2, Xiaojun Jia1,3, Rong Zhang1 & Hue Xue1\n\n1Alibaba Group, 2Zhejiang University, 3 University of Chinese Academy of Sciences {ranjie.drj, yuefeng.chenyf, stone.zhangr, hui.xueh}@alibaba-inc.com ee zhuy@zju.edu.cn, jiaxiaojun@iie.ac.cn\n\nABSTRACT\n\nThe appearance of adversarial examples raises attention from both academia and industry. Along with the attack-defense arms race, adversarial training is the most effective against adversarial examples. However, we find inequality phenomena occur during the l∞-adversarial training, that few features dominate the prediction made by the adversarially trained model. We systematically evaluate such inequality phenomena by extensive experiments and find such phenomena become more obvious when performing adversarial training with increasing adversarial strength (evaluated by ε). We hypothesize such inequality phenomena make l∞-adversarially trained model less reliable than the standard trained model when the few important features are influenced. To validate our hypothesis, we proposed two simple attacks that either perturb important features with noise or occlusion. Experiments show that l∞-adversarially trained model can be easily attacked when a few important features are influenced. Our work sheds light on the limitation of the practicality of l∞-adversarial training.\n\n1\n\nINTRODUCTION\n\nSzegedy et al. (2013) discovered adversarial examples of deep neural networks (DNNs), which pose significant threats to deep learning-based applications such as autonomous driving and face recognition. Prior to deploying DNN-based applications in real-world scenarios safely and securely, we must defend against adversarial examples. After the emergence of adversarial examples, several defensive strategies have been proposed (Guo et al., 2018; Prakash et al., 2018; Mummadi et al., 2019; Akhtar et al., 2018). By retraining adversarial samples generated in each training loop, adversarial training (Goodfellow et al., 2015; Zhang et al., 2019; Madry et al., 2018b) is regarded as the most effective defense against adversarial attacks. The most prevalent adversarial training is l∞ adversarial training, which applies adversarial samples with l∞ bounded perturbation by ε.\n\nNumerous works have been devoted to theoretical and empirical comprehension of adversarial training (Andriushchenko & Flammarion, 2020; Allen-Zhu & Li, 2022; Kim et al., 2021). For example, Ilyas et al. (2019) proposed that an adversarially trained model (robust model for short) learns robust features from adversarial examples and discards non-robust ones. Engstrom et al. (2019) also proposed that adversarial training forces the model learning to be invariant to features to which humans are also invariant. Therefore, adversarial training results in robust models’ feature representations that are more comparable to humans. Theoretically validated by Chalasani et al. (2020), the l∞-adversarial training suppresses the significance of the redundant features, and the robust model, therefore, has sparser and better-behaved feature representations than the standard trained model. In general, previous research indicates that robust models have a sparse representation of features and view such sparse representation as advantageous because it is more human-aligned. Several works investigate this property of robust models and attempt to transfer such feature representation to a standard trained model using various methods (Ross & Doshi-Velez, 2018; Salman et al., 2020; Deng et al., 2021).\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nHowever, contrary to the claim of previous work regarding such sparse feature representation as an advantage, we find that such sparseness also indicates inequality phenomena (see Section 3.1 for detailed explanation) that may pose unanticipated threats to l∞-robust models. During l∞- adversarial training, the model not only suppresses the redundant features (Chalasani et al., 2020) but also suppresses the importance of other features including robust ones. The degree of suppression is proportional to the adversarial attack budget (evaluated by ε). Hence, given the input images for an l∞-robust model, only a handful of features dominate the prediction. Intuitively, standardtrained models make decisions based on various features, and some redundant features serve as a “bulwark” when a few crucial features are accidentally distorted. However, in the case of a l∞ robust model, the decision is primarily determined by a small number of characteristics, so the prediction is susceptible to change when these significant characteristics are modified (see Figure 1). As shown in Figure 1, an l∞-robust model recognizes a street sign using very few regions of the sign. Even with very small occlusions, the robust model cannot recognize a street sign if we obscure the region that the model considers to be the most important (but well recognized by humans and the standardtrained model). Even if an autonomous vehicle is deployed with a robust model that achieves high adversarial robustness against worst-case adversarial examples, it will still be susceptible to small occlusions. Thus, the applicability of such a robust model is debatable.\n\nFigure 1: l∞-robust model fails to recognize street sign with small occlusions. With given feature attribution maps that attribute the importance of each pixel, we occlude the image’s pixels of high importance with small patches. The resultant image fools the robust model successfully. We notice prior works (Tsipras et al.) showed that feature attribution maps of robust models are perceptually aligned. For clarity we strongly suggest the readers check Appendix A.2. )\n\nIn this work, we name such a phenomenon that only a few features are extremely crucial for models’ recognition as “inequality phenomenon”. we study the inequality from two aspects: 1) global inequality: characterized by the dominance of a small number of pixels. 2) regional inequality: characterized by the tendency of pixels deemed significant by the model to cluster in particular regions. We analyze such phenomena on ImageNet- and CIFAR10-trained models with various architectures. We further devise attacks to expose the vulnerabilities resulting from such inequality based on our findings. Experiments demonstrate that under the premise that human observers can recognize the resulting images, l∞-robust models are significantly more susceptible than the standard-trained models. Specifically, they are susceptible to occlusion and noise with error rates of 100% and 94% respectively, whereas standard-trained models are only affected by 30.1% and 34.5%. In summary, our contribution can be summed up in the following manner:\n\n• We identify the occurrence of the inequality phenomenon during l∞-adversarial training. We design correlative indices and assess such inequality phenomena from various perspectives (global and regional). We systematically evaluate such phenomena by conducting extensive experiments on broad datasets and models.\n\n• Then, we identify unrealized threats posed by such inequality phenomena that l∞-robust models are much more vulnerable than standard trained ones under inductive noise or occlusion. In this case, during the l∞-adversarial training, the adversarial robustness is achieved at the expense of another more practical robustness.\n\n• Our work provides an intuitive understanding of the weakness of l∞-robust model’s feature representation from a novel perspective. Moreover, our work sheds light on the limitation and the hardness of l∞-adversarial training.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n2 BACKGROUND AND RELATED WORK\n\n2.1 ADVERSARIAL ATTACK\n\nDNNs are known to have various risks. These risks include adversarial attacks Li et al. (2021); Zhu et al. (2022); Mao et al. (2021); Qi et al.; Gu et al. (2022), backdoor attacks (Guo et al., 2023; Qi et al., 2023), privacy concerns (Li et al., 2020) and etc. Given a model denoted as f (x; θ) : x → Rk and training dataset denoted as D, empirical risk minimization (ERM) is a standard way (denoted as standard training) to train the model f through:\n\nminθE(x,y)∈Dloss(x, y) (1) where y ∈ Rk is the one-hot label for the image and loss (x, y) is usually cross-entropy loss. With such a training scheme, the model typically performs well on clean test samples. Adversarial examples (Szegedy et al., 2013) aim to generate perturbation superimposed on clean sample x to fool a well-trained model f . Adversarial example x′ can be crafted by either following the direction of adversarial gradients (Goodfellow et al., 2015; Kurakin et al., 2016; Madry et al., 2018a; Duan et al., 2021) or optimizing perturbation with a given loss (Carlini & Wagner, 2017; Chen et al., 2018).\n\n2.2 ADVERSARIAL TRAINING\n\nSeveral defensive strategies are proposed to improve the models’ adversarial robustness (Wong & Kolter, 2018; Akhtar et al., 2018; Meng & Chen, 2017; Raghunathan et al., 2018; Wu et al., 2022). However, analysis by Athalye et al. (2018) shows that among various defensive strategies against adversarial examples, only the adversarial training does not rely on the obfuscated gradient and truly increases the model’s robustness. A model is considered robust against adversarial examples if:\n\nargmaxf (x; θ) = argmaxf (x + σ; θ), s.t.||σ||∞ ≤ ε (2) where ε represents the magnitude of the perturbation. Therefore, the core idea of adversarial training is to train models with adversarial examples, formally:\n\nloss(x, y) = E(x,y)∈D[max||σ||∞≤εloss(x + σ, y)], (3) The objective max|σ|≤εloss(x+σ, y) introduces the model to minimize empirical risk on the training data points while also being locally stable in the (radius-ε) neighborhood around each of data points x. The objective is approximated via gradient-based optimization methods such as PGD (Madry et al., 2018b). Several following works attempt to improve adversarial training from various aspects (Shafahi et al., 2019; Sriramanan et al., 2021; Jia et al., 2022b; Cui et al., 2021; Jia et al., 2022a;c).\n\nInterestingly, Ilyas et al. (2019) proposes that by suppressing the importance of non-robust features, adversarial training makes the trained model more focused on robust and perceptually-aligned feature representations. In this process, the feature representation becomes more sparse. Chalasani et al. (2020); Salman et al. (2020); Utrera et al. (2020) suggests that the feature representation generated by a robust model is concise as it is sparse and human-friendly. It only assigns the feature that is truly predictive of the output with significant contributions.\n\n2.3 HOW INEQUALITY FORMS DURING l∞ ADVERSARIAL TRAINING\n\nIn (Chalasani et al., 2020), they theoretically prove the connection between adversarial robustness and sparseness: During l∞-adversarial training, supposed the adversarial perturbation σ satisfying ||σ||∞ ≤ ε, the model attempts to find robust features serving as strong signals against perturbation. Meanwhile, the non-robust ones which serve as relatively weak signals, and their significance (acquired by feature attribution methods) more aggressively shrunk toward zero. The shrinkage rate is proportional to adversaries’ strength (evaluated by ε). In other words, standard training can result in models where many non-robust features have significant importance for models, whereas l∞-adversarial training tends to selectively reduce the magnitude of the significance of non-robust features with weakly relevant or irrelevant signals and push their significance close to zero. In the end, the feature attribution maps generated by gradients-based feature attribution methods (Smilkov et al., 2017; Lundberg & Lee, 2017; Sundararajan et al., 2017) look more sparse. They regard such sparseness as a merit of adversarial training as it produces more concise and human-aligned feature attributions. However, we further study such sparseness and find it introduces a phenomenon of extreme inequality, which results in unanticipated threats to l∞-robust models.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 METHODOLOGY\n\nIn this section, we first introduced the index used to measure inequality from two aspects. Then we propose two types of attacks to validate our hypothesis: extreme inequality brings in unexpected threats to l∞-robust model.\n\n3.1 MEASURING THE INEQUALITY OF A TEST DATA POINT\n\nFirstly, feature attribution maps are required to characterize the inequality degree by given test data point x and model f . Several feature attribution methods have been proposed in recent years (Smilkov et al., 2017; Lundberg & Lee, 2017; Sundararajan et al., 2017). In general, feature attribution methods rank the input features according to their purported importance in model prediction. To be specific, we treat the input image x as a set of pixels x = {xi, i = 1...M } and denote the generated feature attribution map of x of model f as Af (x), where Af (x) is composed of ai. Feature attribution methods attribute an effect ai to each xi, and summing the effects of all feature attributions approximates the output f (x). xi achieves the top-most score (ai) is regarded as the most important pixel for prediction, whereas those with the bottom-most score are considered least important.\n\nWith a given sorted Af (x) = {ai, i = 1...M |ai < ai+1} generated by a typical feature attribution method, if the prediction f(x) can be approximated with the sum of N most important features and N is much less than M , we name such distribution of Af (x) is unequal. Namely, the prediction on x made by model f is dominated by a few pixels. Formally, we use Gini index (Dorfman, 1979) to measure the inequality of the distribution of a given feature attribution map. Given a population set indexed in non-decreasing order Φ = {φi, i = 1...n|φi ≤ φi+1}, Gini coefficient can be calculated as:\n\nGini(Φ) =\n\n(cid:18)\n\n1 n\n\nn + 1 − 2\n\n(cid:80)n\n\ni=1 (n + 1 − i) ∗ φi i=1 φi\n\n(cid:80)n\n\n(cid:19)\n\n(4)\n\nAn advantage of the Gini(·) index is that inequality of the entire distribution can be summarized by using a single statistic that is relatively easy to interpret (see Appendix A.3 for a more detailed comparison between Gini and other sparsity measures). The Gini index ranges from 0, when the value of every φi is equal, to 1, when a single φi takes all the sum. This allows us to compare the inequality degree among feature attributions with different sizes. We define two types of inequality as follows:\n\n• Global inequality: Given a feature attribution map Af (x) = {ai, i = 1...M |ai < ai+1} on test data point x, we only consider the inequality degree of the global distribution of Af (x) and take no into account for other factors, the inequality degree is calculated with Ginig(Af (x)) directly. The higher of Ginig(Af (x)), the more unequal the distribution Af (x), the fewer pixels take the most prediction power. When Ginig(Af (x)) is equal to 1, it indicates one pixel dominates the prediction while all the other pixels have no contribution to the current prediction.\n\n• Regional inequality: We also consider inequality degree together with spatial factor, whether important feature tends to cluster at specific regions. A region is defined as a block with size of n ∗ n of input space. We first divide pixels into different regions and calculate the sum of pixels’ importance by regions, formally, Af r (x) = (cid:8)ari , i = 1...m|ari < ari+1 (cid:9), where ar is the sum of ai in the region. Therefore, the Gini value on Af r (x) reflects the inequality degree of different regions of input space. The higher the value of Ginir(Af r (x)), the more important pixels tend to cluster in the specific regions. When Ginir(Af r (x)) is equal to 1, it represents all pixels that contribute to the prediction cluster in one region (block).\n\nIn what follows, we propose potential threats caused by such inequality (global and regional inequality). We devise attacks utilizing common corruptions to reveal the unreliability of such decision pattern by l∞-robust model.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n3.2 ATTACK ALGORITHMS\n\nWe propose two simple attacks to validate potential threats caused by such inequality phenomena: 1) Inductive noise attack. 2) Inductive occlusion attack.\n\n3.2.1\n\nINDUCTIVE NOISE ATTACK\n\nWe evaluate the models’ performance under attacks designed by two types of noise.\n\n• Noise (Type I): Given an image x, we perturb the most influential pixels of images with\n\nGaussian noise σ ∈ N (0, 1) via masking M . Formally:\n\nx′ = x + M ∗ σ, where Mi =\n\n(cid:26)0,ai < atre 1,ai ≥ atre\n\n(5)\n\nwhere atre represents the threshold. xi with value that is below to the atre will be kept, and xi whose ai ≥ atre is perturbed by Gaussian noise.\n\n• Noise (Type II): About the second type of noise attack, we directly replace important pixels with Gaussian noise, formally x′ = M ∗ x + M ∗ σ, where M represents reverse mask of M . Compared to Noise-I, Noise-II replaces important pixels totally and disturbs images more severely.\n\nIf the model’s decision pattern is extremely unequal, the performance will be highly influenced when important features are corrupted by inductive noise attacks.\n\n3.2.2\n\nINDUCTIVE OCCLUSION ATTACK\n\nWith respect to inductive occlusion attack, we obscure regions of important pixels with occlusions gradually. During the attack, the max count of occlusions is N with a radius at max R. The order of regions to perturb is decided by the value of Af r (x), that region of higher ari is perturbed in priority by occlusions with size r ∈ {1...R}. The number of occlusions is constrained by n ∈ {1...N }. We also consider occlusion with different colors to reflect potential real-world occlusion. The inductive occlusion attack algorithm is listed as follows:\n\nAlgorithm 1 Inductive Occlusion Attack\n\nRequire: Test data point (x, y), Model f , Regional Attribution map Af\n\nr (x), Max count and radius\n\n▷ Ensure the test data x is correctly classified by model f .\n\nN, R, Perturb color c.\n\nEnsure: f (x) = y\n\nn ← 1, r ← 1, x′ = x for n = 1 to N do\n\nfor r = 1 to R do\n\nM ← get perturb mask(Af x′ = M ∗ x + M ∗ c If f (x′) ̸= y :break\n\nr (x), n, r)\n\n▷ A function to acquire the perturbation mask. ▷ Perturb x by mask M with color c.\n\nend for\n\nend for return x′\n\nNote the intention of this work is not to propose strong adversarial attacks. Although either noise or occlusion is beyond the threat model considered in l∞ adversarial training, we intend to reveal In summary, the the threats caused by such inequality phenomena that previous work ignored. extreme inequality decision pattern of l∞-trained adversarial models to result in themselves being more fragile under some corruptions.\n\n4 EXPERIMENTS\n\nIn this section, we first outline the experimental setup. We then evaluate the inequality degree (by Gini) of different models. Then, we evaluate the performance of the proposed attacks. Finally, we perform an ablation study about the selection of feature attribution methods.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n4.1 EXPERIMENTAL SETTINGS\n\nDataset and models. We perform a series of experiments on ImageNet Deng et al. (2009) and CIFAR10 Krizhevsky et al. (2009). With respect to experiments on ImageNet, we use ResNet18 (He et al., 2016), ResNet50, WideResNet50 (Zagoruyko & Komodakis, 2016) provided by Microsoft 1. For CIFAR10, we use ResNet18, DenseNet (Huang et al., 2017) (see A.1 for detailed configurations). Regarding feature attribution methods (implementation by Captum2), we consider methods including Input X Gradients (Shrikumar et al., 2016), Integrated Gradients (Sundararajan et al., 2017), Shapley Value (Lundberg & Lee, 2017) and SmoothGrad (Smilkov et al., 2017). Considering space and time efficiency, we primarily present experimental results based on Integrated Gradients and perform an ablation study on the other feature attribution methods.\n\nMetrics. For all the tests about the models’ performance, we use error rate (%) as the metric to evaluate the model’s performance under corruptions (e.g., noise and occlusions), which is the proportion of misclassified test images among the total number of test images defined as 1\nn=1[f (x) ̸= f (x′)], where x represents clean test images, and x′ represents test images corN rupted by noise and occlusions. For a fair comparison, we first select 1000 random images from ImageNet that are correctly classified by all the models before performing the attack.\n\n(cid:80)N\n\n4.2\n\nINEQUALITY TEST\n\nIn this section, we first evaluate the inequality degree (both global and regional inequality) of l∞-robust models and standard trained models with different architectures (ResNet18, ResNet50, WideResNet, DenseNet) trained on ImageNet and CIFAR10. We also evaluate the inequality degree of different models adversarially trained with increasing adversarial strength (ε = 1, 2, 4, 8). In the case of the evaluation on Gini, We applied the Gini index to the sorted absolute value of the flattened feature attribution maps. On evaluating regional inequality, we set the region’s size as 16 ∗ 16 for experiments on ImageNet and 4 ∗ 4 for CIFAR10. The results are presented in Table 1. As shown\n\nTable 1: Gini index across different models. We evaluate the Gini coefficient of different models trained with different ε on ImageNet and CIFAR10.\n\nDataset\n\nModel\n\nStd. trained\n\nε = 1.0\n\nε = 2.0\n\nε = 4.0\n\nε = 8.0\n\nGlobal Inequality\n\nCIFAR10\n\nResNet18 DenseNet\n\n0.58 ± 0.05 0.57 ± 0.04\n\n0.65 ± 0.05 0.66 ± 0.06\n\n0.67 ± 0.06 0.67 ± 0.06\n\n0.69 ± 0.06 0.69 ± 0.06\n\n0.73 ± 0.06 0.72 ± 0.07\n\nResNet18 DenseNet\n\n0.79 ± 0.02 0.79 ± 0.02\n\n0.87 ± 0.04 0.85 ± 0.04\n\n0.87 ± 0.04 0.86 ± 0.04\n\n0.88 ± 0.04 0.87 ± 0.04\n\n0.88 ± 0.04 0.88 ± 0.03\n\nRegional Inequality\n\nGlobal Inequality\n\nResNet18 ResNet50 WideResNet\n\n0.60 ± 0.04 0.62 ± 0.04 0.62 ± 0.05\n\n0.69 ± 0.06 0.75 ± 0.05 0.74 ± 0.05\n\n0.79 ± 0.04 0.86 ± 0.03 0.79 ± 0.04\n\n0.92 ± 0.01 0.92 ± 0.02 0.88 ± 0.03\n\n0.95 ± 0.01 0.94 ± 0.01 0.94 ± 0.01\n\nImageNet\n\nRegional Inequality\n\nResNet18 ResNet50 WideResNet\n\n0.80 ± 0.02 0.84 ± 0.02 0.81 ± 0.03\n\n0.83 ± 0.04 0.91 ± 0.05 0.86 ± 0.03\n\n0.88 ± 0.03 0.95 ± 0.02 0.88 ± 0.03\n\n0.95 ± 0.01 0.96 ± 0.01 0.93 ± 0.03\n\n0.97 ± 0.01 0.97 ± 0.01 0.97 ± 0.02\n\nin Table 1, on CIFAR10, the global inequality degree of the standard trained model with different architectures is around 0.58. The Gini (global inequality) for l∞-robust model is around 0.73 when ε = 8. Notably, the inequality phenomena is much more obvious on ImageNet. Especially for an adversarially trained Resnet50 ( ε = 8), the Gini achieves 0.94, which indicates that only a handful of pixels dominate the prediction. Experiments on CIFAR10 and ImageNet show that l∞-robust models rely on fewer pixels to support the prediction with the increasing of the adversarial strength\n\n1https://github.com/microsoft/robust-models-transfer 2https://github.com/pytorch/captum\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n(ε). We also test the inequality degree on different classes belonging to ImageNet; classes related to animal tends to have a higher value on Gini index. For example, class ‘Bustard’ has the highest value on Gini of 0.950. Classes related to scenes or stuff tend to have a lower Gini. For example, class ‘Web site’ has the lowest inequality of 0.890 (See Appendix A.7).\n\nWe visualize the features’ attribution of given images for the standard and l∞-adversarially trained ResNet50 respectively in Figure 2. When the model is adversarially trained with weak adversarial\n\nFigure 2: Feature attributions of different models. We visualize feature attributions generated by l∞-robust models (adversarially trained by adversaries of different ε), the larger of ε, the fewer features that model relies on for prediction.\n\nstrength (ε = 1), the model has better feature attribution aligned to human observers. However, when the adversarial strength increases, the model gradually assigns higher importance to fewer pixels and resulting in extreme inequality regarding feature attribution. Moreover, these most important pixels tend to gather in a few specific regions ( Additional visualizations for ImageNet and CIFAR10 are in Appendix A.11 and A.10 respectively).\n\n4.3 EVALUATION UNDER INDUCTIVE NOISE ATTACK\n\nIn this part, we compare the performance of standard- and adversarially- trained ResNet50 under random and inductive noise. We set noise with different scales, including subpixels of 500, 1000, 5000, 10000, and 20000. We present the results in Figure 3.\n\nFigure 3: Evaluation under noise. We plot the error rate of standard- and adversarially-trained models on images perturbed by the increasing number of noise.\n\nUnder random noise, the success rate of attack on the robust model achieves 73.4%, but only 18.8% for standard-trained model. Under Noise of Type I, the robust model is fooled by 94.0%, while the standard trained model is only fooled by 34.5%. Under Noise of Type II, even when we control the amount of noise with a small threshold (e.g., 1000 pixels), more than 50% of predictions made by the robust model is affected. When we enlarge the threshold to 20000, the robust model (ε=8) is almost fooled with a 100% success rate. In summary, compared to the standard trained model,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nl∞-robust model relies on much fewer pixels to make a decision; such a decision pattern results in unstable prediction under noise.\n\n4.4 EVALUATION UNDER INDUCTIVE OCCLUSION ATTACK\n\nIn this part, we perform an inductive occlusion attack and evaluate the standard- and l∞-robust ResNet50s’ performance. We set two group experiments with different thresholds.\n\nStd.\n\nModel\n\nε = 2.0\n\nε = 1.0\n\nTable 2: Models’ performance (Error rate %) under occlusions. We evaluate the models’ performance by gradually occluding important areas with patches of different sizes and colors.\n\nIn the first group of experiments, we generate occlusions with a max count of 5 and a In this case, max radius of 10. the adversarially-trained model is fooled at a 71.7% error rate, but the standard trained model’ predictions are only affected by 31.6%. When we enlarge the threshold and set max count as 10 and radius as 20 for occlusions, both ε = 4 and ε = 8 adversarially trained model can be fooled with 100% success rate while only 41.2% attack success rate for the standard-trained model. We visualize the results in Figure 4. As the figure shows, l∞-adversarially trained model with larger ε could be easily attacked by smaller occlusions even under the same threshold. For example, in Figure 4, the standard trained model can recognize ‘Bulbul‘ well with the head part occluded, but the adversarially trained model fails to recognize the ‘Bulbul’ if only the beak of the bulbul is occluded. Moreover, compared with adversarial perturbation, occlusion is more practi-\n\nOcclusion-G 30.1% 48.2% 56.3% 100.0% 100.0% Occlusion-W 40.1% 59.1% 73.3% 100.0% 100.0% Occlusion-B 41.2% 70.2% 72.2% 100.0% 100.0%\n\nOcclusion-G 23.5% 31.6% 38.4% 32.4% Occlusion-W 28.3% 48.4% 57.5% 61.3% Occlusion-B 31.6% 51.5% 53.3% 48.9%\n\nMax cnt N = 10, R = 20\n\nMax cnt N = 5, R = 10\n\n54.0% 71.7% 64.6%\n\nε = 4.0\n\nε = 8.0\n\nFigure 4: Visualization of occluded images. We visualize images occluded with different patches of different sizes and the corresponding predictions made by standard and l∞-adversarially trained models. Compared to a standard-trained model, the adversarially trained model is fragile when occlusion covers the area of important features.\n\ncal as occlusion frequently appears in the real world. We also evaluate the transferability of attacked results between the robust model and the standard trained model, the results are consistent with our observation (see Appendix A.4).\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n4.5 ABLATION STUDY\n\nWe consider four attribution methods for the ablation study: Input X Gradient (Shrikumar et al., 2016), SmoothGrad (Smilkov et al., 2017), Gradient Shapley Value (GradShap for short) (Lundberg & Lee, 2017) and Integrated Gradients (Sundararajan et al., 2017) (see Appendix A.8 for detailed configuration). We perform an ablation study to evaluate the effect of selection on the feature attribution methods (see Table 3). Among various attribution methods, SmoothGrad produces more\n\nTable 3: Ablation study on selection. We evaluate our hypothesis with different feature attribution methods.\n\nAttribution Method Model\n\nGini Noise I Noise II Occlusion-B Occlusion-G Occlusion-W\n\nInput X Gradient\n\nGradShap\n\nSmoothGrad\n\nIntegrated Gradients\n\nStd. trained Adv. trained\n\nStd. trained Adv. trained\n\nStd. trained Adv. trained\n\nStd. trained Adv. trained\n\n0.63 0.93\n\n0.62 0.93\n\n0.75 0.98\n\n0.62 0.94\n\n16.1% 45.0% 60.9% 90.4%\n\n19.8% 54.7% 62.3% 93.7 %\n\n63.5% 45.0% 82.5% 98.3%\n\n16.5% 55.5% 63.9 % 95.5%\n\n24.7% 63.3%\n\n31.5% 64.8%\n\n32.3% 61.6%\n\n31.6% 64.6%\n\n16.8% 51.2%\n\n24.1% 53.3%\n\n25.6% 49.7%\n\n23.5% 54.0%\n\n23.5% 63.2%\n\n29.9% 71.6%\n\n30.3% 60.8%\n\n28.3% 71.7%\n\nspare feature attribution maps and thus results in higher values on Gini. Regarding evaluation under noise, SmoothGrad increases the inductive noise attack’s success rate. Regarding evaluation under occlusion, the selection of Integrated Gradients improve the attack’s success rate on models.\n\nIn conclusion, the selection of attribution methods slightly affects attacks’ success rates but does not change our conclusion: the distribution of features’ attribution by l∞-robust model is much more unequal; such inequality makes the robust model more susceptible to inductive noise and occlusions.\n\n5 DISCUSSION AND CONCLUSION\n\nIn this work, we study the inequality phenomena that occur during l∞-adversarial training. Specifically, we find l∞-robust models’ feature attribution is not as aligned with human perception as we expect. An ideal human-perceptual aligned model is expected to make decisions based on a series of core feature attributions. For example, if the model classifies an input image as a bird, it should take attributions, including the eye, the beak of the bird, and the shape of the bird, all of these attributions into account. However, we find l∞-robust model only relies on individual attribution (only the bird’s beak) for recognization. We name such phenomena as inequality phenomenon. We perform extensive experiments to evaluate such inequality phenomena and find that l∞ robust model assigns a few features with extremely high importance. Thus, a few features dominate the prediction. Such extreme inequality of l∞-robust model results in unreliability. We also design attacks (by utilizing noise and occlusion) to validate our hypothesis that robust models could be more susceptible under some scenarios. We find an attacker can easily fool the l∞-trained model by modifying important features with either noise or occlusion easily. We suggest that both noise and occlusion are common in a real-world scenario. Therefore, robustness against either noise or occlusion is more essential and crucial than robustness against adversarial examples. Our work reveals the limitation and vulnerability of the current l∞-robust model. We also evaluate if such inequality phenomenon exists in l2-robust model and models trained with sparsity regularization. The evaluation results show that such a phenomenon is a unique property of l∞-robust model (see Appendix A.5 and A.6).\n\nWe also propose a strategy to release such inequality phenomena during l∞-adversarial training. We combine Cutout (DeVries & Taylor, 2017) strategy with adversarial training and force the model learning features from different regions by cutting out part of training images at each iteration during the training (see the result in Appendix A.9). The strategy slightly releases the inequality degree of the robust model. More effective strategies releasing such extreme inequality could be a crucial and promising direction for future work. We hope our work can motivate new research into the characteristics of adversarial training and open up further challenges for reliable and practical adversarial training.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nIn this paper, we identify inequality phenomena that occur during l∞-adversarial training, that l∞- robust model tends to use few features to make the decision. We give a systematical evaluation of such inequality phenomena across different datasets and models with different architectures. We further identified unrealized threats caused by such decision patterns and validated our hypothesis by designing corresponding attacks. Our findings provide a new perspective on inspecting adversarial training. Our goal is to understand current adversarial training’s weaknesses and make DNNs truly robust and reliable. We did not use crowdsourcing and did not conduct research with human subjects in our experiments. We cited the creators when using existing assets (e.g., code, data, models).\n\nREPRODUCIBILITY STATEMENT\n\nWe present the settings of hyper-parameters and how they were chosen in the experiment section. We repeat experiments multiple times with different random seeds and show the corresponding standard deviation in the tables. We plan to open the source code to reproduce the main experimental results later.\n\nREFERENCES\n\nDirect methods for sparse matrices. Oxford University Press, 2017.\n\nNaveed Akhtar, Jian Liu, and Ajmal Mian. Defense against universal adversarial perturbations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3389–3398, 2018.\n\nZeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science\n\ndeep learning. (FOCS), pp. 977–988. IEEE, 2022.\n\nMaksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial\n\ntraining. Advances in Neural Information Processing Systems, 33:16048–16059, 2020.\n\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning, pp. 274–283. PMLR, 2018.\n\nNicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In ACM Workshop on Artificial Intelligence and Security, pp. 3–14. ACM, 2017.\n\nPrasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Xi Wu, and Somesh Jha. Concise explanations of neural networks using adversarial training. In International Conference on Machine Learning, pp. 1383–1391. PMLR, 2020.\n\nPin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: Elastic-net attacks to deep neural networks via adversarial examples. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206– 2216. PMLR, 2020.\n\nJiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. Learnable boundary guided adversarial training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15721–15730, 2021.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 248–255, 2009.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nZhun Deng, Linjun Zhang, Kailas Vodrahalli, Kenji Kawaguchi, and James Y Zou. Adversarial training helps transfer learning via better representations. Advances in Neural Information Processing Systems, 34:25179–25191, 2021.\n\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\n\nwith cutout. arXiv preprint arXiv:1708.04552, 2017.\n\nRobert Dorfman. A formula for the gini coefficient. The review of economics and statistics, pp.\n\n146–149, 1979.\n\nRanjie Duan, Yuefeng Chen, Dantong Niu, Yun Yang, A Kai Qin, and Yuan He. Advdrop: Adversarial attack to DNNs by dropping information. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7506–7515, 2021.\n\nLogan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint arXiv:1906.00945, 2019.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In International Conference on Learning Representations, 2015.\n\nJindong Gu, Hengshuang Zhao, Volker Tresp, and Philip HS Torr. Segpgd: An effective and efficient In Computer Vision– adversarial attack for evaluating and boosting segmentation robustness. ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIX, pp. 308–325. Springer, 2022.\n\nChuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial images using input transformations. In International Conference on Learning Representations, 2018.\n\nJunfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency. In ICLR, 2023.\n\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n\nefficient neural network. Advances in neural information processing systems, 28, 2015.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer vision and Pattern Recognition, pp. 770–778, 2016.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4700–4708, 2017.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In Neural Information Processing Systems, pp. 125–136, 2019.\n\nXiaojun Jia, Yong Zhang, Xingxing Wei, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. In Computer Vision–ECCV Prior-guided adversarial initialization for fast adversarial training. 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part IV, pp. 567–584. Springer, 2022a.\n\nXiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Las-at: Adversarial training with learnable attack strategy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13398–13408, 2022b.\n\nXiaojun Jia, Yong Zhang, Baoyuan Wu, Jue Wang, and Xiaochun Cao. Boosting fast adversarial training with learnable adversarial initialization. IEEE Transactions on Image Processing, 31: 4417–4430, 2022c.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nHoki Kim, Woojin Lee, and Jaewook Lee. Understanding catastrophic overfitting in single-step adversarial training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8119–8127, 2021.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\n\nInternational Conference on Learning Representations, 2016.\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n\nefficient convnets. In International Conference on Learning Representations.\n\nXiaodan Li, Yining Lang, Yuefeng Chen, Xiaofeng Mao, Yuan He, Shuhui Wang, Hui Xue, and Quan Lu. Sharp multiple instance learning for deepfake video detection. In Proceedings of the 28th ACM international conference on multimedia, pp. 1864–1872, 2020.\n\nXiaodan Li, Jinfeng Li, Yuefeng Chen, Shaokai Ye, Yuan He, Shuhui Wang, Hang Su, and Hui Xue. Qair: Practical query-efficient black-box attacks for image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3330–3339, 2021.\n\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances\n\nin neural information processing systems, 30, 2017.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018a.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018b.\n\nXiaofeng Mao, Yuefeng Chen, Shuhui Wang, Hang Su, Yuan He, and Hui Xue. Composite adversarial attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 8884–8892, 2021.\n\nDongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security, pp. 135–147, 2017.\n\nMazda Moayeri, Kiarash Banihashem, and Soheil Feizi. Explicit tradeoffs between adversarial and\n\nnatural distributional robustness. In Advances in Neural Information Processing Systems.\n\nMazda Moayeri, Phillip Pope, Yogesh Balaji, and Soheil Feizi. A comprehensive study of image classification model sensitivity to foregrounds, backgrounds, and visual attributes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19087–19097, 2022.\n\nChaithanya Kumar Mummadi, Thomas Brox, and Jan Hendrik Metzen. Defending against universal In Proceedings of the IEEE/CVF International\n\nperturbations with shared adversarial training. Conference on Computer Vision, pp. 4928–4937, 2019.\n\nAaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James Storer. Deflecting adversarial attacks with pixel deflection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8571–8580, 2018.\n\nGege Qi, GONG Lijun, Yibing Song, Kai Ma, and Yefeng Zheng. Stabilized medical image attacks.\n\nIn International Conference on Learning Representations.\n\nXiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the as-\n\nsumption of latent separability for backdoor defenses. In ICLR, 2023.\n\nAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-\n\nples. In International Conference on Learning Representations, 2018.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nAndrew Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nHadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:3533–3545, 2020.\n\nAli Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in Neural Information Processing Systems, 32, 2019.\n\nAvanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning important features through propagating activation differences. arXiv preprint arXiv:1605.01713, 2016.\n\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ́egas, and Martin Wattenberg. Smoothgrad:\n\nremoving noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.\n\nGaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, et al. Towards efficient and effective adversarial training. Advances in Neural Information Processing Systems, 34:11821–11833, 2021.\n\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks.\n\nIn\n\nInternational conference on machine learning, pp. 3319–3328. PMLR, 2017.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2013.\n\nDimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In International Conference on Learning Representations.\n\nFrancisco Utrera, Evan Kravitz, N Benjamin Erichson, Rajiv Khanna, and Michael W Mahoney. Adversarially-trained deep nets transfer better: Illustration on image classification. In International Conference on Learning Representations, 2020.\n\nEric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR, 2018.\n\nBoxi Wu, Jindong Gu, Zhifeng Li, Deng Cai, Xiaofei He, and Wei Liu. Towards efficient adversarial training on vision transformers. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XIII, pp. 307–325. Springer, 2022.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks.\n\nIn British Machine Vision\n\nConference 2016. British Machine Vision Association, 2016.\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pp. 7472–7482. PMLR, 2019.\n\nYao Zhu, Yuefeng Chen, Xiaodan Li, Kejiang Chen, Yuan He, Xiang Tian, Bolun Zheng, Yaowu Chen, and Qingming Huang. Toward understanding and boosting adversarial transferability from a distribution perspective. IEEE Transactions on Image Processing, 31:6487–6501, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 DETAILS OF MODELS\n\nA.1.1 DETAILS OF MODELS TRAINED ON IMAGENET\n\nWe summarize the clean and robust accuracy (%) of models trained on ImageNet in Table ??. Regarding robust accuracy, we use PGD for evaluation. During the training on ImageNet, the images\n\nTable 4: l∞-robust models’ clean and robust accuracy (ImageNet)\n\nModel\n\nStd. trained\n\nε=1/255\n\nε=2/255\n\nε=4/255\n\nε=8/255\n\nResNet-18 ResNet-50 Wide-ResNet-50\n\n69.76 76.13 81.60\n\n63.46 /33.38 72.05 /45.02 74.65 /47.30\n\n59.63 /29.80 69.10 / 42.75 72.35 /45.95\n\n52.49 /26.52 63.86 / 38.85 68.41 /43.10\n\n42.11 /23.75 54.53 / 33.05 60.82 /38.35\n\nare resized to 256 using interpolation=InterpolationMode.BILINEAR, followed by a central crop of size=224. Finally, the values are first rescaled to [0.0,1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n\nTRAIN TRANSFORM = t r a n s f o r m s . Compose ( [\n\nt r a n s f o r m s . R e s i z e ( 2 5 6 ) , t r a n s f o r m s . C e n t e r C r o p ( 2 2 4 ) , t r a n s f o r m s . R a n d o m H o r i z o n t a l F l i p ( ) , t r a n s f o r m s . T o T e n s o r ( ) ,\n\n] ) TEST TRANSFORM = t r a n s f o r m s . Compose ( [\n\nt r a n s f o r m s . R e s i z e ( 2 5 6 ) , t r a n s f o r m s . C e n t e r C r o p ( 2 2 4 ) , t r a n s f o r m s . T o T e n s o r ( ) ,\n\n] )\n\nA.1.2 DETAILS OF MODELS TRAINED ON CIFAR10\n\nWe summarize the clean and robust accuracy of models trained on CIFAR10 in Table 5. Regarding robust accuracy, we use AutoAttack Croce & Hein (2020) for evaluation.\n\nTable 5: l∞-robust models’ clean and robust accuracy (CIFAR10)\n\nModel\n\nStd. trained\n\nε=1/255\n\nε=2/255\n\nε=4/255\n\nε=8/255\n\nResNet-18 DenseNet\n\n93.90 92.80\n\n92.10 /86.60 91.30 /86.60\n\n90.40 /79.90 89.90 /79.50\n\n88.30 /68.80 85.80 /65.90\n\n81.20 /48.60 79.60 /44.40\n\nTRAIN TRANSFORM = t r a n s f o r m s . Compose ( [\n\nt r a n s f o r m s . RandomCrop ( 3 2 , p a d d i n g = 4 ) , t r a n s f o r m s . R a n d o m H o r i z o n t a l F l i p ( ) , t r a n s f o r m s . T o T e n s o r ( ) ,\n\n] )\n\nTEST TRANSFORM = t r a n s f o r m s . Compose ( [\n\nt r a n s f o r m s . T o T e n s o r ( ) ,\n\n] )\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.2 FURTHER DISCUSSION ABOUT VISUALIZATION OF FEATURE ATTRIBUTION\n\nMost visualization methods apply post-processing techniques during generating feature attribution maps. The post-processing technique is also clarified in (Tsipras et al.): “For CIFAR-10 and ImageNet, we clip gradients to within ±3σ and rescale them to lie in the [0, 1] range.” Thus, the most influential pixels with extremely high values are clipped to a relatively lower value but they actually dominate the prediction (see Figure 5).\n\nFigure 5: Visualizing feature attribution with and without post-processing (by 3 deviations).\n\nWe also provide more visualization of feature attribution maps with and without post-processing. As Figure 6 shows, the post-processed feature attribution maps are more perceptually-aligned with human observers. However, such visualization are not subjective.\n\nFigure 6: More visualization of feature attribution maps with and without post-processing .\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA.3 SPARSITY MEASURE VS. GINI INDEX & MOTIVATION BEHIND USING GINI INDEX\n\nThe sparsity of a vector can be quantified by ||x||0 (duf, 2017), which simply calculates the ratio |x| of non-zero elements. However, the sparsity measure treats an infinitesimally small value the same as a significant value. Even if some of the small coefficients increase by significant values, that change will not be reflected by a change in the value of the sparsity measure. For example, given a vector x1 = [0, 0, 0, 1, 1] and x2 = [0, 0, 0, 1, 1000], the sparsity degree of x1 and x2 is equal to 0.4. However, their distributions are totally different. The distribution of x2 is much more unequal compared with x1. Also, is sensitive to noise, especially in settings where most values of elements are around 0 (e.g., feature attribution map).\n\n||x||0 |x|\n\nIn our case, Gini is able to reflect the change when a small coefficient increases. A Gini coefficient of 0 expresses perfect equality, where all values are the same, while a Gini coefficient of 1 (or 100%) expresses maximal inequality among values. For example, the Gini of x1 = [0, 0, 0, 1, 1] equals to 0.6, and 0.799 for x2 = [0, 0, 0, 1, 1000]. The value of Gini also provides an intuitive understanding of the distribution. When Gini = 0.6, approximately 40% in the population (1-0.6 = 0.4) occupies the total worth. When Gini = 0.799, approximately 21.1% of the population dominates the worth. As for our experiment, Gini of feature attributions by l∞-robust model (ε = 8.0) is about 0.95, representing less than 5% of pixels that dominate the prediction.\n\nA.4 TRANSFERABILITY TEST\n\nwe perform occlusion attacks with two groups of attack budgets:\n\n• Group 1: max count = 5, max radius = 10.\n\n• Group 2: max count = 10, max radius = 20.\n\nWe perform noise attacks with threshold = 5000.\n\nAttack Occ -B (cnt=5, r=10) Occ-B (cnt=10, r=20)\n\nNoise-I\n\nNoise-II\n\nModel\n\nAdv.\n\nAdv. Std.\n\n100.0 22.8\n\nStd.\n\n11.4 100.0\n\nAdv.\n\n100.0 43.6\n\nStd.\n\n17.4 100.0\n\nAdv.\n\nStd.\n\nAdv.\n\nStd.\n\n100.0 26.8\n\n10.6 100.0\n\n100.0 58.2\n\n18.6 100.0\n\nAs the table shows, the transferability between the l∞-robust model and the standard-trained model is low. Transferring attack results from the standard-trained model to the l∞-robust model is easier. If the region of the most important pixels is occluded, the l∞-robust model fails to recognize the images correctly. The experiments are consistent with our observations.\n\nA.5 COMPARED WITH l2-ROBUST MODEL\n\nDue to different properties of lp norm vector space, the case of l2 adversarial training is not the same as l∞ adversarial training. To be specific, l∞ constrains the maximum magnitude of perturbation for each pixel. The adversarial noise is added on each pixel independently during the l∞ adversarial training. Therefore, the model attempts to find the most robust feature against noise and drops the features which could be affected by adversarial noise. Therefore, with increasing the magnitude (ε) of adversarial noise, fewer but more robust features l∞-robust model can rely on for recognition.\n\nDifferent from l∞ norm measures each pixel independently, l2 norm calculates the square root of the inner product of all elements in a vector. Thus, during l2 adversarial training, if a large budget of perturbation perturbs some pixels, the other pixels share the left budget on perturbation. Moayeri et al. provides a game-theoretic understanding of l2-adversarial training: during each loop of l2-adversarial training, the attacker perturbs the features which are predictive for the model. When some predictive features are perturbed with the most budget of perturbation, the features perturbed with small or without perturbation are easier for the model to learn. Furthermore, these less perturbed features then become more predictive at the next training iteration. Thus, the inequality phenomenon does not occur during l2-adversarial training. However, the l2-robust model would use both the object-relevant and object-irrelevant features (e.g., background) for prediction at the\n\n16\n\nPublished as a conference paper at ICLR 2023\n\ngame’s equilibrium. In (Moayeri et al.; 2022), they show that l2-robust model is more sensitive to the background and other spurious features.\n\nModel\n\nClean acc Gini-g Gini-r Occ-B Occ-G Occ-W Noise-I Noise-II\n\nStd. trained l2-AT model linf-AT model\n\n76.13% 56.13% 54.53%\n\n0.62 0.60 0.95\n\n0.84 0.76 0.97\n\n33.0% 28.1% 36.5% 15.8% 51.4% 32.5% 48.3% 2.9% 81.5% 61.0% 77.2% 75.6%\n\n49.8% 36.3% 96.1%\n\nAs explained above, the inequality degree of l2-robust model is similar to the standard trained model (or l2 is even more equal). However, l2-robust model is still more vulnerable to occlusion. We guess it is because the most influential features tend to cluster together for both l∞ at and l2-robust model.\n\nA.6 COMPARED WITH MODELS TRAINED WITH SPARSITY REGULARIZATION\n\nwe perform experiments on l∞-robust model and model trained with regularization for sparsity. We consider two types of sparse models: the model with sparse architecture and models with sparse weights. We compared l∞-robust model with models regularized by the following techniques:\n\n• l1 norm pruning (Li et al.): It prunes filters by removing whole filters in the network to-\n\ngether with their connecting feature maps.\n\n• Weight pruning (Han et al., 2015): It applies mask as regularization and sets the weights to\n\nbe 0. It results in sparser weights and connectivity patterns.\n\nModel\n\nClean Acc Gini g Gini r Occ-B Occ-G Occ-W Noise-I Noise II\n\nAdv. trained L1-norm sparsity Weight-level Pruning\n\n54.53% 73.08% 75.60%\n\n0.94 0.60 0.62\n\n0.97 0.80 0.81\n\n78.2% 80.2% 60.2% 56.8% 57.8% 52.5% 33.1% 29.2% 25.8%\n\n60.9% 40.7% 15.7%\n\n95.1% 89.2% 49.9%\n\nThe sparsity of either model architecture or weights will not result in inequality on feature attribution. The l∞-AT model is much more easily affected by occlusion and noise attack than the two sparse models. We think l∞ can be regarded as a strong regularization: during l∞-adversarial training, the model attempts to find the most robust feature against adversarial noise and discards the features which could be affected by added adversarial noise. With increasing the magnitude (ε) of adversarial noise, only a handful of features l∞-adversarially trained model can rely on for recognition.\n\nA.7 EQUALITY DEGREE OF DIFFERENT CLASSES\n\nWe test the inequality degree of feature attributions’ distribution of 50000 samples from 1000 classes in ImageNet. We present results in Table 6.\n\nTable 6: Global inequality degree of different classes (l∈-Adv. traind, ε = 8)\n\nTop-5\n\nBottom-5\n\nClass Bustard Manhole cover Oystercatcher Redshank\n\nPomeranian\n\nGini\n\n0.950\n\n0.949\n\n0.949\n\n0.949\n\n0.949\n\nClass Web site\n\nSlot\n\nGrocery store Grille\n\nComic book\n\nGini\n\n0.890\n\n0.900\n\n0.901\n\n0.901\n\n0.905\n\nClasses with top-5 inequality (Gini value) are: Bustard, Manhole cover, Oystercatcher, Redshank and Pomeranian. And their Gini values are 0.950, 0.949, 0.949, 0.949 and 0.949. Classes with bottom-5 inequality (Gini value) are: Web site, Slot, Grocery store, Grille and Comic book. Their corresponding Gini values are: 0.890, 0.900, 0.901, 0.901 and 0.905.\n\nRegarding regional inequality, we present results in Table 7. Classes with high regional inequality are similar to classes with high global inequality. Specifically, classes of the top 5 (regional inequality) are Redshank, American coot, Oystercatcher, Bustard and Gazelle. And the Ginir(.) of these\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTable 7: Regional inequality degree of different classes (l∈-Adv. traind, ε = 8)\n\nTop-5\n\nBottom-5\n\nClass Redshank American coot Oystercatcher\n\nBustard Gazelle\n\nGini\n\n0.976\n\n0.975\n\n0.975\n\n0.974\n\nClass Web site\n\nGrocery store\n\nConfectionery\n\nSlot\n\nGini\n\n0.929\n\n0.932\n\n0.934\n\n0.935\n\n0.974\n\nGrille\n\n0.936\n\nclasses are: 0.976, 0.975, 0.975, 0.974 and 0.974. Classes in the bottom 5 (regional inequality) are Web site, Grocery store, Confectionery, Slot, and Grille. And the Ginir(.) of these classes are: 0.929, 0.932, 0.934, 0.935 and 0.936.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nA.8 DETAILS ABOUT FEATURE ATTRIBUTION METHOD\n\nWe use attribution methods including Input X Gradient, Smooth Gradient (short for SmoothGrad), Gradient Shapley Value (short for GradShap) and Integrated Gradients.\n\nInput X Gradient: The Input X Gradient multiplies input with the gradient with respect to each input feature. It is a baseline approach for computing the attribution.\n\nGradShap: Shapley Values aims compute each feature’s attribution based on cooperative game theory. GradShap approximates Shapley Values by computing the expectations of gradients by randomly sampling from the distribution of baselines. It adds noise to each input sample nsamples times, selects a random baseline from the baselines’ distribution, and a random point along the path between the baseline and the input. Then it computes the gradient of outputs with respect to those selected random points. In our evaluation, we set nsamples = 20 for experiments with ImageNet and nsamples = 10 for experiments with CIFAR10. We set baseline = 0 for all the experiments.\n\nIntegrated Gradients: Integrated Gradients is an axiomatic model which assigns an importance score to each input feature by approximating the integral of gradients of the model’s output with respect to the inputs along the path (straight line) from given baseline to inputs. Previous work points out Integrated Gradients method is sensitive to the choice of path. To reduce such sensitivity, Integrated Gradients are usually repeated for nstep steps. For all the experiments, we set nstep = 20, and baseline ∈ N (0, 1).\n\nSmoothGrad: SmoothGrad adds gaussian noise to each input in the batch nsamples times, then applies the given attribution algorithm to each of the samples. It returns the mean of the sampled attributions. SmoothGrad returns a sparser feature attribution map than other methods. In our experiment, we set nsamples = 20\n\nA.9 HOW TO RELEASE INEQUALITY PHENOMENON\n\nWe also try to propose a strategy to release the inequality phenomenon in adversarial training. Intuitively, we hope adversarially trained models learn to find robust features from the whole image rather than focus on a specific robust feature. Towards this end, we incorporate Cutout with adversarial training that Cutout enables the model to learn features from multiple spatial spaces. We evaluate our strategy on CIFAR10. The results are presented in Table 8.\n\nModel Adv. trained Adv. trained+Cutout\n\nGini Gini-R Clean Acc. Adv. Acc. Noise I Noise II Occlusion-B Occlusion-G Occlusion-W 0.73 0.70\n\n48.50 % 40.31 % 1.58% 1.72% 40.78% 47.20 %\n\n100.00% 29.98%\n\n100.00% 93.39%\n\n100.00% 55.06 %\n\n82.10 % 81.40 %\n\n0.88 0.88\n\nTable 8: l∞-adversarial training with and without Cutout.\n\nAs the table indicates, the Cutout strategy can slightly release the inequality of l∞-adversarial training. But at a price, both clean accuracy and adversarial accuracy slightly decrease. Regarding performance under noise and occlusion, the strategy does not improve the adversarial trained model’s performance under noise but improves its performance under occlusions. A more effective strategy to release inequality phenomena is highly required.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nA.10 VISUALIZATION FOR CIFAR10\n\nWe visualize feature attribution maps and attack results for CIFAR10. About the setting of attack, we set max count N = 10, with max radius R = 4. With occlusion with different colors (black, white and grey), success rates on 1000 correct classified images of l∞-adversarially trained model are 60.4%, 60.5%, and 38.1% respectively. And success rates for the standard trained models are 34.6 %, 36.7% and 24.1% respectively. We visualize corresponding results in Figure 8.\n\nFigure 7: Visualization of feature attribution on CIFAR10.\n\nFigure 8: Visualization of occluded images for CIFAR10.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nA.11 MORE VISUALIZATION RESULTS FOR IMAGENET\n\n0.68\n\n0.73\n\n0.79\n\n0.89\n\n0.94\n\n0.68\n\n0.74\n\n0.79\n\n0.88\n\n0.93\n\n0.61\n\n0.62\n\n0.70\n\n0.84\n\n0.90\n\n0.62\n\n0.77\n\n0.83\n\n0.91\n\n0.95\n\n0.62\n\n0.69\n\n0.74\n\n0.85\n\n0.91\n\n0.60\n\n0.69\n\n0.77\n\n0.86\n\n0.92\n\n0.59\n\n0.70\n\n0.75\n\n0.86\n\n0.91\n\nFigure 9: Visualization of feature attributions generated by standard- and adversarially trained model with different ε.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper provides some insights on the vulnerability of l_{infty} adversarial trained model. The paper identifies inequality phenomenon occurs during l_{infty} adversarial training which is quantified by Gini index. To show the later, the paper proposes two methods: inductive noise and occlusion to demonstrate the vulnerability of l_{infty} adversarial trained model caused by such phenomenon. This paper provides a novel perspective and sheds light on the practicality of l_{infty} adversarial trained model.\n\n# Strength And Weaknesses\n\nPros:\n- A very interesting observation, inequality phenomenon occurring during l_{infty} adversarial training. \n- Provides a new perspective on feature representation of l_{infty} adversarial trained model, which will motivate future research.\n- Proposes regional and global Gini to evaluate the inequality phenomenon quantitively that providing an intuitive explanation. \n- Experiments show l_{infty} adversarial trained model is even more fragile than standard trained model under some scenarios, the results are interesting for me.\n\nCons:\n- Though this is a very relevant and timely work related to reliability of l_{infty} adversarial training, it would help if the authors could provide some effective solution to release such phenomenon or other suggestion.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper introduces a novel phenomenon about l_{infty} adversarial training. This paper is well written and well organized, the method is easy to understand and reproduced.\n\n# Summary Of The Review\n\nInteresting phenomenon, reasonable metric (Gini), well-motivated method that demonstrate the vulnerability of l_{infty} adversarial trained model.\nThis is a good paper, with some aspects of the presentation that should be improved.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nGAIN: ON THE GENERALIZATION OF INSTRUCTIONAL ACTION UNDERSTANDING\n\nJunlong Li1, Guangyi Chen2,3, Yansong Tang1, Jinan Bao4, Kun Zhang2,3, Jie Zhou1, Jiwen Lu1,∗ 1Tsinghua University, 2MBZUAI, 3Carnegie Mellon University, 4 University of Alberta\n\nABSTRACT\n\nDespite the great success achieved in instructional action understanding by deep learning and mountainous data, deploying trained models to the unseen environment still remains a great challenge, since it requires strong generalizability of models from in-distribution training data to out-of-distribution (OOD) data. In this paper, we introduce a benchmark, named GAIN, to analyze the GeneralizAbility of INstructional action understanding models. In GAIN, we reassemble steps of existing instructional video training datasets to construct the OOD tasks and then collect the corresponding videos. We evaluate the generalizability of models trained on in-distribution datasets with the performance on OOD videos and observe a significant performance drop. We further propose a simple yet effective approach, which cuts off the excessive contextual dependency of action steps by performing causal inference, to provide a potential direction for enhancing the OOD generalizability. In the experiments, we show that this simple approach can improve several baselines on both instructional action segmentation and detection tasks. We expect the introduction of the GAIN dataset will promote future in-depth research on the generalization of instructional video understanding. The project page is https://jun-long-li.github.io/GAIN.\n\n1\n\nINTRODUCTION\n\nInstructional videos play an essential role for learners to acquire different tasks. The explosion of instructional video data on the Internet paves the way for learners to acquire knowledge and for computer vision community training models, for example, human can train an action segmentation model to understand the video by the dense step prediction of each frame, or an action detection model to localize each step. While a number of datasets for instructional action understanding (IAU) have been proposed over the past years(Alayrac et al., 2016; Das et al., 2013b; Malmaud et al., 2015; Sener et al., 2015) and growing efforts have been devoted to learning IAU models(Zhukov et al., 2019; Huang et al., 2017), the limited generalizability of models remains to be a major obstacle to the deployment in real-world environments. One may ask a question “Suppose the model has learned how to inflate bicycle tires, does it know how to inflate car tires?” In fact, due to potential environmental bias between the training dataset and application scenes, the well-trained model might not be well deployed in an OOD environment (Ren et al., 2019), especially when instructional videos of interest to users are not involved in the finite training dataset.\n\nTo encourage models to learn transferable knowledge, it is desirable to benchmark their generalizability. Though this OOD generalization problem (Barbu et al., 2019; Hendrycks et al., 2021; Hendrycks & Dietterich, 2019) attracts much attention in the field of image recognition, such as ObjectNet (Barbu et al., 2019) and ImageNet-R (Hendrycks et al., 2020), it has barely been explored for the IAU task. A related problem is video domain generalization (Yao et al., 2021) (VDG) for conventional action recognition which focuses on domain generalization when changing the scene or background of the action. However, different from conventional action, the key obstacle to the generalization of instructional action is the distribution shift of action steps under different task categories, which is caused by the collection bias of the datasets. In Fig. 3, we show that the steps under different task categories have different distributions.\n\n∗Corresponding author\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Two examples of constructing new OOD instructional tasks by reassembling the steps of in-distribution videos in training datasets. For example, the OOD task “Make Jelly” consists of five steps: {prepare seasonings, stir, put into molds, take out, cut into pieces}, where the “prepare seasonings” step is in the task “Make Lamb Kebab”, the “put into molds” and “take out” steps come from “Make Chocolate”, and the “stir” and “cut into pieces” steps are in the task “Make Pizza”. The steps in GAIN are consistent with those in the training set, with non-overlapping task categories. GAIN encourages models to transfer the knowledge learned from training data for OOD data.\n\nGiven the motivation that action steps are the key research objects of IAU and have distribution shift when task categories change, we propose a new evaluation strategy to benchmark the generalizability by re-constructing test task categories using the steps of training tasks and evaluating the models with these new task categories. In the reconstruction, we require that training and testing task categories are different but step categories are consistent. As shown at the bottom of Fig. 1, we try to find a new testing task “Make Jelly” with existing step categories including the “prepare seasonings” step in the task “Make Lamb Kebab”, the “ put into molds” and “take out” steps in “Make Chocolate”, and the “ stir” and “ cut into pieces” steps in “Make Pizza”. This construction is non-trivial since existing IAU datasets cannot be directly used. First, for most IAU datasets (such as COIN (Tang et al., 2019)), the steps in different videos are not shared, therefore, we cannot construct testing data by splitting itself. Second, though CrossTask (Zhukov et al., 2019) also collects cross-task videos with partial steps shared, these shared parts are only a minority in the dataset (only 14% steps are shared, i.e. 73 are shared of a total of 517 steps) and most videos have steps that are not shared with others. Besides, because the related tasks are not fine-grained annotated, they cannot be used for evaluation. Furthermore, it is built to investigate whether sharing constituent components improves the performance of weakly supervised learning. It motivates us to collect and annotate a real-world IAU dataset, GAIN. It consists of 1,231 videos of 116 OOD tasks with 230 categories of steps, covering a wide range of daily activities. All videos in our GAIN dataset are employed for evaluation. These videos can be split into two groups: GAIN-C and GAIN-B, as counterparts of the COIN (Tang et al., 2019) and Breakfast (Kuehne et al., 2014) datasets, respectively.\n\nFurthermore, we propose a simple yet effective approach to enhance the generalizability of IAU models by cutting off excessive contextual dependency by performing causal inference. It is inspired by the observation that model generalizability is inevitably influenced by short-cutting with a biased context. Compared with previous methods learn the temporal dependency among video steps by the temporal networks, such as TCN (Lea et al., 2017) applying a hierarchy of temporal convolutions, we propose to reduce the over-dependency between steps to mitigate the negative effect from temporal context bias. For example, if the task “Inflate Bicycle Tires” is always observed together with the “bicycle pumps” during the training process, this knowledge will be difficult to transfer to the OOD task “Inflate Car Tires” with other inflaters. In our approach, we apply the Back-Door Criterion to infer causal effect, and present a Monte Carlo based method to approximate the distribution after “intervention”. The method is evaluated with various baseline methods on both action segmentation and detection tasks, and is shown to produce consistent improvements.\n\nContributions. (1) We propose a new evaluation strategy to benchmark the generalizability of IAU models by evaluating the models on the OOD tasks. (2) We build a real-world OOD instructional video dataset, GAIN, where the OOD tasks are constructed by reassembling the steps of\n\n2\n\nRoast Chicken: {prepare seasonings, remove intestines, brush sauce, bake}Make Jelly: {prepare seasonings, stir, put into molds, take out, cut into pieces}Make Chocolate: {..., put into molds, ..., take out, ...}Make Lamb Kebab: {prepare seasonings,..., brush sauce}Clean Fish: {...,remove intestines, ...}Make Pizza : {..., stir,..., cut into pieces, ..., bake, ...}Training SetGAINPublished as a conference paper at ICLR 2023\n\ntraining datasets. (3) We propose a simple yet effective approach, cutting off excessive contextual dependency by causal inference, which provides a potential direction to enhance generalizability.\n\n2 THE GAIN DATASET\n\nIn this section, we introduce our GAIN dataset, a video-based dataset covering a large range of daily tasks reassembled via a specific framework, which collects the tasks whose categories are different from training tasks to benchmark the generalizability of IAU models. For convenience, we call the tasks whose categories are same with the tasks in the training dataset as in-distribution tasks, and the ones with different categories as OOD tasks. Note that, when we mention “in-distribution” and “OOD”, the variables are steps but not tasks. It means the steps are “in-distribution”/ “OOD” under same/different tasks. To our best knowledge, GAIN is the first dataset to evaluate the generalizability of IAU models on the OOD steps. Fig. 2 shows the pipeline to construct the GAIN dataset. Below, we describe the details of our dataset, including how to benchmark the generalizability, how to collect the data and construct the dataset, and the basic dataset statistics.\n\n2.1 PROBLEM DEFINITION\n\nThe generalizability is of critical importance to the models for the deployment in a real-world environment, especially for IAU systems, e.g., we expect the model can know how to “Inflate Car Tires” after learning how to “Inflate Bicycle Tires”. For this goal, we propose to benchmark the generalizability of IAU models by building an OOD evaluation dataset, in which task categories is constructed by reassembling the steps of the training set. With this construction setting, the step categories are consistent and step distribution is changed.\n\ni }nT i = ST\n\ni=1 contains nT instructional videos X T , where each video is The training dataset X T = {X T composed as a set of steps X T i . This step set can be formulated as ST j=1, where ns is the number of steps in a video. In the conventional experimental setting, both training and evaluation data are in-distribution, which is formulated as X T i.i.d∼ XSource and X E i.i.d∼ XSource, where XSource denotes the data distribution and X E denotes the videos in the evaluation set. To benchmark the generalizability of models, we collect the videos of unseen tasks with seen steps, where videos in the evaluation set are OOD tasks that can be formulated as follows:\n\ni = {sT\n\ni,j}ns\n\nX E\n\nOOD\n\ni.i.d∼ XT arget, X T i.i.d∼ XSource S = ΩE S ,\n\ns.t. ΩT\n\n(1)\n\nS and ΩE\n\nwhere ΩT S respectively denote the set of all steps in the training and evaluation set, and X E OOD denotes the collected OOD evaluation dataset. As shown in Fig. 1, we show some collected videos of our GAIN dataset, where the collected videos follow different step distributions but share the same step space. Finally, we evaluate the models trained on X T with the OOD evaluation dataset X E\n\nOOD to benchmark the OOD generalizability.\n\nTable 1: Comparisons of different evaluation strategies. IV: instructional video; SL: using source label during training; TD: using target data during training; SC: steps are consistent.\n\nHere we distinguish our defined OOD generalizability evaluation from other evaluation strategies. We summarize the comparisons with different evaluation methods in Table 1. First, compared to conventional supervised and unsupervised methods, we focus on the OOD evaluation to benchmark the generalizability of models. Second, UDA (Busto et al., 2018; Zhang et al., 2019) aims to transfer the knowledge from source domain to some known target domain. It needs the domain index (e.g. the target data) for the training process to minimize the domain gap between the source and known target. Compared with UDA, our OOD generalization further considers how to solve the problem without any domain indices. Though zero-shot recognition (ZSR) (Sener & Yao, 2019; Wang et al., 2019) also focuses on the generalizability of models, it is too difficult to conduct zero-shot analysis directly for IAU models since ZSR requires the models to understand unseen action steps. This setting can be used for task-level actions (e.g. classification) given extra descrip-\n\n✗ ✓ UDA (Busto et al., 2018) ✓ ✓ ✓ ✓ ✗ Zero Shot (Sener & Yao, 2019) ✓ ✓ ✓ ✓ ✗ VDG (Yao et al., 2021) ✗ ✓ ✗ ✓ ✗ Ours ✓ ✓ ✗ ✓ ✓\n\nSupervised (Tang et al., 2019) ✓ ✓ ✗ ✗\n\nUnsupervised (Miech et al., 2019) ✓ ✗\n\nMethods IV SL TD OOD SC\n\n✗ ✓\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The pipeline to construct GAIN, which includes Task Selection (left) and Data Collection (right). Given an instructional video training set, we first separate the steps of these tasks and generate a large number of task candidates. Secondly, we filter out the unqualified ones according to three principles. Then, we search for YouTube videos related to the selected tasks and download the videos, which embrace high relevance with queries, explicit instructions, and rich diversity.\n\ntions (Wang et al., 2019), but not for complex action understanding tasks such as action segmentation or action detection. Recently, many methods in the field of image classification have attempted to evaluate the generalizability of models by collecting or generating the OOD data, e.g. ObjectNet (Barbu et al., 2019) and ImageNet-R (Hendrycks et al., 2021). However, how to evaluate the generalizability of models for more complex IAU task has barely been visited. The most related one is VDG (Yao et al., 2021), which evaluates the domain generalization ability of action recognition models when changing the scene or background. Unlike VDG, our setting focuses on the distribution shift of action steps when task categories are changed in the target domain, which is more common in the field of IAU.\n\n2.2 TASK SELECTION\n\nTo construct an evaluation dataset consisting of diverse and high-quality daily tasks, we choose the largest fine-grained annotated dataset, COIN (Tang et al., 2019), and the widely-used instructional video dataset, Breakfast (Kuehne et al., 2014), as the training sets.\n\nHow should we select the new tasks to benchmark the generalizability? We argue that the tasks in our GAIN dataset require three basic principles as follows:\n\n• Task Non-overlapping: The steps in our GAIN dataset are out-of-distribution, which requires the tasks in GAIN to be non-overlapping with those in the original training set. The model performance on these non-overlapping tasks can intuitively indicate the generalizability.\n\n• Step Consistent: Despite the step distributions are different under non-overlapping tasks, we require that the categories of steps in the testing videos are consistent with the training dataset. On the one hand, with totally different steps, IAU will be even more difficult, which deflects our goal to benchmark the generalizability. On the other hand, the steps in the training set are common in daily life, which is of critical importance for IAU.\n\n• Category Diverse: The third principle, category diverse, encourages the annotators to discover more diverse data. In other word, we argue that the larger number of task categories is the better. For example, a dataset (with 3 tasks) contains 2 videos of repairing a car, 2 videos of repairing a roof and a video of repairing a television is more diverse than a dataset (with only 1 task) with 5 videos of repairing a car. More diverse data indicates more reliable benchmarking.\n\nWith the principles above, as shown in Fig. 2, we first generate a large number of task candidates (i.e. step combinations) and then filter out the unqualified ones. Specifically, we apply the steps in the training dataset as the anchor steps and generate 10 step combinations with the caption clues, where each step combination contains 2∼5 steps. The captions in instructional videos often mention steps that are not in the current task but closely related to other steps in the video, which could help to generate step combinations. In total, we generate more than 8,000 task candidates.\n\nHere we provide details to show how we filter the candidates. Given a candidate, we first check whether it is logical. For example, if the candidate is “lifting jack, replace the tire, remove the jack”, it makes sense because it could form a task “Change Car Tire”; it is not acceptable if the candidate is “lifting jack, replace the tire, add the seasoning”, which may never happen sensibly in daily life. Then according to Task Non-overlapping, we drop the logical candidates if they make up tasks that already exist in training sets. Since all candidates are composed of steps directly from training sets, they inherently satisfy the principle Step Consistent.\n\nIn the first round of annotation, we ask 11 annotators to go through these candidates and annotate whether the candidate is reasonable and satisfies the above principles. We filter out the candidates\n\n4\n\n1. A B C2. C D F A3. B E G H4. H J B5. A C D E......✗✓✗✗✓how to change car tirechange car tire......GAINTraining SetSeparated StepsFilter CandidatesSearch VideosCollect VideosTask SelectionData CollectionPublished as a conference paper at ICLR 2023\n\nannotated as unqualified by more than half annotators and finally select 147 candidates. In the second round, annotators are asked to name the new OOD task, refine current step combinations, and collect the videos from the Internet. By filtering out the rare actions, we finally collect 1,231 videos of 116 tasks. In the last round, the annotators label the fine-grained temporal boundaries of each steps in videos.\n\n2.3 DATA COLLECTION\n\nGiven the selected tasks, we search for YouTube videos related to the task names. We use a query with exactly the task name or the task name following a “how to” prefix to locate instructional videos, e.g. for the task “Change Car Tire” we use “change car tire” or “how to change car tire”. To improve the quality and diversity, we adopt several criteria to select videos including: high relevance with queries, explicit instructions, and rich diversity. We prefer videos more relevant to the queries and containing explicit instructions with pictures, since visual models are not able to only learn from narrations. Besides, videos with explicit steps to complete tasks are also favorable, although there might be no vocal instructions. Furthermore, explicit steps in a video do not need to exactly match those in its task – in other words, permuting and being a proper subset of the task are acceptable. Moreover, if similar steps are witnessed in different videos of a task, like “add salt” and “add sugar”, we regard them as the same step. With regard to the undefined steps, they are considered as the background and not further annotated. On the one hand, during the data collection stage, if a video contains a long stretch of undefined but important steps, this video will be filtered out according to the principle of Step Consistent. On the other hand, videos with undefined meaningless steps are acceptable and these steps will be considered as the background. After collecting the videos, we utilize the annotation tool provided in (Tang et al., 2019) to label the corresponding step categories and segments.\n\nFigure 3: The step distributions of the training dataset, original in-distribution test dataset, and our OOD test dataset on COIN. Under the same task categories, the step distribution is similar to the original training and test datasets. With different task categories in GAIN-C, the step distribution changes a lot, which supports our assumption that steps are in-distribution/OOD with same/different task categories.\n\n2.4 STATISTICS\n\nThe final version of the GAIN dataset consists of 1,231 instructional videos related to 116 unseen tasks. Our GAIN dataset is a pure evaluation dataset to benchmark the generalizability of IAU models. Each task in GAIN contains 2∼24 videos with an average of 10 videos. We annotate 6,382 action segments in GAIN with an average of 5 steps in each video. The average length of videos is 2 minutes and 30 seconds, and the average length of steps is 12 seconds. Totally, the GAIN dataset contains OOD videos of 51.2 hours for generalizability evaluation.\n\nGAIN can be divided into two splits as counterparts of COIN (Tang et al., 2019) and Breakfast (Kuehne et al., 2014) datasets, and we name them GAIN-C and GAIN-B, respectively. COIN is a large-scale benchmark with 9,030 training videos and 2,797 testing videos of 180 tasks. As its counterpart, GAIN-C contains 1,000 videos of 100 unseen tasks with a length of 41.6 hours in total, where 5,238 segments are annotated. Fig. 3 shows the step distributions on COIN Train, COIN Test, and our GAIN-C, where horizontal axis denotes the different steps and the vertical axis denotes the frequency rates of these steps. We can observe the step distributions in original COIN Train and Test sets are similar, but different from our GAIN-C. It demonstrates our assuming that under different task categories, the step distributions are different. Breakfast is composed of more than 1.9k cooking-related videos of 10 breakfast routines such as “Make Coffee” and “Cook Pancakes”. Accordingly, the GAIN-B split includes 231 videos of 16 OOD tasks with an average length of 2 minutes and 30 seconds. These tasks consist of 20 fine-grained action categories. We provide more statistical data and analysis in the Appendix.\n\n3 METHOD\n\nIn this section, we first construct a causal graph for the IAU problem. Then, we introduce our method which applies causal inference to mitigate the negative effect of confounding context bias.\n\n5\n\nCOIN TrainCOIN TestGAIN-CFrequency RateStepPublished as a conference paper at ICLR 2023\n\n3.1 CAUSAL GRAPH CONSTRUCTION\n\nThe widely researched action understanding tasks include action segmentation and detection, which both focus on the steps. Without loss of generality, we formulate the task as:\n\nP (Y |S) = fθ(S),\n\n(2)\n\nwhere S denotes a step in the video X, Y is the prediction and fθ represents the model. Then, we formulate the action understanding framework in light of a causal graph G = {V, E}, where the nodes V include the step S, model prediction Y , and context steps Z. Note that X = S ∪ Z and S ∩ Z = ∅ denote video X can be divided by query step S and context steps Z. The links E indicate the dependence (computational but not strict causal direction (Liu et al., 2021)) between two variables. For example, S → Y in the causal graph indicates variable S is the cause of variable Y . We show the casual structure of the IAU problem in Fig. 4(a) and explain it as follows:\n\n• Z → Y ← S indicates that the model prediction depends on both the step S and the context steps Z. For example, when recognizing the current step S, temporal models (e.g. LSTM (Hochreiter & Schmidhuber, 1997) and C3D (Tran et al., 2015)) always use the temporal context clues for current prediction, which leads to Z → Y .\n\n• S ← Z → Y denotes that the video context steps Z simultaneously affects the steps and model prediction. Z → Y has been explained above and S ← Z is intuitive due to the temporal dependency of video. Thus we call the context bias Z is a confounder (Pearl, 2009), which misleads the model to focus on the spurious correlation, reducing the generalizability of the model. The casual graph describes the information flow during the inference. When S is being estimated, other context steps are Z, and since S is affected by Z during the inference, Z points to S.\n\nThen we show that the model prediction is misled by the spurious correlations of context bias when we only consider the likelihood P (Y |S). As shown in Fig. 4(a), we re-write P (Y |S) with the Bayes rule as:\n\nP (Y |S) = ΣzP (Y |S, Z = z)P (Z = z|S),\n\n(3)\n\nwhich denotes that the likelihood P (Y |S) are influenced by P (Z = z|S). Now we use an example to show that P (Z = z|S) is biased. In the video “Inflate bicycle tires” , current content S is “installing the nozzle” and the context Z is “using bicycle pump”. The content S and the context Z are always observed together in the training process and thus P (Z = using bicycle pump|S = installing the nozzle) is higher. It leads the model to predict higher probability P (Y |S = installing the nozzle) when observing the Z = using bicycle pump and vice verse. However, when we apply the model to analyze the OOD video “Inflate car tires”, where Z “using bicycle pump” is absent, the model may be confused and consequently give wrong prediction. Additionally, although there is actually bidirectional effect between S and Z, we find that Bayes’ theorem and Eq.3 remains unchanged. In Fig. 4(a), we apply S ← Z to highlight the bias caused by the con-fonder S ← Z → Y , which is demonstrated by the Fig. 3.\n\nMotivated by the causal inference method (Pearl, 2009; Glymour et al., 2016), we propose to conduct intervention to alleviate the negative effect of context bias. In causal inference, the intervention is represented as do(·). Once intervened, a variable will have no in-coming links anymore and the previous in-coming links in the causal graph are cut off. As shown in Fig. 4(a), when we intervene S with the Back-Door Criterion in (Pearl, 2009), i.e. do(S), the link between S and Z is cut off so as the dependency. We formulate the model prediction process under the intervention:\n\nP (Y |do(S)) = ΣzP (Y |S, Z = z)P (Z = z),\n\n(4)\n\nwhere Z = z is independent from S. Thus, after intervention, when the model predicts from do(S) to the label Y , it fairly takes every z into consideration. Please see more detailed introduction and derive about Back-Door Criterion in Section 3.3 of (Glymour et al., 2016).\n\nHowever, the intervention is a great challenge, since the prediction under this intervention is subject to the prior P (Z), which is difficult to compute numerically. Thus, we simulate conducting the intervention. We replace the numeric process with a sampling process and approximate the prior P (z) by the Monte Carlo method. As shown in Fig. 4(b), we regard each step as an individual instance and put all the steps in a lottery box, i.e. a step pool. Statistically, all the steps in the training set form the population Z = {z1, ..., zn} with n categories, and then we can sample from this population. During the sampling process, the frequency of z is not affected by X anymore and is\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: (a) The causal inference illustration for instructional action understanding. (Top) presents the original causal graph of IAU and the likelihood P (Y |S). (Bottom) shows the causal graph and the causation P (Y |do(S)) after intervention. (b) Approximation with Monte Carlo method. We first dissemble the videos, where s1 only occurs with z1&z2, and sample from the step pool. The prior P (Z) is approximated with the relative frequency and sampled steps constitute the“intervened” videos, where s1 could be observed with z3 or z4.\n\nonly related to the statistics of the training set. We approximate the prior with the relative frequency:\n\nP (Z = z′) =\n\nΣz∈ΩI(z = z′) ∥Ω∥\n\n,\n\n(5)\n\nwhere Ω denotes the sampling population, ∥Ω∥ is the sample size, and I is an indicator function. We use the sampled steps to assemble “intervened” videos to learn causations of X on Y , instead of the spurious correlations due to context bias Z. Fig. 4(b) illustrates an example of this process, in which s1 only occurs with z1&z2 in the original videos and consequently models tend to learn spurious correlations of them. After dissembling and reassembling, in the “intervened” videos, s1 could be observed with others, like z3&z4, and the occurrence of z is not dependent on s1. Technically, our causal “intervention” can be regarded as a new kind of data argumentation, where we dissemble the steps and reassemble them as new video data.\n\n4 EXPERIMENTS\n\nIn this section, we provide performance comparisons between the in-distribution dataset and out-ofdistribution GAIN dataset, and assess the effectiveness of our causal approach on both action segmentation and action detection tasks. We conduct experiments on three datasets, where COIN (Tang et al., 2019) and Breakfast (Kuehne et al., 2014) are used for both training and testing, and our GAIN dataset is only used for evaluation. As mentioned in Section 2.4, COIN and Breakfast are widely used for IAU, so GAIN is split into two groups as counterparts of them, named GAIN-C and GAIN-B respectively. Please refer to Section 2.4 for more descriptions of these datasets. The implementation details, results, and analysis are described below. More experimental results and visualization examples can be found in Appendix.\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nWe conduct experiments with the following baseline methods: (1) LSTM(Hochreiter & Schmidhuber, 1997) is one of the earliest and most popular deep models dealing with temporal modeling. (2) ED-TCN(Lea et al., 2017) applies a hierarchic encoder-decoder framework with temporal convolutions, pooling, and upsampling to learn temporal patterns. We use 5 convolution layers for both the encoder and decoder, whose convolutional filters’ sizes are 25. (3) TResNet(He et al., 2016) adds a residual stream in the encoder-decoder framework. We follow the network structure depicted in (Lei & Todorovic, 2018) and adopt the same experimental setting as ED-TCN’s. (4) MS-TCN++(Li et al., 2020) proposes a multi-stage architecture, which first generates initial predictions and refines them several times. Our implementation is built upon the publicly provided codebase.\n\n7\n\nDissembleReassembleStep Pool···%!%\"&#%#%$“Intervened”Videos%!&#%$&#%\"%#%\"Videos%!%\"&#%#%$&#%#%$\"#+,(/)=!!\"#/,'\"(')Probability\"#/=!!\"#/,'\"('|/)ZSYS: Step Y: Prediction Z: Context biasProbabilityContext biasContext biasZSY+,($)(a)(b)Published as a conference paper at ICLR 2023\n\nTable 2: Evaluation on COIN/GAIN-C with baselines and finer results across domains. · / · denotes performances on COIN/GAIN-C. C means we apply causal-based method.\n\nDomain\n\nLSTM C-LSTM ED-TCN C-ED-TCN TResNet C-TResNet MS-TCN++ C-MS-TCN++\n\nNursing 64.3/59.8 65.6/61.6 61.0/60.7 64.1/61.1 62.6/59.5 64.5/60.0 Vehicle 61.9/60.7 62.6/62.8 59.2/63.7 59.9/67.2 60.6/64.2 59.3/66.1 Leisure 58.0/60.4 59.3/62.9 55.1/60.4 56.9/61.8 54.9/60.0 57.0/60.1 Gadgets 67.4/60.3 68.6/63.9 64.9/63.2 66.4/66.5 64.2/63.9 65.6/64.2 Electrical 64.5/44.6 65.9/48.8 63.7/47.5 62.7/47.8 63.9/44.6 63.2/49.0 Furniture 61.5/57.0 63.5/59.1 58.8/59.3 59.7/58.3 60.4/60.2 60.9/60.4 Science 61.4/41.5 61.9/45.8 56.5/37.7 57.8/42.9 58.0/42.1 58.4/41.7 Pets 61.0/52.0 64.9/54.2 61.9/48.7 63.5/52.7 61.5/51.2 61.6/48.6 Drink 65.1/50.6 66.8/53.5 61.9/50.3 62.5/54.4 61.8/48.8 62.1/53.5 Sport 69.8/63.5 73.4/63.8 68.1/63.6 69.0/67.1 66.8/65.3 66.9/50.3 70.3/47.3 71.8/51.3 64.5/48.8 66.9/50.9 65.7/47.5 67.8/66.1 Dish Housework 62.2/61.4 62.7/59.6 60.6/61.6 59.9/61.0 59.4/59.0 60.8/61.1\n\n65.6/62.1 62.9/63.8 61.8/63.3 68.3/61.4 66.2/44.4 63.8/60.0 61.6/38.8 63.7/52.2 67.1/46.1 72.0/49.5 71.8/59.4 61.5/59.8\n\n68.2/62.1 63.0/66.9 61.2/62.2 68.2/64.7 65.4/45.3 63.6/61.8 63.1/47.7 64.6/52.0 66.5/54.2 70.9/51.6 70.3/66.7 64.2/61.9\n\nOverall\n\n63.9/52.3 65.4/55.1 61.2/52.7 62.2/55.3 61.6/52.4 62.2/54.6\n\n65.5/51.8\n\n65.6/56.0\n\nFor COIN/GAIN-C, we use the temporal video resolution at 10 fps, and extract S3D(Miech et al., 2020) features with a pretrained model on HowTo100M (Miech et al., 2019) as the model input. And for Breakfast/GAIN-B, we use I3D (Carreira & Zisserman, 2017) features (pretrained on Kinetics (Carreira & Zisserman, 2017)) sampled at 15 fps as the model input. As for the original evaluation set, we follow the default setting and present results on split 1. For all experiments, we employ a 1 × 1 convolution layer to project the features into an embedding space, whose dimension is 64. Then, we apply different baseline methods to model the spatio-temporal clues. All settings are the same with both the baseline methods and our methods.\n\n4.2 ACTION SEGMENTATION\n\nSetting: Action segmentation aims at assigning each video frame with a step label. This task is a key step to understand complex actions in instructional videos. We adopt frame-wise accuracy, which is the number of correctly predicted frames divided by the number of total video frames. For Breakfast/GAIN-B, we also adopt edit distance and F1 score (Lea et al., 2017) at overlapping thresholds 10% to further measure the quality of the model prediction.\n\n44.5/35.4 42.7/5.3 44.3/5.9\n\n49.1/18.0 49.2/30.8 43.0/5.5 41.6/3.7 46.4/8.8 44.1/6.7\n\nTable 3: Results on Breakfast/GAIN-B for action segmentation.\n\nMethod ED-TCN C-ED-TCN TResNet C-TResNet MS-TCN++ C-MS-TCN++ LSTM C-LSTM Acc. 47.8/25.5 45.7/37.9 42.4/21.3 F1@10 37.8/3.2 39.0/4.7 Edit\n\nResults: The frame accuracy on COIN/GAIN-C is shown in Table 2 (the Overall row). We observe an obvious performance drop of approximately 10.0 on average, although the steps of the two datasets are shared. Besides, Table 3 shows the results on Breakfast and GAIN-B. The performance gap is more obtrusive since the frame accuracy decreases by approximately 60% on average. It indicates that the current methods lack generalizability on the out-of-distribution tasks. We observe that causal-based methods achieve consistent improvements in the OOD scenario. Besides, we find that the performances of LSTM are poor with the F1 score and edit score, while other baseline methods work well. By qualitatively checking the predictions, we find that LSTM only can tell actions from the background, but fails to classify the action categories correctly. It may be because the LSTM model is overly dependent on temporal relations and weakens the representational capacity.\n\n66.4/32.1 54.7/3.7 56.4/6.4\n\n67.6/16.8 57.1/3.1 54.6/4.9\n\n4.8/0.5 6.2/1.8\n\n4.0/0.3 3.5/1.1\n\nQuantitative Analysis: We compare our methods with the baseline methods to demonstrate the effectiveness. Table 2 (the Overall row) and Table 3 summarize the performance comparisons of our methods with all four baseline methods including LSTM(Hochreiter & Schmidhuber, 1997), ED-TCN(Lea et al., 2017), TResNet(He et al., 2016) and MS-TCN++(Li et al., 2020). For all four baseline methods, our causal-based methods achieve significant and consistent improvements on GAIN and obtains comparable results on the original evaluation sets with the frame accuracy metric. For example, after blocking the causal link between S and Z, Causal MS-TCN++ relatively outperforms the baseline over +8.1% on GAIN-C. Moreover, on GAIN-B the causal inference methods relatively outperform the baselines over +69.3% on average.\n\nDomain Analysis: Following the COIN dataset, we provide more in-depth analysis with experimental results across different domains in Table 2 (Domains are described and showed in the Ap-\n\n8\n\nPublished as a conference paper at ICLR 2023\n\npendix). An obvious performance drop from COIN to GAIN-C occurs on domains ‘Electrical’ and ‘Science’. The reason is that steps in these domains often follow a fixed process, which introduces strong contextual dependency to models and results in poor performance on the OOD tasks. The causal inference approach alleviates these negative effects, for example, Causal LSTM relatively outperforms the baseline with a large margin of +9.4% and +10.4% on domain ‘Electrical’ and ‘Science’, respectively. On domains like ‘Housework’, models obtain comparable results on COIN and GAIN-C, which is because the video collection of GAIN-C is independent of the collection for COIN. So it is possible that the videos we find are easier to be segmented than those in COIN. The “Overall” results show that OOD test set is more challenging.\n\nQualitative Analysis: We qualitatively analyze how our method contributes to the improvement of performance. Fig. 5 demonstrates the visualization of two prediction examples on GAIN-C with MS-TCN++ and the corresponding causal method, where different colors means different step categories. Obviously, Causal MS-TCN++ achieves higher frame accuracy on both two examples. At the top, the original MS-TCN++ predicts 6 kinds of steps for the video “Wash cat”, while Causal MS-TCN predicts the same causations of steps as the ground truth. This demonstrates that our model does not predict the spurious correlations caused by the context bias but focuses on the step itself. At the bottom, we show an example of the video “Scalded shrimp”, the causal one outperforms the baseline method with more smooth predictions.\n\nFigure 5: Visualization examples of action segmentation results on GAIN-C.\n\n4.3 ACTION DETECTION Setting: The goal of action detection is to detect a series of steps and output the temporal boundaries. It is also an important yet challenging task for IAU. We follow the evaluation protocol of (Lea et al., 2017; Singh et al., 2016) by reporting the widely-used segment-wise metric, mean Average Precision with midpoint hit criterion (mAP@mid). Specifically, the criterion of mAP@mid for a true positive is whether or not the temporal midpoint of the output interval is within the corresponding groundtruth action segments.\n\nResults: Table 4 presents the experimental results on COIN, Breakfast, and their counterparts. In this task, we choose LSTM as the baseline. From the training set to GAIN, we observe a huge performance drop by more than 80%, which is related to the weak OOD generalizability of the baseline method. We also compare the causal methods with the baselines. Without any performance cost on the original evaluation set, the causal methods relatively outperform baselines over +33%/+55% on GAIN-C/GAIN-B.\n\nTable 4: Evaluation on training sets and GAIN for action detection. ‘· / ·’ denotes the performances of ‘Training set / GAIN’.\n\nLSTM Causal LSTM\n\n57.7 / 8.5 58.6 / 13.2\n\n32.8 / 6.0 35.2 / 8.0\n\nmAP@mid\n\nBreakfast\n\nMethods\n\nCOIN\n\n5 CONCLUSION\n\nIn this paper, we have introduced a dataset, named GAIN, to benchmark the generalizability of IAU models. Our GAIN dataset contains 1,231 videos of 116 OOD tasks, which are collected by reassembling the in-distribution steps of the training set. Based on GAIN, we have proposed to evaluate the generalizability of models with the performance on the OOD tasks. We have also proposed a causal inference approach to cut off the excessive contextual dependency for enhancing generalizability. We evaluate the generalizability of some widely used methods on GAIN and demonstrate that causal inference is a potential direction to improve generalization. We will release this dataset to promote the real-world deployment of IAU models.\n\nLimitation: By benchmarking the generalizability on GAIN, we offer a testbed and expect work to develop models that can work in dynamic environments. However, the OOD data needs to be found, which is labor-intensive. Thus, the GAIN dataset cannot be omniscient or cover every aspect. Besides, while causal inference shows its potential to improve generalizability, designing algorithms for generalization is still an open question. It is worth devoting efforts and we leave it as future work.\n\n9\n\nGround truthMS-TCN++Causal MS-TCN++Ground truthMS-TCN++Causal MS-TCN++(a) Wash cat(b) Scalded shrimp Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENT\n\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 62125603, and in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI).\n\nREFERENCES\n\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon In CVPR, pp. 4575–\n\nLacoste-Julien. Unsupervised learning from narrated instruction videos. 4583, 2016.\n\nJean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon\n\nLacoste-Julien. Learning from narrated instruction videos. TPAMI, 40(9):2194–2208, 2018.\n\nAndrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Danny Gutfreund, Joshua Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In NeurIPS, 2019.\n\nBiagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In CVPR, pp. 4613– 4623, 2020.\n\nPau Panareda Busto, Ahsan Iqbal, and Juergen Gall. Open set domain adaptation for image and\n\naction recognition. TPAMI, 42(2):413–429, 2018.\n\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In CVPR, pp. 961–970, 2015.\n\nJoao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics\n\ndataset. In CVPR, pp. 6299–6308, 2017.\n\nSilvia Chiappa. Path-specific counterfactual fairness. In AAAI, pp. 7801–7808, 2019.\n\nPradipto Das, Chenliang Xu, Richard F. Doell, and Jason J. Corso. A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR, pp. 2634–2641, 2013a.\n\nPradipto Das, Chenliang Xu, Richard F Doell, and Jason J Corso. A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR, pp. 2634–2641, 2013b.\n\nSandra Eliza Fontes De Avila, Ana Paula Brandao Lopes, Antonio da Luz Jr, and Arnaldo de Albuquerque Ara ́ujo. Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method. PRL, 32(1):56–68, 2011.\n\nHazel Doughty, Dima Damen, and Walterio W. Mayol-Cuevas. Who’s better? who’s best? pairwise\n\ndeep ranking for skill determination. In CVPR, pp. 6057–6066, 2018.\n\nHazel Doughty, Walterio W. Mayol-Cuevas, and Dima Damen. The pros and cons: Rank-aware\n\ntemporal attention for skill determination in long videos. In CVPR, 2019.\n\nYazan Abu Farha, Alexander Richard, and Juergen Gall. When will you do what? - anticipating\n\ntemporal occurrences of activities. In CVPR, pp. 5343–5352, 2018.\n\nAndrew Forney, Judea Pearl, and Elias Bareinboim. Counterfactual data-fusion for online reinforce-\n\nment learners. In ICML, pp. 1156–1164, 2017.\n\nRobert Geirhos, J ̈orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nat. Mach. Intell., 2(11):665–673, 2020.\n\nMadelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer.\n\nJohn Wiley & Sons, 2016.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nYash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual\n\nexplanations. arXiv preprint arXiv:1904.07451, 2019.\n\nChunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset of spatio-temporally localized atomic visual actions. In CVPR, pp. 6047–6056, 2018.\n\nMichael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool. Creating summaries\n\nfrom user videos. In ECCV, pp. 505–520, 2014.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, pp. 770–778, 2016.\n\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\n\nruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.\n\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In ICCV, pp. 8340–8349, 2021.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–\n\n1780, 1997.\n\nDe-An Huang, Joseph J Lim, Li Fei-Fei, and Juan Carlos Niebles. Unsupervised visual-linguistic\n\nreference resolution in instructional videos. In CVPR, pp. 2183–2192, 2017.\n\nDe-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, and Juan Carlos Niebles. Finding ”it”: Weakly-supervised reference-aware visual grounding in instructional videos. In CVPR, pp. 5948–5957, 2018.\n\nHaroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban, Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The thumos challenge on action recognition for videos “in the wild”. CVIU, 155: 1–23, 2017.\n\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning\n\nevents in videos. In ICCV, pp. 706–715, 2017.\n\nHilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and\n\nsemantics of goal-directed human activities. In CVPR, pp. 780–787, 2014.\n\nMatt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NeurIPS,\n\npp. 4066–4076, 2017.\n\nColin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolu-\n\ntional networks for action segmentation and detection. In CVPR, pp. 156–165, 2017.\n\nPeng Lei and Sinisa Todorovic. Temporal deformable residual networks for action segmentation in\n\nvideos. In CVPR, pp. 6742–6751, 2018.\n\nShi-Jie Li, Yazan AbuFarha, Yun Liu, Ming-Ming Cheng, and Juergen Gall. Ms-tcn++: Multi-stage\n\ntemporal convolutional network for action segmentation. TPAMI, 2020.\n\nChang Liu, Xinwei Sun, Jindong Wang, Haoyue Tang, Tao Li, Tao Qin, Wei Chen, and Tie-Yan Liu. Learning causal semantic representation for out-of-distribution prediction. NeurIPS, 34, 2021.\n\nDavid Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Scholkopf, and L ́eon Bottou. Dis-\n\ncovering causal signals in images. In CVPR, pp. 6979–6987, 2017.\n\nJonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy. What’s cookin’? interpreting cooking videos using text, speech and vision. arXiv preprint arXiv:1503.01558, 2015.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, pp. 2630–2640, 2019.\n\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, pp. 9879–9889, 2020.\n\nKarthika Mohan and Judea Pearl. Graphical models for processing missing data. arXiv preprint\n\narXiv:1801.03583, 2018.\n\nSuraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei. Causal induction from visual observations\n\nfor goal directed tasks. arXiv preprint arXiv:1910.01751, 2019.\n\nRameswar Panda, Niluthpol Chowdhury Mithun, and Amit K Roy-Chowdhury. Diversity-aware\n\nmulti-video summarization. TIP, 26(10):4712–4724, 2017.\n\nSunghyun Park, Seung-won Hwang, Fuxiang Chen, Jaegul Choo, Jung-Woo Ha, Sunghun Kim, and In AAAI, pp. 6883–\n\nJinyeong Yim. Paraphrase diversification using counterfactual debiasing. 6891, 2019.\n\nJudea Pearl. Causal inference in statistics: An overview. Stat. Surv., 3:96–146, 2009.\n\nJudea Pearl. The seven tools of causal inference, with reflections on machine learning. Commun.\n\nACM, 62(3):54–60, 2019.\n\nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A DePristo, Joshua V Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In NeurIPS, 2019.\n\nAlexander Richard, Hilde Kuehne, and Juergen Gall. Action sets: Weakly supervised action seg-\n\nmentation without ordering constraints. In CVPR, pp. 5987–5996, 2018a.\n\nAlexander Richard, Hilde Kuehne, Ahsan Iqbal, and Juergen Gall. Neuralnetwork-viterbi: A frame-\n\nwork for weakly supervised video learning. In CVPR, pp. 7386–7395, 2018b.\n\nFadime Sener and Angela Yao. Zero-shot anticipation for instructional activities. In ICCV, 2019.\n\nOzan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Unsupervised semantic parsing\n\nof video collections. In ICCV, pp. 4480–4488, 2015.\n\nBharat Singh, Tim K Marks, Michael Jones, Oncel Tuzel, and Ming Shao. A multi-stream bidirectional recurrent neural network for fine-grained action detection. In CVPR, pp. 1961–1970, 2016.\n\nYale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web\n\nvideos using titles. In CVPR, pp. 5179–5187, 2015.\n\nChen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations\n\nusing contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743, 2019.\n\nKaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. Unbiased scene graph\n\ngeneration from biased training. In CVPR, pp. 3716–3725, 2020.\n\nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. In CVPR, pp. 1207–1216, 2019.\n\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-\n\ntiotemporal features with 3d convolutional networks. In ICCV, pp. 4489–4497, 2015.\n\nPei Wang and Nuno Vasconcelos. Scout: Self-aware discriminant counterfactual explanations. In\n\nCVPR, pp. 8981–8990, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nTan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. Visual commonsense r-cnn.\n\nIn\n\nCVPR, pp. 10760–10770, 2020.\n\nWenguan Wang, Xiankai Lu, Jianbing Shen, David J Crandall, and Ling Shao. Zero-shot video\n\nobject segmentation via attentive graph neural networks. In ICCV, pp. 9236–9245, 2019.\n\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging\n\nvideo and language. In CVPR, pp. 5288–5296, 2016.\n\nXun Xu, Timothy Hospedales, and Shaogang Gong. Transductive zero-shot action recognition by\n\nword-vector embedding. IJCV, 123(3):309–333, 2017.\n\nZhiyu Yao, Yunbo Wang, Jianmin Wang, Philip Yu, and Mingsheng Long. Videodg: Generalizing\n\ntemporal relations in videos to novel domains. TPAMI, 2021.\n\nHuanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian Zhang, and Xiaokang Yang. Fine-grained\n\nvideo captioning for sports narrative. In CVPR, pp. 6006–6015, 2018.\n\nWeichen Zhang, Dong Xu, Wanli Ouyang, and Wen Li. Self-paced collaborative and adversarial\n\nnetwork for unsupervised domain adaptation. TPAMI, 2019.\n\nLuowei Zhou, Nathan Louis, and Jason J. Corso. Weakly-supervised video object grounding from\n\ntext by loss weighting and object interaction. In BMVC, pp. 50, 2018a.\n\nLuowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from\n\nweb instructional videos. In AAAI, 2018b.\n\nDimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and In CVPR, pp.\n\nJosef Sivic. Cross-task weakly supervised learning from instructional videos. 3537–3545, 2019.\n\nAppendix\n\nA RELATED WORK\n\nInstructional Action Understanding: With the explosion of video data on the Internet, learners can acquire knowledge from instructional videos to accomplish different tasks. Many instructional video datasets have been proposed for different goals, such as action detection datasets (Caba Heilbron et al., 2015; Idrees et al., 2017; Gu et al., 2018), video summarization datasets (De Avila et al., 2011; Gygli et al., 2014; Panda et al., 2017; Song et al., 2015), and video caption datasets (Xu et al., 2016; Yu et al., 2018; Miech et al., 2019; Krishna et al., 2017). To analyze instructional videos, diverse research fields are presented in recent years including action segmentation (Richard et al., 2018a;b; Miech et al., 2020; 2019; Sun et al., 2019), procedure segmentation (Zhou et al., 2018b), step localization (Alayrac et al., 2018; Zhukov et al., 2019), action anticipating (Sener & Yao, 2019; Farha et al., 2018), dense video caption (Das et al., 2013a), video grounding (Huang et al., 2017; 2018; Zhou et al., 2018a), and skill determination (Doughty et al., 2018; 2019). Despite the great progress on the in-distribution environment, it is a major challenge to deploy the trained models in the real-world environment.\n\nOut-of-Distribution Generalization: How to generalize the trained model into OOD environments is a key challenge in machine learning (Geirhos et al., 2020). A kind of widely-used methods are zero-shot recognition (ZSR) (Xu et al., 2017; Brattoli et al., 2020; Wang et al., 2019) (cross dataset evaluation), where the categories of samples in the testing set are apparently different from the training set. For instructional video, it is difficult to directly recognize unseen step categories. Another evaluation method is Unsupervised Domain Adaptation (UDA) (Busto et al., 2018; Zhang et al., 2019), which trains the model with the data and annotations in the source domain and target domain index (e.g. unannotated target data). Compared with UDA, OOD generalization further considers the setting without target domain information. VDG (Yao et al., 2021) is an OOD generation problem which evaluates the models by the videos with changing the scene or background. However, this setting is more suitable for conventional actions. On the contrast, the key domain changing in\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nthe instructional video is not the scene but the distribution shift of action steps in changing tasks. The detailed comparisons are summarized in Table 1.\n\nCausal Inference: Causal inference (Pearl, 2009; 2019) plays an important role in machine learning, which investigates causal effects of different variables. Recently, causal inference has been successfully applied to diverse fields including computer vision (Lopez-Paz et al., 2017; Wang et al., 2020), natural language processing (Park et al., 2019), and reinforcement learning (Nair et al., 2019; Forney et al., 2017), due to its ability for removing confounding bias (Tang et al., 2020; Wang et al., 2020), building explainable machine (Wang & Vasconcelos, 2020; Goyal et al., 2019), promoting fairness (Kusner et al., 2017; Chiappa, 2019), and recovering missing data (Mohan & Pearl, 2018). In this paper, we apply causal inference to mitigate the negative effect brought by confounding context bias to enhance the generalizability of IAU models.\n\nB PREREQUISITE: CAUSAL MODEL\n\nIn this section, we provide some prerequisites of causal model that may help to better understand our causal approach. More details could be found in (Glymour et al., 2016).\n\nOur task for video understanding is to predict the label of step based on the observation as P (Y |S). However, the context steps Z also affect the prediction. With the Bayes rule, we can re-write P (Y |S) as:\n\nP (Y |S) = ΣzP (Y |S, Z = z)P (Z = z|S),\n\n(6)\n\nwhich denotes that the likelihood P (Y |S) are influenced by P (Z = z|S). However, P (Z = z|S) is changed in the OOD setting. Now we use an example to show that P (Z = z|S) introduces the observation bias. In the video “Inflate bicycle tires” , current content S is “installing the nozzle” and the context Z is “using bicycle pump”. The content S and the context Z are always observed together in the training process and thus P (Z = using bicycle pump|S = installing the nozzle) is higher. It leads the model to predict higher probability P (Y |S = installing the nozzle) when observing the Z = using bicycle pump and vice verse. However, when we apply the model to analyze the OOD video “Inflate car tires”, where Z “using bicycle pump” is absent, the model may be confused and consequently give wrong prediction. Therefore, we aim at mitigating the influence from Z on S. Before that, we first introduce some definitions in the causal inference to help understand, and the proofs can be found in (Pearl, 2009; Glymour et al., 2016).\n\nDefinition 1 (Intervention) An intervention represents an external force that fixes a variable to a constant value (akin to random assignment if an experiment), and is denoted do(S = s)1, meaning that S is experimentally fixed to the value s.\n\nDefinition 2 (Confound) Consider a pair of variables S and Y . Mathematically, S and Y are confounded if\n\nP (Y |S) ̸= P (Y |do(S)).\n\n(7)\n\nDefinition 3 (Confounder) Z is a confounder (or S and Y are confounded by Z), if Z is associated with Y via paths in the causal graph that are not going through S.\n\nFor example, (a) in the causal model S ← Z → Y , Z is a confounder because (1) both S and Y are associated with it and (2) it is not on a causal path going through S and Y (3) nor a descendant of them; (b) in the causal model S → Z → Y , Z is not a confounder but a mediator (Z modifies the effect of S on Y ).\n\nDefinition 4 (The Backdoor Criterion) Given an ordered pair of variables (S, Y ) in a causal graph, a set of variables Z satisfies the backdoor criterion relative to (S, Y ) if (1) no node in Z is a descendant of S; (2) Z blocks every path between S and Y that contains an arrow into S. If variables Z satisfies the backdoor criterion for (S, Y ), then we can adjust the causal effect of S on Y with\n\nP (Y = y|do(S = s)) = ΣzP (Y = y|S = s, Z = z)P (Z = z).\n\n(8)\n\n1do-operator erases all the arrows that come into S to prevent any information about S from flowing in the\n\nnon-causal direction.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nIn other word, with this criterion we condition on Z such that we (1) block all spurious paths between S and Y ; (2) don’t disturb any directed paths from S to Y ; (3) create no new spurious paths.\n\nWith these definitions, we can show that how the backdoor adjustment can help for our OOD task. When we intervene S with the Back-Door Criterion, i.e. do(S), the link between S and Z is cut off so as the dependency. We formulate the model prediction process under the intervention:\n\nP (Y |do(S)) = ΣzP (Y |S, Z = z)P (Z = z),\n\n(9)\n\nwhere Z = z is independent from S. The only difference between Eq 6 and Eq 9 is that we change P (Z|S) to P (Z), which shows that Z is no longer affected by S. After intervention, when the model predicts from do(S) to the label Y , it fairly takes every z into consideration. Thus, the backdoor adjustment can mitigate the negative effect of the biased confounder Z.\n\nFigure 6: An example of how to generate OOD tasks in GAIN. Given two in-distribution videos of the training sets, we can generate a new OOD task by reassembling the steps of original videos. Best viewed in color.\n\nC MORE DETAILS OF GAIN\n\nC.1 A DETAILED EXAMPLE\n\nTo show how to construct the OOD task in GAIN, in Fig. 6, we display a detailed example about “Hang Up Clock” which can be reassembled by the steps in the training tasks “Hang Up Curtain” and “Install Wood flooring”. Specifically, the “Hang up curtain” task consists of three steps including {drill, install shelves, hang up}, and the “Install wood flooring” task is composed of {cut raw boards, f it on boards, knock in nails}. Our collected OOD task “Hang up clock” contains the “drill” and “hang up” steps in the ‘’Hang up curtain” task and the “knock in nails” step in another.\n\nC.2 TASKS & STEPS\n\nIn order to present more details of our GAIN dataset, we show all the selected tasks with their corresponding steps in Table 7, 8, 9 and Table 10. We display these two tables in the end of Appendix because of the typesetting. The GAIN dataset consists of 1,231 instructional videos related to 116 unseen tasks. Each task in GAIN contains 2∼24 videos with an average of 10 videos. We annotate 6382 action segments in GAIN with an average of 5 steps in each video.\n\nC.3 SAMPLE DISTRIBUTIONS\n\nFig. 7 and Fig. 8 illustrate the sample distribution of GAIN-C and GAIN-B. Statistically, GAIN-C contains 1,000 videos of 100 unseen tasks with a length of 41.6 hours in total, where 5238 segments are annotated. The GAIN-B split includes 231 videos of 16 OOD tasks with an average length of 2 minutes and 30 seconds. These tasks consist of 20 fine-grained action categories.\n\n15\n\nHang up curtain: { drill, install shelves, hang up }Install wood flooring: { cut raw boards, fit on boards, knock in nails}Hang up clock: { drill, knock in nails, hang up }Training:Evaluation:Published as a conference paper at ICLR 2023\n\nFigure 7: The sample distributions of GAIN-C.\n\nFigure 8: The sample distributions of GAIN-B.\n\nC.4 DURATION STATISTICS\n\nFig. 9 illustrates the duration statistics in both video-level and step-level of our GAIN dataset, where the average length of videos is 2 minutes and 30 seconds, and the average length of steps is 12 seconds. Totally, the GAIN dataset contains OOD videos of 51.2 hours for generalizability evaluation.\n\nC.5 VIEWS ANALYSIS\n\nIn Fig. 10, we display the number of views on YouTube across 100 tasks in GAIN-C, which can demonstrate that with the basic principles mentioned in section 3.2, the unseen tasks meet the need of website viewers statistically. We grab the number of views from YouTube by utilizing the Python module youtubesearchpython. We form a query with “how to” preceding the task name (e.g. how to paint the wall) to search for YouTube instructional videos related to the tasks. Then for each task, we only extract the first 20 results and sum up the numbers of views to represent the popularity of this task.\n\nWith approximately 767.9M views, “Make popsicle” becomes the most-viewed task and the last one “Dissolve effervescent tablet” still obtains 391.8K views. The number of views per task is 44.5M on average and the average number of views for the counted videos is 2.2M. The statistical results above prove that the selected tasks in our GAIN dataset are all common daily tasks and satisfy the\n\n16\n\nMakeJellyScaldedShrimpMakeKimchiReverseParkingRoastedChickenWingsChangeTheRefillOfGelPenMakeInstantCoffeeUseNasalSprayRefuelMotorbikeChangeFilterPaintTheWallMakePizzaDoughMakeBananaMilkHighJumpUnloadSpareTireRoastSweetPotatoReplaceScreenProtectorOnPadApplyLubricationOnChainChangeFluorescentTubeApplyHardWaxOilVaccinateCleanWindowChangeViolinStringMakeMashedPotatoesPackBookCoverMakeGarlicBreadFryMeatShredFriedDumplingAssembleBookshelfMakeCoconutJuiceMakePlasterModelPlaySnowBoardMeatWithScrambledEggEggFriedRiceWashCarOpenTheBeerDissolveEffervescentTabletMakePapercutInstallPointersPastePosterMakeMixedJuiceMakeCakeMakePaperCraneMakePopsicleMakeWatermelonJuiceMakeRobberSealRemovePhoneScreenMakeNoodlePeelOrangeCutPapayaRemoveTheShellOfScallopMakeHoneyLemonUseTeakettleMakeMilkTeaChangeBatteryShotPutFryChickenCookDumplingMakeBaoziAssembleDeskCutCarambolaApplyLotionMakeSoyMilkMakeMilkShakeWashClothesUseBalancePumpUpTheBallCutGingerCutWatermelonWashCatPerformVanishingCoinTrickPourACupOfWineHangAClockPackGoodsPlayBowlingChangeDoorbellApplyGlueChangeMemoryCarOnGameConsoleCookHerbalMedcineRoastChickenInstallPaintingOnWallCookPotatoShredIntravenousInjectionWashThePotWashFacePumpUpCarTireGetEarsPiercedCleanCatteryWaterBoilMeatMakeBreadDyeHairMakeWatermelonHatPlayMusicWithCDPlayerPasteCoupletUseWashingMachineApplyLubricationOnDoorUseDryerHangClothesOutPlayClawMachineHaveAPicnic0510152025#SampleMakeJellyMakeBananaMilkMakeInstantCoffeeMakeWatermelonJuiceCutCarambolaCutPapayaMakeMixedJuiceMakeSoyMilkMakeMilkTeaPeelOrangeCutGingerFryMeatShredMakeCoconutJuiceCookPotatoShredMakeMilkShakeMeatWithScrambledEgg0510152025#SamplePublished as a conference paper at ICLR 2023\n\nFigure 9: The duration statistics in the video-level (left) and step-level (right) of GAIN.\n\nFigure 10: The views distributions of tasks in GAIN-C on YouTube.\n\ndemand of website viewers. The learning enthusiasm for diverse tasks reveals the practical value of the generalizability of IAU models.\n\nC.6 DOMAIN ANALYSIS\n\nFig. 11 shows the domain distribution (defined in COIN) of this split. GAIN-C covers 12 semantic domains, which demonstrates the rich diversity of our GAIN dataset.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 11: The domain distribution of GAIN-C.\n\nTable 5: Parameter analysis on the learning rate on COIN/GAIN-C.\n\nLearning Rate\n\nMethods\n\nFrame Accuracy\n\nCOIN\n\nGAIN-C\n\n5e-4\n\n1e-3\n\n2e-3\n\nMS-TCN++ Causal MS-TCN++ MS-TCN++ Causal MS-TCN++ MS-TCN++ Causal MS-TCN++\n\n62.1 64.0 64.7 65.5 65.5 65.6\n\n49.0 52.3 54.3 56.2 51.8 56.0\n\nTable 6: Parameter analysis on different relative sizes of reassembled videos on Breakfast/GAIN-B with frame accuracy. Method Step # Video # Accuracy\n\nCausal ED-TCN 1.5x 1.x 44.5/30.7\n\nEDTCN -\n- 42.4/21.3\n\n1.x 1.x 44.5/35.4\n\n0.5x 1.x 44.5/31.1\n\n1.x 0.5x 43.9/29.1\n\n1.x 1.5x 42.1/37.6\n\nD MORE EXPERIMENTAL ANALYSIS\n\nD.1 PARAMETER ANALYSIS\n\nWe conduct experimental analysis on both COIN/GAIN-C to investigate the effect of hyperparameter learning rate with MS-TCN++(Li et al., 2020). As shown in Table5, with a learning rate of 5e − 4 the model performs unfavorable results on both two datasets, while an increased learning rate, i.e. 1e − 3 or 2e − 3, can notably improve its performance on COIN(+2.3% and +2.5%) as well as the performance on the out-of-distribution tasks (+7.5% and +7.1%).\n\nWe also conduct experiments of different relative sizes of reassembled videos on Breakfast/GAIN-B and the results (frame accuracy) are shown in Table 6. For the number of steps, 1.0×, 0.5×, and 1.5× of Causal ED-TCN denote that we use 1 times, 0.5 times, and 1.5 times step numbers as that of the original ED-TCN. So as the number of videos. Our methods with different setting consistently outperform the baseline on both in-distribution and OOD scenario. Besides, we find that with the same number of augmented steps and videos the model achieves better overall performance, so we adapt this setting for all experiments.\n\nD.2 VISUALIZATION RESULTS\n\nIn section 5.2, we have visualized the ground-truth annotations and the action segmentation results. Due to the limitation of space, we only visualized the results produced by MS-TCN++ method and its causal version. Now in Fig. 12, we illustrate the visualization for different methods including LSTM, ED-TCN, TResNet and MS-TCN++, to demonstrate the effectiveness of our approach.\n\n18\n\nNursing and CareVehicleLeisure and PerformanceGadgetsElectrical ApplianceFurniture and DecorationScience and CraftPets and FruitDrink and SnackDishSportHouseworkPublished as a conference paper at ICLR 2023\n\nFigure 12: Visualization of action segmentation results. The video is associated with the task “Scalded shrimp”. Best viewed in color.\n\nFigure 13: Visualization of a failure case. The video is associated with the task “Make Mixed Juice”. Best viewed in color.\n\nWe show results of the baseline methods and our corresponding causal ones on the task “Scalded shrimp” in GAIN-C. The consistent improvements for different baselines on out-of-distribution tasks indicate that our causal intervention promotes the generalizability of models.\n\nAdditionally, we show a failure case of our approaches in Fig. 13. We analyze the underlying insights from the cases that the causal-based method has lower performance than the baseline, such as the video ”Make Mixed Juice” in the GAIN-C dataset. Take a closer look, we find that there are some strong step dependencies, such as ”juice the fruit” and ”pour the juice”. In this situation, context bias has positive effects on the evaluation. Thus, we got the following insights, despite the better performance on average, our method encourages step independency, which has negative effects on the examples where strong step dependencies occur for both training and testing data. It is reasonable since these steps are in-distribution samples where the context bias has a positive effect, e.g., ”juice the fruit” and ”pour the juice” are successive steps in both COIN and GAIN-C, so the prediction with the concurrence of them is better.\n\n19\n\nGround truthTResNetCausal TResNetMS-TCN++Causal MS-TCN++Scaldedshrimp: { prepare and boil water, pour into the water, pour out, remove the shrimp shell }LSTMCausal LSTMED-TCNCausal ED-TCNGround truthMS-TCN++Causal MS-TCN++Make Mixed JuicePublished as a conference paper at ICLR 2023\n\nTable 7: Tasks and the corresponding steps in GAIN-C. Steps apply glue to the wall and wallpaper, paste and level the wallpaper apply the cleaning agent with towel evenly, wipe off the cleaning agent pour some glue to the face, wipe the glue to a layer\n\nTasks MakeMixedJuice\n\nMakePaperCrane\n\nMakeNoodles\n\nSteps cut ingredients, juice the oranges, pour the orange juice into the cup add some water to the tea, mix the raw materials, knead the dough, cut the flesh fold or bent paper, paint on the paper\n\nTasks ApplyGlue\n\nApplyHardWaxOil\n\nApplyLotion\n\nApplyLubrication OnChain\n\nApplyLubrication OnDoor\n\napply the lubricant on the lock, wipe off the redundant lubricant, check the old chain\n\nMakePapercut\n\napply the lubricant on the lock, wipe off the redundant lubricant\n\nMakePizzaDough\n\nAssembleBookshelf\n\nassemble the frame, boards, install vertical boards\n\ninstall horizontal\n\nMakePlasterModel\n\nAssembleDesk\n\ninstall stand of the seat, install legs on the bed\n\nMakePopsicle\n\nChangeBattery\n\nChangeDoorbell\n\nopen the back cover, replace the battery, install the back cover and waterproof ring screw off the screws used to fix the switch, reset the switch and screw on the screws used to fix the switch\n\nMakeRobberSeal\n\nMakeSoyMilk\n\ndraw an outline, cut along the edges, fold or bent paper\n\nmix the raw materials, add some water to the tea, rub and drag the materials, cut the flesh mix the raw materials, soak them in water, wait for the candle until concretion, remove the gill, add raw materials, add some water to the tea pour the ingredients into the bowl, put the candle wick into a vessel, put the melted soap block into the vessel, take out after freezing, mix the raw materials draw an outline, carve along the outline, remove the peel soak and wash the rice, put yogurt, honey and other ingredients into the juicer, add some water to the tea, mix raw materials, pour the tea into the vessel, juice the oranges clean up the interior of thepumpkin, carve along the outline, draw an out line\n\nChangeFilter\n\nChangeFluorescentTube\n\nChangeMemoryCard OnGameConsole\n\nChangeTheRefillOf GelPen\n\nChangeViolinString\n\nCleanCattery\n\nCleanWindow\n\nCookDumpling\n\nCookHerbalMedcine\n\nCookPotatoShred\n\ntake out the old filter, remove the cap of the new filter, install the new filter, rinse the dish take out the old bulb, install the new bulb, remove the light shell/housing/support, install the light shell/housing/support\n\nuse the needle to open the SIM card slot, put the SIM card into the SIM card slot, press the SIM card slot back\n\nremove cap, put lead into the pen, buckle the cap cut off and remove the old string, fix the new string on the lower part of the guitar, fix the new string on the head of the guitar, adjust the tightness of the new string remove the toy and paper bed from the hamster cage, clean toys and hamster cages, move the toy and paper bed into the hamster cage apply the cleaning agent with towel evenly, wipe off the cleaning agent knead the dough, flatten the dough, add ingredients into cone, knead together, mix raw materials, soak them in water, load the dish prepare and boil water, pour the noodles into the water and stir, filtrate with a filter, pour into a glass peel, cut into strips and pieces, put in the oil to fry\n\nMakeWatermelonHat\n\nMakeWatermelonJuice\n\ncut oranges, juice the oranges, pour the orange juice into the cup\n\nMeatWithScrambledEgg\n\npour the egg into the bowl, stir the egg, prepare meat, cut the flesh, mix the raw materials, put in the oil to fry\n\nOpenTheBeer\n\nopen the bottle carefully\n\nPackBookCover\n\nmeasure the size of the packing paper, cut the packing paper, fold or bent paper\n\nPackGoods\n\nremove cap, put in the battery, close cover\n\nPaintTheWall\n\nPasteCouplet\n\nPastePoster\n\ndip the glue, apply glue to the wall and wallpaper apply glue to the wall and wallpaper, paste and level the wallpaper\n\napply glue to the wall and wallpaper, paste and level the wallpaper\n\nPeelOrange\n\nremove the peel, cut the flesh\n\nCutCarambola\n\ncut off the edge, cut the flesh\n\nCutGinger CutPapaya\n\nremove the peel, slice the pulp peel, cut in half, dig out the seeds with spoon, slice the pulp\n\nCutWatermelon\n\nDissolveEffervescent Tablet\n\ncut off the edge, peel, cut in half, slice the pulp\n\nadd some ingredients to the tea, add some water to the vessel\n\nPerformVanishing CoinTrick\n\nPlayBowling PlayClawMachine\n\nPlayMusicWith CDPlayer\n\nPlaySnowBoard\n\nshow the glass to the audience, block out the glass, show the vanished glass pre-swing, push curling insert money into the vending machine, press the corresponding button, take out the goods\n\ntake out the laptop CD drive, press the SIM card slot back, close the fuel tank cap\n\nski down from the hill, ski up from the hill, rise to the sky\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nTasks DyeHair\n\nEggFriedRice\n\nFriedDumpling\n\nFryChicken\n\nFryMeatShred\n\nGetEarsPierced\n\nHangAClock\n\nHangClothesOut\n\nHaveAPicnic\n\nHighJump\n\nInstallPaintingOnWall\n\nInstallPointers\n\nIntravenousInjection\n\nMakeBananaMilk\n\nMakeBaozi\n\nMakeBread\n\nMakeCake\n\nMakeCoconutJuice\n\nMakeGarlicBread\n\nMakeHoneyLemon\n\nMakeInstantCoffee\n\nMakeJelly\n\nTasks MakeKimchi\n\nMakeMashedPotatoes\n\nMakeMilkShake\n\nMakeMilkTea\n\nTasks PourACupOfWine\n\nTable 8: Tasks and the corresponding steps in GAIN-C. Steps apply the shampoo or hair conditioner, scratch the hair carefully, wash the body wash away, make the hair dry take out some rice, soak and wash the rice, pour the egg into the bowl, stir the egg, mix raw materials, put in the oil to fry knead the dough, flatten the dough, cut the flesh, add ingredients into cone, knead together, put in the oil to fry, mix the raw materials, add some water to the tea prepare seasonings and side dishes, prepare meat, cut the flesh, fry or gril\n\nRefuelMotorbike\n\nPumpUpTheBall\n\nPumpUpCarTire\n\nprepare meat, cut the flesh, put in the oil to fry, load the dish\n\nRemovePhoneScreen\n\nSteps open the bottle carefully, pour in after mix it\n\nscrew off the valve cap and open the valve, install the air nozzle, remove the air nozzle, tighten the valve and screw on the valve cap install the air nozzle, pump up to the tire, remove the air nozzle\n\nopen the fuel tank cap, insert oil gun in the car, pullthe oil gun out, close the fuel tank cap unscrew the screws used to fix the screen, pull out the screen connector and remove the screen\n\ndraw lines to mark the hole, find the position of the hole, drill with an electric drill\n\ndrill in the wall, knock in the nails, hang up curtains wrap the hair by hands, hang the ironed clothes clean up the ground, lay the cushion evenly, load the dish\n\nbegin to run up, begin to jump up, fall to the ground drill in the wall, knock in the nails, paste and level the wallpaper let the flat side of the new needle towards the jack and insert the new needle, screw on the screw tie the tourniquet, disinfect, pull out the needle and press with cotton peel, cut into strips and pieces, add milk, shake and juice, pour the orange juice into the cup, put strawberries and other fruits into the juicer add ingredients into cone, knead together\n\nRemoveTheShellOf Scallop\n\nReplaceScreenProtector OnPad\n\nReverseParking\n\nRoastChicken\n\nRoastedChickenWings\n\nRoastSweetPotato\n\nScaldedShrimp\n\nShotPut\n\ncut oranges, take out the shell, rinse the pot, remove the gill, open up the cover\n\nwipe the screen, paste protector on the screen drive the car forward, drive the car backward, adjust front and back position prepare seasonings and side dishes, remove the intestines and blood vessels, brush sauce or sprinkle seasoning, bake pizza soak them in water, add raw materials, mix the raw materials, bake pizza clean the pumpkin, fry or roast or braise, cut the bread, peel prepare and boil water, pour the noodles into the water and stir, remove the shrimp shell, pour the cooked noodles pre-swing, throw the hammer out\n\nUnloadSpareTire\n\nunscrew the screw, remove the tire\n\nUseBalance\n\nknead the dough, run the toaster and adjust, take out the slice of bread\n\nUseDryer\n\npour the egg into the bowl, add raw materials, mix raw materials, put materials into mold, run the toaster and adjust, take out chocolate dig out the seeds with spoon, put the ingredients into the can, pour the orange juice into the cup, put strawberries and other fruits into the juicer, put yogurt, honey and other ingredients into the juicer, shake and juice mix the raw materials, cut the bread, put the filler on the bread slice, put a slice of bread in, run the toaster and adjust, take out the slice of bread cut ingredients, add some water to the tea, put the ingredients into the can, mix and pickle add tea powder, brew tea and stir, add some ingredients in the coffee, mix the raw materials pour the ingredients into the bowl, stir the mixture, take out chocolate, cut into strips and pieces, put materials into mold\n\nUseNasalSpray\n\nUseTeakettle\n\nUseWashingMachine\n\nVaccinate\n\nWashCar\n\nWashCat\n\nput the sample to be measured on the balance, put the weight until the balance is balanced put the clothes neatly on a ironing table, use a hair dryer to blow hot wall, flip the clothes repeatly wipe nose, fill a nostril with saline and do the same to the other nostril, shake and stir, remove cap\n\npour the tea into the vessel, heat the teapot and wash the cup, close up the cover\n\nopen the fuel tank cap, add some cleaner to clean and wet the lenses and take out the lenses, close up the cover\n\nfill the injection head, disinfect the injecting place, inject to the muscular, pull out the needle and press add detergent and make bubble, clean the surface, wipe off the cleaning agent\n\nuse the body wash, wash the body wash away\n\nTasks WashClothes\n\nTable 9: Tasks and the corresponding steps in GAIN-C. Steps cut into strips and pieces, mix and pickle, clean up and soak, put the ingredients into the can, mix raw materials, add some water to the tea, shake and juice peel, cut potato into strips, soak them in water, add raw materials, mix raw materials, prepare and boil water add milk, shake and juice, pour the orange juice into the cup put yogurt, honey and other ingredients into the juicer, mix the raw materials, pour in after mix it, add ice cubes\n\nWaterBoilMeat\n\nWashThePot\n\nWashFace\n\nSteps add detergent and make bubble, clean toys and hamster cages, wrap the hair by hands\n\nwet and wash hands, apply cleansing milk to the face, wipe up the face\n\nadd detergent and make bubble, flush and wash the interior, scrub the toilet interior prepare and boil water, soak them in water, load the dish\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTable 10: Tasks and the corresponding steps in GAIN-B.\n\nTasks CookPotatoShred CutCarambola CutGinger CutPapaya FryMeatShred MakeBananaMilk MakeCoconutJuice MakeInstantCoffee MakeJelly MakeMilkShake MakeMilkTea MakeMixedJuice MakeSoyMilk MakeWatermelonJuice MeatWithScrambledEgg PeelOrange\n\nSteps peel fruit, cut orange, pour oil cut orange peel fruit, cut fruit peel fruit, cut fruit cut orange, pour oil, take plate peel fruit, cut orange, pour milk, squeeze orange, pour juice, put fruit to bowl put fruit to bowl, pour juice, squeeze orange add teabag, stir coffee pour flour, stir dough, cut bun, put fruit to bowl pour milk, squeeze orange, pour juice pour milk, stir tea, pour water, put fruit to bowl cut orange, squeeze orange, pour juice stir fruit, pour milk, pour water, stir tea, squeeze orange cut orange, squeeze orange, pour juice pour egg into pan, stir fry egg, cut orange, stir fruit, pour oil peel fruit, cut orange\n\n22",
    "reference": "# Summary Of The Paper\n\nThe authors introduce the benchmarking evaluation datasets GAIN constructed from the real world, whose videos contain steps from consistent categories with either COIN or Breakfast datasets but are out-of-distribution and belong to non-overlapping task categories. Furthermore, authors make attempts to alleviate performance drop on GAIN by utilizing causal inference and Monte Carlo method to intervene training samples by disassembling and reassembling.\n\n# Strength And Weaknesses\n\nStrength:\n1. The GAIN dataset is constructed with great effort. The basic principles to construct datasets are well-designed and in line with the definition of IAU generalizability. Also, the collection process of GAIN is in detailed description. As reported in the experiment section, performance on GAIN-C and GAIN-B undergoes an obvious drop compared to COIN and Breakfast, which validate the effectiveness of benchmarking model’s generalizability on GAIN. \n2. Formulation of the causal graph is also clear. The proposed intervention method has solid theoretical proof.\n\nWeakness\n1. Action segmentation papers always report metrics of both framewise accuracy and segmental edit distance and the segmental F1 score. It would be nice to see more metrics reported.\n2. Models used in the experiment section are all pre-trained ones. I was wondering whether models trained from scratch on datasets (there have been multiple recent works focusing on this) still face a huge drop on GAIN. Also, whether they can be improved by causal-based methods is also unknown.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-motivated as well as well-written. Each section of the paper is easy to follow. Visualizations and tables are clear and informative.\n\n# Summary Of The Review\n\nThe paper is well motivated and the proposed GAIN dataset has valuable contribution to the future instructional action understanding research. Therefore, my rating to this paper is \"6: marginally above the acceptance threshold\".\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nMULTI-SKILL MOBILE MANIPULATION FOR OBJECT REARRANGEMENT\n\nJiayuan Gu1, Devendra Singh Chaplot2, Hao Su1, Jitendra Malik2,3 1UC San Diego, 2Meta AI Research, 3UC Berkeley\n\nABSTRACT\n\nWe study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines.\n\n1\n\nINTRODUCTION\n\nBuilding AI with embodiment is an important future mission of AI. Object rearrangement (Batra et al., 2020) is considered as a canonical task for embodied AI. The most challenging rearrangement tasks (Szot et al., 2021; Ehsani et al., 2021; Gan et al., 2021) are often long-horizon mobile manipulation tasks, which demand both navigation and manipulation abilities, e.g., to move to certain locations and to pick or place objects. It is challenging to learn a monolithic RL policy for complex long-horizon mobile manipulation tasks, due to challenges such as high sample complexity, complicated reward design, and inefficient exploration. A practical solution to tackle a long-horizon task is to decompose it into a set of subtasks, which are tractable, short-horizon, and compact in state or action spaces. Each subtask can be solved by designing or learning a skill, so that a sequence of skills can be chained to complete the entire task (Lee et al., 2018; Clegg et al., 2018; Lee et al., 2019; 2021). For example, skills for object rearrangement can be picking or placing objects, opening or closing fridges and drawers, moving chairs, navigating in the room, etc.\n\nAchieving successful object rearrangement using this modular framework requires careful subtask formulation such that skills trained for these subtasks can be chained together effectively. We define three desirable properties for skills to solve diverse long-horizon tasks: achievability, composability, and reusability. Note that we assume each subtask is associated with a set of initial states. Then, achievability quantifies the portion of initial states solvable by a skill. A pair of skills are composable if the initial states achievable by the succeeding skill can encompass the terminal states of the preceding skill. This encompassment requirement is necessary to ensure robustness to mild compounding errors. However, trivially enlarging the initial set of a subtask increases learning difficulty and may lead to many unachievable initial states for the designed/learned skill. Last, a skill is reusable if it can be directly chained without or with limited fine-tuning (Clegg et al., 2018; Lee et al., 2021). According to our experiments, effective subtask formulation is critical though largely overlooked in the literature.\n\n1Project website: https://sites.google.com/view/hab-m3 2Codes: https://github.com/Jiayuan-Gu/hab-mobile-manipulation\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) Method Overview\n\n(b) The Home Assistant Benchmark\n\nFigure 1: 1a provides an overview of our multi-skill mobile manipulation (M3) method. The inactive part of the robot is colored gray. Previous approaches exclusively activate either the mobile platform or manipulator for each skill, and suffer from compounding errors in skill chaining given limited composability of skills. We introduce mobility to manipulation skills, which effectively enlarges the feasible initial set, and a region-goal navigation reward to facilitate learning the navigation skill. 1b illustrates one task (SetTable) in the Home Assistant Benchmark (Szot et al., 2021), where the robot needs to navigate in the room, open the drawers or fridge, pick multiple objects in drawers or fridge and place them on the table. Best viewed in motion at the project website1.\n\nIn the context of mobile manipulation, skill chaining poses many challenges for subtask formulation. For example, an imperfect navigation skill might terminate at a bad location where the target object is out of reach for a stationary manipulation skill (Szot et al., 2021). To tackle such “hand-off” problems, we investigate how to formulate subtasks for mobile manipulation. First, we replace stationary (fixed-base) manipulation skills with mobile counterparts, which allow the base to move when the manipulation is undertaken. We observe that mobile manipulation skills are more robust to compounding errors in skill chaining, and enable the robot to make full use of its embodiment to better accomplish subtasks, e.g., finding a better location with less clutter and fewer obstacles to pick an object. We emphasize how to generate initial states of manipulation skills as a trade-off between composability and achievability in Sec 4.1. Second, we study how to translate the start of manipulation skills to the navigation reward, which is used to train the navigation skill to connect manipulation skills. Note that the goal position in mobile manipulation plays a very different role from that in point-goal (Wijmans et al., 2019; Kadian et al., 2020) navigation. On the one hand, the position of a target object (e.g., on the table or in the fridge) is often not directly navigable; on the other hand, a navigable position close to the goal position can be infeasible due to kinematic and collision constraints. Besides, there exist multiple feasible starting positions for manipulation skills, yet previous works such as Szot et al. (2021) train the navigation skill to learn a single one, which is selected heuristically and may not be suitable for stationary manipulation. Thanks to the flexibility of our mobile manipulation skills, we devise a region-goal navigation reward to address those issues, detailed in Sec 4.2.\n\nIn this work, we present our improved multi-skill mobile manipulation method M3, where mobile manipulation skills are chained by the navigation skill trained with our region-goal navigation reward. It achieves an average success rate of 63% on 3 long-horizon mobile manipulation tasks in the Home Assistant Benchmark (Szot et al., 2021), as compared to 50% for our best baseline. Fig 1 provides an overview of our method and tasks. Our contributions are listed as follows:\n\n1. We study how to formulate mobile manipulation skills, and empirically show that they are more\n\nrobust to compounding errors in skill chaining than stationary counterparts;\n\n2. We devise a region-goal navigation reward for mobile manipulation, which shows better perfor-\n\nmance and stronger generalizability than the point-goal counterpart in previous works;\n\n3. We show that our improved multi-skill mobile manipulation pipeline can achieve superior performance on long-horizon mobile manipulation tasks without bells and whistles, which can serve as a strong baseline for future study.\n\n2\n\nStationary manipulation skillPoint-goal navigation skillRegion-goalnavigation skillMobilemanipulation skillInitial statesTerminal statesInitial statesTerminal statesTerminal statesInitial statesOur approachPrevious approachesInitial statesTerminal statesPublished as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\n2.1 MOBILE MANIPULATION\n\nRearrangement (Batra et al., 2020) is “to bring a given physical environment into a specified state”. We refer readers to Batra et al. (2020) for a comprehensive survey. Many existing RL tasks can be considered as instances of rearrangement, e.g., picking and placing rigid objects (Zhu et al., 2020; Yu et al., 2020) or manipulating articulated objects (Urakami et al., 2019; Mu et al., 2021). However, they mainly focus on stationary manipulation (Urakami et al., 2019; Zhu et al., 2020; Yu et al., 2020) or individual, short-horizon skills (Mu et al., 2021). Recently, several benchmarks like Home Assistant Benchmark (HAB) (Szot et al., 2021), ManipulaTHOR (Ehsani et al., 2021) and ThreeDWorld Transport Challenge (Gan et al., 2021), are proposed to study long-horizon mobile manipulation tasks. They usually demand that the robot rearranges household objects in a room, requiring exploration and navigation (Anderson et al., 2018; Chaplot et al., 2020) between interacting with objects entirely based on onboard sensing, without any privileged state or map information.\n\nMobile manipulation (RAS, 2022) refers to “robotic tasks that require a synergistic combination of navigation and interaction with the environment”. It has been studied long in the robotics community. Ni et al. (2021) provides a summary of traditional methods, which usually require perfect knowledge of the environment. One example is task-and-motion-planning (TAMP) (Srivastava et al., 2014; Garrett et al., 2021; 2020). TAMP relies on well-designed state proposals (grasp poses, robot positions, etc.) to sample feasible trajectories, which is computationally inefficient and unscalable for complicated scenarios.\n\nLearning-based approaches enable the robot to act according to visual observations. Xia et al. (2021) proposes a hierarchical method for mobile manipulation in iGibson (Xia et al., 2020), which predicts either a high-level base or arm action by RL policies and executes plans generated by motionplanning to achieve the action. However, the arm action space is specially designed for a primitive action pushing. Sun et al. (2022) develops a real-world RL framework to collect trash on the floor, with separate navigation and grasping policies. Ehsani et al. (2021); Ni et al. (2021) train an end-toend RL policy to tackle mobile pick-and-place in ManipulaTHOR (Ehsani et al., 2021). However, the reward function used to train such an end-to-end policy usually demands careful tuning. For example, Ni et al. (2021) shows that a minor modification (a penalty for disturbance avoidance) can lead to a considerable performance drop. The vulnerability of end-to-end RL approaches restricts scalability. Most prior works in both RL and robotics separate mobile the platform and manipulator, to “reduce the difficulty to solve the inverse kinematics problem of a kinematically redundant system” (Sereinig et al., 2020; Sandakalum & Ang Jr, 2022). Wang et al. (2020) trains an end-to-end RL policy based on the object pose and proprioception to simultaneously control the base and arm. It focuses on picking a single object up in simple scenes, while our work addresses long-horizon rearrangement tasks that require multiple skills.\n\nSzot et al. (2021) adopts a different hierarchical approach for mobile manipulation. It uses taskplanning (Fikes & Nilsson, 1971) to generate high-level symbolic goals, and individual skills are trained by RL to accomplish those goals. It outperforms the monolithic end-to-end RL policy and the classical sense-plan-act robotic pipeline. It is scalable since skills can be composited to solve different tasks, and benefit from progress in individual skill learning (Yu et al., 2020; Mu et al., 2021). Moreover, different from other benchmarks, the HAB features continuous motor control (base and arm), interaction with articulated objects (opening drawers and fridges), and complicated scene layouts. Thus, we choose the HAB as the platform to study long-horizon mobile manipulation.\n\n2.2 SKILL CHAINING FOR LONG-HORIZON TASKS\n\nSzot et al. (2021) observes that sequentially chaining multiple skills suffers from “hand-off” problems, where a preceding skill terminates at a state that the succeeding skill has either never seen during training or is infeasible to solve. Lee et al. (2018) proposes to learn a transition policy to connect primitive skills, but assumes that such a policy can be found through random exploration. Lee et al. (2021) regularizes the terminal state distribution of a skill to be close to the initial set of the following skill, through a reward learned with adversarial training. Most prior skill chaining methods focus on fine-tuning learned skills. In this work, we instead focus on subtask formulation for skill chaining, which directly improves composability and reusability without additional computation.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\n3 PRELIMINARY\n\n3.1 HOME ASSISTANT BENCHMARK (HAB)\n\nThe Home Assistant Benchmark (HAB) (Szot et al., 2021) includes 3 long-horizon mobile manipulation rearrangement tasks (TidyHouse, PrepareGroceries, SetTable) based on the ReplicaCAD dataset, which contains a rich set of 105 indoor scene layouts. For each episode (instance of task), rigid objects from the YCB (Calli et al., 2015) dataset are randomly placed on annotated supporting surfaces of receptacles, to generate clutter in a randomly selected scene. Here we provide a brief description of these tasks.\n\nTidyHouse: Move 5 objects from starting positions to goal positions. Objects and goals are located in open receptacles (e.g., table, kitchen counter) rather than containers. Complex scene layouts, diverse receptacles, dense clutter all pose challenges. The task implicitly favors collision-free behavior since a latter target object might be knocked out of reach when a former object is moved by the robot. PrepareGroceries: Move 2 objects from the fridge to the counters and move an object from the counter to the fridge. The fridge is fully open initially. The task requires picking and placing an object in a cluttered receptacle with restricted space. SetTable: Move a bowl from a drawer to a table, and move a fruit from the fridge to the bowl on the table. Both the drawer and fridge are closed initially. The task requires interaction with articulated objects as well as picking objects from containers.\n\nAll the tasks demand onboard sensing instead of privileged information (e.g., ground-truth object positions and navigation map). All the tasks use the GeometricGoal (Batra et al., 2020) specification (s0, s∗), which describes the initial 3D (center-of-mass) position s0 of the target object and the goal position s∗. For example, TidyHouse is specified by 5 tuples {(si\n\n∗)}i=1...5.\n\n0, si\n\n3.2 SUBTASK AND SKILL\n\nIn this section, we present the definition of subtask and skill in the context of reinforcement learning. A long-horizon task can be formulated as a Markov decision process (MDP) 1 defined by a tuple (S, A, R, P, I) of state space S, action space A, reward function R(s, a, s′), transition distribution P (s′|s, a), initial state distribution I. A subtask ω is a smaller MDP (S, Aω, Rω, P, Iω) derived from the original MDP of the full task. A skill (or policy), which maps a state s ∈ S to an action a ∈ A, is learned for each subtask by RL algorithms.\n\nSzot et al. (2021) introduces several parameterized skills for the HAB: Pick, Place, Open fridge, Close fridge, Open drawer, Close drawer, Navigate. Each skill takes a 3D position as input, either s0 or s∗. See Appendix C for more details. Here, we provide a brief description of these skills.\n\nPick(s0): pick the object initialized at s0 Place(s∗): place the held object at s∗ Open [container](s): open the container containing the object initialized at s or the goal position s Close [container](s): close the container containing the object initialized at s or the goal position s Navigate(s): navigate to the start of other skills specified by s\n\nNote that s0 is constant per episode instead of a tracked object position. Hence, the target object may not be located at s0 at the beginning of a skill, e.g., picking an object from an opened drawer. Next, we will illustrate how these skills are chained in the HAB.\n\n3.3 SKILL CHAINING\n\nGiven a task decomposition, a hierarchical approach also needs to generate high-level actions to select a subtask and perform the corresponding skill. Task planning (Fikes & Nilsson, 1971) can be applied to find a sequence of subtasks before execution, with perfect knowledge of the environment. An alternative is to learn high-level actions through hierarchical RL. In this work, we use the subtask sequences generated by a perfect task planner (Szot et al., 2021). Here we list these sequences, to highlight the difficulty of tasks 2.\n\n1To be precise, the tasks studied in this work are partially observable Markov decision process (POMDP). 2We only list the subtask sequence of TidyHouse for one object here for illustration. The containers are\n\ndenoted with subscripts f r (fridge) and dr (drawer) if included in the skill.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n0, si\n\nTidyHouse(si PrepareGroceries(s1 Place(s1 Pick(s3 SetTable(s1 Place(s1 Pickfr(s2\n\n∗, s2 ∗) → Navigatefr(s2 0) → Navigatefr(s3 ∗, s2 ∗) → Navigatedr(s1 0) → Navigate(s2\n\n0, s1\n\n0, s1\n\n0) → Pick(si ∗, s3\n\n∗): Navigate(si 0, s2 0) → Pickfr(s2 ∗) → Placefr(s3 ∗) 0) → Opendr(s1 ∗): Navigatedr(s1\n\n∗): Navigatefr(s1\n\n0, s3\n\n0, s2\n\n∗) → Navigatefr(s2\n\n0) → Closedr(s1\n\n∗) → Place(s2\n\n0) → Navigate(si\n\n∗) → Place(si\n\n∗)\n\n0) → Pickfr(s1\n\n0) → Navigate(s2\n\n∗) → Place(s2\n\n0) → Navigate(s1 ∗) → Navigate(s3\n\n∗) → 0) →\n\n0) → Pickdr(s1\n\n0) → Navigatefr(s2\n\n0) → Openfr(s2\n\n0) → Closefr(s2 0)\n\n0) → Navigate(s1 0) → Navigatefr(s2\n\n∗) → 0) →\n\n4 SUBTASK FORMULATION AND SKILL LEARNING FOR MOBILE\n\nMANIPULATION\n\nFollowing the proposed principles (composability, achievability, reusability), we revisit and reformulate subtasks defined in the Home Assistant Benchmark (HAB). The core idea is to enlarge the initial states of manipulation skills to encompass the terminal states of the navigation skill, given our observation that the navigation skill is usually more robust to initial states. However, manipulation skills (Pick, Place, Open drawer, Close drawer) in Szot et al. (2021), are stationary. The composability of a stationary manipulation skill is restricted, since its feasible initial states are limited due to kinematic constraints. For instance, the robot can not open the drawer if it is too close or too far from the drawer. Therefore, these initial states need to be carefully designed given the trade-off between composability and achievability, which is not scalable and flexible. On the other hand, the navigation skill, which is learned to navigate to the start of manipulation skills, is also restricted by stationary constraints, since it is required to precisely terminate at a small set of “good” locations for manipulation. To this end, we propose to replace stationary manipulation skills with mobile counterparts. Thanks to mobility, mobile manipulation skills can have better composability without sacrificing much achievability. For example, a mobile manipulator can learn to first get closer to the target and then manipulate, to compensate for errors from navigation. It indicates that the initial states can be designed in a more flexible way, which also enables us to design a better navigation reward to facilitate learning.\n\nIn the context of mobile manipulation, the initial state of a skill consists of the robot base position, base orientation, and joint positions. For simplicity, we do not discuss the initial states of rigid and articulated objects in the scene, which are usually defined in episode generation. Moreover, we follow previous works (Szot et al., 2021; Lee et al., 2021) to initialize the arm at its resting position and reset it after each skill in skill chaining. Such a reset operation is common in robotics (Garrett et al., 2020). Each skill is learned to reset the arm after accomplishing the subtask as in Szot et al. (2021). Furthermore, for base orientation, we follow the heuristic in Szot et al. (2021) to make the robot face the target position s0 or s∗.\n\n4.1 MANIPULATION SKILLS WITH MOBILITY\n\nWe first present how initial base positions are generated in previous works. For stationary manipulation, a feasible base position needs to satisfy several constraints, e.g., kinematic (the target is reachable) and collision-free constraints. Szot et al. (2021) uses heuristics to determine base positions. For Pick, Place without containers (fridge and drawer), a navigable position closest to the target position is selected. For Pick, Place with containers, a fixed position relative to the container is selected. For Open, Close, a navigable position is randomly selected from a handcrafted region relative to each container. Noise is added to base position and orientation in addition, and infeasible initial states are rejected by constraints. See Fig 2 for examples.\n\nThe above example indicates the difficulty and complexity to design feasible initial states for stationary manipulation. One naive solution is to enlarge the initial set with infeasible states, but this can hurt learning as shown later in Sec 5.4. Besides, rejection sampling can be quite inefficient in this case, and Szot et al. (2021) actually computes a fixed number of feasible initial states offline.\n\nManipulation Skills with Mobility. To this end, we propose to use mobile manipulation skills instead. The original action space (only arm actions) is augmented with base actions. We devise a unified and efficient pipeline to generate initial base positions. Concretely, we first discretize the floor map with a resolution of 5 × 5cm2, and get all navigable (grid) positions. Then, different candidates are computed from these positions based on subtasks. Candidates are either within a radius (e.g., 2m) around the target position for Pick, Place, or a region relative to the container for\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) Pick(stationary)\n\n(b) Pick(mobile)\n\n(c) Close drawer\n\n(d) Close fridge\n\nFigure 2: Initial base positions of manipulation skills. We only show the examples for Pick, Close drawer, Close fridge, as Place, Open drawer, Open fridge share the same initial base positions respectively. Positions are visualized as green points on the floor. The target object in Pick is highlighted by a circle in cyan. Note that the initial base position of Pick(stationary) is a single navigable position closest to the object.\n\nOpen, Close. Finally, a feasible position is sampled from the candidates with rejection and noise. Compared to stationary manipulation, the rejection rate of our pipeline is much lower, and thus can be efficiently employed on-the-fly during training. See Fig 2 for examples.\n\n4.2 NAVIGATION SKILL WITH REGION-GOAL NAVIGATION REWARD\n\nThe navigation skill is learned to connect different manipulation skills. Hence, it needs to terminate within the set of initial achievable states of manipulation skills. We follow Szot et al. (2021) to randomly sample a navigable base position and orientation as the initial state of navigation skill. The challenge is how to formulate the reward function, which implicitly defines desirable terminal states. A common navigation reward (Wijmans et al., 2019) is the negative change of geodesic distance to a single 2D goal position on the floor. Szot et al. (2021) extends it for mobile manipulation, which introduces the negative change of angular distance to the desired orientation (facing the target). The resulting reward function, rt(s, a), for state s and action a is the following (Eq 1):\n\nrt(s, a) = −∆geo(g) − λang∆angI[dgeo\n\nt\n\n(g)≤ ̃D] + λsuccI[dgeo\n\nt\n\n(g)≤D∧dang\n\nt ≤Θ] − rslack\n\n(1)\n\nt\n\nt\n\nt\n\n(xbase t\n\n(xbase t\n\nt−1(xbase\n\n, g) − dgeo\n\nt−1 , g), where dgeo\n\nand the 2D goal position g. dgeo\n\n∆geo(g) = dgeo , g) is the geodesic distance between the current base position xbase , g). ∆ang = dang t − dang t−1 = ∥θt − θ∗∥1 − ∥θt−1 − θ∗∥1, where θt is the current base orientation, and θ∗ is the target orientation. Note that the 2D goal on the floor is different from the 3D goal specification for manipulation subtasks. I[dgeo t ≤ ̃D] is an indicator of whether the agent is close enough to the 2D goal, where ̃D is a threshold. I[dgeo t ≤Θ] is an indicator of navigation success, where D and Θ are thresholds for geodesic and angular distances. rslack is a slack penalty. λang, λsucc are hyper-parameters.\n\n(g) is short for dgeo\n\nt ≤D∧dang\n\n(xbase t\n\nt\n\nt\n\nThis reward has several drawbacks: 1) A single 2D goal needs to be assigned, which should be an initial base position of manipulation skills. It is usually sampled with rejection, as explained in Sec 4.1. It ignores the existence of multiple reasonable goals, introduces ambiguity to the reward (hindering training), and leads the skill to memorize (hurting generalization). 2) There is a hyperparameter ̃D, which defines the region where the angular term ∆ang is considered. However, it can lead the agent to learn the undesirable behavior of entering the region with a large angular distance, e.g., backing onto the target.\n\nRegion-Goal Navigation Reward. To this end, we propose a region-goal navigation reward for Inspired by object-goal navigation, we use the geodesic distance 3 training the navigation skill. between the robot and a region of 2D goals on the floor instead of a single goal. Thanks to the flexibility of our mobile manipulation skills, we can simply reuse the candidates (Sec 4.1) for their initial base positions as the navigation goals. However, these candidates are not all collision-free. Thus, we add a collision penalty rcol = λcolCt to the reward, where Ct is the current collision force and λcol is a weight. Besides, we simply remove the angular term, and find that the success reward is sufficient to encourage correct orientation. Our region-goal navigation reward is as follows:\n\nrt(s, a) = −∆geo({g}) + λsuccI[dgeo\n\nt\n\n({g})≤D∧dang\n\nt ≤Θ] − rcol − rslack\n\n(2)\n\n3The geodesic distance to a region can be approximated by the minimum of all the geodesic distances to\n\ngrid positions within the region.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n5 EXPERIMENTS\n\n5.1 EXPERIMENTAL SETUP\n\nWe use the ReplicaCAD dataset and the Habitat 2.0 simulator (Szot et al., 2021) for our experiments. The ReplicaCAD dataset contains 5 macro variations, with 21 micro variations per macro variation 4. We hold out 1 macro variation to evaluate the generalization of unseen layouts. For the rest of the 4 macro variations, we split 84 scenes into 64 scenes for training and 20 scenes to evaluate the generalization of unseen configurations (object and goal positions). For each task, we generate 6400 episodes (64 scenes) for training, 100 episodes (20 scenes) to evaluate cross-configuration generalization, and another 100 episodes (the hold-out macro variation) to evaluate cross-layout generalization. The robot is a Fetch (Robotics, 2022) mobile manipulator with a 7-DoF arm and a parallel-jaw gripper. See Appendix B for more details about the setup and dataset generation.\n\nObservation space: The observation space includes head and arm depth images (128 × 128), arm joint positions (7-dim), end-effector position (3-dim) in the base frame, goal positions (3-dim) in both base and end-effector frames, as well as a scalar to indicate whether an object is held. The goal position, depending on subtasks, can be either the initial or desired position of the target object. We assume a perfect GPS+Compass sensor and proprioceptive sensors as in Szot et al. (2021), which are used to compute the relative goal positions. For the navigation skill, only the head depth image and the goal position in the base frame are used.\n\nAction space: The action space is a 10-dim continuous space, including 2-dim base action (linear forwarding and angular velocities), 7-dim arm action, and 1-dim gripper action. Grasping is abstract as in Batra et al. (2020); Szot et al. (2021); Ehsani et al. (2021). If the gripper action is positive, the object closest to the end-effector within 15cm will be snapped to the gripper; if negative, the gripper will release any object held. For the navigation skill, we use a discrete action space, including a stop action, as in Yokoyama et al. (2021); Szot et al. (2021). A discrete action will be converted to continuous velocities to move the robot, while arm and gripper actions are masked out.\n\nHyper-parameters: We train each skill by the PPO (Schulman et al., 2017) algorithm. The visual observations are encoded by a 3-layer CNN as in Szot et al. (2021). The visual features are concatenated with state observations and previous action, followed by a 1-layer GRU and linear layers to output action and value. Each skill is trained with 3 different seeds. See Appendix C.1 for details.\n\nMetrics: Each HAB task consists of a sequence of subtasks to accomplish, as illustrated in Sec 3.3. The completion of a subtask is conditioned on the completion of its preceding subtask. We report progressive completion rates of subtasks, and the completion rate of the last subtask is thus the success rate of the full task. For each evaluation episode, the robot is initialized at a random base position and orientation without collision, and its arm is initialized at the resting position. The completion rate is averaged over 9 different runs 5.\n\n5.2 BASELINES\n\nWe denote our method by M3, short for a multi-skill mobile manipulation pipeline where mobile manipulation skills (M) are chained by the navigation skill trained with our region-goal navigation reward (R). We compare our method with several RL baselines. All baselines follow the same experimental setup in Sec 5.1 unless specified. We refer readers to Szot et al. (2021) for a senseplan-act baseline, which is shown to be inferior to the skill chaining pipeline emphasized in this work. Stationary manipulation skills and point-goal navigation reward are denoted by S and P.\n\nMonolithic RL (mono): This baseline is an end-to-end RL policy trained with a combination of reward functions of individual skills. See Appendix D for more details. Stationary manipulation skills + point-goal navigation reward (S+P): This baseline is TaskPlanning+SkillsRL (TP+SRL) introduced in Szot et al. (2021), where stationary manipulation skills are chained by the navigation skill trained with the point-goal navigation reward. Compared to the original implementation, we make several improvements, including better reward functions and training schemes. For reference, the original success rates of all HAB tasks are nearly zero. See Appendix A for more details.\n\n4Each macro variation has a different, semantically plausible layout of large furniture (e.g., kitchen counter and fridge) while each micro variation is generated through perturbing small furniture (e.g., chairs and tables).\n\n53 seeds for RL training multiplied by 3 seeds for initial states\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTidyHouse\n\nPrepareGroceries\n\nSetTable\n\nn o\n\ni t\na r\nu g\nfi n\no c\n- s\ns o\nr\n\nC\n\nt\n\nu o\ny a\nl -\ns s\no r\n\nC\n\nFigure 3: Progressive completion rates for HAB Szot et al. (2021) tasks. The x-axis represents progressive subtasks. The y-axis represents the completion rate of each subtask. The mean and standard error for 100 episodes over 9 seeds are reported. Best viewed zoomed.\n\nMobile manipulation skills + point-goal navigation reward (M+P): Compared to our M3, this baseline does not use the region-goal navigation reward. It demonstrates the effectiveness of proposed mobile manipulation skills. Note that the point-goal navigation reward is designed for the start of stationary manipulation skills.\n\n5.3 RESULTS\n\nFig 3 shows the progressive completion rates of different methods on all tasks. Our method M3 achieves an average success rate of 71.2% in the cross-configuration setting, and 55.0% in the crossIt outperforms all the baselines in both settings, namely mono layout setting, over all 3 tasks. (1.8%/1.8%), S+P (57.4%/31.1%) and M+P (64.9%/36.2%). First, all the modular approaches show much better performance than the monolithic baseline, which verifies the effectiveness of modular approaches for long-horizon mobile manipulation tasks. Mobile manipulation skills are in general superior to stationary ones (M+P vs.S+P). Fig 4 provides an example where mobile manipulation skills can compensate for imperfect navigation. Furthermore, our region-goal navigation reward can reduce the ambiguity of navigation goals to facilitate training (see training curves in Appendix C). Since it does not require the policy to memorize ambiguous goals, the induced skill shows better generalizability, especially in the cross-layout setting (55.0% for M3 vs.36.2% for M+P).\n\n5.4 ABLATION STUDIES\n\nWe conduct several ablation studies to show that mobile manipulation skills are more flexible to formulate than stationary ones, and to understand the advantage of our navigation reward.\n\nCan initial states be trivially enlarged? We conduct experiments to understand to what extent we can enlarge the initial states of manipulation skills given the trade-off between achievability and composability. In the S(L)+P experiment, we simply replace the initial states of stationary manipulation skills with those of mobile ones. The success rates of stationary manipulation skills on subtasks drop by a large margin, e.g., from 95% to 45% for Pick on TidyHouse. Fig 5 shows that S(L)+P (37.7%/18.1%) is inferior to both S+P (57.4%/31.1%) and M+P (64.9%/36.2%). It indicates that stationary manipulation skills have a much smaller set of feasible initial states compared to mobile ones, and including infeasible initial states during training can hurt performance significantly. We also study the impact of initial state distribution on mobile manipulation skills in Appendix F.\n\nIs the collision penalty important for the navigation skill? Our region-goal navigation reward benefits from unambiguous region goals and the collision penalty. We add the collision penalty to the point-goal navigation reward (Eq 1) in S+P(C) and M+P(C) experiments. Fig 5 shows that the collision penalty significantly improves the success rate: S+P(C) (65.2%/44.6%) vs.S+P (57.4%/31.1%)\n\n8\n\npick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success ratemonoS+PM+PM3(ours)pick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success ratemonoS+PM+PM3(ours)open_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success ratemonoS+PM+PM3(ours)pick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success ratemonoS+PM+PM3(ours)pick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success ratemonoS+PM+PM3(ours)open_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success ratemonoS+PM+PM3(ours)Published as a conference paper at ICLR 2023\n\n(a) Stationary Manipulation (S+P)\n\n(b) Mobile Manipulation (M+P)\n\nFigure 4: Qualitative comparison between stationary and mobile manipulation. In this example, the point-goal navigation skill terminates between two drawers (1st image). Mobile manipulation manages to open the correct drawer containing the bowl (last image in the bottom row) while stationary manipulation gets confused and finally opens the wrong drawer (last image in the top row). More qualitative results can be found in Appendix H and on our project website.\n\nTidyHouse\n\nPrepareGroceries\n\nSetTable\n\nn o\n\ni t\na r\nu g\nfi n\no c\n- s\ns o\nr\n\nC\n\nt\n\nu o\ny a\nl -\ns s\no r\n\nC\n\nFigure 5: Progressive completion rates for HAB tasks. The x-axis represents progressive subtasks. The y-axis represents the completion rate of each subtask. Results of ablation experiments are presented with solid lines. The mean and standard error for 100 episodes over 9 seeds are reported.\n\nand M+P(C) (67.9%/49.2%) vs.M+P (64.9%/36.2%). A collision-aware navigation skill can avoid disturbing the environment, e.g., accidentally closing the fridge before placing an object in it. Besides, M+P(C) is still inferior to our M3 (71.2%/55.0%). It implies that reducing the ambiguity of navigation goals helps learn more robust and generalizable navigation skills.\n\n6 CONCLUSION AND LIMITATIONS\n\nIn this work, we present a modular approach to tackle long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), featuring mobile manipulation skills and the region-goal navigation reward. Given the superior performance, our approach can serve as a strong baseline for future study. Besides, the proposed principles (achievability, composability, reusability) can serve as a guideline about how to formulate meaningful and reusable subtasks. However, our work is still limited to abstract grasp and other potential simulation defects. We leave fully dynamic simulation and real-world deployment to future work.\n\n9\n\npick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success rateS+PM+PM3(ours)S(L)+PS+P(C)M+P(C)pick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success rateS+PM+PM3(ours)S(L)+PS+P(C)M+P(C)open_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success rateS+PM+PM3(ours)S(L)+PS+P(C)M+P(C)pick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success rateS+PM+PM3(ours)S(L)+PS+P(C)M+P(C)pick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success rateS+PM+PM3(ours)S(L)+PS+P(C)M+P(C)open_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success rateS+PM+PM3(ours)S(L)+PS+P(C)M+P(C)Published as a conference paper at ICLR 2023\n\nREFERENCES\n\nPeter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018.\n\nDhruv Batra, Angel X Chang, Sonia Chernova, Andrew J Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020.\n\nBerk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR), pp. 510–517. IEEE, 2015.\n\nDevendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan SalakhutdiIn International Conference on Learning\n\nnov. Learning to explore using active neural slam. Representations (ICLR), 2020.\n\nAlexander Clegg, Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Learning to dress: Synthesizing human dressing motion via deep reinforcement learning. ACM Transactions on Graphics (TOG), 37(6):1–10, 2018.\n\nKiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Manipulathor: A framework for visual object manipulation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4495– 4504, 2021.\n\nRichard E Fikes and Nils J Nilsson. Strips: A new approach to the application of theorem proving\n\nto problem solving. Artificial intelligence, 2(3-4):189–208, 1971.\n\nChuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, et al. The threedworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. arXiv preprint arXiv:2103.14025, 2021.\n\nCaelan Reed Garrett, Tom ́as Lozano-P ́erez, and Leslie Pack Kaelbling. Pddlstream: Integrating In Proceedings of symbolic planners and blackbox samplers via optimistic adaptive planning. the International Conference on Automated Planning and Scheduling, volume 30, pp. 440–448, 2020.\n\nCaelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Integrated task and motion planning. Annual review of\n\nKaelbling, and Tom ́as Lozano-P ́erez. control, robotics, and autonomous systems, 4:265–293, 2021.\n\nAbhishek Kadian, Joanne Truong, Aaron Gokaslan, Alexander Clegg, Erik Wijmans, Stefan Lee, Manolis Savva, Sonia Chernova, and Dhruv Batra. Sim2real predictivity: Does evaluation in simulation predict real-world performance? IEEE Robotics and Automation Letters, 5(4):6670– 6677, 2020.\n\nYoungwoon Lee, Shao-Hua Sun, Sriram Somasundaram, Edward S Hu, and Joseph J Lim. Composing complex skills by learning transition policies. In International Conference on Learning Representations, 2018.\n\nYoungwoon Lee, Jingyun Yang, and Joseph J Lim. Learning to coordinate manipulation skills via skill behavior diversification. In International Conference on Learning Representations, 2019.\n\nYoungwoon Lee, Joseph J. Lim, Anima Anandkumar, and Yuke Zhu. Adversarial skill chaining for long-horizon robot manipulation via terminal state regularization. In Conference on Robot Learning, 2021.\n\nTongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Cathera Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTianwei Ni, Kiana Ehsani, Luca Weihs, and Jordi Salvador. Towards disturbance-free visual mobile\n\nmanipulation. arXiv preprint arXiv:2112.12612, 2021.\n\nIEEE\n\nRAS.\n\nMobile manipulation.\n\nhttps://www.ieee-ras.org/\n\nmobile-manipulation, 2022. Accessed: 2022-05-18.\n\nFetch Robotics.\n\nAutonomous mobile robots\n\nthat\n\nimprove productivity.\n\nhttp://\n\nfetchrobotics.com/, 2022. Accessed: 2022-05-18.\n\nThushara Sandakalum and Marcelo H Ang Jr. Motion planning for mobile manipulators—a system-\n\natic review. Machines, 10(2):97, 2022.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nMartin Sereinig, Wolfgang Werth, and Lisa-Marie Faller. A review of the challenges in mobile manipulation: systems design and robocup challenges. e & i Elektrotechnik und Informationstechnik, 137(6):297–308, 2020.\n\nSiddharth Srivastava, Eugene Fang, Lorenzo Riano, Rohan Chitnis, Stuart Russell, and Pieter Abbeel. Combined task and motion planning through an extensible planner-independent interface layer. In 2014 IEEE international conference on robotics and automation (ICRA), pp. 639–646. IEEE, 2014.\n\nCharles Sun, Jedrzej Orbik, Coline Manon Devin, Brian H Yang, Abhishek Gupta, Glen Berseth, and Sergey Levine. Fully autonomous real-world reinforcement learning with applications to mobile manipulation. In Conference on Robot Learning, pp. 308–319. PMLR, 2022.\n\nAndrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n\nYusuke Urakami, Alec Hodgkinson, Casey Carlin, Randall Leu, Luca Rigazio, and Pieter Abbeel. Doorgym: A scalable door opening environment and baseline agent. arXiv preprint arXiv:1908.01887, 2019.\n\nCong Wang, Qifeng Zhang, Qiyan Tian, Shuo Li, Xiaohui Wang, David Lane, Yvan Petillot, and Sen Wang. Learning mobile manipulation through deep reinforcement learning. Sensors, 20(3): 939, 2020.\n\nLuca Weihs, Matt Deitke, Aniruddha Kembhavi, and Roozbeh Mottaghi. Visual room rearrangement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5922–5931, 2021.\n\nErik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations, 2019.\n\nFei Xia, William B Shen, Chengshu Li, Priya Kasimbeg, Micael Edmond Tchapmi, Alexander Toshev, Roberto Mart ́ın-Mart ́ın, and Silvio Savarese. Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. IEEE Robotics and Automation Letters, 5(2): 713–720, 2020.\n\nFei Xia, Chengshu Li, Roberto Mart ́ın-Mart ́ın, Or Litany, Alexander Toshev, and Silvio Savarese. Relmogen: Integrating motion generation in reinforcement learning for mobile manipulation. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 4583–4590. IEEE, 2021.\n\nNaoki Yokoyama, Sehoon Ha, and Dhruv Batra. Success weighted by completion time: A dynamicsaware evaluation criteria for embodied navigation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1562–1569. IEEE, 2021.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pp. 1094–1100. PMLR, 2020.\n\nYuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart ́ın-Mart ́ın. robosuite: A modular simulation framework and benchmark for robot learning. In arXiv preprint arXiv:2009.12293, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA OVERVIEW\n\nCompared to the original implementation (Szot et al., 2021), our implementation benefits from repaired assets (Sec B), improved reward functions and better training schemes (Sec C). Other differences include observation and action spaces. We introduce in observations the target positions in the base frame in addition to those in the end-effector frame. The arm action is defined in the joint configuration space (7-dim) rather than the end-effector Euclidean space (3-dim with no orientation).\n\nB DATASET AND EPISODES\n\nSzot et al. (2021) keeps updating the ReplicaCAD dataset. The major fix is “minor furniture layout modifications in order to better accommodate robot access to the full set of receptacles” 6. The agent radius is also decreased from 0.4m to 0.3m to generate navigation meshes with higher connectivity. Besides, Szot et al. (2021) also improves the episode generator 7 to ensure stable initialization of objects. Those improvements eliminate most unachievable episodes in the initial version. The episodes used in our experiments are generated with the ReplicaCAD v1.4 and the latest habitatlab 8.\n\nCross-configuration and cross-layout settings are the same except for scene layouts. In the crossconfiguration setting, test scene layouts (micro variations) are different but similar to training ones. In the cross-layout setting, test scene layouts (macro variations) are significantly different from training ones. Each macro variation has a different, semantically plausible layout of large furniture (e.g., kitchen counter and fridge) while each micro variation is generated through perturbing small furniture (e.g., chairs and tables). Thus, the cross-layout setting demands stronger generalization on scene layouts.\n\nFor TidyHouse, each episode includes 20 clutter objects and 5 target objects along with their goal positions, located at 7 different receptacles (chair, 2 tables, tv stand, two kitchen counters, sofa). For PrepareGroceries, each episode includes 21 clutter objects located at 8 different receptacles (the 7 receptacles used in TidyHouse and the top shelf of the fridge) and 1 clutter object located at the middle shelf of the fridge. 2 target objects are located at the middle shelf, and each of their goal positions is located at one of two kitchen counters. The third target object is located at one of two kitchen counters, and its goal position is at the middle shelf. SetTable generates episodes similar to PrepareGroceries, except that two target objects, bowl and apple, are initialized at one of 3 drawers and at the middle fridge shelf respectively. Each of their goal positions is located at one of two tables.\n\nC SKILL LEARNING\n\nEach skill is trained to accomplish a subtask and reset its end-effector at the resting position. The robot arm is first initialized with predefined resting joint positions, such that the corresponding resting position of the end-effector is (0.5, 1.0, 0.0) in the base frame 9. The initial end-effector position is then perturbed by a Gaussian noise N (0, 0.025) clipped at 0.05m. The base position is perturbed by a Gaussian noise N (0, 0.1) truncated at 0.2m. The base orientation is perturbed by a Gaussian noise N (0, 0.25) truncated at 0.5 radian. The maximum episode length is 200 steps for all the manipulation skills, and 500 steps for the navigation skill. The episode terminates on success or failure. We use the same reward function for both stationary and mobile manipulation skills, unless specified.\n\nee is the distance between the end-effector and the object, dr\n\nFor all skills, do the end-effector and the resting position, dh fined manipulation handle (a 3D position) of the articulated object, dg joint position of the articulated object and the goal joint position. ∆b for the (negative) change in distance between a and b. For example, ∆o\n\nee is the distance between ee is the distance between the end-effector and a predea is the distance between the a = db a(t) stands ee is the change in distance\n\na(t − 1) − db\n\n6https://github.com/facebookresearch/habitat-sim/pull/1694 7https://github.com/facebookresearch/habitat-lab/pull/764 8https://github.com/facebookresearch/habitat-lab/pull/837 9The positive x and y axes point forward and upward in Habitat.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nbetween the end-effector and the object. Iholding indicates if the robot is holding an (correct) object or handle. Isucc indicates the task success. Ct refers to the current collision force, and C1:t stands for the accumulated collision force.\n\nThe 7-dim arm action stands for the delta joint positions added to the current target joint positions of the PD controller. The input arm action is assumed to be normalized to [−1, 1], and will be scaled by 0.025 (radian). The 2-dim base action stands for linear and angular velocities. The base movement in the Habitat 2.0 is implemented by kinematically setting the robot’s base transformation. The collision between the robot base and navigation meshes is taken into consideration. The input base action is assumed to be normalized to [−1, 1], and will be scaled by 3 (navigation skill) or 1.5 (manipulation skills). For the navigation skill, we follow Szot et al. (2021) to use a discrete action space and translate the discrete action into the continuous one. Concretely, the (normalized) linear velocity from -0.5 to 1 is discretized into 4 choices ({−0.5, 0, 0.5, 1}), and the (normalized) angular velocity from -1 to 1 is discretized into 5 choices (({−1, −0.5, 0, 0.5, 1}). The stop action corresponds to the discrete action representing zero velocities.\n\nPick(s0)\n\n• Objective: pick the object initialized at s0 • Initial base position (noise is applied in addition):\n\n– Stationary: the closest navigable position to s0 – Mobile: a randomly selected navigable position within 2m of s0\n\n• Reward: Ipick indicates whether the correct object is picked and Iwrong indicates whether a wrong\n\nobject is picked.\n\nrt = 4∆o − max(0.001Ct, 0.2) − I[C1:t>5000] − Iwrong − I[do\n\nI!holding + Ipick + 4∆r\n\nIholding + 2.5Isucc ee>0.09]Iholding − 0.002\n\nee\n\nee\n\n• Success: The robot is holding the target object and the end-effector is within 5cm of the resting\n\nposition. Isucc = Iholding ∧ dr\n\nee ≤ 0.05\n\n• Failure:\n\n– I[C1:t>5000] = 1: The accumulated collision force is larger than 5000N . – Iwrong = 1: A wrong object is picked. – I[do\n\nee>0.09]Iholding = 1: The held object slides off the gripper.\n\n• Observation space:\n\n– Depth images from head and arm cameras. – The current arm joint positions. – The current end-effector position in the base frame. – Whether the gripper is holding anything. – The starting position s0 in both the base and end-effector frame.\n\n• Action space: The gripper is disabled to release.\n\nPlace(s∗)\n\n• Objective: place the held object at s∗ • Initial base position (noise is applied in addition):\n\n– Stationary: the closest navigable position to s∗ – Mobile: a randomly selected navigable position within 2m of s∗\n\n• Reward: Iplace indicates whether the object is released within 15cm of the goal position, and Idrop\n\nindicates whether the object is released beyond 15cm.\n\nrt = 4∆s∗\n\no\n\nIholding + Iplace + 4∆r\n\n− min(0.001Ct, 0.2) − I[C1:t>7500] − Idrop − I[do\n\nee\n\nI!holding + 2.5Isucc ee>0.09]Iholding − 0.002\n\n• Success: The object is within 15cm of the goal position and the end-effector is within 5cm of the\n\nresting position. Isucc = ds∗\n\no ≤ 0.15 ∧ I!holding ∧ dr\n\nee ≤ 0.05\n\n14\n\nPublished as a conference paper at ICLR 2023\n\n• Failure:\n\n– I[C1:t>7500] = 1: The accumulated collision force is larger than 7500N . – Idrop = 1: The object is released beyond 15cm of the goal position. ee>0.09]Iholding = 1: The held object slides off the gripper. – I[do\n\n• Observation space:\n\n– Depth images from head and arm cameras. – The current arm joint positions. – The current end-effector position in the base frame. – Whether the gripper is holding anything. – The goal position s∗ in both the base and end-effector frame.\n\n• Action space: The gripper is disabled to grasp after releasing the object.\n\nOpen drawer(s)\n\n• Objective: open the drawer containing the object initialized at s. The goal joint position of the\n\ndrawer is g = 0.45m.\n\n• Initial base position (noise is applied in addition):\n\n– Stationary: a navigable position randomly selected within a [0.80, −0.35] × [0.95, 0.35] re-\n\ngion in front of the drawer.\n\n– Mobile: a navigable position randomly selected within a [0.3, −0.6] × [1.5, 0.6] region in\n\nfront of the drawer.\n\n• Reward: Iopen = dg\n\na ≤ 0.05 indicates whether the drawer is open. Irelease indicates whether the handle is released when the drawer is open. Igrasp indicates whether the correct handle is grasped. abase is the (2-dim) base action.\n\nrt = 2∆h\n\nee\n\nI!open + Igrasp + 2∆g\n\na\n\n−Iwrong − I[dh\n\nIholding + Irelease + 2∆r\n\nIopen + 2.5Isucc ee>0.2]Iholding − Iout − 0.004∥abase∥1\n\nee\n\n• Success: The drawer is open, and the end-effector is within 15cm of the resting position. Isucc =\n\nIopen ∧ I!holding ∧ dr\n\nee ≤ 0.15\n\n• Failure:\n\n– Iwrong = 1: The wrong object or handle is picked. – I[dh – Iout = 1: The robot moves out of a predefined region (a 2m × 3m region in front of the\n\nee>0.2]Iholding = 1: The grasped handle slides off the gripper.\n\ndrawer).\n\n– II[open(t−1)∧!open(t)] = 1: The drawer is not open after being opened. – The gripper releases the handle when the drawer is not open (I!open = 1). – ∆g\n\na >= 0.1: The drawer is opened too fast.\n\n• Observation space:\n\n– Depth images from head and arm cameras. – The current arm joint positions. – The current end-effector position in the base frame. – Whether the gripper is holding anything. – The starting position s in both the base and end-effector frame.\n\nClose drawer(s)\n\n• Objective: close the drawer containing the object initialized at s. The goal joint position is g =\n\n0m.\n\n• Initial joint position: qa ∈ [0.4, 0.5], where qa is the joint position of the target drawer. A random a ≤ 0.1).\n\nsubset of other drawers are slightly open (q′\n\n• Initial base position (noise is applied in addition):\n\n15\n\nPublished as a conference paper at ICLR 2023\n\n– Stationary: a navigable position randomly selected within a [0.3, −0.35]×[0.45, 0.35] region\n\nin front of the drawer.\n\n– Mobile: a navigable position randomly selected within a [0.3, −0.6] × [1.0, 0.6] region in\n\nfront of the drawer.\n\n• Reward: It is almost the same as Open drawer by replacing open with close. Iclose = dg • Success: The drawer is closed, and the end-effector is within 15cm of the resting position. • Failure: It is almost the same as Open drawer by replacing open with close, except that the last\n\na ≤ 0.1.\n\nconstraint ∆g\n\na >= 0.1 is not included.\n\nOpen fridge(s)\n\n• Objective: open the fridge containing the object initialized at s. The goal joint position is g = π 2 . • Initial base position (noise is applied in addition): a navigable position randomly selected within\n\na [0.933, −1.5] × [1.833, 1.5] region in front of the fridge.\n\n• Reward: Iopen = g − qa > 0.15, where qa is the joint position (radian) of the fridge. To avoid the robot from penetrating the fridge due to simulation defects, we add a collision penalty but excludes collision between the end-effector and the fridge.\n\nrt = 2∆h\n\nee\n\nI!open + Igrasp + +2∆g −IC1:t>5000 − Iwrong − I[dh\n\nIholding + Irelease + ∆r\n\nIopen + 2.5Isucc ee>0.2]Iholding − Iout − 0.004∥abase∥1\n\nee\n\na\n\n• Success: The fridge is open, and the end-effector is within 15cm of the resting position. Isucc =\n\nIopen ∧ I!holding ∧ dr\n\nee ≤ 0.15\n\n• Failure:\n\n– Iwrong = 1: The wrong object or handle is picked. – I[dh – Iout = 1: The robot moves out of a predefined region (a 2m × 3.2m region in front of the\n\nee>0.2]Iholding = 1: The grasped handle slides off the gripper.\n\nfridge).\n\n– II[open(t−1)∧!open(t)] = 1: The fridge is not open after being opened. – The gripper releases the handle when the fridge is not open (I!open = 1).\n\n• Observation space:\n\n– Depth images from head and arm cameras. – The current arm joint positions. – The current end-effector position in the base frame. – Whether the gripper is holding anything. – The starting position s in both the base and end-effector frame.\n\nClose fridge(s)\n\n• Objective: close the fridge containing the object initialized at s. The goal joint position is g = 0. • Initial joint position: qa ∈ [ π • Initial base position (noise is applied in addition): a navigable position randomly selected within\n\n2 − 0.15, 2.356], where qa is the joint position of the target fridge.\n\na [0.933, −1.5] × [1.833, 1.5] region in front of the fridge.\n\n• Reward: It is almost the same as Close fridge by replacing open with close. Iclose = dg • Success: The fridge is close, and the end-effector is within 15cm of the resting position.\n\na ≤ 0.15.\n\nNavigate(s) (point-goal)\n\n• Objective: navigate to the start of other skills specified by s • Reward: refer to Eq 1. rslack = 0.002, ̃D = 0.9, λang = 0.25, λsucc = 2.5 • Success: The robot is within 0.3 meter of the goal, 0.5 radian of the target orientation, and has\n\ncalled the stop action at the current time step.\n\n• Observation space:\n\n– Depth images from the head camera. – The goal position s∗ in the base frame.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\n(a) Pick(stationary)\n\n(b) Pick(mobile)\n\n(c) Place(stationary)\n\n(d) Place(mobile)\n\n(e) Open drawer(stationary) (f) Open drawer(mobile) (g)\n\nClose\n\n(h) Close drawer(mobile)\n\ndrawer(stationary)\n\n(i) Open fridge\n\n(j) Close fridge\n\n(k) Navigation (point-goal) (l) Navigation (region-goal)\n\nFigure 6: Training curves for skills. The y-axis represents the success rate of the subtask (including resetting the end-effector at its resting position). Best viewed zoomed.\n\nNavigate(s) (region-goal)\n\n• Objective: navigate to the start of other skills specified by s • Reward: refer to Eq 2. rslack = 0.002, rcol = min(0.001Ct, 0.2), λsucc = 2.5 • Success: The robot is within 0.1 meter of any goal in the region, 0.25 radian of the target orienta-\n\ntion at the current position, and has called the stop action at the current time step.\n\n• Observation space:\n\n– Depth images from the head camera. – The goal position s∗ in the base frame.\n\nC.1 PPO HYPER-PARAMETERS\n\nOur PPO implementation is based on the habitat-lab. The visual encoder is a simple CNN 10. The coefficients of value and entropy losses are 0.5 and 0 respectively. We use 64 parallel environments and collect 128 transitions per environment to update the networks. We use 2 mini-batches, 2 epochs per update, and a clipping parameter of 0.2 for both policy and value. The gradient norm is clipped at 0.5. We use the Adam optimizer with a learning rate of 0.0003. The linear learning rate decay is enabled. The mean of the Gaussian action predicted by the policy network is activated by tanh. The (log) standard deviation of the Gaussian action, which is an input-independent parameter, is initialized as −1.0. Fig 6 shows training curves of skills.\n\nC.2 OTHER IMPLEMENTATION DETAILS\n\nThe PPO algorithm implemented by the habitat-lab does not distinguish the termination of the environment (MDP) and the truncation due to time limit. We fix this issue in our implementation. Furthermore, we separately train all the skills for each HAB task to avoid potential ambiguity. For example, the starting position of an object in the drawer is computed when the drawer is closed at\n\n10https://github.com/facebookresearch/habitat-lab/blob/main/habitat_\n\nbaselines/rl/models/simple_cnn.py\n\n17\n\n020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateTidyHousePrepareGroceriesSetTable(drawer)020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateTidyHousePrepareGroceriesSetTable(drawer)020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateTidyHousePrepareGroceries020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateTidyHousePrepareGroceries020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateTidyHousePrepareGroceriesSetTable020406080100Environment steps (x106)0.00.20.40.60.81.0Success rateTidyHousePrepareGroceriesSetTablePublished as a conference paper at ICLR 2023\n\nthe beginning of an episode. However, the skill Pick needs to pick this object up when the drawer is open and the actual position of the object is different from the starting position. It is inconsistent with other cases when the object is in an open receptacle or the fridge. We observe such ambiguity can hurt performance. See Fig 6 for all task-specific variants of skills.\n\nD MONOLITHIC BASELINE\n\nFor the monolithic baseline, a monolithic RL policy is trained for each HAB task. During training, the policy only handles one randomly selected target object, e.g., picking and placing one object in TidyHouse. During inference, the policy is applied to each target object. We use the same observation space, action space and training scheme as those for our mobile manipulation skills. The main challenge is how to formulate a reward function for those complicated long-horizon HAB tasks that usually require multiple stages. We follow Szot et al. (2021) to composite reward functions for individual skills, given the sequence of subtasks. Concretely, at each time step during training, we infer the current subtask given perfect knowledge of the environment, and use the reward function of the corresponding skill. To ease training, we remove the collision penalty and do not terminate the episode due to collision. Besides, we use the region-goal navigation reward for the navigation subtask. Thanks to our improved reward functions and better training scheme, our monolithic RL baseline is much better than the original implementation in Szot et al. (2021). However, although able to move the object to its goal position, the policy never learns to release the object to complete the subtask Place during training. It might be due to exploration difficulty since Place is the last subtask in a long sequence and previous subtasks all require the robot not to release. To boost its performance, we force the gripper to release anything held at the end of execution during evaluation.\n\nE EVALUATION\n\nE.1 SEQUENTIAL SKILL CHAINING\n\nFor evaluation, skills are sequentially executed in the order of their corresponding subtasks, as described in Sec 3.3. The main challenge is how to terminate a skill without privileged information. Basically, each skill will be terminated if its execution time exceeds its max episode length (200 steps for manipulation skills and 500 steps for the navigation skill). The termination condition of Pick is that an object is held and the end-effector is within 15cm of the resting position, which can be computed based on proprioception only. The gripper is disabled to release for Pick. The termination condition of Place is that the gripper holds nothing and the end-effector is within 15cm of the resting position. The gripper is disabled to grasp for Place. Besides, anything held will be released when Place terminates. For Open and Close, we use a heuristic from Szot et al. (2021): the skill will terminate if the end-effector is within 15cm of the resting position and it has moved at least 30cm away from the resting position during execution. Navigate terminates when it calls the stop action. Furthermore, since the manipulation skills only learn to reset its end-effector, we apply an additional operation to reset the whole arm after each skill. This reset operation is achieved by setting predefined joint positions as the target of the robot’s PD controller.\n\nE.2 PROGRESSIVE COMPLETION RATE\n\nIn this section, we describe how progressive completion rates are computed. The evaluation protocol is the same as Szot et al. (2021) (see its Appendix F), and here we phrase it in a way more friendly to readers with little knowledge of task planning and Planning Domain Definition Language (PDDL). To partially evaluate a HAB task, we divide a full task into a sequence of stages (subgoals). For example, TidyHouse can be considered to consist of pick 0, place 0, pick 1, etc.Each stage can correspond to multiple subtasks. For example, the stage pick i includes Navigate(si 0). Thus, to be precise, the completion rate is computed based on stages instead of subtasks. We define a set of predicates to measure whether the goal of a stage is completed. A stage goal is completed if all the predicates associated with it are satisfied. The predicates are listed as follows:\n\n0) and Pick(si\n\n• holding(target_obj|i): The robot is holding the i-th object. • at(target_obj_pos|i,target_goal_pos|i): The i-th object is within 15cm of its\n\ngoal position.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nTidyHouse\n\nPrepareGroceries\n\nSetTable\n\nn o\n\ni t\na r\nu g\nfi n\no c\n- s\ns o\nr\n\nC\n\nt\n\nu o\ny a\nl -\ns s\no r\n\nC\n\nFigure 7: Progressive completion rates for HAB (Szot et al., 2021) tasks. The x-axis represents progressive subtasks. The y-axis represents the completion rate of each subtask. Results of ablation experiments are presented with solid lines. The mean and standard error for 100 episodes over 9 seeds are reported.\n\n• opened_drawer(target_marker|i): The target drawer is open (the joint position is\n\nlarger than 0.4m).\n\n• closed_drawer(target_marker|i): The target drawer is close (the joint position is\n\nsmaller than 0.1m).\n\n• opened_fridge(target_marker|i): The target fridge is open (the joint position is larger\n\nthan π\n\n2 radian).\n\n• closed_fridge(target_marker|i): The target fridge is close (the joint position is\n\nsmaller than 0.15 radian).\n\nDuring evaluation, we evaluate whether the current stage goal is completed at each time step. If the current stage goal is completed, we progress to the next stage. Hence, the completion rate monotonically decreases. Listings 1, 2, 3 present the stages defined for each HAB task and the predicates associated with each stage. Note that the stage goal place i only indicates that the object has been released at its goal position, but the placement can be unstable (e.g., the object falls down the table), which can lead to the failure of the next stage. Besides, due to abstract grasp, it is difficult to place the object stably since the pose of the grasped object can not be fully controlled. Therefore, we modify the objective of SetTable to make the task achievable given abstract grasp. Concretely, instead of placing the fruit in the bowl, the robot only needs to place the fruit picked from the fridge at a goal position on the table.\n\nF MORE ABLATION STUDIES\n\nIn this section, we study the impact of different initial state distributions on mobile manipulation skills. We study the impact of different initial state distributions on mobile manipulation skills. We enlarge initial states by changing the distributions of the initial base position (the radius around the target) and orientation. For reference, the maximum radius around the target is set to 2m in the main experiments (Sec 5). Several experiments are conducted: M(S)+R, M(L1)+R, M(L2)+R, M(L3)+R. M(S)+R, M(L1)+R and M(L2)+R stand for the experiments where the maximum radii around the target are set to 1.5m, 2.5m and 4m respectively. M(L3)+R keeps the radius as 2m, but samples the initial base orientation from [−π, π], instead of using the direction facing towards the target. Fig 7 shows the quantitative results. Enlarging the initial states in general leads to performance degradation. Compared to M3 (71.2%/55.0%), M(L1)+R (67.4%/49.7%) and M(L3)+R\n\n19\n\npick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success rateM3(ours)M(S)+RM(L1)+RM(L2)+RM(L3)+Rpick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success rateM3(ours)M(S)+RM(L1)+RM(L2)+RM(L3)+Ropen_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success rateM3(ours)M(S)+RM(L1)+RM(L2)+RM(L3)+Rpick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success rateM3(ours)M(S)+RM(L1)+RM(L2)+RM(L3)+Rpick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success rateM3(ours)M(S)+RM(L1)+RM(L2)+RM(L3)+Ropen_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success rateM3(ours)M(S)+RM(L1)+RM(L2)+RM(L3)+RPublished as a conference paper at ICLR 2023\n\nTidyHouse\n\nPrepareGroceries\n\nSetTable\n\nn o\n\ni t\na r\nu g\nfi n\no c\n- s\ns o\nr\n\nC\n\nt\n\nu o\ny a\nl -\ns s\no r\n\nC\n\nFigure 8: Progressive completion rates for HAB (Szot et al., 2021) tasks. The x-axis represents progressive subtasks. The y-axis represents the completion rate of each subtask. Results of ablation experiments are presented with solid lines. The mean and standard error for 100 episodes over 9 seeds are reported.\n\n(67.5%/46.4%) show moderate performance drop. M(L2)+R (55.2%/38.9%) shows the largest performance drop, which indicates that mobile manipulation skills are not able to handle long-range navigation yet. Moreover, M(S)+R (69.5%/52.1%) performs on par with M3. It implies that there usually exists a “sweet spot” of the initial state distribution for mobile manipulation skills as a tradeoff between achievability and composability.\n\nBesides, we extend the S(L)+P experiment described in Sec 5.4, where we simply replace the initial states of stationary manipulation skills with those of mobile ones. We reject the initial states that the target is not reachable due to the kinematic constraint. The constraint is checked via inverse kinematics (IK). The extended experiment is denoted by S(L+IK)+P. Fig 8 shows the quantitative results. The overall success rate of S(L+IK)+P is 44.7%/21.1% in the cross-configuration/crosslayout setting. It indicates that increasing the feasible initial states help stationary manipulation skills compared to S(L)+P (37.7%/18.1%), but still has a large performance drop compared to S+P (57.4%/31.1%). One possible reason is that although the target might be IK-reachable, it can be hard to achieve with stationary manipulation skills due to collision with other objects. However, mobile manipulation skills can first navigate to better locations with fewer obstacles in the front.\n\nG MORE QUANTITATIVE METRICS\n\nIn this section, we present more quantitative metrics in addition to progressive completion rates for the main experiments on 3 HAB tasks. We report the number of successfully placed objects and the average distance between objects and goals in Table 1 and 2. These metrics are analogous to %FIXEDSTRICT and %E in Weihs et al. (2021).\n\nMethod\n\nTidyHouse (5)\n\nPrepareGroceries (3)\n\nSetTable (2)\n\nS+P M+P M3 (ours)\n\n4.58/4.56 4.45/4.54 4.64/4.60\n\n2.33/1.54 2.45/1.79 2.56/2.44\n\n1.45/1.03 1.71/1.08 1.71/1.22\n\nTable 1: The number of successfully placed objects for HAB tasks. The metrics in the crossconfiguration/cross-layout setting are reported. The number of objects to place is shown along with the name of each task.\n\n20\n\npick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success rateS+PS(L)+PS(L+IK)+Ppick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success rateS+PS(L)+PS(L+IK)+Popen_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success rateS+PS(L)+PS(L+IK)+Ppick_0place_0pick_1place_1pick_2place_2pick_3place_3pick_4place_40.00.10.20.30.40.50.60.70.80.91.0Success rateS+PS(L)+PS(L+IK)+Ppick_0place_0pick_1place_1pick_2place_20.00.10.20.30.40.50.60.70.80.91.0Success rateS+PS(L)+PS(L+IK)+Popen_0pick_0place_0close_0open_1pick_1place_1close_10.00.10.20.30.40.50.60.70.80.91.0Success rateS+PS(L)+PS(L+IK)+PPublished as a conference paper at ICLR 2023\n\nMethod\n\nTidyHouse\n\nPrepareGroceries\n\nSetTable\n\nS+P M+P M3 (ours)\n\n0.245/0.256 1.296/0.821 0.198/0.174\n\n0.338/0.982 0.302/0.689 0.239/0.337\n\n3.506/6.481 4.165/4.641 5.208/6.345\n\nTable 2: Average distance between objects and goals for HAB tasks. The metrics in the crossconfiguration/cross-layout setting are reported. Note that the average distance is sensitive to outliers.\n\n(a) Stationary manipulation and point-goal navigation (S+P)\n\n(b) Mobile manipulation and point-goal navigation (M+P)\n\n(c) Mobile manipulation and region-goal navigation (M3)\n\nFigure 9: Qualitative comparison in TidyHouse. In this example, the point-goal navigation skill terminates behind the TV (1st image). The arm is blocked by the TV in stationary manipulation (last image in the top row). The robot manages to move backward and avoid being blocked in mobile manipulation (last image in the middle row). The region-goal navigation skill instead terminates in front of the TV (1st image in the bottom row).\n\nH MORE QUALITATIVE RESULTS\n\nFig 9, 10, 11 show more qualitative comparison of different methods. Their animated versions can be found on our project website.\n\n21\n\nPublished as a conference paper at ICLR 2023\n\n(a) Point-goal navigation\n\n(b) Region-goal navigation\n\nFigure 10: Qualitative comparison in PrepareGroceries. In this example, the point-goal navigation skill accidentally close the fridge (top row). The region-goal navigation skill is able to avoid disturbing the environment due to the collision penalty (bottom row).\n\n(a) Stationary manipulation\n\n(b) Mobile manipulation\n\nFigure 11: Qualitative comparison in SetTable. In this example, the navigation skill terminates at the position where the robot can not reach the target object in the fridge in stationary manipulation. (top row). The robot can move closer to the object and then pick it, to compensate for the navigation skill in mobile manipulation (bottom row).\n\n22\n\nPublished as a conference paper at ICLR 2023\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\n43\n\n44\n\n45\n\npick_0: - \"holding(target_obj|0)\" place_0: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" pick_1: - \"holding(target_obj|1)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" place_1: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" pick_2: - \"holding(target_obj|2)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" place_2: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" - \"at(target_obj_pos|2,target_goal_pos|2)\" pick_3: - \"holding(target_obj|3)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" - \"at(target_obj_pos|2,target_goal_pos|2)\" place_3: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" - \"at(target_obj_pos|2,target_goal_pos|2)\" - \"at(target_obj_pos|3,target_goal_pos|3)\" pick_4: - \"holding(target_obj|4)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" - \"at(target_obj_pos|2,target_goal_pos|2)\" - \"at(target_obj_pos|3,target_goal_pos|3)\" place_4: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" - \"at(target_obj_pos|2,target_goal_pos|2)\" - \"at(target_obj_pos|3,target_goal_pos|3)\" - \"at(target_obj_pos|4,target_goal_pos|4)\"\n\nListing 1: Stage goals and their associated predicates defined for TidyHouse. The stages are listed in the order for progressive evaluation.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\npick_0: - \"holding(target_obj|0)\" place_0: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" pick_1: - \"holding(target_obj|1)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" place_1: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" pick_2: - \"holding(target_obj|2)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" place_2: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"at(target_obj_pos|1,target_goal_pos|1)\" - \"at(target_obj_pos|2,target_goal_pos|2)\"\n\nListing 2: Stage goals and their associated predicates defined for PrepareGroceries. The stages are listed in the order for progressive evaluation.\n\n24\n\nPublished as a conference paper at ICLR 2023\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\nopen_0: - \"opened_drawer(target_marker|0)\" pick_0: - \"holding(target_obj|0)\" place_0: - \"not_holding()\" - \"at(target_obj_pos|0,target_goal_pos|0)\" close_0: - \"closed_drawer(target_marker|0)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" open_1: - \"closed_drawer(target_marker|0)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"opened_fridge(target_marker|1)\" pick_1: - \"closed_drawer(target_marker|0)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"opened_fridge(target_marker|1)\" - \"holding(target_obj|1)\" place_1: - \"closed_drawer(target_marker|0)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"not_holding()\" - \"at(target_obj_pos|1,target_goal_pos|1)\" close_1: - \"closed_drawer(target_marker|0)\" - \"at(target_obj_pos|0,target_goal_pos|0)\" - \"closed_fridge(target_marker|1)\" - \"at(target_obj_pos|1,target_goal_pos|1)\"\n\nListing 3: Stage goals and their associated predicates defined for SetTable. The stages are listed in the order for progressive evaluation.\n\n25",
    "reference": "# Summary Of The Paper\n\nThis submission proposes a modular solution for the task of multi-skill manipulation for object rearrangement. The addressed topic is an emerging one in the domain of embodied AI. The work mainly aims to tackle the issue of inaccurate terminal positions of the navigation skill. To mitigate the errors of positions (e.g., not satisfying some kinematic constraints), the new solution adopt a region-goal navigation reward, instead of point-goal reward in existing methods. The idea has been validated on ReplicaCAS and Habitat simulator. The experimental results are positive.\n\n# Strength And Weaknesses\n\nStrength:\n- The proposed new solution is reasonably improving the performance of a home assisting model. By using region-goal navigation award, the model has higher tolerance for the errors caused in the stage of navigation. \n- The writing is good. I have no complaints about the writing.\n\nWeakness:\n- The experiments are superficially treated. It is crucial for the readers to understand which key designs brought the performance elevation. Although two ablation studies (w.r.t. initial states or collision penalty) are provided, the proof is mainly based on comparing the success rates of different settings or methods. No further in-depth analysis is given.\n- The technical contribution is quite limited. The modification to conventional pipelines is the inclusion of region-goal (rather than point-goal) reward. Though it is proved to be effectiveness by various experiments, one can hardly claim this represents some significant contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: as commented before, the writing is excellent and clearly conveys the key ideas.\nQuality: this is a typical borderline paper in my batch of assigned papers. While the writing / ideas are reasonably good, the technical significant should be further clarified.\nNovelty: the idea of using region-goals is somewhat incremental.\nReproducibility: The appendix provides various technical details for the proposed model, including the choosing of parameters and other implementation details. Normally one can re-implement the base code and achieve roughly some performance of the same level as reported in the paper. There seem no hidden trick in the model. Overall I would regard the re-implementation is non-trivial but achievable.\n\n# Summary Of The Review\n\nEmbodied AI is an emerging and challenging research task. This work attempts to contribute an improved model for manipulation-based object rearrangement. I have several concerns regarding the current version that makes me hesitate to recommend acceptance.\n\nThe technical significance of the proposed method should be clarified. For example, Section 4.2 only briefly describe the proposed region-goal navigation reward, without much convincing explanation of the key choices (such as \"remove the angular term\" since \"find that the success reward is sufficient to encourage correct orientation\"). This makes the draft somewhat more like a technical report. The corresponding experiments faithful report the success rates, from which the main claims are drawn. A few qualitative examples (e.g., Figure 4) are provided for better illustration. However, in-depth analysis is still missing. I would strongly suggest to make the sections of introduce, related work and preliminary knowledge more concise, leaving more space for more quantitative studies and ablation studies.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nThere are no ethics concerns."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\n3D EQUIVARIANT DIFFUSION FOR TARGET-AWARE MOLECULE GENERATION AND AFFINITY PREDICTION\n\nJiaqi Guan1∗, Wesley Wei Qian1∗, Xingang Peng2, Yufeng Su1, Jian Peng1, Jianzhu Ma3 1 Department of Computer Science, University of Illinois Urbana-Champaign 2 School of Intelligence Science and Technology, Peking University 3 Institute for AI industry Research, Tsinghua University {jiaqi, weiqian3, jianpeng}@illinois.edu, majianzhu@air.tsinghua.edu.cn\n\nABSTRACT\n\nRich data and powerful machine learning models allow us to design drugs for a specific protein target in silico. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining.\n\n1\n\nINTRODUCTION\n\nRational drug design against a known protein binding pocket is an efficient and economical approach for finding lead molecules (Anderson, 2003; Batool et al., 2019) and has attracted growing attention from the research community. However, it remains challenging and computationally intensive due to the large synthetically feasible space (Ragoza et al., 2022), and high degrees of freedom for binding poses (Hawkins, 2017). Previous prevailed molecular generative models are based on either molecular string representation (Bjerrum and Threlfall, 2017; Kusner et al., 2017; Segler et al., 2018) or graph representation (Li et al., 2018; Liu et al., 2018; Jin et al., 2018; Shi et al., 2020), but both representations do not take the 3D spatial interaction into account and therefore not well suited for target-aware molecule generation. With recent development in structural biology and protein structure prediction (Jumper et al., 2021), more structural data become available (Francoeur et al., 2020) and unlock new opportunities for machine learning algorithms to directly design drugs inside 3D binding complex (Gebauer et al., 2019; Simm et al., 2020a;b).\n\nRecently, new generation of generative models are proposed specifically for the target-aware molecule generation task (Luo et al., 2021; Ragoza et al., 2022; Tan et al., 2022; Liu et al., 2022; Peng et al., 2022). However, existing approaches suffer from several drawbacks. For instance, Tan et al. (2022) does not explicitly model the interactions between atoms of molecules and proteins in the 3D space, but only considers the target as intermediate conditional embeddings. For those that do consider the atom interactions in the 3D space, Ragoza et al. (2022) represents the 3D space as voxelized grids and model the proteins and molecules using 3D Convolutional Neural Networks (CNN). However, this model is not rotational equivariant and cannot fully capture the 3D inductive\n\n∗Equal Contribution\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nbiases. In addition, the voxelization operation will lead to poor scalability since the number of voxels increases at a cubic rate to the pocket size. Advanced approaches achieve SE(3)-equivariance through different modeling techniques (Luo et al., 2021; Liu et al., 2022; Peng et al., 2022). However, these methods adopt autoregressive sampling, where atoms are generated one by one based on the learned probability density of atom types and atom coordinates. These approaches suffer from several limitations: First, the mismatch between training and sampling incurs exposure bias. Secondly, the model assigns an unnatural generation order during sampling and cannot consider the probability of the entire 3D structure. For instance, it would be easy for the model to correctly place the n-th atom to form a benzene ring if the n − 1-th carbon atoms have already been placed in the same plane. However, it would be difficult for the model to place the first several atoms accurately since there is limited context information available, which yields unrealistic fragments as a consequence. Moreover, the sampling scheme does not scale well when generating large binding molecules is necessary. Finally, current autoregressive models could not estimate the quality of generated molecules. One has to rely on other tools based on physical-chemical energy functions such as AutoDock (Trott and Olson, 2010) to select the drug candidates.\n\nTo address these problems, we propose TargetDiff, a 3D full-atom diffusion model that generates target-aware molecules in a non-autoregressive fashion. Thanks to recent progress in probabilistic diffusion models (Ho et al., 2020; Hoogeboom et al., 2021) and equivariant neural networks (Fuchs et al., 2020; Satorras et al., 2021b), our proposed model can generate molecules in continuous 3D space based on the context provided by protein atoms, and have the invariant likelihood w.r.t global translation and rotation of the binding complex. Specifically, we represent the protein binding pockets and small molecules as atom point sets in the 3D space where each atom is associated with a 3D Cartesian coordinate. We define a diffusion process for both continuous atom coordinates and discrete atom types where noise is gradually added, and learn the joint generative process with a SE(3)-equivariant graph neural network which alternately updates the atom hidden embedding and atom coordinates of molecules. Under certain parameterization, we can extract representative features from the model by forward passing the input molecules once without retraining. We find these features provide strong signals to estimate the binding affinity between the sampled molecule and target protein, which can then be used for ranking drug candidates and improving other supervised learning frameworks for binding affinity prediction. An empirical study on the CrossDocked2020 dataset (Francoeur et al., 2020) shows that TargetDiff generates molecules with more realistic 3D structures and better binding energies towards the protein binding sites compared to the baselines.\n\nOur main contributions can be summarized as follows:\n\n• An end-to-end framework for generating molecules conditioned on a protein target, which explicitly considers the physical interaction between proteins and molecules in 3D space.\n\n• So far as we know, this is the first probabilistic diffusion formulation for target-aware drug design, where training and sampling procedures are aligned in a non-autoregressive as well as SE(3)-equivariant fashion thanks to a shifting center operation and equivariant GNN.\n\n• Several new evaluation metrics and additional insights that allow us to evaluate the model generated molecules in many different dimensions. The empirical results demonstrate the superiority of our model over two other representative baselines.\n\n• Propose an effective way to evaluate the quality of generated molecules based on our framework, where the model can be served as either a scoring function to help ranking or an unsupervised feature extractor to improve binding affinity prediction.\n\n2 RELATED WORK\n\nMolecule Generation with Different Representations Based on different levels of representations, existing molecular generative models can be roughly divided into three categories - stringbased, graph-based, and 3D-structure-based. The most common molecular string representation is SMILES (Weininger, 1988), where many existing language models such as RNN can be re-purposed for the molecule generation task (Bjerrum and Threlfall, 2017; G ́omez-Bombarelli et al., 2018; Kusner et al., 2017; Segler et al., 2018). However, SMILES representation is not an optimal choice since it fails to capture molecular similarities and suffers from the validity issue during the generation phase (Jin et al., 2018). Thus, many graph-based methods are proposed to operate directly on\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ngraphs (Liu et al., 2018; Shi et al., 2020; Jin et al., 2018; 2020; You et al., 2018; Zhou et al., 2019). On the other hand, these methods are very limited in modeling the spatial information of molecules that is crucial for determining molecular properties and functions. Therefore, recent work (Gebauer et al., 2019; Skalic et al., 2019a; Ragoza et al., 2020; Simm et al., 2020a;b) focus on generating molecules in 3D space. More recently, flow-based and diffusion-based generative models (Satorras et al., 2021a; Hoogeboom et al., 2022) are developed to leverage E(n)-Equivariant GNN (Satorras et al., 2021b) and achieve SE(3)-equivariance in molecule generation.\n\nTarget-Aware Molecule Generation As more structural data become available, various generative models are proposed to solve the target-aware molecule generation task. For example, Skalic et al. (2019b); Xu et al. (2021) generate SMILES based on protein contexts. Tan et al. (2022) propose a flow-based model to generate molecular graphs conditional on a protein target as a sequence embedding. Ragoza et al. (2022) try to generate 3D molecules by voxelizing molecules in atomic density grids in a conditional VAE framework. Li et al. (2021) leverage Monte-Carlo Tree Search and a policy network to optimize molecules in 3D space. Luo et al. (2021); Liu et al. (2022); Peng et al. (2022) develop autoregressive models to generate molecules atom by atom in 3D space with GNNs. Despite the progress made in this direction, the models still suffer from several issues, including separately encoding the small molecules and protein pockets (Skalic et al., 2019b; Xu et al., 2021; Tan et al., 2022; Ragoza et al., 2022), relying on voxelization and non-equivariance networks (Skalic et al., 2019b; Xu et al., 2021; Ragoza et al., 2022), and autoregressive sampling (Luo et al., 2021; Liu et al., 2022; Peng et al., 2022). Different from all these models, our equivariant model explicitly considers the interaction between proteins and molecules in 3D and can perform non-autoregressive sampling, which better aligns the training and sampling procedures.\n\nDiffusion Models Diffusion models (Sohl-Dickstein et al., 2015) are a new family of latent variable generative models. Ho et al. (2020) propose denoising diffusion probabilistic models (DDPM) which establishes a connection between diffusion models and denoising score-based models (Song and Ermon, 2019). The diffusion models have shown remarkable success in generating image data (Ho et al., 2020; Nichol and Dhariwal, 2021) and discrete data such as text (Hoogeboom et al., 2021; Austin et al., 2021). Recently, it has also been applied in the domain of molecules. For example, GeoDiff (Xu et al., 2022) generates molecular conformations given 2D molecular graphs. EDM (Hoogeboom et al., 2022) generates 3D molecules. However, the unawareness to potential targets make it hard to be utilized by biologists in real scenarios.\n\n3 METHODS\n\n3.1 PROBLEM DEFINITION\n\nA protein binding site is represented as a set of atoms P = {(x(i) i=1, where NP is the number of protein atoms, xP ∈ R3 represents the 3D coordinates of the atom, and vP ∈ RNf represents protein atom features such as element types and amino acid types. Our goal is to generate binding molecules M = {(x(i) i=1 conditioned on the protein target. For brevity, we denote molecules as M = [x, v], where [·, ·] is the concatenation operator and x ∈ RM ×3 and v ∈ RM ×K denote atom Cartesian coordinates and one-hot atom types respectively.\n\nL )}NM\n\nL , v(i)\n\nP , v(i)\n\nP )}NP\n\n3.2 OVERVIEW OF TARGETDIFF\n\nAs discussed in Sec. 1 and Sec. 2, we hope to develop a non-autoregressive model to bypass the drawbacks raised in autoregressive sampling models. In addition, we also require the model to represent the protein-ligand complex in continuous 3D space to avoid the voxelization operation. Last but not least, the model will also need to be SE(3)-equivariant to global translation and rotation.\n\nWe therefore develop TargetDiff, an equivariant non-autoregressive method for target-aware molecule generation based on the DDPM framework Ho et al. (2020). TargetDiff is a latent variable model of the form pθ(M0|P) = (cid:82) pθ(M0:T |P)dM1:T , where M1, M2, · · · , MT is a sequence of latent variables with the same dimensionality as the data M0 ∼ p(M0|P). As shown in Fig. 1, the approach includes a forward diffusion process and a reverse generative process, both defined as Markov chains. The diffusion process gradually injects noise to data, and the generative process\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: Overview of TargetDiff. The diffusion process gradually injects noise to the data, and the generative process learns to recover the data distribution from the noise distribution with a network parameterized by θ.\n\nlearns to recover data distribution from the noise distribution with a network parameterized by θ:\n\nq(M1:T |M0, P) = ΠT\n\nt=1q(Mt|Mt−1, P)\n\npθ(M0:T −1|MT , P) = ΠT\n\nt=1pθ(Mt−1|Mt, P)\n\n(1)\n\nSince our goal is to generate 3D molecules based on a given protein binding site, the model needs to generate both continuous atom coordinates and discrete atom types, while keeping SE(3)-equivariant during the entire generative process. In the following section, we will elaborate on how we construct the diffusion process, parameterize the generative process, and eventually train the model.\n\n3.3 MOLECULAR DIFFUSION PROCESS\n\nFollowing recent progress in learning continuous distributions Ho et al. (2020) and discrete distributions Hoogeboom et al. (2021) with diffusion models, we use a Gaussian distribution N to model continuous atom coordinates x and a categorical distribution C to model discrete atom types v. The atom types are constructed as a one-hot vector containing information such as element types and membership in an aromatic ring. We formulate the molecular distribution as a product of atom coordinate distribution and atom type distribution. At each time step t, a small Gaussian noise and a uniform noise across all categories are added to atom coordinates and atom types separately, according to a Markov chain with fixed variance schedules β1, . . . , βT :\n\nq(Mt|Mt−1, P) = N (xt; (cid:112)1 − βtxt−1, βtI) · C(vt|(1 − βt)vt−1 + βt/K).\n\n(2)\n\nWe note that the schedules can be different in practice, but we still denote them with the same symbol for conciseness. Here, we decompose the joint molecule distribution as the product of two independent distributions of atom coordinates and atom types during diffusion, because the independent distributions have concise mathematical formulations and we can efficiently draw noisy samples from them. In the next section, we will see the dependencies between atom coordinates and atom types are considered by the model in the generative process.\n\nDenoting αt = 1 − βt and ̄αt = Πt the noisy data distribution q(Mt|M0) of any time step in closed-form:\n\ns=1αs, a desirable property of the diffusion process is to calculate\n\nq(xt|x0) = N (xt;\n\n ̄αtx0, (1 − ̄αt)I)\n\nq(vt|v0) = C(vt| ̄αtv0 + (1 − ̄αt)/K).\n\n(3)\n\n√\n\nUsing Bayes theorem, the normal posterior of atom coordinates and categorical posterior of atom types can both be computed in closed-form:\n\nq(xt−1|xt, x0) = N (xt−1; ̃μt(xt, x0), ̃βtI)\n\nq(vt−1|vt, v0) = C(vt−1| ̃ct(vt, v0)).\n\n(4)\n\nwhere ̃μt(xt, x0) =\n\n√\n\n ̄αt−1βt 1− ̄αt\n\nx0 +\n\n√\n\nαt(1− ̄αt−1)\n\n1− ̄αt\n\nxt, ̃βt = 1− ̄αt−1\n\n1− ̄αt\n\nβt,\n\nand ̃ct(vt, v0) = c⋆/ (cid:80)K\n\nk=1 c⋆\n\nk and c⋆(vt, v0) = [αtvt + (1 − αt)/K] ⊙ [ ̄αt−1v0 + (1 − ̄αt−1)/K].\n\n3.4 PARAMETERIZATION OF EQUIVARIANT MOLECULAR GENERATIVE PROCESS\n\nThe generative process, on reverse, will recover the ground truth molecule M0 from the initial noise MT , and we approximate the reverse distribution with a neural network parameterized by θ:\n\npθ(Mt−1|Mt, P) = N (xt−1; μθ([xt, vt], t, P), σ2\n\nt I) · C(vt−1|cθ([xt, vt], t, P)).\n\n(5)\n\n4\n\n(cid:171)(cid:171)(cid:39)(cid:76)(cid:73)(cid:73)(cid:88)(cid:86)(cid:76)(cid:82)(cid:81)(cid:3)(cid:51)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:42)(cid:72)(cid:81)(cid:72)(cid:85)(cid:68)(cid:87)(cid:76)(cid:89)(cid:72)(cid:3)(cid:51)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:171)(cid:171)(cid:171)(cid:171)(cid:171)(cid:171)(cid:44)(cid:81)(cid:76)(cid:87)(cid:76)(cid:68)(cid:79)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)Published as a conference paper at ICLR 2023\n\nOne desired property of the generative process is that the likelihood pθ(M0|P) should be invariant to translation and rotation of the protein-ligand complex, which is a critical inductive bias for generating 3D objects such as molecules (K ̈ohler et al., 2020; Satorras et al., 2021a; Xu et al., 2022; Hoogeboom et al., 2022). One important piece of evidence for such achievement is that an invariant distribution composed with an equivariant transition function will result in an invariant distribution. Leveraging this evidence, we have the following proposition in the setting of target-aware molecule\n\nProposition 1. Denoting the SE(3)-transformation as Tg, we could achieve invariant likelihood w.r.t Tg on the protein-ligand complex: pθ(Tg(M0|P)) = pθ(M0|P) if we shift the Center of Mass (CoM) of protein atoms to zero and parameterize the Markov transition p(xt−1|xt, xP ) with an SE(3)-equivariant network.\n\nA slight abuse of notation in the following is that we use xt(t = 1, . . . , T ) to denote ligand atom coordinates and xP to denote protein atom coordinates. We analyze the operation of shifting CoM in Appendix B and prove the invariant likelihood in Appendix C.\n\nThere are different ways to parameterize μθ([xt, vt], t, P) and cθ([xt, vt], t, P). Here, we choose to let the neural network predict [x0, v0] and feed it through equation 4 to obtain μθ and cθ which define the posterior distributions. Inspired from recent progress in equivariant neural networks (Thomas et al., 2018; Fuchs et al., 2020; Satorras et al., 2021b; Guan et al., 2022), we model the interaction between the ligand molecule atoms and the protein atoms with a SE(3)-Equivariant GNN:\n\n[ˆx0, ˆv0] = φθ(Mt, t, P) = φθ([xt, vt], t, P).\n\n(6)\n\nAt the l-th layer, the atom hidden embedding h and coordinates x are updated alternately as follows:\n\nhl+1\n\ni = hl\n\ni +\n\nxl+1\n\ni = xl\n\ni +\n\n(cid:88)\n\nj∈V,i̸=j (cid:88)\n\nj∈V,i̸=j\n\nfh(dl\n\nij, hl\n\ni, hl\n\nj, eij; θh)\n\n(xl\n\ni − xl\n\nj)fx(dl\n\nij, hl+1\n\ni\n\n, hl+1\n\nj\n\n, eij; θx) · 1mol\n\n(7)\n\nwhere dij = ∥xi − xj∥ is the euclidean distance between two atoms i and j and eij is an additional feature indicating the connection is between protein atoms, ligand atoms or protein atom and ligand atom. 1mol is the ligand molecule mask since we do not want to update protein atom coordinates. The initial atom hidden embedding h0 is obtained by an embedding layer that encodes the atom information. The final atom hidden embedding hL is fed into a multi-layer perceptron and a softmax function to obtain ˆv0. Since ˆx0 is rotation equivariant to xt and it is easy to see xt−1 is rotation equivariant to x0 according to equation 4, we achieve the desired equivariance for Markov transition. The complete proof can be found in Appendix A.\n\n3.5 TRAINING OBJECTIVE\n\nThe combination of q and p is a variational auto-encoder (Kingma and Welling, 2013). The model can be trained by optimizing the variational bound on negative log likelihood. For the atom coordinate loss, since q(xt−1|xt, x0) and pθ(xt−1|xt) are both Gaussian distributions, the KL-divergence can be written in closed form:\n\nL(x)\n\nt−1 =\n\n1 2σ2 t\n\n∥ ̃μt(xt, x0) − μθ([xt, vt], t, P)∥2 + C = γt∥x0 − ˆx0∥2 + C\n\n(8)\n\nwhere γt = ̄αt−1β2 t (1− ̄αt)2 and C is a constant. In practice, training the model with an unweighted MSE loss (set γt = 1) could also achieve better performance as Ho et al. (2020) suggested. For the atom type loss, we can directly compute KL-divergence of categorical distributions as follows:\n\n2σ2\n\nt\n\nL(v)\n\nt−1 =\n\n(cid:88)\n\nk\n\nc(vt, v0)k log\n\nc(vt, v0)k c(vt, ˆv0)k\n\n.\n\n(9)\n\nThe final loss is a weighted sum of atom coordinate loss and atom type loss: L = L(x) We summarize the overall training and sampling procedure of TargetDiff in Appendix E.\n\nt−1 + λL(v) t−1.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n3.6 AFFINITY RANKING AND PREDICTION AS UNSUPERVISED LEARNER\n\nGenerative models are unsupervised learners. However, in the area of target-aware molecule generation, nobody has established the connection between the generative model and binding affinity, which is an important indicator for evaluating generated molecules. Existing generative models can not (accurately) estimate the quality of generated molecules. Especially for models relying on autoregressive sampling, they have to assign an unnatural order when performing likelihood estimation (if possible) and cannot capture the global context as a whole.\n\nWe first establish the connection between unsupervised generative models and binding affinity ranking / prediction. Under our parameterization, the network predicts the denoised [ˆx0, ˆv0]. Given the protein-ligand complex, we can feed φθ with [x0, v0] while freezing the x-update branch (i.e. only atom hidden embedding h is updated), and we could finally obtain hL and ˆv0:\n\nhl+1\n\ni = hl\n\ni +\n\n(cid:88)\n\nj∈V,i̸=j\n\nfh(dl\n\nij, hl\n\ni, hl\n\nj, eij; θh) l=1...L−1\n\nˆv0 = softmax(MLP(hL)).\n\n(10)\n\nOur assumption is that if the ligand molecule has a good binding affinity to protein, the flexibility of atom types should be low, which could be reflected in the entropy of ˆv0. Therefore, it can be used as a scoring function to help ranking, whose effectiveness is justified in the experiments. In addition, hL also includes useful global information. We found the binding affinity ranking performance can be greatly improved by utilizing this feature with a simple linear transformation.\n\n4 EXPERIMENTS\n\n4.1 SETUP\n\nData We use CrossDocked2020 (Francoeur et al., 2020) to train and evaluate TargetDiff. Similar to Luo et al. (2021), we further refined the 22.5 million docked protein binding complexes by only selecting the poses with a low (< 1 ̊A) and sequence identity less than 30%. In the end, we have 100,000 complexes for training and 100 novel complexes as references for testing.\n\nBaseline For benchmarking, we compare with various baselines: liGAN (Ragoza et al., 2022), AR (Luo et al., 2021), Pocket2Mol (Peng et al., 2022), and GraphBP (Liu et al., 2022). liGAN is a 3D CNN-based method that generates 3D voxelized molecular images following a conditional VAE scheme. AR, Pocket2Mol and GraphBP are all GNN-based methods that generate 3D molecules by sequentially placing atoms into a protein binding pocket. We choose AR and Pocket2Mol as representative baselines with autoregressive sampling scheme because of their good empirical performance. All baselines are considered in Table 3 for a comprehensive comparison.\n\nTargetDiff Our model contains 9 equivariant layers described in equation 7, where fh and fx are specifically implemented as graph attention layers with 16 attention heads and 128 hidden features. We first decide on the number of atoms for sampling by drawing a prior distribution estimated from training complexes with similar binding pocket sizes. After the model finishes the generative process, we then use OpenBabel (O’Boyle et al., 2011) to construct the molecule from individual atom coordinates as done in AR and liGAN. Please see Appendix F for the full details.\n\n4.2 TARGET-AWARE MOLECULE GENERATION\n\nWe propose a comprehensive evaluation framework for target-aware molecule generation to justify the performance of our model and baselines from the following perspectives: molecular structures, target binding affinity and molecular properties.\n\nMolecular Structures First, we plot the empirical distributions of all-atom distances and carboncarbon bond distances in Figure 2, then compare them against the same empirical distributions for reference molecules. For overall atom distances, TargetDiff captures the overall distribution very well, while AR and Pocket2Mol has an over-representation for small atom distances. Due to its limited voxelized resolution, liGAN can only capture the overall shape but not specify modes. Similarly, different carbon-carbon bonds form two representative distance modes in reference molecular\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nBond liGAN AR Pocket2Mol TargetDiff\n\nC−C 0.601\n\n0.609\n\nC=C 0.665\n\n0.620\n\nC−N 0.634\n\n0.474\n\nC=N 0.749\n\n0.635\n\nC−O 0.656\n\n0.492\n\nC=O 0.661\n\n0.558\n\nC:C\n\n0.497\n\n0.451\n\nC:N 0.638\n\n0.552\n\n0.496\n\n0.561\n\n0.416\n\n0.629\n\n0.454\n\n0.516\n\n0.416\n\n0.487\n\n0.369\n\n0.505\n\n0.363\n\n0.550\n\n0.421\n\n0.461\n\n0.263\n\n0.235\n\nFigure 2: Comparing the distribution for distances of allatom (top row) and carbon-carbon pairs (bottom row) for reference molecules in the test set (gray) and model generJensen-Shannon divergence (JSD) ated molecules (color). between two distributions is reported.\n\nTable 1: Jensen-Shannon divergence between the distributions of bond distance for reference vs. generated molecules. ”-”, ”=”, and ”:” represent single, double, and aromatic bonds, respectively. A lower value is better.\n\nRing Size Ref.\n\nliGAN AR\n\nPocket2Mol TargetDiff\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1.7% 28.1% 29.9%\n\n0.0% 15.7% 0.0%\n\n30.2% 29.8% 16.0%\n\n67.4% 22.7% 51.2%\n\n0.7% 2.6% 1.7%\n\n0.0% 0.8% 0.7%\n\n0.0% 0.3% 0.5%\n\n0.1%\n\n0.0%\n\n16.4%\n\n80.4%\n\n2.6%\n\n0.3%\n\n0.1%\n\n0.0%\n\n2.8%\n\n30.8%\n\n50.7%\n\n12.1%\n\n2.7%\n\n0.9%\n\nFigure 3: Median RMSD for rigid fragment before and after the force-field optimization.\n\nTable 2: Percentage of different ring sizes for reference and model generated molecules.\n\nstructures. While we can still see the two modes in TargetDiff generated structures, only a single mode is observed for ones generated by liGAN, AR, and Pocket2Mol. In Table 1, we further evaluated how well different generated molecular structures capture the empirical distributions of bond distances in reference molecules, measured by Jensen-Shannon divergence (JSD) Lin (1991). We found that TargetDiff outperforms other methods with a clear margin across all major bond types.\n\nSecondly, we investigate whether TargetDiff can generate rigid sub-structure / fragment in a consistent fashion (e.g., all carbons in a benzene ring are in the same plane). To measure such consistency, we optimize the generated structure with Merck Molecular Force Field (MMFF) Halgren (1996) and calculate the RMSD between pre-and pos- MMFF-optimized coordinates for different rigid fragments that do not contain any rotatable bonds. As shown in Figure 3, TargetDiff is able to generate more consistent rigid fragments. In a further analysis, we discover that liGAN and AR tend to generate a large amount of 3- and 4- member rings (Table 2). While TargetDiff shows a larger proportion of 7-member-ring, we believe this represents a limitation in the reconstruction algorithm and could be an interesting future direction to replace such post-hoc operation with bond generation.\n\nThese results suggest that TargetDiff can produce more realistic molecular structures throughout the process compared to existing baselines and therefore perceive a more accurate 3D representation of the molecular dynamics leading to better protein binders.\n\nTarget Binding Affinity Figure 4 shows the median Vina energy (computed by AutoDock Vina (Eberhardt et al., 2021)) of all generated molecules for each binding pocket. Based on the Vina energy, generated molecules from TargetDiff show the best binding affinity in 57% of the targets, while the ones from liGAN, AR and Pocket2Mol are only best for 4%, 13% and 26% of all targets. In terms of high-affinity binder, we find that on average 58.1% of the TargetDiff molecules show better binding affinity than the reference molecule, which is clearly better than other baselines (See\n\n7\n\n024681012Distance of all atom pairs (Å)0.00.10.20.30.40.5DensityliGAN JSD: 0.170024681012Distance of all atom pairs (Å)0.00.10.20.30.40.5ARPocket2MolJSD: 0.119JSD: 0.138024681012Distance of all atom pairs (Å)0.00.10.20.30.40.5TargetDiff JSD:0.0890.000.250.500.751.001.251.501.752.00Distance of carbon carbon bond (Å)024681012DensityliGAN JSD: 0.528referenceliGAN0.000.250.500.751.001.251.501.752.00Distance of carbon carbon bond (Å)024681012ARPocket2MolJSD: 0.426JSD: 0.385referenceARPocket2Mol0.000.250.500.751.001.251.501.752.00Distance of carbon carbon bond (Å)024681012TargetDiff JSD: 0.225referenceTargetDiff3456789Fragment Size0.00.20.40.60.81.0Median RMSD (Å)liGANARPocket2MolTargetDiffPublished as a conference paper at ICLR 2023\n\nFigure 4: Median Vina energy for different generated molecules (liGAN vs. AR vs. TargetDiff) across 100 testing binding targets. Binding targets are sorted by the median Vina energy of TargetDiff generated molecules. Lower Vina energy means a higher estimated binding affinity.\n\nFigure 5: a) Binding poses for two poor-AR-binding pockets. b) Estimating the shift in binding poses between generated and reference molecules by calculating the distance between their center of mass (CoM). Each point represents the median CoM distance for one target.\n\nTable 3). We further compute Vina Score and Vina Min in Table 3, where the Vina score function is directly computed or locally optimized without re-docking. They directly reflect the quality of model generated 3D molecules and similarly, our model outperforms all other baselines.\n\nTo better understand the differences in generated molecules, we sample a generated molecule from each model for two pockets where TargetDiff outperforms AR. As shown in Figure 5a, while TargetDiff can generate molecules occupying the entire pocket, AR is only able to generate a molecule that covers part of the space and potentially loses its specificity for the desired target and cause offtarget effects. Let us consider the AR-generated molecules for 4QLK A. Despite having a similar number of atoms as the TargetDiff molecule (27 vs. 29), the frontier network in AR keeps placing molecules deep inside the pocket instead of considering the global structure, and trying to cover the entire binding pocket results in poor binding affinity. To further quantify such effects, we measure the distance between the center of mass (CoM) for reference molecules and the CoM for generated molecules. As shown in Figure 5b, the sequential generation nature of AR results in a larger shift in CoM (1.79 ̊A vs. 1.45 ̊A) and presents sub-optimal binding poses with poorer binding affinity. Molecular Properties Besides binding affinity, we further investigate other molecular properties for generated molecules, including drug likeliness QED (Bickerton et al., 2012), synthesizability SA (Ertl and Schuffenhauer, 2009; You et al., 2018), and diversity computed as the average pairwise\n\nMetric\n\nModel\n\nVina Score (↓) Vina Min (↓) Vina Dock (↓) High Affinity (↑)\n\nQED (↑)\n\nSA (↑)\n\nDiversity (↑)\n\nAvg. Med.\n\nAvg. Med. Avg. Med.\n\nAvg.\n\nMed.\n\nAvg. Med. Avg. Med. Avg. Med.\n\nliGAN ∗ GraphBP ∗\n\nAR\n\nPocket2Mol\n\nTargetDiff\n\nReference\n\n-\n\n-\n\n-5.75\n\n-5.14\n\n-5.47\n\n-6.36\n\n-\n\n-\n\n-5.64\n\n-4.70\n\n-6.30\n\n-6.46\n\n-\n\n-\n\n-6.18\n\n-6.42\n\n-6.64\n\n-6.71\n\n-\n\n-\n\n-5.88\n\n-5.82\n\n-6.83\n\n-6.49\n\n-6.33\n\n-4.80\n\n-6.75\n\n-7.15\n\n-7.80\n\n-7.45\n\n-6.20\n\n-4.70\n\n-6.62\n\n-6.79\n\n-7.91\n\n-7.26\n\n21.1% 11.1% 0.39\n\n14.2%\n\n6.7%\n\n0.43\n\n37.9% 31.0% 0.51\n\n48.4% 51.0% 0.56\n\n58.1% 59.1% 0.48\n\n-\n\n-\n\n0.48\n\n0.39\n\n0.45\n\n0.50\n\n0.57\n\n0.48\n\n0.47\n\n0.59\n\n0.49\n\n0.63\n\n0.74\n\n0.58\n\n0.73\n\n0.57\n\n0.48\n\n0.63\n\n0.75\n\n0.58\n\n0.74\n\n0.66\n\n0.79\n\n0.70\n\n0.69\n\n0.72\n\n-\n\n0.67\n\n0.78\n\n0.70\n\n0.71\n\n0.71\n\n-\n\nTable 3: Summary of different properties of reference molecules and molecules generated by our model and other baselines. For liGAN and GraphBP, AutoDock Vina could not parse some generated atom types and thus we use QVina (Alhossary et al., 2015) to perform docking. See additional evaluation results in Appendix G.\n\n8\n\na)b)4YHJ_AReferenceARTargetDiff4QLK_A4YHJ_ATargetDiff (avg.=1.45Å)AR (avg.=1.79Å)Median Shift in Center of Mass (Å)4QLK_AVina Energy = -7.3Vina Energy = -6.8Vina Energy = -7.9Vina Energy = -6.8Vina Energy = -6.7Vina Energy = -7.5Published as a conference paper at ICLR 2023\n\nMetric\n\nModel\n\nRMSE ↓ Pearson ↑ Spearman ↑ MAE ↓\n\nTransCPI\n\nMONN\n\nIGN\n\n1.741\n\n1.438\n\n1.433\n\nHOLOPROT\n\n1.546\n\nSTAMP-DPI\n\n1.658\n\nEGNN\n\n1.445\n\n0.576\n\n0.624\n\n0.698\n\n0.602\n\n0.545\n\n0.648\n\n0.540\n\n0.589\n\n0.641\n\n0.571\n\n0.411\n\n0.598\n\n1.404\n\n1.143\n\n1.169\n\n1.208\n\n1.325\n\n1.141\n\nEGNN + ours\n\n1.374\n\n0.680\n\n0.637\n\n1.118\n\nFigure 6: Binding affinity ranking results on CrossDocked2020. Spearman’s rank correlation coefficients between different indicators and experimentally measured binding affinity are shown.\n\nTable 4: Binding affinity prediction results on PDBbind v2020. EGNN augmented with our unsupervised features achieves best results on all four metrics.\n\nTanimoto distances (Bajusz et al., 2015; Tanimoto, 1958). As shown in Table 3, TargetDiff can generate more high-affinity binders compared to liGAN, AR, and GraphBP while maintaining similar other 2D metrics. The metrics TargetDiff does fall behind Pocket2Mol are the QED and SA scores. However, we put less emphasis on them because in the context of drug discovery, QED and SA are used as rough filter and would be fine as long as they are in a reasonable range. Therefore, they might not be the metrics we want to optimize against. We believe future investigation around prediction on bonds and fragment-based (instead of atom-based) generation could lead to improvement.\n\n4.3 BINDING AFFINITY RANKING AND PREDICTION\n\nTo justify that our model can serve as an unsupervised learner to improve the binding affinity ranking and prediction, we first check Spearman’s rank correlation on CrossDocked2020. AutoDock Vina score (i.e. vina) and negative log-transformed experimentally measured binding affinity pK are provided along with the dataset. As shown in Figure 6, we found: (1) The entropy of denoised atom type ˆv0 (i.e. v ent) has a reasonable correlation with pK, indicating unsupervised learning can provide a certain degree of information for binding affinity ranking. (2) The entropy score provides some complementary information to traditional chemical / physical-based score function like Vina, since the combination of them (i.e. combined) achieves better correlation. (3) When provided with labeled data, the final hidden embedding hL (i.e. hidden emb) with a simple linear transformation could improve the correlation to a large extent.\n\nWe further demonstrate that our unsupervised learned features could improve supervised affinity prediction on PDBBind v2020 dataset (Liu et al., 2015). We perform a more difficult time split as St ̈ark et al. (2022) in which the test set consists of structures deposited after 2019, and the training and validation set consist of earlier structures. We augment EGNN (Satorras et al., 2021b) with the unsupervised features hL provided by our model, and compare it with two state-of-the-art sequencebased models TransCPI (Chen et al., 2020) and MONN (Li et al., 2020), one complex model IGN (Jiang et al., 2021), two structure-based model HOLOPROT (Somnath et al., 2021) and STAMPDPI (Wang et al., 2022) and finally the base EGNN model. As shown in Table 4, the augmented EGNN clearly improves the vanilla EGNN and achieves the best results among baselines.\n\n5 CONCLUSION\n\nThis paper proposes TargetDiff, a 3D equivariant diffusion model for target-aware molecule generation and enhancing binding affinity prediction. In terms of future work, it would be interesting to incorporate bond generation as part of the diffusion process such that we can skip the bond inference algorithm. In addition to bond inference, another interesting future direction would be incorporating some of the techniques in fragment-based molecule generation Podda et al. (2020) and generating molecules with common and more synthesizable molecular sub-structures.\n\n9\n\nvina02468101214pkr=0.42v_entr=0.35combined02468101214pkr=0.46hidden_embr=0.65Published as a conference paper at ICLR 2023\n\nReproducibility Statements The model implementation, experimental data and model checkpoints can be found here: https://github.com/guanjq/targetdiff\n\nAcknowledgement We thank all the reviewers for their feedbacks through out the review cycles of the manuscript. This work was supported by National Key R&D Program of China No. 2021YFF1201600, U.S. National Science Foundation under grant no. 2019897 and U.S. Department of Energy award DE-SC0018420.\n\nREFERENCES\n\nAmr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate,\n\nand reliable molecular docking with quickvina 2. Bioinformatics, 31(13):2214–2216, 2015.\n\nAmy C Anderson. The process of structure-based drug design. Chemistry & biology, 10(9):787–\n\n797, 2003.\n\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981–17993, 2021.\n\nD ́avid Bajusz, Anita R ́acz, and K ́aroly H ́eberger. Why is tanimoto index an appropriate choice for\n\nfingerprint-based similarity calculations? Journal of cheminformatics, 7(1):1–13, 2015.\n\nMaria Batool, Bilal Ahmad, and Sangdun Choi. A structure-based drug discovery paradigm. Inter-\n\nnational journal of molecular sciences, 20(11):2783, 2019.\n\nG Richard Bickerton, Gaia V Paolini, J ́er ́emy Besnard, Sorel Muresan, and Andrew L Hopkins.\n\nQuantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012.\n\nEsben Jannik Bjerrum and Richard Threlfall. Molecular generation with recurrent neural networks\n\n(rnns). arXiv preprint arXiv:1705.04612, 2017.\n\nLifan Chen, Xiaoqin Tan, Dingyan Wang, Feisheng Zhong, Xiaohong Liu, Tianbiao Yang, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, and Mingyue Zheng. Transformercpi: improving compound– protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. Bioinformatics, 36(16):4406–4414, 2020.\n\nJerome Eberhardt, Diogo Santos-Martins, Andreas F Tillack, and Stefano Forli. Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings. Journal of Chemical Information and Modeling, 61(8):3891–3898, 2021.\n\nPeter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, 1(1):1–11, 2009.\n\nPaul G Francoeur, Tomohide Masuda, Jocelyn Sunseri, Andrew Jia, Richard B Iovanisci, Ian Snyder, and David R Koes. Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design. Journal of Chemical Information and Modeling, 60(9):4200– 4215, 2020.\n\nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d rototranslation equivariant attention networks. Advances in Neural Information Processing Systems, 33:1970–1981, 2020.\n\nNiklas Gebauer, Michael Gastegger, and Kristof Sch ̈utt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. Advances in Neural Information Processing Systems, 32, 2019.\n\nRafael G ́omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos ́e Miguel Hern ́andez-Lobato, Benjam ́ın S ́anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al ́an Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJiaqi Guan, Wesley Wei Qian, Qiang Liu, Wei-Ying Ma, Jianzhu Ma, and Jian Peng. Energy-inspired molecular conformation optimization. In International Conference on Learning Representations, 2022.\n\nThomas A Halgren. Merck molecular force field. i. basis, form, scope, parameterization, and per-\n\nformance of mmff94. Journal of computational chemistry, 17(5-6):490–519, 1996.\n\nPaul CD Hawkins. Conformation generation: the state of the art. Journal of Chemical Information\n\nand Modeling, 57(8):1747–1756, 2017.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nNeural Information Processing Systems, 33:6840–6851, 2020.\n\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr ́e, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34, 2021.\n\nEmiel Hoogeboom, Victor Garcia Satorras, Cl ́ement Vignac, and Max Welling. Equivariant diffu-\n\nsion for molecule generation in 3d. arXiv preprint arXiv:2203.17003, 2022.\n\nDejun Jiang, Chang-Yu Hsieh, Zhenxing Wu, Yu Kang, Jike Wang, Ercheng Wang, Ben Liao, Chao Shen, Lei Xu, Jian Wu, et al. Interactiongraphnet: A novel and efficient deep graph representation learning framework for accurate protein–ligand interaction predictions. Journal of medicinal chemistry, 64(24):18209–18232, 2021.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola.\n\nJunction tree variational autoencoder for In International Conference on Machine Learning, pages 2323–\n\nmolecular graph generation. 2332. PMLR, 2018.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Composing molecules with multiple property\n\nconstraints. arXiv preprint arXiv:2002.03244, 2020.\n\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino RomeraParedes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583–589, July 2021. doi: 10.1038/s41586-021-03819-2. URL https://doi.org/10.1038/s41586-021-03819-2.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nJonas K ̈ohler, Leon Klein, and Frank No ́e. Equivariant flows: exact likelihood generative learning for symmetric densities. In International Conference on Machine Learning, pages 5361–5370. PMLR, 2020.\n\nMatt J Kusner, Brooks Paige, and Jos ́e Miguel Hern ́andez-Lobato. Grammar variational autoen-\n\ncoder. In International Conference on Machine Learning, pages 1945–1954. PMLR, 2017.\n\nShuya Li, Fangping Wan, Hantao Shu, Tao Jiang, Dan Zhao, and Jianyang Zeng. Monn: a multiobjective neural network for predicting compound-protein interactions and affinities. Cell Systems, 10(4):308–322, 2020.\n\nYibo Li, Jianfeng Pei, and Luhua Lai. Structure-based de novo drug design using 3d deep generative\n\nmodels. Chemical science, 12(41):13664–13675, 2021.\n\nYujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative\n\nmodels of graphs. arXiv preprint arXiv:1803.03324, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information\n\ntheory, 37(1):145–151, 1991.\n\nMeng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, and Shuiwang Ji. Generating 3d molecules\n\nfor target protein binding. In International Conference on Machine Learning, 2022.\n\nQi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph variational autoencoders for molecule design. Advances in neural information processing systems, 31, 2018.\n\nZhihai Liu, Yan Li, Li Han, Jie Li, Jie Liu, Zhixiong Zhao, Wei Nie, Yuchen Liu, and Renxiao Wang. Pdb-wide collection of binding data: current status of the pdbbind database. Bioinformatics, 31 (3):405–412, 2015.\n\nShitong Luo, Jiaqi Guan, Jianzhu Ma, and Jian Peng. A 3d generative model for structure-based\n\ndrug design. Advances in Neural Information Processing Systems, 34, 2021.\n\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\n\nIn International Conference on Machine Learning, pages 8162–8171. PMLR, 2021.\n\nNoel M O’Boyle, Michael Banck, Craig A James, Chris Morley, Tim Vandermeersch, and Geoffrey R Hutchison. Open babel: An open chemical toolbox. Journal of cheminformatics, 3(1): 1–14, 2011.\n\nXingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient\n\nmolecular sampling based on 3d protein pockets. arXiv preprint arXiv:2205.07249, 2022.\n\nMarco Podda, Davide Bacciu, and Alessio Micheli. A deep generative model for fragment-based molecule generation. In International Conference on Artificial Intelligence and Statistics, pages 2240–2250. PMLR, 2020.\n\nMatthew Ragoza, Tomohide Masuda, and David Ryan Koes. Learning a continuous representation of 3d molecular structures with deep generative models. arXiv preprint arXiv:2010.08687, 2020.\n\nMatthew Ragoza, Tomohide Masuda, and David Ryan Koes. Generating 3D molecules conditional on receptor binding sites with deep generative models. Chem Sci, 13:2701–2713, Feb 2022. doi: 10.1039/D1SC05976A.\n\nVictor Garcia Satorras, Emiel Hoogeboom, Fabian B Fuchs, Ingmar Posner, and Max Welling. E\n\n(n) equivariant normalizing flows. arXiv preprint arXiv:2105.09016, 2021a.\n\nVı ́ector Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In International Conference on Machine Learning, pages 9323–9332. PMLR, 2021b.\n\nMarwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS central science, 4(1): 120–131, 2018.\n\nChence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382, 2020.\n\nGregor Simm, Robert Pinsler, and Jos ́e Miguel Hern ́andez-Lobato. Reinforcement learning for molecular design guided by quantum mechanics. In International Conference on Machine Learning, pages 8959–8969. PMLR, 2020a.\n\nGregor NC Simm, Robert Pinsler, G ́abor Cs ́anyi, and Jos ́e Miguel Hern ́andez-Lobato. Symmetryaware actor-critic for 3d molecular design. In International Conference on Learning Representations, 2020b.\n\nMiha Skalic, Jos ́e Jim ́enez, Davide Sabbadin, and Gianni De Fabritiis. Shape-based generative modeling for de novo drug design. Journal of chemical information and modeling, 59(3):1205– 1214, 2019a.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nMiha Skalic, Davide Sabbadin, Boris Sattarov, Simone Sciabola, and Gianni De Fabritiis. From target to drug: generative modeling for the multimodal structure-based ligand design. Molecular pharmaceutics, 16(10):4282–4291, 2019b.\n\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015.\n\nVignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-scale representation learning\n\non proteins. Advances in Neural Information Processing Systems, 34:25244–25255, 2021.\n\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\n\nAdvances in Neural Information Processing Systems, 32, 2019.\n\nHannes St ̈ark, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay, and Tommi Jaakkola. Equibind: Geometric deep learning for drug binding structure prediction. In International Conference on Machine Learning, pages 20503–20521. PMLR, 2022.\n\nCheng Tan, Zhangyang Gao, and Stan Z Li. Target-aware molecular graph generation. arXiv preprint\n\narXiv:2202.04829, 2022.\n\nTaffee T Tanimoto. Elementary mathematical theory of classification and prediction. 1958.\n\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint arXiv:1802.08219, 2018.\n\nOleg Trott and Arthur J Olson. Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455–461, 2010.\n\nPenglei Wang, Shuangjia Zheng, Yize Jiang, Chengtao Li, Junhong Liu, Chang Wen, Atanas Patronov, Dahong Qian, Hongming Chen, and Yuedong Yang. Structure-aware multimodal deep learning for drug–protein interaction prediction. Journal of chemical information and modeling, 62(5):1308–1317, 2022.\n\nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36, 1988.\n\nMingyuan Xu, Ting Ran, and Hongming Chen. De novo molecule design through the molecular generative model conditioned by 3d information of protein binding sites. Journal of Chemical Information and Modeling, 61(7):3240–3254, 2021.\n\nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. arXiv preprint arXiv:2203.02923, 2022.\n\nJiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018.\n\nZhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of\n\nmolecules via deep reinforcement learning. Scientific reports, 9(1):1–10, 2019.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nA PROOF OF SE(3)-EQUIVARIANCE OF GENERATIVE MARKOV TRANSITION\n\nOne crucial property the model needs to satisfy is that rotating or translating the protein-ligand complex (M, P) will not change the estimated likelihood pθ(M|P). Leveraging the conclusion from K ̈ohler et al. (2020); Xu et al. (2022), it requires the initial density of our generative process p(MT |P) is SE(3)-invariant and the Markov transition pθ(Mt−1|Mt, P) is SE(3)-equivariant. Since atom types are always invariant to SE(3)-transformation during the generative process, we only need to consider the atom coordinates. More concretely, the model needs to satisfy:\n\np(xT , xP ) = p(Tg(xT , xP ))\n\np(xt−1|xt, xP ) = p(Tg(xt−1)|Tg(xt, xP ))\n\nwhere Tg is the group of SE(3)-transformation, xt and xP denote the atom coordinates of ligand molecule and protein separately. Tg(x) can also be written explicitly as Tg(x) = Rx + b, where R ∈ R3×3 is the rotation matrix and b ∈ R3 is the translation vector.\n\nIn Sec. 3.4, we provide a way to implement SE(3)-equivariance of the generative Markov transition. In this section, we will prove the SE(3)-equivariance of our design.\n\nRecall the equation 7, we update the atom hidden embedding h and coordinates x alternately as follows:\n\nhl+1\n\ni = hl\n\ni +\n\n(cid:88)\n\nfh(dl\n\nij, hl\n\ni, hl\n\nj, eij; θh)\n\nxl+1\n\ni = xl\n\ni +\n\nj∈V,i̸=j (cid:88)\n\nj∈V,i̸=j\n\n(xl\n\ni − xl\n\nj)fx(dl\n\nij, hl+1\n\ni\n\n, hl+1\n\nj\n\n, eij; θx) · 1mol\n\nFirst, it is easy to see dij does not change with the 3D roto-translation Tg:\n\nˆd2 ij = ∥Tg(xi) − Tg(xj)∥2 = ∥(Rxi + b) − (Rxj + b)∥2 = ∥Rxi − Rxj∥2\n\n= (xi − xj)⊤R⊤R(xi − xj) = (xi − xj)⊤I(xi − xj) = ∥xi − xj∥2 = d2\n\nij\n\nSince hi, hj, eij are initially obtained from atom and edge features, which are invariant to SE(3)- transformation, we have hl\n\ni is SE(3)-invariant for any l = 1, . . . , L.\n\nThen, we can prove that x updated from equation 7 is SE(3)-equivariant as follows:\n\nφθ(T (xl)) = T (xl\n\ni) +\n\n(cid:88)\n\n(T (xl\n\ni) − T (xl\n\nj))fx(dl\n\nij, hl+1\n\ni\n\n, hl+1\n\nj\n\n, eij; θx) · 1mol\n\nj∈V,i̸=j (cid:88)\n\n= Rxl\n\ni + b +\n\nR(xl\n\ni − xl\n\nj)fx(dl\n\nij, hl+1\n\ni\n\n, hl+1\n\nj\n\n, eij; θx) · 1mol\n\nj∈V,i̸=j\n\nR(xl\n\ni − xl\n\nj)fx(dl\n\nij, hl+1\n\ni\n\n\n\n(11)\n\n, hl+1\n\nj\n\n, eij; θx) · 1mol\n\n + b\n\n xl\n\ni +\n\n= R\n\n(cid:88)\n\nj∈V,i̸=j\n\n= Rxl+1 i + b = T (φθ(xl))\n\nUnder our parameterization, the neural network predicts [ˆx0, ˆv0]. By stacking L such equivariant layers together, we can draw the conclusion that the output of neural network ˆx0 is SE(3)- equivariant w.r.t the input xt. Finally, we can obtain the mean of posterior ˆxt−1 from equation 4: xt. The last thing the model needs to satisfy is that ˆxt−1 is ˆxt−1 = SE(3)-equivariant w.r.t xt. However, we can see the translation vector will be changed under this formula:\n\n ̄αt−1βt 1− ̄αt\n\nαt(1− ̄αt−1)\n\nˆx0 +\n\n1− ̄αt\n\n√\n\n√\n\nμθ(T (xt), t) =\n\n=\n\n√\n\n√\n\n ̄αt−1βt 1 − ̄αt ̄αt−1βt 1 − ̄αt\n\nT (ˆx0) +\n\nR(ˆx0) +\n\n14\n\n√\n\nαt(1 − ̄αt−1)\n\n1 − ̄αt\n\n√\n\nT (xt)\n\nαt(1 − ̄αt−1)\n\n1 − ̄αt\n\nR(xt) + ̃b\n\n(12)\n\nPublished as a conference paper at ICLR 2023\n\nwhere ̃b =\n\n(cid:16) √\n\n ̄αt−1βt 1− ̄αt\n\n+\n\n√\n\nαt(1− ̄αt−1)\n\n1− ̄αt\n\n(cid:17)\n\nb\n\nAs the Sec. 3.4 discussed, we can move CoM of the protein atoms to zero once to achieve translation invariance in the whole generative process, which is same to how we achieve the invariant initial density. Thus, we only need to consider rotation equivariance in the Markov transition, which is straightforward to see that it can be achieved from equation 12 when b is ignored: μθ(R(xt), t) = R(μθ(xt, t)).\n\nB ANALYSIS OF INVARIANT INITIAL DENSITY\n\nWe assume when the timestep T of the diffusion process is sufficiently large, q(xT |xP ) would be a Gaussian distribution whose mean is the center of protein and standard deviation is one, i.e. (cid:80) xP , ⊗ denotes the kronecker product, Ik q(xT |xP ) ∼ N (CP ⊗ 1NP , I3·NP ), where CP = 1 denotes the k × k identity matrix and 1k denotes the k-dimensional vector filled with one.\n\nNP\n\nTo achieve the SE(3)-invariant initial density, we move the center of protein to zero, i.e. CP = 0. One can also define the initial density on other CoM-free systems such as the ligand or complex CoM-free system. We choose protein CoM-free system here since only one step of shifting center operation is needed at the beginning of generative or diffusion process (protein context is a fixed input). Formally, it can be considered as a linear transformation: ˆxP = QxP , where Q = I3 ⊗(IN − 1\nN 1N 1T N ). It has several benefits in simplifying the formula: In the diffusion process, q(ˆxT |ˆxP ) would be a standard Gaussian when T is sufficiently large; Accordingly, in the generative process, we can sample ˆxT from p(ˆxT |ˆxP ), which is also a standard Gaussian distribution.\n\nFor evaluating a complex position (xT , xP ), we can firstly translate the complex to achieve zero CoM on protein positions, which can also be considered as a linear transformation:\n\n(ˆxT , ˆxP ) = Q(xT , xP ), where Q = I3 ⊗\n\n(cid:18)IM 0\n\nN 1M 1T\n\nN\n\n− 1 IN − 1\n\nN 1N 1T\n\nN\n\n(cid:19)\n\n(13)\n\nThen, we can evaluate the density p(ˆxT |ˆxP ) with the standard normal distribution. We denote ˆp as the density function on this protein zero CoM subspace: ˆp(xT |xP ) = p(Q(xT , xP ))\n\nIt can be seen that for any rigid transformation Tg(x) = Rx + b, we have Q · Tg(xT , xP ) = Q · R(xT , xP ). Since Q is a symmetric projection operator and rotation matrix R is a orthogonal matrix, we have ∥Q · Rx∥2 = ∥x∥2. Given p is an isotropic normal distribution, we can easily have ˆp(Tg(xT , xP )) = ˆp(xT , xP ), which means an SE(3)-invariant density.\n\nC PROOF OF INVARIANT LIKELIHOOD\n\nIn Sec. 3.4, we argue that an invariant initial density composed with an equivariant transition function will result in an invariant distribution. In this section, we will provide the proof of it.\n\nThe two conditions to guarantee an invariant likelihood pθ(M0|P) are as follows:\n\np(xT , xP ) = p(Tg(xT , xP ))\n\n( 1 Invariant Prior)\n\np(xt−1|xt, xP ) = p(Tg(xt−1)|Tg(xt, xP ))\n\n( 2 Equivariant Transition)\n\nWe can obtain the conclusion as follows:\n\npθ(Tg(x0, xP )) =\n\n=\n\n=\n\n(cid:90)\n\n(cid:90)\n\n(cid:90)\n\np(Tg(xT , xP ))\n\nT (cid:88)\n\nt=1\n\npθ(Tg(xt−1)|Tg(xt, xP )))\n\np(xT , xP )\n\np(xT , xP )\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\npθ(Tg(xt−1)|Tg(xt, xP )))\n\n← Apply 1\n\npθ(xt−1|xt, xP )\n\n← Apply 2\n\n= pθ(x0, xP )\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nD DERIVATION OF ATOM TYPES DIFFUSION PROCESS\n\nAccording to Bayes theorem, we have:\n\nq(vt−1|vt, v0) =\n\n=\n\n(cid:80)\n\nvt−1\n\n(cid:80)\n\nq(vt|vt−1, v0)q(vt−1|v0)\n\nq(vt|vt−1, v0)q(vt−1|v0)\n\nq(vt|vt−1)q(vt−1|v0)\n\nq(vt|vt−1)q(vt−1|v0)\n\nvt−1\n\nAccording to Eq. 2 and 3, q(vt|vt−1) and q(vt−1|v0) can be calculated as:\n\nq(vt|vt−1) = C(vt|αtvt−1 + (1 − αt)/K) q(vt−1|v0) = C(vt−1| ̄αt−1v0 + (1 − ̄αt−1)/K)\n\n(14)\n\n(15)\n\nNote that when computing C(vt|αtvt−1 + (1 − αt)/K), the value is αt + (1 − αt)/K if vt = vt−1 and (1 − αt)/K otherwise, which leads to symmetry of this function Hoogeboom et al. (2021), i.e., C(vt|αtvt−1 + (1 − αt)/K) = C(vt−1|αtvt + (1 − αt)/K).\n\nLet c⋆(vt, v0) denotes the numerator of Eq. 14. Then it can be computed as:\n\nc⋆(vt, v0) = q(vt|vt−1)q(vt−1|v0)\n\n= [αtvt + (1 − αt)/K] ⊙ [ ̄αt−1v0 + (1 − ̄αt−1)/K]\n\nand therefore the posterior of atom types is derived as:\n\nq(vt−1|vt, v0) = C(vt−1| ̃ct(vt, v0))\n\n(16)\n\n(17)\n\nwhere ̃ct(vt, v0) = c⋆/ (cid:80)K\n\nk=1 c⋆\n\nk\n\nE OVERALL TRAINING AND SAMPLING PROCEDURES\n\nIn this section, we summarize the overall training and sampling procedures of TargetDiff as Algorithm 1 and Algorithm 2 respectively.\n\nAlgorithm 1 Training Procedure of TargetDiff\n\nInput: Protein-ligand binding dataset {P, M}N\n\ni=1, neural network φθ\n\n1: while φθ not converge do 2: 3: 4: 5:\n\nSample diffusion time t ∈ U(0, . . . , T ) Move the complex to make CoM of protein atoms zero Perturb x0 to obtain xt: xt = Perturb v0 to obtain vt:\n\n√\n\n ̄αtx0 + (1 − ̄αt)ε, where ε ∈ N (0, I)\n\nlog c = log ( ̄αtv0 + (1 − ̄αt)/K) vt = one hot(arg maxi[gi + log ci]), where g ∼ Gumbel(0, 1) Predict [ˆx0, ˆv0] from [xt, vt] with φθ: [ˆx0, ˆv0] = φθ([xt, vt], t, P) Compute the posterior atom types c(vt, v0) and c(vt, ˆv0) according to equation 4 Compute the unweighted MSE loss on atom coordinates and the KL loss on posterior atom\n\ntypes: L = ∥x0 − ˆx0∥2 + α KL(c(vt, v0) ∥ c(vt, ˆv0))\n\nUpdate θ by minimizing L\n\n6: 7: 8:\n\n9:\n\n10: end while\n\nF EXPERIMENT DETAILS\n\nF.1 FEATURIZATION\n\nAt the l-th layer, we dynamically construct the protein-ligand complex as a k-nearest neighbors (knn) graph based on known protein atom coordinates and current ligand atom coordinates, which\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 Sampling Procedure of TargetDiff\n\nInput: The protein binding site P, the learned model φθ. Output: Generated ligand molecule M that binds to the protein pocket.\n\n1: Sample the number of atoms in M based on a prior distribution conditioned on the pocket size 2: Move CoM of protein atoms to zero 3: Sample initial molecular atom coordinates xT and atom types vT :\n\nxT ∈ N (0, I) vT = one hot(arg maxi gi), where g ∼ Gumbel(0, 1)\n\n4: for t in T, T − 1, . . . , 1 do 5: 6: 7: 8: end for\n\nPredict [ˆx0, ˆv0] from [xt, vt] with φθ: [ˆx0, ˆv0] = φθ([xt, vt], t, P) Sample xt−1 from the posterior pθ(xt−1|xt, ˆx0) according to equation 4 Sample vt−1 from the posterior pθ(vt−1|vt, ˆv0) according to equation 4\n\nis the output of the l − 1-th layer. We choose k = 32 in our experiments. The protein atom features include chemical elements, amino acid types and whether the atoms are backbone atoms. The ligand atom types are one-hot vectors consisting of the chemical element types and aromatic information. The edge features are the outer products of distance embedding and bond types, where we expand the distance with radial basis functions located at 20 centers between 0 ̊A and 10 ̊A and the bond type is a 4-dim one-hot vector indicating the connection is between protein atoms, ligand atoms, protein-ligand atoms or ligand-protein atoms.\n\nF.2 MODEL PARAMETERIZATION\n\nOur model consists of 9 equivariant layers as equation 7 shows, and each layer is a Transformer with hidden dim=128 and n heads=16. The key/value embedding and attention scores are generated through a 2-layer MLP with LayerNorm and ReLU activation. We choose to use a sigmoid β schedule with β1 = 1e-7 and βT = 2e-3 for atom coordinates, and a cosine β schedule suggested in Nichol and Dhariwal (2021) with s = 0.01 for atom types. We set the number of diffusion steps as 1000.\n\nF.3 TRAINING DETAILS\n\nbetas=(0.95, 0.999),\n\nis trained via gradient descent method Adam Kingma and Ba (2014) with The model init learning rate=0.001, and clip gradient norm=8. We multiply a factor α = 100 on the atom type loss to balance the scales of two losses. During the training phase, we add a small Gaussian noise with a standard deviation of 0.1 to protein atom coordinates as data augmentation. We also schedule to decay the learning rate exponentially with a factor of 0.6 and a minimum learning rate of 1e-6. The learning rate is decayed if there is no improvement for the validation loss in 10 consecutive evaluations. The evaluation is performed for every 2000 training steps.\n\nbatch size=4\n\nWe trained our model on one NVIDIA GeForce GTX 3090 GPU, and it could converge within 24 hours and 200k steps.\n\nF.4 DETERMINE THE NUMBER OF LIGAND ATOMS\n\nPocket Size Estimation We compute the top 10 farthest pairwise distances of protein atoms, and select the median of it as the pocket size for robustness.\n\nPrior Distribution of Number of Ligand Atoms We compute 10 quantiles of training pocket sizes and estimate the prior distribution of number of ligand atoms for each bin. Specifically, we take the histogram of number of atoms in the training set as the prior distribution. The relationship between estimated prior distribution and actual training testing number of atom distribution is shown in Figure S1. We can see that the number of ligand atoms has a clear positive correlation with pocket sizes and the prior distribution estimated from the training set can also be generalized to the test set.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure S1: Prior distributions of the number of ligand atoms. For each prior distribution within a specific range of pocket size, train/test | pocket size range | median number of ligand atoms | number of data points within current bin are shown as titles.\n\nDuring the training phase, we provide the model with the number of atoms of reference molecules since we use them to perform training. During the generation phase, we do not have to require the generated molecules has the same number of atoms as the reference molecule, and the numbers are randomly sampled from these prior distributions computed based on training data.\n\nG ADDITIONAL EVALUATION RESULTS\n\nIn the main text, we provided the evaluation results in Table. 3 where the Vina score is computed with AutoDock Vina (Eberhardt et al., 2021). Here, we provided the evaluation results in Table S1 where Vina scores are computed with QVina (Alhossary et al., 2015), a faster but less accurate docking tool (following what AR and Pocket2Mol used). We can observe a similar trend that molecules generated by TargetDiff could achieve SOTA binding affinity.\n\nUpon further investigation, we also discover a strong negative correlation between SA score and molecular size as shown in Figure S2 (Pearson R=−0.56, p ≤ 10−80), and the difference in SA score between these model generated molecules could be the artifact of their size differences.\n\n18\n\n0204060800.000.020.040.060.080.100.12train | 15.0-26.5A | 11 | 10000poissonnormalactual0204060800.000.020.040.060.080.100.120.140.16test | 15.0-26.5A | 11 | 13poissonnormalactual0204060800.000.020.040.060.080.100.12train | 26.5-27.9A | 14 | 10001poissonnormalactual0204060800.000.020.040.060.080.100.120.140.16test | 26.5-27.9A | 14 | 12poissonnormalactual0204060800.000.020.040.060.08train | 27.9-28.8A | 19 | 9998poissonnormalactual0204060800.000.050.100.150.20test | 27.9-28.8A | 19 | 9poissonnormalactual0204060800.000.020.040.060.08train | 28.8-29.7A | 21 | 10005poissonnormalactual0204060800.0000.0250.0500.0750.1000.1250.1500.1750.200test | 28.8-29.7A | 21 | 15poissonnormalactual0204060800.000.020.040.060.080.10train | 29.7-30.4A | 23 | 9995poissonnormalactual0204060800.000.050.100.150.200.25test | 29.7-30.4A | 23 | 16poissonnormalactual0204060800.000.010.020.030.040.050.060.070.08train | 30.4-31.2A | 24 | 10000poissonnormalactual0204060800.000.050.100.150.200.25test | 30.4-31.2A | 24 | 12poissonnormalactual0204060800.000.020.040.060.08train | 31.2-32.0A | 27 | 10000poissonnormalactual0204060800.000.050.100.150.200.25test | 31.2-32.0A | 27 | 11poissonnormalactual0204060800.000.010.020.030.040.050.060.070.08train | 32.0-33.0A | 28 | 10000poissonnormalactual0204060800.000.050.100.150.20test | 32.0-33.0A | 28 | 9poissonnormalactual0204060800.000.020.040.060.08train | 33.0-34.6A | 30 | 10000poissonnormalactual0204060800.0000.0250.0500.0750.1000.1250.1500.175test | 33.0-34.6A | 30 | 11poissonnormalactual0204060800.000.010.020.030.040.050.060.070.08train | 34.6-infA | 33 | 10000poissonnormalactual0204060800.0000.0250.0500.0750.1000.1250.1500.175test | 34.6-infA | 33 | 16poissonnormalactualPublished as a conference paper at ICLR 2023\n\nMetric\n\nModel\n\nHigh Affinity (↑)\n\nQED (↑)\n\nSA (↑)\n\nDiversity (↑)\n\nAvg.\n\nMed.\n\nAvg. Med. Avg. Med. Avg. Med.\n\nliGAN\n\nAR\n\n21.1% 11.1% 0.39\n\n33.7% 24.2% 0.51\n\nGraphBP\n\n14.2%\n\n6.7%\n\n0.43\n\nPocket2Mol\n\n49.8% 52.0% 0.56\n\nTargetDiff\n\n51.3% 50.0% 0.48\n\nReference\n\n-\n\n-\n\n0.48\n\n0.39\n\n0.50\n\n0.45\n\n0.57\n\n0.48\n\n0.47\n\n0.59\n\n0.63\n\n0.49\n\n0.74\n\n0.58\n\n0.73\n\n0.57\n\n0.63\n\n0.48\n\n0.75\n\n0.58\n\n0.74\n\n0.66\n\n0.70\n\n0.79\n\n0.69\n\n0.72\n\n-\n\n0.67\n\n0.70\n\n0.78\n\n0.71\n\n0.71\n\n-\n\nTable S1: Summary of different properties of reference molecules and molecules generated by our model and other baselines.\n\nFigure S2: A scatter plot compares the SA score and the number of atoms for a given molecule.\n\nH SAMPLING TIME ANALYSIS\n\nOne major advantage of TargetDiff over auto-regressive based model such AR is that TargetDiff scales better against the size of the molecules. While AR is required to run additional steps to generate larger molecules, the diffusion model can operate on additional atoms in a parallel fashion without sacrificing a lot of inference time.\n\nTo better demonstrate such effect, we randomly select 5 binding pockets as targets and record the time spent in generating 100 molecules for each pocket using both autoregessive models (including AR, Pocket2Mol and GraphBP) and TargetDiff. Since these models have different numbers of parameters and sampling schemes, we first compare the inference time ratio against generating a 10atom molecule for these models, instead of simply comparing their wall time. As shown in Figure S3, as we start to generate larger and larger molecules, the wall time for AR grows almost linearly along with the molecule size, while the wall time for TargetDiff stays relatively flat.\n\nIn terms of wall clock time, AR, Pocket2Mol and GraphBP use 7785s, 2544s and 105s for generating 100 valid molecules on average separately, and it takes 3428s on average for TargetDiff. GraphBP has the fastest sampling time but the quality of generated molecules is lower than other models (See Table 3). TargetDiff has the moderate sampling efficiency compared to AR and Pocket2Mol.\n\nFigure S3: Inference time growth as a function of molecule size for AR, Pocket2Mol, GraphBP and TargetDiff.\n\nI MORE EXAMPLES OF GENERATED RESULTS\n\nPlease see next page.\n\n19\n\n(cid:54)(cid:36)(cid:6)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:87)(cid:82)(cid:80)(cid:86)010203040Molecule Size202468101214Time RatioMedian Size forReference MoleculesARPocket2MolGraphBPTargetDiffPublished as a conference paper at ICLR 2023\n\nFigure S4: More examples of binding poses for generated molecules. To present of fair overview of the model performance, we specifically select the best (1H36 A), median (1DXO A), and worst (2GNS A) targets shown in Figure 4 for visualization along with the generated molecules and calculated Vina energy (kcal/mol).\n\n20\n\n1H36_A (Target 0 in Figure 4)Reference AR TargetDiffvina = -12.8 vina = -10.6 vina = -13.31DXO_A (Target 49 in Figure 4)Reference AR TargetDiffvina = -5.2 vina = -3.9 vina = -7.02GNS_A (Target 99 in Figure 4)Reference AR TargetDiffvina = -4.8 vina = -3.9 vina = -3.3Published as a conference paper at ICLR 2023\n\nJ EXAMPLES WHERE AR OUTPERFORMS TARGETDIFF\n\nPlease see followings for two binding targets where AR outperforms TargetDiff in terms of Vina estimated binding affinity.\n\nFigure S5: Example binding pocket 1 (4KCQ A, target 33 in Figure 4) where AR outperforms TargetDiff in terms of Vina estimated binding affinity. Three examples of AR generated molecules are shown in the top half, and three examples from TargetDiff are shown in the bottom half.\n\n21\n\n4KCQ_A (Target 33 in Figure 4)vina = -10.5vina = -9.5vina = -8.9ARvina = -8.2vina = -7.9vina = -6.3OursPublished as a conference paper at ICLR 2023\n\nFigure S6: Example binding pocket 1 (1R1H A, target 61 in Figure 4) where AR outperforms TargetDiff in terms of Vina estimated binding affinity. Three examples of AR generated molecules are shown in the top half, and three examples from TargetDiff are shown in the bottom half.\n\n22\n\n1R1H_A (Target 61 in Figure 4)vina = -9.0vina = -9.8vina = -9.1ARvina = -6.3vina = -6.8vina = -7.7Ours",
    "reference": "# Summary Of The Paper\n\nIn this paper, TargetDiff, a new 3D diffusion model, is introduced. The model generates molecules in a non-autoregressive fashion. The generated molecules are conditioned on the binding pocket, and the generation is equivariant to rotations and translations thanks to the equivariant GNNs used. Additionally, an unsupervised entropy-based method for ranking molecules is proposed. The model is evaluated against multiple recent generative models: liGAN, AR, GraphBP, and Pocket2Mol. The results indicate the strong performance of the proposed method.\n\n# Strength And Weaknesses\n\nStrengths:\n- Non-autoregressive sampling helps to better fill the available space in the binding pocket.\n- The related work contains a set of very recent publications demonstrating the advancements in the fields of target-based molecule generation and diffusion models.\n- SE(3)-equivariant networks ensure the strong performance independent of the protein orientation.\n- Both continuous and discrete diffusion models are used to model coordinates and atom types, respectively.\n- The main claims of the paper are formally proven.\n- The authors discover a correlation between binding affinity and the entropy of the predicted atom representation when atom coordinates are frozen. Thus, this entropy can be treated as an affinity predictor trained in an unsupervised fashion.\n\nWeaknesses:\n- Currently, the code is not available, but it should be open-sourced upon publication.\n- In Tables 3 and 4, confidence intervals could be added to make these results more convincing.\n\nQuestions:\n- How do you combine v entropy and vina scores in Section 4.3?\n- Coordinates and atom types are modeled independently in Equation 2. Do you think this could have a negative impact on the generated structures?\n\nMinor points:\n- Typos: “more structural data become available and unlock new opportunities”, “protien”->”protein”\n- The full name “Center of Mass (CoM)” should be introduced at its first occurrence.\n- Figure 7 reference is missing.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity.** The paper is written in a clear way. The figures are clean and well support the model description and nicely illustrate the experimental findings. The training details, including pseudocode, are provided.\n\n**Quality.** The model is described in full detail, and the experimental results are supported by many figures showing different aspects of the model. There are four recent models compared against TargetDiff, and multiple metrics are used. The main claims of the paper are proven in the appendix.\n\n**Novelty.** Diffusion models are a new class of generative models, and this is one of the first examples of using them for target-based molecule generation. The concept of affinity predictor trained in an unsupervised fashion also seems novel.\n\n**Reproducibility.** The code is not available, but the authors say they will publish the code when the paper is accepted. Based on the description, the reimplementation would be probably possible but tedious. Pseudocode is provided for the training and sampling procedure.\n\n# Summary Of The Review\n\nBased on the above comments, I am leaning towards the acceptance of this paper.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTRANSFORMERS ARE SAMPLE-EFFICIENT WORLD MODELS\n\nVincent Micheli∗ University of Geneva\n\nEloi Alonso∗ University of Geneva\n\nFrançois Fleuret University of Geneva\n\nABSTRACT\n\nDeep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.\n\n1\n\nINTRODUCTION\n\nDeep Reinforcement Learning (RL) has become the dominant paradigm for developing competent agents in challenging environments. Most notably, deep RL algorithms have achieved impressive performance in a multitude of arcade (Mnih et al., 2015; Schrittwieser et al., 2020; Hafner et al., 2021), real-time strategy (Vinyals et al., 2019; Berner et al., 2019), board (Silver et al., 2016; 2018; Schrittwieser et al., 2020) and imperfect information (Schmid et al., 2021; Brown et al., 2020a) games. However, a common drawback of these methods is their extremely low sample efficiency. Indeed, experience requirements range from months of gameplay for DreamerV2 (Hafner et al., 2021) in Atari 2600 games (Bellemare et al., 2013b) to thousands of years for OpenAI Five in Dota2 (Berner et al., 2019). While some environments can be sped up for training agents, real-world applications often cannot. Besides, additional cost or safety considerations related to the number of environmental interactions may arise (Yampolskiy, 2018). Hence, sample efficiency is a necessary condition to bridge the gap between research and the deployment of deep RL agents in the wild.\n\nModel-based methods (Sutton & Barto, 2018) constitute a promising direction towards data efficiency. Recently, world models were leveraged in several ways: pure representation learning (Schwarzer et al., 2021), lookahead search (Schrittwieser et al., 2020; Ye et al., 2021), and learning in imagination (Ha & Schmidhuber, 2018; Kaiser et al., 2020; Hafner et al., 2020; 2021). The latter approach is particularly appealing because training an agent inside a world model frees it from sample efficiency constraints. Nevertheless, this framework relies heavily on accurate world models since the policy is purely trained in imagination. In a pioneering work, Ha & Schmidhuber (2018) successfully built imagination-based agents in toy environments. SimPLe recently showed promise in the more challenging Atari 100k benchmark (Kaiser et al., 2020). Currently, the best Atari agent learning in imagination is DreamerV2 (Hafner et al., 2021), although it was developed and evaluated with two hundred million frames available, far from the sample-efficient regime. Therefore, designing new world model architectures, capable of handling visually complex and partially observable environments with few samples, is key to realize their potential as surrogate training grounds.\n\nThe Transformer architecture (Vaswani et al., 2017) is now ubiquitous in Natural Language Processing (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020b; Raffel et al., 2020), and is also gaining traction in Computer Vision (Dosovitskiy et al., 2021; He et al., 2022), as well as in Offline\n\n∗Equal contributions, order determined by a coin flip. Correspondence: {first.last}@unige.ch\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nx0\n\nE\n\nˆr0\n\nˆd0\n\nˆr1\n\nˆd1\n\nˆrs−1\n\nˆds−1\n\n0 , . . . , zK z1\n\n0\n\na0\n\nG\n\nˆz1 1 , . . . , ˆzK\n\n1\n\na1\n\nG\n\n. . .\n\nG\n\ns , . . . , ˆzK ˆz1\n\ns\n\nas\n\nD\n\nˆx0\n\nπ\n\nt = 0\n\nD\n\nˆx1\n\nπ\n\nt = 1\n\n. . .\n\nD\n\nˆxs\n\nπ\n\nt = s\n\nFigure 1: Unrolling imagination over time. This figure shows the policy π, depicted with purple arrows, taking a sequence of actions in imagination. The green arrows correspond to the encoder E and the decoder D of a discrete autoencoder, whose task is to represent frames in its learnt symbolic language. The backbone G of the world model is a GPT-like Transformer, illustrated with blue arrows. For each action that the policy π takes, G simulates the environment dynamics, by autoregressively unfolding new frame tokens that D can decode. G also predicts a reward and a potential episode termination. More specifically, an initial frame x0 is encoded with E into tokens z0 = (z1 0 ) = E(x0). The decoder D reconstructs an image ˆx0 = D(z0), from which the policy π predicts the action a0. From z0 and a0, G predicts the reward ˆr0, episode termination ˆd0 ∈ {0, 1}, and in an autoregressive manner ˆz1 = (ˆz1 1 ), the tokens for the next frame. A dashed box indicates image tokens for a given time step, whereas a solid box represents the input sequence of G, i.e. (z0, a0) at t = 0, (z0, a0, ˆz1, a1) at t = 1, etc. The policy π is purely trained with imagined trajectories, and is only deployed in the real environment to improve the world model (E, D, G).\n\n1, . . . , ˆzK\n\n0, . . . , zK\n\nReinforcement Learning (Janner et al., 2021; Chen et al., 2021). In particular, the GPT (Radford et al., 2018; 2019; Brown et al., 2020b) family of models delivered impressive results in language understanding tasks. Similarly to world models, these attention-based models are trained with highdimensional signals and a self-supervised learning objective, thus constituting ideal candidates to simulate an environment.\n\nTransformers particularly shine when they operate over sequences of discrete tokens (Devlin et al., 2019; Brown et al., 2020b). For textual data, there are simple ways (Schuster & Nakajima, 2012; Kudo & Richardson, 2018) to build a vocabulary, but this conversion is not straightforward with images. A naive approach would consist in treating pixels as image tokens, but standard Transformer architectures scale quadratically with sequence length, making this idea computationally intractable. To address this issue, VQGAN (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) employ a discrete autoencoder (Van Den Oord et al., 2017) as a mapping from raw pixels to a much smaller amount of image tokens. Combined with an autoregressive Transformer, these methods demonstrate strong unconditional and conditional image generation capabilities. Such results suggest a new approach to design world models.\n\nIn the present work, we introduce IRIS (Imagination with auto-Regression over an Inner Speech), an agent trained in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer. IRIS learns behaviors by accurately simulating millions of trajectories. Our approach casts dynamics learning as a sequence modeling problem, where an autoencoder builds a language of image tokens and a Transformer composes that language over time. With minimal tuning, IRIS outperforms a line of recent methods (Kaiser et al., 2020; Hessel et al., 2018; Laskin et al., 2020; Yarats et al., 2021; Schwarzer et al., 2021) for sample-efficient RL in the Atari 100k benchmark (Kaiser et al., 2020). After only two hours of real-time experience, it achieves a mean human normalized score of 1.046, and reaches superhuman performance on 10 out of 26 games. We describe IRIS in Section 2 and present our results in Section 3.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Four imagined trajectories in KungFuMaster. We use the same conditioning frame across the four rows, in green, and let the world model imagine the rest. As the initial frame only contains the player, there is no information about the enemies that will come next. Consequently, the world model generates different types and numbers of opponents in each simulation. It is also able to reflect an essential game mechanic, highlighted in the blue box, where the first enemy disappears after getting hit by the player.\n\n2 METHOD\n\nWe formulate the problem as a Partially Observable Markov Decision Process (POMDP) with image observations xt ∈ Rh×w×3, discrete actions at ∈ {1, . . . , A}, scalar rewards rt ∈ R, episode termination dt ∈ {0, 1}, discount factor γ ∈ (0, 1), initial observation distribution ρ0, and environment dynamics xt+1, rt, dt ∼ p(xt+1, rt, dt | x≤t, a≤t). The reinforcement learning objective is to train a policy π that yields actions maximizing the expected sum of rewards Eπ[(cid:80)\n\nt≥0 γtrt].\n\nOur method relies on the three standard components to learn in imagination (Sutton & Barto, 2018): experience collection, world model learning, and behavior learning. In the vein of Ha & Schmidhuber (2018); Kaiser et al. (2020); Hafner et al. (2020; 2021), our agent learns to act exclusively within its world model, and we only make use of real experience to learn the environment dynamics.\n\nWe repeatedly perform the three following steps:\n\n• collect_experience: gather experience in the real environment with the current policy.\n\n• update_world_model: improve rewards, episode ends and next observations predictions.\n\n• update_behavior: in imagination, improve the policy and value functions.\n\nThe world model is composed of a discrete autoencoder (Van Den Oord et al., 2017), to convert an image to tokens and back, and a GPT-like autoregressive Transformer (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020b), whose task is to capture environment dynamics. Figure 1 illustrates the interplay between the policy and these two components during imagination. We first describe the autoencoder and the Transformer in Sections 2.1 and 2.2, respectively. Section 2.3 then details the procedure to learn the policy and value functions in imagination. Appendix A provides a comprehensive description of model architectures and hyperparameters. Algorithm 1 summarizes the training protocol.\n\n2.1 FROM IMAGE OBSERVATIONS TO TOKENS\n\nThe discrete autoencoder (E, D) learns a symbolic language of its own to represent high-dimensional images as a small number of tokens. The back and forth between frames and tokens is illustrated with green arrows in Figure 1.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 3: Pixel perfect predictions in Pong. The top row displays a test trajectory collected in the real environment. The bottom row depicts the reenactment of that trajectory inside the world model. More precisely, we condition the world model with the first two frames of the true sequence, in green. We then sequentially feed it the true actions and let it imagine the subsequent frames. After only 120 games of training, the world model perfectly simulates the ball’s trajectory and players’ movements. Notably, it also captures the game mechanic of updating the scoreboard after winning an exchange, as shown in the blue box.\n\nMore precisely, the encoder E : Rh×w×3 → {1, . . . , N }K converts an input image xt into K i=1 ∈ RN ×d be the corresponding embedding tokens from a vocabulary of size N . Let E = {ei}N table of d-dimensional vectors. The input image xt is first passed through a Convolutional Neural Network (CNN) (LeCun et al., 1989) producing output yt ∈ RK×d. We then obtain the output (cid:13) tokens zt = (z1 (cid:13)2, the index of the closest embedding vector in E (Van Den Oord et al., 2017; Esser et al., 2021). Conversely, the CNN decoder D : {1, . . . , N }K → Rh×w×3 turns K tokens back into an image.\n\nt ) ∈ {1, . . . , N }K as zk\n\nt = argmini\n\nt , . . . , zK\n\nt − ei\n\n(cid:13) (cid:13)yk\n\nThis discrete autoencoder is trained on previously collected frames, with an equally weighted combination of a L1 reconstruction loss, a commitment loss (Van Den Oord et al., 2017; Esser et al., 2021), and a perceptual loss (Esser et al., 2021; Johnson et al., 2016; Larsen et al., 2016). We use a straight-through estimator (Bengio et al., 2013) to enable backpropagation training.\n\n2.2 MODELING DYNAMICS\n\nAt a high level, the Transformer G captures the environment dynamics by modeling the language of the discrete autoencoder over time. Its central role of unfolding imagination is highlighted with the blue arrows in Figure 1.\n\nSpecifically, G operates over sequences of interleaved frame and action tokens. An input sequence (z1 t , at) is obtained from the raw sequence (x0, a0, x1, a1, . . . , xt, at) by encoding the frames with E, as described in Section 2.1.\n\n1 , a1, . . . , z1\n\n0, . . . , zK\n\n1, . . . , zK\n\nt , . . . , zK\n\n0 , a0, z1\n\nAt each time step t, the Transformer models the three following distributions:\n\nTransition:\n\nˆzt+1 ∼ pG\n\nˆrt ∼ pG Reward: Termination: ˆdt ∼ pG\n\n(cid:0)ˆzt+1| z≤t, a≤t (cid:0) ˆrt | z≤t, a≤t (cid:0) ˆdt | z≤t, a≤t\n\n(cid:1) with ˆzk (cid:1)\n\nt+1 ∼ pG\n\n(cid:1)\n\n(cid:0)ˆzk\n\nt+1 | z≤t, a≤t, z<k\n\nt+1\n\n(cid:1)\n\n(1)\n\n(2)\n\n(3)\n\nNote that the conditioning for the k-th token also includes z<k were already predicted, i.e. the autoregressive process happens at the token level.\n\nt+1 := (z1\n\nt+1, . . . , zk−1\n\nt+1 ), the tokens that\n\nWe train G in a self-supervised manner on segments of L time steps, sampled from past experience. We use a cross-entropy loss for the transition and termination predictors, and a mean-squared error loss or a cross-entropy loss for the reward predictor, depending on the reward function.\n\n2.3 LEARNING IN IMAGINATION\n\nTogether, the discrete autoencoder (E, D) and the Transformer G form a world model, capable of imagination. The policy π, depicted with purple arrows in Figure 1, exclusively learns in this imagination MDP.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nFigure 4: Imagining rewards and episode ends in Breakout (top) and Gopher (bottom). Each row depicts an imagined trajectory initialized with a single frame from the real environment. Yellow boxes indicate frames where the world model predicts a positive reward. In Breakout, it captures that breaking a brick yields rewards, and the brick is correctly removed from the following frames. In Gopher, the player has to protect the carrots from rodents. The world model successfully internalizes that plugging a hole or killing an enemy leads to rewards. Predicted episode terminations are highlighted with red boxes. The world model accurately reflects that missing the ball in Breakout, or letting an enemy reach the carrots in Gopher, will result in the end of an episode.\n\nAt time step t, the policy observes a reconstructed image observation ˆxt and samples action at ∼ π(at|ˆx≤t). The world model then predicts the reward ˆrt, the episode end ˆdt, and the next observation ˆxt+1 = D(ˆzt+1), with ˆzt+1 ∼ pG(ˆzt+1 | z0, a0, ˆz1, a1, . . . , ˆzt, at). This imagination procedure is initialized with a real observation x0 sampled from past experience, and is rolled out for H steps, the imagination horizon hyperparameter. We stop if an episode end is predicted before reaching the horizon. Figure 1 illustrates the imagination procedure.\n\nAs we roll out imagination for a fixed number of steps, we cannot simply use a Monte Carlo estimate for the expected return. Hence, to bootstrap the rewards that the agent would get beyond a given time step, we have a value network V that estimates V (ˆxt) ≃ Eπ\n\n(cid:2) (cid:80)\n\n(cid:3).\n\nτ ≥t γτ −tˆrτ\n\nMany actor-critic methods could be employed to train π and V in imagination (Sutton & Barto, 2018; Kaiser et al., 2020; Hafner et al., 2020). For the sake of simplicity, we opt for the learning objectives and hyperparameters of DreamerV2 (Hafner et al., 2021), that delivered strong performance in Atari games. Appendix B gives a detailed breakdown of the reinforcement learning objectives.\n\n3 EXPERIMENTS\n\nSample-efficient reinforcement learning is a growing field with multiple benchmarks in complex visual environments (Hafner, 2022; Kanervisto et al., 2022). In this work, we focus on the well established Atari 100k benchmark (Kaiser et al., 2020). We present the benchmark and its baselines in Section 3.1. We describe the evaluation protocol and discuss the results in Section 3.2. Qualitative examples of the world model’s capabilities are given in Section 3.3.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Returns on the 26 games of Atari 100k after 2 hours of real-time experience, and humannormalized aggregate metrics. Bold numbers indicate the top methods without lookahead search while underlined numbers specify the overall best methods. IRIS outperforms learning-only methods in terms of number of superhuman games, mean, interquartile mean (IQM), and optimality gap.\n\nGame\n\nRandom\n\nHuman MuZero\n\nEfficientZero\n\nSimPLe\n\nCURL\n\nDrQ\n\nSPR\n\nIRIS (ours)\n\nLookahead search\n\nNo lookahead search\n\nAlien Amidar Assault Asterix BankHeist BattleZone Boxing Breakout ChopperCommand CrazyClimber DemonAttack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull KungFuMaster MsPacman Pong PrivateEye Qbert RoadRunner Seaquest UpNDown #Superhuman (↑) Mean (↑) Median (↑) IQM (↑) Optimality Gap (↓)\n\n227.8 5.8 222.4 210.0 14.2 2360.0 0.1 1.7 811.0 10780.5 152.1 0.0 65.2 257.6 1027.0 29.0 52.0 1598.0 258.5 307.3 -20.7 24.9 163.9 11.5 68.4 533.4\n\n0 0.000 0.000 0.000 1.000\n\n7127.7 1719.5 742.0 8503.3 753.1 37187.5 12.1 30.5 7387.8 35829.4 1971.0 29.6 4334.7 2412.5 30826.4 302.8 3035.0 2665.5 22736.3 6951.6 14.6 69571.3 13455.0 7845.0 42054.7 11693.2\n\nN/A 1.000 1.000 1.000 0.000\n\n530.0 38.8 500.1 1734.0 192.5 7687.5 15.1 48.0 1350.0 56937.0 3527.0 21.8 255.0 1256.0 3095.0 87.5 62.5 4890.8 18813.0 1265.6 -6.7 56.3 3952.0 2500.0 208.0 2896.9\n\n5 0.562 0.227 N/A N/A\n\n808.5 148.6 1263.1 25557.8 351.0 13871.2 52.7 414.1 1117.3 83940.2 13003.9 21.8 296.3 3260.3 9315.9 517.0 724.1 5663.3 30944.8 1281.2 20.1 96.7 13781.9 17751.3 1100.2 17264.2\n\n14 1.943 1.090 N/A N/A\n\n616.9 74.3 527.2 1128.3 34.2 4031.2 7.8 16.4 979.4 62583.6 208.1 16.7 236.9 596.8 2656.6 100.5 51.2 2204.8 14862.5 1480.0 12.8 35.0 1288.8 5640.6 683.3 3350.3\n\n1 0.332 0.134 0.130 0.729\n\n711.0 113.7 500.9 567.2 65.3 8997.8 0.9 2.6 783.5 9154.4 646.5 28.3 1226.5 400.9 4987.7 331.0 740.2 3049.2 8155.6 1064.0 -18.5 81.9 727.0 5006.1 315.2 2646.4\n\n2 0.261 0.092 0.113 0.768\n\n865.2 137.8 579.6 763.6 232.9 10165.3 9.0 19.8 844.6 21539.0 1321.5 20.3 1014.2 621.6 4167.9 349.1 1088.4 4402.1 11467.4 1218.1 -9.1 3.5 1810.7 11211.4 352.3 4324.5\n\n3 0.465 0.313 0.280 0.631\n\n841.9 179.7 565.6 962.5 345.4 14834.1 35.7 19.6 946.3 36700.5 517.6 19.3 1170.7 660.6 5858.6 366.5 3617.4 3681.6 14783.2 1318.4 -5.4 86.0 866.3 12213.1 558.1 10859.2\n\n6 0.616 0.396 0.337 0.577\n\n420.0 143.0 1524.4 853.6 53.1 13074.0 70.1 83.7 1565.0 59324.2 2034.4 31.1 259.1 2236.1 7037.4 462.7 838.2 6616.4 21759.8 999.1 14.6 100.0 745.7 9614.6 661.3 3546.2\n\n10 1.046 0.289 0.501 0.512\n\n3.1 BENCHMARK AND BASELINES\n\nAtari 100k consists of 26 Atari games (Bellemare et al., 2013a) with various mechanics, evaluating a wide range of agent capabilities. In this benchmark, an agent is only allowed 100k actions in each environment. This constraint is roughly equivalent to 2 hours of human gameplay. By way of comparison, unconstrained Atari agents are usually trained for 50 million steps, a 500 fold increase in experience.\n\nMultiple baselines were compared on the Atari 100k benchmark. SimPLe (Kaiser et al., 2020) trains a policy with PPO (Schulman et al., 2017) in a video generation model. CURL (Laskin et al., 2020) develops off-policy agents from high-level image features obtained with contrastive learning. DrQ (Yarats et al., 2021) augments input images and averages Q-value estimates over several transformations. SPR (Schwarzer et al., 2021) enforces consistent representations of input images across augmented views and neighbouring time steps. The aforementioned baselines carry additional techniques to improve performance, such as prioritized experience replay (Schaul et al., 2016), epsilon-greedy scheduling, or data augmentation.\n\nWe make a distinction between methods with and without lookahead search. Indeed, algorithms relying on search at decision time (Silver et al., 2016; 2018; Schrittwieser et al., 2020) can vastly improve agent performance, but they come at a premium in computational resources and code complexity. MuZero (Schrittwieser et al., 2020) and EfficientZero (Ye et al., 2021) are the current standard for search-based methods in Atari 100k. MuZero leverages Monte Carlo Tree Search (MCTS) (Kocsis & Szepesvári, 2006; Coulom, 2007) as a policy improvement operator, by unrolling multiple hypothetical trajectories in the latent space of a world model. EfficientZero improves upon MuZero by introducing a self-supervised consistency loss, predicting returns over short horizons in one shot, and correcting off-policy trajectories with its world model.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nFigure 5: Mean, median, and interquartile mean human normalized scores, computed with stratified bootstrap confidence intervals. 5 runs for IRIS and SimPLe, 100 runs for SPR, CURL, and DrQ (Agarwal et al., 2021).\n\n(a) Performance profiles, i.e. fraction of runs above a given human normalized score.\n\n(b) Probabilities of improvement, i.e. how likely it is for IRIS to outperform baselines on any game.\n\nFigure 6: Performance profiles (left) and probabilities of improvement (right) (Agarwal et al., 2021).\n\n3.2 RESULTS\n\nscore_agent−score_random\n\nThe human normalized score is the established measure of performance in Atari 100k. It is defined as score_human−score_random , where score_random comes from a random policy, and score_human is obtained from human players (Wang et al., 2016).\n\nTable 1 displays returns across games and human-normalized aggregate metrics. For MuZero and EfficientZero, we report the averaged results published by Ye et al. (2021) (3 runs). We use results from the Atari 100k case study conducted by Agarwal et al. (2021) for the other baselines (100 new runs for CURL, DrQ, SPR, and 5 existing runs for SimPLe). Finally, we evaluate IRIS by computing an average over 100 episodes collected at the end of training for each game (5 runs).\n\nAgarwal et al. (2021) discuss the limitations of mean and median scores, and show that substantial discrepancies arise between standard point estimates and interval estimates in RL benchmarks. Following their recommendations, we summarize in Figure 5 the human normalized scores with stratified bootstrap confidence intervals for mean, median, and interquartile mean (IQM). For finer comparisons, we also provide performance profiles and probabilities of improvement in Figure 6.\n\nWith the equivalent of only two hours of gameplay, IRIS achieves a superhuman mean score of 1.046 (+70%), an IQM of 0.501 (+49%), an optimality gap of 0.512 (+11%), and outperforms human players on 10 out of 26 games (+67%), where the relative improvements are computed with respect to SPR (Schwarzer et al., 2021). These results constitute a new state of the art for methods without lookahead search in the Atari 100k benchmark. We also note that IRIS outperforms MuZero, although the latter was not designed for the sample-efficient regime.\n\n7\n\n0.500.751.00SimPLeCURLDrQSPRIRIS (ours)Mean0.10.20.30.4Median0.150.300.45Interquartile MeanHuman Normalized Score012345678Human Normalized Score (τ)0.000.250.500.751.00Fraction of runs with score >τSimPLeCURLDrQSPRIRIS (ours)0.000.250.500.751.00SimPLeCURLDrQSPRAlgorithm YP(IRIS > Y)Published as a conference paper at ICLR 2023\n\nFigure 7: Three consecutive levels in the games Frostbite (left) and Krull (right). In our experiments, the world model struggles to simulate subsequent levels in Frostbite, but not in Krull. Indeed, exiting the first level in Frostbite requires a long and unlikely sequence of actions to first build the igloo, and then go back to it from the bottom of the screen. Such rare events prevent the world model from internalizing new aspects of the game, which will therefore not be experienced by the policy in imagination. While Krull features more diverse levels, the world model successfully reflects this variety, and IRIS even sets a new state of the art in this environment. This is likely due to more frequent transitions from one stage to the next in Krull, resulting in a sufficient coverage of each level.\n\nIn addition, performance profiles (Figure 6a) reveal that IRIS is on par with the strongest baselines for its bottom 50% of games, at which point it stochastically dominates (Agarwal et al., 2021; Dror et al., 2019) the other methods. Similarly, the probability of improvement is greater than 0.5 for all baselines (Figure 6b).\n\nIn terms of median score, IRIS overlaps with other methods (Figure 5). Interestingly, Schwarzer et al. (2021) note that the median is only influenced by a few decisive games, as evidenced by the width of the confidence intervals for median scores, even with 100 runs for DrQ, CURL and SPR.\n\nWe observe that IRIS is particularly strong in games that do not suffer from distributional shifts as the training progresses. Examples of such games include Pong, Breakout, and Boxing. On the contrary, the agent struggles when a new level or game mechanic is unlocked through an unlikely event. This sheds light on a double exploration problem. IRIS has to first discover a new aspect of the game for its world model to internalize it. Only then may the policy rediscover and exploit it. Figure 7 details this phenomenon in Frostbite and Krull, two games with multiple levels. In summary, as long as transitions between levels do not depend on low-probability events, the double exploration problem does not hinder performance.\n\nAnother kind of games difficult to simulate are visually challenging environments where capturing small details is important. As discussed in Appendix E, increasing the number of tokens to encode frames improves performance, albeit at the cost of increased computation.\n\n3.3 WORLD MODEL ANALYSIS\n\nAs IRIS learns behaviors entirely in its imagination, the quality of the world model is the cornerstone of our approach. For instance, it is key that the discrete autoencoder correctly reconstructs elements like a ball, a player, or an enemy. Similarly, the potential inability of the Transformer to capture important game mechanics, like reward attribution or episode termination, can severely hamper the agent’s performance. Hence, no matter the amount of imagined trajectories, the agent will learn suboptimal policies if the world model is flawed.\n\nWhile Section 3.2 provides a quantitative evaluation, we aim to complement the analysis with qualitative examples of the abilities of the world model. Figure 2 shows the generation of many plausible futures in the face of uncertainty. Figure 3 depicts pixel-perfect predictions in Pong. Finally, we illustrate in Figure 4 predictions for rewards and episode terminations, which are crucial to the reinforcement learning objective.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n4 RELATED WORK\n\nLEARNING IN THE IMAGINATION OF WORLD MODELS\n\nThe idea of training policies in a learnt model of the world was first investigated in tabular environments (Sutton & Barto, 2018). Ha & Schmidhuber (2018) showed that simple visual environments could be simulated with autoencoders and recurrent networks. SimPLe (Kaiser et al., 2020) demonstrated that a PPO policy (Schulman et al., 2017) trained in a video prediction model outperformed humans in some Atari games. Improving upon Dreamer (Hafner et al., 2020), DreamerV2 (Hafner et al., 2021) was the first agent learning in imagination to achieve human-level performance in the Atari 50M benchmark. Its world model combines a convolutional autoencoder with a recurrent state-space model (RSSM) (Hafner et al., 2019) for latent dynamics learning. More recently, Chen et al. (2022) explored a variant of DreamerV2 where a Transformer replaces the recurrent network in the RSSM and Seo et al. (2022) enhance DreamerV2 in the setting where an offline dataset of videos is available for pretraining.\n\nREINFORCEMENT LEARNING WITH TRANSFORMERS\n\nFollowing spectacular advances in natural language processing (Manning & Goldie, 2022), the reinforcement learning community has recently stepped into the realm of Transformers. Parisotto et al. (2020) make the observation that the standard Transformer architecture is difficult to optimize with RL objectives. The authors propose to replace residual connections by gating layers to stabilize the learning procedure. Our world model does not require such modifications, which is most likely due to its self-supervised learning objective. The Trajectory Transformer (Janner et al., 2021) and the Decision Transformer (Chen et al., 2021) represent offline trajectories as a static dataset of sequences, and the Online Decision Transformer (Zheng et al., 2022) extends the latter to the online setting. The Trajectory Transformer is trained to predict future returns, states and actions. At inference time, it can thus plan for the optimal action with a reward-driven beam search, yet the approach is limited to low-dimensional states. On the contrary, Decision Transformers can handle image inputs but cannot be easily extended as world models. Ozair et al. (2021) introduce an offline variant of MuZero (Schrittwieser et al., 2020) capable of handling stochastic environments by performing an hybrid search with a Transformer over both actions and trajectory-level discrete latent variables.\n\nVIDEO GENERATION WITH DISCRETE AUTOENCODERS AND TRANSFORMERS\n\nVQGAN (Esser et al., 2021) and DALL-E (Ramesh et al., 2021) use discrete autoencoders to compress a frame into a small sequence of tokens, that a transformer can then model autoregressively. Other works extend the approach to video generation. GODIVA (Wu et al., 2021) models sequences of frames instead of a single frame for text conditional video generation. VideoGPT (Yan et al., 2021) introduces video-level discrete autoencoders, and Transformers with spatial and temporal attention patterns, for unconditional and action conditional video generation.\n\n5 CONCLUSION\n\nWe introduced IRIS, an agent that learns purely in the imagination of a world model composed of a discrete autoencoder and an autoregressive Transformer. IRIS sets a new state of the art in the Atari 100k benchmark for methods without lookahead search. We showed that its world model acquires a deep understanding of game mechanics, resulting in pixel perfect predictions in some games. We also illustrated the generative capabilities of the world model, providing a rich gameplay experience when training in imagination. Ultimately, with minimal tuning compared to existing battle-hardened agents, IRIS opens a new path towards efficiently solving complex environments.\n\nIn the future, IRIS could be scaled up to computationally demanding and challenging tasks that would benefit from the speed of its world model. Besides, its policy currently learns from reconstructed frames, but it could probably leverage the internal representations of the world model. Another exciting avenue of research would be to combine learning in imagination with MCTS. Indeed, both approaches deliver impressive results, and their contributions to agent performance might be complementary.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nThe different components and their training objectives are introduced in Section 2 and Appendix B. We describe model architectures and list hyperparameters in Appendix A. We specify the resources used to produce our results in Appendix G. Algorithm 1 makes explicit the interplay between components in the training loop. In Section 3.2, we provide the source of the reported results for the baselines, as well as the evaluation protocol.\n\nThe code is part of the supplementary materials, and will be open-sourced to ensure reproducible results and foster future research. Minimal dependencies are required to run the codebase and we provide a thorough user guide to get started. Training and evaluation can be launched with simple commands, customization is possible with configuration files, and we include scripts to visualize agents playing and let users interact with the world model.\n\nETHICS STATEMENT\n\nThe development of autonomous agents for real-world environments raises many safety and environmental concerns. During its training period, an agent may cause serious harm to individuals and damage its surroundings. It is our belief that learning in the imagination of world models greatly reduces the risks associated with training new autonomous agents. Indeed, in this work, we propose a world model architecture capable of accurately modeling environments with very few samples. However, in a future line of research, one could go one step further and leverage existing data to eliminate the necessity of interacting with the real world.\n\nACKNOWLEDGMENTS\n\nWe would like to thank Maxim Peter, Bálint Máté, Daniele Paliotta, Atul Sinha, and Alexandre Dupuis for insightful discussions and comments. Vincent Micheli was supported by the Swiss National Science Foundation under grant number FNS-187494.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021.\n\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253–279, 2013a.\n\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: 253–279, 2013b.\n\nYoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through\n\nstochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D ̨ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nNoam Brown, Anton Bakhtin, Adam Lerer, and Qucheng Gong. Combining deep reinforcement learning and search for imperfect-information games. Advances in neural information processing systems, 33:17057–17069, 2020a.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020b.\n\nChang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with\n\ntransformer world models. arXiv preprint arXiv:2202.09481, 2022.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.\n\nRémi Coulom. Computing “elo ratings” of move patterns in the game of go. ICGA journal, 30(4):\n\n198–208, 2007.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\n\nRotem Dror, Segev Shlomov, and Roi Reichart. Deep dominance-how to properly compare deep neural models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2773–2785, 2019.\n\nPatrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12873–12883, 2021.\n\nF. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM.\n\nNeural Computation, 12(10):2451–2471, 2000.\n\nDavid Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in\n\nneural information processing systems, 31, 2018.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nDanijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on\n\nLearning Representations, 2022.\n\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pp. 2555–2565. PMLR, 2019.\n\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020.\n\nDanijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\n\ndiscrete world models. In International Conference on Learning Representations, 2021.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked In Proceedings of the IEEE/CVF Conference on\n\nautoencoders are scalable vision learners. Computer Vision and Pattern Recognition, pp. 16000–16009, 2022.\n\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):\n\n1735–1780, 1997.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\n\nmodeling problem. Advances in neural information processing systems, 34, 2021.\n\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\n\nsuper-resolution. In European conference on computer vision, pp. 694–711. Springer, 2016.\n\nŁukasz Kaiser, Mohammad Babaeizadeh, Piotr Miłos, Bła ̇zej Osi ́nski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model based reinforcement learning for atari. In International Conference on Learning Representations, 2020.\n\nAnssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang, Haicheng Chen, Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, François Fleuret, Alexander Nikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons learned. In Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track, Proceedings of Machine Learning Research, 2022. URL https://proceedings.mlr. press/v176/kanervisto22a.html.\n\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2019.\n\nAndrej Karpathy. minGPT: A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training, 2020. URL https://github.com/karpathy/minGPT.\n\nLevente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference\n\non machine learning, 2006.\n\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, 2018.\n\nAnders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In International conference on machine learning, pp. 1558–1566. PMLR, 2016.\n\nMichael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pp. 5639–5650. PMLR, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.\n\nChristopher Manning and Anna Goldie. Cs224n natural language processing with deep learning,\n\n2022.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016.\n\nSherjil Ozair, Yazhe Li, Ali Razavi, Ioannis Antonoglou, Aaron Van Den Oord, and Oriol Vinyals. Vector quantized models for planning. In International Conference on Machine Learning, pp. 8302–8313. PMLR, 2021.\n\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, pp. 7487–7498. PMLR, 2020.\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\n\nstanding by generative pre-training, 2018.\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n\nmodels are unsupervised multitask learners, 2019.\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In\n\nInternational Conference on Learning Representations, 2016.\n\nMartin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, et al. Player of games. arXiv preprint arXiv:2112.03178, 2021.\n\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, L. Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839): 604–609, 2020.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nMike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5149–5152. IEEE, 2012.\n\nMax Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021.\n\nYounggyo Seo, Kimin Lee, Stephen L James, and Pieter Abbeel. Reinforcement learning with actionfree pre-training from videos. In International Conference on Machine Learning, pp. 19561–19579. PMLR, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.\n\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419): 1140–1144, 2018.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford\n\nBook, Cambridge, MA, USA, 2018.\n\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\n\nneural information processing systems, 30, 2017.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pp. 1995–2003. PMLR, 2016.\n\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021.\n\nRoman V Yampolskiy. Artificial Intelligence Safety and Security. Chapman & Hall/CRC, 2018.\n\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\n\nvq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.\n\nDenis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2021.\n\nWeirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games\n\nwith limited data. Advances in neural information processing systems, 34, 2021.\n\nQinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International\n\nConference on Machine Learning, pp. 27042–27059. PMLR, 2022.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA MODELS AND HYPERPARAMETERS\n\nA.1 DISCRETE AUTOENCODER\n\nOur discrete autoencoder is based on the implementation of VQGAN (Esser et al., 2021). We removed the discriminator, essentially turning the VQGAN into a vanilla VQVAE (Van Den Oord et al., 2017) with an additional perceptual loss (Johnson et al., 2016; Larsen et al., 2016).\n\nThe training objective is the following:\n\nL(E, D, E) = (cid:13)\n\n(cid:13)x − D(z)(cid:13)\n\n(cid:13)1 + (cid:13)\n\n(cid:13)sg(E(x)) − E(z)(cid:13) 2\n(cid:13)\n\n2 + (cid:13)\n\n(cid:13)sg(E(z)) − E(x)(cid:13) 2\n(cid:13)\n\n2 + Lperceptual(x, D(z))\n\nHere, the first term is the reconstruction loss, the next two terms constitute the commitment loss (where sg(·) is the stop-gradient operator), and the last term is the perceptual loss.\n\nTable 2: Encoder / Decoder hyperparameters. We list the hyperparameters for the encoder, the same ones apply for the decoder.\n\nHyperparameter\n\nFrame dimensions (h, w) Layers Residual blocks per layer Channels in convolutions Self-attention layers at resolution\n\nValue\n\n64 × 64 4\n2 64 8 / 16\n\nTable 3: Embedding table hyperparameters.\n\nHyperparameter\n\nVocabulary size (N) Tokens per frame (K) Token embedding dimension (d)\n\nValue\n\n512 16 512\n\nNote that during experience collection in the real environment, frames still go through the autoencoder to keep the input distribution of the policy unchanged. See Algorithm 1 for details.\n\nA.2 TRANSFORMER\n\nOur autoregressive Transformer is based on the implementation of minGPT (Karpathy, 2020). It takes as input a sequence of L(K + 1) tokens and embeds it into a L(K + 1) × D tensor using an A × D embedding table for actions, and a N × D embedding table for frames tokens. This tensor is forwarded through M Transformer blocks. We use GPT2-like blocks (Radford et al., 2019), i.e. each block consists of a self-attention module with layer normalization of the input, wrapped with a residual connection, followed by a per-position multi-layer perceptron with layer normalization of the input, wrapped with another residual connection.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nTable 4: Transformer hyperparameters\n\nHyperparameter\n\nValue\n\nTimesteps (L) Embedding dimension (D) Layers (M) Attention heads Weight decay Embedding dropout Attention dropout Residual dropout\n\n20 256 10 4\n0.01 0.1 0.1 0.1\n\nA.3 ACTOR-CRITIC\n\nThe weights of the actor and critic are shared except for the last layer. The actor-critic takes as input a 64 × 64 × 3 frame, and forwards it through a convolutional block followed by an LSTM cell (Mnih et al., 2016; Hochreiter & Schmidhuber, 1997; Gers et al., 2000). The convolutional block consists of the same layer repeated four times: a 3x3 convolution with stride 1 and padding 1, a ReLU activation, and 2x2 max-pooling with stride 2. The dimension of the LSTM hidden state is 512. Before starting the imagination procedure from a given frame, we burn-in (Kapturowski et al., 2019) the 20 previous frames to initialize the hidden state.\n\nTable 5: Training loop & Shared hyperparameters\n\nHyperparameter\n\nValue\n\nEpochs # Collection epochs Environment steps per epoch Collection epsilon-greedy Eval sampling temperature Start autoencoder after epochs Start transformer after epochs Start actor-critic after epochs Autoencoder batch size Transformer batch size Actor-critic batch size\n\nTraining steps per epoch Learning rate Optimizer Adam β1 Adam β2 Max gradient norm\n\n600 500 200 0.01 0.5 5\n25 50 256 64 64\n\n200 1e-4 Adam 0.9 0.999 10.0\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nB ACTOR-CRITIC LEARNING OBJECTIVES\n\nWe follow Dreamer (Hafner et al., 2020; 2021) in using the generic λ-return, that balances bias and variance, as the regression target for the value network. Given an imagined trajectory (ˆx0, a0, ˆr0, ˆd0, . . . , ˆxH−1, aH−1, ˆrH−1, ˆdH−1, ˆxH ), the λ-return can be defined recursively as follows:\n\nΛt =\n\n(cid:40)\n\nˆrt + γ(1 − ˆdt) V (ˆxH )\n\n(cid:104)\n\n(1 − λ)V (ˆxt+1) + λΛt+1\n\n(cid:105)\n\nif\n\nif\n\nt < H\n\nt = H\n\n(4)\n\nThe value network V is trained to minimize LV , the expected squared difference with λ-returns over imagined trajectories.\n\nLV = Eπ\n\n(cid:104) H−1 (cid:88)\n\nt=0\n\n(cid:0)V (ˆxt) − sg(Λt)(cid:1)2(cid:105)\n\n(5)\n\nHere, sg(·) denotes the gradient stopping operation, meaning that the target is a constant in the gradient-based optimization, as classically established in the literature (Mnih et al., 2015; Hessel et al., 2018; Hafner et al., 2020).\n\nAs large amounts of trajectories are generated in the imagination MDP, we can use a straightforward reinforcement learning objective for the policy, such as REINFORCE (Sutton & Barto, 2018). To reduce the variance of REINFORCE gradients, we use the value V (ˆxt) as a baseline (Sutton & Barto, 2018). We also add a weighted entropy maximization objective to maintain a sufficient exploration. The actor is trained to minimize the following REINFORCE objective over imagined trajectories:\n\nLπ = −Eπ\n\n(cid:104) H−1 (cid:88)\n\nt=0\n\nlog(π(at|ˆx≤t)) sg(Λt − V (ˆxt)) + η H(π(at|ˆx≤t))\n\n(cid:105)\n\n(6)\n\nTable 6: RL training hyperparameters\n\nHyperparameter\n\nImagination horizon (H) γ\nλ η\n\nValue\n\n20 0.995 0.95 0.001\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nC OPTIMALITY GAP\n\nFigure 8: Optimality gap, lower is better. The amount by which the algorithm fails to reach a human-level score (Agarwal et al., 2021).\n\nD IRIS ALGORITHM\n\nAlgorithm 1: IRIS Procedure training_loop():\n\nfor epochs do\n\ncollect_experience(steps_collect) for steps_world_model do\n\nupdate_world_model()\n\nfor steps_behavior do\n\nupdate_behavior()\n\nProcedure collect_experience(n):\n\nx0 ← env.reset() for t = 0 to n − 1 do\n\nˆxt ← D(E(xt)) // forward frame through discrete autoencoder Sample at ∼ π(at|ˆxt) xt+1, rt, dt ← env.step(at) if dt = 1 then\n\nxt+1 ← env.reset()\n\nD ← D ∪ {xt, at, rt, dt}n−1\n\nt=0\n\nProcedure update_world_model():\n\nt=τ\n\nSample {xt, at, rt, dt}τ +L−1 ∼ D Compute zt := E(xt) and ˆxt := D(zt) for t = τ, . . . , τ + L − 1 Update E and D Compute pG(ˆzt+1, ˆrt, ˆdt | zτ , aτ , . . . , zt, at) for t = τ, . . . , τ + L − 1 Update G\n\nProcedure update_behavior():\n\nSample x0 ∼ D z0 ← E(x0) ˆx0 ← D(z0) for t = 0 to H − 1 do\n\nSample at ∼ π(at|ˆxt) Sample ˆzt+1, ˆrt, ˆdt ∼ pG(ˆzt+1, ˆrt, ˆdt | z0, a0, . . . , ˆzt, at) ˆxt+1 ← D(ˆzt+1)\n\nCompute V (ˆxt) for t = 0, . . . , H Update π and V\n\n18\n\n0.560.640.72SimPLeCURLDrQSPRIRIS (ours)Optimality GapHuman Normalized ScorePublished as a conference paper at ICLR 2023\n\nE AUTOENCONDING FRAMES WITH VARYING AMOUNTS OF TOKENS\n\nThe sequence length of the Transformer is determined by the number of tokens used to encode a single frame and the number of timesteps in memory. Increasing the number of tokens per frame results in better reconstructions, although it requires more compute and memory.\n\nThis tradeoff is particularly important in visually challenging games with a high number of possible configurations, where the discrete autoencoder struggles to properly encode frames with only 16 tokens. For instance, Figure 9 shows that, when increasing the number of tokens per frame to 64 in Alien, the discrete autoencoder correctly reconstructs the player, its enemies, and rewards.\n\nFigure 9: Tradeoff between the number of tokens per frame and reconstructions quality in Alien. Each column displays a 64 × 64 frame from the real environment (top), its reconstruction with a discrete encoding of 16 tokens (center), and its reconstruction with a discrete encoding of 64 tokens (bottom). In Alien, the player is the dark blue character, and the enemies are the large colored sprites. With 16 tokens per frame, the autoencoder often erases the player, switches colors, and misplaces rewards. When increasing the amount of tokens, it properly reconstructs the frame.\n\nTable 7 displays the final performance of IRIS trained with 64 tokens per frame in three games. Interestingly, even though the world model is more accurate, the performance in Alien only increases marginally (+36%). This observation suggests that Alien poses a hard reinforcement learning problem, as evidenced by the low performance of other baselines in that game. On the contrary, IRIS greatly benefits from having more tokens per frame for Asterix (+121%) and BankHeist (+432%).\n\nTable 7: Returns on Alien, Asterix, and BankHeist with 64 tokens per frame instead of 16.\n\nGame\n\nRandom Human\n\nSimPLe CURL\n\nDrQ\n\nSPR\n\nIRIS (16 tokens)\n\nIRIS (64 tokens)\n\nAlien Asterix BankHeist\n\n227.8 210.0 14.2\n\n7127.7 8503.3 753.1\n\n616.9 1128.3 34.2\n\n711.0 567.2 65.3\n\n865.2 763.6 232.9\n\n841.9 962.5 345.4\n\n420.0 853.6 53.1\n\n570.0 1890.4 282.5\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nF BEYOND THE SAMPLE-EFFICIENT SETTING\n\nIRIS can be scaled up by increasing the number of tokens used to encode frames, adding capacity to the model, taking more optimization steps per environment steps, or using more data. In this experiment, we investigate data scaling properties by increasing the number of environment steps from 100k to 10M. However, to maintain a training time within our computational resources, we lower the ratio of optimization steps per environment steps from 1:1 to 1:50. As a consequence, the results of this experiment at 100k frames would be worse than those reported in the paper.\n\nTable 8: Increasing the number of environment steps from 100k to 10M.\n\nGame\n\nRandom\n\nHuman\n\nIRIS (100k)\n\nIRIS (10M)\n\nAlien Amidar Assault Asterix BankHeist BattleZone Boxing Breakout ChopperCommand CrazyClimber DemonAttack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull KungFuMaster MsPacman Pong PrivateEye Qbert RoadRunner Seaquest UpNDown #Superhuman (↑) Mean (↑) Median (↑) IQM (↑) Optimality Gap (↓)\n\n227.8 5.8 222.4 210.0 14.2 2360.0 0.1 1.7 811.0 10780.5 152.1 0.0 65.2 257.6 1027.0 29.0 52.0 1598.0 258.5 307.3 -20.7 24.9 163.9 11.5 68.4 533.4\n\n0 0.000 0.000 0.000 1.000\n\n7127.7 1719.5 742.0 8503.3 753.1 37187.5 12.1 30.5 7387.8 35829.4 1971.0 29.6 4334.7 2412.5 30826.4 302.8 3035.0 2665.5 22736.3 6951.6 14.6 69571.3 13455.0 7845.0 42054.7 11693.2\n\nN/A 1.000 1.000 1.000 0.000\n\n420.0 143.0 1524.4 853.6 53.1 13074.0 70.1 83.7 1565.0 59324.2 2034.4 31.1 259.1 2236.1 7037.4 462.7 838.2 6616.4 21759.8 999.1 14.6 100.0 745.7 9614.6 661.3 3546.2\n\n10 1.046 0.289 0.501 0.512\n\n1003.1 213.4 9355.6 6861.0 921.6 34562.5 98.0 493.9 9814.0 111068.8 96218.6 34.0 290.3 97370.6 19212.0 5534.4 1793.8 7344.0 39643.8 1233.0 21.0 100.0 4012.1 30609.4 1815.0 114690.1\n\n15 7.488 1.207 2.239 0.282\n\nTable 8 illustrates that increasing the number of environment steps from 100k to 10M drastically improves performance for most games, providing evidence that IRIS could be scaled up beyond the sample-efficient regime. On some games, more data only yields marginal improvements, most likely due to hard exploration problems or visually challenging domains that would benefit from a higher number of tokens to encode frames (Appendix E).\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nG COMPUTATIONAL RESOURCES\n\nFor each Atari environment, we repeatedly trained IRIS with 5 different random seeds. We ran our experiments with 8 Nvidia A100 40GB GPUs. With two Atari environments running on the same GPU, training takes around 7 days, resulting in an average of 3.5 days per environment.\n\nSimPLe (Kaiser et al., 2020), the only baseline that involves learning in imagination, trains for 3 weeks with a P100 GPU on a single environment. As for SPR (Schwarzer et al., 2021), the strongest baseline without lookahead search, it trains notably fast in 4.6 hours with a P100 GPU.\n\nRegarding baselines with lookahead search, MuZero (Schrittwieser et al., 2020) originally used 40 TPUs for 12 hours to train in a single Atari environment. Ye et al. (2021) train both EfficientZero and their reimplementation of MuZero in 7 hours with 4 RTX 3090 GPUs. EfficientZero’s implementation relies on a distributed infrastructure with CPU and GPU threads running in parallel, and a C++/Cython implementation of MCTS. By contrast, IRIS and the baselines without lookahead search rely on straightforward single GPU / single CPU implementations.\n\nH EXPLORATION IN FREEWAY\n\nThe reward function in Freeway is sparse since the agent is only rewarded when it completely crosses the road. In addition, bumping into cars will drag it down, preventing it from smoothly ascending the highway. This poses an exploration problem for newly initialized agents because a random policy will almost surely never obtain a non-zero reward with a 100k frames budget.\n\nFigure 10: A game of Freeway. Cars will bump the player down, making it very unlikely to cross the road and be rewarded for random policies.\n\nThe solution to this problem is actually straightforward and simply requires stretches of time when the UP action is oversampled. Most Atari 100k baselines fix the issue with epsilon-greedy schedules and argmax action selection, where at some point the network configuration will be such that the UP action is heavily favored. In this work, we opted for the simpler strategy of having a fixed epsilon-greedy parameter and sampling from the policy. However, we lowered the sampling temperature from 1 to 0.01 for Freeway, in order to avoid random walks that would not be conducive to learning in the early stages of training. As a consequence, once it received its first few rewards through exploration, IRIS was able to internalize the sparse reward function in its world model.\n\n21",
    "reference": "# Summary Of The Paper\n\nAuthors train very impressive agents for the 100K Atari benchmark. The method consists in representing Atari frames with discrete tokens and training a transformer-based world model and a game agent. The transformer model is trained in a supervised way. The agent is trained entirely using frames from the world model.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1. Simple and easy-to-understand learning algorithm.\n2. Well-thought-through reuse of standard modeling and algorithmic components.\n3. Outstanding results on Atari 100K benchmark, in particular, bearing in mind the limited compute and expressivity of models.\n\nWeaknesses:\n\n1. Models and agents are trained per game. I am curious what would happen if one tries to train a multi-game world model and agent in the spirit of Gato https://arxiv.org/abs/2205.06175 or Multi-game decision transformers https://arxiv.org/abs/2205.15241. \n2. Nit: the paper may benefit from using more modest language. I mean, in particular, the “drastically different architecture” claim or not very elegant comparisons to look-ahead algorithms.\n\nA bug that may turn into a feature:\n\nThe method needs “more tokens” for games that require the detailed representation of images (as authors put it: “Another kind of games difficult to simulate are mazes with moving enemies, such as MsPacman, BankHeist, and Alien”). This characteristic seems to be a limitation but may lead to an interesting long-context benchmark. Some environments may require more frames and details, leading to a new dataset that can be used to benchmark long-context transformer models.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nCreative repurposing of existing components. The method seems to be easy to use and re-implement.\n\n# Summary Of The Review\n\nAuthors train very impressive agents for the 100K Atari benchmark.  Benchmark results are excellent but algorithmic, and modeling novelty is limited.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nONLINE CONTINUAL LEARNING FOR PROGRESSIVE DISTRIBUTION SHIFT (OCL-PDS): A PRACTITIONER’S PERSPECTIVE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe introduce the novel OCL-PDS problem - Online Continual Learning for Progressive Distribution Shift. PDS refers to the subtle, gradual, and continuous distribution shift that widely exists in modern deep learning applications. It is widely observed in industry that PDS can cause significant performance drop. While previous work in continual learning and domain adaptation addresses this problem to some extent, our investigations from the practitioner’s perspective reveal flawed assumptions that limit their applicability on daily challenges faced in realworld scenarios, and this work aims to close the gap between academic research and industry. For this new problem, we build 4 new benchmarks from the Wilds dataset (Koh et al., 2021), and implement 12 algorithms and baselines including both supervised and semi-supervised methods, which we test extensively on the new benchmarks. We hope that this work can provide practitioners with tools to better handle realistic PDS, and help scientists design better OCL algorithms.\n\n1\n\nINTRODUCTION\n\nIn most modern deep learning applications, the input data undergoes a continual distribution shift over time. For example, consider a satellite image classification task as illustrated in Figure 1a. In this task, the input data distribution changes with time due to the changes in landscape, and camera updates which can lead to higher image resolutions and wider color bands. Similarly, in a toxic language detection task on social media illustrated in Figure 1b, the distribution shift can be caused by a shift in trends and hot topics (many people post about hot topics like BLM (Wikipedia contributors, 2022a) and Roe v. Wade (Wikipedia contributors, 2022b) on social media), or a shift in language use (R ̈ottger & Pierrehumbert, 2021; Luu et al., 2022). Such distribution shift can cause significant performance drop in deep models, a widely observed phenomenon known as model drift.\n\nA critical problem for practitioners, therefore, is how to deal with what we term progressive distribution shift (PDS), defined as the subtle, gradual, and continuous distribution shift that widely exists in modern deep learning applications. In this work, we explore handling PDS with online continual learning (OCL), where the learner collects, learns, and is evaluated on online samples from a continually changing data distribution. In Section 2, we formulate the OCL-PDS problem.\n\nThe OCL-PDS problem is closely related to two research areas: domain adaptation (DA) and continual learning (CL), in which there is a rich body of academic work. However, through a literature review and our conversations with practitioners, we find that there still remains a gap between the settings widely used in academic work and in real industrial applications. To close this gap, we commit ourselves to thinking from a practitioner’s perspective, which is the core spirit of this work. Our primary goal is to build tools for investigating the real issues practitioners are facing in their day-to-day work. To achieve this goal, we challenge the prevailing assumptions in previous work, and propose three important modifications to the conventional DA and CL problem settings:\n\n1. Task-free: One point conventional DA and CL settings have in common is assuming clear boundaries between distinct domains (or tasks), but practitioners rarely apply the same model to very different domains in industry. In contrast, OCL studies the task-free CL setting (Aljundi et al., 2019b) where there is no clear boundary, and the distribution shift is continuous. Moreover, in OCL both training and evaluation are online, unlike previous task-free CL settings with offline evaluation, which is not as realistic in a “lifelong” setting.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) FMoW-WPDS benchmark.\n\n(b) CivilComments-WPDS benchmark.\n\nFigure 1: FMoW-WPDS and CivilComments-WPDS benchmarks which we build in this work.\n\n2. Forgetting is allowed: Avoiding catastrophic forgetting is a huge topic in CL, which usually requires no forgetting on all tasks. However, remembering everything is actually impractical, infeasible and potentially harmful, so OCL-PDS only requires remembering recent knowledge and important knowledge which is described by a regression set (Sec. 2.2).\n\n3. Infinite storage: Previous work in CL usually assumes a limited storage (buffer) size. However, storage is not the most pressing bottleneck in most industrial applications. Thus, in OCL-PDS, we assume an infinitely large storage where all historical samples can be stored. However, the learner cannot replay all samples because it would be too inefficient.\n\nTo demonstrate the novelty and practicality of the OCL-PDS problem, in Section 2.3 we will discuss related work, compare OCL-PDS with common and similar settings used in previous work, and elaborate on the reasons why we believe these three key modifications align OCL-PDS more closely with industrial applications and practitioners’ pain points. A more thorough literature review can be found in Appendix A. Then, in Section 3, we will build 4 new benchmarks for OCL-PDS, including both vision and language tasks. When building these benchmarks, we make every effort to make sure that they can reflect real PDS scenarios that practitioners need to deal with.\n\nIn Section 4, we will explore OCL algorithms and how to combine them with semi-supervised learning (SSL) as unlabeled data is very common in practice. In total, we implement 12 supervised and semi-supervised OCL algorithms and baselines adapted to OCL-PDS, which we test extensively on our benchmarks in Section 5. Our key observations in these experiments include: (i) A taskdependent relationship between learning and remembering; (ii) Some existing methods have low performances on regression tasks; (iii) SSL helps improve online performance, but it requires a critical virtual update step. Finally, in Section 6 we discuss remaining problems and limitations.\n\nContributions. Our contributions in this work include: (i) Introducing the novel OCL-PDS problem which more closely aligns with practitioners’ needs; (ii) Releasing 4 new benchmarks for this novel setting; (iii) Adapting and implementing 12 OCL algorithms and baselines, including both supervised and semi-supervised, for OCL-PDS; (iv) Comparing these algorithms and baselines with extensive experiments, which leads to a number of key observations. Overall, we believe that this work is an important step in closing the gap between academic research and industry, and we hope that this work can inspire more practitioners and researchers to investigate and dive deep into realworld PDS. To this end, we release our benchmarks and algorithms, which are easy to use and we hope can help boost the development of OCL algorithms for handling PDS.\n\n2 THE OCL-PDS PROBLEM\n\n2.1 PROBLEM FORMULATION We have a stream of online data S0, S1, · · · , where each St is a batch of i.i.d. samples from distribution Dt that changes with time t continuously, for which we assume that Div(Dt ∥ Dt+1) < ρ for all t for some divergence function Div. Online Continual Learning (OCL) goes as follows:\n\n• At t = 0, receive a labeled training set S0, on which train the initial model f0 • For t = 1, 2, · · · , T, · · · do\n\n1. Data collection: Receive a new unlabeled data batch St = {(x(i) 2. Evaluation: Predict on St with the current model ft−1, and get some feedback 3. Fine-tuning: Update the model ft−1 → ft with all previous information\n\nt )}nt\n\n, y(i)\n\ni=1\n\nt\n\ni.i.d.∼ Dt.\n\nEvaluation metrics. An OCL algorithm is used for fine-tuning and is evaluated by three metrics:\n\n1. Online performance: Denote the performance of fs on St by At time t (as computed in Step 2 - Evaluation) is At horizon T is defined as (A1\n\ns. The online performance at t−1, and the average online performance before\n\n0 + · · · + AT\n\nT −1)/T .\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2. Knowledge retention: Unlike conventional CL that requires no forgetting on all tasks, in OCLPDS the model only needs to remember two types of knowledge: recent knowledge and important knowledge. For a certain recent time window w, the recent performance is defined as (At−w t−1 + · · · + At−1 t−1)/w. The important data is described by a regression set, and the regression set performance is the model’s performance on this set.\n\n3. Training efficiency: This is measured by the average runtime of the fine-tuning step, which is very important for this online setting where the OCL algorithm is run for many times.\n\n2.2 DETAILS\n\nDivergence function. For distributions P and Q, Div(P ∥ Q) is the divergence from P to Q, and can be different from Div(Q ∥ P ). According to our reasoning in Appendix B, an ideal divergence function for OCL-PDS should be asymmetric and bounded, so we cannot use popular functions such as total variation, Wasserstein distance, MMD, KL-divergence and JS-divergence. In this work, we use the ε-KL-divergence (Eqn. (2)), whose definition and properties can be found in Appendix B.\n\nIf |St| = 1, then this is the conventional online learning setting where samples arrive Data batch. one by one. However, industry practitioners seldom update the model over a single sample, and always collect a batch of samples before fine-tuning the model, so we consider data batches instead.\n\nFeedback. Without any feedback from the evaluation process, the problem is fully unsupervised because we only have the unlabeled batches to fine-tune the model. This setting, however, is too hard and unrealistic. Indeed, for most deployed industrial systems, there are tools for evaluating online performance, either through some automated metrics or feedback provided by end-users. Here, we consider the user error report model, where a fraction of the users provide feedback on incorrect model outputs. This model leads to Random Label Feedback (RLF), where the labels of α fraction of the samples in St are provided as feedback. In this scenario, α = 0, α = 1 and α ∈ (0, 1) correspond to the unsupervised, supervised and semi-supervised learning settings, respectively.\n\nAll previous information. We allow the learner to store all previously seen samples and feedback (though the learner cannot really replay all samples as it would be too inefficient), which is starkly different from most previous papers in CL that assume a limited storage size.\n\nRecent knowledge. OCL requires no forgetting on recent knowledge, because (i) in general, practitioners expect the model not to forget too quickly, and (ii) in many applications the same distribution repeats periodically, which makes recent knowledge useful. For example, satellite images in summer and winter look very different (e.g. due to snow), but the images in two consecutive summers look similar, so in this case it is useful to remember the knowledge for at least one year.\n\nIn the software industry, regression refers to the deterioration of performance after Regression set. an update (Yan et al., 2021). The regression set contains the regression data on which making a mistake is more expensive. Moreover, the labeling function P (Y |X) of the regression data changes very little over time if any (no concept shift). The two most common types of regression data in industry are: (i) Frequent data, which appears more often than other data; (ii) Critical data, which weighs more in the model evaluation and whose definition depends on the specific application.\n\n2.3 RELATED WORK AND COMPARISON WITH PREVIOUS SETTINGS\n\nThis work is related to three areas: Domain adaptation (DA), continual learning (CL), and semisupervised learning (SSL). DA provides a learner with labeled samples from a source distribution P and (partially labeled, unlabeled, or no) samples from a target distribution Q, and requires it to learn a good model on Q. Surveys on DA include Lu et al. (2018); Wang & Deng (2018); Ramponi & Plank (2020); Wang et al. (2022b). In CL, the learner needs to continually learn new knowledge from an online stream of data, and settings include task-incremental CL (including domain-incremental and class-incremental CL), task-aware CL, task-agnostic CL, task-free CL and OCL. Surveys on CL include De Lange et al. (2021); Masana et al. (2020); Biesialska et al. (2020). SSL requires the model to learn from a training set consisting of few labeled samples and many unlabeled samples. Surveys on SSL include Van Engelen & Hoos (2020); Ouali et al. (2020); Yang et al. (2021). A more thorough literature review can be found in Appendix A.\n\nThere are several differences between the OCL-PDS problem and conventional DA and CL settings. We now explain why our formulation is more relevant and thus useful for industry practitioners.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nPDS vs DA/DG. Domain adaptation (DA) and its sibling domain generalization (DG) mostly study big, one-shot distribution shifts, i.e. training and testing on two very different domains. This has two problems: (i) Practitioners seldom directly apply a model to a very different domain in industry. Instead, they usually train one model for each domain, and train a domain classifier to distinguish among different domains; (ii) Even if a model needs to be applied to a different domain, practitioners would usually first collect some labeled data from the new domain and then fine-tune the model. It is very rare to have no labels or samples at all from the target domain in industry.\n\nOn the contrary, OCL-PDS is a very common scenario in industry. First, PDS has been widely reported to cause performance drop in industrial applications (Martinel et al., 2016; Jaidka et al., 2018; Huang & Paul, 2019), and practitioners do not often train new models for PDS. Second, OCLPDS studies the semi-supervised setting where the practitioners can collect a few labeled samples and many unlabeled samples, which is more realistic than having no labels or samples at all.\n\nOCL vs CL. OCL falls under the task-free continual learning setting where there is a fixed task and no clear task boundary. It is different from conventional task-incremental CL in three ways:\n\n1. Task-incremental CL has N distinct tasks and requires a single model to learn them all, but what practitioners would usually do in this case is to train N models, one for each task. In contrast, in OCL-PDS the task is fixed but the data distribution is gradually and constantly changing, so it is more reasonable to use and continually fine-tune a single model.\n\n2. Conventional CL requires the model to remember all N tasks with a storage of size M . For\n\nstudying PDS, this requirement has three problems:\n\n(i) It is not so practical as not all old knowledge is important - A satellite image classifier in 2022 does not need to do very well on images in 2002 with different landscapes.\n\n(ii) In the “lifelong” setting with N continually growing, we need M to also grow with N (like M = O(N )). With a fixed M , it is infeasible to remember everything. (iii) Many applications have concept shift where P (Y |X) could change, so remembering old knowledge can be harmful to the performance on the current data distribution. For instance, languages that were not considered offensive 20 years ago are widely recognized as offensive today thanks to the recent civil rights movements.\n\nThus, OCL-PDS only requires remembering recent knowledge and important knowledge.\n\n3. Our setting assumes infinite storage unlike previous settings. This is an over-optimistic assumption as there are real applications where the amount of data is so huge that it is impossible to store all data even for big companies. Real applications also have other considerations such as privacy restrictions so that the data cannot be stored forever. However, storage size is rarely the bottleneck of industrial applications. The point of making this assumption is to not put too much effort into utilizing every bit of storage. Instead, we want to focus on more practically relevant questions, such as how to leverage unlabeled data and how to improve training efficiency.\n\nOther similar settings. First, OCL is different from the task-free CL formulated in some previous work (Aljundi et al., 2019b; de Masson d'Autume et al., 2019; Wang et al., 2022d) where the training is online but the evaluation is offline. Previous work typically splits the data domain into different sections which the learner sees sequentially online, and in the end the learner is evaluated on the entire domain offline. On the contrary, both training and evaluation in OCL are online: For each new batch, the model is first tested and then trained on it. Thus, it is possible to have the real “lifelong” learning setting in OCL where the time horizon T = ∞ but not in the previous setting.\n\nSecond, OCL-PDS is closely related to reinforcement learning (RL) and time series analysis. The difference from RL is that in RL, the agent can learn from a number of episodes, while in OCL-PDS the evaluation is online and one-pass. The difference from time series analysis is that time series focuses on predicting on future data and does not care about forgetting. Moreover, though PDS naturally resides in time series data, in our literature review (Appendix A) we find little work about handling PDS with time series analysis. One such line of work is temporal covariate shift (TCS) (Du et al., 2021) that assumes that P (Y | X) is always fixed, which is not assumed in OCL-PDS.\n\nThird, there are two related settings, gradual domain adaptation (GDA) (Kumar et al., 2020) and gradual concept drift (Liu et al., 2017; Xu & Wang, 2017), that also study gradual shift from one\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ndomain to another with a series of distributions P0, P1, · · · , PT , where P0 is the source domain, PT is the target domain, and Pt and Pt+1 is close for each t. Both settings only require good adaptation performance and do not consider forgetting. However, the concept of regression set widely exists in modern deep learning applications, and no forgetting on the regression set is a critical issue.\n\nFinally, Cai et al. (2021) proposed a similar OCL setting, where the model is also first evaluated on the new batch and then fine-tuned on it. However, OCL-PDS has three important distinctions: (i) Cai et al. (2021) considered a fully supervised setting while OCL-PDS mainly uses a semisupervised setting that is more common in practice; (ii) OCL-PDS uses different evaluation metrics, and in particular measures the critical regression set performance; (iii) Cai et al. (2021) evaluates knowledge retention only at three time steps whereas OCL-PDS is purely online and evaluates all metrics at all time steps. A more detailed comparison can be found in Appendix A.2.1.\n\n3 BENCHMARKS\n\nIn search of benchmarks for this novel OCL-PDS problem setup, we first investigate the benchmarks used in previous work on continual learning and find that most of them are one of the following: (i) Multiple datasets, e.g. Task 1 is MNIST (LeCun & Cortes, 2010), Task 2 is SVHN (Netzer et al., 2011) and so on; (ii) Perturbed image classification, where different tasks are constructed by rotating the images with different angles, shifting the colors with different scales or permuting the pixels with different random seeds; (iii) Split-class classification, e.g. split the 100 classes in CIFAR-100 (Krizhevsky et al., 2009) into 20 groups with 5 classes per group, and make each group a 5-way classification task. None of them is realistic enough to represent the real PDS in practice.\n\nThere are two existing PDS benchmarks: CLOC (Cai et al., 2021) and CLEAR (Lin et al., 2021). Both are image classification tasks and do not have regression sets. Thus, we build a new, more comprehensive suite of benchmarks that covers both language and vision tasks, and both classification and regression tasks, with intuitively defined regression sets. Since it is impossible for us to cover all existing tasks, we also provide our 3-step procedure to build our benchmarks, which can be used to construct PDS benchmarks on other existing datasets.\n\n3.1 THE 3-STEP PROCEDURE TO BENCHMARK OCL-PDS\n\nHere we provide the 3-step procedure we use to benchmark OCL-PDS on an existing dataset:\n\n1. Separate the data into groups (domains). For example, group the data by year. Then, do an OOD check which verifies that there is a significant distribution shift across the groups.\n\n2. Assign shifting group weights to the batches to create a distribution shift across the groups.1 Then, do a shift continuity check which verifies that the shift is continuous (not abrupt).\n\n3. Design a separate regression set which does not intersect with any online batch. Then, do\n\na regression check which verifies that na ̈ıve methods have regression on this set.\n\nMoreover, for each batch (including the regression set), we randomly divide the batch into a training batch and a test batch. The initial training set contains both the first batch and the training regression set. The learner sees the training batches during OCL, and the separate test batches are used to evaluate the recent and regression set performances. The model is never evaluated on training samples it has already seen because it is not useful as the learner can store all these samples in its buffer.\n\nExample: CivilComments-WPDS. Here we briefly demonstrate this procedure and a detailed description can be found in Appendix C.1. The CivilComments dataset contains online comments with topic labels, and we want to model PDS with shifting hot topics on it. In Step 1, we divide the comments into four groups according to their topics, and verify that a model trained on any three groups does poorly on the fourth group; In Step 2, we construct a weight shift among the groups to simulate PDS, and verify that a model trained on labeled samples from previous distributions can do well on the new distribution, so that the shift is continuous; In Step 3, we define the regression set, and verify that catastrophic forgetting will happen if we only train on the new data.\n\n1This is not equivalent to group shift or subpopulation shift as studied in fair machine learning and longtailed learning (learning with imbalanced classes). The group weights here are used to control the scale of the divergence from being too big. We simulate a gradual shift by continuously shifting the group weights.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Benchmarks we build. T + 1 = total number of batches. w = recent time window.\n\nBenchmark\n\nDescription\n\nRegression Set\n\nT + 1 w\n\nCritical data CivilComments-WPDS Toxic language detection on social media Satellite image classification for facilities FMoW-WPDS Frequent data Review sentiment analysis on Amazon.com Frequent data Amazon-WPDS Satellite image regression for wealth index Poverty-WPDS\n\nCritical data\n\n16 25 18 14\n\n5 6\n6 5\n\n3.2 OUR BENCHMARKS\n\nWe build 4 new benchmarks following the guidelines in Appendix C. See Table 1 for a summary. All 4 benchmarks are based on the Wilds datasets (Koh et al., 2021). Please refer to this paper for the potential leverage, broader context and ethic considerations of these datasets. Here we briefly describe the 4 benchmarks we release in this work, and details can be found in Appendix C.\n\nCivilComments-WPDS. This benchmark is based on the CivilComments-Wilds dataset (Borkan et al., 2019), which is a toxic language detection task on social media (Figure 1b). WPDS stands for Wilds-PDS. This benchmark models the shift in hot topics. The regression set contains critical data - Comments with severe harassment, including identity attack and explicit sexual comments.\n\nFMoW-WPDS. This benchmark is based on the FMoW-Wilds dataset (Christie et al., 2018), which is a satellite image facility classification task (Figure 1a). It models the shift in time. The regression set contains frequent data - Data from two highly populated regions: Americas and Asia.\n\nAmazon-WPDS. This benchmark is based on the Amazon-Wilds dataset (Ni et al., 2019), which is a review sentiment analysis task on shopping websites. This benchmark models the shift in language use. The regression set contains frequent data - Data from 10 popular product categories.\n\nPoverty-WPDS. This benchmark is based on the PovertyMap-Wilds dataset (Yeh et al., 2020), which is a satellite image wealth index regression task. This benchmark models the shift in time. The regression set contains critical data - Images from urban areas. Following Koh et al. (2021), performances here are measured by the Pearson correlation between outputs and ground truths.\n\n4 OCL ALGORITHMS\n\nAn OCL algorithm consists of the following three components:\n\n• Continual fine-tuning: How to fine-tune the model on the new data?\n\n• Knowledge retention: How to prevent forgetting?\n\n• Semi-supervised learning: How to leverage the unlabeled data?\n\nIn particular, an OCL algorithm is called supervised if it does not have the semi-supervised learning component, and called semi-supervised if it does. In the rest of this section we will provide an overview of the OCL algorithms we implement: first baselines, then supervised, and finally semisupervised. Implementation details of these algorithms can be found in Appendix D.\n\n4.1 NA ̈IVE BASELINES\n\nThe na ̈ıve baselines are used to measure the difficulty of an OCL-PDS task for interpreting the performances of OCL algorithms. First we have First Batch Only (FBO), where we only train an initial model on the first batch S0 (including the training regression set) with empirical risk minimization (ERM) and use that model till the end, which serves as a lower bound of the online performance as well as an upper bound of the regression set performance (because it directly trains the model on regression data without any forgetting). Then we have i.i.d. offline, where for each t we train a model on a separate training set i.i.d. sampled from Dt and test it on St, which serves an approximate upper bound of the online performance that reflects the generalization gap. Finally, we have New Batch Only (NBO), where we train the model on the new data alone and never care about forgetting, which serves as a lower bound of the knowledge retention performance.\n\n4.2 SUPERVISED OCL ALGORITHMS\n\nRehearsal based methods. Rehearsal was first introduced in Ratcliff (1990); Robins (1995) to prevent catastrophic forgetting in CL, where historical data is stored in a memory buffer and replayed to the learner. The simplest method is ER-FIFO (also called ring buffer (Chaudhry et al., 2019b)),\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ER stands for experience replay. There are three sources of data the model needs to learn or remember: new data, recent data and regression data. Thus, ER-FIFO simply fine-tunes the model over the union of these three sets of data. The buffer looks like a first-in-first-out (FIFO) queue as the new batch replaces the oldest one (while the regression set is never removed).\n\nThere are some variants of ER-FIFO with different strategies of selecting replay samples. In ERFIFO-RW where RW stands for reweighting, the three data sources are balanced so that they have the same probability of being sampled in stochastic gradient descent (SGD), which is useful when different batches have different sizes (the weights can also be customized to adjust between learning and remembering). In Maximally Interfered Retrieval (MIR) (Aljundi et al., 2019a), the model is first “virtually” updated on the new data only, and those previous samples on which the loss increases the most before and after the virtual update are selected. The model is then recovered and fine-tuned with a real update on the selected samples together with the new samples. Similarly, in MaxLoss (Lin et al., 2022) there is also a virtual update step, and previous samples with the highest loss after virtual update are selected to be replayed during real update.\n\nGEM-PDS. This method is a combination of Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) and Average GEM (A-GEM) (Chaudhry et al., 2019a), and we design it specially for OCL-PDS. Denote the gradients of the loss function on the new data, recent data and regression data by g0, g1 and g2, respectively. Gradient descent along g0 might cause the model to forget recent and important knowledge, so instead we find a “pseudo gradient” g that is close to g0, and gradient descent along g won’t lead to forgetting. We solve the following convex optimization problem:\n\nminimize g\n\n∥g − g0∥2\n\n2\n\ns.t.\n\n⟨g, g1⟩ ≥ 0, ⟨g, g2⟩ ≥ 0\n\n(1)\n\nThis problem is always feasible, and the optimal g∗ can be found with a simple procedure described in Appendix D Eqn. (3). The constraints ensure that descent along g∗ won’t increase the loss on the recent and regression data, which can be shown with the Taylor expansion of the loss function. Regularization based methods. The high-level idea of regularization is to keep the model weights close to the initial model which has a good regression set performance, so as to reduce forgetting. Denote the vectorized model weights at time t by θt. In Online L2 Regularization (L2Reg), we add a penalty term λ 2 to the loss function. In a variant called Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), the penalty term is λ 2, where Ft is the Fisher information matrix (FIM) and diag(Ft) only contains the elements on the diagonal of Ft. The FIM ensures that weights that are more influential to the model output change less.\n\n2 ∥diag(Ft)(θt − θ0)∥2\n\n2 ∥θt − θ0∥2\n\n4.3 SEMI-SUPERVISED OCL ALGORITHMS\n\nPseudo Labeling (PL). PL was proposed in Lee (2013) and is also known as self-training. Since Dt is close to Dt−1, if ft−1 can do well on Dt−1, then it is very likely that it can also do well on Dt. Based on this observation, for an unlabeled sample x in St, define its pseudo label simply as ft−1(x). Then, we fine-tune the model ft−1 → ft on the union of the labeled and the pseudolabeled sets with any supervised OCL algorithm. We denote a PL method by adding suffix “PL” after this supervised algorithm, such as ER-FIFO-PL. Note that in classification, ft−1(x) is the “hard” label, so training ft−1 on this label minimizes its entropy on x. Kumar et al. (2020); Wang et al. (2022a) proved the effectiveness of PL in the GDA context. Moreover, pseudo-labeled samples are not replayed for knowledge retention as they could have large label noise.\n\nFurthermore, in our implementation of PL we introduce an innovative virtual update step which we find very important in our experiments. The whole algorithm goes as follows:\n\n1. Virtual update: Fine-tune the model ft−1 → f ′ 2. Pseudo label the unlabeled samples with the updated model: x → (x, f ′ 3. Revert the model weights from f ′\n\nt−1(x)).\n\nt−1 with ERM on the labeled samples only.\n\nt−1 back to ft−1, and fine-tune ft−1 → ft with any\n\nsupervised OCL algorithm on the union of labeled and pseudo-labeled samples.\n\nFixMatch (FM). FixMatch (Sohn et al., 2020) is a variant of PL. In FM, there are two types of data augmentation: a strong one and a weak one, and the pseudo labels are generated on the weakly augmented samples while the model is fine-tuned on the strongly augmented samples, which leads to a consistency regularization effect that makes the model have consistent outputs on the weakly and strongly augmented samples. Currently FixMatch is only implemented for vision tasks. We denote FixMatch by adding suffix “FM” after a supervised algorithm, such as ER-FIFO-FM.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) CivilComments.\n\n(b) FMoW.\n\n(c) CivilComments.\n\n(d) FMoW.\n\nFigure 2: Results of supervised OCL algorithms on CivilComments-WPDS (α = 0.5%) and FMoWWPDS (α = 50%). Each point corresponds to one pair of algorithm and hyperparameters.\n\n5 EXPERIMENTS\n\nWe compare the algorithms on the 4 benchmarks we constructed for OCL-PDS. Each experiment is run 5 times with different random seeds. For the recent performance and the regression set performance (reg performance), we report both the average and the worst performances, which are the mean and minimum of the performance over t = 1, · · · , T , respectively. The reason why we also report the worst performance is that knowledge retention is required for all t. For saving space, we put detailed settings and results in Appendix E. In this section we summarize five remarkable observations we make from our experiments, first supervised and then semi-supervised OCL algorithms.\n\n5.1 RESULTS OF SUPERVISED OCL ALGORITHMS\n\nObservation #1: Strong positive correlation between online and recent performances. On all benchmarks, methods that achieve higher online performances also achieve higher recent performances. In Figures 2a and 2b, we plot the average online and worst recent performances achieved by different supervised OCL algorithms on the CivilComments-WPDS (α = 0.5%) and FMoW-WPDS (α = 50%) benchmarks (α is the fraction of labeled samples), where we can see a strong positive correlation between these two. The reason is that Dt is very close to Dt−1, · · · , Dt−w by formulation, so a model that performs well on Dt naturally performs well on the w recent distributions too. This is also known as the accuracy-on-the-line phenomenon (Miller et al., 2021).\n\nObservation #2: Correlation between online and reg performances differs among benchmarks. In Figure 2c and 2d, we plot the average online and worst reg performances on CivilCommentsWPDS and FMoW-WPDS. We can see that the trends on these two benchmarks are quite different. On CivilComments-WPDS, methods with higher online performances have lower reg performances, while there is no such trend on FMoW-WPDS. One possible reason is that it depends on how the regression set is defined, and how close it is between the regression set distribution and the overall distribution. For CivilComments-WPDS, the regression set samples (severely offensive comments) are very different from the other samples (mostly normal comments), while for FMoW-WPDS the regression set samples from Americas and Asia are closer to the samples from other regions.\n\nObservation #3: Some existing methods do not work well on regression tasks. On the right, we plot the average online and worst reg performances of different supervised OCL algorithms on the Poverty-WPDS benchmark (α = 50%), which is a regression task. We observe that while some methods like ER-FIFO and L2Reg do better than the FBO baseline, some others including MIR and EWC achieve lower online performance than FBO which should be a lower bound. Most existing methods have only been tested on classification tasks before, and this experiment shows that they might not work so well on regression tasks.\n\n5.2 RESULTS OF SEMI-SUPERVISED OCL ALGORITHMS\n\nFigure 3: Poverty-WPDS.\n\nObservation #4: Unlabeled data improves OCL performance. In Figure 4, we plot the performances of ER-FIFO-PL/FM and ER-FIFO-RW-PL/FM (in red) along with the performances of all supervised OCL algorithms (in blue) on CivilComments-WPDS and FMoW-WPDS. We can see that with the same worst reg performance, SSL methods achieve higher average online performances, and vice versa. SSL improves the online performance by learning more new data, but it does not help knowledge retention. Thus, supervised OCL algorithms with high online performances (such as MIR) do not improve much with SSL. We also observe that FixMatch is slightly better than PL.\n\nObservation #5: Virtual update in PL is important. In Figure 5 we plot the performances of ERFIFO-PL and ER-FIFO-RW-PL on CivilComments-WPDS with different epochs of virtual update\n\n8\n\n82848688Avg Online Perf767880828486Worst Recent Perf68707274Avg Online Perf646668707274Worst Recent Perf82848688Avg Online Perf20406080Worst Reg Perf68707274Avg Online Perf747678808284Worst Reg Perf0.740.760.780.8Avg Online Perf0.50.550.60.65Worst Reg PerfNBOER-FIFOER-FIFO-RWGEM-PDSMIRMaxLossL2RegEWCFBOUnder review as a conference paper at ICLR 2023\n\n(a) CivilComments.\n\n(b) FMoW.\n\nFigure 4: Performances of ER-FIFO-PL/FM and ERFIFO-RW-PL/FM (in red). FM is only used for FMoW.\n\nFigure 5: PL with different epochs of virtual update (CivilComments).\n\nas labeled near the points. We can see that without virtual update (0 epoch), the online performance of PL is even lower than the FBO baseline. However, with just 1 epoch of virtual update, the online performance rises above FBO, and with more epochs of virtual update the online performance is higher (but with a lower reg performance). This shows the importance of virtual update though it was not included in previous methods like gradual self-training (Kumar et al., 2020).\n\nOne explanation is that the virtual update step distills the knowledge of P (Y | X) from the new distribution into the current model, so P (f ′ t−1(X) | X) is closer to P (Y | X, Dt). Without virtual update, ft−1 only has knowledge from old distributions, so when the model is trained on the pseudolabeled samples, it reinforces its old knowledge but learns little new knowledge, resulting in a low online performance but high reg performance. This also implies that unsupervised OCL is difficult and perhaps infeasible, because without new labels the learner cannot know how P (Y | X) changes.\n\n6 DISCUSSION\n\nThe application of deep learning has become so wide nowadays that it is very difficult to cover all the tasks with a single general problem formulation. Here we briefly discuss two additional problems that stem from OCL-PDS which practitioners in certain areas in industry might find useful.\n\nFine-grained PDS. In our problem formulation in Section 2.1, the learner can fine-tune the model at every t. However, in applications where new data comes in very fast and the distribution changes very quickly, this could be impractical. One typical example is time series analysis such as stock price prediction. We term this setting fine-grained PDS as the data batches are much smaller and are received much more frequently. In our benchmarks, the time horizon T is around 20, while in a fine-grained PDS benchmark T shoud typically be 2,000 or 20,000.\n\nOne way to handle fine-grained PDS is to have two fine-tune procedures: A fast one that can be done at every t, and a slow one that is run simultaneously with the fast one. For instance, in an ensemble method like Mixture-of-Experts (Masoudnia & Ebrahimpour, 2014), the fast procedure only adjusts the weights of the base models, while the slow procedure trains a new base model with the new data.\n\nAbrupt shift detection. We assumed that Div(Dt ∥ Dt+1) < ρ for all t. However, this might not always be true, and abrupt shifts could happen in practice. For example, for toxic language detection on social media (Figure 1b), when the US Supreme Court overturned Roe v. Wade, posts related to gender, religion, politics and civil rights flooded on social media, causing a sharp spike in the data distribution and probably a significant drop in the model’s performance. Thus, practitioners need a mechanism to detect such abrupt shifts so that prompt human intervention could take place.\n\nSome applications in industry have online metrics that keep track of the online performance, and a significant performance drop triggers human intervention. However, in applications without online metrics or where feedback can be delayed, we need to detect distribution shift with unlabeled data alone, a problem known as OOD detection (Hendrycks & Gimpel, 2017; Lee et al., 2018). A more difficult recently proposed problem is OOD performance prediction (Chen et al., 2021; Jiang et al., 2022; Garg et al., 2022; Baek et al., 2022), i.e. predicting the model’s performance on the new distribution with unlabeled data alone, as distribution shift does not necessarily hurt the performance.\n\nLimitation. While we commit ourselves to studying the real PDS that exists in industrial applications, in this paper for privacy reasons we are limited to working on public datasets, which (despite our effort of making them realistic) are still different from real applications. We hope that in the future there could be more datasets containing real PDS in industry applications released for the better development of this field.\n\n9\n\n82848688Avg Online Perf20406080Worst Reg Perf68707274Avg Online Perf70758085Worst Reg Perf75808590Avg Online Perf6570758085Worst Reg PerfFBO01235100123510ER-FIFO-PLER-FIFO-RW-PLUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nRahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min interfered reLin, and Lucas Page-Caccia. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and trieval. R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 11849– URL http://papers.nips.cc/paper/ 11860. Curran Associates, 9357-online-continual-learning-with-maximal-interfered-retrieval. pdf.\n\nlearning with maximal\n\nOnline continual\n\nInc., 2019a.\n\nRahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11254– 11263, 2019b.\n\nRahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019c.\n\nMartin Arjovsky, L ́eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\narXiv preprint arXiv:1907.02893, 2019.\n\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. Advances in neural information processing systems, 32, 2019.\n\nChristina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. arXiv preprint arXiv:2206.13089, 2022.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019.\n\nMagdalena Biesialska, Katarzyna Biesialska, and Marta R. Costa-juss`a. Continual lifelong learning in natural language processing: A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6523–6541, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.574. URL https://aclanthology.org/2020.coling-main.574.\n\nGilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. Journal of Machine Learning Research, 22(2):1–55, 2021. URL http://jmlr.org/papers/v22/17-679.html.\n\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In Companion Proceedings of The 2019 World Wide Web Conference, 2019.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-\n\nversity press, 2004.\n\nJonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning? In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 872–881. PMLR, 09–15 Jun 2019.\n\nZhipeng Cai, Ozan Sener, and Vladlen Koltun. Online continual learning with natural distribution shifts: An empirical study with visual data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8281–8290, 2021.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912–9924, 2020.\n\nArslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-GEM. In International Conference on Learning Representations, 2019a. URL https://openreview.net/forum?id=Hkf2_sC5FX.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019b.\n\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu, Yingyu Liang, and Somesh Jha. Detecting errors and estimating accuracy on unlabeled data with self-training ensembles. Advances in Neural Information Processing Systems, 34:14980–14992, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020.\n\nGordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world.\n\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018.\n\nAristotelis Chrysakis and Marie-Francine Moens. Online continual learning from imbalanced data.\n\nIn International Conference on Machine Learning, pp. 1952–1961. PMLR, 2020.\n\nMatthias De Lange and Tinne Tuytelaars. Continual prototype evolution: Learning online from nonstationary data streams. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 8250–8259, October 2021.\n\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021.\n\nCyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. Episodic memory in lifelong language learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́eBuc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/ paper/2019/file/f8d2e80c1458ea2501f98a2cafadb397-Paper.pdf.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nCarl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.\n\nYuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and Chongjun Wang. Adarnn: Adaptive learning and forecasting of time series. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 402–411, 2021.\n\nWenying Duan, Xiaoxi He, Lu Zhou, Zimu Zhou, Lothar Thiele, and Hong Rao. Hyper attention recurrent neural network: Tackling temporal covariate shift in time series analysis. arXiv preprint arXiv:2202.10808, 2022.\n\nJohn Duchi and Hongseok Namkoong. Learning models with uniform performance via distribution-\n\nally robust optimization. arXiv preprint arXiv:1810.08750, 2018.\n\nEnrico Fini, Victor G Turrisi da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-supervised models are continual learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9621–9630, 2022.\n\nJean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad Javad Darvishi Bayazi, G. Dumas, and Irina Rish. Woods: Benchmarks for out-of-distribution generalization in time series tasks. ArXiv, abs/2203.09978, 2022.\n\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ̧ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, 2016. URL http://jmlr. org/papers/v17/15-239.html.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSaurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lipton, Behnam Neyshabur, and Hanie Sedghi. Leveraging unlabeled data to predict out-of-distribution performance. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=o_HsiMPYh_x.\n\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=S1v4N2l0-.\n\nArthur Gretton, Dougal Sutherland,\n\nparison of distributions ing Systems [Tutorial], interpretable-comparison-of-distributions-and-models.\n\nand models. 2019.\n\nand Wittawat\n\ncomInformation ProcessURL https://slideslive.com/38923184/\n\nInterpretable\n\nin Neural\n\nJitkrittum.\n\nAdvances\n\nJean-Bastien Grill, Florian Strub, Florent Altch ́e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020.\n\nIshaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=lQdXeXDoWtI.\n\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness withIn Jennifer Dy and Andreas Krause (eds.), out demographics in repeated loss minimization. International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1929–1938, Stockholmsm ̈assan, Stockholm Sweden, 10–15 Jul 2018. PMLR.\n\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on\n\nunsupervised visual representation learning. computer vision and pattern recognition, pp. 9729–9738, 2020.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Hkg4TI9xl.\n\nWeihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learning, pp. 2029– 2037. PMLR, 2018.\n\nXiaolei Huang and Michael J. Paul. Neural temporality adaptation for document classification: Diachronic word embeddings and domain adaptation models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4113–4123, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1403. URL https: //aclanthology.org/P19-1403.\n\nDavid Isele and Akansel Cosgun. Selective experience replay for lifelong learning. In Proceedings\n\nof the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nArthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nKokil Jaidka, Niyati Chhaya, and Lyle Ungar. Diachronic degradation of language models: InIn Proceedings of the 56th Annual Meeting of the Association for sights from social media. Computational Linguistics (Volume 2: Short Papers), pp. 195–200, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2032. URL https: //aclanthology.org/P18-2032.\n\nYiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J Zico Kolter. Assessing generalization of SGD via disagreement. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=WvOGCEAQhxl.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nXisen Jin, Arka Sadhu, Junyi Du, and Xiang Ren. Gradient-based editing of memory examples for online task-free continual learning. Advances in Neural Information Processing Systems, 34: 29193–29205, 2021.\n\nRonald Kemker and Christopher Kanan. Fearnet: Brain-inspired model for incremental learning. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum?id=SJ1Xmf-Rb.\n\nChris Dongjoo Kim, Jinseo Jeong, and Gunhee Kim.\n\nImbalanced continual learning with partitioning reservoir sampling. CoRR, abs/2009.03632, 2020. URL https://arxiv.org/abs/ 2009.03632.\n\nTaesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=cGDAkQo1C0p.\n\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pp. 5637–5664. PMLR, 2021.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n\n2009.\n\nAnanya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5468–5479. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/ kumar20c.html.\n\nPreethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fairness without demographics through adversarially reweighted learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 728–740. Curran Associates, Inc., 2020.\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum? id=BJ6oOfqge.\n\nYann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.\n\nlecun.com/exdb/mnist/.\n\nDong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep\n\nneural networks. 2013.\n\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=ryiAv2xAZ.\n\nZhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis\n\nand machine intelligence, 40(12):2935–2947, 2017.\n\nBill Yuchen Lin, Sida Wang, Xi Lin, Robin Jia, Lin Xiao, Xiang Ren, and Scott Yih. On continual model refinement in out-of-distribution data streams. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3128–3139, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. acl-long.223. URL https://aclanthology.org/2022.acl-long.223.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nZhiqiu Lin, Jia Shi, Deepak Pathak, and Deva Ramanan. The clear benchmark: Continual learning In Thirty-fifth Conference on Neural Information Processing Systems\n\non real-world imagery. Datasets and Benchmarks Track, 2021.\n\nAnjin Liu, Guangquan Zhang, and Jie Lu. Fuzzy time windowing for gradual concept drift adaptation. In 2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), pp. 1–6. IEEE, 2017.\n\nBing Liu. Learning on the job: Online lifelong and continual learning. Proceedings of the AAAI Conference on Artificial Intelligence, 34(09):13544–13549, Apr. 2020. doi: 10.1609/aaai.v34i09. 7079. URL https://ojs.aaai.org/index.php/AAAI/article/view/7079.\n\nVincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta, Gabriele Graffieti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin, Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide Bacciu, and Davide Maltoni. Avalanche: an end-to-end library for continual learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2nd Continual Learning in Computer Vision Workshop, 2021.\n\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. Advances in neural information processing systems, 29, 2016.\n\nDavid Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning.\n\nAdvances in neural information processing systems, 30, 2017.\n\nJie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept IEEE Transactions on Knowledge and Data Engineering, 31(12):2346–2363,\n\ndrift: A review. 2018.\n\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah Smith. Time waits for no one! analysis and challenges of temporal misalignment. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5944–5958, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.naacl-main. 435.\n\nDivyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Representational continuity for unsupervised continual learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Hrka5PA7LW.\n\nArun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\n\nNiki Martinel, Abir Das, Christian Micheloni, and Amit K Roy-Chowdhury. Temporal model adapIn European conference on computer vision, pp. 858–877.\n\ntation for person re-identification. Springer, 2016.\n\nMarc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-incremental learning: survey and performance evaluation on image classification. arXiv preprint arXiv:2010.15277, 2020.\n\nSaeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. Artificial Intelli-\n\ngence Review, 42(2):275–293, 2014.\n\nMichael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165. Elsevier, 1989.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nJohn P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7721–7735. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/miller21b.html.\n\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993, 2018.\n\nSaeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision, pp. 5715–5725, 2017.\n\nHyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8690–8699, June 2021.\n\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading\n\ndigits in natural images with unsupervised feature learning. 2011.\n\nJianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.\n\nYassine Ouali, C ́eline Hudelot, and Myriam Tami. An overview of deep semi-supervised learning.\n\narXiv preprint arXiv:2006.05278, 2020.\n\nAlan Ramponi and Barbara Plank. Neural unsupervised domain adaptation in NLP—A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6838– 6855, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.603. URL https://aclanthology.org/ 2020.coling-main.603.\n\nRoger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and\n\nforgetting functions. Psychological review, 97(2):285, 1990.\n\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert.\n\nicarl: In Proceedings of the IEEE conference on\n\nIncremental classifier and representation learning. Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.\n\nMatthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, , and Gerald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing inIn International Conference on Learning Representations, 2019. URL https: terference. //openreview.net/forum?id=B1gTShAct7.\n\nMark B Ring. Child: A first step towards continual learning. In Learning to learn, pp. 261–292.\n\nSpringer, 1998.\n\nAnthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123–\n\n146, 1995.\n\nElan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk mini-\n\nmization. In International Conference on Learning Representations, 2021.\n\nPaul R ̈ottger and Janet Pierrehumbert. Temporal adaptation of BERT and performance on downIn Findings of the Association Insights from social media. stream document classification: for Computational Linguistics: EMNLP 2021, pp. 2400–2412, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-emnlp.206. URL https://aclanthology.org/2021.findings-emnlp. 206.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n\nShiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In International Conference on Learning Representations, 2020a.\n\nShiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overIn Hal Daum ́e III and Aarti Singh (eds.), parameterization exacerbates spurious correlations. Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8346–8356. PMLR, 13–18 Jul 2020b.\n\nShiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang. Extending the WILDS benchmark for unsupervised adaptation. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=z7p2V6KROOV.\n\nJoan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548–4557. PMLR, 10–15 Jul 2018. URL https://proceedings. mlr.press/v80/serra18a.html.\n\nHidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-\n\nlikelihood function. Journal of statistical planning and inference, 90(2):227–244, 2000.\n\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf.\n\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596– 608. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf.\n\nBaochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In\n\nECCV 2016 Workshops, 2016.\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 30, 2017.\n\nEric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pp. 4068–4076, 2015.\n\nGido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint\n\narXiv:1904.07734, 2019.\n\nJesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning. Machine Learn-\n\ning, 109(2):373–440, 2020.\n\nHaoxiang Wang, Bo Li, and Han Zhao. Understanding gradual domain adaptation: Improved analysis, optimal path and beyond. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 22784–22801.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nPMLR, 17–23 Jul 2022a. URL https://proceedings.mlr.press/v162/wang22n. html.\n\nJindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering, 2022b.\n\nKe Alexander Wang, Niladri Shekhar Chatterji, Saminul Haque, and Tatsunori Hashimoto. Is importance weighting incompatible with interpolating classifiers? In International Conference on Learning Representations, 2022c.\n\nMei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:\n\n135–153, 2018.\n\nZhenyi Wang, Li Shen, Le Fang, Qiuling Suo, Tiehang Duan, and Mingchen Gao.\n\nImproving task-free continual learning by distributionally robust memory evolution. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 22985–22998. PMLR, 17–23 Jul 2022d. URL https://proceedings.mlr.press/v162/wang22v.html.\n\nWikipedia contributors.\n\nthe free encyclopedia, George floyd protests — Wikipedia, 2022a. URL https://en.wikipedia.org/w/index.php?title=George_Floyd_ protests&oldid=1099532980. [Online; accessed 27-July-2022].\n\nWikipedia contributors. Dobbs v. jackson women’s health organization — Wikipedia, the free encyclopedia, 2022b. URL https://en.wikipedia.org/w/index.php?title=Dobbs_ v._Jackson_Women%27s_Health_Organization&oldid=1100806014. [Online; accessed 27-July-2022].\n\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student In Proceedings of the IEEE/CVF Conference on Computer\n\nimproves imagenet classification. Vision and Pattern Recognition (CVPR), June 2020.\n\nDa Xu, Yuting Ye, and Chuanwei Ruan. Understanding the role of importance weighting for deep\n\nlearning. In International Conference on Learning Representations, 2021.\n\nShuliang Xu and Junhong Wang. Dynamic extreme learning machine for data stream classification.\n\nNeurocomputing, 238:433–449, 2017.\n\nSijie Yan, Yuanjun Xiong, Kaustav Kundu, Shuo Yang, Siqi Deng, Meng Wang, Wei Xia, and Stefano Soatto. Positive-congruent training: Towards regression-free model updates. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14299–14308, 2021.\n\nXiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning.\n\nCoRR, abs/2103.00550, 2021. URL https://arxiv.org/abs/2103.00550.\n\nChristopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in africa. Nature Communications, 2020.\n\nHaiyan Yin, Ping Li, et al. Mitigating forgetting in online continual learning with neuron calibration.\n\nAdvances in Neural Information Processing Systems, 34:10260–10272, 2021a.\n\nHaiyan Yin, peng yang, and Ping Li. Mitigating forgetting in online continual learning with neuron calibration. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 10260–10272. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper/2021/ file/54ee290e80589a2a1225c338a71839f5-Paper.pdf.\n\nJaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically In International Conference on Learning Representations, 2018. URL\n\nexpandable networks. https://openreview.net/forum?id=Sk7KsfW0-.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nJaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang. Online coreset selection for In International Conference on Learning Representations,\n\nrehearsal-based continual learning. 2022. URL https://openreview.net/forum?id=f9D-5WNG4Nv.\n\nFriedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.\n\nIn International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.\n\nChen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry. Task agnostic continual learning using\n\nonline variational bayes. arXiv preprint arXiv:1803.10123, 2018.\n\nRuntian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. Doro: Distributional and outlier robust optimization. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12345–12355. PMLR, 18–24 Jul 2021a.\n\nRuntian Zhai, Chen Dan, Arun Suggala, J. Zico Kolter, and Pradeep Ravikumar. Boosted cvar classification. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 21860–21871. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper/2021/ file/b691334ccf10d4ab144d672f7783c8a3-Paper.pdf.\n\nRuntian Zhai, Chen Dan, Zico Kolter, and Pradeep Ravikumar. Understanding why generalized\n\nreweighting does not improve over erm. arXiv preprint arXiv:2201.12293, 2022.\n\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semiIn Proceedings of the IEEE/CVF International Conference on Computer\n\nsupervised learning. Vision, pp. 1476–1485, 2019.\n\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=r1Ddp1-Rb.\n\nAdaptive risk minimization: Learning to adapt\n\nMarvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea In M. RanFinn. zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 23664–23678. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ c705112d1ec18b97acac7e2d63973424-Paper.pdf.\n\nto domain shift.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nAppendix\n\nTable of Contents\n\nA Literature Review\n\n.\n\n. .\nA.1 Distribution Shift .\nA.2 Continual Learning . A.3 Domain Adaptation . .\nA.4 Semi-supervised Learning .\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\nB The Divergence Function\n\nB.1 Existing Divergence Functions .\nB.2 Divergence Function for OCL-PDS . B.3 ε-KL-Divergence .\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nC Benchmark Details\n\nC.1 CivilComments-WPDS . .\nC.2 FMoW-WPDS . .\n. C.3 Amazon-WPDS . .\nC.4 Poverty-WPDS .\n\n. .\n.\n\n. .\n.\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\n. .\n. .\n\nD Algorithm Details\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\n. .\n. .\n\n. .\n.\n\n. .\n. .\n\nE Experiment Details .\n.\n\nE.1 Setup . .\nE.2 Results .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n. .\n\n19 19 20 23 23\n\n24 24 25 25\n\n26 26 28 30 30\n\n30\n\n33 33 33\n\nA LITERATURE REVIEW\n\nA.1 DISTRIBUTION SHIFT\n\nDistribution shift in machine learning refers to the scenario where the model is tested on a distribution Q different from the distribution P on which it was trained, which is different from the conventional machine learning setting where the training and testing sets are i.i.d. sampled from the same distribution. Distribution shift is studied in a number of areas in machine learning, including domain adaptation, continual learning, transfer learning, fair machine learning, long-tailed learning, etc. See Table 2 of Gulrajani & Lopez-Paz (2021) and Table 2 of Wang et al. (2022b) for a comparison among these areas.\n\nIn general, there are two types of distribution shift problem: Domain shift and subpopulation shift (Koh et al., 2021; Sagawa et al., 2022). In domain shift, the training and testing distributions contain different domains, and the goal is to generalize to new domains. This problem is also called Outof-distribution (OOD) generalization, such that the training set is ID (in-distribution) and the test set is OOD. Related areas include domain adaptation, domain generalization, transfer learning, etc. In subpopulation shift, the training and testing distributions consist of the same domains, but the relative proportions are different. Related areas include fair machine learning, long-tailed learning (learning with imbalanced classes), etc. The difference between these two is that in subpopulation shift, the supports of the training and testing data distributions are the same, while it is not for domain shift.\n\nA lot of methods have been proposed to train models that are robust to distribution shift. Here we introduce two general methods, and in the subsequent sections we will talk about methods for specific areas. The most classic method is importance weighting (Shimodaira, 2000), which multiplies the loss on sample x with an importance weight Q(x) P (x) dP . This\n\nP (x) , because (cid:82) f (x)dQ = (cid:82) Q(x)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nmethod requires that P (x) > 0 for all x such that Q(x) > 0, i.e. only works for subpopulation shift. Another general method that is widely used today is Distributionally Robust Optimization (DRO) (Duchi & Namkoong, 2018), which assumes that Q ∈ U (P ) where U (P ) is the uncertainty set that contains a family of distributions that are close to P . Then, DRO trains the model on the worst distribution in U (P ) (with the highest empirical risk), so as to ensure that the model can do well on any distribution in U (P ) which includes Q. A lot of variants of DRO have been proposed and being used today (Hashimoto et al., 2018; Hu et al., 2018; Sagawa et al., 2020a; Lahoti et al., 2020; Zhai et al., 2021a;b). However, there is also a recent line of work that points out some problems with these methods both empirically and theoretically (Byrd & Lipton, 2019; Sagawa et al., 2020b; Gulrajani & Lopez-Paz, 2021; Xu et al., 2021; Wang et al., 2022c). Remarkably, a recent paper Zhai et al. (2022) proved the surprising result that generalized reweighting (GRW) methods, a broad family of methods including importance weighting, DRO and a lot more, cannot do better than ERM for linear models and NTK neural networks (Jacot et al., 2018). Thus, there is still a huge room of improvement for methods for distribution shift.\n\nA.2 CONTINUAL LEARNING\n\nContinual learning (CL), also known as lifelong learning, comes from the philosophy that a learning agent should be able to continually learn new knowledge and improve itself with new data on its own. As detailed in Ring (1998), a continual learner should be able to learn context-dependent knowledge autonomously, incrementally and hierarchically. A more recent paper Liu (2020) introduced the concept of on-the-job learning, where the learner is required to detect new tasks on its own, collect data for the new tasks and then learn with the data. Given these general philosophical ideals, the goal of CL is to mathematically formulate a problem setting that reflects these ideals. Here we point our readers to the Avalanche library (Lomonaco et al., 2021), a recently released Python library that contains many benchmarks and algorithms of continual learning.\n\nA.2.1 SETTINGS\n\nThere are a bunch of continual learning settings, and here we discuss the most widely studied ones.\n\nIn this setting, N tasks T1, · · · , TN are sequentially given to the learner, Task-incremental CL. who is required to learn these tasks one by one without forgetting the previous tasks. The learner cannot see the old tasks while learning a new one, but it has a memory buffer in which it can store data from previous tasks. The goal is to perform well on all N tasks, and the performance is usually evaluated with the average or the minimum of the performances on all tasks. A related setting is class-incremental CL where new classes sequentially appear. Task-incremental CL is the oldest setting that dates back to McCloskey & Cohen (1989), which trained a feed-forward network to first learn addition with one and then learn addition with two, and found that when learning the second task the network forgot the first one. This work pinpointed the catastrophic forgetting problem, and as a result many researchers today believe that “the central problem of continual learning is to overcome the catastrophic forgetting problem” (Aljundi et al., 2019c).\n\nTwo related problems are domain-incremental CL and class-incremental CL. In domain-incremental CL, the data of each task comes from a different domain. In class-incremental CL, samples from new classes appear one by one.\n\nTask-aware CL. In this setting, the tasks are not sequentially given. Rather, each sample has a “task descriptor” indicating which task it belongs to. This setting dates back to the early paper Ratcliff (1990) which trained a multi-layer encoder model to learn four vectors A, B, C and D, which were provided in a cyclic fashion: ABCDABCDABCD... Then, Lopez-Paz & Ranzato (2017) studied task-aware CL in the online learning setting where samples come in an online stream. The goal of task-aware CL is the same as task-incremental CL: To perform well on all N tasks.\n\nTask-agnostic CL. In task-agnostic CL, each sample still belongs to a certain task, but the “task descriptor” is not provided. The model is still required to do well on all N tasks, and it is still evaluated by the average or the minimum of the performances on all tasks. Algorithms for this setting usually do task inference, i.e. inferring the task from the input (Van de Ven & Tolias, 2019), where they assume that samples from the same task are closer to each other. Note that our definition\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nis different from some previous work, and some previous work on task-agnostic CL like Zeno et al. (2018) is in fact under the task-free setting.\n\nTask-free CL. In task-free CL, there is no task at all. The data comes in an online stream and each sample only appears once. This setting was first introduced in Aljundi et al. (2019b), where they split a data domain into several partitions, and sequentially present each partition to the model. At the end of training, the model is evaluated on the entire data domain, i.e. the union of all the partitions. Thus, if the learner is allowed to store all samples and train the model on them, then the problem becomes equivalent to supervised learning. However, the model cannot store all samples because the storage size is assumed to be limited. This problem is also called the data incremental learning problem in De Lange et al. (2021). We can see that in this problem, the training is online (partitions are sequentially given), but the evaluation is offline (test once on the entire domain).\n\nOnline Continual Learning (OCL). We study OCL in this work. It is a variant of task-free CL as there is no task in OCL. The key characteristic of OCL is that both training and evaluation are online, which is different from the previous task-free CL problem. In OCL, for each new data batch, the model is first evaluated on it and then trained on it. Thanks to the online evaluation, we can study the real “lifelong” learning setting in OCL where the time horizon is infinite, which is not possible in the previous task-free CL problem. Note that the term “online continual learning” was used in quite a few previous papers, but most of them refer to “continual learning with online data” (such as Yin et al. (2021a)) which is not the OCL setting we define in this work.\n\nTime series analysis. A related area is time series analysis, where the data comes from a nonstationary online distribution, and the task is to predict future data with the current and past observations. We can see that distribution shift naturally exists in the formulation of time series analysis, but we fail to find much work about dealing with distribution shift with time series analysis.\n\nOne setting in time series analysis that is similar to OCL is temporal covariate shift (TCS), which was first introduced in Du et al. (2021). TCS makes the covariate shift assumption: P (Y |X) is fixed while P (X) shifts with time. The goal of TCS is r-step ahead prediction, i.e. predicting the labels for inputs whose ground truth labels will be revealed r steps later. Other recent papers that study distribution shift with time series analysis include Duan et al. (2022); Gagnon-Audet et al. (2022); Kim et al. (2022).\n\nComparison with a previous work. One previous work Cai et al. (2021) proposed an OCL framework similar to ours. In both settings, the model is evaluated on new online batches before fine-tuned on them. The differences between our work and Cai et al. (2021) are the following:\n\n1. Cai et al. (2021) considered a fully supervised setting: The environment reveals all true labels to the learner after evaluation. In contrast, in OCL-PDS, we use the random label feedback which randomly selects α fraction of the new samples and provides their labels. In particular, we mainly study a semi-supervised learning setting where the environment only reveals a fraction of the labels, which is a more common scenario in industrial applications.\n\n2. The evaluation metrics used in Cai et al. (2021) were: Average online accuracy, backward transfer and forward transfer. The first two metrics correspond to the average online performance and the average recent knowledge retention in OCL-PDS. In addition to these metrics, OCL-PDS also considers the regression set performance which is a very important metric in real applications, the worst (recent/important) knowledge retention performances since knowledge retention is required for all t, and the training efficiency which is very important in an online setting where the algorithm is run for many times.\n\n3. Cai et al. (2021) only evaluates knowledge retention performances at three fixed time steps: H/3, 2H/3 and H, where H is the total number of time steps. In contrast, OCL-PDS is purely online: all metrics including knowledge retention are evaluated at each step.\n\n4. Cai et al. (2021) proposed the CLOC benchmark which is indeed a PDS benchmark. However, CLOC only covers vision classification, while our benchmarks cover both language and vision tasks, and both classification and regression tasks.\n\n5. Cai et al. (2021) only run experience replay (ER) on their benchmark, while our work also compares regularization based methods such as EWC, variants of ER such as GEM,\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nMIR and MaxLoss, as well as semi-supervised learning methods like pseudo-labeling and FixMatch.\n\nA.2.2 METHODS\n\nRehearsal based methods. The concept of “rehearsal” was first introduced in Ratcliff (1990); Robins (1995). Rehearsal-based methods store samples in an operational memory (a memory buffer). When the learner learns a new task, it also reviews the old samples in this buffer in order to prevent catastrophic forgetting, known as experience replay. Almost all previous work assumed that the buffer has a fixed size, so two key questions for these methods are: (i) Which samples to be store in the buffer, and (ii) Which samples to be replayed to the learner.\n\nFor problem (i), the most widely used strategy is Reservoir sampling (Isele & Cosgun, 2018) which maintains the buffer distribution to be the average of all task distributions. Other variants include Chrysakis & Moens (2020); Kim et al. (2020). For problem (ii), MIR (Aljundi et al., 2019a) selects the samples with the highest loss increase after a “virtual update” step to be replayed, MaxLoss (Lin et al., 2022) selects the samples with the highest loss after virtual update, and OCS (Yoon et al., 2022) selects an online coreset that is diverse and has a high affinity to previous tasks. Moreover, experience replay (ER) can be combined with other methods. For example, MER (Riemer et al., 2019) combines ER with meta learning, GMED (Jin et al., 2021) combines ER with adversarial attack, and Wang et al. (2022d) combines ER with DRO.\n\nThere are alternative ways to use this buffer. For example, iCaRL (Rebuffi et al., 2017) stores In other “exemplars” in the buffer and uses a nearest neighbor classifier with these exemplars. words, samples in the buffer are not used for training, but used for inference. Similarly, Continual Prototype Evolution (De Lange & Tuytelaars, 2021) maintains and continually updates a prototype for each class, and uses a nearest neighbor classifier for inference. Another way is introduced in GEM (Lopez-Paz & Ranzato, 2017), where the buffered samples are not directly replayed for training, but instead used to find a “pseudo gradient” with a convex optimization problem so that the loss on previous tasks won’t increase. Variants of GEM include A-GEM (Chaudhry et al., 2019a) and GSS (Aljundi et al., 2019c).\n\nSince the buffer size is assumed to be limited, a bunch of previous papers studied how to utilize the buffer space more efficiently. One line of work proposes to learn the distributions of previous tasks with a generative model, which includes DGR (Shin et al., 2017), FearNet (Kemker & Kanan, 2018) and so on.\n\nRegularization based methods. The high-level idea of these methods is to not change the model weights too much so that the model’s performance on previous tasks won’t drop too much. One type of methods directly add to the objective a penalty term which keeps the model weights close to the old weights, such as EWC (Kirkpatrick et al., 2017). Another type of methods use synaptic regularization (Zenke et al., 2017; Aljundi et al., 2019b) which controls the learning rate of each weight, so that more influential weights change more slowly.\n\nArchitecture based methods. These methods continually update the model architecture in order to learn the new tasks. One type of method changes the architecture by adding a “mask” to some weights, such as Piggyback (Mallya et al., 2018) which learns a mask for each task, learning a hard attention mask with gradient descent (Serra et al., 2018), and NCCL that adds weight calibration modules and feature calibration modules to balance between stability and plasticity (Yin et al., 2021b). Another type of method uses the isolation approach where the model has two parts - A shared part and a task specific part. When a new task arrives, the shared part is updated very little, and a new task specific part is learned on the new task. Examples of isolation methods include progressive neural network (PNN) (Rusu et al., 2016), Learning without Forgetting (LwF) (Li & Hoiem, 2017), dynamically expandable network (DEN) (Yoon et al., 2018), etc.\n\nSemi-supervised/Unsupervised continual learning. There are also some existing continual learning methods that work under a semi-supervised or unsupervised setting. For instance, Lump (Madaan et al., 2022) uses Mixup to interpolate between the current task and previous tasks’ instances to alleviate catastrophic forgetting, and Fini et al. (2022) combines continual learning with self-supervised learning.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 DOMAIN ADAPTATION\n\nDomain adaptation (DA) is a type of distribution shift where “the tasks are the same, and the differences are only caused by domain divergence” (Wang & Deng, 2018). There is a source distribution P and a target distribution Q with distribution shift from P to Q. During training, the learner is provided with samples from both the source and the target distributions, and conditioning on whether the samples from the target distribution is labeled, partially labeled or unlabeled, DA is classified as supervised, semi-supervised and unsupervised DA. Particularly, in supervised DA, the number of target samples is usually very small so that training on these target samples alone cannot lead to a good model. A related area is domain generalization (Gulrajani & Lopez-Paz, 2021; Blanchard et al., 2021) where the learner does not have any samples from the target domain, even unlabeled ones.\n\nIn general, a DA algorithm consists of two parts: Feature alignment and class alignment. The goal of feature alignment is to train a feature encoder Φ that can encode invariant features, i.e. the images of the source and target domains in the feature space are close to each other, or Φ(P ) ≈ Φ(Q). As summarized in Wang & Deng (2018), there are two common ways to achieve feature alignment. The first one is adversarial-based, i.e. training a domain discriminator to distinguish features from the source and target domains, and the features are aligned if this discriminator cannot achieve a high performance. Methods of this type include DANN (Ganin et al., 2016), SagNet (Nam et al., 2021), etc. The second one is discrepancy-based, i.e. minimizing a divergence function between Φ(P ) and Φ(Q), also called the “confusion alignment loss” in Motiian et al. (2017). Methods of this type include CORAL (Sun & Saenko, 2016), IRM (Arjovsky et al., 2019), ARM (Zhang et al., 2021), etc. However, there are also some papers that point out the problems within these methods (Rosenfeld et al., 2021; Gulrajani & Lopez-Paz, 2021).\n\nMoreover, even if we have learned a feature encoder Φ such that Φ(P ) ≈ Φ(Q), we cannot be sure that the same classifier works for both domains, because samples of different classes in P and Q might be mapped to the same latent feature. Thus, the goal of class alignment is either to make sure that samples of the same class are mapped together, or to train a new classifier w′ which works for Φ(Q). Note that class alignment requires labels from the target domain which are unavailable in unsupervised domain adaptation or domain generalization. For instance, Tzeng et al. (2015) used soft labels for class alignment, Long et al. (2016) minimized the cross entropy on the target data while using a residual block to keep the source and target classifiers close, and Motiian et al. (2017) used a similarity penalty between samples from different classes.\n\nA.4 SEMI-SUPERVISED LEARNING\n\nIn many applications, the number of labeled samples are limited, but there are also a large number of unlabeled samples. For example, for image classification tasks, while the labels are hard to obtain, free images can be very easily retrieved from the internet. Semi-supervised learning studies how to train a model on a small set of labeled samples and a large set of unlabeled samples. This is an old area of research and there is a very rich body of work, on which we do not intend to make an exhaustive survey here. Here we briefly discuss several types of methods that are widely being used today, and more methods can be found in surveys such as Van Engelen & Hoos (2020).\n\nGenerating labels for unlabeled samples. Perhaps the most direct and intuitive way of leveraging the unlabeled samples is to try to generate labels for them, and then train the model over all samples as if they were all labeled. The simplest of such methods is pseudo-labeling (Lee, 2013), which first trains a model over the labeled samples alone and then uses this model to pseudo-label the unlabeled samples. This method assumes that the labeled and unlabeled samples come from the same underlying distribution, so if a model can do well on the labeled samples, it should be able to generate good pseudo labels for the unlabeled samples. However, the quality of pseudo labels depends on the generalization ability of the model, and if the number of labeled samples is too small, then the pseudo labels could contain large systematic label noise. Thus, a number of techniques have been proposed to improve pseudo-labeling.\n\nOne such technique is consistency regularization, which is based on the following observation: Given two transformations x′ and x′′ of the same sample x, their labels should be the same. For instance, FixMatch (Sohn et al., 2020) uses two augmentation methods: a weak one and a strong\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\none. It generates pseudo labels on the weakly augmented sample x′, and trains the model on the strongly augmented one x′′. Similarly, Noisy Student (Xie et al., 2020) also uses a weak and a strong augmentation, but it alternates between teacher phases which generate pseudo labels and student phases which learn these labels, until convergence. Some other work defines x′ and x′′ as the outputs of the model at different epochs, such as Temporal Ensembling (Laine & Aila, 2017) and Mean Teachers (Tarvainen & Valpola, 2017). There is also a line of work that leverages adversarial attack, such as Virtual Adversarial Training (VAT) (Miyato et al., 2018).\n\nInterpolation based methods. These methods train the model on the interpolation between labeled and unlabeled samples. This type of methods was initially introduced in MixUp (Zhang et al., 2018), which interpolates between two labeled samples to combat label noise and adversarial attack as it makes the model have linear behavior in between samples. This method is then applied to semi-supervised learning in MixMatch (Berthelot et al., 2019), which combines MixUp with a lot of other techniques including pseudo labeling.\n\nSelf-supervised learning. The high-level idea of self-supervised learning (also known as representation learning) is to train a good feature extractor on the unlabeled data set with an auxiliary task (upstream task), and then train a classifier on top of it on the labeled data set (downstream task). The most famous and widely-used self-supervised learning technique is masked language modeling (MLM) in NLP (Devlin et al., 2018), where the auxiliary task is predicting a masked word within a sentence. MLM has achieved great success in NLP as the feature extractor it learns can be applied to almost any language task and lead to good performance.\n\nInspired by the success of MLM, people also try to apply self-supervised learning to vision tasks. For example, Doersch et al. (2015) extracts random pairs of patches from each image where the auxiliary task is to learn the relative position between each pair of patches, and Gidaris et al. (2018) rotates each image with different angles where the auxiliary task is to learn this rotation angle, which is applied to semi-supervised learning in S4L (Zhai et al., 2019). Today, the most widely used technique is contrastive learning, which extracts different views from each image, and the auxiliary task is to learn which views come from the same image. The feature extractor is trained to learn the similarity between two views: similar if they come from the same image, and different if they do not. This idea was first introduced in Bachman et al. (2019). Currently the most popular contrastive learning methods include SimCLR (Chen et al., 2020), MoCo (He et al., 2020), BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), etc.\n\nB THE DIVERGENCE FUNCTION\n\nIn this section, we dive deep into one important problem in the formulation of the OCL-PDS problem: How to choose the divergence function Div(P ∥ Q) that guarantees the continuity of the distribution shift? As mentioned in Section 2.2, Div(P ∥ Q) refers to the divergence from P to Q, and ideally we want it to reflect the performance of a model which is trained on P and tested on Q.\n\nTo study this problem, first we will review existing divergence functions, and then we will show that an ideal divergence function for OCL-PDS should be asymmetric and bounded, which is unfortunately not satisfied by any popular divergence function. Finally, we will introduce the ε-KLdivergence which is used in this work.\n\nB.1 EXISTING DIVERGENCE FUNCTIONS\n\nThis part is based on the NeurIPS tutorial by Gretton et al. (2019). Generally speaking, there are two types of existing divergence functions: Integral probability metrics (IPMs) and φ-divergences. To quickly understand these two types of divergence, think about how to determine whether two distributions P and Q are equal. There are two ways in general: (a) Compute P − Q and see if it is zero almost everywhere, and (b) Compute P/Q and see if it is one almost everywhere. IPMs correspond to method (a) and φ-divergences correspond to method (b).\n\nIPMs. An IPM is defined as Div(P ∥ Q) = supf ∈F [EX∼P f (X) − EY ∼Qf (Y )] for some func- (cid:82) |P (x) − tion family F. Examples include total variation (TV) defined as Div(P ∥ Q) = 1\n\n2\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nQ(x)|dx, MMD defined as Div(P ∥ Q) = ∥EX∼P [π(X)] − EY ∼Q[π(Y )]∥H for some feature mapping π and reproducing kernel Hilbert space H, and the Wasserstein distance defined as Div(P ∥ Q) = inf γ∈Γ(P,Q)\n\n(cid:82) D(x, y)dγ(x, y) for some distance function D(·, ·).\n\nφ-divergences. A φ-divergence is defined as Divφ(P ∥ Q) = (cid:82) φ φ = − log, then the φ-divergence becomes the reverse KL-divergence, where the popular KLdivergence is defined as DKL(Q ∥ P ) = (cid:82) x Q(x) log Total variation is the only non-trivial function that is both an IPM and a φ-divergence.\n\ndx (note that P and Q are reversed).\n\ndQ. For example, when\n\n(cid:16) Q(x) P (x)\n\ndQ\n\n(cid:17)\n\n(cid:16) dP\n\n(cid:17)\n\nB.2 DIVERGENCE FUNCTION FOR OCL-PDS\n\nIn this part, we show that an ideal divergence function for the OCL-PDS problem should be asymmetric and bounded.\n\nAsymmetric. Suppose we have two very different data domains A and B. Let P = A and Q = 0.5A + 0.5B. A model trained on P would have a very poor performance on Q, because it has never seen any samples from domain B. On the other hand, a model trained on Q could have a good performance on P , because it has seen samples from both A and B. Thus, in this example, we would like to have Div(P ∥ Q) > Div(Q ∥ P ), so Div should be asymmetric.\n\nBounded. The KL-divergence is widely used in machine learning literature, but one problem is that it is unbounded. Recall that in the OCL-PDS problem formulation, we assume that Div(Dt ∥ Dt+1) < ρ for all t. Now consider what would happen if the function Div is unbounded, and we want to introduce data from new domains into the problem. Specifically, Q contains samples from new domains that are not in P , i.e. there exists x such that Q(x) > 0 and P (x) = 0. In this situation, we must have Div(P ∥ Q) = ∞, no matter how small Q(x) is. Therefore, if Div is unbounded like the reverse KL-divergence, then we could never introduce new domains into the problem, which is not desirable. There is a variant of the KL-divergence called JS-divergence, which is defined as Div(P ∥ Q) = 1 2 Q. Although it is bounded, it is also symmetric, so it is not ideal for OCL-PDS.\n\n2 [DKL(P ∥ M ) + DKL(Q ∥ M )] where M = 1\n\n2 P + 1\n\nB.3\n\nε-KL-DIVERGENCE\n\nIn this work, we use the ε-KL-divergence defined as follows:\n\nDiv(P ∥ Q) = EX∼P [g(X)] + EY ∼Q[log(−g(Y ))] + 1\n\nwhere\n\ng(x) = −\n\nQ(x) max{P (x), ε}\n\n(2)\n\nThis divergence functions has the following properties:\n\n(i) This function is a lower bound of the reverse KL-divergence, and it is bounded.\n\n(ii) If g(x) = − Q(x)\n\nP (x) , then this function is equivalent to the reverse KL-divergence (which is its dual formulation). Thus, if for any x such that Q(x) > 0, we have P (x) ≥ ε, then the ε-KL-divergence is equivalent to the reverse KL-divergence.\n\n(iii) In the case where Q contains new domains that are not in P , for example Q = (1 − β)P + β ̃P ε + (1 − β) log(1 − β) ≈ β + DKL(Q ∥\n\nfor some ̃P ⊥ P , then Div(P ∥ Q) = β + β log β (1 − ε)P + ε ̃P ).\n\nFrom the properties, we can see that what we are doing in the ε-KL-divergence is essentially adding an ε lower bound to the denominator of g(x) so that the function becomes bounded.\n\nExample. Given two groups A and B, let ρ = 0.17 and ε = 0.02. Then, we have the following group weight allocation schedule which is widely used in our benchmarks:\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Sample group weight allocation schedule.\n\nt A\n\n0 1\n2 3\n4 5\n\n1.00 0.90 0.69 0.41 0.15 0.00\n\nB\n\n0.00 0.10 0.31 0.59 0.85 1.00\n\nDiv(Dt ∥ Dt+1)\n\n0.1661 0.1674 0.1663 0.1595 0.1625\n\nLimitations. The major limitation of the ε-KL-divergence is that it cannot measure how similar two domains are. For example, suppose we have three domains: A, B and C, and they do not overlap with one another. Samples in A and B are very similar, but they are vastly different from samples in C. In this case, a model trained on A can have a good performance on 0.5A + 0.5B, but a poor performance on 0.5A + 0.5C. However, for the ε-KL-divergence, we have Div(A ∥ 0.5A + 0.5B) = Div(A ∥ 0.5A + 0.5C). We can see that the ε-KL-divergence cannot measure the similarity between A and B or A and C. Another limitation is that the choice of ε is arbitrary and can affect the function value. Nevertheless, even with these two limitations, we still believe that the ε-KL-divergence is suitable for the OCL-PDS problem. Finally, keep in mind that no divergence function can cover every facet of real problems in practice, and that’s why we have three important checking steps in our benchmarking procedure described in Section 3.1.\n\nC BENCHMARK DETAILS\n\nWe build 4 new benchmarks for OCL-PDS following the guidelines listed below:\n\n• We only use public datasets in this work.\n\n• The benchmarks should cover a wide variety of tasks.\n\n• The benchmarks should be realistic and can reflect the real PDS in industry.\n\n• Na ̈ıve methods should be poor on these benchmarks, so special methods are necessary.\n\nIn this section, we will first demonstrate in detail how to use the 3-step procedure described in Section 3.1 with the CivilComments-WPDS benchmark as an example. Then, we will present the details for all other datasets, but without the detailed benchmarking procedure.\n\nC.1 CIVILCOMMENTS-WPDS\n\nHere we present how we construct CivilComments-WPDS from the CivilComments-Wilds dataset with the 3-step procedure including the 3 important checking steps.\n\nStep 1: Separate the data into groups. First, we investigate the metadata we have in this dataset. In CivilComments-Wilds, apart from the target label, each sample also has the following annotations: whether it contains a certain topic (male, female, LGBTQ, christian, muslim, other religions, black, white), and whether it contains a certain type of toxicity (identity attack, explicit sexual, etc.). Based on this metadata, we can model the distribution shift with the shift in hot topics. We separate the samples into four groups according to their topics, as shown in the following table.\n\nTable 3: Samples in CivilComments-Wilds are separated into 4 groups by their topics.\n\nGroup Topic\n\n# Samples\n\n# Toxic\n\n# Non-toxic\n\n0 1\n2 3\n\nNormal Race Religion Gender\n\n269,422 37,459 65,816 75,303\n\n21,297 10,820 8,106 10,571\n\n248,125 26,639 57,710 64,732\n\nWe can observe that: (i) There are less toxicity in normal comments than comments about a specific topic; (ii) The portion of toxic comments about race is much higher than that of other topics.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nThen, we do the OOD check, where we verify that there is a distribution shift across the groups. We perform this check in the following way: For each Group k, we train a model on a training set sampled from all groups except Group k, and then test this model on a validation set sampled from all groups except Group k, and a test sampled from Group k. The results are the following:\n\nTable 4: OOD check results of CivilComments-WPDS (%).\n\nGroup k Topic\n\nValidation Accuracy Test Accuracy OOD Gap: Val - Test\n\n0 1\n2 3\n\nNormal Race Religion Gender\n\n87.14 92.90 92.11 92.20\n\n94.52 78.86 89.88 89.80\n\n-7.38 14.04 2.23 2.40\n\nFrom this table, we can see that Group 1 (race) has the largest OOD performance gap, much larger than other groups. Thus, when allocating the groups in Step 2, we will make sure that the shift to Group 1 is slower. Moreover, we observe that on Group 0, the gap is negative, which means that the OOD performance is better than the ID performance (i.e. A model trained on Groups 1-3 has a higher accuracy on Group 0 than Groups 1-3). This is a counter-intuitive phenomenon as it is usually taken for granted that OOD performance should be lower than ID performance. The cause of this phenomenon might be that Group 0 is much easier to learn than the other groups (for example, the data is more concentrated and depends on fewer features).\n\nStep 2: Assign shifting group weights to the batches. We want to model the distribution shift with the shift in hot topics on social media. At each time t, Dt contains normal comments as well as comments about the current hot topic, i.e. samples from Group 0 exists in every batch. We design the schedule listed in the following table:\n\nTable 5: Group weight schedule for CivilComments-WPDS. For each t, the weights add up to 1.\n\nt Group 0 Group 1 Group 2 Group 3\n\n0 1\n2 3\n4 5\n6 7\n8\n\n1.00 0.90 0.75 0.60 0.40 0.40 0.40 0.40 0.40\n\n0.00 0.10 0.25 0.40 0.60 0.30 0.00 0.00 0.00\n\n0.00 0.00 0.00 0.00 0.00 0.30 0.60 0.30 0.00\n\n0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.30 0.60\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nThen, we do the shift continuity check, whose point is to make sure that the distribution shift in this schedule is continuous. This check goes as follows: For each t, we sample a set from Dt and split it into a training set and a validation set, and then sample a test set from Dt+1. We want to make sure that the gap between the validation and the test performances is not too large for each t. The results are the following:\n\nTable 6: Shift continuity check results of CivilComments-WPDS (%).\n\nt Validation Accuracy Test Accuracy OOD Gap: Val - Test\n\n0 1\n2 3\n4 5\n6 7\n\n94.87 93.49 90.67 87.56 85.60 89.08 91.63 92.05\n\n92.99 91.31 88.10 83.59 89.18 91.59 91.69 91.47\n\n1.88 2.18 2.57 3.97 -3.58 -2.51 -0.06 0.58\n\nWe can see that the OOD gaps are kept under 4%, as opposed to the 14.04% gap we got in the OOD check. And we can observe the same phenomenon again: For some t, the OOD accuracy is higher than the ID occuracy.\n\nThen, based on these results, we design the following sample allocation schedule listed in Table 8. The first batch contains 50000 samples, while all the other batches contain 10000 samples each.\n\nStep 3: Design a separate regression set. We notice that each toxic comment is annotate which types of toxicity the comment contains in the CivilComments-Wilds dataset. Thus, we design the regression set as the set of comments with two types of severe toxicity: identity attack and explicit sexual. Such comments are critical toxic comments that a good detector should be able to detect with high success rate.\n\nThen, we do the regression check, where we verify that na ̈ıve methods have regression on the regression set we constructed. Here we use the NBO baseline described in Section 4.1 as the “na ̈ıve method”. As shown in Figure 6, the regression set performance of NBO quickly declines to under 50% - The performance of random guessing. Thus, the benchmarks passes this check.\n\nFigure 6: Regression check.\n\nTrain/test split. For each t, we split St into a training set and a test set, and the regression set is also split into a training set and a test set. These test sets are used to evaluate the recent and regression set performances. We never evaluate the learner on data it has seen, because the learner has an infinitely large buffer where it can store the data. Instead, we evaluate the learner on separate i.i.d. test sets. Note that the numbers listed in Table 8 are all sizes of the training sets.\n\nC.2 FMOW-WPDS\n\nThis benchmark is built from FMoW-Wilds. First, we separate the data into 5 groups by the year:\n\nTable 7: Samples in FMoW-WPDS are clustered into 5 groups by their year.\n\nGroup Year\n\n# Samples\n\n0 1\n2 3\n4\n\n2002-2013 2014 2015 2016 2017\n\n132,948 54,575 87,358 140,459 54,746\n\n28\n\n051015t20406080100Reg PerformanceUnder review as a conference paper at ICLR 2023\n\nTable 8: Sample allocation schedule for CivilComments-WPDS.\n\nt\n\n0 1\n2 3\n4 5\n6 7\n8 9\n10 11 12 13 14 15\n\nGroup 0 Group 1 Group 2 Group 3\n\n50000 9000 9000 7500 7500 6000 6000 4000 4000 4000 4000 4000 4000 4000 4000 4000\n\n0 1000 1000 2500 2500 4000 4000 6000 3000 3000 0\n0 0\n0 0\n0\n\n0 0\n0 0\n0 0\n0 0\n3000 3000 6000 6000 3000 3000 0\n0\n\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n3000 3000 6000 6000\n\nTotal\n\n131000\n\n27000\n\n24000\n\n18000\n\nTable 9: Sample allocation schedule for FMoW-WPDS.\n\nGroup 0 Group 1 Group 2 Group 3 Group 4\n\nt\n\n0 1\n2 3\n4 5\n6 7\n8 9\n10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n\n120000 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0\n\n0 10000 9000 6900 6900 4100 4100 1500 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n\n42500\n\n0 0\n1000 3100 3100 5900 5900 8500 10000 9000 6900 6900 4100 4100 1500 1500 0\n0 0\n0 0\n0 0\n0 0\n\n71500\n\n0 0\n0 0\n0 0\n0 0\n0 1000 3100 3100 5900 5900 8500 8500 10000 9000 6900 6900 4100 4100 1500 1500 0\n\n80000\n\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 1000 3100 3100 5900 5900 8500 8500 10000\n\n46000\n\nTotal\n\n120000\n\nThen, we allocate the groups with the schedule listed in Table 9. We put 120000 samples from Group 0 into batch 0 for pretraining, and all other batches contain 10000 samples each. The regression set is defined as the set of samples in two highly populated regions: Americas and Asia.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nC.3 AMAZON-WPDS\n\nThis benchmark is built from the Amazon-Wilds dataset. In the original paper (Koh et al., 2021), the ID/OOD sets are divided by the reviewers, and the dataset also contains some other metadata such as year and product categories. However, we find in our experiments that these groups cannot create a sufficiently large distribution shift. Thus, we use the following class split method, which has been widely used in the continual learning literature: We divide the 5 classes of this datasets (corresponding to the 5 stars rating) into 2 groups and 2 classes - positive and negative reviews, as shown in the following table:\n\nTable 10: In Amazon-WPDS, the 5 stars rating are divided into 2 groups and 2 classes.\n\nGroup 0 Group 1\n\n# Samples Group 0\n\nGroup 1\n\ny = 0 y = 1\n\n1, 2 stars 4 stars\n\n3 stars 5 stars\n\ny = 0 y = 1\n\n193,900 1,087,385\n\n377,039 2,343,846\n\nThen, we allocate the groups to the batches with the schedule listed in Table 12. This schedule is based on the weight schedule we obtained in Table 2. The first batch has 50000 samples from Group 1, and then we model a group shift from Group 1 to Group 0. Each subsequent batch has 5000 samples.\n\nFinally, the regression set is defined as the set of reviews from 10 popular product categories, including books, fashion, etc.\n\nC.4 POVERTY-WPDS\n\nThis benchmark is build from the PovertyMap-Wilds dataset, which is a image regression task. First, we cluster the data into 4 group by the year:\n\nTable 11: Samples in Poverty-WPDS are clustered into 4 groups by their year.\n\nGroup Year\n\n# Samples\n\n0 1\n2 3\n\n2009-2011 2012-2013 2014 2015-2016\n\n7129 5005 3494 4041\n\nThen we allocate the groups with the schedule listed in Table 13. We put all samples from Group 0 (except test samples) into the first batch for pretraining, and each subsequent batch contains 800 samples. Finally, the regression set is defined as the samples from urban areas, which can better reflect the overall wealth index of each country. Note that this is a regression task, and we evaluate the model performance with the Pearson correlation following Koh et al. (2021).\n\nD ALGORITHM DETAILS\n\nFirst of all, at t = 0, all OCL algorithms train the initial model on the first labeled batch and the training regression set with empirical risk minimization (ERM). In particular, following Koh et al. (2021), we use the cross entropy loss for classification tasks and the mean squared error for regression tasks. The differences among the methods only appear after t > 0.\n\nBaselines. First, note that the upper bound baseline i.i.d. offline is not an OCL algorithm because it assumes access to Dt at time t. For each t, it trains a model on a set sampled from Dt with a sufficiently large size and no overlapping with St. The size of the training set is different for different benchmarks, but we always make sure that it is at least as large as the union of all batches before time t. i.i.d. offline is only an approximate upper bound of the online performance because\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: Sample allocation schedule for Amazon-WPDS.\n\nGroup 0 Group 1\n\nt\n\n0 1\n2 3\n4 5\n6 7\n8 9\n10 11 12 13 14 15 16 17\n\n0 500 500 1550 1550 1550 2950 2950 2950 2950 4250 4250 4250 4250 5000 5000 5000 5000\n\nTotal\n\n54450\n\n50000 4500 4500 3450 3450 3450 2050 2050 2050 2050 750 750 750 750 0\n0 0\n0\n\n80550\n\nTable 13: Sample allocation schedule for Poverty-WPDS.\n\nt\n\n0 1\n2 3\n4 5\n6 7\n8 9\n10 11 12 13\n\nGroup 0 Group 1 Group 2 Group 3\n\n6359 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n\n0 800 720 720 552 552 328 328 120 120 0\n0 0\n0\n\n0 0\n80 80 248 248 400 400 400 400 400 200 0\n0\n\n0 0\n0 0\n0 0\n72 72 280 280 400 600 800 800\n\nTotal\n\n6359\n\n4240\n\n2856\n\n3304\n\nfor a distribution P , training on multiple distributions other than P might even achieve a better performance on P than training on P directly, as we saw in benchmarking CivilComments-WPDS. However, this baseline gives us a sense of the generalization error, and thus help us interpret the performances of other OCL algorithms.\n\nThen, for FBO, the algorithm does not do anything after t > 0, and the initial model is used till the end. For NBO, for each new batch St, the model is trained on the labeled portion of St only with a fixed number of epochs of ERM, and no previous sample is replayed.\n\nIn ER-FIFO, for each t > 0, the model is fine-tuned on the union ER-FIFO and ER-FIFO-RW. of the new labeled batch, the recent labeled batches, and the training regression set, for a fixed number of epochs (named epochs) of ERM. Specifically, the recent labeled batches and the training regression set are stored in the memory buffer. One issue of this approach is that it does not take\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\ninto account the sizes of the batches and the regression set. For example, if the regression set is much larger than the batches, then it will be hard for the model to learn the new knowledge. Thus, ER-FIFO-RW alters the weights of the three data sets: the new data, the recent data and the regression data. Specifically, it uses uniform sampling over these three sets, so that each set has the same probability of being selected.\n\nMIR and MaxLoss. When there are too many previous samples in the buffer, we cannot replay all of them for efficiency. ER-FIFO-RW solves this problem by randomly sampling previous samples to replay. However, this might not be the most efficient method, as some previous samples might be more useful than the others for preventing catastrophic forgetting. MIR and MaxLoss are two strategies of selecting replay samples, which operate as follows: For each iteration,\n\n1. Sample n new samples and nkr previous samples that require knowledge retention (KR). 2. Virtual update: Fine-tune ft−1 → f ′ 3. Select n samples from the nkr previous samples for replay. Specifically, MIR selects the samples whose loss increase the most before and after virtual update, while MaxLoss selects the samples with the highest loss after virtual update.\n\nt−1 with ERM on the n new samples.\n\n4. Recover the model to ft−1, and fine-tune ft−1 → ft with ERM on the n new samples and\n\nthe n selected previous samples.\n\nThe ratio nkr/n is named as kr size in our code, which should be greater than 1. The larger nkr is, the more likely we can select “useful” replay samples, but the slower the algorithm.\n\nIn this method, for each iteration, we first sample n new data, nkr recent data and GEM-PDS. nkr regression data, on which we estimate the three gradients of the loss function g0, g1 and g2, respectively. A larger nkr allows the learner to estimate g1 and g2 more accurately. The ratio nkr/n is still named as kr size. Then, the pseudo gradient g∗ is the optimal solution of the convex optimization problem Eqn. (1), which can be solved with the following procedure:\n\n1. a = ⟨g0, g1⟩, b = ⟨g1, g1⟩, c = ⟨g1, g2⟩, d = ⟨g0, g2⟩, e = ⟨g2, g2⟩. 2. 3. p = cd − ae, q = ac − bd, r = be − c2.\n\nIf a ≥ 0, d ≥ 0 then return g0.\n\n4.\n\n5.\n\nˆg1 = g0 −\n\nˆg2 = g0 −\n\na b\nd e\n\n6. Return ˆg3 = g0 +\n\np r\n\ng1 +\n\nq r\n\ng2.\n\ng1. If a ≤ 0 and q ≤ 0 then return ˆg1.\n\n(3)\n\ng2. If d ≤ 0 and p ≤ 0 then return ˆg2.\n\nWe can verify with the KKT conditions that this procedure returns the correct solution of Eqn. (1) (e.g. see Section 5.5.3 of Boyd et al. (2004)). The model is then updated with gradient descent along g∗.\n\nOnline L2 Regularization and EWC. Let the loss of model ft parameterized by θt (which is the vectorized model weight) on the labeled batch of St be lt(θt). In online L2 regularization, we minimize the following objective function with a fixed number of epochs of ERM:\n\nmin θt\n\nlt(θt) +\n\nλ 2\n\n∥θt − θ0∥2\n\n2\n\n(4)\n\nwhere we add a L2 penalty between the new model weight θt and the initial model weight θ0. The reasons why we use ∥θt − θ0∥2\n\n2 instead of ∥θt − θt−1∥2\n\n2 are:\n\n1. θ0 has a very good regression set performance, which is not guaranteed for θt−1. So using\n\n∥θt − θ0∥2\n\n2 ensures a higher regression set performance.\n\n2. If T is very large, then the model weight can still change a lot if we use ∥θt − θt−1∥2\n\n2 (as\n\nthe change accumulates with t), so knowledge retention cannot be guaranteed.\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nIn EWC, the objective function is the following:\n\nmin θt\n\nlt(θt) +\n\nλ 2\n\n∥diag(Ft)(θt − θ0)∥2\n\n2\n\n(5)\n\nwhere Ft is the Fisher information matrix (FIM), and diag(Ft) only contains the elements on the diagonal of Ft. Following Kirkpatrick et al. (2017), we estimate diag(Ft) from the first-order derivatives of the loss function on nkr samples for knowledge retention. The ratio nkr/n is still named as kr size. With a larger nkr, we can estimate Ft more accurately. λ is named as lbd.\n\nPseudo Labeling (PL) and FixMatch (FM). In PL and FM, there are two hyperparameters: the number of epochs of the virtual update step epochs v, and the number of epochs of the real finetuning step epochs r. Virtual update is done with ERM on the labeled samples only, while real fine-tuning is done with a supervised OCL algorithm on the union of labeled and pseudo-labeled samples. Thus, PL and FM can be combined with any supervised OCL algorithm, which we denote by adding the suffix “PL” or “FM” to the algorithm, such as ER-FIFO-PL and ER-FIFO-FM.\n\nE EXPERIMENT DETAILS\n\nE.1 SETUP\n\nFollowing Koh et al. (2021), we use a DistilBert-base-uncased for CivilComments-WPDS and Amazon-WPDS, a DenseNet-121 for FMoW-WPDS, and a ResNet-18 for Poverty-WPDS. For the training hyperparameters, we generally use the same ones as in Koh et al. (2021), with one exception that we use a multi learning rate decay scheduler for the two vision benchmarks as we find that it can produce better performances than the old scheduler.\n\nFor each experiment we report the following 6 metrics:\n\n• For online performance, we report the average online performance (avg online) within a\n\nfinite T .\n\n• For knowledge retention, we report the average recent performance (avg recent) and the worst recent performance (worst recent), which is the average and the minimum of the recent performance over t = w, · · · , T . We also report the worst performance because knowledge retention is required for every t. Likewise, we also report the average reg performance (avg reg) (regression set performance) and the worst reg performance (worst reg). As mentioned in Section 3.1, we use separate test batches to evaluate the recent and reg performances.\n\n• For training efficiency, we report the average runtime (avg time) of the method over t = 1, · · · , T . Note that the time used to train the initial model (t = 0) is not computed in the average runtime.\n\nEach experiment is run on a single NVIDIA V100 GPU. Each experiment is run 5 times with different random seeds, and the mean and the standard deviation of the results are reported. In particular, for each benchmark we use 5 fixed initial models: We train 5 initial models on the first batch and the training regression set with ERM for a fixed number of epochs with different random seeds, and then use these 5 models as initial models for all methods. This both alleviates the effect of randomness in the initial models and saves time. Under this setting, FBO can also serve as an upper bound of the worst regression set performance, which is the regression set performance at t = 1 for every method.\n\nE.2 RESULTS\n\nIn Table 14 we list the notations we use in the results. The results are reported in Tables 15-18.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nTable 14: Notations of algorithms.\n\nAlgorithm\n\nHyperparameters\n\nNotation\n\nExample\n\nepochs NBO epochs ER-FIFO epochs ER-FIFO-RW epochs, kr size MIR epochs, kr size MaxLoss epochs, kr size GEM-PDS epochs, lbd L2Reg epochs, lbd, kr size L2Reg-epochs-lbd-kr size EWC epochs v, epochs r ER-FIFO-PL epochs v, epochs r ER-FIFO-RW-PL epochs v, epochs r ER-FIFO-FM ER-FIFO-RW-FM epochs v, epochs r\n\nNBO-epochs ER-FIFO-epochs ER-FIFO-RW-epochs MIR-epochs-kr size MaxLoss-epochs-kr size GEM-PDS-epochs-kr size L2Reg-epochs-lbd\n\nER-FIFO-PL-epochs v-epochs r ER-FIFO-RW-PL-epochs v-epochs r ER-FIFO-FM-epochs v-epochs r ER-FIFO-RW-FM-epochs v-epochs r ER-FIFO-RW-FM-10-5\n\nNBO-5 ER-FIFO-5 ER-FIFO-RW-5 MIR-5-4 MaxLoss-5-4 GEM-PDS-5-4 L2Reg-5-0.1 EWC-5-0.1-4 ER-FIFO-PL-10-5 ER-FIFO-RW-PL-10-5 ER-FIFO-FM-10-5\n\nTable 15: Results on CivilComments-WPDS (α = 0.5%). Accuracies in %.\n\nAlgorithm\n\nFBO i.i.d. offline NBO-100\n\nER-FIFO-1 ER-FIFO-RW-3 ER-FIFO-RW-10 ER-FIFO-RW-100\n\nMIR-10-4 MIR-100-4 MIR-10-10 MIR-100-10\n\nMaxLoss-10-4 MaxLoss-100-4 MaxLoss-10-10 MaxLoss-100-10\n\nGEM-PDS-10-4 GEM-PDS-30-4 GEM-PDS-10-10 GEM-PDS-30-10\n\nL2Reg-10-1.0 L2Reg-10-10.0 L2Reg-10-100.0 L2Reg-100-1.0 L2Reg-100-10.0 L2Reg-100-100.0\n\nEWC-10-1.0-4 EWC-10-10.0-4 EWC-10-100.0-4 EWC-100-1.0-4 EWC-100-10.0-4 EWC-100-100.0-4\n\nER-FIFO-PL-0-1 ER-FIFO-PL-1-1 ER-FIFO-PL-3-1 ER-FIFO-PL-10-1\n\nER-FIFO-PL-RW-0-1 ER-FIFO-PL-RW-1-1 ER-FIFO-PL-RW-3-1 ER-FIFO-PL-RW-10-1\n\nAvg Online Avg Recent Worst Recent Avg Reg\n\nWorst Reg\n\nAvg Time\n\n81.45 (0.37) 90.03 (0.19) 89.19 (0.39)\n\n82.62 (0.34) 85.27 (0.70) 83.94 (0.31) 81.84 (0.59)\n\n87.33 (0.30) 84.16 (0.15) 86.85 (0.29) 83.72 (0.18)\n\n87.32 (0.31) 84.18 (0.44) 86.81 (0.34) 83.71 (0.18)\n\n87.59 (0.39) 87.61 (0.55) 86.95 (0.43) 86.72 (0.43)\n\n88.35 (0.38) 87.13 (0.48) 86.11 (0.35) 87.54 (0.39) 86.91 (0.37) 86.18 (0.35)\n\n88.50 (0.45) 87.51 (0.25) 85.62 (0.18) 86.56 (0.32) 86.24 (0.31) 85.20 (0.21)\n\n77.22 (1.83) 85.21 (0.57) 87.28 (0.33) 87.94 (0.35)\n\n76.55 (1.05) 84.94 (0.85) 87.08 (0.30) 87.65 (0.46)\n\n89.08 (0.23)\n\n85.12 (1.28)\n\n55.20 (7.54)\n\n30.07 (11.67)\n\n95.61 (0.85)\n\n84.79 (0.96)\n\n83.81 (0.41) 80.93 (0.57) 83.32 (0.43) 84.48 (0.68)\n\n75.94 (1.98) 82.29 (0.97) 77.71 (1.43) 83.00 (0.65)\n\n76.25 (1.90) 82.39 (1.32) 77.63 (1.31) 82.73 (0.95)\n\n76.13 (1.38) 74.92 (2.53) 78.33 (1.37) 77.52 (1.71)\n\n70.51 (3.19) 75.63 (2.90) 78.07 (2.07) 74.56 (2.21) 76.30 (2.21) 78.05 (1.87)\n\n68.74 (3.76) 74.67 (2.09) 79.75 (1.46) 77.53 (2.04) 78.21 (1.80) 80.44 (1.44)\n\n85.66 (1.31) 79.54 (0.72) 76.26 (1.49) 73.82 (2.14)\n\n86.39 (0.76) 80.26 (1.40) 76.60 (0.94) 75.28 (2.13)\n\n81.46 (1.00) 75.90 (1.71) 80.63 (1.09) 82.64 (1.10)\n\n57.57 (3.95) 74.65 (0.95) 60.21 (4.29) 78.96 (1.60)\n\n57.15 (2.91) 74.31 (1.74) 60.35 (2.31) 77.99 (1.03)\n\n66.78 (4.01) 61.88 (8.54) 71.28 (2.31) 64.50 (6.38)\n\n59.10 (3.88) 67.15 (4.56) 69.46 (5.63) 65.30 (3.71) 68.32 (2.54) 69.80 (3.40)\n\n54.83 (5.17) 65.57 (2.81) 74.30 (3.14) 71.48 (3.50) 72.01 (3.56) 75.44 (3.06)\n\n83.76 (0.87) 73.69 (2.45) 68.86 (2.63) 64.97 (4.44)\n\n84.36 (1.08) 75.84 (1.36) 68.59 (0.56) 67.58 (3.41)\n\n309.04 (1.12) 7.75 (0.04) 24.48 (0.04) 248.83 (3.52)\n\n40.99 (0.12) 419.84 (2.08) 75.31 (0.37) 757.66 (5.90)\n\n31.27 (0.09) 317.14 (1.47) 52.46 (0.41) 502.09 (6.19)\n\n68.15 (0.07) 195.42 (0.29) 149.77 (0.14) 456.54 (0.72)\n\n10.33 (0.13) 9.82 (0.12) 9.76 (0.12) 94.03 (0.67) 91.92 (0.71) 93.58 (0.39)\n\n38.86 (0.37) 39.00 (0.44) 38.81 (0.08) 371.97 (1.27) 371.44 (0.86) 368.17 (3.44)\n\n510.40 (8.05) 526.34 (4.13) 514.51 (4.14) 501.86 (1.50)\n\n499.08 (1.22) 531.59 (7.74) 522.92 (8.41) 512.99 (2.48)\n\n82.92 (0.30) 85.87 (0.54) 84.09 (0.47) 81.95 (0.78)\n\n86.70 (0.51) 83.36 (0.38) 86.06 (0.58) 83.09 (0.28)\n\n86.76 (0.51) 83.36 (0.40) 86.05 (0.46) 83.12 (0.35)\n\n87.64 (0.28) 87.58 (0.45) 86.86 (0.46) 86.60 (0.62)\n\n88.51 (0.42) 87.20 (0.61) 86.03 (0.51) 87.35 (0.42) 86.65 (0.49) 85.93 (0.46)\n\n88.54 (0.55) 87.39 (0.45) 85.38 (0.38) 86.36 (0.45) 86.01 (0.48) 84.79 (0.43)\n\n76.54 (2.06) 86.46 (0.48) 88.22 (0.38) 88.76 (0.32)\n\n75.92 (1.97) 85.97 (0.91) 88.29 (0.28) 88.64 (0.37)\n\n77.42 (0.97) 81.42 (1.44) 79.48 (0.64) 76.42 (1.54)\n\n80.99 (1.30) 77.36 (1.11) 79.57 (1.42) 77.21 (0.48)\n\n81.04 (0.80) 77.22 (0.79) 79.96 (1.11) 77.66 (0.58)\n\n82.96 (1.01) 82.83 (1.01) 82.22 (0.59) 80.60 (1.61)\n\n84.87 (0.81) 82.09 (0.62) 80.18 (0.97) 82.04 (0.71) 80.94 (0.91) 79.98 (1.04)\n\n84.49 (1.28) 82.06 (1.16) 79.34 (0.79) 80.70 (0.98) 80.24 (1.01) 78.73 (0.82)\n\n69.66 (2.17) 82.81 (0.79) 84.59 (1.02) 85.72 (0.44)\n\n69.13 (2.96) 82.32 (1.44) 84.79 (0.97) 85.57 (0.63)\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm\n\nFBO i.i.d. offline NBO-5\n\nER-FIFO-2 ER-FIFO-RW-2 ER-FIFO-RW-5 ER-FIFO-RW-10\n\nMIR-2-2 MIR-2-4 MIR-2-10\n\nMaxLoss-2-2 MaxLoss-2-4 MaxLoss-2-10\n\nGEM-PDS-2-2 GEM-PDS-2-4 GEM-PDS-2-10\n\nL2Reg-2-0.1 L2Reg-2-1.0 L2Reg-2-10.0\n\nEWC-2-0.1-4 EWC-2-1.0-4 EWC-2-10.0-4\n\nER-FIFO-PL-3-3 ER-FIFO-PL-5-3 ER-FIFO-PL-3-5 ER-FIFO-PL-5-5\n\nER-FIFO-PL-RW-3-3 ER-FIFO-PL-RW-5-3 ER-FIFO-PL-RW-3-5 ER-FIFO-PL-RW-5-5\n\nER-FIFO-FM-3-3 ER-FIFO-FM-5-3 ER-FIFO-FM-3-5 ER-FIFO-FM-5-5\n\nER-FIFO-RW-FM-3-3 ER-FIFO-RW-FM-5-3 ER-FIFO-RW-FM-3-5 ER-FIFO-RW-FM-5-5\n\nTable 16: Results on FMoW-WPDS (α = 50%). Accuracies in %.\n\nAvg Online Avg Recent Worst Recent Avg Reg\n\nWorst Reg\n\nAvg Time\n\n68.00 (0.24) 83.04 (0.25) 71.78 (0.08)\n\n74.01 (0.10) 72.77 (0.04) 73.33 (0.11) 73.56 (0.09)\n\n69.75 (0.09) 71.29 (0.06) 72.59 (0.08)\n\n69.66 (0.21) 71.34 (0.07) 72.64 (0.08)\n\n72.07 (0.08) 72.04 (0.11) 72.05 (0.07)\n\n69.61 (0.11) 67.90 (0.14) 68.02 (0.12)\n\n70.16 (0.16) 68.51 (0.16) 68.13 (0.12)\n\n74.25 (0.15) 74.36 (0.08) 74.55 (0.06) 74.62 (0.05)\n\n73.40 (0.04) 73.49 (0.09) 73.50 (0.15) 73.46 (0.04)\n\n74.81 (0.09) 74.80 (0.06) 75.10 (0.07) 75.16 (0.05)\n\n73.44 (0.05) 73.40 (0.08) 73.71 (0.05) 73.75 (0.15)\n\n73.03 (0.06)\n\n71.27 (0.26)\n\n76.84 (0.23)\n\n72.71 (0.47)\n\n161.90 (2.11)\n\n84.54 (0.21)\n\n76.04 (0.12) 74.56 (0.16) 75.21 (0.06) 75.80 (0.08)\n\n70.74 (0.25) 72.36 (0.10) 73.95 (0.08)\n\n70.53 (0.30) 72.46 (0.05) 74.00 (0.10)\n\n73.21 (0.19) 73.14 (0.09) 73.21 (0.13)\n\n70.14 (0.19) 68.32 (0.20) 68.74 (0.26)\n\n70.76 (0.24) 69.01 (0.27) 68.84 (0.20)\n\n76.34 (0.22) 76.47 (0.23) 76.54 (0.24) 76.67 (0.13)\n\n75.28 (0.25) 75.43 (0.18) 75.41 (0.26) 75.42 (0.14)\n\n76.91 (0.15) 76.86 (0.13) 77.19 (0.13) 77.33 (0.19)\n\n75.37 (0.30) 75.31 (0.12) 75.72 (0.15) 75.68 (0.13)\n\n74.38 (0.34) 73.07 (0.15) 73.72 (0.28) 74.31 (0.29)\n\n68.68 (0.57) 70.44 (0.20) 72.27 (0.15)\n\n68.15 (0.81) 70.52 (0.32) 72.37 (0.21)\n\n71.49 (0.33) 71.54 (0.34) 71.45 (0.37)\n\n66.43 (0.28) 63.78 (0.61) 63.34 (0.43)\n\n67.72 (0.43) 64.84 (0.49) 63.96 (0.35)\n\n74.49 (0.48) 75.00 (0.58) 74.96 (0.42) 74.91 (0.21)\n\n73.80 (0.40) 73.90 (0.34) 73.70 (0.46) 73.89 (0.21)\n\n75.36 (0.62) 75.27 (0.17) 75.46 (0.15) 75.75 (0.60)\n\n73.80 (0.37) 73.83 (0.30) 74.04 (0.41) 74.32 (0.25)\n\n83.91 (0.09) 81.84 (0.17) 82.33 (0.12) 82.63 (0.08)\n\n79.25 (0.25) 80.81 (0.16) 81.90 (0.25)\n\n79.23 (0.13) 80.86 (0.12) 81.90 (0.30)\n\n79.10 (0.15) 79.16 (0.23) 79.17 (0.21)\n\n81.59 (0.17) 82.30 (0.21) 83.74 (0.19)\n\n81.94 (0.10) 82.93 (0.14) 83.69 (0.11)\n\n84.00 (0.18) 83.95 (0.13) 84.40 (0.10) 84.29 (0.11)\n\n82.12 (0.16) 82.10 (0.19) 82.27 (0.08) 82.26 (0.06)\n\n84.37 (0.11) 84.32 (0.16) 84.70 (0.16) 84.71 (0.10)\n\n81.87 (0.19) 81.80 (0.05) 82.27 (0.08) 82.31 (0.05)\n\n80.82 (0.49) 80.34 (0.30) 80.27 (0.34) 80.21 (0.19)\n\n76.48 (0.66) 79.19 (0.52) 80.92 (0.20)\n\n76.40 (0.56) 79.40 (0.35) 80.90 (0.32)\n\n75.19 (0.45) 75.54 (0.75) 75.86 (0.38)\n\n78.84 (0.95) 80.30 (1.00) 82.92 (0.30)\n\n80.20 (0.60) 81.73 (0.42) 83.21 (0.14)\n\n80.34 (0.43) 80.20 (0.50) 80.90 (0.60) 80.65 (0.23)\n\n79.80 (0.37) 80.07 (0.24) 79.52 (0.14) 79.40 (0.31)\n\n80.80 (0.18) 80.36 (0.42) 80.96 (0.27) 80.88 (0.39)\n\n79.62 (0.35) 79.52 (0.41) 79.38 (0.44) 79.52 (0.33)\n\n1458.19 (239.67) 226.31 (10.83) 480.61 (20.08) 953.56 (96.59)\n\n357.74 (3.26) 460.35 (9.11) 710.30 (54.07)\n\n318.70 (3.33) 361.38 (25.58) 553.68 (18.74)\n\n240.80 (3.06) 502.77 (41.03) 969.29 (5.54)\n\n76.42 (1.15) 79.84 (1.27) 79.94 (0.99)\n\n239.01 (1.32) 288.53 (26.63) 287.97 (26.06)\n\n1881.01 (6.93) 1924.03 (15.96) 3071.01 (28.59) 3128.51 (54.12)\n\n573.71 (9.83) 622.28 (6.41) 891.74 (9.63) 944.21 (6.73)\n\n2022.45 (179.49) 2084.54 (183.02) 3056.44 (52.54) 3268.89 (307.79)\n\n572.95 (2.68) 642.91 (37.83) 896.77 (4.61) 971.52 (39.10)\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nTable 17: Results on Amazon-WPDS (α = 0.5%). Accuracies in %.\n\nAlgorithm\n\nFBO i.i.d. offline NBO-100\n\nER-FIFO-1 ER-FIFO-RW-10 ER-FIFO-RW-100\n\nMIR-10-4 MIR-100-4 MIR-10-10 MIR-100-10\n\nMaxLoss-10-4 MaxLoss-100-4 MaxLoss-10-10 MaxLoss-100-10\n\nGEM-PDS-10-4 GEM-PDS-30-4 GEM-PDS-10-10 GEM-PDS-30-10\n\nL2Reg-10-0.1 L2Reg-10-1.0 L2Reg-10-10.0\n\nEWC-10-0.1-4 EWC-10-1.0-4 EWC-10-10.0-4\n\nER-FIFO-PL-0-1 ER-FIFO-PL-1-1 ER-FIFO-PL-3-1 ER-FIFO-PL-10-1\n\nER-FIFO-RW-PL-0-1 ER-FIFO-RW-PL-1-1 ER-FIFO-RW-PL-3-1 ER-FIFO-RW-PL-10-1\n\nAvg Online Avg Recent Worst Recent Avg Reg\n\nWorst Reg\n\nAvg Time\n\n84.85 (0.20) 94.15 (0.19) 90.49 (0.41)\n\n87.12 (0.34) 88.47 (0.67) 86.65 (0.21)\n\n89.46 (0.46) 86.99 (0.48) 89.15 (0.61) 87.16 (0.36)\n\n89.42 (0.43) 87.06 (0.69) 89.32 (0.57) 87.11 (0.45)\n\n89.94 (0.46) 90.32 (0.47) 89.77 (0.61) 90.26 (0.58)\n\n89.01 (0.43) 88.00 (0.51) 86.67 (0.30)\n\n89.32 (0.43) 88.94 (0.45) 88.52 (0.48)\n\n85.75 (1.09) 87.84 (0.57) 88.97 (0.36) 89.47 (0.40)\n\n85.28 (1.37) 88.21 (0.95) 89.19 (0.43) 89.84 (0.54)\n\n90.68 (0.34)\n\n88.58 (0.95)\n\n92.22 (0.43)\n\n88.63 (0.76)\n\n105.68 (1.74)\n\n93.78 (0.26)\n\n88.06 (0.28) 89.32 (0.73) 87.39 (0.31)\n\n90.19 (0.51) 87.87 (0.59) 89.95 (0.66) 88.07 (0.42)\n\n90.14 (0.54) 87.88 (0.75) 90.06 (0.61) 88.06 (0.66)\n\n90.50 (0.41) 90.84 (0.35) 90.42 (0.51) 90.81 (0.55)\n\n89.79 (0.45) 88.83 (0.57) 87.46 (0.35)\n\n90.05 (0.46) 89.77 (0.49) 89.32 (0.57)\n\n86.39 (1.27) 88.94 (0.69) 89.98 (0.26) 90.39 (0.38)\n\n85.96 (1.41) 89.03 (1.09) 90.03 (0.40) 90.56 (0.49)\n\n84.70 (0.40) 86.85 (1.62) 83.22 (1.07)\n\n88.00 (1.55) 84.28 (0.87) 87.28 (2.16) 84.68 (0.38)\n\n87.52 (2.01) 84.42 (1.17) 87.78 (1.78) 84.73 (1.18)\n\n88.74 (1.11) 89.22 (1.08) 88.20 (1.79) 89.32 (1.23)\n\n86.72 (2.43) 85.14 (1.48) 82.21 (1.26)\n\n87.06 (2.60) 86.48 (2.77) 85.75 (2.05)\n\n80.91 (2.85) 86.21 (1.57) 87.72 (0.48) 88.42 (1.43)\n\n80.30 (2.62) 85.95 (2.30) 87.56 (1.09) 88.95 (1.30)\n\n93.58 (0.16) 93.63 (0.30) 93.68 (0.20)\n\n93.31 (0.34) 93.62 (0.38) 93.45 (0.36) 93.78 (0.26)\n\n93.33 (0.28) 93.72 (0.33) 93.49 (0.27) 93.73 (0.30)\n\n92.94 (0.46) 92.55 (0.43) 93.10 (0.46) 92.80 (0.26)\n\n93.23 (0.47) 93.62 (0.20) 93.75 (0.18)\n\n93.20 (0.35) 93.34 (0.27) 93.58 (0.24)\n\n93.94 (0.10) 93.77 (0.22) 93.66 (0.28) 93.20 (0.13)\n\n93.66 (0.44) 93.79 (0.25) 93.68 (0.15) 93.39 (0.31)\n\n93.02 (0.28) 93.00 (0.37) 93.11 (0.37)\n\n91.82 (0.94) 92.86 (0.67) 92.21 (0.82) 93.24 (0.29)\n\n91.82 (0.84) 93.13 (0.41) 92.41 (0.57) 93.18 (0.30)\n\n90.65 (0.47) 89.64 (0.66) 91.30 (0.45) 89.93 (1.16)\n\n91.46 (1.00) 92.94 (0.35) 93.26 (0.18)\n\n91.21 (1.21) 91.60 (1.27) 92.72 (0.34)\n\n93.39 (0.17) 93.17 (0.27) 92.92 (0.42) 92.07 (0.36)\n\n92.97 (0.58) 93.15 (0.33) 92.88 (0.38) 91.80 (0.59)\n\n314.64 (1.60) 17.09 (0.06) 181.74 (1.46)\n\n28.88 (0.23) 286.59 (2.91) 47.99 (0.24) 479.84 (11.69)\n\n24.37 (0.17) 247.32 (6.66) 34.72 (0.24) 358.98 (3.47)\n\n37.30 (0.18) 111.27 (0.22) 82.24 (0.20) 239.08 (0.97)\n\n11.06 (0.16) 8.52 (0.05) 8.53 (0.07)\n\n26.12 (0.41) 23.02 (0.21) 23.34 (0.24)\n\n428.25 (11.42) 427.75 (7.14) 422.17 (2.50) 419.94 (1.45)\n\n277.10 (0.58) 278.52 (1.28) 274.58 (0.62) 280.12 (1.85)\n\nTable 18: Results on Poverty-WPDS (α = 50%). Results are Pearson correlations.\n\nAlgorithm\n\nFBO i.i.d. offline NBO-100\n\nER-FIFO-50 ER-FIFO-RW-50 ER-FIFO-RW-100 ER-FIFO-RW-150\n\nMIR-30-4 MIR-100-4 MIR-30-10 MIR-100-10\n\nMaxLoss-30-4 MaxLoss-100-4 MaxLoss-30-10 MaxLoss-100-10\n\nGEM-PDS-30-4 GEM-PDS-100-4 GEM-PDS-30-10 GEM-PDS-100-10\n\nL2Reg-100-0.1 L2Reg-100-1.0 L2Reg-100-10.0\n\nEWC-100-0.1 EWC-100-1.0 EWC-100-10.0\n\nAvg Online\n\nAvg Recent Worst Recent Avg Reg\n\nWorst Reg\n\nAvg Time\n\n0.781 (0.024) 0.832 (0.010) 0.813 (0.003)\n\n0.791 (0.007) 0.793 (0.006) 0.798 (0.007) 0.799 (0.004)\n\n0.737 (0.021) 0.750 (0.007) 0.759 (0.010) 0.774 (0.007)\n\n0.727 (0.023) 0.740 (0.019) 0.754 (0.012) 0.770 (0.010)\n\n0.771 (0.005) 0.777 (0.006) 0.761 (0.004) 0.767 (0.005)\n\n0.797 (0.007) 0.792 (0.006) 0.780 (0.010)\n\n0.726 (0.009) 0.735 (0.004) 0.716 (0.004)\n\n0.831 (0.004)\n\n0.780 (0.010)\n\n0.583 (0.012)\n\n0.520 (0.035)\n\n204.69 (2.12)\n\n0.638 (0.017)\n\n0.633 (0.018) 0.641 (0.016) 0.641 (0.018) 0.643 (0.015)\n\n0.579 (0.015) 0.585 (0.007) 0.602 (0.008) 0.611 (0.014)\n\n0.575 (0.014) 0.584 (0.017) 0.601 (0.008) 0.610 (0.017)\n\n0.634 (0.014) 0.640 (0.010) 0.634 (0.014) 0.640 (0.009)\n\n0.614 (0.012) 0.642 (0.005) 0.641 (0.004)\n\n0.625 (0.010) 0.648 (0.003) 0.652 (0.006)\n\n0.549 (0.109) 0.612 (0.024) 0.611 (0.032) 0.608 (0.021)\n\n0.502 (0.036) 0.524 (0.043) 0.561 (0.019) 0.568 (0.031)\n\n0.509 (0.024) 0.471 (0.182) 0.561 (0.015) 0.569 (0.029)\n\n0.622 (0.015) 0.627 (0.012) 0.622 (0.019) 0.629 (0.011)\n\n0.559 (0.032) 0.619 (0.010) 0.616 (0.008)\n\n0.601 (0.013) 0.628 (0.007) 0.636 (0.015)\n\n881.86 (57.91) 244.83 (3.85) 426.77 (2.04) 629.51 (8.50)\n\n256.05 (31.52) 847.82 (27.40) 719.60 (42.89) 2148.85 (137.06)\n\n274.64 (25.15) 818.20 (19.62) 557.77 (23.40) 1643.23 (116.68)\n\n338.23 (7.42) 1125.91 (36.97) 750.12 (4.47) 3247.20 (472.93)\n\n249.46 (19.51) 247.66 (6.89) 247.39 (7.83)\n\n884.09 (12.90) 714.14 (61.72) 719.71 (66.79)\n\n0.806 (0.008) 0.807 (0.009) 0.812 (0.009) 0.813 (0.007)\n\n0.759 (0.016) 0.773 (0.015) 0.779 (0.016) 0.793 (0.011)\n\n0.752 (0.020) 0.760 (0.023) 0.776 (0.016) 0.790 (0.015)\n\n0.800 (0.009) 0.809 (0.005) 0.786 (0.009) 0.798 (0.006)\n\n0.804 (0.004) 0.807 (0.004) 0.799 (0.006)\n\n0.744 (0.012) 0.753 (0.007) 0.731 (0.007)\n\n0.748 (0.050) 0.753 (0.028) 0.765 (0.045) 0.765 (0.044)\n\n0.691 (0.050) 0.702 (0.023) 0.694 (0.060) 0.747 (0.029)\n\n0.688 (0.038) 0.677 (0.039) 0.695 (0.053) 0.734 (0.054)\n\n0.783 (0.009) 0.793 (0.013) 0.773 (0.013) 0.781 (0.012)\n\n0.743 (0.027) 0.783 (0.008) 0.778 (0.013)\n\n0.719 (0.018) 0.734 (0.007) 0.718 (0.006)\n\n36",
    "reference": "# Summary Of The Paper\n\nAuthors introduce the novel OCL-PDS problem - Online Continual Learning for Progressive Distribution Shift. Authors contrast their problem with that of continual learning and domain adaptation. Authors build 4 new benchmarks and implement 12 algorithms to test these benchmarks.\n\n# Strength And Weaknesses\n\nStrengths:\n1) Paper is clearly written and the authors point to practical issues with the current approaches. \n2) Authors give very detailed justification of their choices, give details of experiments which will be helpful for other researchers to build on. \n\nWeaknesses: \n1) The proposed approach still has some practical issues (e.g. how does one decide the divergence threshold, how does one make sure that 3-step procedure benchmark OCL-PDS is applicable to real world problems? \n2) The final takeaway is not clear. Authors should discuss some of the real world problems and give some conclusions based on it. It is not clear what part of the discussion is helpful if a real world practitioner wants to use any of the knowledge in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nPaper is clearly written. There is novelty but author's claim that their method is more practical is not justified.\n\n# Summary Of The Review\n\n1) From Appendix B, one can understand why divergence should be asymmetric. But from the example given, it looks like one needs structured asymmetry. Is just having asymmetry enough? \n\n2) Figure 1: It is not clear what distribution shift we are talking about in \"FMoW-WPDS benchmark.\" Are most images in 2002 of prison? Are most images in 2009 of Helipads? \n\n3) Section 2.3: Work is also very related to domain generalization (DG) which is a harder problem compared to domain adaptation (DA) [1]. If one wants to be truly \"online\" then one DG might be a better setting than DA? Authors should add more discussion comparing and contrasting with DG too. \n\n4) In general, real world datasets do not have shift continuity. Or there could be too much non-stationarity. How would ODD check or shift continuity check work in that case?\n\n5) How does one decide threshold on Div (D_t || D_t+1)? In the real world dataset, this could be even harder to determine. \n\n6) I checked the hyperparameters considered In Table 14(in appendix). But the list of hyperparameters tuned does not look complete. The training data generation (domains, dist shift and threshold on div for continual shift) should be treated as hyperparameters too. Can authors comment more on this? \n\n[1] Blanchard, Gilles, Aniket Anand Deshmukh, Ürun Dogan, Gyemin Lee, and Clayton Scott. \"Domain generalization by marginal transfer learning.\" The Journal of Machine Learning Research 22, no. 1 (2021): 46-100.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nGATED INFERENCE NETWORK: LEARNING STATE-SPACE MODELS\n\nINFERENCING AND\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nState-space models (SSMs) perform predictions by learning the underlying dynamics of observed sequence. We propose a new SSM approach in both high and low dimensional observation space, which utilizes Bayesian filtering-smoothing to model system’s dynamics more accurately than RNN-based SSMs and can be learned in an end-to-end manner. The designed architecture, which we call the Gated Inference Network (GIN), is able to integrate the uncertainty estimates and learn the complicated dynamics of the system that enables us to perform estimation and imputation tasks in both data presence and absence. The proposed model uses the GRU cells into its structure to complete the data flow, while avoids expensive computations and potentially unstable matrix inversions. The GIN is able to deal with any time-series data and gives us a strong robustness to handle the observational noise. In the numerical experiments, we show that the GIN reduces the uncertainty of estimates and outperforms its counterparts , LSTMs, GRUs and variational approaches.\n\n1\n\nINTRODUCTION\n\nState estimation and inference in the states in dynamical systems is one of the most interesting problems that has lots of application in signal processing and time series Rauch et al. (1965). In some cases, learning state space is a very complicated task due to the relatively high dimension of observations and measurements, which only provides the partial information about the states. Noise is another significant issue in this scenario, where it is more likely to obtain a noisy observation. Time series prediction and estimating the next scene, e.g, the state prediction or next observation prediction, is another substantial application that again requires the inference within the states which comes from the observations. Classical memory networks such as LSTMs (Hochreiter & Schmidhuber, 1997), GRUs (Cho et al., 2014) and simple RNNs like (Wilson & Finkel, 2009) and (Yadaiah & Sowmya, 2006) fail to give some intuition about the uncertainties and dynamics. A group of approaches perform the Kalman Filtering (KF) in the latent state which usually requires a deep encoder for feature extraction. Krishnan et al. (2017), Ghalamzan et al. (2021) and Hashempour et al. (2020) belong to these group of works. However the mentioned solutions have some restrictions, where they are not able to deal with high dimensional non-linear systems and the classic KF approach is computationally expensive, e.g matrix inversion issue. Likewise, indirect optimization of an objective fuction by using variational inference, like the work of Kingma & Welling (2013), increases the complexity of the model. Moreover, in the variational inference approaches that usually implemented in the context of variational auto encoders for dimension reduction, they do not have access to the loss directly and have to minimize its lower bound instead, which reduce the ability of learning dynamics and affect the performance of the model. KalmanNet Revach et al. (2021) and Ruhe & Forré (2021) use GRU in their structure for the state update. However, they are only able to deal with low-dimensional state space and cannot handle complex high dimensional inputs because of directly using classic Bayesian equations and matrix inversion issue. Moreover, their structure require the full, or at least partial, dynamic information.\n\nThe mentioned restrictions for KF and its variants and variational models in addition the necessity of having a metric to measure the uncertainty, motivate us to introduce the GIN, an end to end structure with dynamics learning ability using Bayesian properties for filtering-smoothing. The contributions of GIN are: (i) modeling high-low dimensional sequences: we show the eligibility of the GIN to infer both cases by a simple adjustment in the observation transferring functions in the\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Observation\n\n(b) LGSSM(smooth)\n\n(c) GIN(filter)\n\n(d) GIN(smooth)\n\nFigure 1: Inferred 5k length trajectories for Lorenz attractor.\n\nproposed structure, where we conduct three experiments of high dimensional non-linear dynamics and two experiments of low dimensional chaotic observation. (ii) Learning/using dynamics: the ability of learning the dynamics(in the lack of them) and utilizing available dynamics(in the presence of them) alongside modeling high-low dimensional observations makes the GIN applicable to a wide range of applications. To attain more accurate inference of observed dynamical system, we apply GRU cells that increases the modeling capability of the Kalman filtering-smoothing. By conduction an ablation study of the GIN being replaced by a linearized Gaussian state transition, we show the GIN is able for better learning state space representation with disentangled dynamics features. (iii) Direct optimization: We show that the posterior and smoothing inference distribution of the state-space model is tractable while dynamics and parameters are estimated by neural networks. Despite variational approaches, this allows us to use recursive Bayesian updates for direct likelihood maximization. (iv) Noise robustness: verified by the numerical results, inferencing for highly distorted sequences is feasible with the GIN. (v) Missing data imputation: by using Bayesian properties, the GIN decides whether to keep the previous information in the memory cell or update them by the obtained observation. Experimental results show the out-performance of the GIN over the SOTA studies in the imputation task.\n\n2 RELATED WORKS\n\nTo deal with complex sensory inputs, some approaches integrate a deep auto encoders into their architecture. Among these works, Embed to Control (E2C) (Watter et al., 2015) uses a deep encoder to obtain the observation and a variational inference about the states. However, these methods are not able to deal with missing data problem and imputation task since they do not rely on memory cells and are not recurrent. Another group of works like BackpropKF (Haarnoja et al., 2016) and RKN (Becker et al., 2019) apply CNNs for dimension-reduction and output both the uncertainty vector and observation, where they move away from variational inference and borrow Bayesian properties for the inference. However, these methods cannot handle the cases with the available knowledge of the dynamics and impose restrictive assumptions over covariance matrices, while the GIN provides a principled way for using the available partial dynamics information and release any assumption over covariance. Toward learning state space (system identification) a group of works like Wang et al. (2007), Ko & Fox (2011) and Frigola et al. (2013) propose algorithms to learn GPSSMs based on maximum likelihood estimation with the iterative EM algorithm. Frigola et al. (2013) obtain sample trajectories from the smoothing distribution, then conditioned on this trajectory they conduct M step for the model’s parameters. Switching linear dynamics systems (SLDS) (Ghahramani & Hinton, 2000), use additional latent variables to switch among different linear dynamics, where the approximate inference algorithms can be utilized to model switching linearity for reducing approximation errors ,however, this approach is not as flexible as general non-linear dynamical systems because the switch transition model is assumed independent of states and observations. To address this problem, Linderman et al. (2017) performs SLDS method through augmentation with a Polya-gamma-distributed variable and a stick-breaking process, however, this approach employs Gibbs sampling for inferring the parameters and therefore is not scalable to large datasets. Auto-regressive Hidden Markov Models (ARHMM) explain time series structures by defining a mapping from past observations to the current observation. (Salinas et al., 2020) is a ARHMM approach, in which target values are used as inputs directly. However, this dependency of the model on the targets makes the model more vulnerable to noise. This issue is addressed in DSSM (Rangapuram et al., 2018), another ARHMM approach, where the target values are only incorporated through the likelihood term. Other group of works consider EM-based variational-inference like Structured Inference Networks (SIN) (Krishnan et al., 2017), where it utilizes a RNN to update the state. Kalman Variational Autoencoder (KVAE) (Fraccaro et al., 2017) and Extended KVAE (EKVAE) (Klushyn et al., 2021) use the original KF equations and apply both filtering and smoothing.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nHowever, these EM-based variational inference methods are not able to estimate the states directly because of optimizing the lower bound of likelihood. Extra complexity is another issue with these approaches, while they are addressed by the proposed structure and direct end-to-end optimization in the GIN. We compare the GIN with these approaches in the experiment section and provide an empirical complexity analysis in appendix A.8.2. We provide a detailed discussion of recent related work in appendix A.8.3.\n\n3 GATED INFERENCE NETWORK FOR SYSTEM IDENTIFICATION\n\nIn the context of System Identification (SI) the GIN is similar to a Hammerstein-Wiener (HW) model (Schoukens & Tiels, 2017) (Gilabert et al., 2005), in the sense that it estimates the system parameters directly from the observations, which is in the figure 2. e(.) and d(.) are implemented with non-linear functions, e.g. auto encoders-MLPs. Transition block in figure 2 represents the dynamics of the system that allows for the inference using the Gaussian state space filtering-smoothing equations. However unlike a HW model, we employ non-linear GRU cells in the transition block that calculate the Kalman Gain (KG) and smoothing gain (SG) in an appropriate manner by circumventing the computational complexity, i.e matrix inversion issues. GRU cells empower the whole system by applying non-linearity to the linearized Gaussian state space models (LGSSMs). Numerical results indicate that by the proposed structure, having a good inference for even the complex non-linear systems with high dimensional observations is feasible. To achieve this, we assume the state fits into Gaussian state space models (GSSMs), which are commonly used to model sequences of vectors.\n\nIn most cases, the dynamics of the system might not be available or hard to obtain; while the process noise and observation noise are unknown (our first three experiments). Accordingly, we construct GIN to learn unknown variables from data in an end to end fashion, then we utilize the constructed KG and SG during inference time to obtain the filteredsmoothed states. The proposed architecture is depicted in figure 4. In the presence of dynamics (our last two experiments), auto-encoder and Dynamics Network in figure 4 are replaced by MLP to model the observation noise.\n\n4 PARAMETERIZATION\n\nFigure 2: The GIN as a HW model for system identification. By appropriate structure selection for e(.) and d(.), the GIN can handle high-low dimensional observations. The proposed architectures for each case are depicted separately with further details in appendix figures 10 and 11. The relation between the internal variables, wt and xt, is simulated by the transition block.\n\nIn this section we show the parameterization of the inference model. Firstly, we refer the readers to the summary of Kalman filtering-smoothing background for the completeness in appendix A.2. In the rest of the paper we use the following notations, where the original observations are o1:T , the transferred observations are w1:T and R1:T are diagonal matrices with r1:T diagonal elements that correspond to the covariance of transferred observation noise. x1:T corresponds to the states and Q1:T are diagonal matrices with q1:T diagonal elements which are the covariance of state process noise. (x− t ) are the mean vector and covariance matrix of the prior state at time step t, i.e. p(xt|w1:t−1), and (x+ t ) are the mean vector and covariance matrix of the posterior state at time step t, i.e. p(xt|w1:t). We define the transition matrices F1:T and emission matrices H1:T as the dynamics of the model, where lack of dynamics in the first three experiments means that (F1:T , H1:T ) are not know and are going to be trained(graphical model is in figure 3), while the presence of dynamics in our last two experiments means that (F1:T , H1:T ) are known(graphical models are in figures 9a and 9b). The dynamics (F1:T , H1:T ) and noise matrices\n\nFigure 3: Graphical model for high dimensional observations. dynt is the model’s dynamics at time t.\n\nt , Σ−\n\nt , Σ+\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: The high level structure of the GIN for high dimensional observation in the lack of dynamics, while for low dimensional cases auto-encoder is replaced by MLPs and dynamics are directly used. The transferred observation wt and its uncertainty rt, are obtained from the encoder(MLPs). In each t−1 is fed to the Dynamic Network to compute ˆFt and ˆHt. In the time step, the last posterior mean x+ Prediction Step the next priors (x− t ) are obtained by using new dynamics and the last posteriors. In the filtering step, by using the priors (x− t ) and the observation (wt, rt), the next posteriors (x+ t ) is feasible in the smoothing step. Finally, the decoder(MLP) is utilized to produce o+ t , which can be the high-low dimensional noise free estimates.\n\nt ) are obtained. Applying smoothing operation over the obtained posteriors (x+\n\nt , Σ−\n\nt , Σ−\n\nt , Σ+\n\nt , Σ+\n\n(R1:T , Q1:T ) construct the system parameters γ1:T = (F1:T , H1:T , Q1:T , R1:T ). Given original observations o1:T and transferred observations w1:T , we want to find good estimate of the latent states x1:T . To achieve this, we want to infer the marginal distributions p(xt|w1:t) for the online inference approach or filtering; and p(xt|w1:T ) for the full inference approach or smoothing.\n\nWe introduce an advantageous prediction parameterization as pγt(xt|xt−1, w1:t−1) = N (Ftxt−1, Qt), where xt−1 ∼ pγt−1(xt−1|w1:t−1) = N (x+ t−1). Then, pγt(xt|w1:t−1) = t + Qt) = N (x− N (Ftx+ t ) is obtained by marginalizing out xt−1 and the Gaussianity of pγt(xt|w1:t−1) results from the Gaussianity of prediction parameterization. By having pγt(xt|w1:t−1) and observing wt, filtering parameterization is introduced as:\n\nt−1, FtΣ+\n\nt−1, Σ+\n\nt , Σ−\n\nt−1FT\n\npγt(xt|w1:t) = N\n\n(cid:18)\n\nx−\n\nt + Kt[wt − Htx−\n\nt ], Σ−\n\nt − Kt[HtΣ−\n\nt HT\n\nt + Rt]KT\n\nt\n\n(cid:19)\n\n= N (cid:0)x+\n\nt , Σ+\n\nt\n\n(cid:1) (1)\n\nwhere Kt is KG. After observing all transferred observations w1:T , one can do backward induction and propagate to the previous states using the chain rule. This procedure, known as smoothing, can be parameterized as:\n\npγt(xt|w1:T ) = N\n\n(cid:18)\n\nx+\n\nt + Jt[xt+1|T − Ft+1x+\n\nt ], Σ+\n\nt + Jt\n\n(cid:2)Σt+1|T − Σ−\n\nt+1\n\n(cid:3)JT\n\nt\n\n(cid:19)\n\n(2)\n\nwhere Jt is SG and we use short handed notation N (xt|T , Σt|T ) instead of (2) . These parameterizations give us some insight to 1-illustrate a tractable way to construct pγ(x|w) and accordingly obtain the posterior and smoothened states, based on which o+ is constructed and 2- appropriately modeling γ and KG(SG) with neural networks.\n\nTo construct the KG and SG networks, we have to find appropriate inputs containing related information to attain the KG and SG. In (1) and (2), KG and SG are given by (3) and (4), respectively.\n\nKt = Σ− t HT t+1.(cid:2)Ft+1Σ+\n\nJt = Σ+\n\nt FT\n\nt .(cid:2)HtΣ−\n\nt HT\n\nt + Rt\n\n(cid:3)−1\n\n∝ (Σ−\n\nt , Rt)\n\nt FT\n\nt+1 + Qt+1\n\n(cid:3)−1\n\n= Σ+\n\nt FT\n\nt+1Σ−\n\nt+1 ∝ Σ−\n\nt+1\n\n(3)\n\n(4)\n\n(3) is proportional to the prior covariance at time t, Σ− t , and the observation noise matrix, Rt, while (4) is proportional to prior covariance matrix at time t+1, Σ− t+1. Our encoder(MLP) directly maps the observation noise matrix from the observation space, but the state covariance is a recursive function of previous states. Consequently, we consider GRU KG and GRU SG which are networks including\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nt ), Rt] and f (Σ−\n\nGRU that map [f (Σ− t+1) to the KG and SG, respectively. GRU KG considers Rt, a diagonal matrix with rt elements in figure 4, as a part of its input to incorporate the effects of observation noise. In the case of high dimensional state space, due to the high dimension of Σ− t and Σ− t+1 , f is a convolutional layer with pooling to extract the valuable information of the covariance matrix that reduces its size, while for the low dimension of Σ− t+1, f is the identity function.\n\nt and Σ−\n\nLearning The Process Noise.\n\nIn the filtering procedure, the process noise in time t is obtained as\n\nQt = Σ−\n\nt − FtΣ+\n\nt−1FT t .\n\n(5)\n\nwhere Σ− time t. It is shown that Qt can be written as a function of Ft as\n\nt , Ft and Σ+\n\nt−1 are prior state covariance, transition matrix and posterior state covariance at\n\nQt = Σ−\n\nt − FtΣ+\n\nt−1FT\n\nt = Σ−\n\nt − Ft\n\n(cid:2)Σ−\n\nt−1 − Kt−1[Ht−1Σ−\n\nt−1HT\n\nt−1 + Rt−1]−1KT\n\nt−1\n\n(cid:3)FT\n\nt\n\n(6)\n\nwhile the derivations are rather lengthy, therefore, we refer to the appendix materials A.3. From (32), the relation of the process noise with the transition matrix indicates that Ft can possess the effects of Qt if we learn it in an appropriate manner. ˆFt(Qt) notation means that the learned transition matrix ˆFt comprises the effects of Qt, while for the simplicity we use ˆFt abbreviation. Therefore, it is possible to rewrite (5) as\n\nΣ−\n\nt = ˆFtΣ+\n\nt−1\n\nˆFT t .\n\n(7)\n\nAnother way to have a a meaningful inference about the process noise matrix is to obtain it from (30) as a recursive function of x+ t−1 and Qt−1. Intuitively, g function in (30) that we call it Q Network, can be implemented by a memory cell, e.g., a GRU cell, to keep the past status of Q ,however, it increases the complexity of the model. Equivalently, one can obtain Qt directly from x+ t−1 with MLP as stated in (32). Both of these solutions can be utilized when the dynamics are known, i.e. we cannot learn the effects of Qt jointly with Ft as Ft is not trainable.\n\nPrediction Step. transition, the next priors are obtained from the current posterior by\n\nSimilar to the model based Kalman Filter, by using dynamics of the system and\n\nx−\n\nt = ˆFtx+\n\nt−1 , Σ−\n\nt = ˆFtΣ+\n\nt−1\n\nˆFT\n\nt\n\n(8)\n\nwhere ˆFt is the learned transition matrix comprises the effects of the process noise from previous section. By which, it is feasible to predict state mean and the state covariance matrix.\n\nTo obtain the next posteriors based on the new observation (wt, rt), i.e. the Filtering Step. output of e(.) in figure 2, we have to use the obtained KG matrix from GRU KG network and learned emission matrix ˆHt to complete updating the state mean vector and state covariance matrix. This procedure is given by t . ˆHT t = ˆHt.Σ− t = x− x+\n\nt , Mt = GRU KG(f (Σ− t = Σ−\n\nt MtMT t ], Σ+\n\nt + Kt.[wt − ˆHtx−\n\nT + Rt, Kt = Σ−\n\nt + Kt.S−\n\nt .KT t .\n\nt ), Rt)\n\nˆHT\n\nS−\n\n(10)\n\n(9)\n\nt\n\nIn addition to avoiding the matrix inversion that arises in the computation of Kalman gain and applying non-linearity to handle more complex dynamics, the architecture of KG network, GRU KG, can reduce the dimension of the input to its corresponding GRU cell, and thus reduces the total amount of parameters quadratically. Additionally, positive rt vector and Cholesky factor consideration, MtMT in (9), guarantee the positive definiteness of the resulted covariance matrices.\n\nt\n\nSmoothing Step. After obtaining filtered states (x+ 1:T ) in filtering step, we employ smoothing properties of Bayesian to get smoothed version of the states. In this stage, we use J1:T matrices obtained from GRU SG network, learned transition matrices ˆF1:T and filtered states (x+ 1:T ). The procedure in each smoothing step is given by:\n\n1:T , Σ+\n\n1:T , Σ+\n\nxt|T = x+\n\nt + Jt\n\nˆFT\n\nJt = Σ+ (cid:2)xt+1|T − ˆFt+1x+\n\nt\n\nt\n\nt+1NtNT\n\nt , Nt = GRU SG(cid:0)f (Σ− t + Jt\n\nt+1)(cid:1) (cid:0)Σt+1|T − ˆFt+1Σ+\n\n(cid:3), Σt|T = Σ+\n\nt\n\nˆFT\n\nt+1\n\n(cid:1)JT\n\nt\n\n(11)\n\n(12)\n\nwhere the first smoothing state is set to the last filtering state, i.e. (xT |T , ΣT |T ) = (x+\n\nT , Σ+\n\nT ) .\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nLearning Dynamics. We can model the dynamics in each time step t as a function of the transferred observations w1:t−1. However, conditioning on the noisy observations can distort the procedure of learning the dynamics. Instead, we use the state x+ t−1 in GSSM that includes the history of the system with considerable lower noise distortion to increase system’s noise robustness, where we generate time correlated noise in our experiments to show this robustness(see appendix A.5). In other words, original transition and emission equations, xt = f (xt−1) + qt and wt = h(xt) + rt, are modeled as xt = ˆFt(x+ t−1)xt + rt. We learn K state transition and emission matrices ˆFk and ˆHk, and combine each one with the state dependent coefficient αk(x+\n\nt−1)xt−1 + qt and wt = ˆHt(x+\n\nt−1).\n\nˆFt(x+\n\nt−1) =\n\nK (cid:88)\n\nk=1\n\nαk\n\nt (x+\n\nt−1) ˆFk t ,\n\nˆHt(x+\n\nt−1) =\n\nK (cid:88)\n\nk=1\n\nαk\n\nt (x+\n\nt−1) ˆHk\n\nt\n\n(13)\n\nwhere a separated neural network with softmax output is utilized to learn αk(x+ t−1) that we call it Dynamics Network. This formulation enables us to follow Bayesian methodology. Despite classic LGSSMs that are not able to learn the dynamics, e.g. EKF and UKF, the trainable dynamics in the GIN are function of the states. For the notation simplicity, we have used ( ˆFt, ˆHt) instead of ( ˆFt(x+\n\nt−1)) in the paper.\n\nt−1), ˆHt(x+\n\n5 FITTING\n\nFor the state estimation task, by implementing p(w1:T |o1:T ), p(xt|w1:T ) and p(st|xt) with encoder, smoothing parameterization and decoder, we maximise the log-likelihood of output p(st|o1:T ) = (cid:82) p(st|xt)p(xt|w1:T )p(w1:T |o1:T )dxtdw1:T ,where st is the estimated state, i.e. equal to o+ in figure 4. For the image imputation task, in addition to the state likelihood, we add the reconstruction pseudo-likelihood for inferring images by using Bernoulli distributions as p(it|xt), i.e. the decoder in figure 4 maps both state st and image it : o+ t = [it, st]. Further details of distribution assumptions and hyper parameter optimization can be found in the appendix A.4 and A.8. After training phase, forecasting desired number of time steps is applicable by plugging the new value xt = ˆFtxt−1 recursively in the model, and so on.\n\nt\n\nLikelihood for Inferring States. determine the log likelihood of the states as: (cid:18)\n\nL(s1:T ) =\n\nlog N\n\nst\n\nT (cid:88)\n\nt=1\n\nConsider the ground truth sequence is defined as s1:T. We\n\ndecmean(xt|T), deccovar(Σt|T)\n\n(14)\n\n(cid:19)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nwhere the decmean(.) and deccovar(.) determines those parts of the decoder that are used to obtain the state mean and state variance, respectively. We use Wishart distribution as a prior for our estimated covariance matrix, which pushes the estimated covariance toward a scale of identity matrix and the scale is a hyper parameter. Such prior prevents getting high log-likelihood due to the high uncertainty.\n\nLikelihood for inferring images. For the imputation task, consider the ground truth as the sequence of images and their corresponding states, which are defined as [s1:T , i1:T ] and the dimension of it is Do. We determine the log likelihood:\n\nL(o+\n\n1:T ) = L(s1:T ) + λ\n\nT (cid:88)\n\nDo(cid:88)\n\nt=1\n\nk=0\n\ni(k)\n\nt\n\nlog(cid:0)deck(xt|T)(cid:1) + (cid:0)1 − i(k)\n\nt\n\n(cid:1)log(1 − deck(xt|T)).\n\n(15)\n\ndeck(xt) defines the corresponding part of the decoder that maps the k-th pixel of it image and λ constant determines the importance of the reconstruction. The first term in RHS is obtained from (14) and we abbreviate the second term as L(i1:T ).\n\n6 EVALUATION AND EXPERIMENTS\n\nWe divide our experiments into two parts, first the tasks in which the observation space is high dimensional like sequence of images, and second the applications that the observation is in low dimension by itself so there is no need to include encoder for dimension reduction. The training algorithms of both cases are added in the appendix section A.11.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Double pendulum state estimation. (x1, x3) refers to the position of the first joint, while (x2, x4) is for the second joint.\n\nTable 2: Pendulum state estimation. By consider n = 3m, intuitively the last part of the state is dedicated to the acceleration information causing a more lieklihood.\n\nModel\n\nLSTM (units=50) LSTM (units=100)\n\nGRU (units=50) GRU (units=100)\n\nKVAE (n=2m) KVAE (n=3m)\n\nSEPos x1\n\n0.163 0.154\n\n0.189 0.164\n\n0.193 0.171\n\nSEPos x3\n\n0.171 0.147\n\n0.183 0.156\n\n0.188 0.159\n\nSEPos x2\n\n0.148 0.134\n\n0.179 0.162\n\n0.178 0.151\n\nSEPos x4\n\n0.167 0.152\n\n0.177 0.145\n\n0.149 0.162\n\nLog Likelihood\n\n3.901 ± 0.706 4.053 ± 0.565\n\n3.886 ± 0.369 3.976 ± 0.231\n\n3.679 ± 0.101 3.801 ± 0.116\n\nRKN (n=2m)\n\n0.134\n\n0.129\n\n0.139\n\n0.118\n\n4.176 ± 0.294\n\nLGSSMfilter(n=3m) LGSSMsmooth(n=3m)\n\nGINfilter(n=2m) GINfilter(n=3m) GINsmooth(n=2m) GINsmooth(n=3m)\n\n0.125 0.109\n\n0.115 0.093 0.091 0.079\n\n0.119 0.111\n\n0.109 0.091 0.104 0.083\n\n0.121 0.104\n\n0.119 0.098 0.101 0.085\n\n0.107 0.101\n\n0.109 0.089 0.092 0.077\n\n4.192 ± 0.127 4.231 ± 0.154\n\n4.224 ± 0.105 4.329 ± 0.151 4.308 ± 0.123 4.477 ± 0.168\n\nModel\n\nLSTM (units=25) LSTM (units=100)\n\nGRU (units=30) GRU (units=100)\n\nKVAE (n=2m) KVAE (n=3m)\n\nRKN (n=2m)\n\nLGSSMfilter LGSSMsmooth\n\nGINfilter(n=2m) GINfilter(n=3m) GINsmooth(n=2m) GINsmooth(n=3m)\n\nSEPos x1\n\n0.092 0.089\n\n0.095 0.091\n\n0.104 0.088\n\n0.078\n\n0.077 0.071\n\n0.073 0.067 0.065 0.059\n\nSEPos x2\n\n0.094 0.087\n\n0.089 0.089\n\n0.095 0.093\n\n0.075\n\n0.073 0.069\n\n0.07 0.066 0.067 0.057\n\nLog Likelihood\n\n5.891 ± 0.151 5.751 ± 0.215\n\n5.986 ± 0.168 5.698 ± 0.205\n\n5.786 ± 0.098 5.858 ± 0.113\n\n6.161 ± 0.23\n\n6.211 ± 0.265 6.242 ± 0.109\n\n6.192 ± 0.239 6.315 ± 0.220 6.292 ± 0.173 6.445 ± 0.165\n\n6.1 HIGH DIMENSIONAL OBSERVATION WITH LACK OF DYNAMICS\n\nWe include three high dimensional experiments. The first two experiments are single pendulum and double pendulum, where the dynamics of the latter one is more complicated. The last experiment is visual odometry task. Intuitive python code in appendix A.12.1.\n\n6.1.1 SINGLE PENDULUM AND DOUBLE PENDULUM\n\nThe inputs of the encoder are the images with size of 24 × 24. The angular velocity is disturbed by transition noise which follows Normal distribution with σ = 0.1 as its standard deviation at each step. In the pendulum experiment, we perform the filtering-smoothing by the GIN where the observation is distorted with high observation noise. Furthermore, we compare GIN with LGSSM, where the GRU cells are omitted from the GIN structure and classic filtering-smoothing equations are used, instead. The log-likelihood and squared error (SE) of positions for single and double pendulum are given in table 2 and 1, respectively. Generated samples from trained smooth-filter distributions are in appendix figures 16-33.\n\nBy randomly deleting the half of images from the generated sequences, we conduct the image imputation task to our model by predicting those missing parts, while the missingness applied to train and test are not same, but random. The results are in table 3 and 4.The GIN outperforms all the other models, although the variational inference models have more complex structures in KAVE\n\nTable 3: Image imputation task for the different models. Models contain boolean masks determining the available and missed images. For uninformed masks, a black image is considered as the input of the cell whenever the image is missed, which requires the model to infer the accurate dynamics for the generation. We conduct uninformed experiment as well.\n\nModel\n\nE2C SIN\n\nKVAE (informed smooth) KVAE (unformed smooth)\n\nEKVAE (informed smooth) EKVAE (unformed smooth)\n\nRKN (informed) RKN (uninformed)\n\nLGSSM(informed smooth) GIN (informed smooth) GIN (unformed smooth)\n\nLog Likelihood\n\n-95.539 ± 1.754 -101.268 ± 0.567\n\n-14.217 ± 0.236 -39.260 ± 5.399\n\n-12.897 ± 0.524 -29.246 ± 3.328\n\n-12.782 ± 0.0160 -12.788 ± 0.0142\n\n-12.695 ± 0.048 -12.215 ± 0.027 -12.246 ± 0.029\n\nFigure 5: Pendulum image imputation. Each figure, beginning from up to down, indicates the ground truth, uninformed observation and the imputation results of the GIN(smoothed). Missingness is applied randomly for train and test.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Image imputation for double pendulum.\n\nModel\n\nKVAE (informed smooth) KVAE (unformed smooth)\n\nEKVAE (informed smooth) EKVAE (unformed smooth)\n\nRKN (informed) RKN (uninformed)\n\nLGSSM(informed smooth) GIN (informed smooth) GIN (unformed smooth)\n\nLog Likelihood\n\n-15.917 ± 0.294 -38.544 ± 6.419\n\n-13.917 ± 0.414 -33.548 ± 4.516\n\n-13.832 ± 0.023 -13.898 ± 0.0191\n\n-13.775 ± 0.013 -13.284 ± 0.021 -13.351 ± 0.019\n\nFigure 6: Double pendulum image imputation. Each figure, beginning from up to down, indicates the ground truth, uninformed observation and the imputation results of the GIN(smoothed).\n\nand EKVAE. We include the results using the MSE as well, to illustrate that our approach is also competitive in prediction accuracy (See A.10).\n\n6.1.2 VISUAL ODOMETRY OF KITTI DATASET\n\nWe also evaluate the GIN with the higher dimensional observations for the visual odometry task on the KITTI dataset Geiger et al. (2012). This dataset consists of 11 separated image sequences with their corresponding labels. In order to extract the positional features, we use a feature extractor network proposed by Zhou et al. in Zhou et al. (2017). The obtained features are considered as the observations of the GIN, i.e. (w, r). Additionally, we compare the results with LSTM, GRU, DeepVO Wang et al. (2017) and KVAE. The results are in table 5 and figure 8, where the common evaluation scheme for the KITTI dataset is exploited. The results of the KVAE degrades substantially as we have to reduce the size of the transferred observation to prevent the complexity of matrix inversion.\n\n6.2 LOW DIMENSIONAL OBSERVATION WITH PRESENCE OF DYNAMICS\n\nWe conduct two experiments, Lorenz attractor problem and the real world dynamics NCLT dataset, where we are aware of the dynamics. Intuitive python code in appendix A.12.2. However, the GIN is able to deal the cases in which the dynamics are not known. To show this, we conduct additional experiment in the appendix, where we do not give the dynamics information of Lorenz attractor and NCLT dataset to the model, so that they will be learned(see figures 42-46).\n\n6.2.1 LORENZ ATRRACTOR\n\nThe Lorenz system is a system of ordinary differential equations that describes a non-linear dynamic system used for atmospheric convection. Due to nonlinear dynamics of this chaotic system (see A.6), it can be a good evaluation for the GIN cell. We evaluate the performance of the GIN on a trajectory of 5k length. Each point in the generated trajectories is distorted with an observation noise that follows Gaussian distribution with standard deviation σ = 0.5. The likelihood with Gaussian distribution is calculated and maximized in the training phase. The mean square error (MSE) of the test data for various number of train-\n\nFigure 7: MSE of Lorenz attractor. Table 5: Comparison of model performance on KITTI dataset. See 34, 35, 36, 37, 38 and 39 figures in A.9 for the visualization results.\n\nLSTM\n\nDeepVO\n\nKVAE\n\nLGSSM\n\nSeq\n\n03 04 05 06 07 10 mean\n\ntrel(%) 8.99 11.88 8.96 9.66 9.83 13.58 10.53\n\nrrel(◦) 4.55 3.44 3.43 2.8 5.48 3.49 3.87\n\nGRU\n\ntrel(%) 9.34 12.36 10.02 10.99 13.70 13.37 11.63\n\nrrel(◦) 3.81 2.89 3.43 3.22 6.52 3.25 3.85\n\ntrel(%) 8.49 7.19 2.62 5.42 3.91 8.11 5.96\n\ntrel(%) 12.14 13.17 11.47 10.93 12.73 14.79 12.53\n\nrrel(◦) 4.38 4.73 5.14 3.98 4.68 10.91 5.63\n\ntrel(%) 7.51 9.12 6.11 6.70 6.59 9.32 7.55\n\nrrel(◦) 3.98 2.64 3.21 3.51 3.49 2.90 3.28\n\nGIN\n\ntrel(%) 6.98 9.14 4.38 6.14 7.21 8.37 7.03\n\nrrel(◦) 3.27 2.28 2.51 2.90 2.98 2.59 2.75\n\nrrel(◦) 6.89 6.97 3.61 5.82 4.60 8.83 6.12\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 8: Generated samples from smoothing distribution for the joint position (x1, x2), equivalent to (o1+, o2+) in figure 4, at 100-th time step of visual odometry experiment. The ground truth is shown with a black point.\n\ning samples are depicted in figure 7. Hybrid is a graphical GNN based model Garcia Satorras et al. (2019) and DSSM Rangapuram et al. (2018) is a version of LGSSM using LSTM cells.\n\nDue to the non-linearity of the dynamics of this system, LGSSM has to use linearization and then use the linearized dynamics to model the transition. The DSSM model performs better for lager amount of data (>10K) because it needs to learn the dynamics. The results of the Hybrid GNN and the GIN are similar, while the results of the GIN are slightly improved. Although, the core of both models is based on the GRU cell, this enhancement may come from the structure of the GIN that learns the observation and process noises separately. Inferred trajectories are in figure 1.\n\n6.2.2 REAL WORLD DYNAMICS: MICHIGAN NCLT DATASET\n\nTo evaluate the performance of the GIN on a real world dataset, the Michigan NCLT dataset CarlevarisBianco et al. (2016) is utilized that encompasses a collection of navigation data gathered by a segwey robot moving inside of the University of Michigan’s North Campus. The states in each time, xt ∈ R4, comprise the position and the velocity in each direction and the observations, yt ∈ R2, include noisy positions. The ultimate purpose is to localize the real position of the segway robot, while only the noisy GPS observations are available. We apply the GIN to find the current location of the segway robot. In this experiment, we randomly select the session 201201-22 captured in a cloudy situation with the length of 6.1 Km. By sampling with 1Hz and removing the unstable GPS observations, 4280 time steps are achieved. For the dynamics of the system, we consider a uniform motion pattern with a constant velocity (see A.7). The training procedure is completed by maximizing the likelihood with Gaussian distribution assumption. The mean squared error of each approach for the test set are mentioned in the table 6, where the GIN (73.12 ± 2.21 MSE) outperforms other approaches.In summary, this experiment indicates that the GIN can generalize with good performance to a real world dataset.\n\nGIN(smooth) Hybrid GNN KalmanNet DSSM Vanilla RNN LGSSM Observation\n\n18.64±0.13 20.73± 0.21 22.2±0.17 29.54±0.58 40.21±0.52 24.38±0.17 25.47±0.08\n\nTable 6: MSE for NCLT experiment.\n\nMSE[dB]\n\nModel\n\n7 CONCLUSION\n\nThe GIN, an approach for representation learning in both high and low dimensional SSMs, is introduced in this paper. The data flow is conducted by Bayesian filtering-smoothing, while, due to the usage of GRU based KG and SG network, the computational issues are tackled resulting in an efficient model with numerical stable results. In the presence of the dynamics, the GIN directly use them, otherwise it directly learns them in an end to end manner, which makes the GIN as a HW model with strong system identification abilities. Insightful representation for the uncertainty of the predictions is incorporated in this approach, while it outperforms its counterparts including LSTMs, GRUs and several generative models with variational inferences.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nBIBLIOGRAPHY\n\nEvan Archer, Il Memming Park, Lars Buesing, John Cunningham, and Liam Paninski. Black box\n\nvariational inference for state space models. arXiv preprint arXiv:1511.07367, 2015.\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nPhilipp Becker, Harit Pandya, Gregor Gebhardt, Cheng Zhao, C James Taylor, and Gerhard Neumann. Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces. In International Conference on Machine Learning, pp. 544–552. PMLR, 2019.\n\nNicholas Carlevaris-Bianco, Arash K Ushani, and Ryan M Eustice. University of michigan north campus long-term vision and lidar dataset. The International Journal of Robotics Research, 35(9): 1023–1035, 2016.\n\nKyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\n\nMarco Fraccaro, Simon Kamronn, Ulrich Paquet, and Ole Winther. A disentangled recognition and nonlinear dynamics model for unsupervised learning. arXiv preprint arXiv:1710.05741, 2017.\n\nRoger Frigola, Fredrik Lindsten, Thomas B Schön, and Carl Edward Rasmussen. Bayesian inference and learning in gaussian process state-space models with particle mcmc. Advances in neural information processing systems, 26, 2013.\n\nVictor Garcia Satorras, Zeynep Akata, and Max Welling. Combining generative and discriminative models for hybrid inference. Advances in Neural Information Processing Systems, 32, 2019.\n\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3354–3361. IEEE, 2012.\n\nZoubin Ghahramani and Geoffrey E Hinton. Variational learning for switching state-space models.\n\nNeural computation, 12(4):831–864, 2000.\n\nAmir Ghalamzan, Kiyanoush Nazari, Hamidreza Hashempour, and Fangxun Zhong. Deep-lfd: deep\n\nrobot learning from demonstrations. Software Impacts, 9:100087, 2021.\n\nP Gilabert, Gabriel Montoro, and E Bertran. On the wiener and hammerstein models for power amplifier predistortion. In 2005 Asia-Pacific Microwave Conference Proceedings, volume 2, pp. 4–pp. IEEE, 2005.\n\nTuomas Haarnoja, Anurag Ajay, Sergey Levine, and Pieter Abbeel. Backprop kf: Learning discriminative deterministic state estimators. In Advances in neural information processing systems, pp. 4376–4384, 2016.\n\nHamidreza Hashempour, Kiyanoush Nazari, Fangxun Zhong, et al. A data-set of piercing needle through deformable objects for deep learning from demonstrations. arXiv preprint arXiv:2012.02458, 2020.\n\nSepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nMaximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAlexej Klushyn, Richard Kurle, Maximilian Soelch, Botond Cseke, and Patrick van der Smagt. Latent matters: Learning deep state-space models. Advances in Neural Information Processing Systems, 34, 2021.\n\nJonathan Ko and Dieter Fox. Learning gp-bayesfilters via gaussian process latent variable models.\n\nAutonomous Robots, 30(1):3–23, 2011.\n\nRahul Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state space models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.\n\nYingzhen Li and Stephan Mandt. Disentangled sequential autoencoder.\n\narXiv preprint\n\narXiv:1803.02991, 2018.\n\nScott Linderman, Matthew Johnson, Andrew Miller, Ryan Adams, David Blei, and Liam Paninski. Bayesian learning and inference in recurrent switching linear dynamical systems. In Artificial Intelligence and Statistics, pp. 914–922. PMLR, 2017.\n\nChristian Naesseth, Scott Linderman, Rajesh Ranganath, and David Blei. Variational sequential monte carlo. In International conference on artificial intelligence and statistics, pp. 968–977. PMLR, 2018.\n\nSyama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. Advances in neural information processing systems, 31, 2018.\n\nHerbert E Rauch, F Tung, and Charlotte T Striebel. Maximum likelihood estimates of linear dynamic\n\nsystems. AIAA journal, 3(8):1445–1450, 1965.\n\nGuy Revach, Nir Shlezinger, Xiaoyong Ni, Adria Lopez Escoriza, Ruud JG van Sloun, and Yonina C Eldar. Kalmannet: Neural network aided kalman filtering for partially known dynamics. arXiv preprint arXiv:2107.10043, 2021.\n\nDavid Ruhe and Patrick Forré. Self-supervised inference in state-space models. arXiv preprint\n\narXiv:2107.13349, 2021.\n\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3): 1181–1191, 2020.\n\nMaarten Schoukens and Koen Tiels. Identification of block-oriented nonlinear systems starting from\n\nlinear approximations: A survey. Automatica, 85:272–292, 2017.\n\nNiklas Wahlström, Thomas B Schön, and Marc Peter Deisenroth. From pixels to torques: Policy\n\nlearning with deep dynamical models. arXiv preprint arXiv:1502.02251, 2015.\n\nJack M Wang, David J Fleet, and Aaron Hertzmann. Gaussian process dynamical models for human motion. IEEE transactions on pattern analysis and machine intelligence, 30(2):283–298, 2007.\n\nSen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 2043–2050. IEEE, 2017.\n\nManuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. arXiv preprint arXiv:1506.07365, 2015.\n\nPaul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the\n\nIEEE, 78(10):1550–1560, 1990.\n\nRobert Wilson and Leif Finkel. A neural implementation of the kalman filter. Advances in neural\n\ninformation processing systems, 22:2062–2070, 2009.\n\nNarri Yadaiah and G Sowmya. Neural network based state estimation of dynamical systems. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, pp. 1042–1049. IEEE, 2006.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nTinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1851–1858, 2017.\n\nA APPENDIX\n\nA.1 GAUSSIAN STATE SPACE MODELS\n\nIn order to model the vectors of time series w = w1:T = [w1, ..., wT ], Gaussian state space models (GSSMs) are commonly applied due to their filtering-smoothing ability. In fact, GSSMs model the first-order Markov process on the state space x = [x1, ..., xT ], which can also include the external control input u = [u1, ..., uT ] by multivariate normality assumption of the state\n\npγt(xt|xt−1, ut) = N (xt; Ftxt−1 + Btut, Q),\n\npγt(wt|xt) = N (wt; Htzt, R).\n\n(16)\n\nFor the cases, which are not controlled via external input, Bt matrix is simply 0 matrix. By Defining γt as parameters which explain how the state state changes during the time, it contains the information of Ft, Bt and Ht which are the state transition, control and emission matrices. In each step, the procedure is distorted via Q and R that are process noise and observation noise, respectively. It is common to initial the first state x1 ∼ N (0, Σ0), then the joint probability distribution of the GSSM is\n\npγ(w, x|u) = pγ(w|x)pγ(x|u) =\n\nT (cid:89)\n\nt=1\n\npγt(wt|xt).p(x1)\n\nT (cid:89)\n\nt=2\n\npγt(xt|xt−1, ut).\n\n(17)\n\nGSSMs have substantial properties that we can utilize. Filtering and smoothing are among these properties which allow us to obtain the filtered and smoothed posterior based on the priors and observations. By applying classic Bayesian properties, we can have a strong tool to handle the missing data in the image imputation task.\n\nA.2 FILTERING AND SMOOTHING PARAMETERIZATION\n\nThe idea of Kalman filter applies two iterative steps, in the former one a prediction is made by the prior state information, while in the latter one an update is done based on the obtained observation. By normality assumption of known additive process and observation noise, the filter can go through the two mentioned steps. In the prediction step, the filter uses the transition matrix F to estimate the next priors (x−\n\nt+1) which are the estimate of the the next states without any observation.\n\nt+1, Σ−\n\nx−\n\nt+1 = Fx+\n\nt , and Σ−\n\nt+1 = FΣ+\n\nt FT + Q, and Q = σ2\n\ntransI\n\n(18)\n\nIn the presence of new observation, the Kalman filter idea goes through the second step and modifies the predicted prior based on the new observation and emission matrix H that results in the next posterior (x+\n\nt+1, Σ+\n\nt+1).\n\nKt+1 = Σ−\n\nt+1HT (cid:0)HΣ−\n\nt+1HT + R(cid:1)−1\n\n, and\n\n(19)\n\nx+\n\nt+1 = x−\n\nt+1 + Σ−\n\nt+1HT (cid:0)HΣ−\n\nt+1HT + R(cid:1)−1\n\n(wt − Hx−\n\nt+1) = x−\n\nt+1 + Kt+1(wt − Hx−\n\nt+1), (20)\n\nΣ+\n\nt+1 = Σ−\n\nt+1 − Σ−\n\nt+1HT (cid:0)HΣ−\n\nt+1HT + R(cid:1)−1\n\nHΣ−\n\nt+1.\n\n(21)\n\nThe whole observation update procedure can be considered as a weighted mean between the the next prior, that comes from state update, and new observation, where this weighting is a function of Q and R that has uncertainty nature.\n\nWe derive smoothing parameterization, where the key idea is to use Markov property, which states that xt is independent of future observations wt+1:T as long as xt+1 is known. However, we are not\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\naware of xt+1, but there is a distribution over it. So by conditioning on xt+1 and then marginalizing out it is possible to obtain xt conditioned on w1:T .\n\np(xt|w1:T ) =\n\n=\n\n(cid:90)\n\n(cid:90)\n\np(xt|xt+1, w1:T )p(xt+1|w1:T )dxt+1\n\np(xt|xt+1, w1:t,(cid:24)(cid:24)(cid:24)(cid:24)wt+1:T )p(xt+1|w1:T )dxt+1\n\nBy using induction and and smoothed distribution for t + 1:\n\nwe calculate the filtered two-slice distribution as follows:\n\np(xt+1|w1:T ) = N (xt+1|T , Σt+1|T )\n\n.p(xt, xt+1|w1:t) = N\n\n(cid:18) (cid:18) x+ x−\n\nt\n\nt+1\n\n(cid:19)\n\n,\n\n(cid:18) Σ+\n\nt\n\nFt+1Σ+\n\nt\n\n(cid:19) (cid:19)\n\nΣ+\n\nt FT Σ−\n\nt+1\n\nt+1\n\nby using Gaussian conditioning we have:\n\np(xt|xt+1, w1:t) = N (x+\n\nt + Jt\n\n(cid:0)xt+1 − Ft+1x+\n\nt\n\n(cid:1), Σ+\n\nt − JtΣ−\n\nt+1JT t )\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\nwhere Jt = Σ+ iterated expectation and covariance:\n\nt Ft+1[Σ−\n\nt+1]−1. We calculate the smoothed distribution for t using the rules of\n\n(cid:3)\n\nxt|T = E(cid:2)E[xt|xt+1, w1:T ] |w1:T = E(cid:2)E[xt|xt+1, w1:t] |w1:T (cid:3) = E(cid:2)x+\n\nt + Jt(xt+1 − Ft+1x+ t + Jt(xt+1|T − Ft+1x+ t )\n\n= x+\n\nt ) |w1:T\n\n(cid:3)\n\n(26)\n\nΣt|T = cov(cid:2)E[xt|xt+1, w1:T ] |w1:T = cov(cid:2)E[xt|xt+1, w1:t] |w1:T = cov(cid:2)x+ = Jtcov(cid:2)xt+1 − Ft+1x+ = JtΣt+1|T JT\n\nt + Jt(xt+1 − Ft+1x+\n\n|w1:T\n\nt\n\nt + Σ+ (cid:0)Σt+1|T − Σ−\n\nt − JtΣ− t+1JT (cid:1)JT t .\n\nt+1\n\nt\n\n= Σ+\n\nt + Jt\n\n(cid:3) + E(cid:2)cov[xt|xt+1, w1:T ] |w1:T (cid:3) + E(cid:2)cov[xt|xt+1, w1:t] |w1:T (cid:3) (cid:3) + E(cid:2)Σ+\n\nt+1JT\n\nt\n\n(cid:3)\n\nt ) |w1:T (cid:3)JT\n\nt + Σ+\n\nt − JtΣ−\n\nt − JtΣ− t+1JT\n\nt\n\n(cid:3)\n\n|w1:T\n\n(27)\n\nA.3 PROCESS NOISE MATRIX\n\nAs stated in (18), we can elaborate the process noise matrix at time t in more details\n\nQt = Σ−\n\nt − FtΣ+\n\nt−1FT\n\nt = Σ−\n\nt − Ft\n\ncombining (18) into (28) results in\n\n(cid:2)Σ−\n\nt−1 − Kt−1[Ht−1Σ−\n\nt−1HT\n\nt−1 + Rt−1]−1KT\n\nt−1\n\n(cid:3)FT\n\nt\n\n(28)\n\nQt = Σ−\n\nt − Ft\n\n(cid:2)[Ft−1Σ+\n\nt−2FT\n\nt−1 + Qt−1]\n\n−Kt−1[Ht−1[Ft−1Σ+\n\nt−2FT\n\nt−1 + Qt−1]HT\n\nt−1 + Rt−1]−1KT\n\nt−1\n\n(cid:3)FT\n\nt\n\n(29)\n\nwhich is a function of Ft, Qt−1, Ft−1 and Ht−1. In the GIN, ˆFt and ˆHt are learned by the Dynamics Network with the input of x+ t−1 is derived as a function of both Ft−1 and Ht−1,\n\nt−1 . From (20), x+\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Without recurrent dependency on qt.\n\n(b) With recurrent dependency on qt.\n\nFigure 9: Graphical models for low dimensional observations experiments.\n\nmeaning the learned ˆFt carries the information of both Ht−1 and Ft−1. Therefore, one can rewrite the equation (29) as\n\n(cid:18)\n\nQt = g\n\nˆFt\n\n(cid:0)x+\n\nt−1\n\n(cid:1), Qt−1\n\n(cid:19)\n\n, where\n\nˆFt = Dynamics Network\n\nx+\n\nt−1\n\n(cid:18)\n\n(cid:0)Ht−1, Ft−1\n\n(cid:1)\n\n(cid:19) .\n\n(30)\n\nwhere g is a nonlinear function mapping x+ choice of structure is in figure 9b. It is possible to go one step further and simplify x+ has Σ−\n\nt−1 and Qt−1 to Qt and the graphical model for such t−1 more, as it\n\nt−1 term in (20), combining it with (18) results in\n\nx+\n\nt−1 = x−\n\nt−1 + [Ft−1Σ+\n\nt−2FT (cid:0)Ht−1[Ft−1Σ+\n\nHT\n\nt−1\n\nt−1 + Qt−1]\n\nt−2FT\n\nt−1 + Qt−1]HT\n\nt−1 + Rt−1\n\n(cid:1)−1\n\n(wt − Ht−1x−\n\nt−1)\n\n(31)\n\nindicating that not only Ft−1 and Ht−1, but also Qt−1 is included in x+ written solely as a function of x+\n\nt−1 and the graphical model for such choice is in figure 9a.\n\nt−1, meaning that Qt can be\n\nQt = g\n\n(cid:18)\n\n(cid:19)\n\nˆFt\n\n(cid:0)x+\n\nt−1\n\n(cid:1)\n\n, where\n\nˆFt = Dynamics Network\n\nx+\n\nt−1\n\n(cid:18)\n\n(cid:0)Ht−1, Ft−1, Qt−1\n\n(cid:1)\n\n(cid:19) .\n\n(32)\n\nWe call g as Q Network, where g can be modeled by a MLP (32) or a recurrent network (30), based on the mentioned explanations. In figure 11, it is shown how the Q Network is integrated into the whole model structure.\n\nA.4 OUTPUT DISTRIBUTION\n\nIn the case of grayscale images, consider each pixel, yi, is one or zero with the probability of pi or 1 − pi respectively, meaning that P (Y = y) = py(1 − p)1−y. By re-writing the probability equation into the exponential families form\n\nfθ(y) = h(y).exp(cid:0)θ.y − ψ(θ)(cid:1) → elog(py(1−p)1−y) = ey log( p\n\n1−p )+ log(1−p)\n\n(33)\n\nand by choosing θ = log( p 1+e−θ . It means that by considering θ as the last layer of the decoder and applying a softmax layer, p is obtained. Equivalently, one can calculate the deviance between real p and estimation of it, ˆp, which is given by\n\n1−p ) and ψ(θ) = log(1 − p), we can obtain p = 1\n\nD(p, ˆp) = (cid:2)p log(\n\np ˆp\n\n) + (1 − p)log(\n\n1 − p 1 − ˆp\n\n)(cid:3)\n\n(34)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nand minimize the deviance with respect to ˆp as we did in (15).\n\nSimilarly, consider x, ˆxθ and θ as the ground truth state, estimated state and the model variables respectively, where the residual follows Gaussian distribution x = ˆxθ + ε ∼ N (ˆxθ, ˆσθ), where ˆσθ is the estimated variance. Then, the negative log likelihood is given by (35) as we obtained it in (14).\n\n−log(L) ∝\n\n1 2\n\nlog(ˆσθ) +\n\n(x − ˆxθ)2 2ˆσθ\n\n(35)\n\nA.5 NOISE GENERATION PROCESS\n\nIn the high dimensional observation experiments, to show the noise robustness of the system, we use time correlated noise generation scheme. It makes the noise factors correlated over time by introducing a sequence of factors ft of the same length of the data sequence. Let f0 ∼ U(0, 1) and ft+1 = min(max(0, ft + rt), 1) with rt ∼ U(−0.2, 0.2), where f0 is the initialized factor and U is the uniform distribution. Then by defining two thresholds, t1 ∼ U(0, 0.25) and t2 ∼ U(0.75, 1), ft < t1 are set to 0 and ft > t2 are set to 1 and the rest are splitted linearly within the range of [0, 1]. The t-th obtained observation is given by ot = ftit + (1 − ft)ipn , where the it is the t-th true image and ipn\n\nis the t-th generated pure noise.\n\nt\n\nt\n\nA.6 LORENZ ATTRACTOR DYNAMICS\n\nThere are three differential equations that model a Lorenz system, x the convection rate, y the horizontal temperature variation and z the vertical temperature variation.\n\ndx dt\n\n= σ(y − x),\n\ndy dt\n\n= x(ρ − z) − y,\n\ndz dt\n\n= xy − βz\n\n(36)\n\nwhere the constant values σ, ρ and β are 10, 28 and − 8 3 , respectively. To construct a trajectory we use Lorenz system equations (36) with dt = 10−5, then we sample from it with the step time of ∆t = 0.01.\n\nBased on the equations of the system (36), the state is st = [xt, yt, zt] and we can write the dynamics of the system as At and obtain the transition matrix Exp[At] = Ft. To achieve this, we use the Taylor expansion of Exp function with 5 degrees.\n\n ̇st = Atst =\n\n\n\n\n\n−10\n\n10 28 − z −1\n\n0 0\n0 − 8 3\n\ny\n\n\n\n\n\n(cid:35)\n\n(cid:34)x y\nz\n\n, and\n\nFt = Exp[At] = I +\n\nJ (cid:88)\n\nj=1\n\n(At.∆t)j j!\n\n(37)\n\nwhere J is the degrees of expansion and I is the identity matrix. For the emission matrix we use Ht = I and for process and observation noise standard deviation, we use Qt = 1 100 σ2I and Rt = σ2I, respectively.\n\nA.7 MOVEMENT MODEL DETAILS FOR THE NCLT EXPERIMENT\n\nWe assume that the segway robot is moved with a constant velocity, that the equations for such dynamics are given by\n\n∂p1 ∂t\n\n= v1,\n\n∂p2 ∂t\n\n= v2,\n\n∂v1 ∂t\n\n= 0,\n\n∂v2 ∂t\n\n= 0, xt = [p1, v1, p2, v2], yt = [p1, p2].\n\n(38)\n\nBy such assumptions for the motion’s equations the transition, process noise distribution, emission and measurement noise distribution matrices can be obtained by\n\nF =\n\n\n\n \n\n1 ∆t 1\n0 0\n0 0\n0\n\n\n\n\n\n , Q = σ2\n\n0 0\n0 0\n1 ∆t 1\n0\n\n\n\n \n\n∆t 0\n0 ∆t 0\n0\n\n0 0\n0 ∆t 0\n\n0 0\n0 0 ∆t\n\n\n\n  , H =\n\n(cid:20)1 0\n\n0 0\n\n0 1\n\n(cid:21)\n\n0 0\n\n,\n\n(39)\n\nR = λ2\n\n(cid:20)1 0\n\n(cid:21)\n\n0 1\n\n.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ∆t = 1 since the sampling frequency is 1Hz. Process and measurement variance parameters, σ and λ, are unknown that the model will learn them. we split the whole sequence into training, testing and validation folds with the length of 3600 ( 18 sequences of length T = 200) , 280 (1 sequence of length T = 280) and 400 (2 sequences of length T = 200), respectively.\n\nA.8 NETWORK STRUCTURE AND PARAMETERS\n\nIn all experiments, Adam optimizer Kingma & Ba (2014) has been used on NVIDIA GeForce GTX 1050 Ti. We conduct a grid search for finding the hyperparameters to rule out the possibility of the models being trained with the suboptimal hyperparameters. To find the initial learning rate, by conducting a grid search between 0.001 and 0.2 with the increment of 0.005, we select the best one among them that corresponds to the highest log-likelihood. With an initial learning rate of 0.006 and an exponential decay with rate of 0.9 every 10 epochs, we employ back propagation through time Werbos (1990) to compute the gradients as we deploy GRU cells in the structure. Layer normalization technique Ba et al. (2016) is used to stabilize the dynamics in the recurrent structure and normalize the filter response. Elu + 1 activation function, can ensure the positiveness of the diagonal elements of the process, noise and covariance matrices.\n\nIn order to prevent the model being stuck in the poor local minima, e.g. focusing on the reconstruction instead of learning the dynamics obtained by filtering-smoothing, we find it useful to use two training tricks for an end-to-end learning:\n\n1- Generating time correlated noisy sequences as consecutive observations, forces the model to\n\nlearn the dynamics instead of focusing on reconstruction, e.g. figure 13 and 15.\n\n2- For the first few epochs, only learn auto-encoder(MLPs) and globally learned parameters, e.g. F(k) and H(k), but not Dynamics Network parameters αt(xt−1). All the parameters are jointly learned, afterwards. This allows the system to learn good embedding and meaningful latent vectors at first, then learns how to employ K different dynamics variables.\n\nIn the lack of dynamics, for the low dimensional observations we use K = 5, while for the high dimensional observations we use K = 15 as they need to learn more complex dynamics. In general, if the GIN is flexible enough, tuning the parameters is not difficult as the GIN is capable to learn how to prune unused elements by the Dynamics Network.\n\nTo prevent the model being stuck into mode collapse, we provided two solutions:\n\n1- By introducing k sets of Fk, Hk, where each set of Fk, Hk models different dynamics, we introduce a loss term with a small constant factor which tries to increase the distance of each pair of Fk, Hk set. Intuitively, the presence of different dynamics can easily modify the states in each update. We found this method as a potential solution to prevent the model go through the mode collapse.\n\n2- Considering the negative distance of consecutive pairs of states as additional loss term with a small constant factor (the distance can be considered as euclidean difference of mean or KL of two consecutive states). Intuitively, this solution is forcing the states to not have overlap with each other and impose them to change in each update step.\n\nIn the simulation results, we have used the first option.\n\nA.8.1 PROPOSED ARCHITECTURE FOR HIGH AND LOW DIMENSIONAL OBSERVATIONS\n\nThe proposed structure to deal with high dimensional observations in the lack of the dynamics(the first three experiments in the paper) is shown in figure 10. While, the proposed structure to handle low dimensional observations in the presence of the dynamics(the last two experiments in the paper) is shown in the figure 11.\n\nA.8.2 EMPIRICAL RUNNING TIMES AND PARAMETERS\n\nWe present the number of parameters of the utilized cell structures in our experiments and their corresponding empirical running times for 1 epoch in the table 7 and 8. In the first row of each model\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Proposed architecture for operating high dimensional observations in the lack of dynamics.\n\nFigure 11: Proposed architecture for operating low dimensional observations in the presence of dynamics.\n\nstructure in the high dimensional experiments, we set the number of their parameters approximately equal to our GIN to indicate the outperformance of the GIN with the same number of the parameters, i.e. tables 1, 2 and 5, while we include the empirical running time and parameters for more complex structures as well. Extra running time of EM-variational approaches, like KVAE, is due to employing classic Bayesian equations because it increases the running time substantially when dealing with higher dimensional observations, however, the GIN circumvent this difficulty. The number of the parameters of the GIN are noticeably lower than other memory cells, e.g. LSTM and GRU, and EM-variational methods as we convert high dimensional sparse covariance matrices into lower dimensional covariance matrices by employing convolutional operator. It allows us to reduce the parameters of internal GRU cells quadratically.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nA.8.3 QUALITATIVE COMPARISON OF THE GIN TO RECENT RELATED WORK.\n\nIn table 9, we make a comparison to show whether algorithms are able to handle high and low dimensional observations, learn dynamics, use available-partial dynamics, estimate state appropriately, provide model’s uncertainty estimates handling noisy data, handle missing data and perform direct optimization. Classic LGSSMs, e.g. EKF and UKF, work based on the linearization of the transition and emission equations and apply classic Bayesian updates over the linearized system with respect to the states. In other words, (F, H) in the classic LGSSMs are not data-deriven nor trainable. Despite classic LGSSMs, in the GIN we use a data-driven based network to learn dynamics, i.e. Dynamics Network in the paper.\n\nA.8.4 SINGLE-PENDULUM AND DOUBLE-PENDULUM EXPERIMENTS\n\nDATA GENERATION.\n\nDataset consists of 1000 train, 100 valid and 100 test sequences with the length of 150. The sequences are distorted via generated noise, while in the informed imputation task half of the images are removed and boolean flags indicating the availability of the observations are passed to the cell instead. If the imputation task is in uninformed type, black images are considered as the observations instead of informing the cell with boolean flags.\n\nENCODER/DECODER AND THE DYNAMICS NETWORK ARCHITECTURE\n\nTo design the dynamics network, we use a MLP including 60 hidden units with Relu activation function and a softmax activation for the last layer. The state mean, with size of n, and number of the bases, with size of k, are the input and output of the dynamics, respectively. The structures of the encoder and decoder are in the table 10. In the table 10, m is the transferred observation dimension that various values for this parameter are taken into account in the results. In the state estimation tasks, out dim is 4 and 8 for the single-pendulum and double-pendulum experiment, respectively. For the imputation task, number of the hidden units of the KG and SG network is set to 40 and 30, respectively. The convolutional layer applied over the covariance matrix has 8 filters with kernel size of 5.\n\nA.8.5 LORENZ ATTRACTOR AND NCLT EXPERIMENTS\n\nIn these two experiments that we have the knowledge of the dynamics, we employ a fully connected with the observations as its input and output dimension of 3 and 2 for Lorenz attractor and NCLT experiments, respectively, to obtain the observation noise, r. The activation function is Elu + 1. Similarly another fully connected with the posterior state as its input and output dimension of 3 and 2 for Lorenz attractor and NCLT experiments, respectively, to attain the uncertainty estimates, o+ σ . To estimate the process noise matrix, a fully connect with the posterior state as the input and Elu + 1 activation function is used. Similarly, a GRU cell that maps the posterior states to the process noise matrix with 10 hidden units can be used.\n\nTable 7: Empirical running times and parameters of high-low dimensional experiments.\n\nCell\n\nLSTM (units=25) LSTM (units=50) LSTM (units=100) GRU (units=30) GRU (units=50) GRU (units=100) KVAE (n=40) KVAE (n=60) RKN (n=100) LGSSM (n=30) LGSSM (n=45) GIN (n=30) GIN (n=45)\n\nSingle Pend\n\nDouble Pend\n\nKITTI\n\nTable 8: Low dimensional experiments.\n\nParam ∼18k ∼36k ∼76k ∼18k ∼27k ∼57k ∼25k ∼36k ∼25k ∼12k ∼15k ∼18k ∼25k\n\nT/E ∼56s ∼70s ∼98s ∼61s ∼65s ∼86s ∼95s ∼114s ∼57s ∼82s ∼98s ∼55s ∼59s\n\nParam ∼18k ∼36k ∼76k ∼18k ∼27k ∼57k ∼25k ∼36k ∼25k ∼12k ∼15k ∼18k ∼25k\n\nT/E ∼56s ∼71s ∼96s ∼62s ∼67s ∼85s ∼97s ∼111s ∼58s ∼84s ∼97s ∼55s ∼58s\n\nParam ∼45k ∼70k ∼120k ∼42k ∼53k ∼90k ∼62k ∼80k ∼45k ∼30k ∼36k ∼42k ∼48k\n\nT/E ∼83s ∼95s ∼131s ∼79s ∼84s ∼111s ∼141s ∼165s ∼79s ∼117s ∼136s ∼80s ∼83s\n\n18\n\nCell\n\nKalmanNet GNN RNN DSSM LGSSM GIN\n\nLorenz\n\nParam\n\nT/E\n\n∼10k\n\n∼35s\n\n∼40k ∼6k ∼9k\n\n∼76s ∼20s ∼28s\n\nNCLT\n\nParam ∼30k ∼10k ∼40k ∼40k ∼6k ∼9k\n\nT/E ∼65s ∼40s ∼79s ∼81s ∼22s ∼31s\n\nUnder review as a conference paper at ICLR 2023\n\nTable 9: Learning the dynamics in LGSSM is shown with ×/✓ because general LGSSMs, e.g. UKF and EKF, are not able to learn the dynamics. However, in our setting and parameterization we use a data driven-based network for obtaining (F, H) to make LGSSMs comparable with the GIN for high dimensional observation experiments.\n\nModel\n\nhigh-d\n\nlow-d\n\nlearn dynamics\n\nuse dynamics\n\nstate est\n\nuncertainty\n\nmissing-noise\n\ndir opt\n\nLSTM (Hochreiter & Schmidhuber, 1997) GRU (Cho et al., 2014)\n\nP2T (Wahlström et al., 2015) E2C (Watter et al., 2015)\n\nBB-VI (Archer et al., 2015) SIN (Krishnan et al., 2017) DVBF (Karl et al., 2016) VSMC (Naesseth et al., 2018) DSA (Li & Mandt, 2018) KVAE (Fraccaro et al., 2017) EKVAE (Klushyn et al., 2021)\n\nrSLSD Linderman et al. (2017) DeepAR Salinas et al. (2020) DSSM Rangapuram et al. (2018) HybridGNN Garcia Satorras et al. (2019) KalmanNet Revach et al. (2021) SSI Ruhe & Forré (2021)\n\nLGSSM GIN\n\n✓ ✓\n\n✓ ✓\n\n× ✓\n✓ ✓\n✓ ×\n×\n\n× ×\n× ×\n× ×\n\n× ✓\n\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n✓ ✓\n✓ ✓\n✓\n\n✓ ✓\n✓ ✓\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n✓ ✓\n✓ ✓\n✓\n\n× ✓\n✓ ×\n× ×\n\n×/✓ ✓\n\nA.9 VISUALIZATION AND THE IMPUTATION\n\n✓ ✓\n\n× ×\n\n× ×\n× ×\n× ×\n×\n\n✓ ×\n× ✓\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n\n✓ ×\n\n× ×\n× ×\n× ×\n×\n\n✓ ✓\n✓ ✓\n✓ ✓\n\n✓ ✓\n\n× ×\n\n× ✓\n\n✓ ✓\n✓ ✓\n✓ ✓\n✓\n\n✓ ✓\n✓ ×\n× ✓\n\n✓ ✓\n\n✓ ✓\n\n× ×\n\n✓ ✓\n✓ ✓\n× ✓\n✓\n\n× ×\n× ✓\n✓ ✓\n\n✓ ✓\n\n✓ ✓\n\n✓ ×\n\n× ×\n× ×\n× ×\n×\n\n× ✓\n✓ ✓\n✓ ✓\n\n✓ ✓\n\nGraphical results of informed, uninformed and noisy observations for image imputation task for both single and double pendulum experiments can be found in 12, 13, 14 and 15 figures. Inference for the trained smoothing and filtering distributions of all high dimensional experiments are in 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38 and 39 figures, where we generated samples from the smoothing distribution, f (xt|w1:T ), and the filtering distribution, f (xt|w1:t). Then we fit density on the generated samples. This visualization shows the effectiveness of the GIN in reducing the uncertainty of the estimates compare to LGSSM and KVAE. Finally, the results of NCLT experiment are in figure 47.\n\nTable 10: The structure of the encoder and decoder for single and double pendulum experiments.\n\nEncoder\n\nDecoder\n\n6 × 6 Conv, 12, max pooling 2 × 2, stride 2 × 2 LayerNormalizer() 4 × 4 Conv, 12, max pooling 2 × 2, stride 2 × 2 LayerNormalizer() fully connected: 40 w: fully connected: m, linear activation r: fully connected, Elu + 1 activation\n\n19\n\no+ o+ if\n\nx : fully connected: out dim, linear activation Σ: fully connected, Elu + 1 activation\n\nimputation task: fully connected: 144 6 × 6 Trns Conv, 16, stride 4 × 4 LayerNormalizer() 4 × 4 Trns Conv, 12, stride 2 × 2 LayerNormalizer() o+\n\ni : 1 × 1 Trns Conv, stride 1 × 1, softmax\n\nUnder review as a conference paper at ICLR 2023\n\nGround Truth\n\nGround Truth\n\nObserved Sequence\n\nObserved Sequence\n\nLGSSM(filter)\n\nLGSSM(filter)\n\nLGSSM(smooth)\n\nLGSSM(smooth)\n\nGIN(filter)\n\nGIN(filter)\n\nGIN(smooth)\n\nGIN(smooth)\n\nFigure 12: Informed(left column) and uninformed(right column) image imputation task for the single pendulum experiments.\n\nFigure 13: Image imputation task for the single pendulum experiment exposed to the noisy observations, where the generated noise has correlation with the time. Each figure, beginning from top to bottom, indicates the ground truth, noisy observation and the imputation results of the GIN.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nGround Truth\n\nGround Truth\n\nObserved Sequence\n\nObserved Sequence\n\nLGSSM(filter)\n\nLGSSM(filter)\n\nLGSSM(smooth)\n\nLGSSM(smooth)\n\nGIN(filter)\n\nGIN(filter)\n\nGIN(smooth)\n\nGIN(smooth)\n\nFigure 14: Informed(left column) and uninformed(right column) image imputation task for the double pendulum experiments.\n\nFigure 15: Image imputation task for the double pendulum experiment exposed to the noisy observations, where the generated noise has correlation with the time. Each figure, beginning from top to bottom, indicates the ground truth, noisy observation and the imputation results of the GIN.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 16: Inference for the single pendulum x1 position at 100-th time step. Generated samples from smoothened distribution, f (x1100|w1:150), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x1Pos 100|w1:150) is the ground truth state with distribution of δ(x1100 − 0.7). We calculate the sample mean and fit a distribution on the samples for further visualization and comparison purpose.\n\nFigure 17: Inference for the single pendulum x2 position at 100-th time step. Generated samples from smoothened distribution, f (x2100|w1:150), trained by the GIN, LGSSM and KVAE, respectively.\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 18: Generated samples from the trained smoothened joint distribution of the single pendulum position, (x1, x2), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 19: Inference for the single pendulum x1 position at 100-th time step. Generated samples from filter distribution, f (x1100|w1:100), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x1Pos\n\n100|w1:100) is the ground truth state with distribution of δ(x1100 − 0.7).\n\nFigure 20: Inference for the single pendulum x2 position at 100-th time step. Generated samples from filter distribution, f (x2100|w1:100), trained by the GIN, LGSSM and KVAE, respectively.\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 21: Generated samples from the trained filter joint distribution of the single pendulum position, (x1, x2), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 22: Inference for the double pendulum x1 position at 100-th time step. Generated samples from smoothened distribution, f (x1100|w1:150), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x1Pos\n\n100|w1:150) is the ground truth state with distribution of δ(x1100 − 0.35).\n\nFigure 23: Inference for the double pendulum x2 position at 100-th time step. Generated samples from smoothened distribution, f (x2100|w1:150), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x2Pos\n\n100|w1:150) is the ground truth state with distribution of δ(x2100 − 0.35).\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 24: Generated samples from the trained smoothened joint distribution of the double pendulum first joint position, (x1, x2), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 25: Inference for the double pendulum x3 position at 100-th time step. Generated samples from smoothened distribution, f (x3100|w1:150), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x3Pos\n\n100|w1:150) is the ground truth state with distribution of δ(x3100 − 1).\n\nFigure 26: Inference for the double pendulum x4 position at 100-th time step. Generated samples from smoothened distribution, f (x4100|w1:150), trained by the GIN, LGSSM and KVAE, respectively.\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 27: Generated samples from the trained smoothened joint distribution of the double pendulum second joint position, (x3, x4), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 28: Inference for the double pendulum x1 position at 100-th time step. Generated samples from filter distribution, f (x1100|w1:100), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x1Pos\n\n100|w1:100) is the ground truth state with distribution of δ(x1100 − 0.35).\n\nFigure 29: Inference for the double pendulum x2 position at 100-th time step. Generated samples from filter distribution, f (x2100|w1:100), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x2Pos\n\n100|w1:100) is the ground truth state with distribution of δ(x2100 − 0.35).\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 30: Generated samples from the trained filter joint distribution of the double pendulum first joint position, (x1, x2), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 31: Inference for the double pendulum x3 position at 100-th time step. Generated samples from filter distribution, f (x3100|w1:100), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x3Pos\n\n100|w1:100) is the ground truth state with distribution of δ(x3100 − 1).\n\nFigure 32: Inference for the double pendulum x4 position at 100-th time step. Generated samples from filter distribution, f (x4100|w1:100), trained by the GIN, LGSSM and KVAE, respectively.\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 33: Generated samples from the trained filter joint distribution of the double pendulum second joint position, (x3, x4), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 34: Inference for the visual odometry x1 position at 100-th time step. Generated samples from smoothened distribution, f (x1100|w1:500), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x1Pos\n\n100|w1:500) is the ground truth state with distribution of δ(x1100 + 50).\n\nFigure 35: Inference for the visual odometry x2 position at 100-th time step. Generated samples from smoothened distribution, f (x2100|w1:500), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x2Pos\n\n100|w1:500) is the ground truth state with distribution of δ(x1100 − 10).\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 36: Generated samples from the trained smoothened joint distribution of the visual odometry joint position, (x1, x2), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 37: Inference for the visual odometry x1 position at 100-th time step. Generated samples from filter distribution, f (x1100|w1:100), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x1Pos\n\n100|w1:100) is the ground truth state with distribution of δ(x1100 + 50).\n\nFigure 38: Inference for the visual odometry x2 position at 100-th time step. Generated samples from filter distribution, f (x2100|w1:100), trained by the GIN, LGSSM and KVAE, respectively. The dashed red line (x2Pos\n\n100|w1:100) is the ground truth state with distribution of δ(x1100 − 10).\n\n(a) GIN\n\n(b) LGSSM\n\n(c) KVAE\n\nFigure 39: Generated samples from the trained filter joint distribution of the visual odometry joint position, (x1, x2), at 100-th time step for the GIN, LGSSM and KVAE, respectively. The ground truth is shown with a black point.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\n(a) LGSSM\n\n(b) GIN(Filter)\n\n(c) GIN(Smooth)\n\nFigure 42: Eigenvalues of the learned transition matrix ˆFt and their corresponding true values in the first 100 time steps for Lorenz attractor experiment. Despite the low dimensional experiments in the paper that we give the dynamics (F, H) to the model, here we show the GIN ability for learning the dynamics, when we do not provide the dynamics information, i.e. (Ft, Ht) in (37).\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n(a) LGSSM\n\n(b) GIN(Filter)\n\n(c) GIN(Smooth)\n\nFigure 44: Eigenvalues of the learned transition matrix ˆFt and their corresponding true values in the first 100 time steps for NCLT dataset experiment. Despite the low dimensional experiments in the paper that we provided the dynamics (F, H) for the model, here we show the GIN ability for learning the dynamics, when we do not provide the dynamics information, i.e. (Ft, Ht) in (39).\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\n(a) LGSSM\n\n(b) GIN(Filter)\n\n(c) GIN(Smooth)\n\nFigure 46: Eigenvalues of the learned transition matrix ˆFt and their corresponding true values in the first 100 time steps for NCLT dataset experiment. Despite the low dimensional experiments in the paper that we provided the dynamics (F, H) for the model, here we show the GIN ability for learning the dynamics, when we do not provide the dynamics information, i.e. (Ft, Ht) in (39).\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 47: NCLT dataset position for the first 50 observations: ground truth positions and the generated trajectories with the GIN, LGSSM, KalmanNet and DSSM approaches are illustrated.\n\nA.10 MSE RESULTS FOR THE STATE ESTIMATION AND ESTIMATED KG-SG\n\nWe compare the learned KG-SG matrices via the GRU cells with their corresponding ground truth for the first 100 time steps of the low dimensional experiments. We calculate the element-wise squared difference of the learned KG and its ground truth, ∆KGt = Tr(cid:0)( ˆKGt − KGt)T ( ˆKGt − KGt)(cid:1), and take the average of all ∆KGt, while similar procedure holds for the SG. The results are provided in table 11.\n\nTable 11: Comparison of leaned KG-SG matrices and ground truth KG-SG. Lorenz attractor and NCLT experiments with dynamics refer to the situation, where are given the dynamics form, i.e. (37)-(39).\n\nExp\n\nLorenz with dynamics NCLT with dynamics Lorenz without dynamics NCLT without dynamics\n\n50 epoch\n\n80 epoch\n\n100 epoch\n\n200 epoch\n\n∆ KG ∆ SG ∆ KG ∆ SG ∆ KG ∆ SG ∆ KG ∆ SG 0.49 2.83 0.73 1.91 4.58 12.31 3.10 9.88\n\n2.49 2.15 13.26 9.62\n\n0.46 0.67 4.19 3.29\n\n0.62 0.95 6.29 4.83\n\n1.52 1.39 8.55 6.73\n\n0.69 0.93 5.40 5.11\n\n1.26 1.24 7.93 7.11\n\nThe MSE results for the single and double pendulum experiments are in the table 12 and 13. In addition to (7), where F matrix includes the effects of the process noise, two other mentioned solutions introduced in section 4, are included in the MSE results as well. Using GRU cell and MLP for mapping x+, as their input, to Q, as their output, where the former one is shown by GRU(Q) and the latter one by MLP(Q) in the tables.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: MSE for single pendulum experiment.\n\nModel\n\nLSTM (units = 25, m = 15) LSTM (units = 25, m = 20) LSTM (units = 25, m = 40) LSTM (units = 100, m = 15) LSTM (units = 100, m = 20) LSTM (units = 100, m = 40)\n\nGRU (units = 30, m = 15) GRU (units = 30, m = 20) GRU (units = 30, m = 40) GRU (units = 100, m = 15) GRU (units = 100, m = 20) GRU (units = 100, m = 40)\n\nMSE\n\n0.092±0.003 0.092±0.005 0.090±0.005 0.089±0.002 0.089±0.005 0.090±0.004\n\n0.095±0.006 0.093±0.002 0.094±0.005 0.091±0.002 0.092±0.004 0.091±0.008\n\nModel\n\nLGSSM filter(m = 15, n = 30, K = 10) LGSSM filter(m = 15, n = 30, K = 15) LGSSM filter(m = 15, n = 45, K = 10) LGSSM filter(m = 15, n = 45, K = 15) LGSSM filter(m = 20, n = 40, K = 10) LGSSM filter(m = 20, n = 40, K = 15) LGSSM filter(m = 20, n = 60, K = 10) LGSSM filter(m = 20, n = 60, K = 15)\n\nLGSSM smooth(m = 15, n = 30, K = 10) LGSSM smooth(m = 15, n = 30, K = 15) LGSSM smooth(m = 15, n = 45, K = 10) LGSSM smooth(m = 15, n = 45, K = 15) LGSSM smooth(m = 20, n = 40, K = 10) LGSSM smooth(m = 20, n = 40, K = 15) LGSSM smooth(m = 20, n = 60, K = 10) LGSSM smooth(m = 20, n = 60, K = 15)\n\nGIN filter(m = 15, n = 30, K = 10) GIN filter(m = 15, n = 30, K = 15) GIN filter(m = 15, n = 45, K = 10) GIN filter(m = 15, n = 45, K = 15) GIN filter(m = 20, n = 40, K = 10) GIN filter(m = 20, n = 40, K = 15) GIN filter(m = 20, n = 60, K = 10) GIN filter(m = 20, n = 60, K = 15)\n\nGIN smooth(m = 15, n = 30, K = 10) GIN smooth(m = 15, n = 30, K = 15) GIN smooth(m = 15, n = 45, K = 10) GIN smooth(m = 15, n = 45, K = 15) GIN smooth(m = 20, n = 40, K = 10) GIN smooth(m = 20, n = 40, K = 15) GIN smooth(m = 20, n = 60, K = 10) GIN smooth(m = 20, n = 60, K = 15)\n\nMLP(Q)\n\nGRU(Q)\n\n0.088±0.005 0.087±0.007 0.084±0.007 0.083±0.004 0.082±0.014 0.081±0.005 0.078±0.012 0.075±0.011\n\n0.083±0.004 0.084±0.008 0.080±0.009 0.078±0.007 0.078±0.004 0.076±0.006 0.073±0.002 0.071±0.011\n\n0.076±0.005 0.075±0.009 0.073±0.008 0.074±0.011 0.072±0.008 0.071±0.004 0.066±0.005 0.064±0.009\n\n0.070±0.003 0.068±0.011 0.065±0.009 0.066±0.007 0.065±0.003 0.064±0.011 0.061±0.012 0.057±0.009\n\n0.088±0.006 0.086±0.004 0.084±0.009 0.082±0.004 0.082±0.011 0.080±0.014 0.076±0.009 0.074±0.008\n\n0.084±0.007 0.083±0.012 0.079±0.003 0.077±0.011 0.076±0.013 0.074±0.010 0.070±0.009 0.068±0.013\n\n0.075±0.004 0.074±0.012 0.072±0.009 0.071±0.005 0.070±0.002 0.071±0.009 0.065±0.006 0.063±0.010\n\n0.068±0.009 0.068±0.007 0.064±0.012 0.063±0.009 0.062±0.008 0.063±0.007 0.057±0.006 0.056±0.004\n\nˆF(Q)\n\n0.089±0.009 0.088±0.011 0.085±0.004 0.084±0.005 0.084±0.009 0.083±0.012 0.079±0.005 0.077±0.006\n\n0.086±0.011 0.085±0.012 0.081±0.008 0.081±0.014 0.082±0.005 0.080±0.003 0.076±0.008 0.073±0.013\n\n0.078±0.013 0.078±0.014 0.074±0.010 0.073±0.015 0.072±0.005 0.071±0.007 0.067±0.009 0.065±0.013\n\n0.071±0.007 0.070±0.008 0.065±0.011 0.064±0.008 0.064±0.005 0.063±0.004 0.059±0.009 0.058±0.005\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\nTable 13: MSE for double pendulum experiment.\n\nModel\n\nLSTM (units = 50, m = 15) LSTM (units = 50, m = 20) LSTM (units = 50, m = 40) LSTM (units = 100, m = 15) LSTM (units = 100, m = 20) LSTM (units = 100, m = 40)\n\nGRU (units = 50, m = 15) GRU (units = 50, m = 20) GRU (units = 50, m = 40) GRU (units = 100, m = 15) GRU (units = 100, m = 20) GRU (units = 100, m = 40)\n\nMSE\n\n0.172±0.012 0.166±0.009 0.167±0.011 0.164±0.006 0.162±0.009 0.159±0.010\n\n0.194±0.014 0.189±0.013 0.188±0.015 0.173±0.009 0.169±0.014 0.166±0.018\n\nModel\n\nLGSSM filter(m = 15, n = 30, K = 10) LGSSM filter(m = 15, n = 30, K = 15) LGSSM filter(m = 15, n = 45, K = 10) LGSSM filter(m = 15, n = 45, K = 15) LGSSM filter(m = 20, n = 40, K = 10) LGSSM filter(m = 20, n = 40, K = 15) LGSSM filter(m = 20, n = 60, K = 10) LGSSM filter(m = 20, n = 60, K = 15)\n\nLGSSM smooth(m = 15, n = 30, K = 10) LGSSM smooth(m = 15, n = 30, K = 15) LGSSM smooth(m = 15, n = 45, K = 10) LGSSM smooth(m = 15, n = 45, K = 15) LGSSM smooth(m = 20, n = 40, K = 10) LGSSM smooth(m = 20, n = 40, K = 15) LGSSM smooth(m = 20, n = 60, K = 10) LGSSM smooth(m = 20, n = 60, K = 15)\n\nGIN filter(m = 15, n = 30, K = 10) GIN filter(m = 15, n = 30, K = 15) GIN filter(m = 15, n = 45, K = 10) GIN filter(m = 15, n = 45, K = 15) GIN filter(m = 20, n = 40, K = 10) GIN filter(m = 20, n = 40, K = 15) GIN filter(m = 20, n = 60, K = 10) GIN filter(m = 20, n = 60, K = 15)\n\nGIN smooth(m = 15, n = 30, K = 10) GIN smooth(m = 15, n = 30, K = 15) GIN smooth(m = 15, n = 45, K = 10) GIN smooth(m = 15, n = 45, K = 15) GIN smooth(m = 20, n = 40, K = 10) GIN smooth(m = 20, n = 40, K = 15) GIN smooth(m = 20, n = 60, K = 10) GIN smooth(m = 20, n = 60, K = 15)\n\nMLP(Q)\n\nGRU(Q)\n\n0.159±0.021 0.153±0.010 0.141±0.015 0.138±0.012 0.137±0.009 0.137±0.008 0.126±0.014 0.124±0.013\n\n0.148±0.014 0.146±0.013 0.136±0.009 0.135±0.017 0.131±0.022 0.129±0.014 0.116±0.016 0.112±0.009\n\n0.125±0.012 0.124±0.019 0.114±0.015 0.112±0.020 0.111±0.009 0.109±0.018 0.094±0.017 0.093±0.009\n\n0.116±0.009 0.113±0.018 0.101±0.014 0.100±0.011 0.098±0.010 0.095±0.014 0.081±0.008 0.079±0.006\n\n0.153±0.009 0.152±0.012 0.139±0.013 0.137±0.017 0.138±0.013 0.136±0.016 0.122±0.015 0.119±0.009\n\n0.144±0.015 0.142±0.017 0.133±0.017 0.133±0.012 0.132±0.011 0.129±0.022 0.115±0.013 0.108±0.014\n\n0.125±0.011 0.121±0.009 0.110±0.017 0.110±0.011 0.111±0.013 0.108±0.009 0.095±0.021 0.091±0.008\n\n0.113±0.017 0.112±0.014 0.101±0.015 0.098±0.008 0.094±0.015 0.092±0.007 0.079±0.009 0.076±0.013\n\nˆF(Q)\n\n0.154±0.013 0.152±0.008 0.144±0.011 0.142±0.007 0.144±0.012 0.141±0.007 0.129±0.009 0.127±0.012\n\n0.147±0.009 0.146±0.014 0.139±0.017 0.137±0.009 0.136±0.014 0.134±0.011 0.123±0.019 0.120±0.010\n\n0.126±0.014 0.124±0.015 0.115±0.011 0.114±0.019 0.113±0.013 0.111±0.009 0.099±0.018 0.097±0.009\n\n0.115±0.011 0.113±0.015 0.105±0.009 0.102±0.013 0.101±0.008 0.098±0.017 0.086±0.013 0.083±0.009\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nA.11 ALGORITHMS\n\nAlgorithm High-Dimensional Observations Training\n\n0:T −1)\n\n0 , Σ+ 0 )\n\n1:T , Σ−\n\n0:T −1, Σ+\n\n1:T ) = Prediction Step ((x+\n\nInput: Ground Truth gt1:T , Observations o1:T , last posteriors (x+ initial posterior (x+ α1:T = Dynamics Network (x+ Obtain ˆF1:T and ˆH1:T by (13) (x− (w1:T , r1:T ) = encoder (o1:T ) ˆH1:T M1:T MT K1:T = Σ− ˆFT J1:T = Σ+ 1:T N1:T NT (x+ 1:T , Σ− 1:T ) = Filtering Step (x− 1:T , Σ+ (x1:T |T , Σ1:T |T ) = Smoothing Step (x+ o+ L1:T = - Likelihood (gt1:T , o+ Backward Propagation ()\n\n1:T , M1:T = GRU KG(Conv2D(Σ− 1:T ))\n\n1:T = decoder (x1:T |T , Σ1:T |T ) 1:T )\n\n1:T , K1:T , w1:T , ˆH1:T ) 1:T , Σ+\n\n1:T , N1:T = GRU SG (Conv2D(Σ−\n\n1:T , J1:T , ˆF1:T )\n\n0:T −1), ˆF1:T )\n\n1:T\n\n1:T\n\n1:T , Σ+\n\n1:T ),\n\n1:T ), r1:T )\n\nAlgorithm Low-Dimensional Observations Training\n\n1:T , Σ+\n\n1:T ),\n\n0:T −1)\n\n0 , Σ+ 0 )\n\nInput: Ground Truth gt1:T , Observations y1:T , last posteriors (x+ initial posterior (x+ if Dynamics are not known then α1:T = Dynamics Network (x+ Obtain ˆF1:T and ˆH1:T by (13) (w1:T , r1:T ) = MLP (y1:T ) 1:T ) = Prediction Step (x+ 1:T , Σ− (x− ˆH1:T M1:T MT K1:T = Σ− ˆFT J1:T = Σ+ 1:T N1:T NT (x+ 1:T , Σ− 1:T ) = Filtering Step (x− 1:T , Σ+ (x1:T |T , Σ1:T |T ) = Smoothing Step (x+ o+ 1:T = MLP (x1:T |T +, Σ1:T |T ) L1:T = - Likelihood (gt1:T , o+ Backward Propagation ()\n\n1:T , M1:T = GRU KG(Σ− 1:T )\n\n1:T , K1:T , w1:T , ˆH1:T ) 1:T , Σ+\n\n1:T , N1:T = GRU SG (Σ−\n\n1:T , J1:T , ˆF1:T )\n\n0:T −1, ˆF1:T )\n\n0:T −1, Σ+\n\n1:T )\n\n1:T\n\n1:T\n\n1:T , r1:T )\n\n0:T −1, Σ+\n\n0:T −1), Q1:T , F1:T )\n\n1:T , M1:T = GRU KG(Σ− 1:T )\n\n1:T , N1:T = GRU SG (Σ−\n\n1:T , r1:T )\n\n1:T , K1:T , y1:T , H1:T ) 1:T , Σ+ 1:T , J1:T , F1:T )\n\n0:T −1, Q1:T )\n\nelse\n\n0:T −1) or GRU (x+\n\nQ network = MLP (x+ (F1:T , H1:T ) = Dynamics r1:T = trainable layer(y1:T ) q1:T = Q network(x+ 0:T −1) 1:T ) = Prediction Step ((x+ (x− 1:T , Σ− K1:T = Σ− 1:T H1:T M1:T MT J1:T = Σ+ 1:T FT 1:T N1:T NT 1:T , Σ− 1:T ) = Filtering Step (x− 1:T , Σ+ (x+ (x1:T |T , Σ1:T |T ) = Smoothing Step (x+ σ1:T |T = Trainable Layer (Σ1:T |T ) o+ L1:T = - Likelihood (gt1:T , o+ Backward Propagation ()\n\n1:T = [x1:T |T , σ1:T |T ]\n\n1:T )\n\nend if\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nA.12 PYTHON INFERENCE CODE\n\nTo demonstrate the simplicity of our proposed GIN, we include intuitive inference code with Tensorflow library for both the high dimensional and low dimensional experiments. The code runs with Python 3.6+. The entire code to reproduce the experiments are available in Github repository.\n\nA.12.1 PYTHON INTUITIVE CODE FOR HIGH DIMENSIONAL EXPERIMENTS.\n\n1 import tensorflow.keras as k 2 import Prediction 3 import Filtering 4 import Smoothing 5 import DynamicsNetwork 6 import Encoder 7 import Decoder 8 import DataGen\n\n9 10 class GIN_CELL(k.layers.Layer):\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\ndef __init__(self, initial_states):\n\nself.x_tm1_+, self.Sigma_tm1_+ = initial_states self.filter_states = [[self.x_tm1_+, self.Sigma_tm1_+]]\n\ndef call(self, inputs):\n\nw_1:T, r_1:T = inputs for w_t, r_t in (w_1:T, r_1:T):\n\nF_hat_t, H_hat_t = DynamicsNetwork(self.x_tm1_+) x_t_-, Sigma_t_- = Prediction(F_hat_t, H_hat_t,... self.x_tm1_+, self.Sigma_tm1_+) x_t_+, Sigma_t_+ = Filtering(x_t_-, Sigma_t_-,... w_t, r_t, H_hat_t) self.filter_states.append([x_t_+, Sigma_t_+]) self.x_tm1_+, self.Sigma_tm1_+ = x_t_+, Sigma_t_+\n\nx_1:T_T, Sigma_1:T_T = Smoothing(self.filter_states,... Sigma_1:T_-, F_hat_1:T) return x_1:T_T, Sigma_1:T_T\n\n27 28 class GIN(k.models.Model):\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\ndef __init__(self, initial_states):\n\nself.x_0_+, self.Sigma_0_+ = initial_states self.GIN_CELL_OBJ = self.GIN_CELL(self.x_0_+, self.Sigma_0_+) self.Encoder = Encoder self.Decoder = Decoder\n\ndef call(self, o_1:T):\n\nw_1:T, r_1:T = self.Encoder(o_1:T) x_1:T_T, Sigma_1:T_T = self.GIN_CELL_OBJ(w_1:T, r_1:T) o_1:T_+ = self.Decoder(x_1:T_T, Sigma_1:T_T) return o_1:T_+\n\n40 41 Data = DataGen() 42 o_1:T_+ = GIN(Data)\n\n37\n\nUnder review as a conference paper at ICLR 2023\n\nA.12.2 PYTHON INTUITIVE CODE FOR LOW DIMENSIONAL EXPERIMENTS.\n\n1 import tensorflow.keras as k 2 import Prediction 3 import Filtering 4 import Smoothing 5 import MLP_ENC 6 import MLP_DEC 7 import DataGen 8 import Dynamics\n\n9 10 class GIN_CELL(k.layers.Layer):\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\ndef __init__(self, initial_states):\n\nself.x_tm1_+, self.Sigma_tm1_+ = initial_states self.filter_states = [[self.x_tm1_+, self.Sigma_tm1_+]]\n\ndef call(self, inputs):\n\nw_1:T, r_1:T, F_1:T, H_1:T = inputs for w_t, r_t, F_t, H_t in (w_1:T, r_1:T, F_1:T, H_1:T):\n\nx_t_-, Sigma_t_- = Prediction(F_t, H_t, self.x_tm1_+,... self.Sigma_tm1_+) x_t_+, Sigma_t_+ = Filtering(x_t_-, Sigma_t_-, w_t, r_t, H_t) self.filter_states.append([x_t_+, Sigma_t_+]) self.x_tm1_+, self.Sigma_tm1_+ = x_t_+, Sigma_t_+\n\nx_1:T_T, Sigma_1:T_T = Smoothing(self.filter_states,... Sigma_1:T_-, F_1:T) return x_1:T_T, Sigma_1:T_T\n\n26 27 class GIN(k.models.Model):\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\ndef __init__(self, initial_states, Dynamics):\n\nself.x_0_+, self.Sigma_0_+ = initial_states self.F_1:T, self.H_1:T = Dynamics self.GIN_CELL_OBJ = self.GIN_CELL(self.x_0_+, self.Sigma_0_+) self.MLP_ENC = MLP_ENC self.MLP_DEC = MLP_DEC\n\ndef call(self, o_1:T):\n\nr_1:T = self.MLP_ENC(o_1:T) x_1:T_T, Sigma_1:T_T = self.GIN_CELL_OBJ(o_1:T, r_1:T,... self.F_1:T, self.H_1:T) Sigma_o_1:T_+ = self.MLP_DEC(Sigma_1:T_T) x_o_1:T_+ = x_1:T_T return x_o_1:T_+, Sigma_o_1:T_+\n\n42 43 Data = DataGen() 44 Dynamics_Matrices = Dynamics() 45 x_o_1:T_+, Sigma_o_1:T_+ = GIN(Data, Dynamics_Matrices)\n\n38",
    "reference": "# Summary Of The Paper\n\nThe authors proposed a novel Gated Inference Network (GIN) to infer and learn state space models in both high and low dimensions. They use concepts in Kalman filtering to obtain estimates of the dynamics. They show that GIN is better at learning state space representations with disentangled dynamics features than existing approaches, they are also robust with noise and able to impute missing data. GIN is efficient in handling the case of unknown dynamics, the models are learned directly from the observations which are fed into encoders to obtain latent observations.\n\n# Strength And Weaknesses\n\n1. For many case studies, the authors do not estimate the latents but assume the latent form. In a typical dynamics and latent estimation study, we are not provided the latent form (for example, position and velocity in the Michigan NCLT dataset). What happens if the authors want to estimate the latents as well as the dynamics?\n2. The paper sets out to perform system identification and learn the parameters of the state space model, but the authors only compare the predicted / estimated states with the ground truth. Certain properties of the recovered matrices can be compared with ground truth, for example, their eigenvalues. The authors need to show that their methods can recover ground truth dynamics.\n3. The paper lacks motivation for the architecture of GIN. For example, is the goal of the encoder and decoder simply to reduce dimensionality? What if the model does not have an encoder and decoder, for example in the case of a low-dimensional input (the authors state that they use an MLP in this case, but is this necessary)?\n4. The data that the authors used to perform the imputation task is simple although it outperforms other methods shown in the paper, the data has a high amount of temporal correlations, so it may not be hard to achieve an imputation task. Imputation in a complex real-world dataset would be more helpful to show the ability of GIN.\n5. The “lack of dynamics” needs to be explained more clearly: it is not clear whether the dynamics are not being inferred or there is no succinct generative dynamical model for the simulations.\n\nMinor comments:\n1. The notation of x_{t+} and x_{t-} appear on page 3 first, but their definitions are in the caption of Figure 3 which is later shown.\n2. The introduction does not clearly state the problem that the authors are considering. There are many typos and colloquial sentences, e.g., “a bunch of approaches”.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity needs to be much improved. It is not clear which problem the authors are focusing on since they have plugged together a 'high-dimensional, lack of dynamics' study and a 'low-dimensional, with the presence of dynamics' study. There needs to be some structure to understand why these two parts are considered in the same framework.\n\nThe ideas in the paper seem promising, but there may be limitations due to the structure of their proposed model (e.g., linear dynamics). These need to be stated clearly.\n\n# Summary Of The Review\n\nDue to the lack of clarity and the weaknesses mentioned above, I am not recommending acceptance in the current state of the submission.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "LEARNING GFLOWNETS FROM PARTIAL EPISODES FOR IMPROVED CONVERGENCE AND STABILITY\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nGenerative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and proInspired by pose a way to exploit this tradeoff to mitigate its harmful effects. the TD(λ) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB(λ), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB(λ) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.\n\n1\n\nINTRODUCTION\n\nGenerative flow networks (GFlowNets; Bengio et al., 2021a) are generative models that construct objects lying in a target space X by taking sequences of actions sampled from a learned policy. GFlowNets are trained so as to make the probability of sampling an object x ∈ X proportional to a given nonnegative reward R(x). GFlowNets’ use of a parametric policy that can generalize to states not seen during training makes them a competitive alternative to methods based on local exploration in various probabilistic modeling tasks (Bengio et al., 2021a; Malkin et al., 2022; Zhang et al., 2022; Jain et al., 2022; Deleu et al., 2022).\n\nGFlowNets solve the variational inference problem of approximating a target distribution over X with the distribution induced by the sampling policy, and they are trained by algorithms reminiscent of reinforcement learning (although GFlowNets model the diversity present in the reward distribution, rather than maximizing reward by seeking its mode). In most past works (Bengio et al., 2021a; Malkin et al., 2022; Zhang et al., 2022; Jain et al., 2022), GFlowNets are trained by exploratory sampling from the policy and receive their training signal from the reward of the sampled object. The flow matching (FM) and detailed balance (DB) learning objectives for GFlowNets proposed in Bengio et al. (2021a;b) resemble temporal difference learning (Sutton & Barto, 2018).\n\nA third objective, trajectory balance (TB), was proposed in Malkin et al. (2022) to address the problem of slow temporal credit assignment with the FM and DB objectives. The TB objective propagates learning signals over entire episodes, while the temporal difference-like objectives (FM and DB) make updates local to states or actions. It has been hypothesized by Malkin et al. (2022) that the improved credit assignment with TB comes at the cost of higher gradient variance, analogous to the bias-variance tradeoff seen in temporal difference learning (TD(n) or TD(λ)) with different eligibility trace schemes (Sutton & Barto, 2018; Kearns & Singh, 2000; van Hasselt et al., 2018; Bengio et al., 2020). This hypothesis is one of the starting points for the present paper.\n\nIn this paper, we propose a new learning objective for GFlowNets, called subtrajectory balance (SubTB, or SubTB(λ) when its real-valued hyperparameter λ is specified). Building upon theoretical results of Bengio et al. (2021b); Malkin et al. (2022), we show how the SubTB(λ) objective allows the flexibility of learning from partial experiences of any length. Experiments on two synthetic and four real-world domains support the following empirical claims:\n\n1\n\n(1) SubTB(λ) improves convergence of GFlowNets in previously studied environments: models trained with SubTB(λ) approach the target distribution in fewer training iterations and are less sensitive to hyperparameter choices.\n\n(2) SubTB(λ) enables training of GFlowNets in environments where past approaches perform\n\npoorly due to sparsity of the reward function or length of action sequences.\n\n(3) The benefits of SubTB(λ) are explained by lower variance of the stochastic gradient, with the parameter λ allowing interpolation between the high-bias, low-variance DB objective and the low-bias, high-variance TB objective.\n\n2 METHOD\n\n2.1 PRELIMINARIES\n\nIn this section, we summarize the necessary preliminaries on GFlowNets. We follow the notation of Malkin et al. (2022), to which the reader is directed for a more thorough exposition written with a view towards motivating the trajectory and subtrajectory balance objectives. A deeper introduction is given in Bengio et al. (2021b). Let G = (S, A) be a directed acyclic graph. The vertices s ∈ S are called states and the directed edges (u→v) ∈ A are actions. If (u→v) is an edge, we say v is a child of u and u is a parent of v. There is a unique initial state s0 ∈ S with no parents. States with no children are called terminal, and the set of terminal states is denoted by X. A trajectory or an action sequence is a sequence of states τ = (sm→sm+1→ . . . →sn), where each (si→si+1) is an action. The trajectory is complete if sm = s0 and sn is terminal. The set of complete trajectories is denoted by T . A (forward) policy is a collection of distributions PF (−|s) over the children of every nonterminal state s ∈ S. A forward policy determines a distribution over T by\n\nPF (τ = (s0→ . . . →sn)) =\n\nn−1 (cid:214)\n\ni=0\n\nPF (si+1|si).\n\n(1)\n\nAny distribution over complete trajectories that arises from a forward policy satisfies a Markov property: the marginal choice of action out of a state s is independent of how s was reached. Conversely, any Markovian distribution over T arises from a forward policy (Bengio et al., 2021b). A forward policy can thus be used to sample terminal states x ∈ X by starting at s0 and iteratively sampling actions from PF, or, equivalently, taking the terminating state of a complete trajectory τ ∼ PF (τ). The marginal likelihood of sampling x ∈ X is the sum of likelihoods of all complete trajectories that terminate at x. Suppose that a nontrivial (not identically 0) nonnegative reward function R : X → R≥0 is given. The learning problem solved by GFlowNets is to estimate a policy PF such that the likelihood of sampling x ∈ X is proportional to R(x). That is, there should exist a constant Z such that\n\nR(x) = Z ∑︁\n\nPF (τ) ∀x ∈ X.\n\nτ=(s0→...→sn=x )\n\n(2)\n\nIf (2) is satisfied, then Z = (cid:205)x ∈ X R(x).\n\n2.2 GFLOWNET TRAINING OBJECTIVES\n\nBecause the sum in (2) may be intractable to compute, it is in general not possible to directly convert this constraint into a training objective. To solve this problem, GFlowNet training objectives introduce auxiliary variables in the parametrization in various ways, but all have the property that (2) is satisfied at the global optimum. The key properties of these objectives are summarized in Table 1.\n\nFlow matching (FM; Bengio et al., 2021a). Motivating the ‘flow network’ terminology, Bengio et al. (2021a) proved that (2) is satisfied if PF arises from an edge flow function satisfying certain constraints. Namely, an assignment F : A → R≥0 of a nonnegative number (flow) to each action defines a policy via\n\nPF (t|s) =\n\nF (s→t) (cid:205)t ′:(s→t ′ ) ∈ A F (s→t′)\n\n.\n\n(3)\n\nA sufficient condition for the terminating distribution of PF to be proportional to the reward R(x) is that a family of flow-matching (flow in = flow out) conditions is satisfied at all interior states and a\n\n2\n\nTable 1: Summary of GFlowNet training objectives.\n\nObjective\n\nFlow matching Detailed balance Trajectory balance\n\nSubtrajectory balance\n\nParametrization edge flow F (s→t; θ) state flow F (s; θ), policies PF (−|−; θ), PB (−|−; θ) initial state flow Z θ , policies PF (−|−; θ), PB (−|−; θ) state flow F (s; θ), policies PF (−|−; θ), PB (−|−; θ)\n\nLocality state s action s→t complete trajectory τ (partial) trajectory τ\n\nfamily of reward-matching conditions is satisfied at terminal states:\n\n∑︁\n\nF (s→t) =\n\n∑︁\n\nF (t→u)\n\n∀t ∈ S \\ (X ∪ {s0}),\n\ns:(s→t ) ∈ A ∑︁\n\ns:(s→x ) ∈ A\n\nu:(t→u) ∈ A\n\nF (s→x) = R(x)\n\n∀x ∈ X.\n\n(4)\n\nThe flow F (s→t) is then proportional to the marginal likelihood that a complete trajectory sampled from PF includes the action s→t.\n\nIn Bengio et al. (2021a), a GFlowNet is described by a parametric estimate of the edge flow function, F (u→v; θ) (a neural net with parameters θ). These conditions can be converted into objectives that are minimized when (4) is satisfied. For example, the flow-matching objective at a nonterminal state s is defined by\n\n(cid:18)\n\nLFM(s) =\n\nlog\n\n(cid:205)s:(s→t ) ∈ A F (s→t; θ) + ε (cid:205)u:(t→u) ∈ A F (t→u; θ) + ε\n\n(cid:19) 2\n\n,\n\n(5)\n\nwhere ε is a smoothing constant that can safely be set to 0 if the flows are constrained to be strictly positive, and a similar objective (or a constraint by construction) is defined to force the flow F (s→x) into terminal states x to match R(x). If these objectives are globally minimized for all states s, then the policy PF (−|−; θ) defined by F (−; θ) via (3) satisfies (2), with Z = (cid:205)t:(s0→t ) ∈ A F (s→t; θ) = (cid:205)x ∈ X R(x). The question of how to sample states s for training is discussed below. Detailed balance (DB; Bengio et al., 2021b; Malkin et al., 2022). In the DB parametrization, a forward policy model PF (−|−; θ) is learned directly, jointly with two additional objects: a backward policy model PB (−|−; θ), which can predict a distribution over the parents of any noninitial state, and a state flow function F (s; θ) (typically parametrized in the log domain). The detailed balance conditions state that\n\nF (s; θ)PF (t|s; θ) = F (t; θ)PB (s|t; θ) (6) for all actions (s→t) and F (x; θ) = R(x) for x terminal. Satisfaction of these conditions for all actions (s→t) and x ∈ X implies that PF samples proportionally to the reward (i.e., satisfies (2), with Z = F (s0)). The DB condition (6) can be converted into a squared log-ratio objective LDB(s→t) in the same way that (4) yields (5), and LDB(s→t) can be optimized over sampled actions (s→t).\n\nTrajectory balance (TB; Malkin et al., 2022). The parametrization required for the TB objective includes forward and backward policy models PF (−|−; θ) and PB (−|−; θ), as well as an estimate Z θ of the constant of proportionality in (2). Satisfaction of the following condition for all complete trajectories τ = (s0→ . . . →sn) implies that (2) is satisfied:\n\nZ θ PF (τ; θ) = R(sn)PB (τ|sn; θ),\n\n(7)\n\nwhere we have used the conventions\n\nPF (τ; θ) =\n\nn−1 (cid:214)\n\ni=0\n\nPF (si+1|si; θ), PB (τ|sn; θ) =\n\nn−1 (cid:214)\n\ni=0\n\nPB (si |si+1; θ).\n\nThe condition (7) can again be made into a squared log-ratio objective LTB (τ) and optimized for complete trajectories τ taken from some training policy. In Malkin et al. (2022), the TB objective was empirically demonstrated to have better convergence properties than FM and DB on various problem domains.\n\nTraining policy and exploration. Global minimization of the FM, DB, and TB objectives for all values of their respective arguments (states, actions, or complete trajectories) implies satisfaction of (2). Therefore, given a sufficiently expressive model and convergence of the optimization procedure, a GFlowNet policy that samples x with likelihood proportional to R(x) can be trained by minimizing any of these losses over a distribution with full support, enabling offline training of GFlowNets. As\n\n3\n\nin other RL algorithms, the distribution over sampled states, actions, or episodes can be fixed and off-policy, or can vary over the course of training and use available information about terminal states in interesting ways (Zhang et al., 2022; Deleu et al., 2022). The simplest approach, which is also taken in this paper, is on-policy learning or a very similar off-policy variant that flattens the current policy to ensure exploration. Complete trajectories τ = (s0→ . . . →sn) are sampled from the forward policy PF (−|−; θ) (tempered or mixed with a uniform policy with a small weight so as to ensure full support and exploration). One then takes gradient descent steps on LTB (τ), on LDB (si→si+1) over all actions in τ, or on LFM(si) for all intermediate states in τ.\n\nThe GFlowNets in this paper are trained on-policy, or off-policy with a training policy that is a mixture of PF with a uniform policy: τ = (s0→s1→ . . . →sn) is sampled with si+1 ∼ (1 − ε)PF (si+1|si; θ) + ε\n\n#{t:(s→t ) ∈ A } . Here ε is the random exploration weight.\n\n1\n\n2.3 SUBTRAJECTORY BALANCE: LEARNING FROM PARTIAL EPISODES\n\nRecall the GFlowNet parametrization used in the DB objective above, with a state flow estimator F (−|−; θ) and a pair of policies PF (−|−; θ), PB (−|−; θ). It is shown in §A.2 of Malkin et al. (2022) that the detailed balance conditions (6) are satisfied for all actions if and only if the following subtrajectory balance constraint holds for all (not necessarily complete) trajectories τ = (sm→ . . . →sn):\n\nn−1 (cid:214)\n\nF (sm; θ)\n\nPF (si+1|si; θ) = F (sn; θ)\n\nn−1 (cid:214)\n\nPB (si |si+1; θ),\n\n(8)\n\ni=m\n\ni=m\n\nwhere we again enforce that F (x; θ) = R(x) if x is terminal. Observe that the DB condition (6) is a special case of (8) when the trajectory consists of one action, and the TB condition (7) is precisely the case when τ is complete, with the identification Z θ = F (s0; θ).\n\nThe above constraint yields the subtrajectory balance objective\n\n(cid:32)\n\n(cid:33) 2\n\nlog\n\nLSubTB(τ) =\n\ni=m PF (si+1|si; θ) i=m PB (si |si+1; θ)\n\nF (sm; θ) (cid:206)n−1 F (sn; θ) (cid:206)n−1 If this objective is made equal to 0 for all partial trajectories τ, where R(sn) is substituted for F (sn; θ) if sn is terminal, then the policy PF satisfies the desired condition (2). (Proof: When LSubTB (τ) = 0, (8) is satisfied, implying satisfaction of both (7) and (6). Either of these conditions is a sufficient condition for (2), as shown by Bengio et al. (2021b); Malkin et al. (2022).) Extracting subtrajectories for training. Suppose that an episode (complete trajectory) τ = (s0→s1→ . . . →sn) is sampled for training. There are (cid:0)n+1\n\n(9)\n\n.\n\n(10) Having sampled a complete trajectory τ for training, we make gradient steps on a convex combination of the subtrajectory balance losses LSubTB(τi: j ): θ ← θ − ∇θ L, where\n\nτi: j := (si→si+1→ . . . →s j ),\n\n2\n\n(cid:1) = O (n2) nontrivial subtrajectories: 0 ≤ i < j ≤ n.\n\nL =\n\n(cid:205)\n\n0≤i< j ≤n λ j −iLSubTB(τi: j ) 0≤i< j ≤n λ j −i\n\n(cid:205)\n\n.\n\n(11)\n\nHere λ > 0 is a hyperparameter controlling the weights assigned to subtrajectories of different lengths, and when λ is set to 1, it leads to a uniform weighting scheme. Notice that the λ → 0+ limit leads precisely to the average detailed balance loss LDB(si→si+1) over all transitions in τ, while the λ → +∞ limit gives the trajectory balance objective LTB (τ).1\n\nOther schemes for weighting subtrajectories are possible and should be explored in future work.\n\nComputational considerations. It may appear that the optimization of (11) induces a computation cost that is quadratic in the trajectory length. However, a closer inspection of the gradient of (11) with respect to the state flows log F (si; θ) and the forward and backward policy logits shows that gradient computation requires only one forward and one backward pass through the neural networks giving log F (s; θ), log PF (−|si; θ), and log PB (−|si; θ). The quadratic computation cost is incurred only in performing linear operations on these log-flows and policy logits, not in the evaluation of the deep networks. Thus the SubTB loss has little computation overhead over DB or TB.\n\n1When a batch of trajectories is used for training, the convex combination weights may either be normalized over all subtrajectories of all trajectories in the batch, or normalized independently over the subtrajectories of each trajectory. For consistency, we choose the first option for the experiments in this paper.\n\n4\n\nlog\n\nZ ·\n\nm−1 (cid:214)\n\ni=0\n\nPF (si+1|si) PB (si |si+1)\n\nHypothesized benefits. We hypothesize that SubTB(λ) brings two benefits to GFlowNet training: VARIANCE REDUCTION. The TB loss terms LTB(τ) for trajectories τ that take a given sequence of actions until a state s, then diverge, share the terms log Z and the policy logits for all transitions preceding s inside the square. However, the ‘tail’ of the TB loss, involving the forward and backward policy logits for transitions that appear after s in τ, can be seen as a stochastic least-squares regression target. That is, if s = sm in a trajectory τ = (s0→s1→ . . . →sn), then (cid:32)\n\n(cid:33)\n\n(cid:33)\n\n.\n\n(12)\n\n(13)\n\nis regressed to\n\n(cid:32)\n\nlog\n\nR(sn) ·\n\nn−1 (cid:214)\n\nPB (si |si+1) PF (si+1|si)\n\ni=m Similarly, for trajectories that share the transitions following s but may differ in their initial actions, (12) is a stochastic regression target for (13). The subtrajectory balance loss terms LSubTB (τm: j ) for partial trajectories beginning at s regress the log-state flow log F (s) to (parts of) expressions like (13), while loss terms LSubTB(τi:m) regress (parts of) expressions like (12) to the log-state flow log F (s). The learned log F (s) is thus a learned estimate of a stochastic piece of the TB loss for trajectories that contain s. Replacing a stochastic term in the TB loss by a learned estimate of its expectation is guaranteed to introduce bias into the gradient (with respect to the gradient of the TB loss), but is expected to reduce variance. This is akin to the variance-reducing effect of actor-critic methods in RL.\n\nThis hypothesis is studied empirically in our experiments and in particular §4.1.1, where we provide evidence that SubTB(λ) is a practically useful interpolation between TB (high variance) and DB (low variance, high bias relative to the true TB gradient) losses.\n\nFASTER LEARNING DUE TO GENERALIZATION OF STATE FLOWS. Another benefit of subtrajectory balance for convergence speed may come from the ability of estimated state flow functions log F (s; θ) to be modeled with high precision and generalize between states s faster than the often high-dimensional policy logits log PF (−|s; θ), log PB (−|s; θ). Such generalization is important in problems where the state graph becomes ‘wide’ far from the initial state, making the learning signal sparse at states that are near termination. Indeed, in all of our experiment domains except the hypergrids in §4.1 – and for the largest hypergrids – the number of terminal states is many orders of magnitude larger than the total number of states seen in training.\n\n3 RELATED WORK\n\nEligibility traces. SubTB(λ) draws inspiration from the TD(λ) algorithm in RL (Sutton, 1988; Sutton & Barto, 2018), which forms an estimate of the expected return via a convex combination of n-step returns, each weighed by (1 − λ)λn−1. The parameter λ ∈ [0, 1] enables a bias-variance tradeoff (Kearns & Singh, 2000). Intuitively, larger λ leads to lower bias and higher variance, since the estimate of the expected return approaches the single-point Monte Carlo estimate as λ → 1. We take inspiration from this idea to mix together different (possibly all) subtrajectories, akin to how n-step returns are mixed together. We hypothesize that the right mixing may reduce variance, compared to TB, with the additional benefits of inducing consistency between the flows of intermediate states, and thus of helping propagate credit faster and enable faster convergence. In addition, GFlowNet training objectives are reminiscent of residual gradient RL methods (Baird, 1995; Zhang et al., 2020) since the “endpoint” (e.g. F (sn) in (9)) is also considered in the gradient.\n\nMaxEnt RL. RL has a rich literature on energy-based, or maximum entropy, methods (Ziebart, 2010; Mnih et al., 2016; Haarnoja et al., 2017; Nachum et al., 2017; Schulman et al., 2017; Haarnoja et al., 2018), which are close or equivalent to the GFlowNet framework in certain settings (in particular when the MDP has a tree structure (Bengio et al., 2021a)). Also related are methods that maximize entropy not on the policy, but rather on the state visitation distribution (Hazan et al., 2019; Islam et al., 2019; Zhang et al., 2021) or some proxy of it (Eysenbach et al., 2018), which achieve a similar objective to GFlowNet models by flattening the state visitation distribution. If the state graph of the environment is a directed tree, the loss LSubTB on individual subtrajectories is equivalent to that of path consistency learning (Nachum et al., 2017). However, attempts to use path consistency learning in settings without intermediate rewards have only computed the loss on subtrajectories that have length 1 or include a terminal state (Guo et al., 2021).\n\n5\n\nFigure 3: L1 distance between empirical and target distributions over the course of training on the hypergrid environment. SubTB(λ = 0.9) consistently gives faster convergence than TB, the strongest objective from past work, on all grid sizes. The difference is especially visible for the harder variant of the reward function (last row). The x-axis is the cumulative number of training trajectories (episodes).\n\n4 EXPERIMENTS\n\n4.1 HYPERGRID: ROBUSTNESS TO SPARSE REWARDS\n\nWe study the synthetic hypergrid environment introduced in Bengio et al. (2021a). The set of interior states is a d-dimensional hypergrid of size H × H × · · · × H with a multimodal reward function concentrated near each of the 2d corners of the hypergrid (see Bengio et al. (2021a); Malkin et al. (2022) and Fig. 1). The initial state is (0, 0, . . . , 0), and each action is a step that increments one of the d coordinates by 1 without leaving the grid. A special termination action is also allowed from each state. This environment is designed to challenge a learning agent to infer and discover new modes from those that have been already been visited.\n\nFigure 1: 16 × 16 hypergrid reward function.\n\nWe study various sizes of 2-dimensional and 4-dimensional hypergrids, using the hardest variant of the reward function from past work (the minimal reward, away from the corners of the grid, is set to 10−3). We train GFlowNets to sample from the target reward functions and plot the evolution of the L1 distance between the target distribution and the empirical distribution of the last 2 · 105 states seen in training.2 In all cases, we tune the learning rates for the TB and SubTB(λ = 0.9) objectives. (See §A for details.)\n\nThe results (mean and standard deviation over three random runs) are shown in the first two rows of Fig. 3. Models trained with SubTB(λ) converge faster, and with less variance between random seeds, to the true distribution than with TB for all hypergrid sizes.\n\nWe also study an even sparser variant of the environment, in which the background reward is set to 10−4. In this case, SubTB(λ) continues to perform strongly (last row of Fig. 3), while models trained with TB do not even discover all modes of the target distribution for grids larger than 8 × 8 (Fig. 2). Additional results are given in §A.1. In particular, SubTB(λ) continues to perform strongly when only subtrajectories of less than a certain length are used for training, which can be beneficial in realistic settings where only partial episodes are given. We also show the effect of λ on the convergence rate (Fig. A.2) and of more exploratory training policies (Fig. A.3).\n\nFigure 2: Distribution of 2 × 105 samples from GFlowNets trained on the harder variant of the 32 × 32 grid with TB and SubTB(λ) objectives.\n\n2Such an evaluation is possible in this synthetic environment because the exact target distribution function can be tractably computed. Note that the metric shown in Fig. 3 differs from what is called ‘L1 distance’ in past work, as we do not divide by the total number of states.\n\n6\n\n1031041051060.00.51.01.52.0L18×8TBSubTB(0.9)DB10310410510612×1210310410510616×1610310410510620×2010310410510624×241031041051060.00.51.01.52.0L128×2810310410510632×3210310410510640×4010310410510650×5010310410510660×601031041051060.00.51.01.52.0L112×12×12×12TBSubTB(0.9)DB10310410510616×16×16×1610310410510620×20×20×201031041051060.00.51.01.52.0L124×24×24×2410310410510628×28×28×2810310410510632×32×32×321031041051060.00.51.01.52.0L18×8 (hard)TBSubTB(0.9)DB10310410510612×12 (hard)10310410510616×16 (hard)10310410510620×20 (hard)10310410510624×24 (hard)10310410510628×28 (hard)10310410510632×32 (hard)0102030051015202530TB0102030051015202530SubTB(0.9)Figure 4: Mean cosine similarity between small-batch (2k) and large-batch (1024) gradients at selected training iterations. Left: Small-batch vs. large-batch gradients of DB, SubTB(λ), and TB objectives. Right: Small-batch DB, SubTB(λ), and TB gradients vs. large-batch TB gradient.\n\n4.1.1 A CLOSER LOOK AT GRADIENT VARIANCE\n\nWe take a closer look at gradient bias and variance to understand the benefits of training GFlowNets with SubTB(λ). The methodology of these experiments is inspired by Ilyas et al. (2020).\n\nWe train GFlowNets on the 8 × 8 grid environment using SubTB(λ = 0.8) and monitor various gradient metrics during training. To remove the effect of parameter sharing between policies at different states and to isolate the effect of the objective, we use a tabular representation of the GFlowNet, i.e., all flows and policy logits are optimized as independent parameters.\n\nj\n\nGradient variance. To measure gradient variance, we use the following procedure for each training objective (DB, TB, or SubTB(λ)). A large batch of 210 = 1024 trajectories is sampled, and the gradient g (0) of the objective with respect to the policy logits at all states is computed for each trajectory τj in the batch. Then, for each k ∈ {0, 1, . . . , 9}, the gradients g (0) are combined into 210−k sub-batches, each of size 2k. The subbatch gradient g (k ) for the i-th sub-batch is set to the average of trajectory gradients g (0) contained within the sub-batch and computed for i ∈ {1, 2, . . . , 210−k }. We then report the average cosine similarity between the sub-batch and full-batch gradients:\n\ni\n\ni\n\nj\n\nFigure 5: Mean cosine similarity between small-batch (64) and large-batch (1024) gradients on the 8 × 8 grid environment. Above: Self-similarity of the DB, SubTB(λ), and TB gradients, showing DB < SubTB(λ) < TB in gradient variance. Below: Similarity of small-batch DB, SubTB(λ), and TB gradients to the large-batch TB gradient, showing that the small-batch SubTB(λ) gradient is a good estimator of large-batch TB.\n\n1 210−k\n\n210−k ∑︁\n\ni=1\n\ni\n\ng (k ) · g (10) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 g (10) 1\n\ng (k )\n\n(cid:13) (cid:13) (cid:13)\n\n.\n\n(cid:13) (cid:13) (cid:13)\n\ni If this quantity is positive, then gradient steps of infinitesimally small norm along the stochastic sub-batch gradient decrease the full-batch objective in expectation. Fig. 4 (left) shows the dependence of this metric on k at various iterations. A steeper curve, such as those of DB and SubTB(λ), indicates lower gradient variance. Fig. 5 (top) shows the metric at k = 6 (corresponding to the batch size of 64 used for training) over the course of training. We see that the DB gradient has the highest self-consistency at all iterations, TB has the lowest, and SubTB(λ = 0.8) is in between.\n\nGradient bias. We next compare the small-batch stochastic gradients with large-batch stochastic gradients, using different objectives for the small and full batches. Specifically, we compare the small-batch DB, SubTB(λ), and TB gradients with the full-batch TB gradient. (The full-batch TB gradient can be seen as a ‘canonical’ gradient against which bias can be measured, as its expectation equals the gradient of the KL divergence between the distribution over trajectories defined by PF and that defined by the reward R and PB; see §A.3 of Malkin et al. (2022).)\n\nFig. 5 (bottom) shows the cosine similarity at the batch size used for training. Notably, at intermediate iterations, the similarity of SubTB(λ) with TB is higher than that of TB with TB: despite its bias, the small-batch SubTB(λ) gradient estimates the full-batch TB gradient better than the small-batch TB gradient does. Fig. 4 (right) shows the dependence of the similarity on k at selected iterations and suggests that this effect may be even larger for smaller batch sizes. Moreover, at k = 10, the similarity of SubTB(λ) vs. TB always lies between DB vs. TB and TB vs. TB, indicating that SubTB(λ) interpolates between TB’s unbiased and DB’s biased estimates of the TB gradient.\n\nThe effect of learned state flows. For additional experiments, see §A.2.\n\n7\n\n0246810K0.00.20.40.60.81.0Cosine SimilarityTraining Step = 10000246810KTraining Step = 20000246810KTraining Step = 3000Loss Comp NameDB vs DBSubTB vs SubTBTB vs TB0246810K0.00.20.40.60.81.0Cosine SimilarityTraining Step = 10000246810KTraining Step = 20000246810KTraining Step = 3000Loss Comp NameDB vs TBSubTB vs TBTB vs TB050010001500200025003000Training Step0.40.50.60.70.80.9Cosine SimilarityGradient Self-Similarity, Learned Flows Loss Comp NameDB vs DBSubTB vs SubTBTB vs TB050010001500200025003000Training Step0.10.20.30.40.50.60.70.8Cosine SimilaritySimilarity with TB Gradient, Learned Flows Loss Comp NameDB vs TBSubTB vs TBTB vs TBFigure 6: Correlation between marginal sampling log-likelihood and log-reward on the molecule task. For each hyperparameter setting on the x-axis, we plot the best result over choices of the other hyperparameter(s) – α in the left plot, β in the centre plot, and both α and β in the right plot – with a solid line. The mean result over values of other hyperparameter(s) is plotted with a dashed line.\n\n4.2 SMALL MOLECULE SYNTHESIS\n\nWe use SubTB(λ) to train models on the molecule generation task of Bengio et al. (2021a). The task is to generate binders of the sEH (soluble epoxide hydrolase) protein, based on a docking prediction (Trott & Olson, 2010). To be precise, molecules are generated by sequentially joining ‘blocks’ from a fixed library to the partial molecular graph (Jin et al., 2020; Kumar et al., 2012), resulting in a state space of estimated size 1012. The reward function R is given by a pretrained proxy model made available by Bengio et al. (2021a). To adjust the greediness of the agent, an inverse temperature hyperparameter β is used, i.e., the reward used for training is R(x) = (cid:101)R(x) β, where (cid:101)R(x) is the proxy’s prediction. We train models with the DB, TB, and SubTB(λ) objectives, with four values each of λ, β, and learning rate, averaging the results over 3 random runs for each setting. We measure how well the trained models match the target distribution by the correlation of log R(x) and log p θ (x), the log-probability assigned to x by the GFlowNet, computed on a held-out set of terminal states x.3 The results are shown in Fig. 6. SubTB(λ), in particular with λ = 1, performs better than both DB and TB when the optimal hyperparameters α, β are used (solid lines) and is far more robust to the choice of hyperparameters (dashed lines). Additional details can be found in §B.\n\n4.3 SEQUENCE GENERATION\n\nWe consider three sequence generation tasks in which sequences are generated left to right, with each action appending one symbol from a vocabulary to a partial sequence: a synthetic task with varying sequence lengths and vocabulary sizes (§4.3.1), a practical biological sequence design task (§4.3.2), and a new protein design task with longer sequences (4.3.3). For all three tasks, we consider the baselines Soft Actor-Critic (Haarnoja et al., 2018; Christodoulou, 2019), A2C with Entropy regularization (Williams & Peng, 1991; Mnih et al., 2016) and MARS-like MCMC (Xie et al., 2021) and compare them with three GFlowNet training objectives: TB, FM, and SubTB(λ).\n\nIn §F, we also study a non-autoregressive sequence generation problem (inverse protein folding).\n\n4.3.1 BIT SEQUENCES\n\nWe consider the synthetic sequence generation setting from Malkin et al. (2022), where the goal is to generate sequences of bits of fixed length n = 120. The reward is specified by a set of modes M ⊂ X = {0, 1}n that is unknown to the learning agent. The reward of a generated sequence x is defined in terms of Hamming distance d from the modes: R(x) = exp(− miny ∈ M d (x, y)). The vocabulary size can be varied: for any integer k dividing 120, we take a vocabulary consisting of words of length k (so that the vocabulary size is 2k and the full sequence is generated in n k actions). By varying the value of k and keeping n and M constant, we study the behavior of learning agents with varying action space sizes and trajectory lengths without changing the underlying modeling problem. Most experiment settings are taken from Malkin et al. (2022); see §C.\n\n3Comparing the exact sampling and target distributions, like in §4.1, is not possible here, since we cannot enumerate all terminal states. However, the marginal likelihood that a trained GFlowNet generates a given x is tractable to compute by dynamic programming. For a model that samples perfectly from the target distribution, log(R(x)) and log p θ (x) would differ by a constant log Z independent of x and thus be perfectly correlated.\n\n8\n\n481016reward exponent 0.60.40.20.00.20.40.60.8logp(x) to logR(x) correlationFMDBTBSubTB(1)104103learning rate 0(DB)0.5124(TB)SubTB parameter Figure 7: Left: For the number of bits k ∈ {1, 2, 4, 6, 8, 10} in each vocabulary token, we plot the Spearman correlation between the sampling probability and reward on a test set for each method. Training with SubTB(λ) leads to policies that have the highest correlation with the reward across all lengths and vocabulary sizes. Right: For k = 1, the number of modes discovered by each method over the course of training is plotted. SubTB(λ) discovers more modes faster.\n\nModels are evaluated by computing the Spearman correlation, on a test set of sequences x, between the probability of generating x and the reward R(x). We also track the number of modes discovered during the training process for all the methods, see Fig. 7. We find that models trained with the SubTB(λ) objective have a higher Spearman correlation at the end of training and discover modes faster compared to the other GFlowNet objectives and non-GFlowNet baselines.\n\n4.3.2 ANTIMICROBIAL PEPTIDE GENERATION\n\nTable 2: Results on the AMP generation task (mean and standard error over 3 runs). Algorithm\n\nTop-100 Reward Top-100 Diversity\n\nNext, we consider the task of generating peptides with antimicrobial properties (AMPs). These sequences have maximum length 60 and use a vocabulary of 20 amino acids (and an end-of-sequence token), resulting in a state space of size 2160. The reward function is a pretrained proxy neural network (See Jain that estimates the antimicrobial activity. et al. (2022) for details on this task.) We train GFlowNets with the SubTB(λ), TB, and FM losses and compare them with baselines. To evaluate the trained models, we sample 2048 sequences from the policy, then compute the mean reward and mean pairwise edit distance of the top-100 reward sequences. The metrics and model architecture are taken from Malkin et al. (2022); see §D. The results are presented in Table 2. SubTB(λ) provides significant improvements over all the baselines (including TB, FM, and DB GFlowNets) in both reward and diversity.\n\nGFN-LSubTB(λ) GFN-LTB GFN-LFM/LDB SAC AAC-ER MCMC\n\n42.23 ± 3.4 31.42 ± 2.9 12.61 ± 1.32 8.36 ± 1.44 7.32 ± 0.76 12.56 ± 1.45\n\n0.96 ± 0.02 0.90 ± 0.03 0.78 ± 0.05 0.80 ± 0.01 0.79 ± 0.02 0.75 ± 0.02\n\n4.3.3 FLUORESCENT PROTEIN GENERATION\n\nTable 3: Results on the GFP generation task (mean and standard error over 3 runs). Algorithm\n\nTop-100 Reward Top-100 Diversity\n\nWe consider the task of generating protein sequences with fluorescence properties (Trabucco et al., 2022) to evaluate SubTB(λ) in settings with longer trajectories. In this task, sequences have a fixed length of 237, and the size of the state space is 20237. The proxy reward function R(x) is trained on a dataset of proteins with their fluorescence scores from Sarkisyan et al. (2016). The metrics and models are the same as in §4.3.2; see §E for details.\n\nGFN-LSubTB(λ) GFN-LTB GFN-LFM/LDB SAC AAC-ER MCMC\n\n1.18 ± 0.10 0.76 ± 0.19 0.30 ± 0.08 0.23 ± 0.03 0.22 ± 0.02 0.28 ± 0.01\n\n204.44 ± 0.45 204.31 ± 0.44 190.21 ± 6.78 120.32 ± 15.57 113.65 ± 21.31 169.17 ± 12.44\n\nThe GFlowNet objectives outperform all other methods in both metrics, finding more diverse and higher-reward sequences (Table 3). SubTB(λ) significantly outperforms TB, while achieving a similar diversity. We note that the advantage of SubTB(λ) is greater than that in the AMP task (Table 2) and speculate that the benefits of SubTB(λ) become more prominent for longer action sequences.\n\n5 DISCUSSION AND CONCLUSION\n\nWe have given evidence of a bias-variance tradeoff in GFlowNet training algorithms. The highvariance stochastic regression objective of TB and the low-variance local consistency objective of DB lie at opposite ends of this range. We showed that SubTB(λ) can harness the variance-reducing effects of local objectives while retaining the fast credit assignment properties of trajectory-level objectives. We see learnable strategies for selecting and weighting (sub)trajectories for training – e.g., a dynamic choice of λ and an active-learning approach to sampling trajectories – as the most interesting questions for future work. The ability of subtrajectory objectives to learn from incomplete episodes also makes their application in RL environments an appealing research direction.\n\n9\n\n246810Number of Bits in Action Space0.10.20.30.40.50.60.70.8Spearman Correlation (p, r)GFN - SubTrajectory-lambdaGFN - Trajectory BalanceGFN - Flow MatchingA2C - EntropySoft Actor-CriticMCMC01000020000300004000050000Training Steps051015202530Number of ModesGFN - SubTrajectory-lambdaGFN - Trajectory BalanceGFN - Flow MatchingA2C - EntropySoft Actor-CriticMCMCREPRODUCIBILITY STATEMENT\n\nWe provide extensive experiment details, such as learning rates, batch sizes, number of training steps, choices of λ, description of attempted hyperparameters, and additional clarifying experiments in the Appendices. Code for experiments on the hypergrid domain (§4.1) and on the molecule domain (§4.2) is also provided with the submission.\n\nREFERENCES\n\nLeemon Baird. Residual algorithms: Reinforcement learning with function approximation. Inter-\n\nnational Conference on Machine Learning (ICML), 1995.\n\nEmmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal\n\ndifference learning. International Conference on Machine Learning (ICML), 2020.\n\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS), 2021a.\n\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward Hu, Mo Tiwari, and Emmanuel Bengio.\n\nGFlowNet foundations. arXiv preprint 2111.09266, 2021b.\n\nDavid Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for\n\nrobust design. International Conference on Machine Learning (ICML), 2019.\n\nSidhartha Chaudhury, Sergey Lyskov, and Jeffrey J Gray. PyRosetta: a script-based interface for implementing molecular modeling algorithms using Rosetta. Bioinformatics, 26(5):689–691, 2010.\n\nPetros Christodoulou. Soft actor-critic for discrete action settings. arXiv preprint 1910.07207, 2019.\n\nTristan Deleu, Ant ́onio G ́ois, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer, and Yoshua Bengio. Bayesian structure learning with generative flow networks. Uncertainty in Artificial Intelligence (UAI), 2022.\n\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. International Conference on Learning Representations (ICLR), 2018.\n\nHan Guo, Bowen Tan, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Text generation with efficient\n\n(soft) Q-learning. arXiv preprint 2106.07704, 2021.\n\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with\n\ndeep energy-based policies. International Conference on Machine Learning (ICML), 2017.\n\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. International Conference on Machine Learning (ICML), 2018.\n\nElad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy\n\nexploration. International Conference on Machine Learning (ICML), 2019.\n\nAndrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry International Con-\n\nRudolph, and Aleksander Madry. A closer look at deep policy gradients. ference on Learning Representations (ICLR), 2020.\n\nRiashat Islam, Zafarali Ahmed, and Doina Precup. Marginalized state distribution entropy regular-\n\nization in policy optimization. arXiv preprint 1912.05128, 2019.\n\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena InterSimine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. national Conference on Machine Learning (ICML), 2022.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Chapter 11. junction tree variational autoencoder for molecular graph generation. Drug Discovery, pp. 228–249, 2020. ISSN 2041-3211.\n\n10\n\nMichael J Kearns and Satinder P Singh. Bias-variance error bounds for temporal difference updates.\n\nConference on Learning Theory (COLT), 2000.\n\nAshutosh Kumar, Arnout Voet, and Kam Y.J. Zhang. Fragment based drug design: from experimen-\n\ntal to computational approaches. Current medicinal chemistry, 19(30):5128–5147, 2012.\n\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS), 2022.\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. Neural Information Processing Systems (NIPS), 2016.\n\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between value and policy based reinforcement learning. Neural Information Processing Systems (NIPS), 2017.\n\nMalak Pirtskhalava, Anthony A Amstrong, Maia Grigolava, Mindia Chubinidze, Evgenia Alimbarashvili, Boris Vishnepolsky, Andrei Gabrielian, Alex Rosenthal, Darrell E Hurt, and Michael Tartakovsky. Dbaasp v3: Database of antimicrobial/cytotoxic activity and structure of peptides as a resource for development of new therapeutics. Nucleic Acids Research, 49(D1):D288–D297, 2021.\n\nCarol A Rohl, Charlie EM Strauss, Kira MS Misura, and David Baker. Protein structure prediction\n\nusing Rosetta. In Methods in enzymology, volume 383, pp. 66–93. Elsevier, 2004.\n\nKaren S Sarkisyan, Dmitry A Bolotin, Margarita V Meer, Dinara R Usmanova, Alexander S Mishin, George V Sharonov, Dmitry N Ivankov, Nina G Bozhanova, Mikhail S Baranov, Onuralp Soylemez, et al. Local fitness landscape of the green fluorescent protein. Nature, 533(7603):397–401, 2016.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint 1707.06347, 2017.\n\nSam Sinai, Richard Wang, Alexander Whatley, Stewart Slocum, Elina Locane, and Eric Kelsic. AdaLead: A simple and robust adaptive greedy search algorithm for sequence design. arXiv preprint 2010.02141, 2020.\n\nRichard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3\n\n(1):9–44, 1988.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.\n\nBrandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine. Design-bench: Benchmarks for data-driven offline model-based optimization. International Conference on Machine Learning (ICML), 2022.\n\nOleg Trott and Arthur J Olson. AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455–461, 2010.\n\nHado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-\n\ndayil. Deep reinforcement learning and the deadly triad. arXiv preprint 1812.02648, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NIPS), 2017.\n\nRonald Williams and Jing Peng. Function optimization using connectionist reinforcement learning\n\nalgorithms. Connection Science, 3:241–, 09 1991. doi: 10.1080/09540099108946587.\n\nYutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. MARS: International Conference on\n\nMarkov molecular sampling for multi-objective drug discovery. Learning Representations (ICLR), 2021.\n\n11\n\nChuheng Zhang, Yuanying Cai, Longbo Huang, and Jian Li. Exploration by maximizing R ́enyi entropy for reward-free RL framework. Association for the Advancement of Artificial Intelligence (AAAI), 2021.\n\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML), 2022.\n\nShangtong Zhang, Wendelin Boehmer, and Shimon Whiteson. Deep residual reinforcement learn-\n\ning. Autonomous Agents and Multi-Agent Systems (AAMAS), 2020.\n\nBrian D Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal\n\nentropy. Carnegie Mellon University, 2010.\n\n12\n\nFigure A.1: Additional results for hypergrid experiments. Above: The evolution of the L1 between empirical sampling and target distributions on the harder variants of 4-dimensional grids, in the same format as Fig. 3. Below: The number of cumulative distinct terminal states visited as a function of training time on the standard 2-dimensional grid. Models trained with SubTB(λ) discover more states faster.\n\nA EXPERIMENT DETAILS: HYPERGRID\n\nThe environment is identical to that in Malkin et al. (2022), with reward function parameters (R0, R1, R2) = (10−3, 0.5, 2) for the standard variant of the grid and (10−4, 1.0, 3.0) for the harder variant. The models giving logits of PF (−|s) and PB (−|s), as well as log F (s), are MLPs of the same architecture as in Bengio et al. (2021a), taking a one-hot representation of the coordinates of s as input and sharing all layers except the last. The initial state flow log Z = log F (s0) is an independent parameter whose learning rate is set to 10× the learning rate of other parameters.\n\nAll models are trained with the Adam optimizer and a batch size of 16 for a total of 106 trajectories (62500 batches). The optimal learning rate for each experiment is chosen from {0.0005, 0.00075, 0.001, 0.003, 0.005, 0.0075, 0.01}, and λ = 0.9 is chosen as the optimal value from the set {0.8, 0.9, 0.99}.\n\nGradient bias and variance experiments are conducted in the harder variant of the 8 × 8 grid. The tabular GFlowNet is trained using Adam with a learning rate 0.007 and the SubTB(λ = 0.8) objective.\n\nA.1 ADDITIONAL EXPERIMENTS\n\nFig. A.1 shows additional results on more difficult grid environments.\n\nWe perform another experiment in which only short (up to length 4) subtrajectories are used for training with the SubTB(λ) objective (i.e., the sum in (11) is truncated to exclude pairs (i, j) with j − i > 4). The results, shown in Fig. A.4, show that SubTB(λ) continues to perform strongly in this restricted setting.\n\n13\n\n1031041051060.00.51.01.52.0L18×8×8×8(hard)10310410510612×12×12×12(hard)10310410510616×16×16×16(hard)10310410510620×20×20×20(hard)10310410510624×24×24×24(hard)10310410510628×28×28×28(hard)10310410510632×32×32×32(hard)10310410510640×40×40×40(hard)TBSubTB(0.9)1001042040608×810010405010015012×12100104010020016×16100104010020030040020×20100104020040060024×24100104020040060080028×281001040250500750100032×3210010405001000150040×4010010401000200050×50100104010002000300060×60Figure A.2: Empirical L1 curves on the 8 × 8 grid for varying values of λ.\n\nFig. A.2 shows the effect of the SubTB parameter λ on the training curves, showing a gradual interpolation between DB and TB and fastest convergence at values slightly less than 1.\n\nFig. A.3 contains visualizations of the exploration behavior of different training algorithms. It shows that TB can perform better with off-policy training and can benefit from a higher temperature of the policy logits, but still does not learn as fast as SubTB(λ), nor does it find all the modes in the maximum number of training iterations.\n\nA.2 MORE ON BIAS AND VARIANCE: THE EFFECT OF LEARNED STATE FLOWS\n\nTo better understand the variance-reducing properties of SubTB(λ), we perform the gradient bias experiments with a modified computation of gradients that removes the factor of learning the state flows. Recall from §2.1 that a forward policy PF uniquely determines a distribution over trajectories. If the initial state flow Z and forward policy PF are fixed, there is a unique state flow function F F and backward policy PB that satisfy the detailed balance conditions (6). This ‘true forward’ flow function, written F F (s) = Z (cid:205)τ:s∈ τ PF (τ), is determined by an initial state flow fixed to the true\n\n14\n\n1031050.000.250.500.751.001.251.501.752.008×8, =0.011031050.000.250.500.751.001.251.501.752.008×8, =0.11031050.000.250.500.751.001.251.501.752.008×8, =0.21031050.000.250.500.751.001.251.501.752.008×8, =0.31031050.000.250.500.751.001.251.501.752.008×8, =0.71031050.000.250.500.751.001.251.501.752.008×8, =0.81031050.000.250.500.751.001.251.501.752.008×8, =0.91031050.000.250.500.751.001.251.501.752.008×8, =1.31031050.000.250.500.751.001.251.501.752.008×8, =3.01031050.000.250.500.751.001.251.501.752.008×8, =5.01031050.000.250.500.751.001.251.501.752.008×8, =10.01031050.000.250.500.751.001.251.501.752.008×8, =15.01031050.000.250.500.751.001.251.501.752.008×8, =20.01031050.000.250.500.751.001.251.501.752.008×8, =30.01031050.000.250.500.751.001.251.501.752.008×8, =50.01031050.000.250.500.751.001.251.501.752.008×8, =100.0DBSubTB()TB interpolation (hard grid)Figure A.3: Training GFlowNets on the harder variants of 2-dimensional grids using a tempered training policy (left), and a training policy that takes a uniformly random action with probability ε at each sampling step (right).\n\nFigure A.4: Training GFlowNets using only short subtrajectories in different grid environments using the SubTB(λ = 0.9) objective.\n\npartition function Z = (cid:205)x ∈ X R(x) and the learned forward policy PF. Similarly, the ‘true backward’ flow function, written F B (s) = (cid:205)τ:s∈ τ PB (τ)R(x τ) where x τ is the terminal state of τ, is uniquely determined by the reward function R and the learned backward policy PB. In particular, F B (s0) = (cid:205)x ∈ X R(x). We repeat the experiments on gradient bias, but by replacing the learned state flows F in the losses by either the true forward or the true backward state flows (F F or F B respectively) computed exactly using the current values of the learned PF and PB. (These modifications are not applied in training, but are used only to compute the gradient similarities. The small size of the environment makes computation of the true state flows tractable; this is not possible in general.)\n\nThe gradient similarity over the course of training is shown in Fig. A.5 (cf. Fig. 5 in the main text). The similar behavior of SubTB(λ) with learned and true forward state flows suggests that the learned state flows remain close enough to their optimal values and that the variance-reducing benefits of SubTB(λ) with true state flows are retained.\n\nB EXPERIMENT DETAILS: MOLECULES\n\nAll experiments with SubTB(λ) are based upon the published code of Malkin et al. (2022), which extends that of Bengio et al. (2021a). The proxy model giving the reward, the held-out set of molecules used to compute the correlation metric, and the GFlowNet model architecture – a graph neural network – are identical to those in Bengio et al. (2021a), and the off-policy exploration rate and early stopping likelihood are the same as those tuned for the training with the TB objective in\n\n15\n\n1041051060.000.250.500.751.001.251.501.752.0012×121041051060.000.250.500.751.001.251.501.752.0016×161041051060.000.250.500.751.001.251.501.752.0020×201041051060.000.250.500.751.001.251.501.752.0024×241041051060.000.250.500.751.001.251.501.752.0028×281041051060.000.250.500.751.001.251.501.752.0032×32temp=1temp=2temp=3temp=4SubTBTraining with tempered training policy1041051060.000.250.500.751.001.251.501.752.0012×121041051060.000.250.500.751.001.251.501.752.0016×161041051060.000.250.500.751.001.251.501.752.0020×201041051060.000.250.500.751.001.251.501.752.0024×241041051060.000.250.500.751.001.251.501.752.0028×281041051060.000.250.500.751.001.251.501.752.0032×32=0.0=0.0625=0.125=0.25=0.5=1SubTBTraining with different uniform random action probabilities 1031041051060.00.51.01.52.0L132×32(hard)10310410510640×40(hard)TBSubTB(0.9)SubTB(0.9) shortFigure A.5: Gradient similarity with state flows analytically computed in two ways (see §A.2). (Compare with Fig. 5.)\n\nMalkin et al. (2022). All models are trained for a maximum of 50000 batches of 4 trajectories each. (Some training runs terminated early because of numerical overflows in the gradients, in which case we report the metric of the last stable model whose cumulative number of batches is a multiple of 5000.)\n\nC EXPERIMENT DETAILS: BIT SEQUENCES\n\nThe modes M as well as the test sequences are selected as described in Malkin et al. (2022). The policy for all methods is parameterized by a Transformer (Vaswani et al., 2017) with 3 layers, dimension 64, and 8 attention heads. All methods are trained for 50,000 iterations with minibatch size of 16 using Adam optimizer. For GFlowNets with FM objective as well as the baselines, we use the exact same implementation and hyperparameters reported in Malkin et al. (2022). For TB and SubTB(λ), we pick the best learning rate from {0.0075, 0.001, 0.001, 0.003, 0.005} for forward logits, and for Z, use a learning rate of 10× the learning rate for forward logits. For SubTB(λ), we found the best λ value of 1.9 from the values {0.8, 0.9, 1.1, 1.3, 1.5, 1.7, 1.9, 2.0}.\n\nD EXPERIMENT DETAILS: ANTIMICROBIAL PEPTIDE GENERATION\n\nFollowing Malkin et al. (2022) we use the following amino acids: [‘A’, ‘C’, ‘D’, ‘E’, ‘F’, ‘G’, ‘H’, ‘I’, ‘K’, ‘L’, ‘M’, ‘N’, ‘P’, ‘Q’, ‘R’, ‘S’, ‘T’, ‘V’, ‘W’, ‘Y’]. We take 6438 known AMP sequences and 9522 non-AMP sequences from the DBAASP database Pirtskhalava et al. (2021). The classifier that serves as the proxy reward function is trained on this dataset, using 20% of the data as the validation set. The reward model is a Transformer, with 4 hidden layers, hidden dimension 64, and 8 attention heads. We train it with a minibatch of size 256, with learning rate 10−4, and with early stopping on the validation set. We use a Transformer with 3 hidden layers with hidden dimension 64 with 8 attention heads as the architecture of the policy for all methods. All methods are trained for 20, 000 iterations, with a minibatch size of 16, using the reported hyperparameters for all the baselines from (Malkin et al., 2022). For TB and SubTB(λ), we pick the best learning rates from {0.005, 0.007, 0.01, 0.03, 0.05, 0.07} for forward logits and from {0.007, 0.01, 0.03, 0.05} for log Z. For SubTB(λ), the best performing λ value of 1.9 chosen from {0.9, 0.99, 1.1, 1.2, 1.3, 1.4, 1.6, 1.7, 1.8, 1.9, 2.0} is used.\n\nE EXPERIMENT DETAILS: FLUORESCENT PROTEIN GENERATION\n\nWe consider a variant of the GFP task from Trabucco et al. (2022). The vocabulary of amino acids is the same as §D: [‘A’, ‘C’, ‘D’, ‘E’, ‘F’, ‘G’, ‘H’, ‘I’, ‘K’, ‘L’, ‘M’, ‘N’, ‘P’, ‘Q’, ‘R’, ‘S’, ‘T’, ‘V’, ‘W’, ‘Y’]. Following Trabucco et al. (2022), we consider the dataset of 56,086 proteins from Sarkisyan et al. (2016) processed based on Brookes et al. (2019). Each protein is accompanied by a score quantifying its fluorescence. As with the AMP data, we keep 20% of the data as a validation set used for early-stopping. The regressor trained with the dataset is a Transformer, with 4 hidden layers, hidden dimension 64, and 8 attention heads. We train it with a minibatch of size 256, with learning rate 10−4, with early stopping on the validation set. The architecture of the policy for all methods is a Transformer with 3 hidden layers with hidden dimension 64 with 8 attention heads. All methods are trained for 20, 000 iterations, with a minibatch size of 16. We use the same implementation for all methods as the ones used in §D.\n\n16\n\n05001000150020002500300035004000Training Step0.00.20.40.60.8Cosine SimilaritySimilarity with TB Gradient, True Forward Flows Loss Comp NameDB vs TBSubTB vs TBTB vs TB05001000150020002500300035004000Training Step0.00.20.40.60.8Cosine SimilaritySimilarity with TB Gradient, True Backward Flows Loss Comp NameDB vs TBSubTB vs TBTB vs TBFigure F.1: The Spearman correlation between the sampling probability and reward on a test set is plotted over the course of training for each value of λ.\n\nTo define an exploratory training policy, we set the the random action probability to 0.01 selected from {0.0001, 0.0005, 0.001, 0.01} and the reward exponent β (having the same meaning as in §4.2) to 3 selected from {2, 3, 4}. For trajectory balance we use a learning rate of 5 × 10−3 selected from {10−5, 10−4, 5 × 10−4, 10−3, 5 × 10−3} for the flow parameters and 1 × 10−2 for log Z. For SubTB(λ), we choose the best λ from {0.7, 0.8, 0.9, 0.99}, and found λ = 0.99 to perform the best. For TB and SubTB(λ), we tune for the best learning rates from {0.0001, 0.0003, 0.0005, 0.00075, 0.001} for the forward logits. For log Z, we use a learning rate of 10× the learning rate for the forward logits.\n\nFor FM we use a learning rate of 10−3 selected from {10−5, 10−4, 5 × 10−4, 10−3, 5 × 10−3} with leaf loss coefficient λT = 30. For A2C with entropy regularization we share parameters between the actor and critic networks, and use learning rate of 5 × 10−3 selected from {10−5, 10−4, 5 × 10−4, 10−3, 5 × 10−3} with entropy regularization coefficient 5 × 10−2 selected from {10−4, 10−3, 5 × 10−3, 10−2, 5 × 10−2}. For SAC we use the formulation in Christodoulou (2019) with a learning rate of 10−3 selected from {10−5, 10−4, 5 × 10−4, 10−3, 5 × 10−3}, a target network update frequency of 400 and initial random steps of 200. For the MARS baseline, we set the learning rate to 5 × 10−4 selected from {10−5, 10−4, 5 × 10−4, 10−3, 5 × 10−3}. We run the experiments on 3 seeds and report the mean and standard error over the three runs in Table 3.\n\nF INVERSE PROTEIN FOLDING: NON-AUTOREGRESSIVE SEQUENCE\n\nGENERATION\n\nWe consider the inverse protein folding problem suggested in Sinai et al. (2020). A target protein 3D backbone conformation is given, and the task is to sample amino acid sequences of a fixed length L = 40 from the Boltzmann distribution corresponding to their energy in the target conformation. The energy is provided by a physics model (Rohl et al., 2004; Chaudhury et al., 2010). The policy model is a 3-layer convolutional architecture that closely follows previous work (Sinai et al., 2020). Specifically, for the policy function, the convolution size was set to 7 with 32 hidden features and ReLU activation in each layer. The policy network has one additional convolutional layer of size 20 (number of amino acids), and without the activation function. The flow network has an additional two linear layers of sizes [1280,64], and [64, 1] with ReLU activation in between. We report mean result over three runs.\n\nFor this task, rather than generating sequences from left to right, we consider an action space in which actions modify one letter at a time at arbitrary positions. The first action uniformly randomly samples an amino acid sequence. On each subsequent action, the agent selects a position in the sequence and replaces the letter in this position with another letter in the vocabulary. Generation terminates after exactly N = 40 replacement steps. The forward policy is conditioned on the number of steps taken so far in the trajectory; the backward policy is fixed to be uniform over the N · L actions.\n\nAs a metric of how well the learned model matches the target distribution, we measure the correlation between log R(x) and the marginal sampling likelihood log p θ (x) on a held-out set of terminal\n\n17\n\nstates. The results are presented in Fig. F.1. We observe that intermediate values of lambda lead to the best fit to the target distribution.\n\n18",
    "reference": "# Summary Of The Paper\n\nThe paper propose a novel GFlowNet training objective called subtrajectory balance $SubTB(\\lambda)$ that can learn from partial The paper proposes a novel GFlowNet training objective called subtrajectory balance $SubTB(\\lambda)$ that can learn from partial action subsequences of varying lengths. Empirical results demonstrate that $SubTB(\\lambda)$ can improve the convergence of GFlowNets in some simple environments.\n\n# Strength And Weaknesses\n\nThe proposed method is novel, and the paper is well-written. However, I have some concerns and thus do not recommend acceptance. \n\nQ1. My biggest concern is that the experiments are quite simple. How well does $SubTB(\\lambda)$ perform over complex tasks in high-dimensional environments?\n\nQ2. The motivation of $SubTB(\\lambda)$ is not very celar. More theoretical analysis is required to explain why $SubTB(\\lambda)$ improves the convergence of GFlowNets.\n\nQ3. The current version lacks discussion about the limitations of $SubTB(\\lambda)$.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is novelty, but the motivation is not very clear.\n\n# Summary Of The Review\n\nI have some concerns and thus do not recommend acceptance.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nACTIVE SAMPLING FOR NODE ATTRIBUTE COMPLETION ON GRAPHS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nNode attribute is one kind of crucial information on graphs, but real-world graphs usually face attribute-missing problem where attributes of partial nodes are missing and attributes of the other nodes are available. It is meaningful to restore the missing attributes so as to benefit downstream graph learning tasks. Popular GNN is not designed for this node attribute completion issue and is not capable of solving it. Recent proposed Structure-attribute Transformer (SAT) framework decouples the input of graph structures and node attributes by a distribution matching technique, and can work on it properly. However, SAT leverages nodes with observed attributes in an equally-treated way and neglects the different contributions of different nodes in learning. In this paper, we propose a novel active sampling algorithm (ATS) to more efficiently utilize the nodes with observed attributes and better restore the missing node attributes. Specifically, ATS contains two metrics that measure the representativeness and uncertainty of each node’s information by considering the graph structures, representation similarity and learning bias. Then, these two metrics are linearly combined by a Beta distribution controlled weighting scheme to finally determine which nodes are selected into the train set in the next optimization step. This ATS algorithm can be combined with SAT framework together, and is learned in an iterative manner. Through extensive experiments on 4 public benchmark datasets and two downstream tasks, we show the superiority of ATS in node attribute completion.\n\n1\n\nINTRODUCTION\n\nNode attribute, known as a kind of important information on graphs, plays a vital role in many graph learning tasks. It boosts the performance of Graph Neural Network (GNN) Defferrard et al. (2016); Kipf & Welling (2017); Xu et al. (2019b); Veliˇckovi ́c et al. (2018) in various domains, e.g. node classification Jin et al. (2021); Xu et al. (2019a) and community detection Sun et al. (2021); Chen et al. (2017). Meanwhile, node attribute provides human-perceptive demonstrations for the non-Euclidean structured data Zhang et al. (2019); Li et al. (2021). In spite of its indispensability, real-world graphs may have missing node attributes due to kinds of reasons Chen et al. (2022). For example, in citation graphs, key terms or detailed content of some papers may be inaccessible because of copyright protection. In social networks, profiles of some users may be unavailable due to privacy protection. When observing the attributes of partial nodes on graphs, it is significant to restore the missing attributes of the other nodes so as to benefit the downstream graph learning tasks. Namely, this is the goal of node attribute completion task.\n\nCurrently, there are limited works on the node attribute completion problem. Recent graph learning algorithms such as network embedding Cui et al. (2018) and GNN are not targeted for this problem and are limited in solving it. Random walk based methods Perozzi et al. (2014); Tang et al. (2015); Grover & Leskovec (2016) are effective in learning node embeddings on large-scale graphs. While they only take the graph structures into consideration and ignore the rich information from node attributes. Attributed random walk models Huang et al. (2019); Lei Chen & Bronstein (2019) can potentially deal with this problem but they rely on high-quality random walks and carefully designed sampling strategies which are hard to be guaranteed Yang et al. (2019). The popular GNN framework takes graph structures and node attributes as a coupled input and can work on the node attribute completion problem by some attribute-filling tricks, while these tricks introduce noise in learning and bring worse performance. In last few years, researchers begin to concentrate on the learning\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nproblem on the attribute-missing graphs. Chen et al. (2022) propose a novel structure-attribute transformer (SAT) framework that can handle the node attribute completion case. SAT leverages structures and attributes in a decoupled scheme and achieves the joint distribution modeling by matching the latent codes of structures and attributes.\n\nAlthough SAT has shown great promise on node attribute completion problem, it leverages the nodes with observed attributes in an equally-treated manner and ignores the different contributions of nodes in the learning schedule. Given limited nodes with observed attributes, it is more important to notice that different nodes have different information (e.g. degrees, neighbours, etc.) and should Importance re-weighting Wang et al. (2017); have different importance in the learning process. Fang et al. (2020); Byrd & Lipton (2019) on the optimization objective may come to mind to be a potential solution. Whereas, the information of nodes is influenced by each other and has more complex patterns. The importance distribution is implicit, intractable and rather complicated, raising great difficulties to design its formulation. It’s challenging to find a more practical way to exert the different importance of the partial nodes with observed attributes at different learning stages.\n\nIn this paper, we propose an active sampling algorithm named ATS to better leverage the partial nodes with observed attributes and help SAT model converge to a more desirable state. In particular, ATS measures the representativeness and uncertainty of node information on graphs to adaptively and gradually select nodes from the candidate set to the train set after each training epoch, and thus encourage the model to consider the node’s importance in learning. The representativeness and uncertainty are designed by considering the graph structures, representation similarity and learning bias. Furthermore, it is interesting to find that the learning prefers nodes of high representativeness and low uncertainty at the early stage while low representativeness and high uncertainty at the late stage. Thereby, we proposes a Beta distribution controlled weighting scheme to exert adaptive learning weights on representativeness and uncertainty. In this way, these two metrics are linearly combined as the final score to determine which nodes are selected into the train set in next optimization epoch. The active sampling algorithm (ATS) and the SAT model are learned in an iterative manner until the model converges. Our contributions are as summarized follows:\n\n• In node attribute completion, to better leverage the partial nodes with observed attributes, we advocate to use active sampling algorithm to adaptively and gradually select samples into the train set in each optimization epoch and help the model converge to a better state.\n\n• We propose a novel ATS algorithm to measure the importance of nodes by designed representativeness and uncertainty metrics. Furthermore, when combining these two metrics as the final score function, we propose a Beta distribution controlled weighting scheme to better exert the power of representativeness and uncertainty in learning.\n\n• We combine ATS with SAT, a newly node attribute completion model, and conduct extensive experiments on 4 public benchmarks. Through the experimental results, we show that our ATS algorithm can help SAT reach a better optimum, and restore higher-quality node attributes that benefit downstream node classification and profiling tasks.\n\n2 RELATED WORK\n\n2.1 DEEP GRAPH LEARNING\n\nWith the development of deep representation learning in the Euclidean vision domain Voulodimos et al. (2018), researchers have studied a lot of deep learning methods on the non-Euclidean graphs Zhang et al. (2022b). Random walk based methods can learn node embeddings by random walks , which only considers the structural information and cannot generalize to new graphs. To tackle this problem, the attributed random walk based methods (e.g.GraphRNA Huang et al. (2019)) apply random walks on both structures and attributes. These random walk based methods are practical, but they demand hardly-acquired high-quality random walks to guarantee good performance. Graph Neural Network (GNN) Scarselli et al. (2008); Defferrard et al. (2016); Kipf & Welling (2017) realizes ’graph-in, graph-out’ that transforms the embeddings of node attributes while maintaining the connectivity Sanchez-Lengeling et al. (2021). GNN performs a message passing scheme, which is reminiscent of standard convolution as in Graph Convolutional Networks (GCN) Kipf & Welling (2017). GNN can infer the distribution of nodes based on node attributes and edges and achieve impressive results on graph-related tasks. There are also numerous creative modifications in GNN.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nGAT Veliˇckovi ́c et al. (2018) introduces multi-head attention into GNN. GraphSAGE Hamilton et al. (2017) moves to the inductive learning setting to deal with large-scale graphs.\n\nRecently, more works have emphasized the importance of node attributes in graph-related downstream tasks. Both SEAL Pan et al. (2022) and WalkPool Zhang & Chen (2018) encode node representations with node attributes to achieve superior link prediction performance. In most realworld scenarios, attributes of some nodes may be inaccessible, so the node attribute completion task appears. Despite GNN’s success, there are few works on this task. Recent SAT Chen et al. (2022) assumes a shared-latent space assumption on graphs and proposes a novel GNN-based distribution matching algorithm. It decouples structures and attributes and simultaneously matches the distribution of respective latent vectors. WGNN developed by Chen et al. (2021) learns node representations in Wasserstein space without any imputation. Jin et al. (2021) propose the HGNN-AC model to learn topological embedding and attribute completion with weighted aggregation. PaGNNs Jiang & Zhang (2020) can reconstruct the missing attributes based on a partial message-propagation scheme. Among them, SAT performs well and has open-source implementations, so we refer to SAT as a primary base model for completing missing node attributes.\n\n2.2 ACTIVE SAMPLING ON GRAPHS\n\nActive learning assists the model to achieve as better performance as possible while labeling as few samples as possible Ren et al. (2021). It’s usually combined with deep learning model to select the most influential samples from unlabeled dataset and then label them for training to reduce the annotation cost Yoo & Kweon (2019). There are also some works of active learning on graph data. Early works Gadde et al. (2014); Gu et al. (2013); Ji & Han (2012) mainly take graph structures into consideration and design the query strategy regardless of node attributes. With the development of deep learning, many active learning algorithms are designed based on GNN. The query strategy of AGE Cai et al. (2017) measures the amount of the information contained in different nodes to select the most informative candidate node. Similar to AGE, ANRMAB Gao et al. (2018) adopts the weighted sum of different heuristics, but it adjusts the weights based on a multi-armed bandit framework. Caramalau et al. (2021) discuss two novel sampling methods: UncertainGCN and CoreGCN, which are based on uncertainty sampling and CoreSet Sener & Savarese (2017), respectively.\n\nNevertheless, most of today’s popular active sampling algorithms on graphs aim to resolve the node classification task and focus on how to reduce the annotation cost. For this node attribute completion task, since the attribute-observed nodes are limited and the dimension of node attributes is much higher than node classes, we demand a more advanced active sampling algorithm to help the primary model utilize the attribute-observed nodes more efficiently and learn the complicated attribute distribution better. In addition, the current query strategies measure the uncertainty by an unsupervised manner, but we propose a supervised one to make the sampling closer to the primary model.\n\n3 PRELIMINARY\n\n3.1 PROBLEM DEFINITION\n\nNo\n\nFor node attribute completion task, we denote G = (V, A, X) as a graph with node set V = {v1, v2, . . . , vN }, the adjacency matrix A ∈ RN ×N and the node attribute matrix X ∈ RN ×F . } is the set of attribute-observed nodes. The attribute information of V o 1, vo V o = {vo is X o = {xo }. } is the set of attribute-missing nodes. The attribute information of V u is 1 , vu V u = {vu } and the structural information of V u is Au = {au 1 , xu X u = {xu }. More specifically, we have V = V u ∪ V o, V u ∩ V o = ∅, and N = No + Nu. We expect to complete the missing node attributes X u based on the observed node attributes X o and structural information A.\n\n} and the structural information of V o is Ao = {ao\n\n2, ..., vo 1, xo 2 , ..., vu 2 , ..., xu\n\n2 , ..., au\n\n2, ..., xo\n\n2, ..., ao\n\n1 , au\n\n1, ao\n\nNu\n\nNu\n\nNu\n\nNo\n\nNo\n\nFor active sampling algorithm, we denote the total training set as T , in which the node attributes are known. The current training set of SAT model is T L and the set containing the candidate nodes is denoted as T U . We have T = T L ∪ T U . We design a reasonable sampling strategy named ATS which iteratively transfers the most suitable candidate nodes from T U to T L to boost the training efficiency of SAT until T U = ∅ and the model converges.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.2 STRUCTURE-ATTRIBUTE TRANSFORMER\n\nFigure 1: The general architecture of SAT. The attributes and the structure are encoded by EX and EA and reconstructed by DX and DA. Meanwhile, SAT matches the latent codes of structures and attributes to a prior distribution by adversarial distribution matching.\n\nSince we combine SAT with our ATS to demonstrate how ATS works, we briefly introduce SAT in this part. The general architecture of SAT is shown in Figure 1. SAT inputs structures and attributes in a decoupled manner, and matches the joint distribution of structures and attributes by a paired structure-attribute matching and an adversarial distribution matching. During the paired structure-attribute matching, we have a structure encoder EA (a two-layer GNN such as GCN) and an attribute encoder EX (a two-layer MLP) that encodes the structural information ai and the attribute information xi into za and zx, respectively. Then two decoders DA and DX decode za and zx as the structures ai and attributes xi in both parallel and cross ways. Encoders and decoders are parameterized by φ and θ respectively. The joint reconstruction loss Lr of SAT can be written as:\n\nmin θx,θa,φx,φa\n\nLr = −\n\n1 2\n\nExi [Eqφx (zx|xi)[log pθx (xi|zx)]] −\n\n1 2\n\nEai[Eqφa (za|ai)[log pθa (ai|za)]]\n\n−\n\n1 2\n\nλc · Eai[Eqφa (za|ai)[log pθx(xi|za)]] −\n\n1 2\n\nλc · Exi[Eqφx (zx|xi)[log pθa (ai|zx)]]\n\n(1)\n\nwhere pθ x and pθ a are the encoders, qφx and pφa are the decoders. The first two terms in Eq. 1 represent the self-reconstruction stream. The latent variable zx, za are decoded to ˆX o, ˆA by twolayer MLP decoders DX , DA respectively. The last two terms indicate the cross-reconstruction stream, where zx and za are decoded to ˆA and ˆX o, respectively.\n\nDuring the adversarial distribution matching, SAT expects to match the posterior distributions qφx (zx|xi) and qφa (za|ai) to a Gaussian prior p(z) ∼ N (0, 1). Inspired by Makhzani et al. (2015), SAT adopts an efficient adversarial matching approach between zx, za and samples from the Gaussian distribution. The adversarial distribution matching loss Ladv can be written as a minimax game:\n\nmin ψ\n\nmax φx,φa\n\nLadv = − Ezp∼p(z)[log D(zp)] − Ezx∼qφx (zx|xi)[log(1 − D(zx))] − Ezp∼p(z)[log D(zp)] − Eza∼qφa (za|ai)[log(1 − D(za))]\n\nwhere ψ is the parameters of the shared discriminator D.\n\nIn summary, the objective function of SAT is:\n\nmin Θ\n\nmax Φ\n\nL = Lr + Ladv\n\n(2)\n\n(3)\n\nwhere Θ = {θx, θa, φx, φa, ψ}, Φ = {φx, φa}. In the training phase of the node attribute completion task, SAT aims to minimize the reconstruction loss between ˆA, ˆX o and A, X o in Eq. 1, as well as the adversarial loss in Eq. 2. In testing, it encodes the structural information Au of attributemissing nodes by the encoder EA and then restore their missing attributes X u by the decoder DX .\n\n4 METHOD\n\nWe design the query strategy of ATS by measuring the representativeness and uncertainty of the candidate nodes. Then we combine the scores of uncertainty and representativeness as the final score by an adaptive reweighting scheme and select the nodes with the highest scores for next learning epoch. We will explain these more in the following parts.\n\n4\n\n?zx?zaAdversarial Distribution MatchingEXDXDAEAp(z)Under review as a conference paper at ICLR 2023\n\n4.1 QUERY STRATEGY OF ATS\n\nRepresentativeness: The major and typical patterns among the nodes are vital for the model to converge to the right direction. In this section, we introduce the concept of representativeness as a sampling metric. This metric is composed of two parts: 1) information density φdensity and 2) structural centrality φcentrality. The former mainly focuses on measuring the similarity between the corresponding latent vectors of attributes and structures. The latter indicates how closely a node is connected to its neighbours on graph. In other words, the information density is inspired by the good representation learning ability of SAT and the structural centrality is natural to mine the information on the graph structures. These two aspects offer us a comprehensive analysis of the representativeness in both implicit and explicit ways.\n\nWe first focus on the information density. SAT proposes a shared-latent space assumption for node attribute completion. We can study the node similarities through the implied features learned by the model. If there is a dense distribution of representation vectors in a local region of the latent space, the corresponding nodes will have more similar features and this region will contain further mainstream information, so we expect to train these more representative nodes in priority. For node attribute completion task, although there are attribute embeddings and structure embeddings in shared-latent space, our ATS only uses the structure embeddings zai to calculate the φdensity as shown in Eq. 4 since we rely on the structural representations to restore the missing node attributes. In order to find the central node located in high-density region, we employ the K-means algorithm in the latent space and calculate the Euclidean distance between each node and its clustering center. as the clustering center of zai in Given d as the metric of Euclidean distance in l2-norm and Czai latent space, the formulation of φdensity is written as:\n\nφdensity(vi) =\n\n1 1 + d(zai, Czai\n\n)\n\n, vi ∈ T U\n\n(4)\n\nThe larger the φdensity is, the more representative the node is, and the node contains more representative features that are worthy of the model’s attention.\n\nBesides the feature analysis in latent space, the node representativeness can also be inferred from the explicit graph structures. We can study the connections between nodes and develop a metric to calculate the node centrality based on the structural information. Intuitively, the centrality can have a positive correlation with the number of neighbours. At the early stage of training, if we can focus on these nodes, the model will learn the approximate distribution of the data faster and reduce the influence caused by the noisy ones. PageRank Page et al. (1999) algorithm is an effective randomwalk method to acquire the visiting probabilities of nodes. We utilize the PageRank score as the structural centrality φcentrality, which is shown as below:\n\nφcentrality(vi) = ρ\n\n(cid:88)\n\nAij\n\nj\n\nφcentrality(vj) k Ajk\n\n(cid:80)\n\n+\n\n1 − ρ\n\nN U , vi ∈ T U\n\n(5)\n\nwhere N U is the number of nodes in T U , ρ is the damping parameter. The larger φcentrality is, the more representative the node is, and the node is more closely associated with its neighbours.\n\nUncertainty: Uncertainty reflects the learning state of the current model towards the nodes. When the model is reliable, it’s reasonable to pay more attention on the nodes that have not been sufficiently learned. Uncertainty is a commonly-used query criterion in active learning. However, as mentioned before, the uncertainty in other sampling algorithms Cai et al. (2017); Caramalau et al. (2021); Zhang et al. (2022a) usually works for node classifications and is designed in an unsupervised manner to reduce the annotation cost. In this paper, for the node attribute completion task, in order to know the training status of the model more accurately, we consider the observed attributes and structures as supervision, and use the learning loss in SAT as the uncertainty metric, noted as φentropy(vi).\n\nφentropy(vi) = Lr(vi) + Ladv(vi), vi ∈ T U\n\n(6)\n\nWe can input the attributes of candidate nodes and the corresponding graph structures into SAT, and then obtain their loss values. The larger φentropy(vi) is, the more uncertainty of node vi has. From the perspective of information theory, nodes with greater uncertainty contain more information. Sampling these nodes can help the model get the information that has not been learned in previous training, thus enhancing the training efficiency.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4.2 SCORE FUNCTION AND BETA DISTRIBUTION CONTROLLED WEIGHTING SCHEME\n\nWe have presented three metrics of our query strategy. Then, a question arises: How to combine these metrics to score each node? Combing the metrics with a weighted sum is a possible solution but still faces great difficulties. First, the values of different metrics are incomparable because of the distinct dimensional units. Second, the different metrics may take different effects at different learning stages. To solve these issues, we introduce a percentile evaluation and design a Beta-distribution controlled re-weighting scheme to exert the functions of each metric, since Beta distribution is a suitable model for the random behavior of percentages and proportions Gupta & Nadarajah (2004).\n\nDenote Pφ(vi, T U ) as the percentage of the candidate nodes in T U which have smaller values than the node vi with metric φ. For example, if there are 5 candidate nodes and the scores of one metric is [1, 2, 3, 4, 5], the percentile of the corresponding 5 nodes will be [0, 0.2, 0.4, 0.6, 0.8]. We apply the percentile to three metrics and define the final score function of ATS as:\n\nS(vi) = α · Pentropy(vi, T U ) + β · Pdensity(vi, T U ) + γ · Pcentrality(vi, T U )\n\n(7)\n\nwhere α + β + γ = 1. At the sampling stage, ATS will select one or several nodes with the largest S and add them to the training set T L for the next training epoch of SAT.\n\nALGORITHM 1: ATS algorithm Input: SAT parameters, G, T U , T L initialization of T L and hyper-parameters; while ne < total epoch do\n\n// Training stage loss ← SAT (T L); // Update SAT loss.backward(); update(SAT.params); // Sampling stage if #T U > 0 then\n\n// SAT returns the loss values and latent representations za za, φentropy ← SAT (T U ); φdensity ← getDensity(za); φcentrality ← getCentrality(G); γ ← Beta(1, nt); α, β ← 1−γ 2 ; S ← α · Pentropy + β · Pdensity + γ · Pcentrality; // select the node with the highest score T S ← activeSample(S, T U ); // renew the training set of SAT T L ← T L ∪ T S; // renew the candidate set T U ← T U \\ T S;\n\nend\n\nend\n\nFurther, it is worth noting that the uncertainty and the information density are determined by the training result returned from SAT. At an early training stage, the model is unstable and the returned training result may not be quite reliable. A sampling process based on inaccurate model-returned results may lead to undesirable results. Hence, we set the weights to time-sensitive ones. The structure-related weight γ is more credible so it can be larger initially. As the training epoch increases, the model can pay more attention to φentropy and φdensity, while the weight γ will decrease gradually. We formalize this by sampling γ from a Beta distribution, of which the expectation becomes smaller with the increase of training epoch. The weighting values are defined as:\n\nγ ∼ Beta(1, nt), nt =\n\nne ε\n\nand α = β =\n\n1 − γ 2\n\n.\n\n(8)\n\nwhere nt is one of the determinants in Beta distribution; ε is used to control the expectation of γ; ne denotes the current number of epochs. We obtain the expectation by calculating the average value of 10,000 random samples.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\n4.3\n\nITERATIVE TRAINING AND IMPLEMENTATION\n\nIn general, our method consists of two stages: one is SAT, responsible for the training stage; the other is ATS, responsible for the sampling stage. Before the training, we divide total training set T into T U and T L. We randomly sample 1% of the nodes in T as the initial nodes of T L and the rest composes T U . SAT will be trained on the changeable T L. Once SAT accomplishes a single training epoch, ATS starts the sampling process. We sample the most representative and informative candidate nodes from T U according to the query strategy. These selected nodes are added to T L and removed from T U . Then SAT will be trained on the renewed T L at next epoch. The training stage and the sampling stage alter iteratively until T U is null. Finally ATS is terminated and SAT will continue training to convergence. We clarify the learning process in Algorithm 1.\n\n5 EXPERIMENTS AND ANALYSIS\n\n5.1 DATASETS\n\nWe utilize 4 public benchmarks whose node attributes are categorical vectors. The information of used datasets is as follows: 1) Cora. Cora McCallum et al. (2000) is a citation graph with 2,708 papers as nodes and 10,556 citation links as edges. Each node has a multi-hot attribute vector with 1,433 dimensions. The attribute vectors consist of different word tokens to determine whether they appear or not. 2) Citeseer. Citeseer Sen et al. (2008) is another citation graph which is larger than Cora. It contains 3,327 nodes and 9,228 edges. Like Cora, each node has a multi-hot attribute vector with 3,703 dimensions. 3) Amazon-Computer and 4) Amazon-Photo. These two datasets are generated from Amazon co-purchase graph. The node represents the item and the edge represents the two items are usually purchased at the same time. The node attribute is a multi-hot vector with the set of words involved in the item description. Amazon-Computer Shchur et al. (2018) has 13,752 items and 245,861 edges. Amazon-Photo Shchur et al. (2018) has 7,650 nodes and 119,081 edges.\n\n5.2 EXPERIMENTAL SETUP\n\nBaselines: We compare SAT model combined with ATS with other baselines introduced in Chen et al. (2022): NeighAggre S ̧ ims ̧ek & Jensen (2008), VAE Kingma & Welling (2013), GCN Kipf & Welling (2017), GraphSage Hamilton et al. (2017), GAT Veliˇckovi ́c et al. (2018), Hers Hu et al. (2019), GraphRNA Huang et al. (2019), ARWMF Lei Chen & Bronstein (2019) and original SAT. Details about how they work on node attribute completion are illustrated in Appendix A.1.\n\nEvaluation metrics: In node attribute completion, the restored attributes can provide side information for nodes and benefit downstream tasks. By following SAT Chen et al. (2022), we study the effect of ATS on two downstream tasks including node classification task in the node level and profiling task in the attribute level. In node classification, the restored attributes serve as one kind of data augmentation and supply more information to the down-stream classification task. In profiling, we aim to predict the possible profile (e.g. key terms of papers in Cora) in each attribute dimension.\n\nParameters setting: In the experiment, we randomly sample 40% nodes with attributes as training data, 10% nodes as validation data and the rest as test data. The attributes of validation and test nodes are unobserved in training. For the baselines, the parameters setting and the experiment results refer to Chen et al. (2022). For our ATS method, the SAT’s setting remains the same, such as λc. We mainly have two hyper-parameters: ε in the weighting scheme and cluster numbers in the estimation of density φdensity. Considering the objective of the Beta distribution weighting scheme, ε should be larger than the total sampling times. Hence in Cora and Citeseer, we set ε = 1500 and when it comes to Amazon Photo and Amazon Computer, we set ε = 2000. In addition, we set the cluster number as 10, 15, 10, 15 for Cora, Citeseer, Amazon Photo and Amazon Computer.\n\n5.3 OVERALL COMPARISON\n\n5.3.1 NODE CLASSIFICATION\n\nClassification is an effective downstream task to test the quality of the recovered attributes. In node classification task, the nodes with restored attributes are split into 80% training data and 20%\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ntest data. Then we conduct five-fold cross-validation in 10 times and take the average results of evaluation metrics as the model performance. We use two supervised classifiers: MLP and GCN. The MLP classifier is composed by two fully-connected layers, which classifies the nodes based on attributes. The GCN classifier is an end-to-end graph representation learning model, which can learn the structure and attributes simultaneously. Results are shown in Table 1.\n\nAccording to the results of “X” row where only node attributes are used, the optimized SAT model with our proposed ATS algorithm achieves obvious improvement than original SAT model. Our ATS can also adapt to SAT with different GNN backbones (e.g. GCN and GAT) and achieve higher classification accuracy than the original models. For the results of “A+X” row where both structures and node attributes are used by a GCN classifier, our method achieves the highest score in Citeseer and Amazon-Computer, with 0.84% and 0.56% respectively, because ATS contains the density metric and can help the model better learn the inner semantic structures.\n\nTable 1: Node classification of the node-level evaluation for node attribute completion. ”X” indicates the MLP classifier that only considers the node attributes. ”A+X” indicates the GCN classifier that considers both the structures and node attributes.\n\nMethod\n\nCora\n\nCiteseer Amazon-Computer Amazon-Photo\n\nX\n\nA+X\n\nNeighAggre VAE GCN GraphSage GAT Hers GraphRNA ARWMF SAT(GCN) SAT(GAT)\n\n0.6248 0.2826 0.3943 0.4852 0.4143 0.3046 0.7581 0.7769 0.7644 0.7937 ATS+SAT(GCN) 0.7850 0.8065 ATS+SAT(GAT) 0.6494 NeighAggre 0.3011 VAE 0.4387 GCN 0.5779 GraphSage 0.4525 GAT 0.3405 Hers 0.8198 GraphRNA 0.8025 ARWMF 0.8327 SAT(GCN) 0.8579 SAT(GAT)\n\nATS+SAT(GCN) 0.8366 0.8573 ATS+SAT(GAT)\n\n0.5539 0.2551 0.3768 0.3933 0.2129 0.2585 0.6320 0.2267 0.6010 0.6475 0.6370 0.6662 0.5413 0.2663 0.4079 0.4278 0.2688 0.3229 0.6394 0.2764 0.6599 0.6767 0.6750 0.6851\n\n0.8365 0.3747 0.3660 0.3747 0.3747 0.3747 0.6968 0.5608 0.7410 0.8201 0.8198 0.8402 0.8715 0.4023 0.3974 0.4019 0.4034 0.4025 0.8650 0.7400 0.8519 0.8766 0.8752 0.8822\n\n0.8846 0.2598 0.2683 0.2598 0.2598 0.2598 0.8407 0.4675 0.8762 0.8976 0.8827 0.9028 0.901 0.3781 0.3656 0.3784 0.3789 0.3794 0.9207 0.6146 0.9163 0.9260\n\n0.9181 0.9251\n\n5.3.2 PROFILING\n\nThe model outputs the restored attributes in different dimensions with probabilities. Higher corresponding probabilities of ground-truth attributes signify better performance. In this section, we use two common metrics Recall@k and NDCG@k to evaluate the profiling performance. The experiment results are shown in Table 2.\n\nAccording to the profiling results in Table 2, on the basis of the advantages established by the SAT model towards other baselines, the combination of the ATS algorithm and SAT model (ATS+SAT) obtains even higher performance in almost all the evaluation metrics and almost all the datasets. For example, ATS+SAT(GAT) obtains a relative 13.5% gain of Recall@10 and a relative 13.3% gain of NDCG@10 on Citeseer compared with SAT(GAT). The main reason of these results is that the active sampling algorithm ATS helps SAT model to realize different importance of different nodes in learning, and thus facilitates better distribution modeling of the high-dimensional node attributes.\n\n5.4 STUDY OF THE WEIGHTING SCHEME\n\nBesides the active sampling metrics, the Beta distribution controlled weighting scheme is also a highlight of the ATS algorithm. We will verify the effectiveness of our proposed scheme in comparison with other weighting schemes, such as the fixed weighting scheme and the linear variation weighting scheme. For the fixed one, the values of γ are 0.2, 1 2 . For the linear variation one, γ decreases linearly from 1 to 0.5 or from 1 to 0.\n\n3 , 0.6, and α = β = 1−γ\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Profiling of the attribute-level evaluation for node attribute completion.\n\nCora\n\nCiteseer\n\nAmazon-Computer\n\nAmazon-Photo\n\nMethod NeighAggre VAE GCN GraphSage GAT Hers GraphRNA ARWMF SAT(GCN) SAT(GAT) ATS+SAT(GCN) ATS+SAT(GAT) NeighAggre VAE GCN GraphSage GAT Hers GraphRNA ARWMF SAT(GCN) SAT(GAT) ATS+SAT(GCN) ATS+SAT(GAT) NeighAggre VAE GCN GraphSage GAT Hers GraphRNA ARWMF SAT(GCN) SAT(GAT) ATS+SAT(GCN) ATS+SAT(GAT) NeighAggre VAE GCN GraphSage GAT Hers GraphRNA ARWMF SAT(GCN) SAT(GAT) ATS+SAT(GCN) ATS+SAT(GAT)\n\nRecall@10 Recall@20 Recall@50 NDCG@10 NDCG@20 NDCG@50\n\n0.0906 0.0887 0.1271 0.1284 0.1350 0.1226 0.1395 0.1291 0.1508 0.1653 0.1560 0.1640 0.0511 0.0382 0.0620 0.0612 0.0561 0.0576 0.0777 0.0552 0.0764 0.0811 0.0854 0.0921 0.0321 0.0255 0.0273 0.0269 0.0271 0.0273 0.0386 0.0280 0.0391 0.0421 0.0421 0.0440 0.0329 0.0276 0.0294 0.0295 0.0294 0.0292 0.0390 0.0294 0.0410 0.0427 0.0426 0.0438\n\n0.1413 0.1228 0.1772 0.1784 0.1812 0.1723 0.2043 0.1813 0.2182 0.2345 0.2259 0.2355 0.0908 0.0668 0.1097 0.1097 0.1012 0.1025 0.1272 0.1015 0.1280 0.1349 0.1400 0.1487 0.0593 0.0502 0.0533 0.0528 0.0530 0.0525 0.0690 0.0544 0.0703 0.0746 0.0746 0.0775 0.0616 0.0538 0.0573 0.0562 0.0573 0.0574 0.0703 0.0568 0.0743 0.0765 0.0765 0.0785\n\n0.1961 0.2116 0.2962 0.2972 0.2972 0.2799 0.3142 0.296 0.3429 0.3612 0.3527 0.3616 0.1501 0.1296 0.2052 0.2058 0.1957 0.1973 0.2271 0.1952 0.2377 0.2431 0.2580 0.2635 0.1306 0.1196 0.1275 0.1278 0.1278 0.1273 0.1465 0.1289 0.1514 0.1577 0.1575 0.1617 0.1361 0.1279 0.1324 0.1322 0.1324 0.1328 0.1508 0.1327 0.1597 0.1635 0.1631 0.1651\n\n0.1217 0.1224 0.1736 0.1768 0.1791 0.1694 0.1934 0.1824 0.2112 0.2250 0.2161 0.2258 0.0823 0.0601 0.1026 0.1003 0.0878 0.0904 0.1291 0.0859 0.1298 0.1385 0.1441 0.1570 0.0788 0.0632 0.0671 0.0664 0.0673 0.0676 0.0931 0.0694 0.0963 0.1030 0.1032 0.1074 0.0813 0.0675 0.0705 0.0712 0.0705 0.0714 0.0959 0.0727 0.1006 0.1047 0.1039 0.1067\n\n0.1548 0.1452 0.2076 0.2102 0.2099 0.2031 0.2362 0.2182 0.2546 0.2723 0.2628 0.2733 0.1155 0.0839 0.1423 0.1393 0.1253 0.1279 0.1703 0.1245 0.1729 0.1834 0.1896 0.2037 0.1156 0.0970 0.1027 0.1020 0.1028 0.1025 0.1333 0.1053 0.1379 0.1463 0.1464 0.1519 0.1196 0.1031 0.1082 0.1079 0.1083 0.1094 0.1377 0.1098 0.1450 0.1498 0.1491 0.1529\n\n0.1850 0.1924 0.2702 0.2728 0.2711 0.2596 0.2938 0.2776 0.3212 0.3394 0.3298 0.3405 0.1560 0.1251 0.2049 0.2034 0.1872 0.1900 0.2358 0.1858 0.2447 0.2545 0.2672 0.2791 0.1923 0.1721 0.1824 0.1822 0.1830 0.1825 0.2155 0.1851 0.2243 0.2346 0.2347 0.2412 0.1998 0.1830 0.1893 0.1896 0.1892 0.1906 0.2232 0.1915 0.2359 0.2421 0.2411 0.2450\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Amazon photo\n\nFigure 2: Visualization of profiling performance of different weighting schemes on test data during training process. We compare our Beta distribution controlled weighting scheme with other weighting schemes(e.g. fixed weight, linear variation).\n\nFrom Figure 2, we see our proposed weighting scheme outperforms other schemes because Beta distribution changes the weights dynamically during the sampling process and meanwhile remains some randomness to improve the robustness of the algorithm.\n\n6 CONCLUSION\n\nIn this paper, we propose a novel active sampling algorithm ATS to better solve the node attribute completion problem. In order to distinguish the differences in the amount of information among nodes, ATS utilizes the proposed uncertainty and representativeness metrics to select the most informative nodes and renew the training set after each training epoch. Further, the Beta distribution controlled weighting scheme is proposed to adjust the metric weights dynamically according to the training status. The sampling process increases the running time of each epoch within an affordable cost, but meanwhile helps the base model achieve superior performance on profiling and node classification tasks. Therefore, ATS is effective in boosting the quality of restored attributes.\n\n9\n\n0.60.81.01.21.41.61.82.0Epoch×1030.1900.1950.2000.2050.2100.2150.2200.2250.230Recall@20fixed-0.2fixed-1/3fixed-0.6linear-0linear-0.5beta0.60.81.01.21.41.61.82.0Epoch×1030.1000.1100.1200.1300.140Recall@20fixed-0.2fixed-1/3fixed-0.6linear-0linear-0.5beta0.60.81.01.21.41.61.82.0Epoch×1030.0700.0710.0720.0730.0740.0750.076Recall@20fixed-0.2fixed-1/3fixed-0.6linear-0linear-0.5betaUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning?\n\nIn International Conference on Machine Learning, pp. 872–881. PMLR, 2019.\n\nHongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. Active learning for graph embed-\n\nding. arXiv preprint arXiv:1705.05085, 2017.\n\nRazvan Caramalau, Binod Bhattarai, and Tae-Kyun Kim. Sequential graph convolutional network for active learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9583–9592, 2021.\n\nXu Chen, Siheng Chen, Jiangchao Yao, Huangjie Zheng, Ya Zhang, and Ivor W. Tsang. Learning on attribute-missing graphs. IEEE Trans. Pattern Anal. Mach. Intell., 44(2):740–757, feb 2022. ISSN 0162-8828. doi: 10.1109/TPAMI.2020.3032189. URL https://doi.org/10.1109/ TPAMI.2020.3032189.\n\nZhengdao Chen, Xiang Li, and Joan Bruna. Supervised community detection with line graph neural\n\nnetworks. arXiv preprint arXiv:1705.08415, 2017.\n\nZhixian Chen, Tengfei Ma, Yangqiu Song, and Yang Wang. Wasserstein diffusion on graphs with\n\nmissing attributes. arXiv preprint arXiv:2102.03450, 2021.\n\nPeng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding. IEEE Transac-\n\ntions on Knowledge and Data Engineering, 31(5):833–852, 2018.\n\nMicha ̈el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in neural information processing systems, pp. 3844–3852, 2016.\n\nTongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama. Rethinking importance weighting for deep learning under distribution shift. Advances in Neural Information Processing Systems, 33: 11996–12007, 2020.\n\nAkshay Gadde, Aamir Anis, and Antonio Ortega. Active semi-supervised learning using sampling theory for graph signals. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 492–501, 2014.\n\nLi Gao, Hong Yang, Chuan Zhou, Jia Wu, Shirui Pan, and Yue Hu. Active discriminative network representation learning. In IJCAI International Joint Conference on Artificial Intelligence, 2018.\n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 855–864. ACM, 2016.\n\nQuanquan Gu, Charu Aggarwal, Jialu Liu, and Jiawei Han. Selective sampling on graphs for clasIn Proceedings of the 19th ACM SIGKDD international conference on Knowledge\n\nsification. discovery and data mining, pp. 131–139, 2013.\n\nArjun K Gupta and Saralees Nadarajah. Handbook of beta distribution and its applications. CRC\n\npress, 2004.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nAdvances in neural information processing systems, 30, 2017.\n\nLiang Hu, Songlei Jian, Longbing Cao, Zhiping Gu, Qingkui Chen, and Artak Amirbekyan. Hers: Modeling influential contexts with heterogeneous relations for sparse and cold-start recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3830–3837, 2019.\n\nXiao Huang, Qingquan Song, Yuening Li, and Xia Hu. Graph recurrent networks with attributed random walks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 732–740, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMing Ji and Jiawei Han. A variance minimization criterion to active learning on graphs. In Artificial\n\nIntelligence and Statistics, pp. 556–564. PMLR, 2012.\n\nBo Jiang and Ziyan Zhang. Incomplete graph representation and learning via partial graph neural\n\nnetworks. arXiv preprint arXiv:2003.10130, 2020.\n\nDi Jin, Cuiying Huo, Chundong Liang, and Liang Yang. Heterogeneous graph neural network via\n\nattribute completion. In Proceedings of the Web Conference 2021, pp. 391–400, 2021.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\n\nworks. In International Conference on Learning Representations (ICLR), 2017.\n\nJoan Bruna Lei Chen, Shunwang Gong and Michael M. Bronstein. Attributed random walk as matrix\n\nfactorization. Graph Representation Learning Workshop NeurIPS, 2019.\n\nXiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. Braingnn: Interpretable brain graph neural network for fmri analysis. Medical Image Analysis, 74:102233, 2021.\n\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial\n\nautoencoders. arXiv preprint arXiv:1511.05644, 2015.\n\nAndrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the Information Retrieval, 3(2):127–163,\n\nconstruction of internet portals with machine learning. 2000.\n\nLawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:\n\nBringing order to the web. Technical report, Stanford InfoLab, 1999.\n\nLiming Pan, Cheng Shi, and Ivan Dokmani ́c. Neural link prediction with walk pooling. In Interna-\n\ntional Conference on Learning Representations, 2022.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repreIn Proceedings of the 20th ACM SIGKDD international conference on Knowledge\n\nsentations. discovery and data mining, pp. 701–710. ACM, 2014.\n\nPengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM Computing Surveys (CSUR), 54(9):1–40, 2021.\n\nBenjamin Sanchez-Lengeling, Emily Reif, Adam Pearce, and Alexander B. Wiltschko. A gentle\n\nintroduction to graph neural networks. Distill, 2021. doi: 10.23915/distill.00033.\n\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008.\n\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\n\nCollective classification in network data. AI magazine, 29(3):93–93, 2008.\n\nOzan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set\n\napproach. arXiv preprint arXiv:1708.00489, 2017.\n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G ̈unnemann. Pitfalls\n\nof graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.\n\n ̈Ozg ̈ur S ̧ ims ̧ek and David Jensen. Navigating networks by using homophily and degree. Proceedings\n\nof the National Academy of Sciences, 105(35):12758–12762, 2008.\n\nJianyong Sun, Wei Zheng, Qingfu Zhang, and Zongben Xu. Graph neural network encoding for\n\ncommunity detection in attribute networks. IEEE Transactions on Cybernetics, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Largescale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067–1077. International World Wide Web Conferences Steering Committee, 2015.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ. accepted as poster.\n\nAthanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, Eftychios Protopapadakis, and Diego Andina. Deep learning for computer vision: A brief review. Intell. Neuroscience, 2018, jan 2018. ISSN 1687-5265. doi: 10.1155/2018/7068349. URL https://doi.org/10.1155/ 2018/7068349.\n\nYixin Wang, Alp Kucukelbir, and David M Blei. Robust probabilistic modeling with bayesian data reweighting. In International Conference on Machine Learning, pp. 3646–3655. PMLR, 2017.\n\nDongkuan Xu, Wei Cheng, Dongsheng Luo, Yameng Gu, Xiao Liu, Jingchao Ni, Bo Zong, Haifeng Chen, and Xiang Zhang. Adaptive neural network for node classification in dynamic networks. In 2019 IEEE International Conference on Data Mining (ICDM), pp. 1402–1407. IEEE, 2019a.\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural In International Conference on Learning Representations, 2019b. URL https:\n\nnetworks? //openreview.net/forum?id=ryGs6iA5Km.\n\nQiang Yang, Zhi-Hua Zhou, Zhiguo Gong, Min-Ling Zhang, and Sheng-Jun Huang. Advances in Knowledge Discovery and Data Mining: 23rd Pacific-Asia Conference, PAKDD 2019, Macau, China, April 14-17, 2019, Proceedings, volume 11441. Springer, 2019.\n\nDonggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF\n\nconference on computer vision and pattern recognition, pp. 93–102, 2019.\n\nChuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. Heterogeneous graph neural network. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 793–803, 2019.\n\nMuhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural\n\ninformation processing systems, 31, 2018.\n\nWentao Zhang, Yexin Wang, Zhenbang You, Meng Cao, Ping Huang, Jiulong Shan, Zhi Yang, and Bin Cui. Information gain propagation: a new way to graph active learning with soft labels. arXiv preprint arXiv:2203.01093, 2022a.\n\nZ. Zhang, P. Cui, and W. Zhu. Deep learning on graphs: A survey. IEEE Transactions on Knowledge ISSN 1558-2191. doi: 10.1109/TKDE.\n\namp; Data Engineering, 34(01):249–270, jan 2022b. 2020.2981333.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Amazon photo\n\nFigure 3: Ablation study of different metrics in ATS. We show the recall@20 result of different combinations of the sampling metrics on 3 benchmarks. The horizontal coordinate refers to the different sampling criteria combinations. ’E’ indicates the entropy metric; ’D’ indicates the density metric; ’C’ indicates the centrality metric; ’E+D+C’ indicates our ATS algorithm.\n\nA APPENDIX\n\nA.1 DETAILS ABOUT THE BASELINES\n\nNeighAggre is an intuitive attribute aggregation algorithm. It completes one node’s missing attributes by averaging its neighbour nodes’ attributes, which is a simple but efficient method to take advantage of the structural information. VAE is a famous generative model, which consists of an encoder and a decoder. For test nodes without the attributes, the encoder will generate the corresponding latent code through the neighbour aggregation. Then the decoder will restore the missing attributes. GCN, GraphSage and GAT are three typical graph representation learning methods. For attribute-missing scenario, only the graph structure will be encoded to latent codes. The missing attributes will be recovered by the decoders of these GNN methods from the latent code generated by the encoders. Hers is a cold-start recommendation method. GraphRNA and ARWMF are two attributed random walk based methods to learn the node representations, which can be extended to deal with the missing attributes problems. They separate the graph structure and node attributes and learn the node embeddings by random walks.\n\nA.2 ABLATION STUDY OF DIFFERENT METRICS IN ATS\n\nIn this section, we conduct the ablation study to investigate the effects of three different metrics in ATS. The experimental settings remain the same as the profiling task. We use Recall@20 to evaluate the performance of different metric combinations. The results are shown in Figure 3.\n\nIn Cora, centrality-only sampling method hurts the profiling performance. Different metrics focus on different aspects and the result shows that they can complement each other. The uncertainty metric focuses on the training status of the model, while the representativeness metric focuses on the implied information from both the structure and attribute aspects. Generally, any subgroup of the sampling criteria is inferior to the results achieved by the complete ATS.\n\nA.3 EMPIRICAL TIME COMPLEXITY ANALYSIS\n\nOur ATS is an active sampling procedure based on the SAT model, so it’s critical to study the extra processing time cost by the ATS. Thus we conduct an experiment to count the running time of different parts of the ATS compared with the original SAT. These different parts are forward process, uncertainty and representativeness. The forward process means the forward propagation, which is essential to calculate the uncertainty score. We implement the experiment on a machine with one Nvidia 1080Ti GPU.\n\nAccording to the running time shown in Figure 4, the forward propagation in ATS is much faster than SAT due to the time-consuming back propagation in SAT. Although the processing time of uncertainty metric and representativeness metric is relatively higher than SAT because of the clustering and percentile calculations, it’s comparable with the time of SAT. With the addition of the ATS algorithm, the time required for each epoch will increase within an acceptable range.\n\n13\n\nEDCE+DE+CD+CE+D+CQuery Strategy0.21000.21250.21500.21750.22000.22250.22500.22750.2300Recall@200.21950.22390.21120.22010.22280.22150.2243EDCE+DE+CD+CE+D+CQuery Strategy0.13000.13250.13500.13750.14000.14250.14500.14750.1500Recall@200.13680.13630.13900.13760.13860.14000.1400EDCE+DE+CD+CE+D+CQuery Strategy0.0700.0720.0740.0760.0780.080Recall@200.07570.07490.07620.07580.07590.07590.0765Under review as a conference paper at ICLR 2023\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Amazon Photo\n\n(d) Amazon Computer\n\nFigure 4: The comparison among the average processing GPU time per epoch of different model components. ’Forward’ indicates the forward propagation that is a part of the calculation in uncertainty metric.\n\nA.4 SENSITIVITY OF THE HYPERPARAMETERS\n\nAs mentioned in Section 5.2, cluster number is a vital hyper-parameter that determines the information density of each node. We conduct the experiments on both the profiling and classification tasks with different cluster numbers.\n\n(a) Recall on Cora\n\n(b) Recall on Citeseer\n\n(c) Recall on Amazon Photo\n\n(d) X - Cora\n\n(e) X - Citeseer\n\n(f) X - Amazon Photo\n\n(g) A+X - Cora\n\n(h) A+X - Citeseer\n\n(i) A+X - Amazon Photo\n\nFigure 5: Results with different cluster numbers when calculating the density score in the representativeness metric. (a-c) show the Recall@20 results for profiling task. (d-f) show the attribute-only classification accuracy with the use of MLP classifier. (g-h) show the classification accuracy considering both the structure and attribute information.\n\nThe results of Figure 5 show that too large or too small cluster numbers are not conducive to the training. If there are not enough cluster centers, the sampling algorithm is not robust to extract the density of the embedding distribution. On the other hand, if there are too many cluster centers, it will\n\n14\n\nSATForward processUncertaintyRepresentativenessModel components0.000.020.040.060.080.10Time (s/epoch)0.07140.00470.06130.0933SATForward processUncertaintyRepresentativenessModel components0.000.020.040.060.080.100.120.14Time (s/epoch)0.08530.00470.07750.1308SATForward processUncertaintyRepresentativenessModel components0.000.050.100.150.200.250.300.350.40Time (s/epoch)0.08580.00450.32300.2881SATForward processUncertaintyRepresentativenessModel components0.00.20.40.60.81.0Time (s/epoch)0.13160.00710.92790.7766810121520Cluster numbers0.21600.21700.21800.21900.22000.22100.22200.2230Recall@201012152025Cluster numbers0.08350.08400.08450.08500.0855Recall@2081012152030Cluster numbers0.07580.07590.07600.07610.07620.0763Recall@20810121520Cluster numbers0.76750.77000.77250.77500.77750.78000.78250.7850Accuracy1012152025Cluster numbers0.63200.63300.63400.63500.63600.63700.63800.6390Accuracy81012152030Cluster numbers0.88000.88100.88200.88300.8840Accuracy810121520Cluster numbers0.82400.82600.82800.83000.83200.83400.8360Accuracy1012152025Cluster numbers0.66700.66800.66900.67000.67100.67200.67300.67400.6750Accuracy81012152030Cluster numbers0.91800.91850.91900.91950.92000.9205AccuracyUnder review as a conference paper at ICLR 2023\n\nintroduce more disturbance and might separate the nodes belonging to the same class. We determine the value of hyper-parameter based on the Recall@20 results in the profiling task.\n\n15",
    "reference": "# Summary Of The Paper\n\nWhile the completion of missing node attributes is an important problem, the Structure-Attribute Transformer (SAT) is recent work that tackles that problem with leveraging all the observed attributes equally. The authors challenge the part of using all the observed attributes in an equal manner, and propose the SAT learning strategy based on active node sampling. For the active sampling, authors propose the the representativeness and the uncertain measurements for nodes on a graph and use them for active sampling. When the proposed sampling strategy is combined with SAT, it shows better performance in node classification tasks compared to the other baseline models, including the original SAT.\n\n# Strength And Weaknesses\n\n* Strengths\n- The proposed node measurements are clear and reasonable to represent the importance for active sampling.\n- The manuscript is easy to follow.\n- The proposed method combined with SAT shows the better performance compared to baseline models\n\n* Weakness\n- The objective of the active sampling-based strategy would be great if it is clearly defined. The efficiency is described as the objective, but it needs to be further clarified and better-defined.\n- Despite aiming the efficiency, the experiments or evaluations with respect to efficiency or training time are missing.\n- The proposed active sampling is very tightened with SAT. It is not clear why the sampling should be coupled with SAT, not some general embedding-based learning. More generalization of the proposed method or clear explanation specific to SAT would be great to have. \n- Experiments could be more enforced. The ablation study with respect to learning speed would be great to have. For example, when the same learning strategy is applied (starting from 1% training and expanding the training size over epochs), how soon does the proposed method achieve a reasonable performance as opposed to the uniform sampling? Or, how different is the learning algorithm's convergence speed when gradually expanding the training node set as opposed to directly training on the entire dataset?\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The manuscript is straightforward to follow and the proposed method and experiments are clearly present.\n- The proposed method is incremental in the sense that some common measurements are used for active sampling and it is applied to only a certain kind of method.\n\n# Summary Of The Review\n\nDespite the clearness and fair performance of the proposed method, the proposed method needs to be 1) more generalized, 2) evaluated on the matters of interests, and 3) more studied to the role of each strategy part to understand the impact of proposed methods better.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSAMPLE-EFFICIENT MULTI-OBJECTIVE MOLECULAR OPTIMIZATION WITH GFLOWNETS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nMany crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as an expensive black-box optimization problem over the discrete chemical space. Computational methods have achieved initial success but still struggle with simultaneously optimizing multiple competing properties in a sample-efficient manner. In this work, we propose a multiobjective Bayesian optimization (MOBO) algorithm leveraging the hypernetworkbased GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. Inspired by reinforcement learning, we further propose a hindsight-like off-policy strategy to share highperforming molecules among different preferences in order to speed up learning for HN-GFN. Through synthetic experiments, we illustrate that HN-GFN has adequate capacity to generalize over preferences. Extensive experiments show that our framework outperforms the best baselines by a large margin in terms of hypervolume in various real-world MOBO settings.\n\n1\n\nINTRODUCTION\n\nDesigning novel molecular structures with desired properties, also referred to as molecular optimization, is a crucial task with great application potential in scientific fields ranging from drug discovery to material design. Molecular optimization can be naturally formulated as a black-box optimization problem over the discrete chemical space, which is combinatorially large (Polishchuk et al., 2013). Recent years have witnessed the trend of leveraging computational methods, such as deep generative models (Jin et al., 2018) and combinatorial optimization algorithms (You et al., 2018; Jensen, 2019), to facilitate the optimization. However, the applicability of most prior approaches in real-world scenarios is hindered by two practical constraints: (i) realistic oracles (e.g., wet-lab experiments and high-fidelity simulations) require substantial costs to synthesize and evaluate molecules (Gao et al., 2022), and (ii) chemists commonly seek to optimize multiple properties of interest simultaneously (Jin et al., 2020b). For example, in addition to effectively inhibiting a disease-associated target, an ideal drug is desired to be easily synthesizable and non-toxic.\n\nBayesian optimization (BO) (Jones et al., 1998; Shahriari et al., 2015) provides a sample-efficient framework for globally optimizing expensive black-box functions. The basic idea is to construct a cheap-to-evaluate surrogate model, typically a Gaussian Process (GP) (Rasmussen, 2003), to approximate the true function (also known as the oracle) on the observed dataset. The core objective of BO is to optimize an acquisition function (built upon the surrogate model) in order to obtain informative candidates with high utility for the next round of evaluations. This loop is repeated until the evaluation budget is exhausted. Owing to the fact that a large batch of candidates can be evaluated in parallel in biochemical experiments, we perform batch BO (with large-batch and low-round settings (Angermueller et al., 2020)) to significantly shorten the entire cycle of optimization.\n\nAs multi-objective optimization (MOO) problems are prevalent in scientific and engineering applications, MOBO also received broad attention and achieved promising performance by effectively optimizing differentiable acquisition functions (Daulton et al., 2020). Nevertheless, it is less prominent in discrete problems, especially considering batch settings. The difficulty lies in the fact that no gradients can be leveraged to navigate the discrete space for efficient and effective optimization of\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: MOBO loop for molecular optimization using an evidential surrogate model M for uncertainty estimation and HN-GFN for acquisition function optimization. In each round, the policy πθ is trained with reward function Rλ, where λ is sampled from Dir(α) per iteration. A new batch of candidates is sampled from the approximate Pareto front according to λtarget ∈ Λ.\n\nthe acquisition function. Although most of the existing discrete molecular optimization methods can be adopted as the acquisition function optimizer to alleviate this issue, they suffer from the following limitations. 1) Most approaches do not explicitly discuss the diversity of the proposed candidates, which is a key consideration in batch settings as the surrogate model cannot exactly reproduce the oracle’s full behaviors. Therefore, we not only want to cover more high modes of the surrogate model but also to obtain candidates that bring additional information about the search space. 2) Most multi-objective methods (Xie et al., 2021; Fu et al., 2022) simply rely on a scalarization function, parameterized by a predefined preference vector reflecting the trade-off between objectives, and turn the MOO problem into a single-objective one. Unfortunately, an ideal trade-off is unclear before optimization (even with domain knowledge), and many potential trade-offs of interest are worth exploring. In principle, it is possible to independently train multiple optimization models, each conditioned on a distinct preference vector, to cover the objective space. Practically, this trivial strategy cannot efficiently scale with the number of objectives (Navon et al., 2021).\n\nThe recently proposed GFlowNets (Bengio et al., 2021a) are a class of generative models over discrete objects (e.g., molecular graphs) that aim to learn a stochastic policy for sequentially constructing objects with a probability proportional to a reward function (e.g., the acquisition function). Hence, GFlowNets possess merit in generating diverse and high-reward objects, which makes them appealing in the batch BO context where exploration plays a significant role (Jain et al., 2022).\n\nIn this work, we present a MOBO algorithm based on GFlowNets for sample-efficient multiobjective molecular optimization. We propose a hypernetwork-based GFlowNet (HN-GFN) as the acquisition function optimizer within MOBO to sample a diverse batch of candidates from an approximate Pareto front. Instead of defining a fixed reward function as usual in past work (Bengio et al., 2021a), we train a unified GFlowNet on the distribution of reward functions (random scalarizations parameterized by preference vectors) and control the policy using a single preferenceconditioned hypernetwork. While sampling candidates, HN-GFN explores various trade-offs between competing objectives flexibly by varying the input preference vector. Inspired by Hindsight Experience Replay (Andrychowicz et al., 2017) in RL, we further introduce a hindsight-like offpolicy strategy to share high-performing molecules among different preferences and speed up learning for HN-GFN. As detailed in our reported experiments, we first evaluate HN-GFN through synthetic experiments to verify that HN-GFN is capable of generalizing over preference vectors, then apply the proposed framework to real-world scenarios. Remarkably, our framework outperforms the best baselines by 60% and 24% (relative improvement in terms of hypervolume in the settings with two and four objectives), respectively. Our key contributions are summarized below:\n\n• We propose HN-GFN, a unified GFlowNet that can efficiently sample candidates from an\n\napproximate Pareto front using a single hypernetwork.\n\n2\n\nEvidential Surrogate Model MTraining : λ~DiraSampling : λtarget∈ΛHypernetworkh(∙;φ)θpredf1f2...fM??...?=×N×brepresents the initial state and represents the complete object.GFlowNet πθ: θ=(θmpnn,θpred)......S1S0.........S2Sn−1Sn......Rλx1Rλx2Rλx3Under review as a conference paper at ICLR 2023\n\n• We introduce a hindsight-like off-policy strategy to speed up learning in HN-GFN.\n\n• Experiments verify that our MOBO algorithm based on HN-GFN can find high-quality\n\nPareto front more efficiently compared to state-of-the-art baselines.\n\n2 RELATED WORK\n\nMolecular optimization. Recently, molecular optimization has been approached with a wide variety of computational methods, which can be mainly grouped into three categories: 1) Latent space optimization (LSO) methods perform the optimization over the low-dimensional continuous latent space learned by generative models such as variational autoencoders (VAEs) (G ́omez-Bombarelli et al., 2018; Maus et al., 2022). These methods require the latent representations to be discriminative, but the training of the generative model is decoupled from the optimization objectives, imposing challenges for optimization (Tripp et al., 2020). Instead of navigating the latent space, combinatorial optimization methods search for the desired molecular structures directly in the explicit discrete space with 2) evolutionary algorithms (Jensen, 2019) and 3) deep neural networks to guide the searching (You et al., 2018). However, most prior methods only focus on optimizing a single property, from non-biological properties such as drug-likeliness (QED) (Bickerton et al., 2012) and synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009), to biological properties that measure the binding energy to a protein target (Bengio et al., 2021a). Despite the above advances, multi-objective molecular optimization has recently received wide attention (Jin et al., 2020b; Xie et al., 2021; Fu et al., 2022). For example, MARS (Xie et al., 2021) employs Markov chain Monte Carlo (MCMC) sampling to find novel molecules satisfying several properties. However, most approaches require a notoriously large number of oracle calls to evaluate molecules on-the-fly (Jin et al., 2020b; Xie et al., 2021). In contrast, we tackle this problem in a sample-efficient manner.\n\nGFlowNet. GFlowNets (Bengio et al., 2021a) aim to sample composite objects proportionally to a reward function, instead of maximizing it as usual in RL (Sutton & Barto, 2018). GFlowNets are related to the MCMC methods due to the same objective, while amortizing the high cost of sampling (mixing between modes) over training a generative model (Zhang et al., 2022). GFlowNets have made impressive progress in various applications, such as active learning (Jain et al., 2022), discrete probabilistic modeling (Zhang et al., 2022), and Bayesian structure learning (Deleu et al., 2022). For a thorough discussion and mathematical treatment, we refer the readers to Bengio et al. (2021a;b)\n\nBayesian Optimization for discrete spaces. While the application of BO in continuous domains It is has proliferated during the last decade, effort in applying it to discrete spaces is lacking. much more challenging to construct surrogate models and optimize acquisition functions in discrete spaces, compared to continuous spaces. One common approach is to define GPs with discrete kernels (Moss et al., 2020) and solve the acquisition function optimization problem with evolutionary algorithms (Kandasamy et al., 2018). Moreover, AmortizedBO (Swersky et al., 2020) proposes to augment the evolutionary algorithms with RL.\n\nMulti-objective Bayesian Optimization. BO has been widely used in MOO problems for efficiently optimizing multiple competing black-box functions. Most popular approaches are based on hypervolume improvement (Daulton et al., 2020), random scalarizations (Knowles, 2006; Paria et al., 2020), and entropy search (Hern ́andez-Lobato et al., 2016). While there have been several approaches that take parallel evaluations (Bradford et al., 2018; Konakovic Lukovic et al., 2020) and diversity (Konakovic Lukovic et al., 2020) into account, they are limited to continuous domains.\n\n3 BACKGROUND\n\n3.1 PROBLEM FORMULATION\n\nWe address the problem of searching over a discrete chemical space X to find molecular graphs x ∈ X that maximize a vector-valued objective f (x) = (cid:0)f1(x), f2(x), . . . , fM (x)(cid:1) : X → RM , where fm is a black-box function (also known as the oracle) evaluating a certain property of molecules. Practically, realistic oracles are extremely expensive to evaluate with either high-fidelity simulations\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nor wet-lab experiments. We thus propose to perform optimization in as few oracle evaluations as possible, since the sample efficiency is paramount in such a scenario.\n\nThere is typically no single optimal solution to the MOO problem, as different objectives may contradict each other. Consequently, the goal is to recover the Pareto front – the set of Pareto optimal solutions which cannot be improved in any one objective without deteriorating another (Ehrgott, 2005; Miettinen, 2012). In the context of maximization, a solution f (x) is said to Pareto dominates another solution f (x′) iff fm(x) ≥ fm(x′) ∀m = 1, . . . , M and ∃m′ such that fm′(x) > fm′(x′), and we denote f (x) ≻ f (x′). A solution f (x∗) is Pareto optimal if not Pareto dominated by any solution. The Pareto front can be written as P ∗ = {f (x∗) : {f (x) : f (x) ≻ f (x∗) } = ∅}.\n\nThe quality of a finite approximate Pareto front P is commonly measured by hypervolume (HV) (Zitzler & Thiele, 1999) – the M-dimensional Lebesgue measure λM of the space dominated by P and bounded from below by a given reference point r ∈ RM : HV (P, r) = λM (∪|P| i=1[r, yi]), where [r, yi] denotes the hyper-rectangle bounded by r and yi = f (xi).\n\n3.2 BATCH BAYESIAN OPTIMIZATION\n\nBayesian optimization (BO) (Shahriari et al., 2015) provides a model-based iterative framework for sample-efficient black-box optimization. Given an observed dataset D, BO relies on a Bayesian surrogate model M to estimate a posterior distribution over the true oracle evaluations. Equipped with the surrogate model, an acquisition function a : X → R is induced to assign the utility values to candidate objects for deciding which to next evaluate the oracle. Compared with the costly oracle, the cheap-to-evaluate acquisition function can be efficiently optimized. We consider the scenario where the oracle is given an evaluation budget of N rounds with fixed batches of size b.\n\nTo be precise, we have access to a random initial dataset D0 = {(x0 i = f (x0 i ) is true oracle evaluation. In each round i ∈ {1, . . . , N }, the acquisition function is maximized to yield a batch of candidates Bi = {xi j). The observed dataset Di−1 is then augmented for the next round: Di = Di−1 ∪ {(xi\n\nj=1 to be evaluated in parallel on the oracle yi 1, yi\n\nj = f (xi j)}b j=1.\n\ni=1, where y0\n\ni )}n\n\ni , y0\n\nj}b\n\n4 METHOD\n\nIn this section, we present the proposed MOBO algorithm based on hypernetwork-based GFlowNet (HN-GFN), shown in Figure 1. Our key idea is to extend GFlowNets as the acquisition function optimizer for MOBO, with the objective to sample a diverse batch of candidates from the approximate Pareto front. To begin, we introduce GFlowNets in the context of molecule design, then describe how GFlowNet can be biased by a preference-conditioned hypernetwork to sample molecules according to various trade-offs between objectives. Next, we propose a hindsight-like off-policy strategy to speed up learning in HN-GFN. Lastly, we introduce the evidential surrogate model.\n\n4.1 PRELIMINARIES\n\nGFlowNets (Bengio et al., 2021a) seek to learn a stochastic policy π for sequentially constructing discrete objects x ∈ X with a probability π(x) ∝ R(x), where X is a compositional space and R : X → R≥0 is a non-negative reward function. The generation process of object x ∈ X can be represented by a sequence of discrete actions a ∈ A that incrementally modify a partially constructed object, which is denoted as state s ∈ S. Let generation process begin at a special initial state s0 and terminate with a special action indicating that the object is complete (s = x ∈ X ), the construction of an object x can be defined as a complete trajectory τ = (s0 → s1 → · · · → sn = x).\n\nFollowing fragment-based molecule design (Bengio et al., 2021a; Xie et al., 2021), we first define a vocabulary of building blocks (molecule fragments), then generate molecular graphs by sequentially attaching a fragment to an atom of the partially constructed molecules. There are multiple action sequences leading to the same state, and no fragment deleting actions, the space of possible action sequences can thus be denoted by a directed acyclic graph (DAG) G = (S, E), where the edges in E are transitions s → s′ from one state to another. To learn the aforementioned desired policy, Bengio et al. (2021a) propose to see the DAG structure as a flow network.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nMarkovian flows. Bengio et al. (2021b) first define a trajectory flow F : T → R≥0 on the set of all complete trajectories T to measure the unnormalized density. The edge flow and state flow can then be defined as F (s → s′) = (cid:80) s∈τ F (τ ), respectively. The τ ∈T F (τ ) . If flow F is Markovian, the trajectory flow F determines a probability measure P (τ ) = forward transition probabilities PF can be computed as PF (s′|s) = F (s→s′)\n\ns→s′∈τ F (τ ) and F (s) = (cid:80)\n\nF (τ )\n\n(cid:80)\n\n.\n\nF (s)\n\nFlow matching objective. A flow is consistent if the following flow consistency equation is satisfied ∀s ∈ S:\n\nF (s) =\n\n(cid:88)\n\nF (s′ → s) = R(s) +\n\n(cid:88)\n\nF (s → s′′)\n\n(1)\n\ns′∈P aG (s)\n\ns′′:s∈P aG (s′′)\n\nwhere P aG(s) is a set of parents of s in G. As proved in Bengio et al. (2021a), if the flow consistency equation is satisfied with R(s) = 0 for non-terminal state s and F (x) = R(x) ≥ 0 for terminal state x, a policy π defined by the forward transition probability π(s′|s) = PF (s′|s) samples object x with a probability π(x) ∝ R(x). GFlowNets propose to approximate the edge flow F (s → s′) using a neural network Fθ(s, s′) with enough capacity, such that the flow consistency equation is respected at convergence. To achieve this, Bengio et al. (2021a) define a temporal difference-like (Sutton & Barto, 2018) learning objective, called flow-matching (FM):\n\n(cid:32)\n\nLFM(s, R; θ) =\n\nlog\n\n(cid:80)\n\ns′∈P aG (s) Fθ(s′, s)\n\nR(s) + (cid:80)\n\ns′′:s∈P aG (s′′) Fθ(s, s′′)\n\n(cid:33)2\n\n(2)\n\nBengio et al. (2021a) prove that we can use any exploratory policy (cid:101)π with full support to sample training trajectories and obtain the consistent flow Fθ(s, s′) by minimizing the FM objective. Consequently, a policy defined by this approximate flow πθ(s′|s) = PFθ (s′|s) = Fθ(s→s′) can also sample objects x with a probability πθ(x) proportionally to reward R(x). Practically, the training trajectories are sampled from an exploratory policy which is a mixture between PFθ and a uniform distribution over allowed actions (Bengio et al., 2021a).\n\nFθ(s)\n\n4.2 HYPERNETWORK-BASED GFLOWNETS\n\nOur proposed HN-GFN aims at sampling a diverse batch of candidates from the approximate Pareto front with a unified model. A common approach to MOO is to decompose it into a set of scalar optimization problems with different scalarization functions and apply standard single-objective optimization methods to gradually approximate the Pareto front (Knowles, 2006; Zhang & Li, 2007). We here consider convex combinations (weighted sum) of the objectives. Let λ = (λi, · · · , λM ) ∈ SM be a preference vector defining the trade-off between the competing properties, where SM = {λ ∈ Rm| (cid:80) i λi = 1, λi ≥ 0} is the M − 1 simplex. Then the scalarization function can be formulated as sλ(x) = (cid:80)\n\ni λif i(x).\n\nTo support parallel evaluations in BO, one can obtain candidates according to different scalarizations (Daulton et al., 2020). Practically, this approach hardly scales efficiently with the number of objectives for discrete problems. Taking GFlowNet as an example, we need to train multiple GFlowNets independently for each choice of the reward function Rλ(x) = sλ(x) to cover the objective space:\n\nθ∗ λ = arg min\n\nEs∈S LFM(s, Rλ)\n\nθ\n\n(3)\n\nOur key motivation is to design a unified GFlowNet to sample candidates according to different reward functions, even ones not seen during training. Instead of defining the reward function with a fixed preference vector λ, we propose to train a preference-conditioned GFlowNet on a distribution of reward functions Rλ, where the preference vector λ is sampled from a simplex SM :\n\nθ∗ = arg min\n\nθ\n\nEλ∈SM\n\nEs∈S LFM(s, Rλ)\n\n(4)\n\nNote that the preliminary concept of conditional GFlowNet was originally introduced in Bengio et al. (2021b). We study and instantiate this concept, aiming to facilitate MOO in the context of molecule design.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nRemarks. Our proposed optimization scheme of training a single model to fit a family of loss functions fits into the framework of YOTO (Dosovitskiy & Djolonga, 2019). As proved in Dosovitskiy & Djolonga (2019), assuming an infinite model capacity, the proposed optimization scheme (Eq. 4) is as powerful as the original one (Eq. 3), since the solutions to both loss functions coincide. Nevertheless, the assumption of infinite capacity is extremely strict and hardly holds, so how to design the conditioning mechanism in practice becomes crucial.\n\n4.2.1 HYPERNETWORK-BASED CONDITIONING MECHANISM\n\nWe propose to condition the GFlowNets on the preference vectors via hypernetworks (Ha et al., 2016). Hypernetworks are deep networks that generate the weights of a target network based on inputs. In vanilla GFlowNets, the flow predictor Fθ is parameterized with the MPNN (Gilmer et al., 2017) over the graph of molecular fragments, with two prediction heads approximating F (s, s′) and F (s) based on the node and graph representations respectively. These two heads are parameterized with multi-layer perceptrons (MLPs).\n\nOne can view the training of HN-GFN as learning an agent to perform multiple policies that correspond to different goals (reward functions R) defined in the same environment (state space S and action space A). Therefore, we propose to only condition the weights of prediction heads θpred with hypernetworks, while sharing the weights of MPNN θmpnn, leading to more generalizable state representations. More precisely, a hypernetwork h(·; φ) takes as inputs the preference vector λ to output the weights θpred = h(λ; φ) of prediction heads in the flow predictor Fθ. For brevity, we write θ = (θmpnn, θpred). Following Navon et al. (2021), we parametrize h using a MLP with multiple heads, each generating weights for different layers of the target network.\n\n4.2.2 AS THE ACQUISITION FUNCTION OPTIMIZER\n\nTraining. At each iteration, we first randomly sample a new preference vector λ from a Dirichlet distribution Dir(α). Then the HN-GFN is trained in a usual manner with the reward function set as Rλ(x) = a(μ(sλ(x)), σ(sλ(x)); M), where μ and σ are posterior mean and standard deviation estimated by M.\n\nSampling. At each round i, we use the trained HN-GFN to sample a diverse batch of b candidates. Let Λi be the set of l target preference vectors λi target ∈ Λi and evaluate them on the oracle in parallel. In practice, we simply sample λi target from Dir(α), but it is worth noting that this prior distribution can also be defined adaptively based on the trade-off of interest. As the number of objectives increases, we choose a larger l to cover the objective space.\n\nl molecules per λi\n\ntarget. We sample b\n\n4.3 HINDSIGHT-LIKE OFF-POLICY STRATEGY\n\nResorting to the conditioning mechanism, HN-GFN can learn a family of policies to achieve various goals, i.e., one can treat sampling high-reward molecules for a particular preference vector as a separate goal. As verified empirically in Jain et al. (2022), since the FM objective is off-policy and offline, we can use offline trajectories to train the target policy for better exploration, so long as the assumption of full support holds. Our key insight is that each policy can learn from the valuable experience (high-reward molecules) of other similar policies.\n\nTo achieve this, inspired by Hindsight Experience Replay (Andrychowicz et al., 2017) in RL, we propose to share high-performing molecules among policies by re-examining them with different preference vectors. Because there are infinite possible preference vectors, here we only focus on Λi, target ∈ Λi. which are based on to sample candidates at round i, and build a replay buffer for each λi After sampling some trajectories during training, we store in the replay buffers the complete object x with the reward Rλi\n\n(x).\n\ntarget\n\nAlgorithm 2 describes the training procedure for HN-GFN with the proposed hindsight-like strategy. At each iteration, we first sample a preference vector from a mixture between Dir(α) and a uniform distribution over Λi: (1 − γ)Dir(α) + γUniform. If Λ is chosen, we construct half of the training batch with offline trajectories from the corresponding replay buffer of molecules encountered with the highest rewards. Otherwise, we incorporate offline trajectories from the current observed dataset Di instead to ensure that HN-GFN samples correctly in the vicinity of the observed Pareto set.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Left: The distribution of Top-100 JNK3 scores over different preference vectors. Right: The progression of the average Top-20 rewards over the course of training of the HN-GFN in optimizing GSK3β and JNK3 with different strategies.\n\n4.4 EVIDENTIAL SURROGATE MODEL\n\nWhile GPs are well-established in continuous spaces, they scale poorly with the number of observations and do not perform well in discrete spaces (Swersky et al., 2020). There has been significant work in efficiently training non-Bayesian neural networks to estimate the uncertainty (Gal & Ghahramani, 2016). In this work, we use evidential deep learning (Amini et al., 2020) to explicitly learn the epistemic uncertainty. Compared with the widely used MC Dropout (Gal & Ghahramani, 2016) and Deep Ensembles (Lakshminarayanan et al., 2017), evidential deep learning presents the advantages of faster inference speed and superior calibrated uncertainty (Soleimany et al., 2021). As for the acquisition function, we use Upper Confidence Bound (Srinivas et al., 2010) to incorporate epistemic uncertainty. To be precise, the objectives are modeled with a single multi-task network and the acquisition function is applied to the scalarization. See Appendix C.3 for more discussion.\n\n5 EXPERIMENTS\n\nWe first verify that HN-GFN has adequate capacity to generalize over preference vectors in a synthetic scenario. Next, we evaluate the effectiveness of the proposed MOBO algorithm based on HN-GFN in practical scenarios, which are more in line with real-world molecular optimization. Implementation details and additional results are provided in the Appendix.\n\n5.1 SYNTHETIC SCENARIO\n\nHere, our goal is to demonstrate that we can leverage the HN-GFN to sample molecules with preference-conditioned property distributions. The HN-GFN is used as a stand-alone optimizer outside of MOBO to directly optimize the scalarizations of oracle scores. As the oracle cannot be called as many times as necessary practically, we refer to this scenario as a synthetic scenario. To better visualize the trend of the property distribution of the sampled molecules as a function of the preference vector, we only consider two objectives: inhibition scores against glycogen synthase kinase-3 beta (GNK3β) and c-Jun N-terminal kinase-3 (JNK3) (Li et al., 2018; Jin et al., 2020b).\n\nCompared methods. We compare HN-GFN against the following methods. Preference-specific GFlowNet is a vanilla GFlowNet trained independently for a particular preference vector. Note that the preference-specific GFlowNet is treated as ”gold standard” rather than the baseline, as it is trained and evaluated using the same preference vector. Concat-GFN and FiLM-GFN are two variations of the conditional GFlowNet based on FiLM (Perez et al., 2018) and concatenation, respectively. MOEA/D (Zhang & Li, 2007) and NSGA-III (Deb & Jain, 2013) are two multi-objective evolutionary algorithms that also incorporate preference information. We perform evolutionary algorithms over the 32-dim latent space learned by HierVAE (Jin et al., 2020a), which gives better optimization performance than JT-VAE (Jin et al., 2018).\n\nMetrics. All the above methods are evaluated over the same set of 5 evenly spaced preference vectors. For each GFlowNet-based method, we sample 1000 molecules per preference vector as\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Evaluation of different methods on the synthetic scenario\n\nMethod MOEA/D NSGA-III preference-specific GFlowNet\n\nHV 0.182 ± 0.045 0.364 ± 0.041 0.545 ± 0.055\n\nDiv\n\nCor\n\nn/a n/a 0.786 ± 0.013\n\nn/a n/a 0.653 ± 0.003\n\nConcat-GFN FiLM-GFN\n\nHN-GFN\n\n0.534 ± 0.069 0.431 ± 0.045\n\n0.786 ± 0.004 0.795 ± 0.014\n\n0.646 ± 0.008 0.633 ± 0.009\n\n0.550 ± 0.074\n\n0.797 ± 0.015\n\n0.666 ± 0.010\n\nTable 2: Evaluation of different methods on MOBO scenarios (mean ± std over 3 runs)\n\nHierVAE+qParEGO HierVAE+qEHVI LaMOO GP-BO MARS\n\nGSK3β + JNK3\n\nGSK3β + JNK3 + QED + SA\n\nHV 0.205 ± 0.015 0.341 ± 0.072 0.279 ± 0.090 0.368 ± 0.020 0.418 ± 0.095\n\nDiv\n\nn/a n/a n/a 0.347 ± 0.059 0.653 ± 0.072\n\nHV 0.186 ± 0.009 0.211 ± 0.006 0.190 ± 0.069 0.335 ± 0.021 0.273 ± 0.020\n\nDiv\n\nn/a n/a n/a 0.562 ± 0.031 0.754 ± 0.027\n\nHN-GFN HN-GFN w/ hindsight\n\n0.572 ± 0.087 0.669 ± 0.061\n\n0.810 ± 0.003 0.793 ± 0.007\n\n0.389 ± 0.012 0.416 ± 0.023\n\n0.744 ± 0.008 0.738 ± 0.009\n\nthe solutions. We compare the aforementioned methods on the following metrics: Hypervolume indicator (HV) measures the volume of the space dominated by the Pareto front of the solutions and bounded from below by the preference point (0, 0). Diversity (Div) is the average pairwise Tanimoto distance over Morgan fingerprints. Correlation (Cor) is the Spearman’s rank correlation coefficient between the probability of sampling molecules from an external test set under the GFlowNet and their respective rewards in the logarithmic domain (Nica et al., 2022). See more details in Appendix B.1.2. In a nutshell, HV and Div measure the quality of the solutions, while Cor measures how well the trained model is aligned with the given preference vector.\n\nExperimental results. As shown in Table 1, HN-GFN outperforms the baselines and achieves competitive performance to the preference-specific GFlowNets (gold standard) on all the metrics. Compared to the GFlowNet-based methods, the evolutionary algorithms (MOEA/D and NSGAIII) fail to find high-scoring molecules, especially the MOEA/D. HN-GFN outperforms ConcatGFN and FiLM-GFN in terms of HV and Cor, implying the superiority of the well-designed hypernetwork-based conditioning mechanism. The comparable performance of HN-GFN and preference-specific GFlowNets illustrates that HN-GFN can generalize over preference vectors. Therefore, the unified HN-GFN provides a significantly efficient way to explore various trade-offs between objectives. In Figure 2 (Left), we visualize the trend of the empirical distribution of JNK3 as the preference weight increases. Intuitively, HN-GFN and preference-specific GFlowNets show consistent trends: the larger the preference weight, the higher the average score.\n\n5.2 MULTI-OBJECTIVE BAYESIAN OPTIMIZATION\n\nNext, we evaluate the effectiveness of HN-GFN as an acquisition function optimizer within MOBO in the practical scenarios, where there is a limited evaluation budget for oracle. We consider the following objective combinations of varying size:\n\n• GNK3β+JNK3: Jointly inhibiting Alzheimer-related targets GNK3β and JNK3.\n\n• GNK3β+JNK3+QED+SA: Jointly inhibiting GNK3β and JNK3 while being drug-like and\n\neasy-to-synthesize.\n\nWe rescale the SA score such that all the above properties have a range of [0,1] and higher is better. For both combinations, we consider starting with |D0| = 200 random molecules and further querying the oracle N = 8 rounds with batch size b = 100.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nBaselines. We compare HN-GFN with the following methods (from each category mentioned in section 2) as the acquisition function optimizer: HierVAE (Jin et al., 2020a) with qParEGO/qEHVI (Daulton et al., 2020) and LaMOO (Zhao et al., 2022) are LSO methods. GPBO (Tripp et al., 2021) uses Graph GA (Jensen, 2019) to optimize the acquisition function defined based on a GP with Tanimoto kernel. MARS (Xie et al., 2021) applies MCMC sampling to optimize the acquisition function defined based on the same surrogate function as HN-GFN. Note that the RL-based P-MOCO (Xi Lin, 2022) is also implemented but fails to optimize the properties.\n\nExperimental results. Table 2 shows that HN-GFN achieves superior performance over the baselines in terms of HV and Div, especially trained with the hindsight-like off-policy strategy. Note that the Div is computed among the batch of 100 candidates per round, we omit this metric for LSO methods as they only support 160 rounds with batch size 5 due to memory constraint. Our HN-GFN w/ hindsight outperforms the best baselines MARS and GP-BO of the two objective combinations by a large margin (60.0% and 24.2% relative improvement) with respect to HV, respectively. The promising performance can be attributed to the ability of HN-GFN to sample a diverse batch of candidates from the approximate Pareto front. Another interesting observation, in the more challenging settings where four objectives are optimized, is that MARS generates diverse candidates via MCMC sampling but fails to find high-quality Pareto front, indicating that HN-GFN can find high-reward modes better than MARS. The computational costs are discussed in the Appendix B.4.\n\n5.3 ABLATIONS\n\nIn the first round of MOBO, for each λtarget ∈ Λ we sample Effect of the hindsight-like strategy. 100 molecules every 500 training steps and compute the average Top-20 reward over Λ. In Figure 2 (Right), as we vary γ from 0 to 1, the hindsight-like strategy significantly boosts average rewards, demonstrating that sharing high-performing molecules among policies is effective for speeding up the training of HN-GFN. We choose γ = 0.2 for the desired trade-off between reward and generalization, see Appendix C.2 for a detailed explanation.\n\nEffect of α. Next, we study the effect of the prior distribution of preference vectors Dir(α). We consider the more challenging GNK3β+JNK3+QED+SA combination, where the difficulty of optimization varies widely for various properties. Table 3 shows that the distribution skewed toward harder properties results in better optimization performance.\n\nEffect of scalarization functions. In addition to the weighted sum (WS), we consider the Tchebycheff (Miettinen, 2012) that is also commonly used in MOO. Table 3 shows that Tchebycheff leads to a worse Pareto front compared to WS. We conjecture that the non-smooth reward landscapes induced by Tchebycheff are harder to optimize.\n\nTable 3: Ablation study of the α and scalarization functions on GNK3β+JNK3+QED+SA\n\n(1,1,1,1)\n\nHV 0.312 ± 0.039 0.815 ± 0.015 Div\n\nα (3,3,1,1) 0.385 ± 0.018 0.758 ± 0.018\n\n(3,4,2,1) 0.416 ± 0.023 0.738 ± 0.009\n\nscalarization function WS 0.416 ± 0.023 0.738 ± 0.009\n\nTchebycheff 0.304 ± 0.075 0.732 ± 0.014\n\n6 CONCLUSION\n\nWe have introduced a MOBO algorithm for sample-efficient multi-objective molecular optimization. This algorithm leverages a hypernetwork-based GFlowNet (HN-GFN) to sample a diverse batch of candidates from the approximate Pareto front. In addition, we present a hindsight-like off-policy strategy to improve optimization performance. Our algorithm outperforms existing approaches on synthetic and practical scenarios. Future work includes extending this algorithm to other discrete optimization problems such as biological sequence design and neural architecture search.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAlexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression.\n\nAdvances in Neural Information Processing Systems, 33:14927–14937, 2020.\n\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017.\n\nChristof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, and Lucy Colwell. Model-based reinforcement learning for biological sequence design. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=HklxbgBKvr.\n\nMaximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. Botorch: a framework for efficient monte-carlo bayesian optimization. Advances in neural information processing systems, 33:21524–21538, 2020.\n\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems, 34:27381–27394, 2021a.\n\nYoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou, Mo Tiwari, and Emmanuel Bengio.\n\nGflownet foundations. arXiv preprint arXiv:2111.09266, 2021b.\n\nG Richard Bickerton, Gaia V Paolini, J ́er ́emy Besnard, Sorel Muresan, and Andrew L Hopkins.\n\nQuantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012.\n\nJulian Blank and Kalyanmoy Deb. Pymoo: Multi-objective optimization in python. IEEE Access,\n\n8:89497–89509, 2020.\n\nEric Bradford, Artur M Schweidtmann, and Alexei Lapkin. Efficient multiobjective optimization Journal of global\n\nemploying gaussian processes, spectral sampling and a genetic algorithm. optimization, 71(2):407–438, 2018.\n\nSamuel Daulton, Maximilian Balandat, and Eytan Bakshy. Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization. Advances in Neural Information Processing Systems, 33:9851–9864, 2020.\n\nKalyanmoy Deb and Himanshu Jain. An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part i: solving problems with box constraints. IEEE transactions on evolutionary computation, 18(4):577–601, 2013.\n\nTristan Deleu, Ant ́onio G ́ois, Chris Emezue, Mansi Rankawat, Simon Lacoste-Julien, Stefan Bauer, and Yoshua Bengio. Bayesian Structure Learning with Generative Flow Networks. arXiv preprint, 2022.\n\nAlexey Dosovitskiy and Josip Djolonga. You only train once: Loss-conditional training of deep\n\nnetworks. In International conference on learning representations, 2019.\n\nMatthias Ehrgott. Multicriteria optimization, volume 491. Springer Science & Business Media,\n\n2005.\n\nPeter Ertl and Ansgar Schuffenhauer.\n\nlike molecules based on molecular complexity and fragment contributions. cheminformatics, 1(1):1–11, 2009.\n\nEstimation of synthetic accessibility score of drugJournal of\n\nTianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W Coley, and Jimeng Sun. DifferIn International Conference on Learning\n\nentiable scaffolding tree for molecule optimization. Representations, 2022.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059. PMLR, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWenhao Gao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. Sample efficiency matters: BenchIn ICML 2022 2nd AI for Science Workshop, 2022. URL\n\nmarking molecular optimization. https://openreview.net/forum?id=u7YqrmM5zrF.\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International conference on machine learning, pp. 1263–1272. PMLR, 2017.\n\nRafael G ́omez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos ́e Miguel Hern ́andez-Lobato, Benjam ́ın S ́anchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Al ́an Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–276, 2018.\n\nDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\n\nDaniel Hern ́andez-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive entropy search for multi-objective bayesian optimization. In International conference on machine learning, pp. 1492–1501. PMLR, 2016.\n\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure FP Dossou, Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al. Biological sequence design with gflownets. In International Conference on Machine Learning, pp. 9786–9801. PMLR, 2022.\n\nJan H Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for\n\nthe exploration of chemical space. Chemical science, 10(12):3567–3572, 2019.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola.\n\nJunction tree variational autoencoder for molecular graph generation. In International conference on machine learning, pp. 2323–2332. PMLR, 2018.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In International conference on machine learning, pp. 4839–4848. PMLR, 2020a.\n\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using In International conference on machine learning, pp. 4849–4859.\n\ninterpretable substructures. PMLR, 2020b.\n\nDonald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive\n\nblack-box functions. Journal of Global optimization, 13(4):455–492, 1998.\n\nKirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Neural architecture search with bayesian optimisation and optimal transport. Advances in neural information processing systems, 31, 2018.\n\nJoshua Knowles. Parego: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1): 50–66, 2006.\n\nMina Konakovic Lukovic, Yunsheng Tian, and Wojciech Matusik. Diversity-guided multi-objective bayesian optimization with batch evaluations. Advances in Neural Information Processing Systems, 33:17708–17720, 2020.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017.\n\nYibo Li, Liangren Zhang, and Zhenming Liu. Multi-objective de novo drug design with conditional\n\ngraph generative model. Journal of cheminformatics, 10(1):1–24, 2018.\n\nNatalie Maus, Haydn T Jones, Juston S Moore, Matt J Kusner, John Bradshaw, and Jacob R Gardner. Local latent space bayesian optimization over structured inputs. arXiv preprint arXiv:2201.11872, 2022.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science & Business\n\nMedia, 2012.\n\nHenry Moss, David Leslie, Daniel Beck, Javier Gonzalez, and Paul Rayson. Boss: Bayesian optimization over string spaces. Advances in neural information processing systems, 33:15476– 15486, 2020.\n\nAviv Navon, Aviv Shamsian, Ethan Fetaya, and Gal Chechik. Learning the pareto front with hyper-\n\nnetworks. In ICLR, 2021.\n\nAndrei Cristian Nica, Moksh Jain, Emmanuel Bengio, Cheng-Hao Liu, Maksym Korablyov, Michael M Bronstein, and Yoshua Bengio. Evaluating generalization in gflownets for molecule design. In ICLR2022 Machine Learning for Drug Discovery, 2022.\n\nBiswajit Paria, Kirthevasan Kandasamy, and Barnab ́as P ́oczos. A flexible framework for multiIn Uncertainty in Artificial\n\nobjective bayesian optimization using random scalarizations. Intelligence, pp. 766–776. PMLR, 2020.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nPavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of druglike chemical space based on gdb-17 data. Journal of computer-aided molecular design, 27(8): 675–679, 2013.\n\nCarl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on machine\n\nlearning, pp. 63–71. Springer, 2003.\n\nBobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1): 148–175, 2015.\n\nAva P Soleimany, Alexander Amini, Samuel Goldman, Daniela Rus, Sangeeta N Bhatia, and Connor W Coley. Evidential deep learning for guided molecular property prediction and discovery. ACS central science, 7(8):1356–1367, 2021.\n\nNiranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optiIn Proceedings of the 27th mization in the bandit setting: no regret and experimental design. International Conference on International Conference on Machine Learning, pp. 1015–1022, 2010.\n\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nKevin Swersky, Yulia Rubanova, David Dohan, and Kevin Murphy. Amortized bayesian optimization over discrete spaces. In Conference on Uncertainty in Artificial Intelligence, pp. 769–778. PMLR, 2020.\n\nAustin Tripp, Erik Daxberger, and Jos ́e Miguel Hern ́andez-Lobato. Sample-efficient optimization in the latent space of deep generative models via weighted retraining. Advances in Neural Information Processing Systems, 33:11259–11272, 2020.\n\nAustin Tripp, Gregor NC Simm, and Jos ́e Miguel Hern ́andez-Lobato. A fresh look at de novo\n\nmolecular design benchmarks. In NeurIPS 2021 AI for Science Workshop, 2021.\n\nQingfu Zhang Xi Lin, Zhiyuan Yang. Pareto set learning for neural multi-objective combinatorial In International Conference on Learning Representations, 2022. URL https:\n\noptimization. //openreview.net/forum?id=QuObT9BTWo.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. Mars: Markov molecular sampling for multi-objective drug discovery. In International Conference on Learning Representations, 2021.\n\nJiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. Advances in neural information processing systems, 31, 2018.\n\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua arXiv preprint\n\nGenerative flow networks for discrete probabilistic modeling.\n\nBengio. arXiv:2202.01361, 2022.\n\nQingfu Zhang and Hui Li. Moea/d: A multiobjective evolutionary algorithm based on decomposi-\n\ntion. IEEE Transactions on evolutionary computation, 11(6):712–731, 2007.\n\nYiyang Zhao, Linnan Wang, Kevin Yang, Tianjun Zhang, Tian Guo, and Yuandong Tian. MultiIn International Conference on Learning\n\nobjective optimization by learning space partition. Representations, 2022. URL https://openreview.net/forum?id=FlwzVjfMryn.\n\nEckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study and the strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257–271, 1999.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA ALGORITHMS\n\nAlgorithm 1 describes the overall framework of the proposed MOBO algorithm, where HN-GFN is leveraged as the acquisition function optimizer. Algorithm 2 describes the training procedure for HN-GFN within MOBO.\n\nAlgorithm 1 MOBO based on HN-GFN\n\nInput: oracle f = (f1, . . . , fM ), initial dataset D0 = {(x0 i ))}n parameter of Dirichlet distribution α, number of rounds N , batch size b Initialization: surrogate model M, parameters of HN-GFN πθ for i = 1 to N do\n\ni , f (x0\n\ni=1, acquisition function a,\n\nFit surrogate model M on dataset Di−1 Sample the set of target preference weights Λ ∼ Dir(α) Train πθ with reward function Rλ(x) = a(μ(sλ(x)), σ(sλ(x)); M) Sample query batch Bi = {xi Evaluate batch Bi with f and augment the dataset Di+1 = Di ∪ {(xi\n\nj=1 based on λtarget ∈ Λ\n\nj}b\n\n▷ Algorithm 2\n\nj, f (xi\n\nj))}b\n\nj=1\n\nend for\n\nAlgorithm 2 Training procedure for HN-GFN with the hindsight-like off-policy strategy\n\nInput: available dataset Di, reward function R, minibatch size m, set of target preference vectors Λ, proportion of hindsight-like strategy γ, replay buffers {Rλ}λ∈Λ while not converged do Flag ∼ Bernoulli(γ) if Flag = 1 then\n\nλ ∼ Λ Sample m\n\nelse\n\n2 trajectories from replay buffer Rλ\n\nλ ∼ Dir(α) Sample m\n\n2 trajectories from the available dataset Di\n\nend if θ = (θmpnn, h(λ; φ)) Sample m Compute reward Rλ(x) on terminal states x from each trajectory in the minibatch Update parameters θmpnn and φ with a stochastic gradient descent step w.r.t Eq. 2\n\n2 trajectories from policy (cid:101)π and store terminal states x in Rλ for all λ ∈ Λ\n\nend while\n\nB IMPLEMENTATION DETAILS\n\nB.1 EXPERIMENTAL SETTINGS\n\nB.1.1 MOLECULE DOMAIN\n\nFollowing (Bengio et al., 2021b), the molecules are generated based on a set of 105 building blocks. The same substructure containing multiple stems (atoms for linking another building block) is served as separate building blocks. We allow the GFlowNet to sample molecules with 2-8 blocks. As for the oracles, we adopt the property prediction models (random forest) released by (Xie et al., 2021) to evaluate the inhibition ability of generated molecules against GSK3β and JNK3.\n\nB.1.2 METRICS\n\nDiversity. Diversity (Div) is the average pairwise Tanimoto distance over Morgan fingerprints. In the synthetic scenario, for each preference vector, we sample 1000 molecules, calculate the Div among the Top-100 molecules, and report the averages over preferences. In MOBO, the DiV is computed among the batch of 100 candidates per round, as GP-BO and MARS are not preferenceconditioned. And we believe this metric possibly is more aligned with how these methods might be used in a biology or chemistry experiment.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nCorrelation. Correlation (Cor) is the Spearman’s rank correlation coefficient between the probability of sampling molecules from an external test set under the GFlowNet and their respective rewards in the logarithmic domain: Cor = Spearman’s ρlog(π(x)),log(R(x)) The external test set is obtained in two steps: First, we generate a random dataset containing 300K molecules uniformly based on the number of building blocks; Next, we sample the test sets with uniform property distribution corresponding to GSK3β and JNK3, respectively, from the 300K molecules. The final test set contains 6062 molecules.\n\nB.2 BASELINES\n\nAll the baselines are implemented using the publicly released source codes with adaptations for our MOBO scenarios. Our evolutionary algorithms are implemented in PyMOO (Blank & Deb, 2020), and the LSO methods are implemented in BoTorch (Balandat et al., 2020). LaMOO and GP-BO utilize EHVI as the acquisition function. For all GP-based methods, each objective is modeled by an independent GP.\n\nB.3 HN-GFN\n\nWe implement the proposed HN-GFN in PyTorch (Paszke et al., 2019). The values of key hyperparameters are illustrated in Table 4.\n\nSurrogate model: We use the 12-layer MPNN as the base architecture of the surrogate model in our experiments. In MOBO, a single multi-task MPNN is trained with a batch size of 64 using the Adam optimizer with a dropout rate of 0.1 and a weight decay rate of 1e-6. We apply early stopping to improve generalization.\n\nHN-GFN: HN-GFN contains a vanilla GFlowNet and a preference-conditioned hypernetwork. The architecture of GFlowNet is a 10-layer MPNN, and the hypernetwork is a 3-layer MLP with multiple heads, each generating weights for different layers of the target network. The HN-GFN is trained with Adam optimizer to optimize the Flow Matching objective.\n\nB.4 EMPIRICAL RUNNING TIME\n\nThe efficiency is compared on the same computing facilities using 1 Tesla V100 GPU. In the context of MOBO, the running time of three LSO methods (i.e., HierVAE+qParEGO, HierVAE+qEHVI, and LaMOO) is around 3 hours, while GP-BO optimizes much faster and costs only 13 minutes. In contrast, the time complexity of deep-learning-based discrete optimization methods is much larger. MARS costs 32 hours, while our proposed HN-GFN costs 10 hours. With the hindsight-like training strategy, the running time of HN-GFN will increase roughly by 33%.\n\nHowever, if we look at the problem in a bigger picture, the time costs for model training are most likely negligible in comparison to those of evaluating the molecular candidates in real-world applications. Hence, we argue that the high quality of the candidates (the performance of the MOBO algorithm) is more essential than having a lower training cost.\n\nC ADDITIONAL RESULTS\n\nC.1 SYNTHETIC SCENARIO\n\nAs illustrated in Figure 3, the distribution of Top-100 GSK3β scores shows a consistent trend in preference-specific GFlowNet and our proposed HN-GFN, although the trend is not as significant as the JNK3 property.\n\nC.2 EFFECT OF THE HINDSIGHT-LIKE STRATEGY\n\nThere is a trade-off between reward and generalization. As we vary γ from 0 to 1, the training distribution of preference vectors moves from Dir(α) to the set of target preference vectors Λ.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Hyper-parameters used in the real-world MOBO experiments.\n\nHyper-parameter\n\nHidden size Learning rate λ for evidential regression Number of iterations Early stop patience Dropout Weight decay\n\nβ\n\nGSK3β + JNK3 GSK3β + JNK3 + QED + SA Surrogate model 64 2.5e-4 0.1 10000 500 0.1 1e-6\n\n64 1e-3 0.1 10000 500 0.1 1e-6\n\nAcquisition function (UCB)\n\n0.1\n\nHN-GFN\n\nLearning rate Reward exponent Reward norm Trajectories minibatch size Offline minibatch size hindsight γ Uniform policy coefficient Hidden size for GFlowNet Hidden size for hypernetwork Training steps α\n\n5e-4 8\n1.0 8\n8 0.2 0.05 256 100 5000 (1,1)\n\n0.1\n\n5e-4 8\n1.0 8\n8 0.2 0.05 256 100 5000 (3,4,2,1)\n\nFigure 3: Comparison of the distribution of Top-100 GSK3β scores sampled by different preference vectors using preference-specific GFlowNets and HN-GFN.\n\nExclusively training the HN-GFN with the finite target preference vectors can lead to poor generalization. In practice, although we only sample candidates based on Λ, we argue that it is vital to keep the generalization such that we can leverage the trained HN-GFN to explore various preference vectors adaptively. In Figure 2 (Right), we found that increasing γ leads to slight (not significant) improvement in average reward compared to γ = 0.2. Hence, we believe 0.2 is the desired trade-off.\n\nC.3 EFFECT OF SURROGATE MODELS\n\nWe conduct ablation experiments to study the effectiveness of different surrogate models. We consider the following three surrogate models: evidential regression (Amini et al., 2020), Deep Ensembles (Lakshminarayanan et al., 2017), and GP based on Tanimoto kernel (Tripp et al., 2021). As\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nshown in Table 5, we can observe that evidential regression leads to better optimization performance than Deep Ensembles. While the HV of evidential regression and GP is comparable, evidential regression can propose more diverse candidates. Furthermore, we argue that GP is less flexible over discrete spaces than evidential regression and Deep Ensembles, as different kernels need to be designed according to the data structures.\n\nTable 5: Evaluation of different surrogate models on MOBO scenarios\n\nHN-GFN (Evidential) HN-GFN (Ensemble) HN-GFN (GP)\n\nGSK3β + JNK3\n\nGSK3β + JNK3 + QED + SA\n\nHV 0.669 ± 0.061 0.583 ± 0.103 0.662 ± 0.054\n\nDiv 0.793 ± 0.007 0.797 ± 0.004 0.739 ± 0.008\n\nHV 0.416 ± 0.023 0.355 ± 0.048 0.421 ± 0.037\n\nDiv 0.738 ± 0.009 0.761 ± 0.012 0.683 ± 0.018\n\nC.4 SAMPLED MOLECULES IN MOBO EXPERIMENTS\n\nWe give some examples of sampled molecules from the Pareto front by HN-GFN in the GSK3β + JNK3 + QED + SA optimization setting (Figure 4). The numbers below each molecule refer to GSK3β, JNK3, QED, and SA scores respectively.\n\nFigure 4: Sampled molecules from the approximate Pareto front by HN-GFN.\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a multi-objective Bayesian optimization approach for the molecules design problem. The proposed approach uses the hypernetwork-based GFlowNets as an acquisition function optimizer and uses a scalarization approach to combine the multiple objectives.\n\n# Strength And Weaknesses\n\nStrengths: \n+ The paper presents an important scientific application. \n+ The paper presents some promising experimental results, though I have some reservations about the robustness of the results\n+ The paper is easy to follow \n+ The paper addresses the multi-objective problem, which is relatively less studied in the context of molecules, but it is worth noting that it has been recently extensively studied in the general Bayesian optimization problem\n\nWeaknesses\n\n+ The proposed technique is a direct combination of existing techniques with no new substantial addition. Therefore, the technical contribution and novelty are weak\n+ The paper uses the following statement “We assume that the oracle can be called as many times as necessary.” In Expensive settings, this is not usually true. \n+ The paper does not provide any time complexity analysis of the training. This is problematic because existing approaches are actually very fast, while gflownet is certainly much more expensive, so a time comparison and a discussion about complexity and tradeoffs are important.\n+ The paper discusses the sampling of the scalars extensively, but later in experiments, it is mentioned that 5 evenly-spaced preference vectors are used. It is not clear how this works exactly. \n+ State of the art \n    - State-of-the-art methods in molecular optimization are not stated or compared to.  The following is considered SOTA work and covers a wide range of relevant methods and benchmarks that should be discussed for fairness. [1]\n    - Most multi-objective BO papers are not mentioned nor compared to. In batch optimization the most efficient and high-performing methods are [2,3,4]. It is also misleading to state that no previous paper discussed diversity while [2] is a diversity-oriented method, and several single objective batch BO papers discussed diversity. \n    - The paper uses a scalarization technique where scalars are sampled from a distribution. This technique was previously proposed and used [5]. \n    - The Preference-based problem has been studied beyond the discussion that was mentioned in the paper. It is concerning to completely ignore principled existing work and claim it as a novelty. The following are some of the approaches, to name a few [6,7,8]\n\n+ The experimental setup is weak and surprising:\n- the paper states in the beginning that the evaluation was on several synthetic experiments and real-world experiments, but there are only two experiments. \n    - The paper uses three runs only to report the mean and standard error. BO papers report AT LEAST 10 runs usually, and most recent papers report 50 to 100 runs. I don’t think 3 runs can provide any statistical significance or deliver any conclusions about performance. In fact, even expensive deep learning models are usually tested with a higher number of runs. \n    - The diversity is not reported for some of the algorithms. If diversity is a metric applied to the Pareto front, why can’t it be applied to some of the approaches? \n    - The paper reports results for batch size 100 only. Batch BO papers usually evaluate several batch sizes. \n    - There is a total absence of many relevant baselines from molecular optimization, Bayesian optimization, and preference-based optimization. The paper is mainly experimental since the technical novelty is weak. Therefore, it needs to present a thorough experimental evaluation. \n\n[1] Maus, Natalie, et al. \"Local Latent Space Bayesian Optimization over Structured Inputs.\" arXiv preprint arXiv:2201.11872 (2022).\n\n[2] Konakovic Lukovic, Mina, Yunsheng Tian, and Wojciech Matusik. \"Diversity-guided multi-objective bayesian optimization with batch evaluations.\" Advances in Neural Information Processing Systems 33 (2020): 17708-17720.\n\n[3] Eric Bradford, Artur M Schweidtmann, and Alexei Lapkin. Efficient multiobjective optimization employing gaussian processes, spectral sampling and a genetic algorithm. Journal of global optimization, 71(2):407–438, 2018.\n\n[4] Syrine Belakaria and Aryan Deshwal. Uncertainty-aware search framework for multi-objective bayesian optimization. In AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\n[5] Paria, Biswajit, Kirthevasan Kandasamy, and Barnabás Póczos. \"A flexible framework for multi-objective bayesian optimization using random scalarizations.\" Uncertainty in Artificial Intelligence. PMLR, 2020.\n\n[6] Abdolshah M, Shilton A, Rana S, Gupta S, Venkatesh S. Multi-objective Bayesian optimization with preferences over objectives. Advances in neural information processing systems. 2019;32.\n\n[7] Taylor, Kendall, et al. \"Bayesian preference learning for interactive multi-objective optimization.\" Proceedings of the Genetic and Evolutionary Computation Conference. 2021.\n\n[8] Lin, Zhiyuan Jerry, et al. \"Preference Exploration for Efficient Bayesian Optimization with Multiple Outcomes.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe novelty is limited since the paper uses a combination of previous approaches. \n\nReproducibility is also concerning since the paper reports only three runs.\n\n# Summary Of The Review\n\nThe paper addresses an important problem however, the novelty and experimental setup are limited.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nOPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN GRADIENT DIRECTION VIA CONVEX OPTIMIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe computation of Wasserstein gradient direction is essential for posterior sampling problems and scientific computing. The approximation of the Wasserstein gradient with finite samples requires solving a variational problem. We study the variational problem in the family of two-layer networks with squared-ReLU activations, towards which we derive a semi-definite programming (SDP) relaxation. This SDP can be viewed as an approximation of the Wasserstein gradient in a broader function family including two-layer networks. By solving the convex SDP, we obtain the optimal approximation of the Wasserstein gradient direction in this class of functions. We also propose practical algorithms using subsampling and dimension reduction. Numerical experiments including PDEconstrained Bayesian inference and parameter estimation in COVID-19 modeling demonstrate the effectiveness and efficiency of the proposed method.\n\n1\n\nINTRODUCTION\n\nBayesian inference plays an essential role in learning model parameters from the observational data with applications in inverse problems, scientific computing, information science, and machine learning (Stuart, 2010). The central problem in Bayesian inference is to draw samples from a posterior distribution, which characterizes the parameter distribution given data and a prior distribution.\n\nThe Wasserstein gradient flow (Otto, 2001; Ambrosio et al., 2005; Junge et al., 2017) has shown to be effective in drawing samples from a posterior distribution, which attracts increasing attention in recent years. For instance, the Wasserstein gradient flow of Kullback-Leibler (KL) divergence connects to the overdampled Langevin dynamics. The time-discretization of the overdamped Langevin dynamics renders the classical Langevin Monte Carlo Markov Chain (MCMC) algorithm. In this sense, the computation of Wasserstein gradient flow yields a different viewpoint for sampling algorithms. In particular, the Wasserstein gradient direction also provides a deterministic update of the particle system (Carrillo et al., 2021b). Based on the approximation or generalization of the Wasserstein gradient direction, many efficient sampling algorithms have been developed, including Wasserstein gradient descent (WGD) with kernel density estimation (KDE) (Liu et al., 2019), Stein variational gradient descent (SVGD) (Liu & Wang, 2016), and neural variational gradient descent (di Langosco et al., 2021), etc.\n\nMeanwhile, neural networks exhibit tremendous optimization and generalization performance in learning complicated functions from data. They also have wide applications in Bayesian inverse problems (Rezende & Mohamed, 2015; Onken et al., 2020; Kruse et al., 2019; Lan et al., 2021). According to the universal approximation theorem of neural networks (Hornik et al., 1989; Lu et al., 2017), any arbitrarily complicated functions can be learned by a two-layer neural network with nonlinear activations and a sufficient number of neurons. Functions represented by neural networks naturally provide an approximation towards the Wasserstein gradient direction.\n\nHowever, due to the nonlinear and nonconvex structure of neural networks, optimization algorithms including stochastic gradient descent may not find the global optima of the training problem. Recently, based on a line of works (Pilanci & Ergen, 2020; Sahiner et al., 2020; Bartan & Pilanci, 2021a), the regularized training problem of two-layer neural networks with ReLU/polynomial activation can be formulated as a convex program. Indeed, by solving the convex program, we can\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nconstruct the entire set of global optima of the nonconvex training problem (Wang et al., 2020). Theoretical analysis (Wang et al., 2022) shows that global optima of the training problem correspond to the simplest models with good generalization properties. Moreover, numerical results (Pilanci & Ergen, 2020) show that neural networks found by solving the convex program can achieve higher train accuracy and test accuracy compared to neural networks trained by SGD with the same number of parameters.\n\nIn this paper, we study a variational problem, whose optimal solution corresponds to the Wasserstein gradient direction. Focusing on the family of two-layer neural networks with squared ReLU activation, we formulate the regularized variational problem in terms of samples. Directly training the neural network to minimize the loss may get the neural network stuck at local minima or saddle points and it often leads to biased sample distribution from the posterior. Instead, we analyze the convex dual problem of the training problem and study its semi-definite program (SDP) relaxation by analyzing the geometry of dual constraints. The resulting SDP can be efficiently solved by convex optimization solvers such as CVXPY (Diamond & Boyd, 2016). We then derive the corresponding relaxed bidual problem (dual of the relaxed dual problem). Thus, the optimal solution to the dual problem yields an optimal approximation of the Wasserstein gradient direction in a broader function family. We also analyze the choice of the regularization parameter and present a practical implementation using subsampling and parameter dimension reduction to improve computational efficiency. Numerical results for experiments including PDE-constrained inference problems and Covid-19 parameter estimation problems illustrate the effectiveness and efficiency of our method.\n\n1.1 RELATED WORKS\n\nThe time and spatial discretizations of Wasserstein gradient flows are extensively studied in literature (Jordan et al., 1998; Junge et al., 2017; Carrillo et al., 2021a;b; Bonet et al., 2021; Liutkus et al., 2019; Frogner & Poggio, 2020). Recently, neural networks have been applied in solving or approximating Wasserstein gradient flows (Mokrov et al., 2021; Lin et al., 2021b;a; Alvarez-Melis et al., 2021; Bunne et al., 2021; Hwang et al., 2021; Fan et al., 2021). For sampling algorithms, di Langosco et al. (2021) learns the transportation function by solving an unregularized variational problem in the family of vector-output deep neural networks. Compared to these studies, we focus on a convex SDP relaxation of the varitional problem induced by the Wasserstein gradient direction. Meanwhile, Feng et al. (2021) form the Wasserstein gradient direction as the mininimizer the Bregman score and they apply deep neural networks to solve the induced variational problem.\n\nIn comparison to previous works on the convex optimization formulations of neural networks using SDP (Bartan & Pilanci, 2021a;b), they focus on the polynomial activation and give the exact convex In comparison, we focus on the neural optimization formulation (instead of convex relaxation). networks with the squared ReLU activation, which has not been considered before. Our method can also apply to the analysis of supervised learning problem using squared ReLU activated neural networks.\n\n2 BACKGROUND\n\nIn this section, we briefly review the Wasserstein gradient descent and present its variational formulation. In particular, we focus on the Wasserstein gradient descent direction of KL divergence functional. Later on, we design a neural network convex optimization problem to approximate the Wasserstein gradient in samples.\n\n2.1 WASSERSTEIN GRADIENT DESCENT\n\nConsider an optimization problem in the probability space:\n\ninf ρ∈P\n\nDKL(ρ(cid:107)π) =\n\n(cid:90)\n\nρ(x)(log ρ(x) − log π(x))dx,\n\n(1)\n\nHere the integral is taken over Rd and the objective functional DKL(ρ(cid:107)π) is the KL divergence from ρ to π. The variable is the density function ρ in the space P = {ρ ∈ C∞(Rd)| (cid:82) ρdx = 1, ρ > 0}. The function π ∈ C∞(Rd) is a known probability density function of the posterior distribution. By solving the optimization problem (1), we can generate samples from the posterior distribution.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nA known fact (Villani, 2003, Chapter 8.3.1) is that the Wasserstein gradient descent flow for the optimization problem (1) satisfies\n\n(cid:18)\n\n∂tρt =∇ ·\n\nρt∇\n\nδ δρt\n\n(cid:19)\n\nDKL(ρt(cid:107)π)\n\n= ∇ · (ρt(∇ log ρt − ∇ log π))\n\n(a) = ∆ρt − ∇ · (ρt∇ log π),\n\nδ δρt\n\nis the L2 first variation operator w.r.t. ρt, ∇ · F denotes the divergence where ρt(x) = ρ(x, t), of a vector valued function F : Rd → Rd and ∆ is the Laplace operator. In step (a) we uses the fact that ρt∇ log ρt = ∇ρt. This equation is also known as the gradient drift Fokker-Planck equation. It corresponds to the following updates in terms of samples:\n\ndxt = −(∇ log ρt(xt) − ∇ log π(xt))dt,\n\n(2)\n\nwhere xt follows the distribution of ρt. Clearly, when ρt = π, the above dynamics reach the equilibrium, which implies that the samples xt are generated by the posterior distribution.\n\nTo solve the Wasserstein gradient flow (2), we consider a forward Eulerian discretization in time. In the l-th iteration, suppose that {xn l } are samples drawn from ρl. The update rule of Wasserstein gradient descent (WGD) on the particle system {xn\n\nl } follows l − αl∇Φl(xn\n\nl ),\n\nxn\n\nl+1 = xn\n\n(3)\n\nwhere Φl : Rd → R is a function which approximates log ρl − log π and αl > 0 is the step size.\n\n2.2 VARIATIONAL FORMULATION OF WGD\n\nGiven the particles {xn}N tion Φ approximating the function log ρ − log π. Consider\n\nn=1, we design the following variational problem to choose a suitable func-\n\ninf Φ∈C1(Rd)\n\n(cid:90)\n\n1 2\n\n(cid:107)∇Φ(x − (∇ log ρ(x) − ∇ log π(x))(cid:107)2\n\n2ρ(x)dx.\n\n(4)\n\nThe objective functional evaluates the least-square discrepancy between ∇ log ρ − ∇ log π and ∇Φ weighted by the density ρ. The optimal solution follows Φ = log ρ−log π, up to a constant shift. Let H ⊆ C 1(Rd) be a finite dimensional function space. The following proposition gives a formulation of (4) in H. Proposition 1 Let H ⊆ C 1(Rd) be a function space. The variational problem (4) in the domain H can be reformulated to\n\n(cid:90)\n\n1 2\n\ninf Φ∈H\n\n(cid:107)∇Φ(x)(cid:107)2\n\n2ρdx +\n\n(cid:90)\n\n∆Φ(x)ρ(x)dx +\n\n(cid:90)\n\n(cid:104)∇ log π(x), ∇Φ(x)(cid:105) ρ(x)dx.\n\n(5)\n\nRemark 1 A similar variational problem has been studied in (di Langosco et al., 2021). If we replace ∇Φ for Φ ∈ H by a vector field Ψ in certain function family, then, the quantity in (5) is the negative regularized Stein discrepancy defined in (di Langosco et al., 2021) between ρ and π based on Ψ. This problem is also similar to the varitional problem for the score matching estimator in (Hyv ̈arinen & Dayan, 2005) by parameterizing Φ in a given probabilistic model. In comparison, our method can be viewed as a special case of score matching by using a two-layer neural network.\n\nTherefore, by replacing the density ρ by finite samples {xn}N finite samples forms\n\nn=1 ∼ ρ, the problem (5) in terms of\n\ninf Φ∈H\n\n1 N\n\nN (cid:88)\n\nn=1\n\n(cid:18) 1 2\n\n(cid:107)∇Φ(xn)(cid:107)2\n\n2 + ∆Φ(xn)\n\n(cid:19)\n\n+\n\n1 N\n\nN (cid:88)\n\nn=1\n\n(cid:104)∇ log π(xn), ∇Φ(xn)(cid:105) .\n\n(6)\n\n3 OPTIMAL NEURAL NETWORK APPROXIMATION OF WASSERSTEIN\n\nGRADIENT\n\nIn this section, we focus on functional space H of functions represented by two-layer neural networks. We derive the primal and dual problem of the regularized Wasserstein variational problems.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nBy analyzing the dual constraints, a convex SDP relaxation of the dual problem is obtained. We also present a practical implementation estimation of ∇ log ρ − ∇ log π and discuss the choice of the regularization parameter.\n\nLet ψ be an activation function. Consider the case where H is a class of two-layer neural network with the activation function ψ(x):\n\nH = (cid:8)Φθ ∈ C 1(Rd)|Φθ(x) = αT ψ(W T x)(cid:9) ,\n\n(7)\n\nwhere θ = (W, α) is the parameter in the neural network with W ∈ Rd×m and α ∈ Rm. Remark 2 We can extend this model to handle the bias term by add an entry of 1 in x1, . . . , xn.\n\nFor two-layer neural networks, we can compute the gradient and Laplacian of Φ ∈ H as follows:\n\n∇Φθ(x) =\n\nm (cid:88)\n\ni=1\n\nαiwiψ(cid:48)(wT\n\ni x) = W (ψ(cid:48)(W T x) ◦ α),\n\n∆Φθ(x) =\n\nm (cid:88)\n\ni=1\n\nαi(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni x).\n\n(8)\n\n(9)\n\nHere ◦ represents the element-wise multiplication. By adding a regularization term to the variational problem (6), we obtain\n\nmin θ\n\n1 2N\n\nN (cid:88)\n\nn=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nm (cid:88)\n\ni=1\n\nαiwiψ(cid:48)(wT\n\ni xn)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\n+\n\n1 N\n\nN (cid:88)\n\n(cid:42) m (cid:88)\n\nn=1\n\ni=1\n\nαiwiψ(cid:48)(wT\n\ni xn), ∇ log π(xn)\n\n(cid:43)\n\n(10)\n\n+\n\n1 N\n\nN (cid:88)\n\nm (cid:88)\n\nn=1\n\ni=1\n\nαi(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) +\n\nβ 2\n\nR(θ),\n\nwhere β > 0 is the regularization parameter. We focus on the squared ReLU activation ψ(z) = + = (max{z, 0})2. Note that a non-vanishing second derivative is required for the Laplacian (z)2 term in (9), which makes the ReLU activation inadequate. For this activation function, we consider the regularization function R(θ) = (cid:80)m Remark 3 We note that ∇Φθ(x) and ∆Φθ(x) are all piece-wise degree-3 polynomials of the parameters θ. Hence, we consider a specific cubic regularization term above, analogous to (Bartan & Pilanci, 2021a). By choosing this regularization term, we can derive a simplified dual problem.\n\ni=1((cid:107)wi(cid:107)3\n\n2 + |αi|3).\n\nBy utilizing the arithmetic and geometric mean (AM-GM) inequality, we can rescale the first and second-layer parameters and formulate the regularized variational problem (10) as follows. Proposition 2 (Primal problem) The regularized variational problem (10) can be reformulated to (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nαiwiψ(cid:48)(wT\n\n2ψ(cid:48)(cid:48)(wT\n\nαi(cid:107)wi(cid:107)2\n\nmin W,α\n\ni xn)\n\ni xn)\n\nm (cid:88)\n\nN (cid:88)\n\nN (cid:88)\n\nm (cid:88)\n\n1 2\n\n+\n\nn=1\n\ni=1\n\nn=1\n\nN (cid:88)\n\n+\n\nαiwiψ(cid:48)(wT\n\ni xn), ∇ log π(xn)\n\n+ ̃β(cid:107)α(cid:107)1,\n\n(cid:43)\n\n(11)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) i=1 (cid:42) m (cid:88)\n\nn=1\n\ni=1\n\ns.t. (cid:107)wi(cid:107)2 ≤ 1, i ∈ [m], where ̃β = 3 · 2−5/3N β and we denote [m] = {1, . . . , m}.\n\nIn short, the optimal value of (10) and (11) are the same. We can obtain the optimal solution of (11) by rescaling the optimal solution of (10) and vice versa. For simplicity, we write Y ∈ RN ×d whose n-row is ∇ log π(xn) for n ∈ [N ]. We introduce the slack variable zn = (cid:80)m n wi) for n ∈ [N ] and denote Z = [z1\n\nzN ]T ∈ RN ×d. Then, we can simplify the problem (11) to\n\ni=1 αiwiψ(cid:48)(xT\n\n. . .\n\nmin W,α,Z\n\n1 2\n\n(cid:107)Z(cid:107)2\n\nF +\n\nN (cid:88)\n\nm (cid:88)\n\nn=1\n\ni=1\n\nαi(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) + tr(Y T Z) + ̃β(cid:107)α(cid:107)1,\n\ns.t. zn =\n\nm (cid:88)\n\ni=1\n\nαiwiψ(cid:48)(xT\n\nn wi), n ∈ [N ], (cid:107)wi(cid:107)2 ≤ 1, i ∈ [m].\n\n4\n\n(12)\n\nUnder review as a conference paper at ICLR 2023\n\nTo derive the convex relaxtion of neural network training problem, the dual problem plays an import role. By applying the Lagrangian duality, we can derive the dual problem of (12) as follows.\n\nProposition 3 (Dual problem) The dual problem of the regularized variational problem (12) is\n\nmax Λ\n\n−\n\n1 2\n\n(cid:107)Λ + Y (cid:107)2\n\nF , s.t. max\n\nw:(cid:107)w(cid:107)2≤1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nN (cid:88)\n\nn=1\n\n(cid:107)w(cid:107)2\n\n2ψ(cid:48)(cid:48)(xT\n\nn w) − λT\n\nn wψ(cid:48)(xT\n\nn w)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤ ̃β,\n\n(13)\n\nwhich provides a lower-bound on (12).\n\nWe note that the dual problem can be infeasible if the regularization parameter ̃β is below certain threshold. In other words, if the regularization term is missing or the regularization parameter is not large enough, the optimal value of the dual problem is −∞ and the primal problem is not lower bounded.\n\n3.1 ANALYSIS OF DUAL CONSTRAINTS AND THE RELAXED DUAL PROBLEM\n\nNow, we analyze the constraint in the dual problem. We note that it is closely related to the regularization parameter, which we will discuss later. For simplicity, we take ψ(cid:48)(cid:48)(0) = 0 as the subgradient of ψ(cid:48)(z) at z = 0, i.e., taking the left derivative of ψ(cid:48)(z) at z = 0. Let X = [x1, . . . , xN ]T ∈ RN ×d. Denote the set of all possible hyper-plane arrangements corresponding to the rows of X as\n\nS = {diag(I(Xw ≥ 0))|w ∈ Rd, w (cid:54)= 0}.\n\n(14)\n\nHere I(s) = 1 if the statement s is correct and I(s) = 0 otherwise. Let p = |S| be the cardinality of S, and write S = {D1, . . . , Dp}. According to (Cover, 1965), we have the upper bound p ≤\n\n(cid:16) e(N −1) r\n\n(cid:17)r\n\n2r\n\n, where r = rank(X). Based on the analysis of the dual constraints, we can derive a\n\nconvex SDP as a relaxed dual problem.\n\nProposition 4 (Relaxed dual problem) The relaxed dual problem is the following SDP:\n\nmax\n\nΛ,{r(j,−),r(j,+)}p\n\nj=1\n\n−\n\n1 2\n\n(cid:107)Λ + Y (cid:107)2\n\nF ,\n\ns.t. ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,−)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, r(j,−) ≥ 0, j ∈ [p],\n\n(15)\n\n− ̃Aj(Λ) − ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,+)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, r(j,+) ≥ 0, j ∈ [p],\n\nwhere we denote [p] = {1, . . . , p}. For j ∈ [p], we denote Aj(Λ) = −ΛT DjX − X T DjΛ, , ̃Bj = Bj = 2 tr(Dj)Id, ̃Aj(Λ) = (cid:20)\n\n(cid:20)Aj(Λ) 0\n\n0 0 −1\n\nand H (j)\n\n(cid:20)Bj 0\n\n, H (j)\n\n0 =\n\nn =\n\n(cid:20)Id\n\n0 0\n\n0 0\n\n(cid:21)\n\n(cid:21)\n\n(cid:21)\n\n(cid:21)\n\n, n ∈ [N ] The vector ed+1 ∈ Rd+1 satisfies that (ed+1)i =\n\n0 (1 − 2(Dj)nn)xT\n\nn\n\n(1 − 2(Dj)nn)xn 0\n\n0 for i ∈ [d] and (ed+1)d+1 = 1.\n\nThe optimal value of (15) gives a lower bound on the dual problem (13), and hence on the primal problem (12).\n\nThe relaxed bi-dual problem provides insights on approximating the primal problem via convex optimization, which is derived as follows. As an equivalent formulation of the convex dual problem (15), it can be viewed as a convex relaxation of the primal problem (12).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nProposition 5 (Relaxed bi-dual problem) The dual of the relaxed dual problem (15) is as follows\n\nmin\n\nZ,{(S(j,+),S(j,−))}p\n\nj=1\n\n1 2\n\n(cid:107)Z + Y (cid:107)2\n\nF −\n\n1 2\n\n(cid:107)Y (cid:107)2\n\nF +\n\np (cid:88)\n\nj=1\n\ntr( ̃Bj(S(j,+) − S(j,−)))\n\np (cid:88)\n\n(cid:16)\n\ntr\n\n+ ̃β\n\n(S(j,+) + S(j,−))ed+1eT\n\nd+1\n\n(cid:17)\n\n,\n\ns.t. Z =\n\nj=1\n\np (cid:88)\n\nj=1\n\n ̃A∗\n\nj (S(j,−) − S(j,+)),\n\n(16)\n\ntr(S(j,−)H (j)\n\nn ) ≤ 0, tr(S(j,+)H (j)\n\nn ) ≤ 0, n = 0, . . . , N, j ∈ [p].\n\nHere A∗\n\nj is the adjoint operator of the linear operator Aj.\n\nAs (15) is a convex problem and the Slater’s condition is satisfied, the optimal values of (15) and (16) are same. The bi-dual problem (16) is closely related to the primal problem (12). Indeed, any feasible solutions of the primal problem (11) can be mapped to feasible solutions of (16). We note that the mapping from the primal solution to the bi-dual solution cannot go both ways, unless these two problems are equivalent.\n\nTheorem 1 Suppose that (Z, W, α) is feasible to the primal problem (12). Then, there exist matrices {S(j,+), S(j,−)}p j=1) is feasible to the relaxed bi-dual problem (16). Moreover, the objective value of the relaxed bi-dual problem (16) at (Z, {S(j,+), S(j,−)}p\n\nj=1 constructed from (W, α) such that (Z, {S(j,+), S(j,−)}p\n\nj=1) is the same as objective value of the primal problem (12) at (Z, W, α).\n\nLet J(Z, {S(j,+), S(j,−)}p a feasible solution (Z, {S(j,+), S(j,−)}p tion of the primal problem (12). By Theorem 1, there exist matrices {S(j,+), S(j,−)}p that (Z ∗, {S(j,+), S(j,−)}p J(Z ∗, {S(j,+), S(j,−)}p (Z ∗, W ∗, α∗). On the other hand, let ( ̃Z ∗, { ̃S(j,+), ̃S(j,−)}p relaxed bi-dual problem (16). From the optimality of ( ̃Z ∗, { ̃S(j,+), ̃S(j,−)}p\n\nj=1) denote the objective value of the relaxed bi-dual problem (16) at j=1). Let (Z ∗, W ∗, α∗) denote a globally optimal soluj=1 such j=1) is a feasible solution of the relaxed bi-dual problem (16) and j=1) is the same as the objective value of (12) at its global minimum j=1) denote an optimal solution of the\n\nj=1), we have\n\nJ( ̃Z ∗, { ̃S(j,+), ̃S(j,−)}p\n\nj=1) ≤ J(Z ∗, {S(j,+), S(j,−)}p\n\nj=1).\n\n(17)\n\nj=1) than at (Z ∗, {S(j,+), S(j,−)}p\n\nNote that at (Z ∗, W ∗, α∗) we obtain the optimal approximation of ∇ log ρ−∇ log π at x1, . . . , xN in the family of two-layer squared-ReLU networks (7). Smaller or equal objective value of the relaxed bi-dual problem (16) can be achieved at ( ̃Z ∗, { ̃S(j,+), ̃S(j,−)}p j=1). Therefore, we can view ̃Z ∗ gives an optimal approximation of ∇ log ρ − ∇ log π evaluated on x1, . . . , xN in a broader function family including the two-layer squared ReLU neural networks. From the derivation of the relaxed bi-dual problem, we have the relation ̃Z ∗ = −Λ∗ − Y , where (Λ∗, {r(j,+), r(j,−)) is optimal to the relaxed dual problem (15) and ( ̃Z ∗, { ̃S(j,+), ̃S(j,−)}p j=1) is optimal to the relaxed bi-dual problem (16). Therefore, by solving Λ∗ from the relaxed dual problem (15), we can use −Λ∗ − Y as the approximation of ∇ log ρ − ∇ log π evaluated on x1, . . . , xN . Remark 4 We note that solving the proposed convex optimization problem 15 renders the approximation of the Wasserstein gradient direction. Compared to the two-layer ReLU networks, it induces a broader class of functions represented by {S(j,+), S(j,−)}p j=1. This contains more variables than the neural network function.\n\n3.2 PRACTICAL IMPLEMENTATION\n\nAlthough the number p of all possible hyper-plane arrangements is upper bounded by 2r((N − 1)e/r)r with r = rank(X), it is computationally costly to enumerate all possible p matrices D1, . . . , Dp to represent the constraints in the relaxed dual problem (4). In practice, we random vectors u1, . . . , uM ∼ N (0, Id) and generate a subset first randomly sample M i.i.d.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nˆS = {diag(I(Xuj ≥ 0)|j ∈ [M ]}. of S. Then, we optimize the randomly sub-sampled version of the relaxed dual problem based on the subset ˆS and obtain the solution Λ. Here −Λ − Y is used as the direction to update the particle system X. If the regularization parameter is too large, then we will have −Λ − Y = 0, which makes the particle system unchanged. Therefore, to ensure that ̃β is not too large, we decay ̃β by a factor γ1 ∈ (0, 1). This also appears in (Ergen et al., 2021). On the other hand, if ̃β is too small resulting the relaxed dual problem (4) infeasible, we increase ̃β by multiplying γ−1 2 , where γ2 ∈ (0, 1). Detailed explanation of the adjustment of the regularization parameter can be found in Appendix D. The overall algorithm is summarized in Algorithm 1.\n\nAlgorithm 1 Convex neural Wasserstein descent\n\nRequire: initial positions {xn\n\n0 }N\n\nn=1, step size αl, initial regularization parameter ̃β0, γ1, γ2 ∈ (0, 1).\n\n1: while not converge do 2:\n\nForm Xl and Yl based on {xn Solve Λl from the relaxed dual problem (15) with ̃β = ̃βl. if the relaxed dual problem with ̃β = ̃βl is infeasible then\n\nn=1 and {∇ log π(xn\n\nl )}N\n\nl }N\n\nn=1.\n\nSet Xl+1 = Xl for n ∈ [N ] and set ̃βl+1 = γ−1\n\n ̃βl.\n\n2\n\nUpdate Xl+1 = Xl + αl(Λl + Yl) for n ∈ [N ] and set ̃βl+1 = γ1\n\n ̃βl.\n\n3: 4: 5: 6: 7: 8: 9: end while\n\nend if\n\nelse\n\nApplying the standard interior point method (Boyd et al., 2004) leads to the computational time\n\nO((max{N, d2}ˆp)6).\n\n(18)\n\nFor high-dimensional problems, i.e., d is large, the computational cost of solving (15) can be large. In this case, we apply the dimension-reduction techniques (Zahm et al., 2018; Chen & Ghattas, 2020; Wang et al., 2021a) to reduce the parameter dimension d to a data-informed intrinsic dimension ˆd, which is often very low, i.e., ˆd (cid:28) d, which can dramatically decrease the computational time (18).\n\n4 NUMERICAL EXPERIMENTS\n\nIn this section, we present numerical results to compare WGD approximated by neural networks (WGD-NN) and WGD approximated using convex optimization formulation of neural networks (WGD-cvxNN). The performance of compared methods is assessed by the sample goodness-of-fit of the posterior. For WGD-NN, in each iteration, it updates the particle system using (3) with a function Φ represented by a two-layer squared ReLU neural network. The parameters of the neural network is obtained by directly solving the nonconvex optimization problem (10). For high-dimensional problems, we apply the dimension reduction technique and compare the projected versions (pWGDNN and pWGD-cvxNN).\n\nWe note that although the cost for solving the relaxed dual problem (15) using standard convex optimization solvers in WGD-cvxNN can be higher compared to that by a direct neural network training in WGD-NN, this cost difference is negligible in the entire optimization dominated by the likelihood evaluation when the model (e.g., PDE) is expensive to solve. In such cases WGDcvxNN and WGD-NN have similar computational complexity but WGD-cvxNN achieves better performance. We use the standard convex optimization solver CVXPY (Diamond & Boyd, 2016) with MOSEK(ApS, 2019) inner solver. Applying randomized SDP solvers (Yurtsever et al., 2021), randomized second-order methods (Pilanci & Wainwright, 2017; Lacotte et al., 2021) or advanced SDP solvers (Zhao et al., 2010; Yang et al., 2015; Wang et al., 2021b) for large-scale problem can improve the computation time. Moreover, the induced SDPs have specific structures of many similar constraints. Solving the SDP (15) can be accelerated by designing a specialized convex optimization solver, which is left for future work.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n4.1 A TOY EXAMPLE\n\nWe test the performance of WGD on a bimodal 2-dimensional double-banana posterior distribution introduced in (Detommaso et al., 2018). We first generate 300 posterior samples by a Stein variational Newton (SVN) method (Detommaso et al., 2018) as the reference, as shown in Figure 1. We evaluate the performance of WGD-NN and WGD-cvxNN by calculating the maximum mean discrepancy (MMD) between their samples in each iteration and the reference samples. In the comparison, we use N = 50 samples and run for 100 iterations with step sizes αl = 10−3. For WGD-cvxNN, we set β = 1, γ1 = 0.95 and γ2 = 0.9510. For WGD-NN, we use m = 200 neurons and optimize the regularized training problem (10) using all samples with the Adam optimizer (Kingma & Ba, 2014) with learning rate 10−3 for 200 sub-iterations. We also set the regularization parameter β = 1 and decrease it by a factor of 0.95 in each iteration. We find that this setup of parameters is more suitable.\n\nThe posterior density and the sample distributions by WGD-cvxNN and WGD-NN at the final step of 100 iterations are shown in Figure 1. It can be observed that WGD-cvxNN provides more representative samples than WGD-NN for the posterior density. In Figure 2, we plot the MMD of the samples by WGD-cvxNN and WGD-NN compared to the reference SVN samples at each iteration. We observe that the samples by WGD-cvxNN achieves much smaller MMD than those of WGD-NN compared to the reference SVN samples, which is consistent with the results shown in Figure 1.\n\nFigure 1: Posterior density and sample distributions by WGD-cvxNN and WGD-NN at the final step of 100 iterations, compared to the reference SVN samples (right).\n\nFigure 2: MMD of WGD-cvxNN and WGD-NN samples compared to the reference SVN samples.\n\n4.2 PDE-CONSTRAINED NONLINEAR BAYESIAN INFERENCE\n\nIn this experiment, we consider a nonlinear Bayesian inference problem constrained by the following partial differential equation (PDE) (Chen & Ghattas, 2020) with application to subsurface (Darcy) flow in a physical domain D = (0, 1)2,\n\nv + ex∇u = 0 ∇ · v = h\n\nin D, in D,\n\n(19)\n\nwhere u is pressure, v is velocity, h is force, ex is a random (permeability) field equipped with a Gaussian prior x ∼ N (x0, C) with covariance operator C = (−δ∆ + γI)−α where we set δ = 0.1, γ = 1, α = 2 and x0 = 0. This problem is widely used in many areas, for instance, estimating permeability in groundwater flow, thermal conductivity in material science or electrical impedance in medical imaging, We impose Dirichlet boundary conditions u = 1 on the top boundary and u = 0 on the bottom boundary, and homogeneous Neumann boundary conditions on the left\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nand right boundaries for u. We use a finite element method with piecewise linear elements for the discretization of the problem, resulting in 81 dimensions for the discrete parameter. The data is generated as pointwise observation of the pressure field at 49 points equidistantly distributed in (0, 1)2, corrupted with additive 5% Gaussian noise. We use a DILI-MCMC algorithm Cui et al. (2016) with 10000 effective samples to compute the sample mean and sample variance, which are used as the reference values to assess the goodness of the samples.\n\nFigure 3: Ten trials and the RMSE of the sample mean (top) and sample variance (bottom) by pWGD-NN and pWGD-cvxNN at different iterations. Nonlinear inference with PDE constraint.\n\nWe run pWGD-cvxNN and pWGD-NN with 64 samples for ten trials with step size αl = 10−3, where we set β = 10, γ1 = 0.95, and γ2 = 0.9510 for both methods. The RMSE of the sample mean and sample variance are shown in Figure 3 for the two methods at each of the iterations. We can observe that pWGD-cvxNN achieves smaller errors for both the sample mean and the sample variance compared to pWGD-NN at each iteration. Moreover, pWGD-cvxNN provides much smaller variation of the sample mean and sample variance for the ten trials compared to pWGD-NN. Furthermore, by an effective reduction of the parameter dimension from 81 to data-informed 20 in our pWGD-cvxNN, as used and analyzed in (Zahm et al., 2018; Chen & Ghattas, 2020; Wang et al., 2021a), the time for solving the SDP is significantly reduced from about 800 seconds in average to less than 1 second (about 0.7 in average), making our pWGD-cvxNN computationally efficient.\n\n4.3 BAYESIAN INFERENCE FOR COVID-19\n\nIn this experiment, we use Bayesian inference to learn the dynamics of the transmission and severity of COVID-19 from the recorded data for New York state. We use the model, parameter, and data as in Chen & Ghattas (2020). More specifically, we use a compartmental model for the modeling of the transmission and outcome of COVID-19. We take the number of hospitalized cases as the observation data to infer a social distancing parameter, a time-dependent stochastic process that is equipped with a Tanh–Gaussian prior to model the transmission reduction effect of social distancing, which becomes 96 dimensions after discretization.\n\nWe use the projected Stein variational gradient descent (pSVGD) method Chen & Ghattas (2020) as the reference to evaluate the goodness of samples. We run pWGD-cvxNN and pWGD-NN using 64 samples for 100 iterations with step size αl = 10−3, where we set β = 10, γ1 = 0.95, and γ2 = 0.9510 for both methods as in the last example. From Figure 4 we can observe that pWGDcvxNN produces more consistent results than pWGD-NN compared to the reference pSVGD results, for both the sample mean and 90% credible interval, both in the inference of the social distancing parameter and in the prediction of the hospitalized cases.\n\n5 CONCLUSION\n\nIn the context of Bayesian inference, we approximate Wasserstein gradient direction by the gradient of functions in the family of two-layer neural networks. We propose a convex SDP relaxation of the dual of the variational primal problem, which can be solved efficiently using convex optimization methods instead of directly training the neural network as a nonconvex optimization problem. In particular, we established that the gradient obtained by the new formulation and convex optimization is at least as good as the one approximated by functions in the family of two-layer neural networks, which is demonstrated by various numerical experiments. By stacking the two-layer neu-\n\n9\n\n020406080# iterations1.21.00.80.60.40.20.0Log10(RMSE of mean)pWGD-NNpWGD-cvxNN020406080# iterations0.80.60.40.20.00.20.40.6Log10(RMSE of variance)pWGD-NNpWGD-cvxNNUnder review as a conference paper at ICLR 2023\n\nFigure 4: Comparison of pWGD-cvxNN and pWGD-NN to the reference by pSVGD for Bayesian inference of the social distancing parameter (left) from the data of the hospitalized cases (right) with sample mean and 90% credible interval.\n\nral networks in each step together, our proposed method formulate a deep neural network to learn the transportation map from prior to posterior. In future studies, specialized solvers for structured SDPs, including the relaxed dual problem, can lead to drastic accelerations of our proposed method and it is of central importance for the practical applications of our algorithms to real-world problems. We also expect to extend our convex optimization formulation of neural networks to the calculation/approximation of generalized Wasserstein flows. We also expect to apply deep neural networks for the approximation of Wasserstein gradient flows based on recent works on convex optimization formulations of deep neural networks (Wang et al., 2021c; Ergen & Pilanci, 2021a;b).\n\nREFERENCES\n\nDavid Alvarez-Melis, Yair Schiff, and Youssef Mroueh. Optimizing functionals on the space of\n\nprobabilities with input convex neural networks. arXiv preprint arXiv:2106.00774, 2021.\n\nLuigi Ambrosio, Nicola Gigli, and Giuseppe Savar ́e. Gradient flows: in metric spaces and in the\n\nspace of probability measures. Springer Science & Business Media, 2005.\n\nMOSEK ApS. Mosek optimization suite, 2019.\n\nBurak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex optimization of polynomial activation neural networks in fully polynomial-time. arXiv preprint arXiv:2101.02429, 2021a.\n\nBurak Bartan and Mert Pilanci. Training quantized neural networks to global optimality via semidefIn International Conference on Machine Learning, pp. 694–704. PMLR,\n\ninite programming. 2021b.\n\nCl ́ement Bonet, Nicolas Courty, Franc ̧ois Septier, and Lucas Drumetz. Sliced-wasserstein gradient\n\nflows. arXiv preprint arXiv:2110.10972, 2021.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-\n\nversity press, 2004.\n\nCharlotte Bunne, Laetitia Meng-Papaxanthos, Andreas Krause, and Marco Cuturi. Jkonet: Proximal optimal transport modeling of population dynamics. arXiv preprint arXiv:2106.06345, 2021.\n\nJose A Carrillo, Katy Craig, Li Wang, and Chaozhen Wei. Primal dual methods for wasserstein\n\ngradient flows. Foundations of Computational Mathematics, pp. 1–55, 2021a.\n\nJose A Carrillo, Daniel Matthes, and Marie-Therese Wolfram. Lagrangian schemes for wasserstein\n\ngradient flows. Handbook of Numerical Analysis, 22:271–311, 2021b.\n\nPeng Chen and Omar Ghattas. Projected stein variational gradient descent. Advances in Neural\n\nInformation Processing Systems, 33:1947–1958, 2020.\n\n10\n\nMarAprMayJun0.00.20.40.60.81.0NN vs pSVGD social distancingmean_NNmean_pSVGDMarAprMayJuncvxNN vs pSVGD social distancing0.00.20.40.60.81.0mean_cvxNNmean_pSVGDMarAprMayJun05000100001500020000NN vs pSVGD # hospitalizedmean_NNmean_pSVGDMarAprMayJuncvxNN vs pSVGD # hospitalized05000100001500020000mean_cvxNNmean_pSVGDUnder review as a conference paper at ICLR 2023\n\nThomas M Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE transactions on electronic computers, (3):326–334, 1965.\n\nTiangang Cui, Kody JH Law, and Youssef M Marzouk. Dimension-independent likelihood-informed\n\nmcmc. Journal of Computational Physics, 304:109–137, 2016.\n\nGianluca Detommaso, Tiangang Cui, Alessio Spantini, Youssef Marzouk, and Robert Scheichl. A\n\nstein variational newton method. arXiv preprint arXiv:1806.03085, 2018.\n\nLauro Langosco di Langosco, Vincent Fortuin, and Heiko Strathmann. Neural variational gradient\n\ndescent. arXiv preprint arXiv:2107.10731, 2021.\n\nSteven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex\n\noptimization. Journal of Machine Learning Research, 17(83):1–5, 2016.\n\nTolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks via convex programs. In International Conference on Machine Learning, pp. 2993–3003. PMLR, 2021a.\n\nTolga Ergen and Mert Pilanci. Path regularization: A convexity and sparsity inducing regularization\n\nfor parallel relu networks. arXiv preprint arXiv:2110.09548, 2021b.\n\nTolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demystifying batch normalization in relu networks: Equivalent convex optimization models and implicit regularization. arXiv preprint arXiv:2103.01499, 2021.\n\nJiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein gradient flow. arXiv\n\npreprint arXiv:2112.02424, 2021.\n\nXingdong Feng, Yuan Gao, Jian Huang, Yuling Jiao, and Xu Liu. Relative entropy gradient sampler\n\nfor unnormalized distributions. arXiv preprint arXiv:2110.02787, 2021.\n\nCharlie Frogner and Tomaso Poggio. Approximate inference with wasserstein gradient flows. In International Conference on Artificial Intelligence and Statistics, pp. 2581–2590. PMLR, 2020.\n\nKurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are uni-\n\nversal approximators. Neural networks, 2(5):359–366, 1989.\n\nHyung Ju Hwang, Cheolhyeong Kim, Min Sue Park, and Hwijae Son. The deep minimizing move-\n\nment scheme. arXiv preprint arXiv:2109.14851, 2021.\n\nAapo Hyv ̈arinen and Peter Dayan. Estimation of non-normalized statistical models by score match-\n\ning. Journal of Machine Learning Research, 6(4), 2005.\n\nVaithilingam Jeyakumar and GY Li. Trust-region problems with linear inequality constraints: exact sdp relaxation, global optimality and robust optimization. Mathematical Programming, 147(1): 171–206, 2014.\n\nRichard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker–\n\nplanck equation. SIAM journal on mathematical analysis, 29(1):1–17, 1998.\n\nOliver Junge, Daniel Matthes, and Horst Osberger. A fully discrete variational scheme for solving nonlinear fokker–planck equations in multiple space dimensions. SIAM Journal on Numerical Analysis, 55(1):419–443, 2017.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nJakob Kruse, Gianluca Detommaso, Robert Scheichl, and Ullrich K ̈othe. Hint: Hierarchical invertible neural transport for density estimation and bayesian inference. arXiv preprint arXiv:1905.10687, 2019.\n\nJonathan Lacotte, Yifei Wang, and Mert Pilanci. Adaptive newton sketch: Linear-time optimization with quadratic convergence and effective hessian dimensionality. In International Conference on Machine Learning, pp. 5926–5936. PMLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nShiwei Lan, Shuyi Li, and Babak Shahbaba. Scaling up bayesian uncertainty quantification for\n\ninverse problems using deep neural networks. arXiv preprint arXiv:2101.03906, 2021.\n\nAlex Tong Lin, Samy Wu Fung, Wuchen Li, Levon Nurbekyan, and Stanley J Osher. Alternating the population and control neural networks to solve high-dimensional stochastic mean-field games. Proceedings of the National Academy of Sciences, 118(31), 2021a.\n\nAlex Tong Lin, Wuchen Li, Stanley Osher, and Guido Mont ́ufar. Wasserstein proximal of gans.\n\narXiv preprint arXiv:2102.06862, 2021b.\n\nChang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and accelerating particle-based variational inference. In International Conference on Machine Learning, pp. 4082–4092. PMLR, 2019.\n\nQiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference\n\nalgorithm. In Advances in neural information processing systems, pp. 2378–2386, 2016.\n\nAntoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St ̈oter. Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions. In International Conference on Machine Learning, pp. 4104–4113. PMLR, 2019.\n\nZhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neural networks: A view from the width. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6232–6240, 2017.\n\nPetr Mokrov, Alexander Korotin, Lingxiao Li, Aude Genevay, Justin Solomon, and Evgeny Burnaev.\n\nLarge-scale wasserstein gradient flows. arXiv preprint arXiv:2106.00736, 2021.\n\nDerek Onken, Samy Wu Fung, Xingjian Li, and Lars Ruthotto. Ot-flow: Fast and accurate continu-\n\nous normalizing flows via optimal transport. arXiv preprint arXiv:2006.00104, 2020.\n\nFelix Otto. The geometry of dissipative evolution equations: the porous medium equation. Commu-\n\nnications in Partial Differential Equations, 26(1-2):101–174, 2001.\n\nMert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks. In International Conference on Machine Learning, pp. 7695–7705. PMLR, 2020.\n\nMert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm\n\nwith linear-quadratic convergence. SIAM Journal on Optimization, 27(1):205–245, 2017.\n\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-\n\ntional conference on machine learning, pp. 1530–1538. PMLR, 2015.\n\nArda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network problems are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms. arXiv preprint arXiv:2012.13329, 2020.\n\nAndrew M Stuart. Inverse problems: a Bayesian perspective. Acta numerica, 19:451–559, 2010.\n\nC ́edric Villani. Topics in optimal transportation. American Mathematical Soc., 2003.\n\nYifei Wang, Jonathan Lacotte, and Mert Pilanci. The hidden convex optimization landscape of twolayer relu neural networks: an exact characterization of the optimal solutions. arXiv preprint arXiv:2006.05900, 2020.\n\nYifei Wang, Peng Chen, and Wuchen Li.\n\nProjected wasserstein gradient descent for high-\n\ndimensional bayesian inference. arXiv preprint arXiv:2102.06350, 2021a.\n\nYifei Wang, Kangkang Deng, Haoyang Liu, and Zaiwen Wen. A decomposition augmented lagrangian method for low-rank semidefinite programming. arXiv preprint arXiv:2109.11707, 2021b.\n\nYifei Wang, Tolga Ergen, and Mert Pilanci. Parallel deep neural networks have zero duality gap.\n\narXiv preprint arXiv:2110.06482, 2021c.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nYifei Wang, Yixuan Hua, Emmanuel Cand ́es, and Mert Pilanci. Overparameterized relu neural networks learn the simplest models: Neural isometry and exact recovery. arXiv preprint arXiv:2209.15265, 2022.\n\nLiuqin Yang, Defeng Sun, and Kim-Chuan Toh. Sdpnal\n\n+\n\n+: a majorized semismooth newton-cg augmented lagrangian method for semidefinite programming with nonnegative constraints. Mathematical Programming Computation, 7(3):331–366, 2015.\n\nAlp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. Scalable semidefinite programming. SIAM Journal on Mathematics of Data Science, 3(1):171–200, 2021.\n\nOlivier Zahm, Tiangang Cui, Kody Law, Alessio Spantini, and Youssef Marzouk. Certified dimension reduction in nonlinear bayesian inverse problems. arXiv preprint arXiv:1807.03712, 2018.\n\nXin-Yuan Zhao, Defeng Sun, and Kim-Chuan Toh. A newton-cg augmented lagrangian method for\n\nsemidefinite programming. SIAM Journal on Optimization, 20(4):1737–1765, 2010.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA CODES FOR NUMERICAL EXPERIMENT\n\nAll codes for the numerical experiment can be found in https://github.com/ai-submit/ OptimalWasserstein.\n\nB COMPARISON WITH PREVIOUS WORKS ON CONVEX OPTIMIZATION\n\nFORMULATION OF NEURAL NETWORKS\n\nPrevious works on convex optimization formulation of neural networks mainly focus on the supervised learning problem of two-layer neural networks using convex loss functions (e.g., squared loss, logistic loss). Our work utilizes a similar convex analytic framework to solve the variational problem of approximating the Wasserstein gradient direction, which is different from supervised learning. The convex optimization approach is related to the idea of infinite width neural networks modeled as probability measures. The dual problem itself is equivalent to the convex dual problem when the neural network in the primal problem has infinitely many neurons. However, the convex optimization approach tackles networks of arbitrary width that are able to learn useful representations, while the infinite width limit is quite limited (limited to basically kernel methods).\n\nC ADDITIONAL NUMERICAL EXPERIMENT\n\nC.1 PDE-CONSTRAINED LINEAR BAYESIAN INFERENCE\n\nIn this experiment, we consider a linear Bayesian inference problem constrained by a partial differential equation (PDE) model for contaminant diffusion in environmental engineering in domain D = (0, 1),\n\n−κ∆u + νu = x\n\nin D,\n\nwhere x is a contaminant source field parameter in domain D, u is the contaminant concentration which we can observe at some locations, κ and ν are diffusion and reaction coefficients. For simplicity, we set κ, ν = 1, u(0) = u(1) = 0, and consider 15 pointwise observations of u with 1% noise, equidistantly distributed in D. We consider a Gaussian prior distribution x ∼ N (0, C) with covariance given by a differential operator C = (−δ∆+γI)−α with δ, γ, α > 0 representing the correlation length and variance, which is commonly used in geoscience. We set δ = 0.1, γ = 1, α = 1. In this linear setting, the posterior is Gaussian with the mean and covariance given analytically, which are used as reference to assess the sample goodness. We solve this forward model by a finite element method with piece-wise linear elements on a uniform mesh of size 2k, k ≥ 1. We project this high-dimensional parameter to the data-informed low dimensions as in Wang et al. (2021a) to alleviate the curse of dimensionality when applying WGD-cvxNN and WGD-NN, which we call pWGD-cvxNN and pWGD-NN, respectively. For k = 4 we have 17 dimensions for the discrete parameter and 4 dimensions after projection.\n\nWe run pWGD-cvxNN and pWGD-NN using 16 samples for 200 iterations with αl = 10−3, β = 5, γ1 = 0.95, and γ2 = 0.9510 for both methods. We use m = 200 neurons for pWGD-NN and train it by the Adam optimizer for 200 sub-iterations as in the first example. From Figure 5, we observe that pWGD-cvxNN achieves better root mean squared error (RMSE) than pWGD-NN for both the sample mean and the sample variance compared to the reference.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Ten trials and the RMSE of the sample mean (top) and sample variance (bottom) by pWGD-NN and pWGD-cvxNN at different iterations. Linear inference problem.\n\nD CHOICE OF THE REGULARIZATION PARAMETER\n\nAs the constraints in the relaxed dual problem (15) depends on the regularization parameter ̃β, it is possible that for small ̃β, the relaxed dual problem (15) is infeasible. Consider the following SDP\n\nmin ̃β, s.t. ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,−)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0,\n\n− ̃Aj(Λ) − ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,+)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0,\n\n(20)\n\nr(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p].\n\nj=1. Let ̃β1 be the optimal value of the above probHere the variables are ̃β, Λ and {r(j,+), r(j,−)}p lem. Then, only for ̃β ≥ ̃β1, there exists Λ ∈ RN ×d satisfying the constraints in (15). In other words, the relaxed dual problem (15) is feasible. We also note that ̃β1 only depends on the samples X and it does not depend on the value of ∇ log π evaluated on x1, . . . , xN . On the other hand, consider the following SDP\n\nmin ̃β, s.t. ̃Aj(Y ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,−)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0,\n\n− ̃Aj(Y ) − ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,+)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0,\n\n(21)\n\nr(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p],\n\nj=1. Let ̃β2 be the optimal value of the above problem. where the variables are ̃β and {r(j,+), r(j,−)}p For ̃β ≥ ̃β2, as Y is feasible for the constraints in (15), the optimal value of the relaxed dual problem (15) is 0. In short, only when ̃β ∈ [ ̃β1, ̃β2], the variational problem (15) is non-trivial. To ensure that solving the relaxed dual problem (15) gives a good approximation of the Wasserstein gradient direction, we shall avoid choosing ̃β either too small or too large.\n\n15\n\n0255075100125150175200# iterations1.21.00.80.60.40.20.0Log10(RMSE of mean)pWGD-NNpWGD-cvxNN0255075100125150175200# iterations1.00.80.60.40.20.00.20.4Log10(RMSE of variance)pWGD-NNpWGD-cvxNNUnder review as a conference paper at ICLR 2023\n\nE PROOFS\n\nE.1 PROOF OF PROPOSITION 1\n\nPROOF We first note that\n\n(cid:90)\n\n(cid:90)\n\n(cid:107)∇Φ − ∇ log ρ + ∇ log π(cid:107)2\n\n2ρdx\n\n(cid:107)∇Φ(cid:107)2\n\n2ρdx +\n\n(cid:90)\n\n(cid:104)∇ log π − ∇ log ρ, ∇Φ(cid:105) ρdx\n\n(22)\n\n1 2\n1 2\n\n=\n\n+\n\n(cid:90)\n\n1 2\n\n(cid:107)∇ log ρ − ∇ log π(cid:107)2\n\n2ρdx.\n\nWe notice that the term 1 by parts, we can compute that\n\n2\n\n(cid:82) (cid:107)∇ log ρ−∇ log π(cid:107)2\n\n2ρdx does not depend on Φ. Utilizing the integration\n\n(cid:90)\n\n(cid:104)∇ log ρ, ∇Φ(cid:105) ρdx =\n\n(cid:90) (cid:28) ∇ρ ρ\n\n(cid:29)\n\n, ∇Φ\n\nρdx\n\n(cid:90)\n\n=\n\n(cid:104)∇ρ, ∇Φ(cid:105) dx\n\n(cid:90)\n\n= −\n\n∆Φρdx.\n\nTherefore, the variational problem (4) is equivalent to\n\ninf Φ∈C∞(Rd)\n\n(cid:90)\n\n1 2\n\n(cid:107)∇Φ(cid:107)2\n\n2ρdx +\n\n(cid:90)\n\n(cid:104)∇ log π, ∇Φ(cid:105) ρdx +\n\n(cid:90)\n\n∆Φρdx.\n\nBy restricting the domain C∞(Rd) to H, we complete the proof.\n\nE.2 PROOF OF PROPOSITION 2\n\n(23)\n\n(24)\n\nPROOF Suppose that ˆwi = β−1 Let θ(cid:48) = {( ˆwi, ˆαi)}m\n\ni=1. We note that\n\ni wi and ˆαi = β2\n\ni αi, where βi > 0 is a scale parameter for i ∈ [m].\n\nˆαi ˆwiψ(cid:48)( ˆwT\n\ni xn) = βiαiwiψ(cid:48) (cid:0)β−1\n\ni wT\n\ni xn\n\n(cid:1) = αiwiψ(cid:48)(wT\n\ni xn),\n\nand\n\nˆαi(cid:107) ˆwi(cid:107)2\n\n2ψ(cid:48)(cid:48)( ˆwT\n\ni xn) = αi(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)( ˆwT\n\ni xn) = αi(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn).\n\n(25)\n\n(26)\n\nThis implies that Φθ(x) = Φθ(cid:48)(x) and ∇ · Φθ(x) = ∇ · Φθ(cid:48)(x). For the regularization term R(θ), we note that\n\n(cid:107) ˆwi(cid:107)3\n\n2 + (cid:107)ˆαi(cid:107)3\n\n2 =β6\n\ni |αi|3 + β−3\n\n2\n\ni (cid:107)wi(cid:107)3 1\n2\n\ni (cid:107)wi(cid:107)3\n\nβ−3\n\n2 +\n\n=β6\n\ni |αi|3 +\n\n=3 · 2−2/3(cid:107)wi(cid:107)2\n\n2|αi|.\n\n1 2\n\nβ−3\n\ni (cid:107)wi(cid:107)3\n\n2\n\n(27)\n\nThe optimal scaling parameter is given by αi = 2−1/9 (cid:107)wi(cid:107)1/3 |αi|1/3 change (cid:107)wi(cid:107)2 ̃β N\n\ni=1 (cid:107)ui(cid:107)1. This completes the proof.\n\n(cid:80)m\n\n2\n\n1\n\n2|αi|, we can simply let (cid:107)wi(cid:107)2 = 1. Thus, the regularization term β\n\n. As the scaling operation does not\n\n2 R(θ) becomes\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nE.3 PROOF OF PROPOSITION 3\n\nPROOF Consider the Lagrangian function\n\nL(Z, W, α, Λ) =\n\n1 2\n\n(cid:107)Z(cid:107)2\n\nF +\n\nN (cid:88)\n\nm (cid:88)\n\nαi(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) + tr(Y T Z) + ̃β(cid:107)α(cid:107)1\n\ni=1\n\nn=1 (cid:32)\n\nλT\n\nn\n\nzn −\n\n+\n\nN (cid:88)\n\nn=1\n\n(cid:33)\n\nαiwiψ(cid:48)(xT\n\nn wi)\n\nm (cid:88)\n\ni=1\n\n(28)\n\n= ̃β(cid:107)α(cid:107)1 +\n\nm (cid:88)\n\nαi\n\nN (cid:88)\n\ni=1\n\nn=1\n\n(cid:0)(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) − λT\n\nn wiψ(cid:48)(xT\n\nn wi)(cid:1)\n\n+\n\n1 2\n\n(cid:107)Z(cid:107)2\n\nF + tr((Y + Λ)T Z).\n\nFor fixed W , the constraints on Z and α are linear and the strong duality holds. Thus, we can exchange the order of minZ,α and maxΛ. Thus, we can compute that\n\nmin Z,W,α\n\nmax Λ\n\nL(Z, W, α, Λ)\n\n= min\n\nW\n\nmax Λ\n\nmin α,Z\n\nL(Z, W, α, Λ)\n\n= min\n\nW\n\nmax Λ\n\nmin α,Z\n\n ̃β(cid:107)α(cid:107)1 +\n\nm (cid:88)\n\ni=1\n\nN (cid:88)\n\nαi\n\n(cid:0)(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) − λT\n\nn wiψ(cid:48)(xT\n\nn wi)(cid:1) +\n\n1 2\n\n(cid:107)Z(cid:107)2\n\nF + tr((Y + Λ)T Z)\n\n= min\n\nW\n\nmax Λ\n\n−\n\n1 2\n\n(cid:107)Λ + Y (cid:107)2\n\nF +\n\nI\n\nmax wi:(cid:107)wi(cid:107)2≤1\n\nn=1 (cid:32)\n\nm (cid:88)\n\ni=1\n\nN (cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) n=1\n\n(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) − yT\n\nn wiψ(cid:48)(xT\n\n(cid:12) (cid:12) (cid:12) n wi) (cid:12) (cid:12)\n\n(cid:33)\n\n≤ ̃β\n\n.\n\n(29)\n\nBy exchanging the order of min and max, we can derive the dual problem:\n\nmax Λ\n\nmin W\n\n−\n\n1 2\n\n(cid:107)Λ + Y (cid:107)2\n\nF +\n\nm (cid:88)\n\nI\n\ni=1\n\n(cid:32)\n\nmax wi:(cid:107)wi(cid:107)2≤1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nN (cid:88)\n\nn=1\n\n(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) − yT\n\nn wiψ(cid:48)(xT\n\n(cid:12) (cid:12) (cid:12) n wi) (cid:12) (cid:12)\n\n(cid:33)\n\n≤ ̃β\n\n= max\n\nΛ\n\n−\n\n= max\n\nΛ\n\n−\n\n1 2\n\n1 2\n\n(cid:107)Λ + Y (cid:107)2\n\nF s.t.\n\nmax wi:(cid:107)wi(cid:107)2≤1\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nN (cid:88)\n\nn=1\n\n(cid:107)wi(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT\n\ni xn) − yT\n\nn wiψ(cid:48)(xT\n\n(cid:12) (cid:12) (cid:12) n wi) (cid:12) (cid:12)\n\n≤ ̃β, i ∈ [m]\n\n(cid:107)Λ + Y (cid:107)2\n\nF s.t. max\n\nw:(cid:107)w(cid:107)2≤1\n\nN (cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) n=1\n\n(cid:107)w(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT xn) − yT\n\nn wψ(cid:48)(xT\n\n(cid:12) (cid:12) (cid:12) n w) (cid:12) (cid:12)\n\n≤ ̃β, i ∈ [m]\n\n(30)\n\nThis completes the proof.\n\nE.4 PROOF OF PROPOSITION 4\n\nPROOF Based on the hyper-plane arrangements D1, . . . , Dp, the dual constraint is equivalent to that for all j ∈ [p],\n\n(cid:12) (cid:12)2 tr(Dj)(cid:107)w(cid:107)2\n\n2 − 2wT ΛT DjXw(cid:12)\n\n(cid:12) ≤ ̃β\n\n(31)\n\nholds for all w ∈ Rd satisfying (cid:107)w(cid:107)2 ≤ 1, (2Dj − I)Xw ≥ 0. This is equivalent to say that for all j ∈ [p]\n\n− ̃β ≥ min 2 tr(Dj)(cid:107)w(cid:107)2\n\n2 − 2wT ΛT DjXw,\n\ns.t. (cid:107)w(cid:107)2 ≤ 1, 2(Dj − I)Xw ≥ 0,\n\n ̃β ≤ max 2 tr(Dj)(cid:107)w(cid:107)2\n\n2 − 2wT ΛT DjXw,\n\ns.t. (cid:107)w(cid:107)2 ≤ 1, 2(Dj − I)Xw ≥ 0.\n\n17\n\n(32)\n\nUnder review as a conference paper at ICLR 2023\n\nFrom a convex optimization perspective, the natural idea to interpret the constraint (32) is to transform the minimization problem into a maximization problem. We can rewrite the minimization problem in (32) as a trust region problem with inequality constraints:\n\nmin w∈Rd\n\nwT (Bj + Aj(Λ)) w,\n\ns.t. (cid:107)w(cid:107)2 ≤ 1, (2Dj − I)Xw ≥ 0.\n\n(33)\n\nAs the problem (33) is a convex problem, by taking the dual of (33) w.r.t. w, we can transform (33) into a maximization problem. However, as (33) is a trust region problem with inequality constraints, the dual problem of (33) can be very complicated. According to (Jeyakumar & Li, 2014), the optimal value of the problem (33) is bounded by the optimal value of the following SDP\n\nmin Z∈Sd+1\n\ntr(( ̃Aj(Λ) + ̃Bj)Z),\n\ns.t. tr(H (j)\n\nn Z) ≤ 0, n = 0, . . . , N,\n\nZd+1,d+1 = 1, Z (cid:23) 0.\n\nfrom below.\n\nLemma 1 The dual problem of SDP (34) takes the form\n\nmax −γ, s.t. S = ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nrnH (j)\n\nn + γed+1eT\n\nd+1, r ≥ 0, S (cid:23) 0,\n\nin variables r =\n\n\n\n\n\n ∈ RN +1 and γ ∈ R.\n\n\n\n \n\nr0 ... rN\n\nPROOF Consider the Lagrangian\n\nL(Z, r, γ) = tr(( ̃Aj(y) + ̃Bj)Z) +\n\nN (cid:88)\n\nn=0\n\nrn tr(H (j)\n\nn Z) + γ(tr(Zed+1eT\n\nd+1) − 1),\n\n(34)\n\n(35)\n\n(36)\n\nwhere r ∈ RN +1 (35).\n\n+\n\nand γ ∈ R. By minimizing L(Z, r, γ) w.r.t. Z ∈ Sd+1\n\n+ , we derive the dual problem\n\nThe constraints on Λ in the dual problem (13) include that the optimal value of (34) is bounded from below by − ̃β. According to Lemma 1, this constraint is equivalent to that there exist r ∈ RN +1 and γ such that\n\n−γ ≥ − ̃β, S = ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nrnH (j)\n\nn + γed+1eT\n\nd+1, r ≥ 0, S (cid:23) 0.\n\n(37)\n\nAs ed+1eT r ∈ RN +1 such that\n\nd+1 is positive semi-definite, the above condition on Λ is also equivalent to that there exist\n\n ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nTherefore, the following convex set of Λ\n\nrnH (j)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, r ≥ 0.\n\n(cid:110)\n\nΛ : ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,−)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, r(j,−) ≥ 0\n\n(cid:111)\n\nis a subset of the set of Λ satisfying the dual constraints\n\n(cid:26)\n\nΛ :\n\nmin (cid:107)w(cid:107)2≤1,(2Dj −I)w≥0\n\nwT (Bj + Aj(Λ)) w ≥ − ̃β\n\n(cid:27)\n\n18\n\n(38)\n\n(39)\n\n(40)\n\nUnder review as a conference paper at ICLR 2023\n\nOn the other hand, the constraint on Λ\n\nis equivalent to\n\nmax (cid:107)w(cid:107)2≤1,(2Dj −I)w≥0\n\nwT (Bj + Aj(Λ)) w ≤ ̃β\n\nmin (cid:107)w(cid:107)2≤1,(2Dj −I)w≥0\n\n−wT (Bj + Aj(Λ)) w ≥ − ̃β.\n\n(41)\n\n(42)\n\nBy applying the previous analysis on the above trust region problem, the following convex set of Λ\n\n(cid:110)\n\nΛ : − ̃Aj(Λ) − ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,+)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, r(j,+) ≥ 0\n\nis a subset of the set of Λ satisfying the dual constraints\n\n(cid:26)\n\nΛ :\n\nmax (cid:107)w(cid:107)2≤1,(2Dj −I)w≥0\n\nwT (Bj + Aj(Λ)) w ≤ ̃β\n\n(cid:27)\n\n.\n\n(cid:111)\n\n(43)\n\n(44)\n\nTherefore, replacing the dual constraint maxw:(cid:107)w(cid:107)2≤1 ̃β by\n\n(cid:12) (cid:12) (cid:12)\n\n(cid:80)N\n\nn=1 (cid:107)w(cid:107)2\n\n2ψ(cid:48)(cid:48)(wT xn) − yT\n\nn wψ(cid:48)(xT\n\n(cid:12) (cid:12) (cid:12) ≤ n w)\n\n ̃Aj(Λ) + ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,−)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, j ∈ [p],\n\n− ̃Aj(Λ) − ̃Bj +\n\nN (cid:88)\n\nn=0\n\nn H (j) r(j,+)\n\nn + ̃βed+1eT\n\nd+1 (cid:23) 0, j ∈ [p],\n\n(45)\n\nr(j,−) ≥ 0, r(j,+) ≥ 0, j ∈ [p].\n\nwe obtain the relaxed dual problem. As its feasible domain is a subset of the feasible domain of the dual problem, the optimal value of the relaxed dual problem gives a lower bound for the optimal value of the dual problem.\n\nE.5 PROOF OF PROPOSITION 5\n\nPROOF Consider the Lagrangian function\n\nL(Λ, r, S) = −\n\n1 2\n\n(cid:107)Λ + Y (cid:107)2\n\n2 −\n\n(cid:32)\n\n(cid:32)\n\ntr\n\nS(j,−)\n\n ̃Aj(Λ) + ̃Bj +\n\nn H (j) r(j,−)\n\nn +\n\n(cid:33)(cid:33)\n\n ̃β 2\n\ned+1eT\n\nd+1\n\nN (cid:88)\n\nn=0\n\np (cid:88)\n\nj=1 (cid:32)\n\n(cid:32)\n\ntr\n\nS(j,+)\n\n− ̃Aj(Λ) − ̃Bj +\n\nn H (j) r(j,+)\n\nn +\n\n(cid:33)(cid:33)\n\ned+1eT\n\nd+1\n\n,\n\n ̃β 2\n\nN (cid:88)\n\nn=0\n\n−\n\np (cid:88)\n\nj=1\n\nwhere we write\n\n(cid:16)\n\n(cid:16)\n\nr =\n\nS =\n\nr(1,−), . . . , r(p,−), r(1,+), . . . , r(p,+)(cid:17) S(1,−), . . . , S(p,−), S(1,+), . . . , S(p,+)(cid:17)\n\n∈ (cid:0)RN +1(cid:1)2p\n\n,\n\n∈ (cid:0)Sd+1\n\n+\n\n(cid:1)2p\n\n.\n\n(46)\n\n(47)\n\nHere we write Sd+1 problem (16).\n\n+ = {S ∈ Sd+1|S (cid:23) 0}. By maximizing w.r.t. Λ and r, we derive the bi-dual\n\nE.6 PROOF OF THEOREM 1\n\nSuppose that (Z, W, α) is a feasible solution to (11). Let Dj1, . . . , Djk be the enumeration of {diag(I(Xwi ≥ 0))|i ∈ [m]}. For i ∈ [k], we let\n\nS(ji,+) =\n\n(cid:88)\n\nl:αl≥0,diag(I(Xwl≥0))=Dji\n\nαl\n\n(cid:21)\n\n(cid:20)wlwT wT l\n\nl wl 1\n\n, S(ji,−) = 0,\n\n(48)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\nS(ji,+) = 0, S(ji,−) = −\n\n(cid:88)\n\nl:αl<0,diag(I(Xwl≥0))=Dji\n\nαl\n\n(cid:21)\n\n(cid:20)wlwT wT l\n\nl wl 1\n\n.\n\n(49)\n\nFor j /∈ {j1, . . . , jk}, we simply set S(j,+) = 0, S(j,−) = 0. As (cid:107)wi(cid:107)2 ≤ 1 and Dji = I(Xwi ≥ 0), we can verify that tr(S(j,−)H (j) n ) ≤ 0 are satisfied for j = j1, . . . , jm and n = 0, 1, . . . , N . This is because for n = 0, as H (ji)\n\nn ) ≤ 0, tr(S(j,+)H (j)\n\n, it follows that\n\n(cid:20)Id\n\n(cid:21)\n\n0 =\n\n0 0 −1\n\ntr(S(ji,+)H (ji)\n\n0\n\n) =\n\n(cid:88)\n\nαl((cid:107)wl(cid:107)2 − 1) ≤ 0,\n\nl:αl≥0,diag(I(Xwl≥0))=Dji\n\ntr(S(ji,−)H (ji)\n\n0\n\n) = −\n\n(cid:88)\n\nαl((cid:107)wl(cid:107)2 − 1) ≤ 0.\n\nl:αl<0,diag(I(Xwl≥0))=Dji\n\nFor n = 1, . . . , N , we have\n\ntr(S(ji,+)H (ji)\n\n0\n\n) =\n\n(cid:88)\n\n2αl(1 − 2(Dji)nn)xT\n\nn wl ≤ 0,\n\nl:αl≥0,diag(I(Xwl≥0))=Dji\n\ntr(S(ji,−)H (ji)\n\n0\n\n) = −\n\n(cid:88)\n\nαl(1 − 2(Dji)nn)xT\n\nn wl ≤ 0.\n\nl:αl<0,diag(I(Xwl≥0))=Dji\n\n(50)\n\n(51)\n\nBased on the above transformation, we can rewrite the bidual problem in the form of the primal problem (12). For S ∈ Sd+1, we note that\n\ntr(S ̃Aj(Λ))\n\n= − tr((ΛT DjX + X T DjΛ)S1:d,1:d) = − 2 tr(ΛT DjXS1:d,1:d),\n\nwhere S1:d,1:d denotes the d × d block of S consisting the first d rows and columns. This implies that ̃A∗\n\nj (S) = −2DjXS1:d,1:d. Hence, we have\n\n ̃Aji (S(ji,+) − S(ji,−)) = −\n\n(cid:88)\n\n2αlDjiXwlwT\n\nl = −\n\n(cid:88)\n\n2αl(Xwl)+wT l .\n\nl:diag(I(Xwl≥0)\n\nl:diag(I(Xwl≥0)\n\nTherefore, we have\n\np (cid:88)\n\nj=1\n\n ̃A∗\n\nj (S(j,−) − S(j,+)) = 2\n\nm (cid:88)\n\ni=1\n\nαi(Xwi)+wT i .\n\nAs n-th row of Z satisfies that zn = 2 (cid:80)m\n\ni=1 αiwi(xT\n\nn wi)+, this implies that\n\nZ = 2\n\nm (cid:88)\n\ni=1\n\nαi(Xwi)+wT\n\ni =\n\np (cid:88)\n\nj=1\n\n ̃A∗\n\nj (S(j,−) − S(j,+)).\n\nHence (Z, {(S(j,−), (S(j,−)}p\n\nj=1) is feasible to the relaxed bi-dual problem (16).\n\nWe can also compute that\n\np (cid:88)\n\nj=1\n\ntr( ̃Bj(S(j,+) − S(j,−))) = 2\n\nm (cid:88)\n\nαi\n\nN (cid:88)\n\ni=1\n\nn=1\n\nI(xT\n\nn wi ≥ 0)(cid:107)wi(cid:107)2 2,\n\nand\n\n(cid:16)\n\ntr\n\np (cid:88)\n\nj=1\n\n(S(j,+) + S(j,−))ed+1eT\n\nd+1\n\n(cid:17)\n\n=\n\nm (cid:88)\n\ni=1\n\n|αi|.\n\nthe primal problem (12) with (Z, W, α) and the relaxed bi-dual problem (16) with\n\nThus, (Z, {(S(j,−), (S(j,−)}p\n\nj=1) have the same objective value.\n\n20",
    "reference": "# Summary Of The Paper\n\nThis paper uses an SDP relaxation for the problem of computing the Wasserstein gradient. They use numerical algorithms that make their proposed method suitable for practical scenarios involving Bayesian inference.\n\n# Strength And Weaknesses\n\nStrength\n======\n\n- The idea of using a convex SDP relaxation of the dual of the variational primal problem is interesting to understand the behaviour of a particular family of two-layer neural nets.\n\nWeaknesses\n=========\n\n- Using convex SDP relaxations is not a new idea per se: see, e.g., \"Semidefinite Relaxation of Quadratic Optimization Problems\" by Tom Luo *et al*.\n\nFeedback\n=======\n\n- This sentence: \"However, due to the nonlinear and nonconvex structure of neural networks, optimization algorithms\nincluding **stochastic gradient descent may not find the global optima** of the training problem\" is presented as if, in general, not finding the global optima is an issue in SGD. In training neural nets you precisely do not want to train till the global optima as that would likely mean **overfitting**.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper reads well and code is provided for reproducibility. The paper's novelty is limited as it applies well-known ideas to a somewhat specific (as in, not generalizable) problem.\n\n# Summary Of The Review\n\nOverall the paper uses a straightforward technique to a problem that has limited practical applications (learning two-layers neural nets).\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nBASIC BINARY CONVOLUTION UNIT FOR BINARIZED IMAGE RESTORATION NETWORK\n\nBin Xia1, Yulun Zhang2, Yitong Wang3, Yapeng Tian4, Wenming Yang1∗, Radu Timofte5, and Luc Van Gool2 1Tsinghua University 2ETH Z ̈urich 3ByteDance Inc 4University of Texas at Dallas 5University of W ̈urzburg\n\nABSTRACT\n\nLighter and faster image restoration (IR) models are crucial for the deployment on resource-limited devices. Binary neural network (BNN), one of the most promising model compression methods, can dramatically reduce the computations and parameters of full-precision convolutional neural networks (CNN). However, there are different properties between BNN and full-precision CNN, and we can hardly use the experience of designing CNN to develop BNN. In this study, we reconsider components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. We conduct systematic analyses to explain each component’s role in binary convolution and discuss the pitfalls. Specifically, we find that residual connection can reduce the information loss caused by binarization; BatchNorm can solve the value range gap between residual connection and binary convolution; The position of the activation function dramatically affects the performance of BNN. Based on our findings and analyses, we design a simple yet efficient basic binary convolution unit (BBCU). Furthermore, we divide IR networks into four parts and specially design variants of BBCU for each part to explore the benefit of binarizing these parts. We conduct experiments on different IR tasks, and our BBCU significantly outperforms other BNNs and lightweight models, which shows that BBCU can serve as a basic unit for binarized IR networks. The code is available at https://github.com/Zj-BinXia/BBCU\n\n1\n\nINTRODUCTION\n\nImage restoration (IR) aims to restore a high-quality (HQ) image from its low-quality (LQ) counterpart corrupted by various degradation factors. Typical IR tasks include image denoising, superresolution (SR), and compression artifacts reduction. Due to its ill-posed nature and high practical values, image restoration is an active yet challenging research topic in computer vision. Recently, the deep convolutional neural network (CNN) has achieved excellent performance by learning a mapping from LQ to HQ image patches for image restoration (Chen & Pock, 2016; Zhang et al., 2018a; Tai et al., 2017; Xia et al., 2023). However, most IR tasks require dense pixel prediction and the powerful performance of CNN-based models usually relies on increasing model size and computational complexity. That requires extensive computing and memory resources. While, most hand-held devices and small drones are not equipped with GPUs and enough memory to store and run the computationally expensive CNN models. Thus, it is quite essential to largely reduce its computation and memory cost while preserving model performance to promote IR models.\n\nBinary neural network (Courbariaux et al., 2016) (BNN, also known as 1-bit CNN) has been recognized as one of the most promising neural network compression methods (He et al., 2017; Jacob et al., 2018; Zoph & Le, 2016) for deploying models onto resource-limited devices. BNN could achieve 32× memory compression ratio and up to 64× computational reductions on specially designed processors (Rastegari et al., 2016). Nowadays, the researches of BNN mainly concentrate on high-level tasks, especially classification (Liu et al., 2018; 2020), but do not fully explored in lowlevel vision, like image denoising. Considering the great significance of BNN for the deployment of IR deep networks and the difference between high-level and low-level vision tasks, there is an\n\n∗Corresponding Author: Wenming Yang, yang.wenming@sz.tsinghua.edu.cn\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(a) Super-Resolution. Take SRResNet as backbone and test on 4× Urban100. Figure 1: Our BBCU achieves the SOTA performance on IR tasks with efficient computation.\n\n(b) Denoising. Take DnCNN as backbone and test on σ = 15 Urban100.\n\n(c) Deblocking.Take DnCNN3 as backbone and test on q = 10 Classic5.\n\nurgent need to explore the property of BNN on low-level vision tasks and provide a simple, strong, universal, and extensible baseline for latter researches and deployment.\n\nRecently, there have been several works exploring the application of BNN on image SR networks. Specifically, Ma et al. (Ma et al., 2019) tried to binarize the convolution kernel weight to decrease the SR network model size. But, the computational complexity is still high due to the preservation of full-precision activations. Then BAM (Xin et al., 2020) adopted the bit accumulation mechanism to approximate the full-precision convolution for the SR network. Zhang et al. (Zhang et al., 2021b) designed a compact uniform prior to constrain the convolution weight into a highly narrow range centered at zero for better optimization. BTM (Jiang et al., 2021) further introduced knowledge distillation (Hinton et al., 2015) to boost the performance of binarized SR networks.\n\nHowever, the above-mentioned binarized SR networks can hardly achieve the potential of BNN. In this work, we explore the properties of three key components of BNN, including residual connection (He et al., 2016), BatchNorm (BN) (Ioffe & Szegedy, 2015), and activation function (Glorot et al., 2011) and design a strong basic binary Conv unit (BBCU) based on our analyses.\n\n(1) For the IR tasks, we observe that residual connection is quite important for binarized IR networks. That is because that BNN will binarize the input full-precision activations to 1 or -1 before binary convolution (BC). It means that BNN would lose a large amount of information about the value range of activations. By adding the full-precision residual connection for each binary convolution (BC), BNN can reduce the effect of value range information loss.\n\n(2) Then, we explore the BN for BBCU. BNN methods (Liu et al., 2020) for classification always adopt a BN in BC. However, in IR tasks, EDSR (Lim et al., 2017) has demonstrated that BN is harmful to SR performance. We find that BN in BNN for IR tasks is useful and can be used to balance the value range of residual connection and BC. Specifically, as shown in Fig. 2 (b), the values of the full-precision residual connection are mostly in the range of - 1 to 1 because the value range of input images is around 0 to 1 or -1 to 1, while the values of the BC without BN are large and ranges from -15 to 15 for the bit counting operation (Fig. ??). The value range of BC is larger than that of the residual connection, which covers the preserved full-precision image information in the residual connection and limits the performance of BNN. In Fig. 2 (a), the BN in BNN for image restoration can realize the value range alignment of the residual connection and BC.\n\n(3) Based on these findings, to remove BN, we propose a residual alignment (RA) scheme by multiplying the input image by an amplification factor k to increase the value range of the residual connection rather than using BN to narrow the value range of the BC (Fig. 2 (c)). Using this scheme can improve the performance of binarized IR networks and simplify the BNN structure (Sec. 4.5).\n\n(4) In Fig. 2 (d), different from BNNs (Liu et al., 2020; 2018) for classification, we further move the activation function into the residual connection and can improve performance (Sec. 4.5). That is because activation function would narrow the negative value ranges of residual connection. Its information would be covered by the next BC with large negative value ranges (Fig. 2 (c)).\n\n(5) Furthermore, we divide IR networks into four parts: head, body, upsampling, and tail (Fig. 3 (a)). These four parts have different input and output channel numbers. Previous binarized SR networks (Xin et al., 2020; Jiang et al., 2021) merely binarize the body part. However, upsampling part accounts for 52.3% total calculations and needs to be binarized. Besides, the binarized head and tail parts are also worth exploring. Thus, we design different variants of the BBCU to binarize these four parts (Fig. 3 (b)). Overall, our contributions can be mainly summarized in threefold:\n\n2\n\n1020304050607080Number of OPs (G)25.025.125.225.325.425.525.6PSNR (dB)BAMIRNetBTMReActNetBBCU-L (Ours)BBCU-M (Ours)0.760.770.780.790.800.810.820.830.84Number of OPs (G)30.531.031.532.0PSNR (dB)Bi-RealIRNetBTMReActNetBBCU (Ours)0.630.640.650.660.670.680.69Number of OPs (G)29.629.729.829.930.0PSNR (dB)Bi-RealIRNetBTMReActNetBBCU (Ours)Published as a conference paper at ICLR 2023\n\n• We believe our work is timely. The high computational and memory cost of IR networks hinder their application on resource-limited devices. BNN, as one of the most promising compression methods, can help IR networks to solve this dilemma. Since the BNN-based networks have different properties from full-precision CNN networks, we reconsider, analyze, and visualize some essential components of BNN to explore their functions.\n\n• According to our findings and analyses on BNN, we specially develop a simple, strong, universal, and extensible basic binary Conv unit (BBCU) for IR networks. Furthermore, we develop variants of BBCU and adapt it to different parts of IR networks.\n\n• Extensive experiments on different IR tasks show that BBCU can outperform the SOTA BNN methods (Fig. 1). BBCU can serve as a strong basic binary convolution unit for future binarized IR networks, which is meaningful to academic research and industry.\n\n2 RELATED WORK\n\n2.1\n\nIMAGE RESTORATION\n\nAs pioneer works, SRCNN (Dong et al., 2015b), DnCNN (Zhang et al., 2017), and ARCNN (Dong et al., 2015a) use the compact networks for image super-resolution, image denoising, and compression artifacts reduction, respectively. Since then, researchers had carried out its study with different perspectives and obtained more elaborate neural network architecture designs and learning strategies, such as residual block (Kim et al., 2016; Zhang et al., 2021a; Cavigelli et al., 2017), dense block (Zhang et al., 2018c; 2020; Wang et al., 2018), attention mechanism (Zhang et al., 2018b; Xia et al., 2022a; Dai et al., 2019), GAN (Gulrajani et al., 2017; Wang et al., 2018), and others (Wei et al., 2021; Peng et al., 2019; Jia et al., 2019; Fu et al., 2019; Kim et al., 2019; Fu et al., 2021; Soh et al., 2020; Xia et al., 2022d;c;b), to improve model representation ability. However, these IR networks require high computational and memory costs, which hinders practical application on edge devices. To address this issue, we explore and design a BBCU for binarized IR networks.\n\n2.2 BINARY NEURAL NETWORKS\n\nBinary neural network (BNN) is the most extreme form of model quantization as it quantizes convolution weights and activations to only 1 bit, achieving great speed-up compared with its full-precision counterpart. As a pioneer work, BNN (Courbariaux et al., 2016) directly applied binarization to a full-precision model with a pre-defined binarization function. Afterward, XNOR-Net (Rastegari et al., 2016) adopted a gain term to compensate for lost information to improve the performance of BNN (Courbariaux et al., 2016). After that, Bi-Real (Liu et al., 2018) introduced residual connection to preserve full-precision information in forward propagation. IRNet (Qin et al., 2020) developed a scheme to retain the information in forward and backward propagations. Recently, ReActNet (Liu et al., 2020) proposed generalized activation functions to learn more representative features. For super-resolution task, Ma et al. (2019) binarized convolution weights to save model size. However, it can hardly speed up inference enough for retaining full-precision activations. Then, Xin et al. (2020) further binarized activations and used the bit accumulation mechanism to approximate the full-precision convolution for SR networks. Besides, Zhang et al. (2021b) introduced a compact uniform prior for better optimization. Subsequently, Jiang et al. (2021) designed a binary training mechanism by adjusting the feature distribution. In this paper, we reconsider the function of each basic component in BC and develop a strong, simple, and efficient BBCU.\n\n3 PROPOSED METHOD\n\n3.1 BASIC BINARY CONV UNIT DESIGN\n\nj (X f\n\nj ⊗ W f\n\nAs shown in Fig. 2(a), we first construct the BBCU-V1. Specifically, the full-precision convolution X f j , W f j , and ⊗ are full-precision activations, weights, and Conv respectively) is approximated by the binary convolution X b j . For binary convolution, both weights and activations are binarized to -1 and +1. Efficient bitwise XNOR and bit counting operations can replace computationally heavy floating-point matrix multiplication, which can be defined as:\n\nj ⊗ W b\n\nxb\n\ni,j = Sign\n\n(cid:16)\n\nxf\n\ni,j\n\n(cid:17)\n\n=\n\nX b\n\nj ⊗ W b (cid:40)\n\nj = bitcount (cid:0)XNOR (cid:0)X b +1, if xf −1, if xf\n\ni,j > αi,j i,j ≤ αi,j\n\n, xf\n\nj , W b\n\nj\n\n(cid:1)(cid:1) ,\n\ni,j ∈ X f\n\nj , xb\n\ni,j ∈ X b\n\nj , i ∈ [0, C),\n\n(1)\n\n(2)\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: The illustration of the improvement process of our BBCU. (a) The initial BBCU design. (b) We remove BatchNorm to explore its actual function in IR tasks. We find that BatchNorm is essential because it can balance the value range gap between residual connection and binary convolution. (c) We further propose the residual alignment (RA) by multiplying an amplification factor k on the input image to address the value range gap. (d) Based on BBCU-V3, we move the activation function into the residual connection to reduce the negative value information loss.\n\n(cid:13) (cid:13) (cid:13)1\n\n(cid:13) (cid:13) (cid:13)W f n\n\nj\n\nwb\n\ni,j =\n\nSign\n\n(cid:17)\n\n(cid:16)\n\nwf\n\ni,j\n\n=\n\n \n\n\n\n+\n\n−\n\nj ∥1\n\nj ∥1\n\n∥W f n\n∥W f n\n\n, if wf\n\ni,j > 0\n\n, if wf\n\ni,j ≤ 0\n\n, wf\n\ni,j ∈ W f\n\nj , wb\n\ni,j ∈ W b\n\nj , i ∈ [0, C),\n\nj ∈ RC×H×W and W f\n\nwhere X f tion weights in j-th layer, respectively. Similarly, X b are binarized activations and convolution weights in j-th layer separately. xf are the elements of i-th channel of X f cient controlling the threshold of sign function for i-th channel of X f\n\n(3) j ∈ RCout×Cin×Kh×Kw are full-precision activations and convoluj ∈ RCout×Cin×Kh×Kw i,j, and wb\n\nj respectively. αi,j is the learnable coeffij . It is notable that the weight\n\nj ∈ RC×H×W and W b\n\nj , and W b\n\ni,j, wf\n\nj , W f\n\ni,j, xb\n\nj , X b\n\ni,j\n\nis the binarization method is inherited from XONR-Net (Rastegari et al., 2016), of which average of absolute weight values and acts as a scaling factor to minimize the difference between binary and full-precision convolution weights.\n\nj ∥1\n\n∥W f n\n\nThen, we use the RPReLU (Liu et al., 2020) as our activation function, which is defined as follows:\n\nf (yi,j) =\n\n(cid:26) yi,j − γi,j + ζi,j, if yi,j > γi,j\n\nβi,j (yi,j − γi,j) + ζi,j, if yi,j ≤ γi,j\n\n, yi,j ∈ Yj, i ∈ [0, C),\n\n(4)\n\nwhere Yj ∈ RC×H×W is the input feature maps of RPReLU function f (.) in j-th layer. yi,j is the element of i-th channel of Yj. γi,j and ζi,j are learnable shifts for moving the distribution. βi,j is a learnable coefficient controlling the slope of the negative part, which acts on i-th channel of Yj.\n\nDifferent from the common residual block, which consists of two convolutions used in full-precision IR network (Lim et al., 2017), we find that residual connection is essential for binary convolution to supplement the information loss caused by binarization. Thus, we set a residual connection for each binary convolution. Therefore, the BBCU-V1 can be expressed mathematically as: j ⊗ W b\n\nj+1 = f (cid:0)BatchNorm (cid:0)X b\n\n(cid:1) + τj + X f\n\n(cid:1) = f (cid:0)κj\n\nj ⊗ W b\n\n(cid:1) + X f\n\n(cid:0)X b\n\nX f\n\n(cid:1) ,\n\n(5)\n\nj\n\nj\n\nj\n\nj\n\nwhere κj, τj ∈ RC are learnable parameters of BatchNorm in j-th layer.\n\n4\n\nSignRPReLUBatchNormBinary Conv×kRARA÷k...(a)BasicBinaryConvUnitV1(BBCU-V1)(b)BasicBinaryConvUnitV2(BBCU-V2)(c)BasicBinaryConvUnitV3(BBCU-V3)SignRPReLUSignRPReLUBinary ConvBinary Conv...×kRARA÷k...(d)BasicBinaryConvUnitV4(BBCU-V4,Adopted)SignRPReLUBinary Conv...Published as a conference paper at ICLR 2023\n\nIn BNNs, the derivative of the sign function in Eq. 2 is an impulse function that cannot be utilized in training. Thus, we adopt the approximated derivative function as the derivative of the sign function. It can be expressed mathematically as:\n\nApprox\n\n\n\n\n\n∂ Sign\n\n(cid:16)\n\nxf\n\ni\n\n∂xf\n\ni\n\n(cid:17)\n\n\n\n =\n\n \n\n\n\n2 + 2\n\n2 − 2\n\n0\n\n(cid:16)\n\n(cid:16)\n\nxf\n\ni − αi\n\nxf\n\ni − αi\n\n(cid:17)\n\n(cid:17)\n\nif αi − 1 ⩽ xf if αi ⩽ xf\n\ni < αi\n\ni < αi + 1 otherwise\n\n.\n\n(6)\n\nHowever, EDSR (Lim et al., 2017) demonstrated that BN changes the distribution of images, which is harmful to accurate pixel prediction in SR. So, can we also directly remove the BN in BBCUV1 to obtain BBCU-V2 (Fig. 2 (b))? In BBCU-V1 (Fig. 2 (a)), we can see that the bit counting operation of binary convolution tends to output large value ranges (from -15 to 15). In contrast, residual connection preserves the full-precision information, which flows from the front end of IR network with a small value range from around -1 to 1. By adding a BN, its learnable parameters can narrow the value range of binary convolution and make it close to the value range of residual connection to avoid full-precision information being covered. The process can be expressed as:\n\n(cid:1) + τj (7) where κj, τj ∈ RC are learnable parameters of BN in j-th layer. Thus, compared with BBCU-V1, BBCU-V2 simply removes BN and suffers a huge performance drop.\n\n(cid:1) → Mean (cid:0)(cid:12)\n\nMean (cid:0)(cid:12)\n\nj ⊗ W b\n\n(cid:0)X b\n\n(cid:12)X f\n\n(cid:12)κj\n\n(cid:1) ,\n\n(cid:12) (cid:12)\n\n(cid:12) (cid:12)\n\nj\n\nj\n\nAfter the above exploration, we know that BN is essential for BBCU, because it can balance the value range of binary convolution and residual connection. However, BN changes image distributions limiting restoration. In BBCU-V3, we propose residual alignment (RA) scheme by multiplying the value range of input image by an amplification factor k (k > 1) to remove BN Figs. 2(c) and 3(b):\n\nMean (cid:0)(cid:12)\n\n(cid:12)X b\n\n(cid:12) (cid:12)\n\n(cid:1) ← Mean (cid:0)(cid:12)\n\n(cid:12)kX f\n\n(cid:1) .\n\n(cid:12) (cid:12)\n\nj\n\nj also is amplified, which we define as kX f\n\n(8) We can see from the Eq. 8, since the residual connection flows from the amplified input image, the value range of X f j . Meanwhile, the values of binary convolution are almost not affected by RA, because X b j . Different from BatchNorm, RA makes the value range of residual connection close to binary convolution (-60 to 60). Besides, we find that using RA to remove the BatchNorm has two main benefits: (1) Similar to full-precision IR networks, the binarized IR networks would have better performance without BatchNorm. (2) The structure of BBCU becomes more simple, efficient, and strong.\n\nj filters amplitude information of kX f\n\nj\n\nBased on the above findings, we are aware that the activation function (Eq. 4) in BBCU-V3 (Fig. 2(c)) narrows the negative value range of residual connection, which means it loses negative full-precision information. Thus, we further develop BBCU-V4 by moving the activation function into the residual connection to avoid losing the negative full-precision information. The experiments (Sec. 4.5) show that our design is accurate. We then take BBCU-V4 as the final design of BBCU.\n\n3.2 ARM IR NETWORKS WITH BBCU\n\nAs shown in Fig. 3(a), the existing image restoration (IR) networks could be divided into four parts: head H, body B, upsampling U, and tail T . If the IR networks do not need increasing resolution (Zhang et al., 2017), it can remove the upsampling U part. Specifically, given an LQ input ILQ, the process of IR network restoring HQ output ˆIHQ can be formulated as:\n\nˆIHQ = T (U(B(H(ILQ)))). (9) Previous BNN SR works (Xin et al., 2020; Jiang et al., 2021; Zhang et al., 2021b) concentrate on binarizing the body B part. However, upsampling U part accounts for 52.3% total calculations and is essential to be binarized. Besides, the binarized head H and tail T parts are also worth exploring.\n\nAs shown in Fig. 3(b), we further design different variants of BBCU for these four parts. (1) For the head H part, its input is ILQ ∈ R3×H×W and the binary convolution output with C channels. Thus, we cannot directly add ILQ to the binary convolution output for the difference in the number of channels. To address the issue, we develop BBCU-H by repeating ILQ to have C channels. (2) For body B part, since the input and output channels are the same, we develop BBCU-B by directly adding the input activation to the binary convolution output. (3) For upsampling U part, we develop BBCU-U by repeating the channels of input activations to add with the binary convolution. (4) For the tail T part, we develop BBCU-T by adopting ILQ as the residual connection. To better evaluate\n\n5\n\nPublished as a conference paper at ICLR 2023\n\n(a) The IR network can be Figure 3: The illustration of full-precision and binary IR networks. generally divided into four parts: head, body, upsampling, and tail. Notably, for IR tasks whose resolution remains unchanged, such as denoising and deblocking, we can ignore the upsampling part. (b) We equip different parts of binarized IR networks with the variants of BBCU. the benefits of binarizing each part on computations and parameters, we define two metrics:\n\nVC = (cid:16)\n\n(cid:16)\n\nPSNRf − PSNRb(cid:17)\n\nPSNRf − PSNRb(cid:17)\n\n/ (cid:16)\n\n(cid:16)\n\nOPsf − OPsb(cid:17) Parmsf − Parmsb(cid:17)\n\n,\n\n(10)\n\n(11) where PSNRb, OPsb, and Parmsb denote the performance, calculations, and parameters after binarizing one part of networks. Similarly, PSNRf , OPsf , and Parmsf measure full-precision networks.\n\nVP =\n\n/\n\n,\n\nWe adopt reconstruction loss Lrec to guide the image restoration training, which is defined as:\n\nLrec =\n\n(cid:13) (cid:13)\n\n(cid:13)IHQ − ˆIHQ\n\n(cid:13) (cid:13) (cid:13)1\n\n,\n\n(12)\n\nwhere IHQ and ˆIHQ are the real and restored HQ images, respectively. ∥ · ∥1 denotes the L1 norm.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENT SETTINGS\n\nTraining Strategy. We apply our method to three typical image restoration tasks: image superresolution, color image denoising, and image compression artifacts reduction. We train all models on DIV2K (Agustsson & Timofte, 2017), which contains 800 high-quality images. Besides, we adopt widely used test sets for evaluation and report PSNR and SSIM. For image super-resolution, we take simple and practical SRResNet (Ledig et al., 2017) as the backbone. The mini-batch contains 16 images with the size of 192×192 randomly cropped from training data. We set the initial learning rate to 1×10−4, train models with 300 epochs, and perform halving every 200 epochs. For image denoising and compression artifacts reduction, we take DnCNN and DnCNN3 as backbone (Zhang et al., 2017). The mini-batch contains 32 images with the size of 64×64 randomly cropped from training data. We set the initial learning rate to 1×10−4, train models with 300,000 iterations, and perform halving per 200,000 iterations. The amplification factor k in the residual alignment is set to 130. We implement our models with a Tesla V100 GPU.\n\nOPs and Parameters Calculation of BNN. Following Rastegari et al. (2016); Liu et al. (2018), the operations of BNN (OPsb) is calculated by OPsb = OPsf / 64 (OPsf indicates FLOPs), and the parameters of BNN (Parmsb) is calculated by Parmsb = Parmsf / 32.\n\n4.2 EVALUATION ON IMAGE SUPER-RESOLUTION\n\nFollowing BAM (Xin et al., 2020), we take SRResNet (Ledig et al., 2017) as backbone. We binarize body B part of SRResNet with some SOTA BNNs including BNN (Courbariaux et al., 2016), BiReal (Liu et al., 2018), IRNet (Qin et al., 2020), ReActNet (Liu et al., 2020), and BAM (Xin et al., 2020), BTM (Jiang et al., 2021). For comparison, we adopt our BBCU to binarize body B part to obtain BBCU-L. Furthermore, we binarize body B and upsampling U parts simultaneously to develop\n\n6\n\nReluConvHeadHBodyBUpsamplingUTailTW∈R!×#×$!×$\"W∈R!×%×$!×$\"...W∈R&!×%×$!×$\"W∈R#×%×$!×$\"Full-PrecisionConvReluPixelShuffleW’∈R!×#×$!×$\"BBCU-BW’∈R!×%×$!×$\"...W’∈R&!×%×$!×$\"W’∈R#×%×$!×$\"×kResidual Alignment (RA)÷kRABinary ConvRPReLUSignConvBasicBinaryConvUnitforBody(BBCU-B)RPReLUSign(b)ThearchitectureofBinarizedIRNetwork(a)ThearchitectureofFull-PrecisionIRNetworkI!\"I#\"I!\"I#$BBCU-HC×H×WC×H×WC×H×WRepeat4C×H×WBasicBinaryConvUnitforUpsampling(BBCU-U)C×H×W3×4H×4WBBCU-UBlinearI)*BasicBinaryConvUnitforTail(BBCU-T)I+*3×H×WC×H×WBasicBinaryConvUnitforHead(BBCU-H)I+*RepeatInput&OutputBBCU-THeadHBodyBUpsamplingUTailTPublished as a conference paper at ICLR 2023\n\nTable 1: Quantitative comparison (average PSNR/SSIM) with BNNs for classical image SuperResolution on benchmark datasets. Best and second best performance among BNNs are in red and blue colors, respectively. OPs are computed based on LQ images with a resolution of 320×180.\n\nMethods\n\nScale Ops (G)\n\nParams (K)\n\nSet5\n\nSet14\n\nB100\n\nUrban100\n\nManga109\n\nPSNR\n\nSSIM PSNR\n\nSSIM PSNR\n\nSSIM PSNR\n\nSSIM PSNR\n\nSSIM\n\nSRResNet SRResNet-Lite Bicubic BNN Bi-Real BAM IRNet BTM ReActNet BBCU-M (Ours) BBCU-L (Ours)\n\nSRResNet SRResNet-Lite Bicubic BNN Bi-Real BAM IRNet BTM ReActNet BBCU-M (Ours) BBCU-L (Ours)\n\n×2\n\n×4\n\n85.43 3.08 -\n18.55 18.55 20.52 18.55 18.55 18.55 1.83 18.55\n\n146.14 5.39 -\n79.20 79.20 81.17 79.20 79.20 79.20 3.95 79.20\n\n1367 49 -\n225 225 226 225 225 225 46 225\n\n1515 54 -\n372 372 373 372 372 372 51 372\n\n38.00 37.21 33.97 32.25 32.32 37.21 37.27 37.22 37.26 37.44 37.58\n\n32.16 31.40 28.63 27.56 27.75 31.24 31.38 31.43 31.54 31.54 31.79\n\n0.9605 0.9578 0.9330 0.9118 0.9123 0.9560 0.9579 0.9575 0.9579 0.9584 0.9590\n\n0.8951 0.8843 0.8128 0.7896 0.7935 0.8780 0.8835 0.8850 0.8859 0.8862 0.8905\n\n33.59 32.86 30.55 29.25 29.47 32.74 32.92 32.93 32.97 33.04 33.18\n\n28.60 28.11 26.21 25.51 25.79 27.97 28.08 28.16 28.19 28.20 28.38\n\n0.9171 0.9113 0.8750 0.8406 0.8424 0.9100 0.9115 0.9118 0.9124 0.9127 0.9143\n\n0.7822 0.7692 0.7087 0.6820 0.6879 0.7650 0.7679 0.7706 0.7705 0.7718 0.7762\n\n32.19 31.67 29.73 28.68 28.74 31.6 31.76 31.77 31.81 31.81 31.91\n\n27.58 27.26 26.04 25.54 25.59 27.15 27.24 27.29 27.31 27.31 27.41\n\n0.8997 0.8936 0.8494 0.8104 0.8111 0.8910 0.8941 0.8945 0.8954 0.8946 0.8962\n\n0.7364 0.7243 0.6719 0.6466 0.6478 0.7190 0.7227 0.7256 0.7252 0.7263 0.7303\n\n32.11 30.48 27.07 25.96 26.35 30.20 30.63 30.79 30.85 30.84 31.12\n\n26.11 25.19 23.24 22.68 22.91 24.95 25.21 25.34 25.35 25.35 25.62\n\n0.9282 0.9109 0.8456 0.8088 0.8161 0.9060 0.9122 0.9146 0.9156 0.9149 0.9179\n\n0.7870 0.7544 0.6114 0.6352 0.6450 0.7450 0.7536 0.7605 0.7603 0.7602 0.7696\n\n38.56 36.70 31.24 29.16 29.64 -\n36.77 36.76 36.92 37.20 37.50\n\n30.46 28.92 25.07 24.19 24.57 -\n28.97 29.19 29.25 29.22 29.69\n\n0.9770 0.9722 0.9383 0.9127 0.9167 -\n0.9724 0.9724 0.9728 0.9738 0.9746\n\n0.9089 0.8863 0.7904 0.7670 0.7752 -\n0.8863 0.8912 0.8912 0.8918 0.8992\n\nHR\n\nSRResNet-Lite\n\nBi-Real\n\nBBCU-M\n\nHR\n\nSRResNet-Lite\n\nBi-Real\n\nBBCU-M\n\nBicubic\n\nSRResNet\n\nReActNet\n\nBBCU-L\n\nBicubic\n\nSRResNet\n\nReActNet\n\nBBCU-L\n\nHR\n\nSRResNet-Lite\n\nBi-Real\n\nBBCU-M\n\nHR\n\nSRResNet-Lite\n\nBi-Real\n\nBBCU-M\n\nBicubic\n\nSRResNet\n\nReActNet\n\nBBCU-L\n\nBicubic\n\nSRResNet\n\nReActNet\n\nBBCU-L\n\nFigure 4: Visual comparison of BNNs for 4× image super-resolution.\n\nBBCU-M. Besides, we reduce the number of channels of SRResNet to 12, obtaining SRResNetLite. In addition, we use Set5 (Bevilacqua et al., 2012), Set14 (Zeyde et al., 2010), B100 (Martin et al., 2001), Urban100 (Huang et al., 2015), and Manga109 (Matsui et al., 2017) for evaluation.\n\nThe quantitative results (PSNR and SSIM), the number of parameters, and operations of different methods are shown in Tab. 1. Compared with other binarized methods, our BBCU-L achieves the best results on all benchmarks and all scale factors. Specifically, for 4× SR, our BBCU-L surpasses ReActNet by 0.25dB, 0.27dB, and 0.44dB on Set5, Urban100, and Manga109, respectively. For 2× SR, BBCU-L also achieves 0.32dB, 0.27dB, and 0.58dB improvement on these three benchmarks compared with ReActNet. Furthermore, our BBCU-M achieves the second best performance on most benchmarks consuming 5% of operations and 13.7% parameters of ReActNet (Liu et al., 2020) on 4× SR. Besides, BBCU-L significantly outperforms SRResNet-Lite by 0.16dB and 0.3dB with less computational cost, showing the superiority of BNN. The qualitative results are shown in Fig. 4, and our BBCU-L has the best visual quality containing more realistic details close to respective ground-truth HQ images. More qualitative results are provided in appendix.\n\n4.3 EVALUATION ON IMAGE DENOISING\n\nFor image denoising, we use DnCNN (Zhang et al., 2017) as the backbone and binarize its body B part with BNN methods, including BNN, Bi-Real, IRNet, BTM, ReActNet, and our BBCU. We also develop DnCNN-Lite by reducing the number of channels of DnCNN from 64 to 12. The standard benchmarks: Urban100 (Huang et al., 2015), BSD68 (Martin et al., 2001), and Set12 (Shan et al.,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nTable 2: Quantitative comparison (average PSNR) with BNNs for classical image denoising on benchmark datasets. Best and second best performance among BNNs are in red and blue colors, respectively. OPs is computed based on LQ images with a resolution of 320×180.\n\nMethods\n\nOPs (G)\n\nParams (K)\n\nσ = 15\n\nCBSD68 σ = 25\n\nσ = 50\n\nσ = 15\n\nKodak24 σ = 25\n\nσ = 50\n\nσ = 15\n\nUrban100 σ = 25\n\nσ = 50\n\nDnCNN DnCNN-Lite BNN Bi-Real IRNet BTM ReActNet BBCU (Ours)\n\n38.42 1.38 0.8 0.8 0.8 0.8 0.8 0.8\n\n667 24 24 24 24 24 24 24\n\n33.90 32.26 26.90 30.73 31.37 32.25 32.26 33.08\n\n31.24 29.64 22.67 28.72 29.01 29.91 29.95 30.56\n\n27.95 26.49 16.41 25.63 26.75 26.79 26.93 27.33\n\n34.60 32.73 27.12 30.97 31.61 32.75 32.78 33.66\n\n32.14 30.25 22.58 29.17 29.54 30.64 30.65 31.28\n\n28.95 27.22 16.25 26.11 27.48 27.62 27.70 28.15\n\n32.98 30.97 26.67 30.18 30.57 30.99 31.27 32.27\n\n30.81 28.46 22.67 28.18 28.35 29.05 29.20 29.96\n\n27.59 25.21 16.54 25.11 26.00 25.86 26.01 26.61\n\nTable 3: Quantitative comparison (average PSNR/SSIM) with BNNs for classical JPEG compression artifact reduction. Best and second best performance among BNNs are in red and blue colors, respectively. OPs is computed based on LQ images with a resolution of 320×180.\n\nMethods\n\nOPs (G)\n\nParams (K)\n\nq = 10\n\nq = 20\n\nq = 30\n\nq = 40\n\nq = 10\n\nq = 20\n\nq = 30\n\nq = 40\n\nLive1\n\nClassic5\n\nDnCNN-3 Dncnn-3-Lite BNN Bi-Real IRNet BTM ReActNet BBCU (Ours)\n\n38.29 1.36 0.66 0.66 0.66 0.66 0.66 0.66\n\n665 24 22 22 22 22 22 22\n\n29.19/0.8123 28.90/0.8061 28.48/0.7925 28.67/0.8011 28.73/0.8019 28.75/0.8032 28.81/0.8025 29.06/0.8087\n\n31.59/0.8802 31.27/0.8746 30.82/0.8661 31.06/0.8706 31.13/0.8704 31.18/0.8725 31.20/0.8709 31.43/0.8780\n\n32.98/0.9090 32.62/0.9029 32.19/0.8965 32.42/0.8998 32.49/0.8993 32.51/0.9005 32.52/0.8981 32.80/0.9067\n\n33.96/0.9247 33.52/0.9179 33.12/0.9128 33.36/0.9158 33.40/0.9148 33.38/0.9153 33.37/0.9123 33.75/0.9221\n\n29.40/0.8026 29.80/0.8000 29.41/0.7879 29.54/0.7934 29.63/0.7947 29.65/0.7968 29.70/0.7956 30.00/0.8028\n\n31.63/0.8610 32.09/0.8611 31.71/0.8537 31.88/0.8573 31.96/0.8570 32.02/0.8597 32.04/0.8581 32.27/0.8645\n\n32.91/0.8861 33.39/0.8868 33.04/0.8817 33.21/0.8845 33.28/0.8836 33.30/0.8853 33.32/0.8831 33.59/0.8903\n\n33.77/0.9003 34.25/0.9011 33.89/0.8964 34.08/0.8996 34.12/0.8983 34.11/0.8991 34.11/0.8964 34.45/0.9044\n\n2019) are applied to evaluate each method. Additive white Gaussian noises (AWGN) with different noise levels σ (15, 25, 50) are added to the clean images.\n\nThe quantitative results of image denoising are shown in Tab. 2, respectively. As one can see, our BBCU achieves the best performance among compared BNNs. In particular, our BBCU surpasses the state-of-the-art BNN model ReActNet by 0.82dB, 0.88dB, and 1dB on CBSD68, Kodak24, and Urban100 datasets respectively as σ = 15. Compared with DnCNN-Lite, our BBCU surpasses it by 0.92dB, 1.03dB, and 1.5dB on these three benchmarks as σ = 15 consuming 58% computations of DnCNN-Lite. Qualitative results are provided in appendix.\n\n4.4 EVALUATION ON JPEG COMPRESSION ARTIFACT REDUCTION\n\nFor this JPEG compression deblocking, we use practical DnCNN-3 (Zhang et al., 2017) as the backbone and replace the full-precision body B part of DnCNN3 with some competitive BNN methods, including BNN, Bi-Real, IRNet, BTM, ReActNet, and our BBCU. The compressed images are generated by Matlab standard JPEG encoder with quality factors q ∈ {10, 20, 30, 40}. We take the widely used LIVE1 (Sheikh, 2005) and Classic5 (Foi et al., 2007) as test sets to evaluate the performance of each method. The quantitative results are presented in Tab. 3. As we can see, our BBCU achieves the best performance on all test sets and quality factors among all compared BNNs. Specifically, our BBCU surpasses the state-of-the-art BNN model ReActNet by 0.38dB and 0.34dB on the Live1 and Classic5 as q = 40. In addition, our BBCU surpasses DnCNN-3-Lite by 0.16dB and 0.2dB on benchmarks as q = 10. The visual comparisons are provided in appendix.\n\n4.5 ABLATION STUDY\n\nSet5\n\nSet14\n\nOPs (G)\n\nMethods\n\nTable 4: PSNR (dB) values (4×) on four types of basic binary convolution unit (BBCU).\n\nBasic Binary Convolution Unit. To validate BBCU for the binarized IR network, we binarize the body part of SRResNet with four variants of BBCU (Fig. 2) separately. The results are shown in Tab. 4. (1) Compared with BBCUV1, BBCU-V2 declines by 0.14dB and 0.24dB on Urban100 and Manga109. This is because BBCU-V2 simply removes the BN making the value range of binary Conv far larger than residual connection and cover full-precision information (Fig. 2). (2) BBCU-V3 adds residual alignment scheme on BBCU-V2, which addresses the value range imbalance between binary Conv and residual connection and removes the BN. Since the BN is harmful for IR networks, BBCU-V3 surpasses BBCU-V1 by 0.17dB, 0.16dB, and 0.29dB on Set5, Urban100, and Manga109 respectively. (3) BBCU-V4 moves the activation function into the residual connection, which preserves the full-precision negative values of residual connection (Fig. 2). Thus, BBCU-V4 outperforms BBCU-V3.\n\nBBCU-V1 BBCU-V2 BBCU-V3 BBCU-V4\n\nB100 Urban100 Manga109\n\n25.35 25.21 25.51 25.62\n\n28.19 28.07 28.31 28.38\n\n27.31 27.22 27.37 27.41\n\n31.54 31.37 31.71 31.79\n\n29.25 29.01 29.54 29.69\n\n79.20 79.20 79.20 79.20\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 5: The breakpoint position of residual connection.\n\nTable 6: The binarization benefit of different parts in IR networks. We test models on Set14, and PSNRf is 28.60dB.\n\nPosition Idx\n\nPSNRb (dB)\n\nHead H Body B\n\nUpsampling U\n\nTail T\n\n1 5\n10 15 20 25 -\n\n25.43 25.44 25.48 25.46 25.45 25.44 25.62\n\nOPsf (M) OPsb (M) Paramsf (K) Paramsb (K)\n\nPSNRb (dB)\n\n99.53 1.56 1.73 0.05\n\n28.58\n\nVC ↓ (×10−6) VP ↓ (×10−3)\n\n204.14 11.91\n\n67947.73 1061.68 1179.65 36.86\n\n28.38\n\n3.29 0.19\n\n76441.19 1194.39 294.91 9.22\n\n28.59\n\n0.13 0.04\n\n1592.53 24.88 1.73 0.05\n\n27.76\n\n535.83 500.00\n\nFigure 5: The effect of number of binary convolution in residual connection on SRResNet.\n\nFigure 6: The effect of amplification factor on SRResNet with various number of channels.\n\nResidual Connection. We take SRResNet as the backbone with 32 Convs in body part and explore the relationship between performance and the number of binary convolution in residual connection. Specifically, for both full-precision and binarized SRResNet, we set 1,2,4,8,16, and 32 Convs with a residual connection as basic convolution units, respectively. We evaluate SRResNet equipped with these basic convolution units on 4× Urban100 (see Fig. 5). (1) For full-precision networks, it is best to have 2 Convs with a residual connection to form a basic convolution unit. Besides, if there are more than 4 Convs in a basic convolution unit, its performance would sharply drop. For binarized networks, it is best to equip each binary convolution with a residual connection. In addition, compared with the full-precision network, the binarized network is more insensitive to the growth of the Convs number in the basic convolution unit. (2) For binarized SRResNet, we delete residual connection of a BBCU in a certain position. In Tab. 5, it seems that once the residual connection is removed (the full-precision information is lost) at any position, the binarized SRResNet would suffer a severe performance drop.\n\nAmplification Factor. As shown in Fig. 6, the performance of full-precision remain basically unchanged with the variation of amplification factor k. However, the binarized network is sensitive to the k. The best k∗ is related to the number of channels n, which empirically fits k∗ = 130n/64. Intuitively, the larger number of channels makes binary convolution count more elements and have larger results, which needs larger k to increase the value of residual connection for balance.\n\nThe Binarization Benefit for Different Parts of IR Network. We binarize one part in SRResNet with BBCU in Fig. 3 while keeping other parts full-precision. The results are shown in Tab. 6. We use VC (Eq. 10) and VP (Eq. 11) as metric to value binarization benefit of different parts. We can see that Upsampling part is most worth to be binarized. However, it is ignored by previous works (Xin et al., 2020). The binarization benefit of first and last convolution is relatively low.\n\n5 CONCLUSION\n\nThis work devotes attention to exploring the performance of BNN on low-level tasks and search of generic and efficient basic binary convolution unit. Through decomposing and analyzing existing elements, we propose BBCU, a simple yet effective basic binary convolution unit, that outperforms existing state-of-the-arts with high efficiency. Furthermore, we divide IR networks to four parts, and specially develop the variants of BBCU for them to explore the binarization benefits. Overall, BBCU provide insightful analyses on BNN and can serve as strong basic binary convolution unit for future binarized IR networks, which is meaningful to academic research and industry.\n\n9\n\n051015202530Number of Binary Convolution in Residual Connection25.425.625.826.0PSNR (dB)Full-Precision NetworkBinarized Network0100200300400500Amplification Factor k24.5024.7525.0025.2525.5025.7526.0026.25PSNR (dB)Full-Precision Network w 32 ChannelsBinarized Network w 32 ChannelsFull-Precision Network w 64 ChannelsBinarized Network w 64 ChannelsFull-Precision Network w 128 ChannelsBinarized Network w 128 ChannelsPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was partly supported by the Alexander von Humboldt Foundation, the National Natural Science Foundation of China(No. 62171251), the Natural Science Foundation of Guangdong Province(No.2020A1515010711), the Special Foundations for the Development of Strategic Emerging Industries of Shenzhen(Nos.JCYJ20200109143010272 and CJGJZD20210408092804011) and Oversea Cooperation Foundation of Tsinghua.\n\nREFERENCES\n\nEirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution:\n\nDataset and study. In CVPRW, 2017. 6\n\nMarco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie line Alberi Morel. Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, 2012. 7\n\nLukas Cavigelli, Pascal Hager, and Luca Benini. Cas-cnn: A deep convolutional neural network for\n\nimage compression artifact suppression. In IJCNN, 2017. 3\n\nYunjin Chen and Thomas Pock. Trainable nonlinear reaction diffusion: A flexible framework for\n\nfast and effective image restoration. TPAMI, 2016. 1\n\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016. 1, 3, 6\n\nTao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network\n\nfor single image super-resolution. In CVPR, 2019. 3\n\nChao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by\n\na deep convolutional network. In ICCV, 2015a. 3\n\nChao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep\n\nconvolutional networks. TPAMI, 2015b. 3\n\nAlessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Pointwise shape-adaptive dct for high-\n\nquality denoising and deblocking of grayscale and color images. TIP, 2007. 8\n\nXueyang Fu, Zheng-Jun Zha, Feng Wu, Xinghao Ding, and John Paisley. Jpeg artifacts reduction\n\nvia deep convolutional sparse coding. In ICCV, 2019. 3\n\nXueyang Fu, Menglu Wang, Xiangyong Cao, Xinghao Ding, and Zheng-Jun Zha. A model-driven\n\ndeep unfolding method for jpeg artifacts removal. TNNLS, 2021. 3\n\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks.\n\nIn Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011. 2\n\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-\n\nproved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017. 3\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In CVPR, 2016. 2\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-\n\nworks. In ICCV, 2017. 1\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2015. 2\n\nJia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from trans-\n\nformed self-exemplars. In CVPR, 2015. 7\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\n\nreducing internal covariate shift. In ICML, 2015. 2\n\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In CVPR, 2018. 1\n\nXixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Focnet: A fractional optimal control network\n\nfor image denoising. In CVPR, 2019. 3\n\nXinrui Jiang, Nannan Wang, Jingwei Xin, Keyu Li, Xi Yang, and Xinbo Gao. Training binary neural\n\nnetwork without batch normalization for image super-resolution. In AAAI, 2021. 2, 3, 5, 6\n\nJiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep\n\nconvolutional networks. In CVPR, 2016. 3\n\nYoonsik Kim, Jae Woong Soh, Jaewoo Park, Byeongyong Ahn, Hyun-Seung Lee, Young-Su Moon, and Nam Ik Cho. A pseudo-blind convolutional neural network for the reduction of compression artifacts. TCSVT, 2019. 3\n\nChristian Ledig, Lucas Theis, Ferenc Husz ́ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In CVPR, 2017. 6\n\nBee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep resid-\n\nual networks for single image super-resolution. In CVPRW, 2017. 2, 4, 5\n\nZechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In ECCV, 2018. 1, 2, 3, 6\n\nZechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise\n\nbinary neural network with generalized activation functions. In ECCV, 2020. 1, 2, 3, 4, 6, 7\n\nYinglan Ma, Hongyu Xiong, Zhe Hu, and Lizhuang Ma. Efficient super resolution using binarized\n\nneural network. In CVPRW, 2019. 2, 3\n\nDavid Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 7\n\nYusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 2017. 7\n\nYali Peng, Lu Zhang, Shigang Liu, Xiaojun Wu, Yu Zhang, and Xili Wang. Dilated residual net-\n\nworks with symmetric skip connection for image denoising. Neurocomputing, 2019. 3\n\nHaotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In CVPR, 2020. 3, 6\n\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet\n\nclassification using binary convolutional neural networks. In ECCV, 2016. 1, 3, 4, 6\n\nChuanhui Shan, Xirong Guo, and Jun Ou. Residual learning of deep convolutional neural networks\n\nfor image denoising. Journal of Intelligent & Fuzzy Systems, 2019. 7\n\nHR Sheikh.\n\nLive image quality assessment database release 2.\n\nhttp://live. ece. utexas.\n\nedu/research/quality, 2005. 8\n\nJae Woong Soh, Sunwoo Cho, and Nam Ik Cho. Meta-transfer learning for zero-shot super-\n\nresolution. In CVPR, 2020. 3\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nYing Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Memnet: A persistent memory network for\n\nimage restoration. In ICCV, 2017. 1\n\nXintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In ECCVW, pp. 0–0, 2018. 3\n\nYunxuan Wei, Shuhang Gu, Yawei Li, Radu Timofte, Longcun Jin, and Hengjie Song. Unsupervised\n\nreal-world image super resolution via domain-distance aware training. In CVPR, 2021. 3\n\nBin Xia, Yucheng Hang, Yapeng Tian, Wenming Yang, Qingmin Liao, and Jie Zhou. Efficient\n\nnon-local contrastive attention for image super-resolution. AAAI, 2022a. 3\n\nBin Xia, Jingwen He, Yulun Zhang, Yucheng Hang, Wenming Yang, and Luc Van Gool. Structured sparsity learning for efficient video super-resolution. arXiv preprint arXiv:2206.07687, 2022b. 3\n\nBin Xia, Yapeng Tian, Yucheng Hang, Wenming Yang, Qingmin Liao, and Jie Zhou. Coarse-to-fine embedded patchmatch and multi-scale dynamic aggregation for reference-based super-resolution. In AAAI, 2022c. 3\n\nBin Xia, Yapeng Tian, Yulun Zhang, Yucheng Hang, Wenming Yang, and Qingmin Liao. arXiv preprint\n\nMeta-learning based degradation representation for blind super-resolution. arXiv:2207.13963, 2022d. 3\n\nBin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, and Luc Van Gool. Knowledge distillation based degradation estimation for blind super-resolution. ICLR, 2023. 1\n\nJingwei Xin, Nannan Wang, Xinrui Jiang, Jie Li, Heng Huang, and Xinbo Gao. Binarized neural\n\nnetwork for single image super resolution. In ECCV, 2020. 2, 3, 5, 6, 9\n\nRoman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-\n\nrepresentations. In International conference on curves and surfaces, 2010. 7\n\nKai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:\n\nResidual learning of deep cnn for image denoising. TIP, 2017. 3, 5, 6, 7, 8\n\nKai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-\n\nbased image denoising. TIP, 2018a. 1\n\nKai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play\n\nimage restoration with deep denoiser prior. TPAMI, 2021a. 3\n\nLei Zhang, Zhiqiang Lang, Wei Wei, and Yanning Zhang. Embarrassingly simple binarization for\n\ndeep single imagery super-resolution networks. TIP, 2021b. 2, 3, 5\n\nYulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.\n\nImage super-\n\nresolution using very deep residual channel attention networks. In ECCV, 2018b. 3\n\nYulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image\n\nsuper-resolution. In CVPR, 2018c. 3\n\nYulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image\n\nrestoration. TPAMI, 2020. 3\n\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\n\narXiv:1611.01578, 2016. 1\n\n12",
    "reference": "# Summary Of The Paper\n\nThis paper presents a method to improve upon binary neural network for image restoration (IR). It shows that batch normalization can help to obtain a good accuracy performance when binarizing IR models. Based on this observation, the authors introduce a basic binary convolution unit that benefits from batch norm properties using residual alignment. It has been shown that IR binary models outperform existing models across different tasks and datasets.\n\n# Strength And Weaknesses\n\nStrengths:\n\n-- The basic binary convolution unit is very simple and effective. IR models equipped with such units outperform existing works.\n\n-- The binary IR models were tested on a wide range of tasks and datasets.\n\n-- The paper is well-motivated, easy to read and well-structured.\n\nWeaknesses:\n\n-- The weight of technical novelty of this work is much lower than that of its empirical novelty. The proposed unit is obtained based on empirical experiments rather than a concrete mathematical foundation.\n\n--  It is not very clear if the improvement comes from RA or RPReLU. What I am curious to see is an ablation study for BBCU-V4 without RA.\n\n-- There is a small degradation in performance when binarizing body and upsampling parts in image super-resolution. I am wondering why it was not explored by prior works. If it was, then please compare.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clear and its results can be reproduced. In terms of novelty, I believe its empirical novelty is more significant.\n\n# Summary Of The Review\n\nIn general, this paper introduces a simple and effective binary unit that further closes the gap between the binary model and its full-precision counterpart. The reasoning and development of the idea is based on empirical results, which its technical novelty is limited.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nVISUAL EXPERTISE AND THE LOG-POLAR TRANSFORM EXPLAIN IMAGE INVERSION EFFECTS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nVisual expertise can be defined as the ability to discriminate among subordinatelevel objects in homogeneous classes, such as identities of faces within the class ”face”. Despite being able to discriminate many faces, subjects perform poorly at recognizing even familiar faces once inverted. This face-inversion effect is in contrast to subjects’ performance identifying inverted objects for which their experience is at a basic level, which results in less impairment. Experimental results have suggested that when identifying mono-oriented objects, such as cars, car novices’ performance is between that of faces and other objects. We build an anatomicallyinspired neurocomputational model to explore this effect. Our model includes a foveated retina and the log-polar mapping from the visual field to V1. This transformation causes changes in scale to appear as horizontal translations, leading to scale equivariance. Rotation is similarly equivariant, leading to vertical translations. When fed into a standard convolutional network, this provides rotation and scale invariance. It may be surprising that a rotation-invariant network shows any inversion effect at all. This is because there is a crucial topological difference between scale and rotation: Rotational invariance is discontinuous, with V1 ranging from 90°(vertically up) to 270°(vertically down). Hence when a face is inverted, the configural information in the face is disrupted while feature information is relatively unaffected. We show that the inversion effect arises as a result of visual expertise, where configural information becomes relevant as more identities are learned at the subordinate level. Our model matches the classic result: faces suffer more from inversion than mono-oriented objects, which are more disrupted than non-mono-oriented objects when objects are only familiar at a basic level.\n\n1\n\nINTRODUCTION\n\nSince 1969, researchers have been studying the effects of inverting images (Yin, 1969). Some researchers have focused on defining the bounds of inversion effects: what the measurable effect is for what types of images (Farah et al., 1995; Yin, 1969; Jacques et al., 2007; Rezlescu et al., 2017). Others looked to explain how inversion effects arise: what part of the brain was active during inversion tasks or what level of experience a participant had with the stimuli in the experiment (Gauthier et al., 2000; Gauthier & Bukach, 2007; Gauthier et al., 2014; Kanwisher et al., 1997; 1998; Richler et al., 2011; Wang et al., 2014).\n\nIn Yin (1969), participants studied a set of images during the training phase, and then they were shown pairs of images in testing and asked to select the image that was in the study set. Trials with upright images and trials with inverted images were compared to determine the inversion effect. Using images of faces resulted in a strong and significant inversion effect - performance was much worse for inverted faces. Images of houses - a mono-oriented category - had a lesser, but still significant effect. Images of airplanes had an insignificant inversion effect. We draw two conclusions from this work: the effects of inversion on performance are greater when images of faces are used as the stimuli and insignificant when images of certain objects (e.g., planes, that are less mono-oriented than houses) are used as the stimuli (Yin, 1969). The second conclusion is that not all objects produce the same inversion effects. Mono-oriented objects, which are objects that are typically seen in only one orientation such as the houses in Yin’s 1969 work, do show an inversion effect, though it is smaller than that of faces (Yin, 1969).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nSince Yin’s 1969 paper on inversion, a great deal of research has focused on explaining inversion effects. Why is it that different stimuli - faces, objects, mono-oriented objects - produce different inversion effects? One explanation of inversion effects, supported by brain imaging and behavioral studies, is that visual expertise changes the way we process visual stimuli. Faces are processed holistically, which means that not just the features, but the configuration of the features matters. When such stimuli are inverted, the configuration is disrupted, and we are left with featural processing (Gauthier et al., 2000; 1999; 2003). Similar inversion effects have been observed in experts of other domains, such as dog show judges or bird watchers (Diamond & Carey, 1986; Gauthier et al., 2000).\n\nVisual expertise is defined with respect to Rosch’s basic level categories. In a category hierarchy, the basic level is the level at which objects are most commonly labeled, such as ”chair”, ”tree”, or ”car”. Basic level categories define broad categories of objects that share properties such as general appearance, function, and common parts (Rosch et al., 1976). For example, cars can look very different from each other, but they all have wheels, an enclosed space for passengers, and are used for ground transportation. Visual expertise is defined as having proficiency in differentiating subordinate-level sub-classes of basic level categories. For example, subordinates of the basic level category ”tree” could include ”sugar maple”, ”american elm”, or ”northern red oak”.\n\nMost people are face experts in this sense. It has been estimated that we are able to identify on the order of 5,000 different people (Jenkins et al., 2018). Identity is a subordinate-level judgment because faces share the same features (eyes, nose, mouth, ears, etc.) in the same general configuration. We also process faces holistically (Gauthier & Bukach, 2007). This means that instead of just using the features of a face to recognize a person, we use the configural information, such as the distance between the eyes, or the distance from the nose to the mouth. Hence, expertise is fine grained discrimination of homogeneous categories. The research into expertise suggests that experts in other domains, such as cars or birds, also use configural information when viewing basic level categories in which the participants are experts (Gauthier et al., 2000).\n\nWe conduct experiments in order to ask if there is a way to characterize inversion effects in different stimuli in terms of levels of expertise. In doing so, we explore the changes in visual signal processing that occur between novice level and expert level. To do this, We build an anatomically inspired network that incorporates foveation - high resolution central vision and low resolution peripheral vision - and the log polar mapping between the visual field and the primary visual cortex (Polimeni et al., 2006). The log polar mapping causes changes in image scale to appear as horizontal translations. When presented as input to a convolutional neural network (CNN), which is translation invariant, the log polar mapping makes the network relatively scale invariant. Image rotation is similarly equivariant in the log-polar representation, because it leads to vertical translations. However, the two differ topologically: pixels that shift vertically can ”fall off” the edge of the image and wrap around to the opposite edge. This causes a rearranging of features in the image, hence a disruption of configural information.\n\nUsing this model, we test the inversion effects of different types of stimuli across increasing expertise in order to gain an insight into how and why visual processing changes based on the visual stimulus. Our model is consistent with the view that expertise plays a significant role in the way we process visual inputs, and leads to the inversion effects seen in previous work.\n\n2 METHODS\n\n2.1 MODEL\n\nWe use ResNet-50 (He et al., 2016) to perform all experiments, trained from scratch with the foveated, log-polar representation. We call this LPnet. We compare our results to a ”vanilla” ResNet50 with standard images. Unless otherwise noted, all experiments use the Adam optimizer, an initial learning rate of 1e-4, and a minibatch size of 48.\n\n2.2 DATA\n\nTo test the effects of expertise in visual processing, we use four different datasets. To model experts, the first three datasets are images of faces, cars, and dogs, generally mono-oriented objects, with targets at the subordinate level. To model novices, who mainly know basic-level labels, the fourth\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nA\n\nB\n\nC\n\nD\n\nFigure 1: Example images used for recognition experiments from four datasets: (A) a lab-gathered face dataset, (B) Comprehensive Cars dataset, (C) ImageNet (dog categories only), and (D) ImageNet (dog categories excluded).\n\ndataset contains 128 categories with basic-level targets. 124 are ImageNet categories, containing objects that are seen in a variety of orientations in natural scenes. The remaining four are monooriented: faces, cars, houses, and dogs. We randomly chose our sets with a 80/10/10 split.\n\nFaces We used a dataset collected by lab members which includes 128 separate identities and approximately 200 example images per identity. The images for each identity portray the person in a variety of contexts, with differing backgrounds, lighting conditions, orientations, and facial expressions. Example images from the dataset are shown in Figure 1A.\n\nSubordinate level mono-oriented objects: Cars Cars are an appropriate choice for mono-oriented objects because they are almost exclusively seen upright in natural scenes. ”Car” is also a basic level category with a number of sub-classes. Car expertise is associated with discrimination at the level of model, e.g., 2010 Toyota Camry. The dataset used is the Comprehensive Cars dataset (Yang et al., 2015). These images include cars of a variety of makes, models, and years. The dataset contains 136,726 total images across 163 car makes and 1,716 car models. The cars are of varying orientations, lighting conditions, and backgrounds. Examples of cars from the Comprehensive Cars dataset are shown in Figure 1B.\n\nSubordinate level mono-oriented objects: Dogs In the same way that models of cars are subordinates of the category ”car”, dog breeds are subordinates of the category ”dog”. We use 117 dog breeds included in ImageNet (Deng et al., 2009). Like all of ImageNet, these images are highly variable in pose, context, and scale. Because of this we use all available images from ImageNet in these 117 categories. Examples of dogs in the dataset are shown in Figure 1C.\n\nBasic level categories We use a subset of 124 categories from ImageNet (Deng et al., 2009) which were chosen specifically because they are naturally viewed in multiple orientations, such as ”ladle”, ”screwdriver”, or ”dumbbell” (Figure 1D). We avoided categories such as ”clock” or ”candle” which, although can be oriented multiple ways, are naturally seen primarily in a limited number of orientations. The labels for the stimuli in this category are at the basic level, instead of at the subordinate level as in the previous two datasets (Rosch et al., 1976). Again, due to the within-category variance, we used all examples in ImageNet in order to achieve acceptable performance. We also included 4 categories (bringing the total number of categories to 128) that are mono-oriented. They are: ”face”, ”car”, ”dog”, and ”house”. Using these mono-oriented objects in our experiment with basic level categorization allows us to compare how performance changes between experts and novices as visual expertise general increases. We aggregate data from each of the subordinates of ”face”, ”car”, and ”dog” to get a varied sample of images for these categories. We matched the number of images approximately to the number of images the ImageNet categories included.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nCrop 1\n\nCrop 2\n\n0°\n\n15°\n\n180°\n\n0°\n\n15°\n\n180°\n\nd r\na d\nn a\nt\n\nS\n\nr a\nl\n\no P\n\ng o\nL\n\nFigure 2: A visualization of the log polar transformation for two crops of the same image. The top row shows standard images, or a depiction of the visual field. The bottom row shows images that have been transformed, using the log polar mapping that approximates the representation of the visual field in V1. For each crop, we show both image types at three amounts of rotation: 0°, 15°, and 180°.\n\n2.3 DATA TRANSFORMATIONS\n\nCropping We augment our data by performing random cropping on the images. We take four random crops of each image, with each crop including approximately 65% of the pixels in the image. The crops cannot extend past the edge of the image, so none of the crops include any padding.\n\nRotation We randomly rotated our training images to be between -15°and 15°, because scenes may be viewed with some small amount of rotation from the tilting of the viewers own head. To study the effects of inversion in the network, our validation images are shown at 0°and 180°.\n\nFoveation For LPNet only, we foveate each crop using the algorithm described in Jiang et al. (2015). The foveation leaves the center of the crop (the point of fixation) at a high resolution and transforms the periphery to be at a lower resolution. The further a pixel is from the center of the image, the greater the degree of blurring. This mimics the foveation of the retina (Jiang et al., 2015).\n\nLog-polar transform For LPNet only, we further preprocessed our images to create an anatomically-realistic mapping of the visual field onto the visual cortex. Previous work has shown the validity of using log polar transformations as a 2D approximation to the mapping of the visual field onto the visual cortex in primates (Polimeni et al., 2006) and in computational models of facial recognition (Anonymous). For each case of rotation, we first rotated the image and then took the log polar transformation of the image. After the images have undergone a log polar transformation, the changes in degrees of rotation appear as changes in vertical translation. This is in contrast to the shifts that occur when scaled images undergo a log polar transformation, which are in the horizontal direction. This vertical shift causes pixels to “fall off” the edge of V1. Those pixels wrap around to the opposite side of V1. Instead of appearing as a simple translation, changes in rotation in images that have undergone a log polar transformation result in a fundamental rearranging of image components. A visualization of the log polar transform is provided in Figure 2.\n\nAll transformations cropping, rotation, and foveation and log-polarization for LPNet are done at the beginning of every epoch. For LPNet, the cropping has a large effect, as the fixation point changes with each crop. To see this, note the difference in the two fixations in Figure 2.\n\n3 EXPERIMENT I: FACES AND OBJECTS\n\nBased on Yin (1969), we first explore the difference in the effect of inversion based on whether a participant is viewing faces (objects of expertise) or objects for which the subject is a novice, so only categorized at the basic level. In that work, Yin found that images of faces produced a significant and large inversion effect, while images of airplanes did not produce a significant inversion effect.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n3.1 EXPERIMENTAL SETUP\n\nWe define expertise in our model as the ability to differentiate between subordinates of basic level categories. When using subordinate level visual stimuli, such as individual face identities, the network has to learn to discriminate between very similar visual stimuli. For example, faces share features and the same overall organization of features. Just as with humans, being able to discriminate very few subordinates demonstrates a low level of expertise with a particular basic level category of visual stimuli. Being able to discriminate visually between many subordinates demonstrates high visual expertise with that category. Our network learned to perform categorization tasks using 4, 8, 16, 32, 64, or 128 categories of either faces or non mono-oriented objects.\n\nOver training, we increase the number of classifications the network makes in order to mimic the increase of visual expertise over time, similar to first knowing only family members, then adding family friends, then pre-K, etc. The network is trained for 40 epochs with 4 category outputs (identities for faces, object categories for the ”novice” network). After 40 epochs, 4 new categories are added. There is an immediate sharp drop in accuracy, but during the next 40 epochs the network learns the 8 categories. This continues for a total of 240 epochs, across 4, 8, 16, 32, 64, and 128 category outputs. Each phase of training is essentially performing pretraining of the network for the next phase of training by learning features that are helpful for discrimination.\n\nDuring training, cropped images are rotated randomly between -15°and 15°. Testing images are presented in two conditions: upright (0°) and inverted (180°). The upright condition provides a baseline so that we can measure the amount the accuracy decreases after inversion. The amount the accuracy drops in the inverted condition is relative to the accuracy of the upright condition, so we report the percent drop in accuracy due to inversion, i.e.,\n\nAcclost = (Accup − Accinv)/Accup\n\n(1)\n\nBy looking at the percent drop between the accuracies for the two testing conditions, we can determine the effect inversion had on the network’s recognition capabilities.\n\nThis experiment includes four configurations of networks and data: (1) CNN with face dataset (2) CNN with object dataset (3) LPNet with face dataset and (4) LPNet with object dataset.\n\n3.2 RESULTS\n\nWe ran all experiments five times and averaged the results. Figure 3 shows the accuracy over training. The first row is for standard CNNs and the second row is LPNet. The effect of the stimuli and the category level of the stimuli, either basic level or subordinate level, is clear. The green line is the training accuracy, the yellow line is the validation accuracy for upright images, and the magenta line is the validation accuracy for inverted images. For the face stimuli, even in early phases of training, there is a performance gap between the upright validation accuracy and the inverted validation accuracy. As the network learns to differentiate more identities, this performance gap continues to increase. When using objects as the training stimuli, the difference in performance between the two validation conditions is overall much smaller, with no apparent gap during the first phase of training. In addition, the size of the gap changes less throughout training.\n\nThe results on the CNN and LPNet have similar trends in that both show performance gaps between the two validation conditions which increase as the number of identities increases. This gap is larger for the CNN, in part because the inverted condition is more difficult for it. With inverted faces, the CNN fails nearly completely on inverted images of faces with an accuracy of approximately 4%. This is because the log polar transform provides some rotation invariance that standard CNNs do not have. Hence LPnet is more representative of human ability on inverted images.\n\nFigure 4 shows the percent accuracy lost as in Eq. 1, i.e., the accuracy lost relative to the upright accuracy, directly measuring how much inverting an image will disrupt performance in any given experiment. In Figure 4, it is clear that image inversion has a significant impact with face stimuli, and a much smaller impact with object stimuli. This mirrors the first conclusion of Yin 1969. One of the categories included in the object experiment is houses. In order to recreate Yin’s experiment, we also plot the percent accuracy lost on just the house category. The percent accuracy lost for houses is much lower than that of faces, but slightly higher than objects. This mirrors the second conclusion of Yin 1969. Figure 4 also shows how the percent accuracy lost on a standard CNN is higher than the\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFace Stimuli\n\nObject Stimuli\n\nN N\nC\n\nt e\nN P\nL\n\nEpochs\n\nEpochs\n\nEpochs\n\nEpochs\n\nFigure 3: Accuracy throughout training on standard CNNs and LPNet for face and object stimuli. The green line is training accuracy, the orange line is accuracy on validation images at 0°, and the magenta line is accuracy on validation images rotated 180°. Shaded regions are +/-1 standard deviation.\n\nCNN\n\nLPNet\n\nNumber of Categories\n\nNumber of Categories\n\nFigure 4: Percent accuracy lost across all phases of training on a standard CNN and LPNet. Shown for faces, car models, and dog breeds (all expert networks), and houses and objects from the basiclevel categorizer.\n\ncorresponding values on LPnet. This is because there is such a significant performance decrease on inverted images; the standard CNN is losing a larger proportion of the upright validation accuracy.\n\n4 EXPERIMENT II: EXPERTISE EFFECTS WITH CARS MODELS AND DOG\n\nBREEDS\n\nIn the previous experiment we saw that the inversion effect for faces was significant and the inversion effect for objects using a network with ability at the basic level was not significant. Here we explore\n\n6\n\n040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val0204060801001200.00.20.40.60.81.0FacesCarsDogsHousesObjects0204060801001200.00.20.40.60.81.0CarsFacesDogsHousesObjectsUnder review as a conference paper at ICLR 2023\n\nexpertise with mono-oriented objects, cars and dogs, to compare the inversion effect of faces to that of mono-oriented objects identified by models with expert level knowledge. We can also determine if the degree of expertise of the network changes the magnitude of inversion effects.\n\n4.1 EXPERIMENT SETUP\n\nTo study expertise in mono-oriented objects, we trained two different sets of networks: one to distinguish the sub-class of car models and one to distinguish the sub-class of dog breeds. Being able to differentiate between different car models (e.g. Toyota Camry, Hyundai Santa Fe) is indicative of a car ”expert”. We again run experiments with both standard CNNs and LPNet.\n\nAside from the data chosen, this experiment follows the same setup and procedure as the previous experiment. We increase the number of identities being differentiated during each phase of training. For car models, our phases of training include 4, 8, 16, 32, 64, and 128 category outputs. The mean number of examples per car model used in our experiments is 151 images. For dogs, our phases of training include 4, 8, 16, 32, 64, and 117 category outputs. This is limited by the number of dog breeds available in ImageNet. These images inherently have more variation in pose (dogs sitting, standing, jumping, rolling around, etc.), so we used all available images from the dog categories of ImageNet, which averages to approximately 1500 images per dog breed.\n\n4.2 RESULTS\n\nThe results for these experiments are shown in Figure 5 with the first row of plots representing results on a standard CNN and the second row representing results on LPNet. For all plots, the validation accuracy gap is much smaller at the beginning of training, but continues to increase as more categories are added. Like the previous experiment, it is much more difficult for the CNN to distinguish inverted images than it is for LPNet, because of the rotation invariance provided by LPNet. Looking just at LPNet plots, the car and dog upright validation accuracies (Figure 5) and the face upright validation accuracies (Figure 3) vary widely. In addition, the number of percentage points lost because of inversion in these experiments also varies. This is because of differences in data that are hard to control for, like more complicated backgrounds for cars and dogs, similar contexts for faces, and amount of data available to train on. Despite these differences, when looking at the percent accuracy lost in Figure 4, it is clear that the network experienced a very similar inversion effect in each expert LPNet network. This is because percent accuracy lost measures how much of the upright validation accuracy was lost due to inversion (how big of an impact inversion had), not the net number of percentage points lost.\n\nDiscriminating between different subordinate categories may become harder or easier for the network depending on the stimulus class itself or the context of the images. However, the percent accuracy lost shows that image inversion affects LPNet trained as an expert on car models or dogs in almost the same way it affects LPNet trained on faces. This is consistent with the Diamond & Carey (1986) results.\n\n5 EXPERIMENT III: RECREATION OF HUMAN EXPERIMENT: COMPLETE\n\nCOMPOSITE PARADIGM\n\nTo further compare network performance more directly to human data, we recreate the complete composite paradigm (Gauthier & Bukach, 2007; Meinhardt et al., 2014) for LPNet with face data and for a standard CNN with face data. The complete composite paradigm is a way to measure holistic face processing. Subjects view two faces and are asked to attend only to the top half of the face. They are then asked to determine if the top halves are the same or different. This is done in four conditions. In congruent trials the answer for the top and bottom images are the same: both the top and bottom halves are the same or both the top and bottom halves are different. Incongruent trials have the same top half of the face with different bottom halves of the face or vice versa. Holistic processing is measured as the difference between congruent and incongruent trials. Figure 6A shows a diagram of the complete composite paradigm from (Meinhardt et al., 2014) and Figure 6B shows example images used in our experiment.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nCar Model Stimuli\n\nDog Breed Stimuli\n\nN N\nC\n\nt e\nN P\nL\n\nEpochs\n\nEpochs\n\nFigure 5: Accuracy throughout training on standard CNNs and LPNet for car model stimuli and dog breed stimuli. LPNet dog breed plot averaged over three runs.\n\nEpochs\n\nEpochs\n\nA\n\nSame\n\nDifferent\n\nB\n\nt\n\nn e\nu r\ng n\no C\n\nt\n\nn e\nu r\ng n\no c\nn I\n\nFigure 6: (A) A figure from (Meinhardt et al., 2014) showing different combinations of facial components included in the complete composite paradigm. (B) Images of whole and mismatched faces used to recreate the complete composite paradigm in our network. Here, ”Same” judgments are on the top half of the face.\n\nWe compare the output from the last convolutional layer of the network for different images and calculate the cosine similarity for a pair of images. The fixation point for the network on the images is between the eyes (as subjects are instructed to attend to the top potion of the face) and three pixels left of center, which comes from data that the first fixation people make when viewing a face is to the left of center (Hsiao & Cottrell, 2008). We perform three trials for each network using three sets of images and average over the trials to get the final cosine similarities. This experiment is done for both upright and inverted images.\n\n8\n\n040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted Val040801201602002400.00.20.40.60.81.0TrainingUpright ValInverted ValUnder review as a conference paper at ICLR 2023\n\nNetwork Trial type\n\nCNN CNN LPNet LPNet\n\nSame Different Same Different\n\nUpright Images |congruent - incongruent |\n\nInverted Images |congruent - incongruent |\n\n0.3086 0.0913 0.2463 0.0848\n\n0.4061 0.0661 0.1983 0.1196\n\nTable 1: Difference in cosine similarity between congruent and incongruent trials for CNN and LPNet.\n\n5.1 RESULTS\n\nIn Gauthier (2007), the holistic processing effect is calculated as the difference between the congruent and incongruent trials for d’ or the hit rate. Because we do not have the data to calculate a hit rate, we instead subtract our cosine similarities between congruent and incongruent trials. As seen in Table 1, both LPNet and the CNN show holistic processing of upright faces with a difference between congruent and incongruent trials being larger than zero. The CNN shows a larger effect, meaning it preforms holistic processing to a greater degree. This helps explain why CNNs preform so poorly on inverted images; they rely on the configural information but do not have any rotation invariance. The CNN has the same performance with inverted faces. LPNet, however, sees a drastic decrease in holistic processing for inverted faces, just as people do [REF].\n\n6 CONCLUSION\n\nWe explored image inversion effects and the impact of visual expertise on performance. By using images of faces, objects, cars, and dogs, we were able to show that LPNet, a convolutional neural network that includes a foveated retina and the log polar mapping from the visual field to V1, can reproduce experimental results of image inversion despite being nominally rotation invariant. We showed that there is a larger effect on performance from inverting images of faces than images of objects, which increases as the number of categories being discriminated increases. We then explored the result that images of mono-oriented objects have an inversion effect between that of faces and objects by showing performance of a house-novice network. Expert networks on monooreinted objects do not show an effect between that of faces and objects. We showed that LPNet trained to distinguish car models or dog breeds showed similar inversion effects to faces. This suggests that inversion effects are not dependent on the stimulus, but rather on the level of visual expertise with the stimulus and on the categorization level of the stimulus (expert or novice viewing subordinate or basic level categories).\n\nImage inversion effects are dependent on the level of expertise because of an expert’s reliance on configural information to do fine grain discrimination. These effects occur with both CNNs and LPNet. LPNet is more realistic to human vision for the inclusion of the log polar transform, which provides scale and rotation invariance. This is seen with inverted images, when CNNs nearly completely fail to perform differentiation while LPNet is still able to perform some differentiation. We have shown that, when using the log polar transform, the inversion of an image causes a rearrangement of features and a loss of configural information.\n\nThe log polar transform approximates the mapping from the visual field to V1, meaning that disruption of configural information occurs before V1 - before the cortex gets the visual input. When LPnet is not an expert, the loss of configural information has a minimal impact on discrimination ability, and it is able to perform the task with only minor performance changes. As LPNet is asked to do more fine grain discrimination between categories, it relies more heavily on the configural information held within the image.\n\nOur results support the hypothesis that the source of the inversion effect in visual expertise, including face expertise, is disruption of configural information at the level of V1.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAnonymous.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255, 2009.\n\nR Diamond and S Carey. Why faces are and are not special: An effect of expertise. Journal of\n\nExperimental Psychology: General, 115(2):107–117, 1986.\n\nMartha J. Farah, James W. Tanaka, and H. Maxwell Drain. What causes the face inversion effect? Journal of Experimental Psychology: Human Perception and Performance, 21:628–634, 1995.\n\nIsabel Gauthier and Cindy Bukach. Should we reject the expertise hypothesis? Cognition, 103:\n\n322–330, 2007.\n\nIsabel Gauthier, Michael Tarr, Adam Anderson, Pawel Skudlarski, and John Gore. Activation of the middle fusiform ’face area’ increases with expertise in recognizing novel objects. Nature neuroscience, 2:568–73, 07 1999.\n\nIsabel Gauthier, Pawel Skudlarskiand John C. Gore, and Adam W. Anderson. Expertise for cars and birds recruits brain areas involved in face recognition. Nature Neuroscience, 3:191–197, 2000.\n\nIsabel Gauthier, Tim Curran, Kim M Curby, and Daniel Collins. Perceptual interference supports a\n\nnon-modular account of face processing. Nature Neuroscience, 6:428–432, 2003.\n\nIsabel Gauthier, Rankin W. McGugin, Jennifer J. Richler, Grit Herzmann, Magen Speegle, and Ana E. Van Gulick. Experience moderates overlap between object and facerecognition, suggesting a common ability. Journal of Vision, 14:1–12, 2014.\n\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. 2016 IEEE\n\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016.\n\nJanet Huiwen Hsiao and Garrison Cottrell. Two fixations suffice in face recognition. Psychological\n\nscience, 19:998–1006, 2008.\n\nCorentin Jacques, Olivier d’Arripe, and Bruno Rossion. The time course of the inversion effect\n\nduring individual face discrimination. Journal of Vision, 7, 6 2007.\n\nR. Jenkins, A. J. Dowsett, and A. M. Burton. How many faces do people know? Proceedings of the\n\nRoyal Society B: Biological Sciences, 285), 2018.\n\nMing Jiang, Shengsheng Huang, Juanyong Duan, and Qi Zhao. Salicon: Saliency in context. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1072–1080, 2015.\n\nN. Kanwisher, F. Tong, and K. Nakayama. The effect of face inversion on the human fusiform face\n\narea. Cognition, 68, 1998.\n\nNancy Kanwisher, Josh McDermott, and Marvin M. Chun. The fusiform face area: A module in human extrastriate cortex specialized for face perception. Journal of Neuroscience, 17(11): 4302–4311, 1997.\n\nG ̈unter Meinhardt, Bozana Meinhardt-Injac, and Malte Persike. The complete design in the composite face paradigm: role of response bias, target certainty, and feedback. Frontiers in human neuroscience, 2014.\n\nJ.R. Polimeni, M. Balasubramanian, and E.L. Schwartz. Multi-area visuotopic map complexes in\n\nmacaque striate and extra-striate cortex. Vision Research, 2006.\n\nC. Rezlescu, T. Susilo, Jeremy B. Wilmer, and A. Caramazza. The inversion, part-whole, and composite effects reflect distinct perceptual mechanisms with varied relationships to face recognition. Journal of Experimental Psychology: Human Perception and Performance, 43:1961–1973, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJennifer J. Richler, Olivia S. Cheung, and Isabel Gauthier. Holistic processing predicts face recog-\n\nnition. Psychological Science, 22:464–471, 2011.\n\nEleanor Rosch, Carolyn B Mervis, Wayne D Gray, David M Johnson, and Penny Boyes-Braem.\n\nBasic objects in natural categories. Cognitive Psychology, 8(3):382–439, 1976.\n\nPanqu Wang, Isabel Gauthier, and Garrison Cottrell. Experience matters: Modeling the relationship between face and object recognition. Proceedings of the 36th Annual Conference of the Cognitive Science Society, 2014.\n\nLinjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. A large-scale car dataset for finegrained categorization and verification. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3973–3981, 2015.\n\nRobert K Yin. Looking at upside-down faces. Journal of Experimental Psychology, 81:141–145,\n\n1969.\n\n11",
    "reference": "# Summary Of The Paper\n\nThrough of a set of \"machine psychophysics\" experiments, the authors probe on the image inversion effects comparing faces and other objects with a family of neural networks that are \"classical\", and other that incorporate both foveation + log-polar mapping to simulate visual processes up to V1. Authors find that their log-polar + foveation neural network model explains the image inversion effect while modern CNN's do not.\n\n# Strength And Weaknesses\n\nSee below for Summary of Review. TLDR: I think the main strength of this paper is the scientific question and trying to understand what are the computational mechanisms of human perception that when simulated can disrupt face perception. The main weakness of this paper is that I do not understand the premise of using a foveated field of view to account for holistic processing of faces when faces can be accurately identified when they also lie within our fovea (!)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper starts of quite clear in the first 2-3 pages, but later has a sense of being rushed for the last 2 pages (there are even some typos of \"[REF]\") in the manuscript. \n\nThe reproducibility aspect however seems very reasonable, and it should be able to do so with the details provided in the paper. Kudos as well to the authors for using error bars in these experiments!\n\n# Summary Of The Review\n\nI think the topic of this paper is quite exciting as it can greatly advance out understanding of face perception in humans, and overall let us know what makes faces so special compared to everyday objects (and this could in turn be helpful for machine vision). \n\nHowever: while the experiments in this paper are done with great precision and detail (error bars, optimizers are well documented to train the networks presumably trained from scratch, multiple classes & datasets), I think the premise of explaining the image inversion effect through early staged of visual processing is quite bold, (but exciting!), however I do not know what foveation has to do with this. If humans can recognize faces without foveation, then how does that results fit into this methodology? I could argue the same for log-polar mapping.\n\nPerhaps a missing experiment that remains to be done here is to re-do the experiments with the faces shrunken to the foveal area of the log-polar mapping and later evaluate the outcomes? Quite frankly, I'm also a bit confused, as I'm not sure how to make sense of these results even by having the log-polar network as a control. Maybe there is a confounding variable in this analysis, but I can't wrap my head around what the confounding variable is or could be. \n\nIt occurs to me that maybe some additional experiments could be done with scenes, but I'm not sure what is to be proven: perhaps that there is no image inversion effects in scenes? (I'm arguing for scenes here because that is a believable scenario where I can see having a model that incorporates a wide field of view through a log-polar transform be plausible -- similar to Deza & Konkle, ArXiv 2020.)\n\nMissing references:\n* Brain-like functional specialization emerges spontaneously in deep neural networks, by Dobbs et al. Science Advances 2022.\n(In general several works from Dobbs that involve training neural networks on faces and objects and comparing these resuilts)\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTHE SURPRISING COMPUTATIONAL POWER OF NONDETERMINISTIC STACK RNNS\n\nBrian DuSell and David Chiang Department of Computer Science and Engineering University of Notre Dame {bdusell1,dchiang}@nd.edu\n\nABSTRACT\n\nTraditional recurrent neural networks (RNNs) have a fixed, finite number of memory cells. In theory (assuming bounded range and precision), this limits their formal language recognition power to regular languages, and in practice, RNNs have been shown to be unable to learn many context-free languages (CFLs). In order to expand the class of languages RNNs recognize, prior work has augmented RNNs with a nondeterministic stack data structure, putting them on par with pushdown automata and increasing their language recognition power to CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic CFLs), but in this paper, we show that nondeterminism and the neural controller interact to produce two more unexpected abilities. First, the nondeterministic stack RNN can recognize not only CFLs, but also many non-context-free languages. Second, it can recognize languages with much larger alphabet sizes than one might expect given the size of its stack alphabet. Finally, to increase the information capacity in the stack and allow it to solve more complicated tasks with large alphabet sizes, we propose a new version of the nondeterministic stack that simulates stacks of vectors rather than discrete symbols. We demonstrate perplexity improvements with this new model on the Penn Treebank language modeling benchmark.\n\n1\n\nINTRODUCTION\n\nStandard recurrent neural networks (RNNs), including simple RNNs (Elman, 1990), GRUs (Cho et al., 2014), and LSTMs (Hochreiter & Schmidhuber, 1997), rely on a fixed, finite number of neurons to remember information across timesteps. When implemented with finite precision, they are theoretically just very large finite automata, restricting the class of formal languages they recognize to regular languages (Kleene, 1951). In practice, too, LSTMs cannot learn simple non-regular languages such as {w#wR | w ∈ {0, 1}∗} (DuSell & Chiang, 2020). To increase the theoretical and practical computational power of RNNs, past work has proposed augmenting RNNs with stack data structures (Sun et al., 1995; Grefenstette et al., 2015; Joulin & Mikolov, 2015; DuSell & Chiang, 2020), inspired by the fact that adding a stack to a finite automaton makes it a pushdown automaton (PDA), raising its recognition power to context-free languages (CFLs).\n\nRecently, we proposed the nondeterministic stack RNN (NS-RNN) (DuSell & Chiang, 2020) and renormalizing NS-RNN (RNS-RNN) (DuSell & Chiang, 2022), augmenting an LSTM with a differentiable data structure that simulates a real-time nondeterministic PDA. (The PDA is real-time in that it executes exactly one transition per input symbol scanned, and it is nondeterministic in that it executes all possible sequences of transitions.) This was in contrast to prior work on stack RNNs, which exclusively modeled deterministic stacks, theoretically limiting such models to deterministic CFLs (DCFLs), which are a proper subset of CFLs (Sipser, 2013). The RNS-RNN proved more effective than deterministic stack RNNs at learning both nondeterministic and deterministic CFLs.\n\nIn practical terms, giving RNNs the ability to recognize context-free patterns may be beneficial for modeling natural language, as syntax exhibits hierarchical structure; nondeterminism in particular is necessary for handling the very common phenomenon of syntactic ambiguity. However, the RNSRNN’s reliance on a PDA may still render it inadequate for practical use. For one, not all phenomena in human language are context-free, such as cross-serial dependencies. Secondly, the RNS-RNN’s\n\n1\n\nPublished as a conference paper at ICLR 2023\n\noutput (cid:8)\n\ncontroller (cid:8)\n\n· · ·\n\nstack (cid:8) input (cid:8)\n\nyt−1\n\nht−1\n\nxt−1\n\nrt−2\n\n· · ·\n\nat−1 rt−1\n\nst−1\n\nyt\n\nht\n\nxt\n\nyt+1\n\nht+1\n\nxt+1\n\nat rt\n\nst\n\n· · ·\n\nat+1\n\n· · ·\n\nFigure 1: Conceptual diagram of the RNN controller-stack interface, unrolled across a portion of time. The LSTM memory cell ct is not shown.\n\ncomputational cost restricts it to small stack alphabet sizes, which is likely insufficient for storing detailed lexical information. In this paper, we show that the RNS-RNN is surprisingly good at overcoming both difficulties. Whereas an ordinary weighted PDA must use the same transition weights for all timesteps, the RNS-RNN can update them based on the status of ongoing nondeterministic branches of the PDA. This means it can coordinate multiple branches in a way a PDA cannot—for example, to simulate multiple stacks, or to encode information in the distribution over stacks.\n\nOur contributions in this paper are as follows. We first prove that the RNS-RNN can recognize all CFLs and intersections of CFLs despite restrictions on its PDA transitions. We show empirically that the RNS-RNN can model some non-CFLs; in fact it is the only stack RNN able to learn {w#w | w ∈ {0, 1}∗}, whereas a deterministic multi-stack RNN cannot. We then show that, surprisingly, an RNS-RNN with only 3 stack symbol types can learn to simulate a stack of no fewer than 200 symbol types, by encoding them as points in a vector space related to the distribution over stacks. Finally, in order to combine the benefits of nondeterminism and vector representations, we propose a new model that simulates a PDA with a stack of vectors instead of discrete symbols. We show that the new vector RNS-RNN outperforms the original on the Dyck language and achieves better perplexity than other stack RNNs on the Penn Treebank. Our code is publicly available.1\n\n2 STACK RNNS\n\nIn this paper, we examine two styles of stack-augmented RNN, using the same architectural framework as in our previous work (DuSell & Chiang, 2022) (Fig. 1). In both cases, the model consists of an LSTM, called the controller, connected to a differentiable stack. At each timestep, the stack receives actions from the controller (e.g. to push and pop elements). The stack simulates those actions and produces a reading vector, which represents the updated top element of the stack. The reading is fed as an extra input to the controller at the next timestep. The actions and reading consist of continuous and differentiable weights so the whole model can be trained end-to-end with backpropagation; their form and meaning vary depending on the particular style of stack.\n\nWe assume the input w = w1 · · · wn is encoded as a sequence of vectors x1, · · · , xn. The LSTM’s memory consists of a hidden state ht and memory cell ct (we set h0 = c0 = 0). The controller computes the next state (ht, ct) given the previous state, input vector xt, and stack reading rt−1:\n\n(ht, ct) = LSTM\n\n(ht−1, ct−1),\n\n(cid:18)\n\n(cid:20) xt rt−1\n\n(cid:21)(cid:19)\n\n.\n\nThe hidden state generates the stack actions at and logits yt for predicting the next word wt+1. The previous stack and new actions generate a new stack st, which produces a new reading rt:\n\nat = ACTIONS(ht)\n\nyt = Whyht + bhy\n\nst = STACK(st−1, at)\n\nrt = READING(st).\n\nEach style of stack differs only in the definitions of ACTIONS, STACK, and READING.\n\n2.1 SUPERPOSITION STACK RNN\n\nWe start by describing a stack with deterministic actions—the superposition stack of Joulin & Mikolov (2015)—which we include because it was one of the best-performing stack RNNs we\n\n1https://github.com/bdusell/nondeterministic-stack-rnn\n\n2\n\nPublished as a conference paper at ICLR 2023\n\ninvestigated previously (2020; 2022). The superposition stack simulates a combination of partial stack actions by computing three new, separate stacks: one with all cells shifted down (push), kept the same (no-op), and shifted up (pop). The new stack is an element-wise interpolation (“superposition”) of these three stacks. The stack elements are vectors, and at = (at, vt), where the vector at is a probability distribution over the three stack operations. The push operation pushes vector vt, which can be learned or set to ht. The stack reading is the top vector element.\n\n2.2 RENORMALIZING NONDETERMINISTIC STACK RNN\n\nThe main focus of this paper is the renormalizing nondeterministic stack RNN (RNS-RNN) (DuSell & Chiang, 2022). The RNS-RNN’s stack module is a simulation of a real-time weighted PDA, complete with its own finite state machine and stack. Let Q be the set of states and Γ be the stack alphabet of the PDA. The initial PDA state is q0 ∈ Q, and the initial stack is ⊥ ∈ Γ. The PDA’s computation is governed by weighted transitions that manipulate its state and stack contents; a valid sequence of transitions is called a run. The PDA is nondeterministic in that all possible runs are simulated in parallel. Each run has a weight, which is the product of the weights of its transitions.\n\nThe ACTIONS emitted by the controller are the PDA’s transition weights. The weights at t, denoted ∆[t], are computed as ∆[t] = exp(Waht + ba). Let q, r ∈ Q and u, v ∈ Γ∗, and let ∆[t][q, u → r, v] denote the weight, at timestep t, of popping u from the stack, pushing v, and transitioning to state r if the previous state was q and the previous stack top was u. Transitions have one of three forms, where x, y ∈ Γ: ∆[t][q, x → r, xy] (push y), ∆[t][q, x → r, y] (replace x with y), and ∆[t][q, x → r, ε] (pop x). We say that transitions limited to these three forms are in restricted form.\n\nThe READING is the marginal distribution, over all runs, of each pair (r, y) ∈ Q × Γ, where r is the current PDA state, and y is the top stack symbol. Let τi be a PDA transition, let π = τ1 · · · τt be a PDA run, and let ψ(π) = (cid:81)t i=1 ∆[i][τi] be the weight of run π. Let π ⇝ t, r, y mean that run π ends at timestep t in state r with y on top of the stack. The stack reading rt ∈ R|Q|·|Γ| is defined as (cid:80)\n\nrt[(r, y)] =\n\n(cid:80)\n\nr′,y′\n\nπ⇝t,r,y ψ(π) (cid:80)\n\nπ⇝t,r′,y′ ψ(π)\n\n.\n\n(1)\n\nEquation (1) is sufficient for describing the RNS-RNN mathematically, but it sums over an exponential number of runs, so the RNS-RNN relies on a dynamic programming algorithm to compute it in O(n3) time (Lang, 1974). The rest of this section describes this algorithm and may safely be skipped unless the reader is interested in these implementation details or in Eqs. (9) to (11).\n\nThe algorithm uses a tensor of weights called the stack WFA, so named because it can be viewed as a weighted finite automaton (WFA) that encodes the weighted language of all possible stacks the PDA can have at time t. Each WFA state is of the form (i, q, x), representing a configuration where the PDA is in state q with stack top x at time i. For any WFA transition from (i, q, x) to (t, r, y), its weight is equal to the sum of the weights of all runs that bring the PDA from configuration (i, q, x) to (t, r, y) (possibly over multiple timesteps) without modifying x, with the net effect of putting a single y on top of it. The tensor containing the stack WFA’s transition weights, also called inner weights, is denoted γ, and elements are written as γ[i → t][q, x → r, y]. For 1 ≤ t ≤ n − 1 and −1 ≤ i ≤ t − 1,\n\nγ[−1 → 0][q, x → r, y] = I[q = q0 ∧ x = ⊥ ∧ r = q0 ∧ y = ⊥]\n\nγ[i → t][q, x → r, y] = I[i = t−1] ∆[t][q, x → r, xy] (cid:88)\n\n+\n\nγ[i → t−1][q, x → s, z] ∆[t][s, z → r, y]\n\n(2) (3)\n\ninit. push\n\nrepl.\n\ns,z\n\nt−2 (cid:88)\n\n(cid:88)\n\n+\n\nγ[i → k][q, x → u, y] γ′[k → t][u, y → r] pop\n\nγ′[k → t][u, y → r] =\n\nu\n\nk=i+1 (cid:88)\n\nγ[k → t−1][u, y → s, z] ∆[t][s, z → r, ε]\n\n(0 ≤ k ≤ t − 2). (4)\n\ns,z\n\nThe RNS-RNN sums over all runs using a tensor of forward weights denoted α, where the element α[t][r, y] is the total weight of reaching the stack WFA state (t, r, y). These weights are normalized\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nto get the final stack reading at t.\n\nα[−1][r, y] = I[r = q0 ∧ y = ⊥]\n\nα[t][r, y] =\n\nt−1 (cid:88)\n\n(cid:88)\n\ni=−1\n\nq,x\n\nα[i][q, x] γ[i → t][q, x → r, y]\n\n(0 ≤ t ≤ n − 1)\n\nrt[(r, y)] =\n\nα[t][r, y]\n\n(cid:80)\n\nr′,y′ α[t][r′, y′]\n\n.\n\n(5)\n\n(6)\n\n(7)\n\nWe have departed slightly from the original definitions of γ and α (DuSell & Chiang, 2022), for two reasons: (1) to implement an asymptotic speedup by a factor of |Q| (Butoi et al., 2022), and (2) to fix a peculiarity with the behavior of the initial ⊥. See Appendix A for details.\n\n3 RECOGNITION POWER\n\nIn this section, we investigate the power of RNS-RNNs as language recognition devices, proving that they can recognize all CFLs and all intersections of CFLs. These results hold true even when the RNS-RNN is run in real time (one timestep per input, with one extra timestep to read EOS). Although Siegelmann & Sontag (1992) showed that even simple RNNs are as powerful as Turing machines, this result relies on assumptions of infinite precision and unlimited extra timesteps, which generally do not hold true in practice. The same limitation applies to the neural Turing machine (Graves et al., 2014), which, when implemented with finite precision, is no more powerful than a finite automaton, as its tape does not grow with input length. Previously, Stogin et al. (2020) showed that a variant of the superposition stack is at least as powerful as real-time DPDAs. Here we show that RNS-RNNs recognize a much larger superset of languages.\n\nFor this section only, we allow parameters to have values of ±∞, to enable the controller to emit probabilities of exactly zero. Because we use RNS-RNNs here for accepting or rejecting strings (whereas in the rest of the paper, we only use them for predicting the next symbol), we start by providing a formal definition of language recognition for RNNs (cf. Chen et al., 2018).\n\nDefinition 1. Let N be an RNN controller, possibly augmented with one of the stack modules above. Let ht ∈ Rd be the hidden state of N after reading t symbols, and let σ be the logistic sigmoid function. We say that N recognizes language L if there is an MLP layer y = σ(W2 σ(W1h|w|+1 + b1) + b2) such that, after reading w · EOS, we have y > 1\n\n2 iff w ∈ L.\n\nThe question of whether RNS-RNNs can recognize all CFLs can be reduced to the question of whether real-time PDAs with transitions in restricted form (Section 2.2) can recognize all CFLs. The real-time requirement does not reduce the power of PDAs (Greibach, 1965), but what about restricted form? We prove that it does not either, and so RNS-RNNs can recognize all CFLs.\n\nProposition 1. For every context-free language L, there exists an RNS-RNN that recognizes L.\n\nProof sketch. We construct a CFG for L and convert it into a modified Greibach normal form, called 2-GNF, which we can convert into a PDA P whose transitions are in restricted form. Then we construct an RNS-RNN that emits, at every timestep, weight 1 for transitions of P and 0 for all others. Then y > 1\n\n2 iff the PDA ends in an accept configuration. See Appendix B.1 for details.\n\nProposition 2. For every finite set of context-free languages L1, . . . , Lk over the same alphabet Σ, there exists an RNS-RNN that recognizes L1 ∩ · · · ∩ Lk.\n\nProof sketch. Without loss of generality, assume k = 2. Let P1 and P2 be PDAs recognizing L1 and L2, respectively. We construct a PDA P that uses nondeterminism to simulate P1 or P2, but the controller can query P1 and P2’s configurations, and it can set y > 1 2 iff P1 and P2 both end in accept configurations. See Appendix B.2 for details.\n\nSince the class of languages formed by the intersection of k CFLs is a proper superset of the class formed by the intersection of (k − 1) CFLs (Liu & Weiner, 1973), this means that RNS-RNNs are considerably more powerful than nondeterministic PDAs.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\n4 NON-CONTEXT-FREE LANGUAGES\n\nWe now explore the ability of stack RNNs to recognize non-context-free phenomena with a language modeling task on several non-CFLs. Each non-CFL, which we describe below, can be recognized by a real-time three-stack automaton (see Appendix C for details). We also include additional nonCFLs in Appendix C.\n\nw#wR#w The language {w#wR#w | w ∈ {0, 1}∗}. w#w The language {w#w | w ∈ {0, 1}∗}. ww′ The language {ww′ | w ∈ {0, 1}∗ and w′ = φ(w)}, where φ is the homomorphism φ(0) =\n\n2, φ(1) = 3.\n\nww The language {ww | w ∈ {0, 1}∗}.\n\nThe above languages all include patterns like w · · · w, which are known in linguistics as cross-serial dependencies. In Swiss German (Shieber, 1985), the two w’s are distinguished by part-of-speech (a sequence of nouns and verbs, respectively), analogous to ww′. In Bambara (Culy, 1985), the two w’s are the same, but separated by a morpheme o, analogous to w#w.\n\nWe follow our previous experimental framework (DuSell & Chiang, 2022). If L is a language, let Ll be the set of all strings in L of length l. To sample a string w ∈ L, we first uniformly sample a length l from [40, 80], then sample uniformly from Ll (we avoid sampling lengths for which Ll is empty). So, the distribution from which w is sampled is\n\npL(w) =\n\n1 (cid:12){l ∈ [40, 80] | Ll ̸= ∅}(cid:12) (cid:12) (cid:12)\n\n1 |L|w||\n\n.\n\nWe require models to predict an EOS symbol at the end of each string, so each language model M defines a probability distribution pM (w). Let the per-symbol cross-entropy of a probability distribution p on a set of strings S, measured in nats, be defined as\n\nH(S, p) =\n\n− (cid:80) (cid:80)\n\nw∈S log p(w) w∈S(|w| + 1)\n\n.\n\nThe +1 in the denominator accounts for the fact that the model must predict EOS. Since we know the exact distribution from which the data is sampled (for each non-CFL above, |L|w|| can be computed directly from |w|), we can evaluate model M by measuring the cross-entropy difference between the learned and true distributions, or H(S, pM ) − H(S, pL). Lower is better, and 0 is optimal.\n\nWe compare five architectures, each of which consists of an LSTM controller connected to a different type of data structure. We include a bare LSTM baseline (“LSTM”). We also include a model that pushes learned vectors of size 10 to a superposition stack (“Sup. 10”), and another that pushes the controller’s hidden state (“Sup. h”). Since each of these languages can be recognized by a threestack automaton, we also tested a model that is connected to three independent instances of the superposition stack, each of which has vectors of size 3 (“Sup. 3-3-3”). Finally, we include an RNSRNN with |Q| = 3 and |Γ| = 3, where |Q| = 3 is sufficient for the model to be able to simulate at least three different stacks. In all cases, the LSTM controller has one layer and 20 hidden units. We encoded all input symbols as one-hot vectors.\n\nBefore each training run, we sampled a training set of 10,000 examples and a validation set of 1,000 examples from pL. For each language and architecture, we trained 10 models and report results for the model with the lowest cross-entropy difference on the validation set. For each language, we sampled a single test set that was reused across all training runs. Examples in the test set vary in length from 40 to 100, with 100 examples sampled uniformly from Ll for each length l. Additional training details can be found in Appendix D.\n\nWe show the cross-entropy difference on the validation and test sets in Fig. 2. We show results for additional non-CFLs in Appendix E. Strings in w#wR#w contain two hints to facilitate learning: explicit boundaries for w, and extra timesteps in the middle, which simplify the task of transferring symbols between stacks (for details, compare the solutions for w#wR#w and w#w described in Appendix C). Only models that can simulate multiple stacks, Sup. 3-3-3 and RNS 3-3, achieved\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nLSTM\n\nSup. 10\n\nSup. 3-3-3\n\nSup. h\n\nRNS 3-3\n\nw#wR#w\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100 120 140\n\nw#w\n\n100\n\nww′\n\n0\n\n50\n\n150\n\n200\n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n140\n\nww\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n0.4\n\n0.2\n\n0\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0.3\n\n0.2\n\n0.1\n\n0\n\nw#wR#w\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n100\n\nw#w\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n100\n\nww′\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n100\n\nww\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n1\n\n0.5\n\n0\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n0.4\n\n0.2\n\n0\n\n0.4\n\n0.2\n\n0\n\n0\n\n50\n\n100 Epoch\n\n150\n\n200\n\n40\n\n50\n\n60\n\n80\n\n90\n\n100\n\n70 Length\n\nFigure 2: Performance on non-context-free languages. Cross-entropy difference in nats, on the validation set by epoch (left), and on the test set by string length (right). Each line is the best of 10 runs, selected by validation perfomance. On w#wR#w, which has both explicit boundaries and extra timesteps in the middle, only multi-stack models (Sup. 3-3-3 and RNS 3-3) achieved optimal cross-entropy. On w#w and ww′, which have no extra timesteps, only RNS 3-3 did.\n\noptimal cross-entropy here, although RNS 3-3 did not generalize well on lengths not seen in training. Only RNS 3-3 effectively learned w#w and ww′. This suggests that although multiple deterministic stacks are enough to learn to copy a string given enough hints in the input (explicit boundaries, extra timesteps for computation), only the nondeterministic stack succeeds when the number of hints is reduced (no extra timesteps). No stack models learned to copy a string without explicit boundaries (ww), suggesting a nondeterministic multi-stack model (as opposed to one that uses nondeterminism to simulate multiple stacks) may be needed for such patterns.\n\n5 CAPACITY\n\nIn this section, we examine how much information each model can transmit through its stack. Consider the language {w#wR | w ∈ Σ∗}. It can be recognized by a real-time PDA with |Γ| = 3 when |Σ| = 2, but not when |Σ| > 2, as there is always a sufficiently long w such that there are more possible w’s than possible PDA configurations upon reading the #. Similarly, because all the neural stack models we consider here are also real-time, we expect that they will be unable to model context-free languages with sufficiently large alphabets. This is especially relevant to natural languages, which have very large vocabulary sizes.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nNeural networks can encode large sets of distinct types in compact vector representations; in fact, Schwartz et al. (2018) and Peng et al. (2018) showed connections between the vector representations in RNNs and the states of WFAs. However, since the RNS-RNN simulates a discrete stack, it might struggle on tasks that require it to store strings over alphabets with sizes greater than |Γ|. On the other hand, a model that uses a stack of vectors, like the superposition stack, might model such languages more easily by representing each symbol type as a different cluster of points in a vector space. Here, we make the surprising finding that the RNS-RNN can vastly outperform the superposition stack even for large alphabets, though not always. In addition, to see if we can combine the benefits of nondeterminism with the benefits of vector representations, we propose a new variant of the RNSRNN that models a PDA with a stack of discrete symbols augmented with vectors.\n\n5.1 VECTOR RNS-RNN\n\nThe RNS-RNN is computationally expensive for large |Γ|. To address this shortcoming, we propose a new variant, the Vector RNS-RNN (VRNS-RNN), that uses a stack whose elements are symbols drawn from Γ and augmented with vectors of size m, which it can use to encode large alphabets. Its time and space complexity scale only linearly with m. Now, each PDA run involves a stack of elements in Γ×Rm. We assume the initial stack consists of the element (⊥, v0), where v0 = σ(wv), and wv is a learned parameter. The stack operations now have the following semantics:\n\nPush q, x → r, y If q is the current state and (x, u) is on top of the stack, go to state r and push\n\n(y, vt) with weight ∆[t][q, x → r, xy], where vt = σ(Wvht + bv).\n\nReplace q, x → r, y If q is the current state and (x, u) is on top of the stack, go to state r and replace (x, u) with (y, u) with weight ∆[t][q, x → r, y]. Note that we do not replace u with vt; we replace the discrete symbol only and keep the vector the same. When x = y, this is a no-op.\n\nPop q, x → r If q is the current state and (x, u) is on top of the stack, go to state r and remove\n\n(x, u) with weight ∆[t][q, x → r, ε], uncovering the stack element beneath.\n\nLet v(π) denote the top stack vector at the end of run π. The stack reading rt ∈ R|Q|·|Γ|·m now includes, for each (r, y) ∈ Q × Γ, an interpolation of v(π) for every run π ⇝ t, r, y, normalized by the weight of all runs.\n\nrt[(r, y, j)] =\n\n(cid:80)\n\n(cid:80)\n\nπ⇝t,r,y ψ(π) v(π)[j] π⇝t,r′,y′ ψ(π)\n\n(cid:80)\n\n(8)\n\nr′,y′ We compute the denominator using γ and α as before. To compute the numerator, we compute a new tensor ζ which stores ψ(π) v(π). For 1 ≤ t ≤ n − 1 and −1 ≤ i ≤ t − 1,\n\nζ[−1 → 0][q, x → r, y] = I[q = q0 ∧ x = ⊥ ∧ r = q0 ∧ y = ⊥] v0\n\nζ[i → t][q, x → r, y] = I[i = t−1] ∆[t][q, x → r, xy] vt\n\n(cid:88)\n\n+\n\ns,z\n\nζ[i → t−1][q, x → s, z] ∆[t][s, z → r, y]\n\n(9) (10)\n\ninit.2 push\n\nrepl.\n\nt−2 (cid:88)\n\n(cid:88)\n\n+\n\nk=i+1\n\nu\n\nζ[i → k][q, x → u, y] γ′[k → t][u, y → r].\n\npop\n\nWe compute α as before, and we compute the normalized stack reading rt as follows.\n\nrt[(r, y, j)] =\n\nηt[r, y][j] r′,y′ α[t][r′, y′]\n\n(cid:80)\n\nηt[r, y] =\n\nt−1 (cid:88)\n\n(cid:88)\n\ni=−1\n\nq,x\n\nα[i][q, x] ζ[i → t][q, x → r, y]\n\n(11)\n\n5.2 EXPERIMENTS\n\nWe evaluate models using cross-entropy difference as in Section 4. We express each language L as a PCFG, using the same PCFG definitions as in prior work (DuSell & Chiang, 2020), but modified\n\n2Our code and experiments implement ζ[−1 → 0][q, x → r, y] = v0 instead due to a mistake found late in the publication process. Consequently, in Eq. (11), runs can start with any (r, y) ∈ Q × Γ in the numerator, but only (q0, ⊥) in the denominator. Empirically the VRNS-RNN still appears to work as expected.\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nLSTM VRNS 1-1-3\n\nSup. 3 VRNS 2-1-3\n\nRNS 1-3 VRNS 2-3-3\n\nRNS 2-3\n\nw#wR\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n2 1.75 1.5 1.25 1\n0.75 0.5 0.25 0\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\nDyck\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nAlphabet Size k\n\nAlphabet Size k\n\nFigure 3: Mean cross-entropy difference on the validation set vs. input alphabet size. Contrary to expectation, RNS 2-3, which models a discrete stack of only 3 symbol types, learns to solve w#wR with 200 symbol types more reliably than models with stacks of vectors. On the more complicated Dyck language, vector stacks perform best, with our newly proposed VRNS-RNN performing best.\n\n0.6 0.4 0.2 0\n−0.2 −0.4\n\n−0.4 −0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 4: When RNS 2-3 is run on w#wR with 40 symbol types, the stack readings are as visualized above. The readings are 6-dimensional vectors, projected down to 2 dimensions using PCA. The color of each point represents the top stack symbol. Points corresponding to the same symbol type cluster together, indicating the RNS-RNN has learned to encode symbols as points in the 5dimensional simplex. The disorganized points in the middle are from the first and last timesteps of the second half of the string, which appear to be irrelevant for prediction.\n\nto include k symbol types. In order to sample from and compute pL(w), we used the same sampling and parsing techniques as before (DuSell & Chiang, 2020). We tested the information capacity of each model on two DCFLs, varying their alphabet size k from very small to very large.\n\nw#wR The language {w#wR | w ∈ {0, 1, · · · , k − 1}∗}. Dyck The language of strings over the alphabet {(1, )1, (2, )2, · · · , (k, )k} where all brackets are\n\nproperly balanced and nested in pairs of (i and )i.\n\nWe compare four types of architecture, including an LSTM baseline (“LSTM”), and a superposition stack that pushes learned vectors of size 3 (“Sup. 3”). We use the notation “RNS |Q|-|Γ|” for RNS-RNNs, and “VRNS |Q|-|Γ|-m” for VRNS-RNNs. All details of the controller and training procedure are the same as in Section 4. We varied the alphabet size k from 2 to 200 in increments of 40. For each task, architecture, and alphabet size, we ran 10 random restarts.\n\nIn Fig. 3, for each task, we show the mean cross-entropy difference on the validation set as a function of alphabet size; we provide plots of the best performance in Appendix F. On w#wR, the singlestate RNS 1-3, and even VRNS 1-1-3 and Sup. 3, struggled for large k. Only the multi-state models RNS 2-3, VRNS 2-1-3, and VRNS 2-3-3 show a clear advantage over the LSTM. Surprisingly, RNS 2-3, which models a discrete stack alphabet of only size 3, attained the best performance on large alphabets; in Fig. 7, it is the only model capable of achieving optimal cross-entropy on all alphabet sizes. On the Dyck language, a more complicated DCFL, the model rankings are as expected: vector stacks (Sup. 3 and VRNS) performed best, with the largest VRNS model performing best. RNSRNNs still show a clear advantage over the LSTM, but not as much as vector stack RNNs.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Validation and test perplexity on the Penn Treebank of the best of 10 random restarts for each architecture. The model with the best test perplexity is our new VRNS-RNN when it combines a modest amount of nondeterminism (3 states and 3 stack symbols) with vectors of size 5.\n\nModel\n\nVal. ↓\n\nTest ↓\n\nLSTM, 256 units Sup. (push hidden), 247 units Sup. (push learned), |vt| = 22 RNS 1-29 RNS 2-13 RNS 4-5 VRNS 1-1-256 VRNS 1-1-32 VRNS 1-5-20 VRNS 2-3-10 VRNS 3-3-5\n\n129.99 124.99 125.68 131.17 128.97 126.06 130.60 124.49 128.35 129.30 124.71\n\n125.90 121.05 120.74 128.11 122.76 120.19 126.70 120.45 124.63 124.03 120.12\n\nIf RNS 2-3 has only 3 symbol types at its disposal, how can it succeed on w#wR for large k? Recall that rt is a vector that represents a probability distribution over Q × Γ. Perhaps the RNS-RNN, via rt, represents symbol types as different clusters of points in R|Q|·|Γ|. To test this hypothesis, we selected the RNS 2-3 model with the best validation performance on w#wR for k = 40 and evaluated it on 100 samples drawn from pL. For each symbol between # and EOS, we extracted the stack reading vector computed just prior to predicting that symbol. Aggregating over all 100 samples, we reduced the stack readings to 2 dimensions using principal component analysis. We plot them in Fig. 4, labeling each point according to the symbol type to be predicted just after the corresponding stack reading. Indeed, we see that stack readings corresponding to the same symbol cluster together, suggesting that the model is orchestrating the weights of different runs in a way that causes the stack reading to encode different symbol types as points in the 5-dimensional simplex. We show heatmaps of actual reading vectors in Appendix G.\n\n6 NATURAL LANGUAGE MODELING\n\nWe now examine how stack RNNs fare on natural language modeling, as the combination of nondeterminism and vector representations in the VRNS-RNN may prove beneficial. Following our prior work (DuSell & Chiang, 2022), we report perplexity on the Penn Treebank as preprocessed by Mikolov et al. (2011). We used the same LSTM and superposition stack baselines, and various sizes of RNS-RNN and VRNS-RNN. The controller has one layer and, unless otherwise noted, 256 hidden units. For each architecture, we trained 10 random restarts and report results for the model with the best validation perplexity. Appendix H has additional details.\n\nWe show results in Table 1. Most stack RNNs achieved better test perplexity than the LSTM baseline. The best models are those that simulate more nondeterminism (VRNS when |Q| = 3 and |Γ| = 3, and RNS when |Q| = 4 and |Γ| = 5). Although the superposition stack RNNs outperformed the LSTM baseline, it is the combination of both nondeterminism and vector embeddings (VRNS 3-3-5) that achieved the best performance, combining the ability to process syntax nondeterministically with the ability to pack lexical information into a vector space on the stack.\n\n7 CONCLUSION\n\nWe showed that the RNS-RNN (DuSell & Chiang, 2022) can recognize all CFLs and a large class of non-CFLs, and it can even learn cross-serial dependencies provided the boundary is explicitly marked, unlike a deterministic multi-stack architecture. We also showed that the RNS-RNN can far exceed the amount of information it seemingly should be able to encode in its stack given its finite stack alphabet. Our newly proposed VRNS-RNN combines the benefits of nondeterminism and vector embeddings, and we showed that it has better performance than other stack RNNs on the Dyck language and a natural language modeling benchmark.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nTo facilitate reproducibility, we have publicly released all code we used to conduct our experiments and generate the figures and tables in this paper. During both development and experimentation, we ran our code in containers to simplify reproducing our software environment. Our code includes the original Docker image definition we used, as well as the exact shell commands we used for each experiment, figure, and table. We have thoroughly documented our experimental settings in Sections 4 to 6 and Appendices D and H.\n\nACKNOWLEDGMENTS\n\nThis research was supported in part by a Google Faculty Research Award to Chiang. We would like to thank Darcey Riley and Stephen Bothwell for their comments on an earlier draft of this paper, and the Center for Research Computing at the University of Notre Dame for providing the computing infrastructure for our experiments.\n\nREFERENCES\n\nJean-Michel Autebert, Jean Berstel, and Luc Boasson. Context-free languages and pushdown automata. In Grzegorz Rozenberg and Arto Salomaa (eds.), Handbook of Formal Languages, pp. 111–174. Springer, 1997. doi: 10.1007/978-3-642-59136-5 3.\n\nAlexandra Butoi, Brian DuSell, Tim Vieira, Ryan Cotterell, and David Chiang. Algorithms for\n\nweighted pushdown automata. In Proc. EMNLP, 2022.\n\nYining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks as weighted language recognizers. In Proc. NAACL HLT (Long Papers), pp. 2261–2271, 2018. doi: 10.18653/v1/N18-1205.\n\nKyunghyun Cho, Bart van Merri ̈enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the propIn Proc. Workshop on erties of neural machine translation: Encoder–decoder approaches. Syntax, Semantics and Structure in Statistical Translation (SSST), pp. 103–111, 2014. doi: 10.3115/v1/W14-4012.\n\nChristopher Culy. The complexity of the vocabulary of Bambara. Linguistics and Philosophy, 8:\n\n345–351, 1985.\n\nBrian DuSell and David Chiang. Learning context-free languages with nondeterministic stack RNNs. In Proc. Conference on Computational Natural Language Learning (CoNLL), pp. 507– 519, 2020. doi: 10.18653/v1/2020.conll-1.41.\n\nBrian DuSell and David Chiang. Learning hierarchical structures with differentiable nondeterminIn Proc. ICLR, 2022. URL https://openreview.net/pdf?id=5LXw_\n\nistic stacks. QplBiF.\n\nJeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990. doi: 10.1016/\n\n0364-0213(90)90002-E.\n\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines, 2014. URL http://\n\narxiv.org/abs/1410.5401. arXiv:1410.5401.\n\nEdward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman,\n\nLearning to transduce with unbounded memory.\n\nsom. ume 5648-learning-to-transduce-with-unbounded-memory.pdf.\n\n1828–1836,\n\n2015.\n\npp.\n\n2,\n\nand Phil BlunIn Proc. NeurIPS, volURL https://papers.nips.cc/paper/\n\nSheila A. Greibach. A new normal-form theorem for context-free phrase structure grammars. J.\n\nACM, 12(1):42–52, 1965. doi: 10.1145/321250.321254.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):\n\n1735–1780, 1997. doi: 10.1162/neco.1997.9.8.1735.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nArmand\n\nJoulin\n\nstackand Tomas Mikolov. nets. 190–198, https://proceedings.neurips.cc/paper/2015/hash/\n\naugmented 2015. 26657d5ff9020d2abefe558796b99584-Abstract.html.\n\nIn Proc. NeurIPS,\n\nrecurrent URL\n\npatterns with\n\nalgorithmic\n\nInferring\n\nvolume\n\npp.\n\n1,\n\nS. C. Kleene. Representation of events in nerve nets and finite automata. Technical Report RM-704, RAND, 1951. URL https://www.rand.org/content/dam/rand/pubs/ research_memoranda/2008/RM704.pdf.\n\nBernard Lang. Deterministic techniques for efficient non-deterministic parsers.\n\nloquium on Automata, Languages, and Programming (ICALP), pp. 255–269, 1974. 10.1007/978-3-662-21545-6 18.\n\nIn Proc. Coldoi:\n\nLeonard Y. Liu and Peter Weiner. An infinite hierarchy of intersections of context-free languages.\n\nMath. Syst. Theory, 7(2):185–192, 1973. doi: 10.1007/BF01762237.\n\nTomas Mikolov, Anoop Deoras, Stefan Kombrink, L. Burget, and J. Cernock ́y. Empirical evaluation and combination of advanced language modeling techniques. In Proc. INTERSPEECH, pp. 605–608, 2011. URL https://www.isca-speech.org/archive_v0/archive_ papers/interspeech_2011/i11_0605.pdf.\n\nHao Peng, Roy Schwartz, Sam Thomson, and Noah A. Smith. Rational recurrences.\n\nIn Proc.\n\nEMNLP, pp. 1203–1214, 2018. doi: 10.18653/v1/D18-1152.\n\nRoy Schwartz, Sam Thomson, and Noah A. Smith. Bridging CNNs, RNNs, and weighted finite-state\n\nmachines. In Proc. ACL (Long Papers), pp. 295–305, 2018. doi: 10.18653/v1/P18-1028.\n\nStuart M. Shieber. Evidence against the context-freeness of natural language. Linguistics and Phi-\n\nlosophy, 8:333–344, 1985.\n\nHava T. Siegelmann and Eduardo D. Sontag. On the computational power of neural nets. In Proc. Workshop on Computational Learning Theory (COLT), pp. 440–449, 1992. doi: 10.1145/130385. 130432.\n\nMichael Sipser. Introduction to the Theory of Computation. Cengage Learning, 3rd edition, 2013.\n\nJohn Stogin, Ankur Arjun Mali, and C. Lee Giles. Provably stable interpretable encodings of context free grammars in RNNs with a differentiable stack, 2020. URL https://arxiv.org/abs/ 2006.03651v3. arXiv:2006.03651.\n\nG. Z. Sun, C. Lee Giles, H. H. Chen, and Y. C. Lee. The neural network pushdown automaton: Model, stack, and learning simulations. Technical Report UMIACS-TR-93-77 and CS-TR-3118, University of Maryland, 1995. URL https://arxiv.org/abs/1711.05738. Revised version.\n\nGail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision RNNs for language recognition. In Proc. ACL (Short Papers), pp. 740–745, 2018. doi: 10.18653/v1/P18-2117.\n\nDani Yogatama, Yishu Miao, G ́abor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and Phil Blunsom. Memory architectures in recurrent neural network language models. In Proc. ICLR, 2018. URL https://openreview.net/pdf?id=SkFqf0lAZ.\n\nA CHANGES TO THE RNS-RNN\n\nIn this section, we describe two minor changes to the original definition of the RNS-RNN that improve its time complexity and expressivity. We first reiterate the original definitions for γ and α\n\n11\n\nPublished as a conference paper at ICLR 2023\n\n(DuSell & Chiang, 2022). For 0 ≤ i < t ≤ n − 1,\n\nγ[i → t][q, x → r, y] =\n\nI[i = t−1] ∆[t][q, x → r, xy]\n\n(cid:88)\n\n+\n\nγ[i → t−1][q, x → s, z] ∆[t][s, z → r, y]\n\npush\n\nrepl.\n\ns,z\n\nt−2 (cid:88)\n\n+\n\n(cid:88)\n\n(cid:88)\n\nk=i+1\n\nu\n\ns,z\n\nγ[i → k][q, x → u, y] γ[k → t−1][u, y → s, z] ∆[t][s, z → r, ε] pop\n\nα[0][r, y] = I[r = q0 ∧ y = ⊥]\n\nα[t][r, y] =\n\nt−1 (cid:88)\n\n(cid:88)\n\ni=0\n\nq,x\n\nα[i][q, x] γ[i → t][q, x → r, y]\n\n(1 ≤ t ≤ n).\n\n(12)\n\n(13)\n\n(14)\n\nNote that in an RNN where rt−1 is used to compute ht and yt, the last timestep t = n is not needed, so ∆[t] is only defined for 1 ≤ t ≤ n − 1, and γ and α only need to be computed for t ≤ n − 1.\n\nNow, we describe the two changes we have made to Eqs. (12) to (14) to arrive at Eqs. (2) to (6).\n\nAsymptotic speedup As can be seen in Eqs. (7) and (12) to (14), the RNS-RNN’s time complexity is O(|Q|4|Γ|3n3), and its space complexity is O(|Q|2|Γ|2n2). The computational complexity of the RNS-RNN limits us to relatively small sizes for Q and Γ. However, it is possible to improve its asymptotic time complexity with respect to |Q| with a simple change: precomputing the product γ[k → t−1][u, y → s, z] ∆[t][s, z → r, ε] used in the pop rule in Eq. (12), which we store in a tensor γ′. This reduces the time complexity of the RNS-RNN to O(|Q|3|Γ|3n2 + |Q|3|Γ|2n3), allowing us to train larger models.\n\nBottom symbol fix There are some peculiarities of the initial ⊥ marker that are not described in previous work: (1) the initial instance of ⊥ at the bottom of the stack can never be popped or replaced, (2) any symbol that sits directly above it cannot be popped (but it can be replaced), and (3) the ⊥ symbol type can be reused freely elsewhere in the stack. Point (2) is odd because the ⊥ symbol can never be uncovered again after the first timestep, possibly complicating detection of the bottom of the stack later on. We rectify this by allowing the symbol above the initial ⊥ to be popped, and allowing the initial ⊥ symbol to be replaced with a different symbol type at any time. We do this by simulating an extra push action at t = −1.\n\nB PROOFS OF LANGUAGE RECOGNITION RESULTS\n\nB.1 PROOF OF PROPOSITION 1\n\nIn this section, we prove that the restricted form of real-time PDA used in the RNS-RNN can recognize all CFLs. We then show how to convert PDAs in this form to RNS-RNN recognizers (assuming some parameters can have values of ±∞), proving that RNS-RNNs can recognize all CFLs.\n\nDefinition 2. A pushdown automaton is a tuple (Q, Σ, Γ, δ, q0, F, ⊥), where\n\n• Q is a finite set of states\n\n• Σ is a finite input alphabet\n\n• Γ is a finite stack alphabet\n\n• δ ⊆ Q × Γ × (Σ ∪ ε) × Q × Γ∗ is a set of transitions\n\n• q0 ∈ Q is the start state\n\n• F ⊆ Q is the set of accept states\n\n12\n\nPublished as a conference paper at ICLR 2023\n\n• ⊥ ∈ Γ is the bottom stack symbol.\n\nA PDA always starts in state q0 with a stack consisting of ⊥, and it accepts its input iff there is a run that terminates in an accept state with ⊥ on top of the stack. Definition 3. A restricted PDA is one whose transitions have one of the following forms, where q, r ∈ Q, a ∈ Σ, and x, y ∈ Γ:\n\nq, x a−→ r, xy q, x a−→ r, y q, x a−→ r, ε\n\npush y on top of x\n\nreplace x with y\n\npop x.\n\nThe usual construction for removing non-scanning transitions (Autebert et al., 1997) involves converting to Greibach normal form (GNF) and then converting to a PDA. However, this construction produces transitions of the form q, x a−→ r, zy where x ̸= z, which our restricted form does not allow. Simulating such transitions in a restricted PDA presents a challenge because it requires performing a replace and then a push while scanning only one symbol. To simulate such transitions, we need to use a modified GNF, defined below. Lemma 3. For any CFG G, there is a CFG equivalent to G that has the following form (called 2–Greibach normal form):\n\n• The start symbol S does not appear on any right-hand side.\n\n• Every rule has one of the following forms:\n\nS → ε A → a A → abB1 · · · Bp\n\np ≥ 0.\n\nProof. Convert G to Greibach normal form. Then for every rule A → aA1 · · · Am and every rule A1 → bB1 · · · Bl, substitute the second rule into the first to obtain A → abB1 · · · BlA2 · · · Am. Then discard the first rule.\n\nLemma 4. For any CFG G in 2-GNF, there is a PDA equivalent to G whose transitions all have one of the forms:\n\nq, x a−→ r, xy1 · · · yk q, x a−→ r, y q, x a−→ r, ε.\n\n(15)\n\nProof. We split the rules into four cases:\n\nS → ε\n\nA → a\n\nA → ab A → abB1 · · · Bp\n\np ≥ 1.\n\nThe PDA has an initial state q0 and a main loop state qloop. It works by maintaining all of the unclosed constituents on the stack, which initially is S. The state qloop is an accept state. For now, we allow the PDA to have one non-scanning transition, q0, ⊥ ε−→ qloop, ⊥S.\n\nIf G has the rule S → ε, we make state q0 an accept state. For each rule in G of the form A → a, we add a pop transition qloop, A a−→ qloop, ε. For each rule in G of the form A → ab, we add a new state q, a replace transition qloop, A a−→ q, A, and a pop transition q, A b−→ qloop, ε. This simply scans two symbols while popping A.\n\nFor each rule in G of the form A → abB1 · · · Bp where p ≥ 1, we add a new state q, a replace tranb−→ qloop, BpBp−1 · · · B1. These two transitions sition qloop, A a−→ q, Bp, and a push transition q, Bp\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nare equivalent to scanning two symbols while replacing A with BpBp−1 · · · B1 on the stack. Note that we have taken advantage of the fact that 2-GNF affords us two scanned input symbols to work around the restriction that push transitions of the form q, x a−→ r, xy1 · · · yk cannot modify the top symbol and push new symbols in the same step. We have split this action into a replace transition followed by a push transition, using the state machine to remember what to scan and push after the replace transition. Finally, we remove the non-scanning transition q0, ⊥ ε−→ qloop, ⊥S, and for every transition qloop, S a−→ r, α, we add a push transition q0, ⊥ a−→ r, ⊥α. Now all transitions are scanning.\n\nLemma 5. For any PDA P in the form (15), there is a PDA equivalent to P in restricted form.\n\nProof. At this point, there is a maximum length k such that q, x a−→ r, xα is a transition and k = |α|. We redefine the stack alphabet of the PDA to be Γ′ = Γ ∪ Γ2 · · · ∪ Γk. Stack symbols now represent strings of the original stack symbols. Let α denote a single stack symbol for any string α. We replace every push transition q, x a−→ r, xβ with push transitions q, αx a−→ r, αx β for all α ∈ (cid:83)k−1 i=0 Γi. We replace every replace transition q, x a−→ r, y with replace transitions q, αx a−→ r, αy for all α. And we replace every pop transition q, x a−→ r, ε with replace transitions q, αx a−→ r, α for all α with |α| ≥ 1, and a pop transition q, x a−→ r, ε.\n\nSo for every CFL L, there exists a restricted PDA P that recognizes L. The last step is to construct an RNS-RNN. As noted in Section 3, here we do allow parameters with values of ±∞.\n\nLemma 6. For any restricted-form PDA P that recognizes language L, there is an RNS-RNN that recognizes L.\n\nProof. Let P = (Q, Σ, Γ, δ, q0, F, ⊥).\n\nWe write ct(w) for the total number of runs of P that read w and end with ⊥ on top of the stack. We assume that ct(w) > 0; this can always be ensured by adding to P an extra non-accepting state qtrap and transitions q0, ⊥ a−→ qtrap, ⊥ and qtrap, ⊥ a−→ qtrap, ⊥ for all a ∈ Σ.\n\nFor any state set X ⊆ Q, we write ct(w, X) for the total number of runs of P that read w and end in a state in X with ⊥ on top of the stack. Then for any string w, if w ∈ L, then ct(w, F ) ≥ 1; otherwise, ct(w, F ) = 0.\n\nWe use the following definition for the LSTM controller of the RNS-RNN, where it, ft, and ot are the input, forget, and output gates, respectively, and gt is the candidate memory cell.\n\nit = σ(Wi\n\nft = σ(Wf\n\n(cid:21)\n\n(cid:20) xt ht−1 (cid:20) xt ht−1\n\n+ bi)\n\n(cid:21)\n\n+ bf )\n\ngt = tanh(Wg\n\n(cid:21)\n\n+ bg)\n\n(cid:20) xt ht−1 (cid:21)\n\n(cid:20) xt ht−1\n\not = σ(Wo\n\n+ bo)\n\nct = ft ⊙ ct−1 + it ⊙ gt ht = ot ⊙ tanh(ct)\n\nConstruct an RNS-RNN as follows. At each timestep t, upon reading input embedding xt, make the controller emit a weight of 1 for all transitions of P that scan input symbol wt, and a weight of 0 for all other transitions (by setting the corresponding weights of ∆[t] to −∞).\n\nLet n = |w|. After reading w, the stack reading rn is the probability distribution of states and top stack symbols of P after reading w, which the controller can use to compute hn+1 and the MLP\n\n14\n\nPublished as a conference paper at ICLR 2023\n\noutput y as follows. Let\n\np =\n\n(cid:88)\n\nf ∈F\n\nrn[(f, ⊥)] =\n\nct(w, F ) ct(w)\n\n.\n\nNote that p is positive if w ∈ L, and zero otherwise. An affine layer connects hn, xn+1, and rn to the candidate memory cell gn+1 (recall that the input at n + 1 is EOS). Designate a unit gaccept in gn+1. For each f ∈ F , set the incoming weight from rn[(f, ⊥)] to 1 and all other incoming weights to 0, so that gaccept n+1 = tanh(p). Set this unit’s input gate to 1 and forget gate to 0, so that memory cell caccept n+1 = tanh(tanh(p)), which is positive if w ∈ L and zero otherwise.\n\nn+1 = tanh(p). Set its output gate to 1, so that hidden unit haccept\n\nn+1\n\nn+1\n\nFinally, to compute y, use one hidden unit y1 in the output MLP layer, and set the incoming weight from haccept to 1, and all other weights to 0. So y1 = σ(tanh(tanh(p))), which is greater than 1 if w ∈ L and equal to 1 2 otherwise. Set the weight connecting y1 to y to 1, and set the bias term 2 if w ∈ L, and equal to 1 to − 1 otherwise.\n\n2 , so that y = σ(σ(tanh(tanh(p))) − 1\n\n2 ), which is greater than 1\n\n2\n\n2\n\nB.2 PROOF OF PROPOSITION 2\n\nWithout loss of generality, assume k = 2. Let P1 = (Q1, Σ, Γ1, δ1, s1, F1, ⊥) and P2 = (Q2, Σ, Γ2, δ2, s2, F2, ⊥) be restricted-form PDAs recognizing L1 and L2, respectively. We can construct both so that Q1 ∩ Q2 = ∅, and s1 and s2 each have no incoming transitions. Construct a new PDA\n\nP = (Q, Σ, Γ1 ∪ Γ2, δ, s, F ) Q = (Q1 \\ {s1}) ∪ (Q2 \\ {s2}) ∪ {s}\n\nδ(q, x, a) =\n\nF =\n\n \n\nδ1(q, x, a) δ2(q, x, a) δ1(s1, x, a) ∪ δ2(s2, x, a)\n\nq ∈ Q1 \\ {s1} q ∈ Q2 \\ {s2} q = s (cid:26)(F1 \\ {s1}) ∪ (F2 \\ {s2}) ∪ {s}\n\n\n\n(F1 \\ {s1}) ∪ (F2 \\ {s2})\n\ns1 ∈ F1 ∧ s2 ∈ F2 otherwise.\n\nSo far, this is just the standard union construction for PDAs.\n\nConstruct an RNS-RNN that sets ∆[t] according to δ, and assume ct(w) > 0, as in Lemma 6. Let\n\np1 =\n\np2 =\n\n(cid:88)\n\nf ∈F1\n\n(cid:88)\n\nf ∈F2\n\nrn[(f, ⊥)] =\n\nrn[(f, ⊥)] =\n\nct(w, F1) ct(w)\n\nct(w, F2) ct(w)\n\n.\n\nLet n = |w|. For each pi, designate a unit gaccept i,n+1 in gn+1 that will be positive if p1 > 0 and negative if p1 = 0. In order to ensure that gaccept i,n+1 ̸= 0, we subtract a small value from pi that is smaller than the smallest possible non-zero value of pi. The RNS-RNN computes this value as follows. Let b = |Q|(2|Γ| + 1), which is the maximum number of choices P can make from any given configuration. Designate one memory cell coffset is b , and at each subsequent timestep, the forget gate is used to multiply this cell by 1 initialized to 1 (the input gate is set to 0). So after reading t symbols, coffset bt . Set the output gate for that cell to 1, so there is a hidden unit hoffset bn ). The smallest non-zero value of pi is\n\nct(w) , and since ct(w) ≤ bn and tanh(x) < x when x > 0, hoffset\n\nin hn that contains the value tanh( 1\n\nin ct. At the first timestep, coffset\n\nis smaller than it.\n\n= 1\n\nn\n\nn\n\n1\n\nb\n\nt\n\nt\n\nt\n\nAs for the incoming weights to gaccept i,n+1 , for each f ∈ Fi, set the weight for rn[(f, ⊥)] to 1, and set the weight for hoffset ), which is positive if pi > 0 and negative otherwise. Set this unit’s input gate to 1 and forget gate to 0, so that memory cell caccept i,n+1 ), which is positive if pi > 0 and negative otherwise.\n\ni,n+1 . Set its output gate to 1, so that hidden unit haccept\n\nto −1; set all other weights to 0. So gaccept\n\ni,n+1 = tanh(pi − hoffset\n\ni,n+1 = tanh(gaccept\n\ni,n+1 = gaccept\n\nn\n\nn\n\n15\n\nPublished as a conference paper at ICLR 2023\n\ni,n+1 to ∞, and all other weights to 0. So yi = σ(∞ · haccept\n\nIn the output MLP’s hidden layer, for each haccept i,n+1 , include a unit yi, and set its incoming weight from haccept i,n+1 ), which is 1 if pi > 0 and 0 otherwise. Finally, to compute y, set the incoming weights from each yi to 1, and set the bias term to − 3 2 ), which is greater than 1 2 if both p1 > 0 and p2 > 0, meaning w ∈ L1 ∩ L2, and less than 1\n\n2 . So y = σ(y1 + y2 − 3\n\n2 otherwise.\n\nC ADDITIONAL DISCUSSION OF NON-CONTEXT-FREE LANGUAGES\n\nBelow we discuss each of the non-CFLs of Section 4 in more detail, including details of how a real-time multi-stack automaton could recognize it. We also describe three non-CFLs not included in Section 4.\n\nanbncn The language {anbncn | n ≥ 0}, a classic example of a non-CFL (Sipser, 2013). A two-stack automaton can recognize this language as follows. While reading the a’s, push them to stack 1. While reading the b’s, match them with a’s popped from stack 1 while pushing b’s to stack 2. While reading the c’s, match them with b’s popped from stack 2. As the stacks are only needed to remember the count of each symbol type, this language is also an example of a counting language; Weiss et al. (2018) showed that LSTMs can learn this language by using their memory cells as counters.\n\nw#wR#w The language {w#wR#w | w ∈ {0, 1}∗}. A two-stack automaton can recognize it as follows. While reading the first w, push it to stack 1. While reading the middle wR, match it with symbols popped from stack 1 while pushing wR to stack 2. While reading the last w, match it with symbols popped from stack 2. The explicit # symbols are meant to make it easier for a model to learn when to transition between these three phases.\n\nw#nw The language {w#nw | w ∈ {0, 1}∗, n ≥ 0, and |w| = n}. A two-stack automaton can recognize it as follows. While reading the first w, push it to stack 1. While reading #n, move the symbols from stack 1 to stack 2 in reverse. While reading the final w, match it with symbols popped from stack 2. Unlike w#wR#w, the middle #n section offers a model few hints that it should push w to the stack beforehand.\n\nw#w The language {w#w | w ∈ {0, 1}∗}. A three-stack automaton can recognize this language as follows. Let w = uv where |u| = |v| (for simplicity assume |w| is even). While reading the first u, push it to stack 1. While reading the first v, push it to stack 2, and move the symbols from stack 1 to stack 3 in reverse. While reading the second u, match it with symbols popped from stack 3, and move the symbols from stack 2 to stack 1 in reverse. While reading the second v, match it with symbols popped from stack 1. The explicit # symbol is meant to make learning this task easier.\n\nww′ The language {ww′ | w ∈ {0, 1}∗ and w′ = φ(w)}, where φ is the homomorphism φ(0) = 2, φ(1) = 3. A three-stack automaton can recognize this language using a similar strategy to w#w. In this case, a switch to a different alphabet, rather than a # symbol, marks the second half of the string.\n\nwwRw The language {wwRw | w ∈ {0, 1}∗}. A two-stack automaton can recognize it using a\n\nsimilar strategy to w#wR#w, but it must nondeterministicaly guess |w|.\n\nww The language {ww | w ∈ {0, 1}∗}, another classic example of a non-CFL (Sipser, 2013). A three-stack automaton can recognize it using a similar strategy to w#w, except it must nondeterministically guess |w|.\n\nD ADDITIONAL DETAILS FOR FORMAL LANGUAGE EXPERIMENTS\n\nHere we describe the training procedure used for the non-CFL and capacity experiments in Sections 4 and 5 in more detail. We trained each model by minimizing its cross-entropy (summed over the timestep dimension of each batch) on the training set, and we used per-symbol cross-entropy on the validation set as the early stopping criterion. We optimized the parameters of the model with Adam. For each training run, we randomly sampled the initial learning rate from a log-uniform distribution over [5 × 10−4, 1 × 10−2], and we used a gradient clipping threshold of 5. We initialized all fully-connected layers except for those in the LSTM controller with Xavier uniform initialization,\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nLSTM\n\nSup. 10\n\nSup. 3-3-3\n\nSup. h\n\nRNS 3-3\n\n·10−2\n\nanbncn\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\nw#nw\n\n0\n\n50\n\n100\n\n150\n\n200\n\nwwRw\n\n4\n\n2\n\n0\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0.4\n\n0.2\n\n0\n\nanbncn\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n100\n\nw#nw\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n100\n\nwwRw\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n0\n\n50\n\n100 Epoch\n\n150\n\n200\n\n40\n\n50\n\n60\n\n80\n\n90\n\n100\n\n70 Length\n\nFigure 5: Performance on three non-CFLs not included in Section 4. Cross-entropy difference in nats, on the validation set by epoch (left), and on the test set by string length (right). Each line is the best of 10 runs, selected by validation perfomance. All models easily solved anbncn. As with w#nw, only multi-stack models (Sup. 3-3-3 and RNS 3-3) solved w#nw. As with ww, no models solved wwRw.\n\nand all other parameters uniformly from [−0.1, 0.1]. We used mini-batches of size 10; each batch always contained examples of equal lengths. We randomly shuffled batches before each epoch. We multiplied the learning rate by 0.9 after 5 epochs of no improvement on the validation set, and we stopped early after 10 epochs of no improvement.\n\nE ADDITIONAL RESULTS FOR NON-CFL EXPERIMENTS\n\nIn Fig. 5, we show results for three non-CFLs which we did not include in Section 4. The input and forget gates of the LSTM controller are not tied, so the values of its memory cells are not bounded to (0, 1) and can be used as counters. All models easily learned anbncn, likely because the LSTM controller by itself can solve it using a counting mechanism (Weiss et al., 2018). Only the models capable of simulating multiple stacks, Sup. 3-3-3 and RNS 3-3, achieved optimal cross-entropy on w#wR#w and w#nw. No models succeeded on the unmarked copying tasks (wwRw and ww), although Sup. 10 achieved the best performance on the test set for wwRw.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nF ADDITIONAL RESULTS FOR CAPACITY EXPERIMENTS\n\nHere, we describe the languages of Section 5 in more detail, plus an additional language, wwR.\n\nw#wR The language {w#wR | w ∈ {0, 1, · · · , k − 1}∗}. This is a simple deterministic CFL. Dyck The language of strings over the alphabet {(1, )1, (2, )2, · · · , (k, )k} where all brackets are properly balanced and nested in pairs of (i and )i. This is a more complicated but still deterministic CFL.\n\nwwR The language {wwR | w ∈ {0, 1, · · · , k − 1}∗}. This is a nondeterministic CFL which\n\nrequires a model to guess |w|.\n\nIn Fig. 6, we show the results of the same experiments as in Section 5, but with standard deviations included. We also show results for the language wwR. In Fig. 7, for the same experiments, we show the minimum cross-entropy difference on the validation set out of all 10 random restarts, rather than the mean. None of the models performed significantly better than the LSTM on the nondeterministic wwR language, suggesting that they cannot simultaneously combine the tricks of encoding symbol types as points in high-dimensional space and nondeterministically guessing |w|. Although the VRNS-RNN does combine both nondeterminism and vectors, the nondeterminism only applies to the discrete symbols of Γ, of which there are no more than 3, far fewer than k when k ≥ 40.\n\nG HEATMAPS OF STACK READING VECTORS\n\nIn Fig. 8 we show heatmaps of stack reading vectors across time on an example string in w#wR when k = 40.\n\nH ADDITIONAL DETAILS FOR NATURAL LANGUAGE EXPERIMENTS\n\nHere we describe the Penn Treebank experiments in more detail. For each architecture, we used a word embedding layer of the same size as the hidden state. In order to preserve context across batches, we trained all models using truncated backpropagation through time (BPTT), treating each dataset as one long sequence and limiting batches to a length of 35. As we noted previously (DuSell & Chiang, 2022), training stack RNNs with truncated BPTT requires bounding the size of the stack data structure, as having it grow indefinitely from batch to batch would be computationally infeasible. We limited the depth of the superposition stack to 10, following Yogatama et al. (2018) and our previous paper (DuSell & Chiang, 2022). To limit the size of the RNS-RNN, we used the incremental execution technique we devised previously (DuSell & Chiang, 2022), which limits non-zero entries in γ to those where t − i ≤ D, for some constant window size D. We applied the same technique to the VRNS-RNN by imposing the same limitation on both γ and ζ, restricting non-zero entries of ζ to those where t − i ≤ D. In both cases, we set D = 35. We used the standard train/validation/test splits for the Penn Treebank.\n\nWe trained each model by minimizing its cross-entropy (averaged over the timestep dimension of each batch) on the training set, using per-symbol perplexity on the validation set as the early stopping criterion. We optimized the parameters of the model with simple SGD. For each training run, we randomly sampled the initial learning rate from a log-uniform distribution over [1, 100], and the gradient clipping threshold from a log-uniform distribution over [0.0112, 1.12]. We initialized all parameters uniformly from [−0.05, 0.05]. We used a mini-batch size of 32. We divided the learning rate by 1.5 whenever validation perplexity did not improve after an epoch, and we stopped early after 2 epochs of no improvement.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nLSTM VRNS 1-1-3\n\nSup. 3 VRNS 2-1-3\n\nRNS 1-3 VRNS 2-3-3\n\nRNS 2-3\n\nw#wR\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nDyck\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nwwR\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n2.5\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n1\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n2.5\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nAlphabet Size k\n\nFigure 6: Mean cross-entropy difference on the validation set vs. input alphabet size. The shaded regions indicate one standard deviation. We include experiments on wwR; no models performed significantly better on average than the LSTM baseline.\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nLSTM VRNS 1-1-3\n\nSup. 3 VRNS 2-1-3\n\nRNS 1-3 VRNS 2-3-3\n\nRNS 2-3\n\nw#wR\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nDyck\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nwwR\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n. f\nf i\n\nD y\np o\nr t\n\nn e\n- s\ns o\nr\n\nC\n\n2\n\n1.75\n\n1.5\n\n1.25\n\n1\n\n0.75\n\n0.5\n\n0.25\n\n0\n\n0.8\n\n0.6\n\n0.4\n\n0.2\n\n0\n\n2.5\n\n2\n\n1.5\n\n1\n\n0.5\n\n0\n\n2\n\n40\n\n80\n\n120\n\n160\n\n200\n\nAlphabet Size k\n\nFigure 7: Best cross-entropy difference on the validation set vs. input alphabet size. On w#wR, surprisingly, only RNS 2-3 achieved optimal cross-entropy for all alphabet sizes. On the more complicated Dyck language, our new VRNS-RNN (VRNS 2-1-3, VRNS 2-3-3) achieved the best performance for large alphabet sizes. No models performed much better than the LSTM baseline on wwR, although RNS 2-3 performed well for k = 40.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nl\n\no b\nm y\nS\n\nt\n\nu p\nn I\n\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 #\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 EOS\n\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 #\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 EOS\n\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 1\n2 3\n0 #\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 3\n2 1\n0 EOS\n\n,⊥) (q0\n\n,0) (q0\n\n,1) (q1\n\n,⊥) (q1\n\n,0) (q1\n\n,1)\n\n(q0\n\n,⊥) (q0\n\n,0) (q0\n\n,1) (q1\n\n,⊥) (q1\n\n,0) (q1\n\n,1)\n\n(q0\n\n,⊥) (q0\n\n,0) (q0\n\n,1) (q1\n\n,⊥) (q1\n\n,0) (q1\n\n,1)\n\n(q0\n\nReading Element\n\nReading Element\n\nReading Element\n\nFigure 8: Heatmaps of rt over time on a string from w#wR when k = 40, generated from the best RNS 2-3 model (left) and two other random restarts (middle, right). The w string repeats the pattern 0123, which is clearly seen in the reading vectors. Black = 1, and white = 0.\n\n21",
    "reference": "# Summary Of The Paper\n\nThe paper presents a part empirical / part theoretical analysis of the representational power of non-deterministic stack RNNs.  It is shown theoretically that renormalizing nondeterministic stack RNNs  (RNS-RNNs) have the power to recognize context-free languages and their intersections (Props 1 and 2).  The ability of RNS-RNNs to recognize context-free languages is investigated empirically, whose performance is shown to surprisingly exceed what might be expected theoretically.  The authors show that the model is using the non-deterministic state of the stack to exceed the limitations that would otherwise exist for a deterministic model (Fig. 3).  This prompts the introduction of a novel architecture, where the stack is allowed to store real vectors rather than only discrete symbols, and it is shown that this representation achieves SOTA performance when modeling real language data in terms of perplexity (Table 1).\n\n# Strength And Weaknesses\n\nStengths:\n\n-  The paper is clearly presented, and investigates an interesting phenomena.  Particularly, the comparison of neuro-symbolic models' representational strength using formal languages is a promising approach, which is currently under-explored.\n-  The theoretical analysis is thorough,\n-  The empirical analysis is convincing, and provides good motivation for why the vector-valued stack should increase the model's power (essentially by allowing the model to choose the number of symbols stored on the stack).\n\nWeaknesses:\n\n-  The theoretical analysis is essentially the translation of a pushdown automata (PDA) construction into a RNS-RNN, which in itself is not surprising (although useful for determining the dimensions of an RNS-RNN equivalent to a given PDA strength).\n-  It is unclear if there is a specific advantage to using vector RNS-RNNs with its stack memory model in terms of performance as opposed to say models with access to a key-value based memory model, as in a DQN model.  It would be interesting to see such comparisons, both on the formal and natural language data (for the former, one would expect that there are limitations on representational strength imposed by the number of possible keys, but it is not clear how non-determinism would affect this).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity, quality, novelty and reproducibility are good.\n\n# Summary Of The Review\n\nAn interesting investigation of the representational strength of a neuro-symbolic model, combining theoretical and empirical analysis.  The work will be of interest to those working on formal languages, deep-learning and natural language processing.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nNone."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nWASSERSTEIN FAIR AUTOENCODERS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nAutoencoders, or nonlinear factor models parameterized by neural networks, have become an indispensable tool for generative modeling and representation learning in high dimensions. Imposing structural constraints such as conditional independence on the latent variables (representation, or factors) in order to capture invariance or fairness with autoencoders has been attempted through adding ad hoc penalties to the loss function mostly in the variational autoencoder (VAE) context, often based on heuristic arguments. In this paper, we demonstrate that Wasserstein autoencoders (WAEs) are highly flexible in embracing structural constraints. Well-known extensions of VAEs for this purpose are gracefully handled within the framework of the seminal result by Tolstikhin et al. (2018). In particular, given a conditional independence structure of the generative model (decoder), corresponding encoder structure and penalties are induced from the functional constraints that define the WAE. This property of WAEs opens up a principled way of penalizing autoencoders to impose structural constraints. Utilizing this generative model structure, we present results on fair representation and conditional generation tasks, and compare them with other preceding methods.\n\n1\n\nINTRODUCTION\n\nThe ability to learn informative representation of data with minimal supervision is a key challenge in machine learning (Tschannen et al., 2018), toward obtaining which autoencoders have become an indispensable toolkit. An autoencoder consists of the encoder, which maps the input to a lowdimensional representation, and the decoder, that maps a representation back to a reconstruction of the input. Thus an autoencoder can be considered a nonlinear factor analysis model as the latent variable provided by the encoder carries the meaning of “representation” and the decoder can be used for generative modeling of the input data distribution. Most autoencoders can be formulated as minimizing some “distance” between the distribution PX of input random variable X and the distribution g♯PZ of the reconstruction G = g(Z), where Z is the latent variable or representation having distribution PZ and g is either deterministic or probabilistic decoder (in the latter case g is read as the conditional distribution of G given Z), which is variationally described in terms of an encoder QZ|X . For instance, the variational autoencoder (VAE, Kingma & Welling, 2014) minimizes\n\nDVAE(PX , g♯PZ) = inf\n\nQZ|X ∈Q\n\nEPX [DKL(QZ|X ∥PZ) − EQZ|X log g(Z)]\n\n(1)\n\nover the set of probabilistic decoders or conditional densities g of G given Z, where DKL is the Kullback-Leibler (KL) divergence, and the Wasserstein autoencoder (WAE, Tolstikhin et al., 2018) minimizes\n\nDWAE(PX , g♯PZ) = inf\n\nQZ|X ∈Q\n\nEPX\n\nEQZ|X dp(X, g(Z))\n\n(2)\n\nover the set of deterministic decoders g, where d is the metric in the space of input X and p ≥ 1. Set Q restricts the search space for the encoder. In VAEs, a popular choice is a class of normal distributions\n\nQ = {QZ|X regular conditional distribution : Z|{X = x} ∼ N (μ(x), Σ(x)), (μ, Σ) ∈ N N }\n\nwhere N N is a class of functions parametrized by neural networks. In WAEs, the choice\n\nQ = {QZ|X regular conditional distribution : QZ ≜ EPX QZ|X = PZ} makes the left-hand side of Eq. (2) equal to the (p-th power of) the p-Wasserstein distance between PX and g♯PZ (Tolstikhin et al., 2018, Theorem 1); QZ is called an aggregate posterior of Z. If Q is\n\n(3)\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\na set of Dirac measures, i.e., Q = {QZ|X : QZ|X=x = δf (x), f ∈ N N } , then minimizing Eq. (2) reduces to the learning problem of a deterministic unregularized autoencoder.\n\nOf course, the notion of “informativeness” depends on the downstream task. The variation in the observations that are not relevant to the particular task is often called “nuisance” and is desirable to be suppressed from the representation. For example, in finding “fair representations,” (Zemel, 2013) sensitive information such as gender or socioeconomic status should be removed from latent representations; in obtaining representations of facial images, those that are invariant to lighting conditions, poses, or wearing of eyeglasses are often sought. A popular approach to this goal is to explicitly separate informative and nuisance variables in the generative model by factorization. This approach imposes a structure on the decoder. Additionally the encoder is further factorized and a penalty promoting independence between the encoded representation and nuisance variable can be added. A well-known example is the variational fair autoencoder (VFAE, Louizos et al., 2016), in which a variant of the “M1+M2” graphical model (Kingma et al., 2014) is used to factorize the decoder and a resembling factorization of the encoder (variational posterior) is assumed. Independence of the representation from nuisance variable is encouraged by adding a maximum mean discrepancy (MMD, Gretton et al., 2007) between conditional variational posteriors; in Lopez et al. (2018), MMD is replaced by the Hilbert-Schmidt Independence Criterion (HSIC, Gretton et al., 2007). Other authors employ penalties derived from the mutual information (MI) (Moyer et al., 2018; Song et al., 2019; Creager et al., 2019). Another example is the Fader Networks (Lample et al., 2018), in which the deterministic decoder takes an additional input of the attribute (such as whether or not eyeglasses are present in a portrait) and an adversarial penalty that hinders the accurate prediction of the attribute by the deterministic, unfactorized encoder.\n\nThese examples illustrate that, while the generative model (decoder structure) can be chosen suitably for the downstream task, there is no principled way of imposing the corresponding encoder structure. In this paper, we show that the WAE framework allows us to automatically determine the encoder structure corresponding to imposed decoder structure. Specifically, when the deterministic decoder g in Eq. (2) is modified to handle the conditional independence structure of the imposed generative model, then the constraint set (amounting to the Q in Eq. (3)) that makes the LHS of Eq. (2) a proper (power of) Wasserstein distance determines the factorization of the (deterministic) encoder. In practice, the hard constraints in Q is relaxed and Eq. (2) is solved in a penalized form. Following the approach of Tolstikhin et al. (2018), the cited constraint set can be systemically translated to penalties. Therefore, in addition to the theoretical advantage that the penalized form equals a genuine distributional distance for sufficiently large penalty parameter while that of Eq. (1) remains a lower bound of the negative log-likelihood of the model, the ad hoc manner of designing penalties prevalent in the VAE literature can be avoided in the WAE framework. Further, the allowance of deterministic encoder/decoder promotes better generation performance in many downstream tasks.\n\nWe explain how the WAE framework leads to structured encoders given a generative model through examples reflecting downstream tasks in Sect. 3 after providing necessary background in Sect. 2. We would call these structured uses of WAEs the Wasserstein Fair Autoencoders (WFAEs). After reviewing relevant ideas in Sect. 4, WFAEs are experimented in Sect. 5 for datasets including VGGFace2 (Cao et al., 2018). We conclude the paper in Sect. 6.\n\n2 PRELIMINARIES\n\nIn fitting a given probability distribution PX of a random variable X on a measurable space (X , B(X )), where X ⊂ RD equipped with metric d, by a generative model PG of sample G on the same measurable space, one may consider minimizing the (pth power of) p-Wasserstein distance between the two distributions, i.e.,\n\n(cid:26)\n\nmin PG∈M\n\nW p\n\np (PX , PG) :=\n\ninf π∈P(PX ,PG)\n\n(cid:27)\n\nEπ dp(X, G)\n\n.\n\nHere, M is the model space of probability distributions, P(PX , PG) is the coupling or the set of all joint distributions on (X × X , B(X × X )) having marginals PX and PG. Often the sample G is generated by transforming a variable in a latent space. When G = g(Z) a.s. for a latent variable Z in a probability space (Z, B(Z), PZ), Z ⊂ Rl, and measurable function g, then PG is denoted by g♯PZ, where ♯ is the push forward operator. In this setting, as discussed in Sect. 1, Tolstikhin et al.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nZ\n\nS\n\ng\n\nG\n\nY\n\nS\n\nZ2\n\ng2\n\nZ1\n\ng1\n\nG\n\nY\n\nZ\n\nS\n\nG g\n\n(a) Example 1\n\n(b) Example 2\n\n(c) Example 3\n\nFigure 1: Examples of generative models for WFAEs\n\n(2018) show that W p p (PX , g♯PZ) = DWAE(PX , g♯PZ) (Eq. (2)), with the constraint set Q on the probabilistic encoders QZ|X given in Eq. (3). It is further claimed by Patrini et al. (2020) that the set of conditional distributions QZ|X can be reduced to be deterministic, i.e., Z = f (X) a.s. for f measurable. However, this claim is not in general true unless g is injective:\n\nTheorem 1 Let d(x, y) = ∥x − y∥2 for x, y ∈ X . If PX has a density with respect to the Lebesgue measure, and the measurable function g : Z → X is injective, then\n\nW 2\n\n2 (PX , g♯PZ) = inf\n\nf ∈Q\n\nEPX d2(X, g(f (X))),\n\n(4)\n\nwhere Q is the set of all measurable functions from X to Z such that f♯PX = PZ.\n\nThe proof of this result is provided in Appendix A of the Supplement.\n\nRemark 1 In Patrini et al. (2020, Theorem A.2), it is incorrectly claimed that for the right inverse ̃g of g when the codomain of the latter is restricted to its range, ( ̃g ◦ g)♯PZ(F ) is equal to PZ( ̃g−1(g−1(F )), instead of the correct PZ(g−1( ̃g−1(F ))). This confusion invalidates the rest of the argument of the cited theorem.\n\nIn practice the set Q can be relaxed to F, a class of all measurable functions parameterized by deep neural networks, which contains a minimizer of the right-hand side (RHS) of Eq. (4); the constraint f♯PX = PZ can be met by adding a penalty λD(f♯PX ∥PZ) for sufficiently large multiplier λ > 0 and a divergence D between two distribution. Thus if we define the distortion criterion\n\nδ(f, g) = EPX d2(X, g(f (X))) + λD(f♯PX ∥PZ),\n\nthen the generative modeling problem based on 2-Wasserstein distance can be formulated as\n\ninf g∈G\n\ninf f ∈F\n\nδ(f, g),\n\n(5)\n\nfor G a set of injective measurable functions from Z to X , typically parameterized by deep neural networks. The function f : X → Z has an interpretation of an encoder and g : Z → X has an interpretation of a decoder. Typically l ≪ D.\n\n3 LEARNING INVARIANT REPRESENTATIONS WITH WFAES\n\nOften generative modeling is more complicated than just involving a latent variable Z in Z and its reconstruction G in X . For example, data may come with labels, which can be employed in the generation process to learn invariant representations.\n\nExample 1 Let us begin with a simple generative model shown in Fig. 1a (Louizos et al., 2016, Fig. 1); see also (Kingma et al., 2014, M2). Here, variable S ∈ S ⊂ RB represents the observed nuisance variation, and Z models the remaining information on G (with which we want to mimic the observable variable X) that is independent of S. Thus the Z encodes the representation invariant to the unwanted variation in S. Denoting the marginal distribution of the nuisance variable S by PS, the distribution of model G is g♯(PZ ⊗ PS), where ⊗ is used to denote a product distribution. The goal is to make the joint distribution PGS of (G, S) close to PXS of (X, S), the observable.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nIf we let ̃g(z, s) = (g(z, s), s), then PGS = ̃g♯(PZ ⊗ PS). Recall that X is equipped with metric d. Equip S with another metric d′ and X × S with ̃d = (cid:112)d2 + (d′)2. Then, by applying Theorem 1 to PXS, PZ ⊗ PS, and ̃g, we obtain\n\nW 2\n\n2 (PXS, ̃g♯(PZ ⊗ PS)) = inf ̃f ∈ ̃F\n\nEPXS\n\n ̃d2 (cid:16)\n\n[X, S], ̃g( ̃f (X, S))\n\n(cid:17)\n\n= inf f ∈F\n\nEPXS\n\n ̃d2 ([X, S], [g(f (X, S), S), S]) = inf\n\nf ∈F\n\nEPXS d2 (X, g(f (X, S), S)) ,\n\nwhere ̃F = { ̃f : X × S → Z × S : ̃f♯PXS = PZ ⊗ PS}, F = {f : X × S → Z : (f, ΠS)♯PXS = PZ ⊗ PS}, and ΠS : X × S → S : ΠS(x, s) = s is the orthogonal projection from X × S onto S. The second equality holds by noting that ̃f (x, s) = (f (x, s), h(x, s)) and taking h = ΠS. The latter constraint set F means that\n\nf (X, S) d= Z,\n\nf (X, S) ⊥⊥ S.\n\n(6)\n\nFollowing formulation equation 5 for the unstructured case, we can incorporate constraint equation 6 into the learning problem in a penalized form\n\nmin g\n\nmin f\n\nEPXS d2(cid:0)X, g(f (X, S), S)(cid:1) + λ1D(f♯PXS∥PZ) + λ2H((f, ΠS)♯PXS),\n\nwhere D is an appropriate divergence between two probability distributions such as MMD or the generative adversarial network (GAN) loss as suggested by Tolstikhin et al. (2018), and H promotes independence between two random variables f (X, S) and S, such as the HSIC (Lopez et al., 2018).\n\nExample 2 Consider a more involved generative model shown in Fig. 1b, which is employed by the VFAE (Louizos et al., 2016, Fig. 2) as an extension of the “M1 + M2” semi-supervised model (Kingma et al., 2014). This graphical model actually describes the conditional distribution PX|S of X given S, since S and Y are allowed to be correlated. Instead, it is required\n\nZ1 ⊥⊥ S\n\n(7)\n\nin order to impose invariance to the nuisance variable S. Let g : Y × Z2 × S → X × Y × S as g(y, z2, s) = (cid:0)g1 (cid:0)g2(y, z2), s(cid:1), y, s(cid:1). Denoting the marginal distribution of the nuisance variable S by PS and the joint distribution of Y and S by PY S, the distribution of model G is g♯(PY S ⊗ PZ2 ). The goal is to make the joint distribution PGS of (G, S) close to PXS of (X, S) when Y is not observed, and PGY S of (G, Y, S) to PXY S of (X, Y, S) when the data is fully observed.\n\nFirst consider the case that Y is missing. Let ΠXS be the orthogonal projection operator from X × Y × S onto X × S. Then by applying Theorem 1 to PXS, PY S ⊗ PZ2, and ΠXSg, we obtain\n\nW 2\n\n2 (PXS, ΠXSg♯(PY S ⊗ PZ2 )) = inf\n\nf ∈Funobs\n\nEPXS\n\n ̃d2 ([X, S], ΠXSg(f (X, S), S))\n\n= inf\n\nf ∈Funobs\n\nEPXS\n\n ̃d2 ([X, S], [g1(g2(f (X, S)), S), S]) = inf\n\nf ∈Funobs\n\nEPXS d2(X, g1(g2(f (X, S)), S)),\n\nwhere Funobs = {(f unobs Z2, (f unobs\n\n, ΠS, f unobs\n\n: X × S → Y, f unobs 1\n)♯PXS = PY S ⊗ PZ2}. The latter constraint set means\n\n)| f unobs 1\n\n, f unobs 2\n\n2\n\n2\n\n1\n\n: X × S →\n\n(f unobs\n\n1\n\n(X, S), S) d= (Y, S),\n\nf unobs\n\n2\n\n(X, S) d= Z2,\n\n(f unobs\n\n1\n\n(X, S), S) ⊥⊥ f unobs\n\n2\n\n(X, S).\n\n(8)\n\nNow consider the case Y is observed. Equip Y with a metric d′′ and X × Y × S with ̆d = (cid:112)d2 + (d′′)2 + (d′)2. Then by applying Theorem 1 to PXY S, PY S ⊗ PZ2 , and g, we obtain\n\nW 2\n\n2 (PXY S, g♯(PY S ⊗ PZ2 )) = inf\n\nf obs\n\n2 ∈Fobs\n\n= inf f obs\n\n2 ∈Fobs\n\n= inf f obs\n\n2 ∈Fobs\n\nEPXY S\n\n ̆d2 (cid:0)[X, Y, S], g(Y, f obs\n\n2\n\n(X, Y, S), S)(cid:1)\n\nEPXY S\n\n ̆d2 (cid:0)[X, Y, S], [g1(g2(Y, f obs\n\n2\n\n(X, Y, S)), S), Y, S](cid:1)\n\nEPXY S d2(X, g1(g2(Y, f obs\n\n2\n\n(X, Y, S)), S)),\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nwhere Fobs = {f obs (x, y, s) (cid:55)→ y, ΠS : (x, y, s) (cid:55)→ s are projections. The latter constraint set means\n\n: X × Y × S → Z2 : (ΠY , ΠS, f obs\n\n2\n\n2\n\n)♯PXY S = PY S ⊗ PZ2 } and ΠY :\n\nf obs\n\n2\n\n(X, Y, S) d= Z2,\n\n(Y, S) ⊥⊥ f obs\n\n2\n\n(X, Y, S).\n\n(9)\n\nIn order to combine the two Wasserstein losses and constraints Eq. (7) to (9), let us extend Y to ̄Y = Y ∪ {∗}, where ‘∗’ represents the missing value. For any (f unobs ) ∈ Funobs and 2 ∈ Fobs, define f1 : X × ̄Y × S → Y and f2 : X × ̄Y × S → Z2 as f obs (cid:26)f obs\n\n, f unobs 2\n\n(cid:26)y,\n\n1\n\ny ̸= ∗, y = ∗,\n\nf2(x, y, s) =\n\n(x, y, s), (x, s),\n\n2\n\nf unobs\n\n2\n\ny ̸= ∗, y = ∗.\n\nf1(x, y, s) =\n\nf unobs\n\n1\n\n(x, s),\n\nThen we can formulate the learning problem for the WFAE in a penalized form\n\nmin g1,g2\n\nmin f1,f2\n\nEPXY S d2(cid:16)\n\nX, g1\n\n(cid:0)g2(Y, f2(X, Y, S)), S(cid:1)(cid:17)\n\n+ EPXS d2(cid:16)\n\nX, g1\n\n(cid:0)g2(f1(X, ∗, S), f2(X, ∗, S)), S(cid:1)(cid:17)\n\n+ λ1D1((f1, ΠS)♯PXY S∥PY S) + λ2D2(f2♯PXY S∥PZ2) + λ3H3((f1, ΠS, f2)♯PXY S) + λ4H4((g2 ⋆ f1)♯(PXY S ⊗ PZ2)),\n\nwhere g2 ⋆ f1(x, y, z2, s) = (g2(f1(x, y, s), z2), s); D1 and D2 are appropriate divergences between two probability distributions, and H3, H4 promotes independence between two random variables. Note, unlike Example 1 in which only the encoder f is constrained, Eq. (7) imposes a constraint on the decoder g2. Also note that, the divergence D1 can be estimated in a two-sample fashion, namely from the a sample drawn from PY S, i.e., (yi, si) with yi observed, and another sample drawn from (f1, ΠS)♯PXY S, either as (yj, sj) if yj is observed or (f1(xj, ∗, sj), sj) otherwise. Hence all the data from the minibatch can be utilized. Likewise, divergence D2 and the independence penalties H3 and H4 can utilize the full minibatch.\n\nRemark 2 VAE-based models, e.g., VFAE, assume a specific factorization of the variational posterior (encoder). Since the factor qφ(y|z) for imputing Y does not appear in the evidence lower bound (ELBO) of the observed likelihood, an additional penalty on this factor evaluated for the fully observed sample is coined (Louizos et al., 2016, Eq. 5), making the bound not tight. In the WFAE, on the contrary, the D1 term that arises naturally from constraint equation 8 for the Wasserstein distance penalizes the imputing encoder f1 for both fully (by requiring f1(xj, yj, sj) = yj) and partially (by the divergence) observed samples.\n\nExample 3 The model shown in Fig. 1c extends Example 1 with two independent nuisance variables that can be missing. Here Y may represent a person’s identity in her portrait, which may be missing, and S partially observed attributes (e.g., sunglasses on/off, mouth open/closed, and gender). In this setup we want two different portraits of a person to have similar values of Z, and those of two different people to have quite distinct values of Z, even if the encoder does not know whose portraits they are. We may also want Z to represent something immune even to gender switch.\n\nProceeding as Example 2, we obtain for ̃g : Y × Z × S → X × Y × S: ̃g(y, z, s) = (g(y, z, s), y, s),\n\nW 2\n\n2 (PX , ΠX ̃g♯(PY ⊗ PZ ⊗ PS)) =\n\nEPX d2(X, g(f X\n\n1 (X), f X\n\n2 (X), f X\n\n3 (X))),\n\n3 )∈F X F X = {(f1, f2, f3) : (f1, f2, f3)♯PX = PY ⊗ PZ ⊗ PS, f1 : X → Y, f2 : X → Z, f3 : X → S}\n\n(f X\n\n1 ,f X\n\ninf 2 ,f X\n\nwhen both Y and S are unobserved,\n\nW 2\n\n2 (PXS, ΠXS ̃g♯(PY ⊗ PZ ⊗ PS)) =\n\ninf ,f XS 2\nF XS = {(f1, f2) : (f1, f2, ΠS)♯PXS = PY ⊗ PZ ⊗ PS, f1 : X × S → Y, f2 : X × S → Z}\n\nEPXS d2(X, g(f XS\n\n(X, S), f XS\n\n)∈F XS\n\n(f XS 1\n\n2\n\n1\n\n(X, S), S)),\n\nwhen only Y is unobserved,\n\nW 2\n\n2 (PXY , ΠXY ̃g♯(PY ⊗ PZ ⊗ PS)) =\n\ninf ,f XY 3\nF XY = {(f2, f3) : (ΠY , f2, f3)♯PXY = PY ⊗ PZ ⊗ PS, f2 : X × Y → Z, f3 : X × Y → S}\n\nEPXY d2(X, g(Y, f XY\n\n(X, Y ), f XY\n\n)∈F XY\n\n(f XY 2\n\n3\n\n2\n\n(X, Y ))),\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nwhen only S is unobserved, and\n\nW 2\n\n2 (PXY S, ̃g♯(PY ⊗ PZ ⊗ PS)) =\n\nEPXY S d2(X, g(Y, f XY S\n\n2\n\n(X, Y, S), S)),\n\nf XY S F XY S = {f2 : (ΠY , f2, ΠS)♯PXY S = PY ⊗ PZ ⊗ PS, f2 : X × Z → S}\n\n3\n\ninf ∈F XY S\n\nwhen the data are fully observed. If we expand Y to ̄Y = Y ∪ {∗} and S to ̄S = S ∪ {∗}, then the learning problem is\n\nmin g\n\nmin (f1,f2,f3)∈F\n\nEPXY S d2(cid:0)X, g(cid:0)Y, f2(X, Y, S), S)(cid:1)(cid:1) + EPXY d2(cid:0)X, g(cid:0)Y, f2(X, Y, ∗), f3(X, Y, ∗))(cid:1)(cid:1)\n\n+ EPXS d2(cid:0)X, g(cid:0)f1(X, ∗, S), f2(X, ∗, S), S)(cid:1)(cid:1) + EPX d2(cid:0)X, g(cid:0)f1(X, ∗, ∗), f2(X, ∗, ∗), f3(X, ∗, ∗))(cid:1)(cid:1) + λ1D1(f1♯PXY S∥PY ) + λ2D2(f2♯PXY S∥PZ) + λ3D3(f3♯PXY S∥PS) + λ4H4((f1, f2, f3)♯PXY S),\n\nwhere H4 measures dependence of three random variables, e.g., the d-variate HSIC (Lopez et al., 2018) with d = 3, and F = {(f1, f2, f3) : f1 : X × ̄Y × ̄S → Y, f2 : X × ̄Y × ̄S → Z, f3 : X × ̄Y × ̄S → S}, for\n\nf1(x, y, s) =\n\n \n\n\n\n1 (x),\n\nf X f XS 1\ny,\n\n(x, s),\n\ny = ∗, s = ∗, y = ∗, s ̸= ∗, y ̸= ∗,\n\nf3(x, y, s) =\n\n \n\n\n\nf X f XY 3\ns,\n\n3 (x),\n\n(x, y),\n\ny = ∗, s = ∗, y ̸= ∗, s = ∗, s ̸= ∗,\n\nand f2(x, y, s) is equal to f X y = ∗, s ̸= ∗, and to f XY S\n\n2 (x) if y = ∗, s = ∗, to f XY (x, y, s) otherwise.\n\n2\n\n2\n\n(x, y) if y ̸= ∗, s = ∗, to f XS\n\n2\n\n(x, s) if\n\nRemark 3 If variable Y is removed and S is fully observed, Example 3 reduces to Example 1, where the f2(x, y, s) from the former corresponds to the f (x, s) from the latter. The Fader Networks (Lample et al., 2018) implicitly use this model to obtain attribute-invariant representations of facial images. The adversarial penalty for training the network (Lample et al., 2018, Eq. 2) can be understood as promoting independence between S and ˆZ = f (X, S). While in Lample et al. (2018) the encoder f does not depend on S, Example 1 shows that it is more natural to take S as an input to remove its effect on ˆZ. Example 3 can be considered a generalization of the Fader Networks for missing attributes and unknown identities.\n\n4 RELATED WORK\n\nThe literature on VAEs is vast. β-VAE (Higgins et al., 2017) is one of well-known ways of adding penalties to the ELBO of a VAE, which adds one proportional to the expected KL divergence between the variational posterior (encoder) and prior PZ. It is observed that this penalty promotes factorization of the aggregate posterior QZ (Kim & Mnih, 2018). In fair representation, VFAE (Louizos et al., 2016) and HSIC-constrained VAE (HCV, Lopez et al., 2018) add penalties to the ELBO for semi-supervised disentanglement along this line. Adversarial penalties have been also considered (Edwards & Storkey, 2016; Madras et al., 2018). Song et al. (2019) bring an information-theoretic interpretation to these approaches. In this regard, penalizing MI between nuisance variable S and encoded latent variable Z (Moyer et al., 2018; Song et al., 2019; Creager et al., 2019) or its tractable upper bounds, e.g., based on a variational approximation (Rodr ́ıguez-G ́alvez et al., 2021), has been advocated. Recently proposed FairDisCo (Liu et al., 2022) uses the L2 distance between the joint density of S and Z and the product density of their marginals, showing its asymptotic equivalence to the MI. However, as stated in Sect. 1, these penalties promoting desired structures are chosen rather ad hoc and loosens the already-not-tight ELBO. Furthermore, there is no principle for choosing the encoder structure corresponding the imposed decoder structure.\n\nThe WAE framework discussed in the previous section can overcome these pitfalls in VAEs. The WAE literature has focused on improving the divergence in the penalized form of Eq. (2) that matches the prior PZ and the aggregated posterior. The original proposal by Tolstikhin et al. (2018) is to employ either the MMD or GAN divergence. Kolouri et al. (2019) propose to use the sliced Wasserstein distance in order to simplify computation. Patrini et al. (2020) consider the Sinkhorn\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Fair representation trade-off.\n\nFigure 3: T-SNE map of Z1 in Extended Yale B\n\ndivergence (Genevay et al., 2018), computing of which can be boosted by using the Sinkhorn algorithm (Cuturi, 2013). Xu et al. (2020) and Nguyen et al. (2021) propose and improve the relational divergence called the fused Gromov-Wasserstein distance. The latter three works consider the setting in which the prior PZ is structured. In contrast, we focus on the setting in which the decoder is structured and nuisance information is (partially) available. According to the taxonomy of Tschannen et al. (2018), the former is close to the clustering meta-prior whereas the latter is close to the disentangling one. We emphasize that the cited divergences are compatible with our framework.\n\n5 EXPERIMENTS\n\nWe experimented WFAEs with various real-world datasets. The generative models for these datasets mainly follow Examples 2 and 3, in most of which variable Y (and sometimes S) has the meaning of a label and thus categorical. In order to embed this variable to the Euclidean space RB where B does not necessarily depend on the number of categories, we employed the entity embedding network (Guo & Berkhahn, 2016) for observed labels. The trained embedding network naturally becomes a pretrained encoder f1 or f3 from Examples 2 and 3. A by-product of this embedding is that it is even possible to impute categories not present in training data.\n\n5.1 FAIR REPRESENTATIONS\n\nTo demonstrate the performance of WFAEs on fair representation, we reproduced experiments in Liu et al. (2022) using two categorical datasets, namely the Adult Income and Health datasets. Refer to the appendix for data summary and network implementation. The generative model for the WFAE was the structure of Example 2. With the Z1 = g2(Y, f2(X, Y, S)) encoded from the trained model, we quantified the trade-off between fairness and utility (Zhao et al., 2017): we classified S and Y using random forest method, calculated the area under the ROC curve (AUC) on the test data (sAUC and yAUC) as a function of demographic parity ∆DP, and compared the performance with the HSICconstrained VFAE (HCV) and the FairDisCo. The results are summarized in Fig. 2. While WFAE shows a clear trade-off, other methods are relatively insensitive to demographic parity.\n\n5.2\n\nINVARIANT REPRESENTATIONS\n\nThe same structure as Sect. 5.1 is used to test the ability of WFAEs to learn invariant representations of controlled photographs. The cropped Extended Yale B dataset Georghiades et al. (2001); Lee et al. (2005) comprises of facial images of 38 human subjects in various lighting conditions. For each subject, the pictures of the person are split into training and test data with a fixed ratio, resulting in 1,664 and 750 images for the training and test respectively. We set the identity of the image as Y and the lighting condition (elevation and azimuth of the light direction normalized into [−1, 1] × [−1, 1]) as S. In the training stage, we first trained f1 to estimate Y , then trained the rest of the network with f1 held fixed. In consequence, we were able to encode and decode the test data without the information about Y by replacing it with f1(X, ∗, S). Although we trained the model\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nNetwork\n\nVAE HCV\n\nFairDisCo WFAE\n\nID accuracy\n\n0.71 0.75 1.0 0.97\n\nLighting group (Acc.)\n\nLighting direction (MSE)\n\nRF\n\n0.74 0.60 0.34 0.35\n\nLogistic\n\n0.74 0.45 0.37 0.28\n\nRF\n\n0.03 0.13 -\n0.23\n\nLinear\n\n0.07 0.22 -\n0.23\n\nInvariant\n\nTable 1: representation of Extended Yale B. RF=random forest, Logistic=logistic regression, Linear=linear regression. Classification accuracy for discrete and mean squared error for continuous variables.\n\nNetwork\n\nFairDisCo WFAE Test Data\n\nExtended Yale B\n\nMNIST\n\nSharpness\n\n3.36e-4 4.43e-4 3.09e-3\n\nFID\n\n63.7 66.5 -\n\nSharpness\n\nFID\n\n3.45e-2 9.79e-2 1.83e-1\n\n111.1 19.4 -\n\nTable 2: Sample generation quality measures.\n\nwith continuous S, we present some of the results with S categorized in 5 directions, as in Lopez et al. (2018). The results are presented Table 1. The Z1 encoded by WFAE shows better performance in predicting Y and worse in predicting S than others, suggesting better invariant representation. The t-SNE visualization of Z1 in Fig. 3 accords with this result, showing noticeable separation with respect to Y , but not with respect to S. In panel A of Fig. 4 (top left), the green box depicts generated images by encoding the test image X and nuisance data S into Z1 = g2(f1(X, ∗, S), f2(X, ∗, S)), and then computing g1(Z1, S). Those in the red box were generated by using the same Z1 but setting S = (±0.3, ±0.3). WFAE produced reconstructions closer to the input than HCV and FairDisCo, and perturbing S only kept the identity of the input in the generated images. The sharpness and the Fr ́echet inception distance (FID) scores are shown Table 2 to assess the sample generation quality. WFAE produced sharper images than FairDisCo, confirming the visual inspection. The FID scores should be taken with caution, though. Since the sample generation is conducted by varying the “lighting direction” attribute (considered as the S variable) the generated samples should be different from the test data with scarce images. Rather, it may indicate samples generated from FairDisCo is less sensitive to S, which can also be verified visually.\n\n5.3 CONDITIONAL GENERATION\n\nWe further investigated the conditional generation capability of WFAEs using the MNIST and VGGFace2 datasets (Cao et al., 2018).\n\nMNIST. We treated the digit attribute as S. The generative model for the data is similar to Example 3, but without Y . We first trained encoder f3 that estimates S, then trained the rest of the network. The final network was tested with images with digit information removed. We also trained a network without the encoder f2 for Z, hence it decodes an image using only estimated S. Fig. 4 summarize the results, all of which were generated from test data without information of S. Penal A (top right) shows decoded samples from g with estimated S and i) not using f2 (blue box), ii) using encoded Z = f2(X, S) from the test data (green box), and iii) using Z sampled from prior PZ (red box). Decoded images with the same S all retained their digit information. Reconstruction without using f3, although recognizable, produced degraded images, implying loss of information. FairDisCo with a similar architecture produced quite degraded results; see also Table 2.1 In panel B (top), we estimated Z from the source and S from the target and generated new images by g(Z, S).\n\nVGGFace2. This dataset contains 3.14M training images of faces of total 8631 subjects and 169k test images of total 500 subjects, with partially observed binary attributes such as gender, wearing of sunglasses, and openness of mouth, available for a subset of 30,000 images. Here, we treat the identity of the image as class Y and the vector of attributes as S. The generative model for this dataset is the same as Example 3. The class-preserving generation and style transfer tasks were conducted in the same manner as MNIST. In addition, we also tried generating samples with manipulated attributes. Since the attribute encoder f3 embeds S in the Euclidean space, we could extrapolate input S to decoder g beyond 0 and 1. For this attribute manipulation task, we compared results with Fader Networks trained with a similar architecture. Fig. 4 shows sample images for all tasks. Although images of persons who were not in the training data were used, the WFAE could successfully generate images retaining the identity while employing other identity-invariant\n\n1In this experiment there is no Y and the digit class plays the role of S. So the generation is usual class-\n\nconditional one, hence the FID scores are lower for the WFAE as expected from visual comparison.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Conditional generation examples of WFAE\n\nfeatures, e.g., camera angle, lighting condition (panel A, bottom). Note in the style transfer task (panel B, bottom), the generated images possess the styles from the source data and tend to preserve the specified attribute of the target data. For example, the generated images tend to have open mouth if the target image has mouth wide open. In the attribute manipulation task, we could successfully generate images with the desired attributes changed. In panel C, letting the “Mouth Open” attribute positive produced decoded images having grinning mouth; making it negative produced images with lips all closed. For the Fader Networks, we extrapolated the attribute scores to a large magnitude as far as ±400, but it only caused deformation of the original image; see Remark 3.\n\n6 CONCLUSION\n\nWe have shown that the WAE framework is rich enough to handle various conditional independence structures, leading to much more principled formulation of learning problems than the VAE counterparts. Importantly, a conditional independence structure imposes on the decoder automatically determines the encoder structure and the associated constraints. We hope this paper stimulates further research on extensions of WAEs in this direction, for instance, to complex hierarchical structures.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nQiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In Proc. 13th IEEE Int. Conf. Automatic Face & Gesture Recognition (FG 2018), pp. 67–74. IEEE, 2018.\n\nElliot Creager, David Madras, J ̈orn-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard Zemel. Flexibly fair representation learning by disentanglement. In International conference on machine learning, pp. 1436–1445. PMLR, 2019.\n\nMarco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In Adv. Neural\n\nInf. Process. Syst. (NIPS 2013), 2013.\n\nHarrison Edwards and Amos Storkey. Censoring representations with an adversary. In International\n\nConference on Learning Representations, 2016.\n\nAude Genevay, Gabriel Peyre, and Marco Cuturi. Learning Generative Models with Sinkhorn Divergences. In Proc. 21st Int. Conf. Artif. Intell. Statist. (AISTATS 2018), pp. 1608–1617. PMLR, 2018. URL https://proceedings.mlr.press/v84/genevay18a.html. ISSN: 2640-3498.\n\nAthinodoros S. Georghiades, Peter N. Belhumeur, and David J. Kriegman. From few to many: IEEE Trans.\n\nIllumination cone models for face recognition under variable lighting and pose. Pattern Anal. Mach. Intell., 23(6):643–660, 2001.\n\nArthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bernhard Sch ̈olkopf, and Alex Smola. A kernel statistical test of independence. Adv. Neural Inf. Process. Syst. (NIPS 2007), 20, 2007.\n\nCheng Guo and Felix Berkhahn. Entity Embeddings of Categorical Variables. April 2016. URL\n\nhttps://arxiv.org/abs/1604.06737v1.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In Proc. Int. Conf. Learn. Represent. (ICLR 2017), pp. 22, 2017.\n\nHyunjik Kim and Andriy Mnih. Disentangling by factorising.\n\nIn Proc. Int. Conf. Mach. Learn.\n\n(ICML 2018), pp. 2649–2658. PMLR, 2018.\n\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proc. Int. Conf.\n\nLearn. Represent. (ICLR 2014), 12 2014.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proc. Int. Conf. Learn.\n\nRepresent. (ICLR 2014), 2014.\n\nDiederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-Supervised Learning with Deep Generative Models. In Adv. Neural Inf. Process. Syst. (NIPS 2014). arXiv, October 2014. Number: arXiv:1406.5298 arXiv:1406.5298 [cs, stat].\n\nSoheil Kolouri, Phillip E Pope, Charles E Martin, and Gustavo K Rohde. Sliced-wasserstein auto-\n\nencoders. In Proc. Int. Conf. Learn. Represent. (ICLR 2019), pp. 19, 2019.\n\nGuillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, and In Adv. Marc’Aurelio Ranzato. Fader Networks: Manipulating Images by Sliding Attributes. Neural Inf. Process. Syst. (NIPS 2018). arXiv, January 2018. URL http://arxiv.org/ abs/1706.00409. Number: arXiv:1706.00409 arXiv:1706.00409 [cs].\n\nKuang-Chih Lee, Jeffrey Ho, and David J Kriegman. Acquiring linear subspaces for face recognition\n\nunder variable lighting. IEEE Trans. Pattern Anal. Mach. Intell., 27(5):684–698, 2005.\n\nJi Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao Xu, and Hanghang Tong. Fair representation learning: An alternative to mutual information. In Proc. ACM SIGKDD Conf. Knowledge Discovery and Data Mining (KDD 2022), pp. 1088–1097. Association for Computing Machinery, 2022. URL https://doi.org/10.1145/3534678.3539302.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRomain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on autoencoding variational Bayes. In Adv. Neural Inf. Process. Syst. (NeurIPS 2018), pp. 6114–6125, 2018.\n\nChristos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard S Zemel. The variational fair\n\nautoencoder. In Proc. Int. Conf. Learn. Represent. (ICLR 2016), 2016.\n\nDavid Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and transferable representations. In International Conference on Machine Learning, pp. 3384–3393. PMLR, 2018.\n\nDaniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Galstyan, and Greg Ver Steeg.\n\nInvariant representations without adversarial training. Advances in Neural Information Processing Systems, 31, 2018.\n\nKhai Nguyen, Son Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein. In Proc. Int. Conf. Learn. Represent. (ICLR 2021), 2021. URL http://arxiv.org/abs/2010.01787. arXiv: 2010.01787.\n\nGiorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertain. Artif. Intell. (UAI 2020), pp. 733–743, 2020.\n\nGabriel Peyr ́e and Marco Cuturi. Computational optimal transport. Foundations and Trends in\n\nMachine Learning, 11(5–6):355–607, 2019.\n\nBorja Rodr ́ıguez-G ́alvez, Ragnar Thobaben, and Mikael Skoglund. A variational approach to pri-\n\nvacy and fairness. In 2021 IEEE Information Theory Workshop (ITW), pp. 1–6. IEEE, 2021.\n\nJiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning controllable fair representations. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2164–2173. PMLR, 2019.\n\nIlya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Sch ̈olkopf. Wasserstein auto-\n\nencoders. In Proc. Int. Conf. Learn. Represent. (ICLR 2018), 2018.\n\nMichael Tschannen, Olivier Bachem, and Mario Lucic. Recent Advances in Autoencoder-Based Representation Learning. In 3rd Workshop on Bayesian Deep Learning, Montreal, Canada, December 2018. URL http://arxiv.org/abs/1812.05069. arXiv: 1812.05069.\n\nHongteng Xu, Dixin Luo, Ricardo Henao, Svati Shah, and Lawrence Carin. Learning Autoencoders with Relational Regularization. In Proc. Int. Conf. Mach. Learn. (ICML 2020), volume 119, pp. 10576–10586. PMLR, 2020. arXiv: 2002.02913.\n\nOmry Yadan. Hydra - a framework for elegantly configuring complex applications. Github, 2019.\n\nURL https://github.com/facebookresearch/hydra.\n\nRichard Zemel. Learning Fair Representations. volume 28 of 3, pp. 325–333. PMLR, 2013.\n\nIn Proc. Int. Conf. Mach. Learn. (ICML 2013),\n\nShengjia Zhao, Jiaming Song, and Stefano Ermon. Learning hierarchical features from deep gener-\n\native models. In Proc. Int. Conf. Mach. Learn. (ICML 2017), pp. 4091–4099, 2017.\n\nA PROOF OF THEOREM 1\n\nProof 1 Under the conditions of the theorem statement, the Monge-Kantorovich equivalence holds (see, e.g., Peyr ́e & Cuturi, 2019, Theorem 2.1):\n\nW 2\n\n2 (PX , PG) =\n\ninf T :X →X :T♯PX =PG\n\nEPX d2(X, T (X)).\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHence it suffices to show that\n\ninf f :X →Z:f♯PX =PZ\n\n(cid:90)\n\nX\n\nor equivalently\n\nd2(x, g(f (x)))dPX =\n\ninf T :X →X :T♯PX =PG\n\n(cid:90)\n\nX\n\ndp(x, T (x))dPX\n\n{g ◦ f : f : X → Z, f♯PX = PZ} = {T : X → X : T♯PX = PG}.\n\nThe forward inclusion ⊂ holds since for any measurable f : X → Z such that f♯PX = PX f −1 = PZ we have g ◦ f : X → X measurable and for any Borel set E ⊂ X\n\n(g ◦ f )♯PX (E) = PX (g ◦ f )−1(E) = PX (f −1(g−1(E)))\n\n= g♯[PX q−1](E) = g♯f♯PX (E) = g♯PZ(E) = PG(E).\n\nFor the backward inclusion ⊃, suppose T : X → X is measurable and satisfies T♯PX = PG. Since g is injective, it has a left inverse g† : X → Z. Let f = g† ◦ T . Then for any Borel set F ⊂ Z,\n\nf♯PX (F ) = PX (g† ◦ T )−1(F ) = PX (T −1((g†)−1(F )))\n\n= T♯PX ((g†)−1(F )) = PG((g†)−1(F )) = g♯PZ((g†)−1(F )) = PZ(g−1((g†)−1(F ))) = PZ((g† ◦ g)−1(F )) = PZ(F ),\n\nwhich completes the proof.\n\nB ADDITIONAL DETAILS FOR THE FAIR REPRESENTATION EXPERIMENT\n\nFollowing Liu et al. (2022), fair representation experiment were held for the Adult Income and Health datasets, whose characteristics are described in Table 3. Note that all variables were categorized: one-hot encoding was used for variables with multiple category to make all data either 0 or 1. The encoder-decoder architecture of the network was adopted from (Louizos et al., 2016) (Table 4)\n\nData\n\nTraining data size\n\nTest Data Number\n\nCovariate Dimension\n\nResponse Variable\n\nSensitive Variable\n\nAdult Income Health\n\n30162 44116\n\n15060 11030\n\n115 254\n\naccount hospitalization\n\ngender age\n\nTable 3: Information on categorical datasets for fair representation task.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nC FURTHER IMPLEMENTATION DETAILS\n\nIn the source code attached, settings for all experiments are gathered as configuration files in directory configs/train info. All the network architecture are listed in .py files in src/model directory, and model and architecture keyword in the configuration file specifies which architecture to use among them. The running script run.sh that states which configuration was used for each experiment, managed by Hydra 1.1.1.Yadan (2019), can be found in the experiments directory. The prior PZ for the encoded Z was set to be a normal distribution N (0, 2Il), where l is the dimension of the latent space Z. For the penalty divergences Di, we used the generative adversarial network (GAN) loss, which requires an additional discriminator (Tolstikhin et al., 2018). All the networks were trained using ADAM (Kingma & Ba, 2014) without any learning rate scheduling.\n\nExtended Yale B The cropped version of the Extended Yale Face Database B dataset (Georghiades et al., 2001; Lee et al., 2005) were resized into a size of 128×128. The encoder-decoder architecture of the network had total of 18.5M parameters, and the discriminator architecture had 881 parameters (Table 5). After pre-training the Y -encoder with 2,100 iterations, we optimized the network for 5,200 iterations, which took about 40 minutes. The results were compared with the HSIC-contrained variational fair autoencoder (HCV, Table 6) and FairDisCo Table 7.\n\nMNIST The encoder-decoder architecture of the network had 3.8M parameters, and the discriminator architecture had 7.4k parameters (Table 8). We pre-trained the S-encoder with 6,000 iterations, then optimized the rest of the network for 11,700 iterations, which took about half an hour. The results were compared with HCV and FairDisCo with the S information available for decoding (Table 9 and Table 10).\n\nVGGFace2 The face region of the collected data were cropped and resized into a size of 128×128. The encoder-decoder architecture of the network had 88.4M parameters, and the discriminator architecture had 206k parameters (Table 11). We pre-trained the (Y, S)-encoder with 3,000 iterations, then optimized the rest of the network for 30,000 iterations, which took 16 hours. The results were compared with the Fader Network having an encoder-decoder architecture with 70.2M parameters and a discriminator architecture with 483k parameters (Table 12) trained for 20,000 iterations, which took 11 hours.\n\nComputing infrastructure We trained the networks with Intel® Xeon® CPU E5-2650 v4 @ 2.20GHz processors and Nvidia Titan X Pascal GPUs with 12GB memory. For the VGGFace2 experiments, we trained the network using four GPUs; those for the other experiments were all trained using a single GPU. All the implementations were based on Python 3.6, PyTorch 1.10.2, PyTorch Lightning 1.5.10, and CUDA 10.2.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nMap\n\nf2\n\ng1\n\ng2\n\nDiscriminator\n\nLayer\n\nOperation\n\nFilters Batch norm Activation\n\nLinked layer\n\n1 μ\nσ2 Output (Z2|X, Y, S)\n\nDense Dense Dense Sample Z2|X, Y, S\n\n1 2\n\nDense Dense\n\n1 μ\nσ2 Output (Z1|Z2, Y )\n\nDense Dense Dense Sample Z1|Z2, Y\n\n1 2\n3 4\n5\n\nDense Dense Dense Dense Dense\n\n100 dZ dZ -\n\n100 dX\n\n100 dZ dZ -\n\n4dZ 4dZ 4dZ 4dZ 1\n\nYes No No -\n\nYes No\n\nYes No No -\n\nNo No No No No\n\nReLU -\n- -\n\nReLU Sigmoid\n\nReLU -\n- -\n\nReLU ReLU ReLU ReLU -\n\nX 1, Y , S 1, Y , S μ, σ2\n\ng2, S 1\n\nZ2, S 1\n1 μ, σ2\n\nf1 1\n2 3\n4\n\nTable 4: WFAE architecture for the fairness representation experiment: for the Adult income dataset, dX = 115, dZ = 50, and for the Health dataset, dX = 254, dZ = 8.\n\nMap\n\nf1\n\nf2\n\ng1\n\ng2\n\nDiscriminator\n\nLayer\n\nOperation\n\nFilters Kernel\n\nStrides\n\nBatch norm\n\nActivation\n\nLinked layer\n\n1 2\n3 4\n5 6\n\n1 2\n3 4\n5 6\n\n1 2\n3 4\n5 6\n7 8\n\n1\n\n1 2\n3 4\n5\n\nConvolution Convolution Convolution Convolution Convolution Dense\n\nConvolution Convolution Convolution Convolution Convolution Dense\n\n64 128 256 512 1024 8\n\n32 64 128 256 512 2\n\nDense Transpose Convolution Transpose Convolution Convolution Transpose Convolution Transpose Convolution Convolution Convolution\n\n8x8x1024 512 256 256 128 64 64 1\n\nDense\n\nDense Dense Dense Dense Dense\n\n50\n\n16 16 16 16 1\n\n5x5 5x5 5x5 3x3 3x3 -\n\n5x5 5x5 5x5 3x3 3x3 -\n\n- 3x3 3x3 3x3 5x5 5x5 5x5 5x5\n\n-\n\n- -\n- -\n-\n\n2x2 2x2 2x2 2x2 2x2 -\n\n2x2 2x2 2x2 2x2 2x2 -\n\n- 2x2 2x2 1x1 2x2 2x2 1x1 1x1\n\n-\n\n- -\n- -\n-\n\nYes Yes Yes Yes Yes -\n\nYes Yes Yes Yes Yes -\n\nNo Yes Yes Yes Yes Yes Yes No\n\nYes\n\nNo No No No No\n\nReLU ReLU ReLU ReLU ReLU -\n\nReLU ReLU ReLU ReLU ReLU -\n\n- LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU Sigmoid\n\nLeakyReLU\n\nReLU ReLU ReLU ReLU -\n\nX 1\n2 3\n4 5\n\nX 1\n2 3\n4 5\n\ng2 1\n2 3\n4 5\n6 7\n\nf1, f2\n\nf2 1\n2 3\n4\n\nTable 5: WFAE architecture for the Extended Yale B dataset.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nMap\n\nQZ1|X,S\n\nQZ2|Z1,Y\n\nQY |Z1\n\nPZ1|Z2,Y\n\nPX|Z1,S\n\nMap\n\nQZ2|X,S\n\nPZ1|Z2,Y\n\nPX|Z1,S\n\nLayer\n\nOperation\n\nFilters Kernel\n\nStrides Batch norm Activation\n\nLinked layer\n\n1 2\n3 4\n5 μ\nσ2 Output (Z1|X, S)\n\n1 μ\nσ2 Output (Z2|Z1, Y )\n\n1 2\n\n1 μ\nσ2 Output (Z1|Z2, Y )\n\nConvolution Convolution Convolution Convolution Convolution Dense Dense Sample Z1|X, S\n\nDense Dense Dense Sample Z2|Z1, Y\n\nDense Dense\n\nDense Dense Dense Sample Z1|Z2, Y\n\n64 128 256 512 1024 10 10 -\n\n20 10 10 -\n\n20 38\n\n20 10 10 -\n\n1 2\n3 4\n5 6\n7 8\n\nDense Transpose Convolution Transpose Convolution Convolution Transpose Convolution Transpose Convolution Convolution Convolution\n\n8x8x1024 512 256 256 128 64 64 1\n\n5x5 5x5 5x5 3x3 3x3 -\n- -\n\n- -\n- -\n\n- -\n\n- -\n- -\n\n- 3x3 3x3 3x3 5x5 5x5 5x5 5x5\n\n2x2 2x2 2x2 2x2 2x2 -\n- -\n\n- -\n- -\n\n- -\n\n- -\n- -\n\n- 2x2 2x2 1x1 2x2 2x2 1x1 1x1\n\nYes Yes Yes Yes Yes No No -\n\nYes No No -\n\nYes No\n\nYes No No -\n\nNo Yes Yes Yes Yes Yes Yes No\n\nReLU ReLU ReLU ReLU ReLU -\n- -\n\nReLU -\n- -\n\nReLU -\n\nReLU -\n- -\n\n- ReLU ReLU ReLU ReLU ReLU ReLU Sigmoid\n\nX 1\n2 3\n4 5, S 5, S μ, σ2\n\nZ1, Y 1\n1 μ, σ2\n\nZ1 1\n\nZ2, Y 1\n1 μ, σ2\n\nZ1, S 1\n2 3\n4 5\n6 7\n\nTable 6: HCV architecture for the Extended Yale B dataset.\n\nLayer\n\nOperation\n\nFilters Kernel\n\nStrides Batch norm Activation\n\nLinked layer\n\n1 2\n3 4\n5 μ\nσ2 Output (Z2|X, S)\n\n1 μ\nσ2 Output (Z1|Z2, Y )\n\nConvolution Convolution Convolution Convolution Convolution Dense Dense Sample Z2|X, S\n\nDense Dense Dense Sample Z1|Z2, Y\n\n32 64 128 256 512 2\n2 -\n\n50 10 10 -\n\n1 2\n3 4\n5 6\n7 8\n\nDense Transpose Convolution Transpose Convolution Convolution Transpose Convolution Transpose Convolution Convolution Convolution\n\n8x8x1024 512 256 256 128 64 64 1\n\n5x5 5x5 5x5 3x3 3x3 -\n- -\n\n- -\n- -\n\n- 3x3 3x3 3x3 5x5 5x5 5x5 5x5\n\n2x2 2x2 2x2 2x2 2x2 -\n- -\n\n- -\n- -\n\n- 2x2 2x2 1x1 2x2 2x2 1x1 1x1\n\nYes Yes Yes Yes Yes No No -\n\nYes No No -\n\nNo Yes Yes Yes Yes Yes Yes No\n\nReLU ReLU ReLU ReLU ReLU -\n- -\n\nReLU -\n- -\n\n- ReLU ReLU ReLU ReLU ReLU ReLU Sigmoid\n\nX 1\n2 3\n4 5, S 5, S μ, σ2\n\nZ2, Y 1\n1 μ, σ2\n\nZ1, S 1\n2 3\n4 5\n6 7\n\nTable 7: FairDisCo architecture for the Extended Yale B dataset.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nMap\n\nf2\n\nf3\n\ng\n\nDiscriminator\n\nLayer\n\nOperation\n\nFilters Kernel\n\nStrides Batch norm\n\nActivation\n\nLinked layer\n\n1 2\n3 4\n5\n\n1 2\n3 4\n5\n\n1 2\n3 4\n5\n\n1 2\n3 4\n5\n\nConvolution Convolution Convolution Convolution Dense\n\nConvolution Convolution Convolution Convolution Dense\n\n64 128 256 512 6\n\n32 32 64 64 6\n\nDense Transpose Convolution Transpose Convolution Convolution Convolution\n\n7x7x256 128 64 64 1\n\nDense Dense Dense Dense Dense\n\n48 48 48 48 1\n\n4x4 4x4 4x4 4x4 -\n\n4x4 4x4 4x4 4x4 -\n\n- 4x4 4x4 4x4 4x4\n\n- -\n- -\n-\n\n2x2 2x2 2x2 2x2 -\n\n2x2 1x1 2x2 1x1 -\n\n- 2x2 2x2 1x1 1x1\n\n- -\n- -\n-\n\nYes Yes Yes Yes No\n\nYes Yes Yes Yes -\n\nNo Yes Yes Yes No\n\nNo No No No No\n\nReLU ReLU ReLU ReLU -\n\nReLU ReLU ReLU ReLU -\n\n- LeakyReLU LeakyReLU LeakyReLU Sigmoid\n\nReLU ReLU ReLU ReLU -\n\nX 1\n2 3\n4\n\nX 1\n2 3\n4\n\nf2, f3 1\n2 3\n4\n\nf3 1\n2 3\n4\n\nTable 8: WFAE architecture for the MNIST dataset.\n\nMap\n\nQZ|X,S\n\nPX|Z,S\n\nMap\n\nQZ|X,S\n\nPX|Z,S\n\nLayer\n\nOperation\n\nFilters Kernel\n\nStrides Batch norm Activation\n\nLinked layer\n\n1 2\n3 4\n5 μ\nσ2 Output (Z1|X, S)\n\nConvolution Convolution Convolution Convolution Dense Dense Dense Sample Z1|X, S\n\n64 128 256 512 100 6\n6 -\n\n1 2\n3 4\n5\n\nDense Transpose Convolution Transpose Convolution Convolution Convolution\n\n7x7x256 128 64 64 1\n\n4x4 4x4 4x4 4x4 -\n- -\n-\n\n- 4x4 4x4 4x4 4x4\n\n2x2 2x2 2x2 2x2 -\n- -\n-\n\n- 2x2 2x2 1x1 1x1\n\nYes Yes Yes Yes Yes No No -\n\nNo Yes Yes Yes No\n\nReLU ReLU ReLU ReLU ReLU -\n- -\n\n- ReLU ReLU ReLU Sigmoid\n\nX 1\n2 3\n4, S 5\n5 μ, σ2\n\nZ, S 1\n2 3\n4\n\nTable 9: HCV architecture for the MNIST dataset.\n\nLayer\n\nOperation\n\nFilters Kernel\n\nStrides Batch norm Activation\n\nLinked layer\n\n1 2\n3 4\nμ σ2 Output (Z1|X, S)\n\nConvolution Convolution Convolution Convolution Dense Dense Sample Z1|X, S\n\n64 128 256 512 6\n6 -\n\n1 2\n3 4\n5\n\nDense Transpose Convolution Transpose Convolution Convolution Convolution\n\n7x7x256 128 64 64 1\n\n4x4 4x4 4x4 4x4 -\n- -\n\n- 4x4 4x4 4x4 4x4\n\n2x2 2x2 2x2 2x2 -\n- -\n\n- 2x2 2x2 1x1 1x1\n\nYes Yes Yes Yes No No -\n\nNo Yes Yes Yes No\n\nReLU ReLU ReLU ReLU -\n- -\n\n- ReLU ReLU ReLU Sigmoid\n\nX 1\n2 3\n4 4\nμ, σ2\n\nZ, S 1\n2 3\n4\n\nTable 10: FairDisCo architecture for the MNIST dataset.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nMap\n\nLayer\n\nOperation\n\nFilters\n\nKernel\n\nStrides Batch norm\n\nActivation\n\nLinked layer\n\n(f1, f2)\n\nf3\n\ng\n\nDiscriminator\n\n1 2\n3 4\n5\n\n1 2\n3 4\n5 6\n7 8\n9\n\n1 2\n3 4\n5 6\n7 8\n9 10\n\n1 2\n3 4\n5\n\nConvolution Convolution Convolution Convolution Dense\n\nConvolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Dense\n\n128 256 512 1024 71\n\n128 128 256 256 512 512 1024 1024 32\n\n5x5 5x5 5x5 5x5 -\n\n5x5 5x5 5x5 5x5 5x5 3x3 3x3 3x3 -\n\nDense Transpose Convolution Residual Block Transpose Convolution Residual Block Transpose Convolution Residual Block Transpose Convolution Residual Block Convolution\n\n8x8x1024 512 512 256 256 128 128 64 64 3\n\n- 5x5 5x5, 1x1 5x5 5x5, 1x1 5x5 3x3, 1x1 5x5 3x3, 1x1 3x3\n\nDense Dense Dense Dense Dense\n\n256 256 256 256 1\n\n- -\n- -\n-\n\n2x2 2x2 2x2 2x2 -\n\n2x2 1x1 2x2 1x1 2x2 1x1 2x2 1x1 -\n\n- 2x2 1x1 2x2 1x1 2x2 1x1 2x2 1x1 1x1\n\n- -\n- -\n-\n\nYes Yes Yes Yes -\n\nYes Yes Yes Yes Yes Yes Yes Yes -\n\nNo Yes Yes Yes Yes Yes Yes Yes Yes No\n\nNo No No No No\n\nReLU ReLU ReLU ReLU -\n\nReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU -\n\n- LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU Sigmoid\n\nReLU ReLU ReLU ReLU -\n\nX 1\n2 3\n4\n\nX 1\n2 3\n4 5\n6 7\n8\n\nf1, f2, f3 1\n2 3\n4 5\n6 7\n8 9\n\nf3 1\n2 3\n4\n\nTable 11: WFAE architecture for the VGGFace2 dataset.\n\nMap\n\nLayer\n\nOperation\n\nFilters\n\nKernel\n\nStrides Batch norm\n\nActivation\n\nLinked layer\n\nEncoder f\n\nDecoder g\n\nDiscriminator\n\n1 2\n3 4\n5 6\n7 8\n9\n\n1 2\n3 4\n5 6\n7 8\n9 10\n\n1 2\n3 4\n5\n\nConvolution Convolution Convolution Convolution Convolution Convolution Convolution Convolution Dense\n\n128 128 256 256 512 512 1024 1024 96\n\n5x5 5x5 5x5 5x5 5x5 3x3 3x3 3x3 -\n\nDense Transpose Convolution Residual Block Transpose Convolution Residual Block Transpose Convolution Residual Block Transpose Convolution Residual Block Convolution\n\n8x8x1024 512 512 256 256 128 128 64 64 3\n\n- 5x5 5x5, 1x1 5x5 5x5, 1x1 5x5 3x3, 1x1 5x5 3x3, 1x1 3x3\n\nDense Dense Dense Dense Dense\n\n384 384 384 384 7\n\n- -\n- -\n-\n\n2x2 1x1 2x2 1x1 2x2 1x1 2x2 1x1 -\n\n- 2x2 1x1 2x2 1x1 2x2 1x1 2x2 1x1 1x1\n\n- -\n- -\n-\n\nYes Yes Yes Yes Yes Yes Yes Yes -\n\nNo Yes Yes Yes Yes Yes Yes Yes Yes No\n\nNo No No No No\n\nReLU ReLU ReLU ReLU ReLU ReLU ReLU ReLU -\n\n- LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU LeakyReLU Sigmoid\n\nReLU ReLU ReLU ReLU -\n\nX 1\n2 3\n4 5\n6 7\n8\n\nS, Z 1\n2 3\n4 5\n6 7\n8 9\n\nZ 1\n2 3\n4\n\nTable 12: Fader Network architecture for the VGGFace2 dataset.\n\n17",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a new analysis of Wasserstein Autoencoders and claims the new analysis reveals a learning objective form that naturally can be optimized without the addition of ``ad-hoc\" penalties. The paper describes this anaylsis and presents a full derivation of their construction in three prototypical generative model examples with side or nuisance information. They evaluate their method in a few settings where conditional generative models are deployed.\n\n# Strength And Weaknesses\n\nStrengths\n1) The paper is detailed in its derivation for the numerous examples it presents.\n\n2) The paper does a good job of setting up the problem and describing necessary background in a way that flows nicely with the story of the paper.\n\n3) The notation, analysis, and arguments are typically very clear, and a careful reader would be able to follow most of the discussion with mild background in Wasserstein method and VAEs.\n\nWeaknesses\n1) The paper does not provide a solution for the problem and claim suggested in the abstract or introduction. The authors claim that penalties and constraints can be derived directly from, or induced from, the conditional independence structure of the WAE. The introduction describes a number of prior literature that use a variety of metrics to enforce or push independence, and claim that none of these are a ``principled way ofo imposing the encoder structure\". The end of the preliminaries describes a funtion $\\delta$ that already has a new, arbitrary divergence $\\mathcal{D}$. The core of the paper, the three examples presented, all have additional regularization/constraint terms arbitrarily appended to their objectives, explicitly suggesting they can take the form of an MMD loss, GAN loss, HSIC loss, etc.\n\n2) There is no general algorithmic or analytic procedure presented that might generalize past the three problems described here. The experimental setups shown follow the examples shown, but it's not clear what the examples are providing over the existing methods when the arbitrary regularizers are not added.\n\n3) The experimental evaluation in general is not very convincing. The most important piece in experimental setups similar to this is choosing the regularization weights. This can drastically change the results of all models tested, and no discussion is provided that clarifies how or which weights were chosen.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity\nThe writing in general is clear and the analysis is easy to follow. All terms, notations, and steps are well described.\n\nThe main thesis of the paper is hard to follow, especially throughout the main body of the paper in the examples.\n\nQuality\nThe quality of the paper is not up to the typical standard at ICLR. The main claim is not supported by the text, and it's not clear what else is being provided.\n\nNovelty\nThe authors correct a previous theorem in prior work, but otherwise it is not clear what the novel observation and value proposition is.\n\nReproducibility\nCode is provided and with the appendix, seems to be sufficient to replicate the experiments but I have not gone through in detail.\n\n# Summary Of The Review\n\nIt is not clear what the paper is proposing and the main claim is not justified at all. The paper seems to have derived some formulations and then tacked on arbitrary regularizers, completely antithetical to the claims in the abstract and introduction.\n\n# Correctness\n\n1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLEARNING RATIONALIZABLE EQUILIBRIA IN MULTIPLAYER GAMES\n\nYuanhao Wang∗∗1, Dingwen Kong∗∗2, Yu Bai3, Chi Jin1 1Princeton University, 2Peking University, 3Salesforce Research yuanhao@princeton.edu, dingwenk@pku.edu.cn yu.bai@salesforce.com, chij@princeton.edu\n\nABSTRACT\n\nA natural goal in multi-agent learning is to learn rationalizable behavior, where players learn to avoid any Iteratively Dominated Action (IDA). However, standard no-regret based equilibria-finding algorithms could take exponential samples to find such rationalizable strategies. In this paper, we first propose a simple yet sample-efficient algorithm for finding a rationalizable action profile in multi-player general-sum games under bandit feedback, which substantially improves over the results of Wu et al. (2021). We further develop algorithms with the first efficient guarantees for learning rationalizable Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE). Our algorithms incorporate several novel techniques to guarantee the elimination of IDA and no (swap-)regret simultaneously, including a correlated exploration scheme and adaptive learning rates, which may be of independent interest. We complement our results with a sample complexity lower bound showing the sharpness of our guarantees.\n\n1\n\nINTRODUCTION\n\nA common objective in multi-agent learning is to find various equilibria, such as Nash equilibria (NE), correlated equilibria (CE) and coarse correlated equilibria (CCE). Generally speaking, a player in equilibrium lacks incentive to deviate assuming conformity of other players to the same equilibrium. Equilibrium learning has been extensively studied in the literature of game theory and online learning, and no-regret based learners can provably learn approximate CE and CCE with both computational and statistical efficiency (Stoltz, 2005; Cesa-Bianchi & Lugosi, 2006).\n\nHowever, not all equilibria are created equal. As shown by Viossat & Zapechelnyuk (2013), a CCE can be entirely supported on dominated actions—actions that are worse off than some other strategy in all circumstances—which rational agents should apparently never play. Approximate CE also suffers from a similar problem. As shown by Wu et al. (2021, Theorem 1), there are examples where an ε-CE always plays iteratively dominated actions—actions that would be eliminated when iteratively deleting strictly dominated actions—unless ε is exponentially small. It is also shown that standard no-regret algorithms are indeed prone to finding such seemingly undesirable solutions (Wu et al., 2021). The intrinsic reason behind this is that CCE and approximate CE may not be rationalizable, and existing algorithms can indeed fail to find rationalizable solutions.\n\nDifferent from equilibria notions, rationalizability (Bernheim, 1984; Pearce, 1984) looks at the game from the perspective of a single player without knowledge of the actual strategies of other players, and only assumes common knowledge of their rationality. A rationalizable strategy will avoid strictly dominated actions, and assuming other players have also eliminated their dominated actions, iteratively avoid strictly dominated actions in the subgame. Rationalizability is a central solution concept in game theory (Osborne & Rubinstein, 1994) and has found applications in auctions (Battigalli & Siniscalchi, 2003) and mechanism design (Bergemann et al., 2011).\n\nIf an (approximate) equilibrium only employs rationalizable actions, it would prevent irrational behavior such as playing dominated actions. Such equilibria are arguably more reasonable than\n\n∗Equal contribution.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nunrationalizable ones, and constitute a stronger solution concept. This motivates us to consider the following open question:\n\nCan we efficiently learn equilibria that are also rationalizable?\n\nDespite its fundamental role in multi-agent reasoning, rationalizability is rarely studied from a learning perspective until recently, with Wu et al. (2021) giving the first algorithm for learning rationalizable strategies from bandit feedback. However, the problem of learning rationalizable CE and CCE remains a challenging open problem. Due to the existence of unrationalizable equilibria, running standard CE or CCE learners will not guarantee rationalizable solutions. On the other hand, one cannot hope to first identify all rationalizable actions and then find an equilibrium on the subgame, since even determining whether an action is rationalizable requires exponentially many samples (see Proposition 2). Therefore, achieving rationalizability and approximate equilibria simultaneously is nontrivial and presents new algorithmic challenges.\n\nIn this work, we address the challenges above and give a positive answer to our main question. Our contributions can be summarized as follows:\n\n• As a first step, we provide a simple yet sample-efficient algorithm for identifying a ∆- rationalizable 1 action profile under bandit feedback, using only (cid:101)O (cid:0) LN A (cid:1)2 samples in normalform games with N players, A actions per player and a minimum elimination length of L. This greatly improves the result of Wu et al. (2021) and is tight up to logarithmic factors when L = O(1).\n\n∆2\n\n• Using the above algorithm as a subroutine, we develop exponential weights based algorithms that (cid:1) samples, and ∆-rationalizable\n\ncan provably find ∆-rationalizable ε-CCE using (cid:101)O (cid:0) LN A ε-CE using (cid:101)O guarantees for learning rationalizable approximate CCE and CE.\n\nN A2 min{ε2,∆2}\n\n∆2 + N A\n\n∆2 +\n\n(cid:16) LN A\n\n(cid:17)\n\nε2\n\nsamples. To the best of our knowledge, these are the first\n\n• We also provide reduction schemes that find ∆-rationalizable ε-CCE/CE using black-box algorithms for ε-CCE/CE. Despite having slightly worse rates, these algorithms can directly leverage the progress in equilibria finding, which may be of independent interest.\n\n1.1 RELATED WORK\n\nRationalizability and iterative dominance elimination. Rationalizability (Bernheim, 1984; Pearce, 1984) is a notion that captures rational reasoning in games and relaxes Nash Equilibrium. Rationalizability is closely related to the iterative elimination of dominated actions, which has been a focus of game theory research since the 1950s (Luce & Raiffa, 1957). It can be shown that an action is rationalizable if and only if it survives iterative elimination of strictly dominated actions3 (Pearce, 1984). There is also experimental evidence supporting iterative elimination of dominated strategies as a model of human reasoning (Camerer, 2011).\n\nEquilibria learning in games. There is a rich literature on applying online learning algorithms to learning equilibria in games. It is well-known that if all agents have no-regret, the resulting empirical average would be an ε-CCE (Young, 2004), while if all agents have no swap-regret, the resulting empirical average would be an ε-CE (Hart & Mas-Colell, 2000; Cesa-Bianchi & Lugosi, 2006). Later work continuing this line of research include those with faster convergence rates (Syrgkanis et al., 2015; Chen & Peng, 2020; Daskalakis et al., 2021), last-iterate convergence guarantees (Daskalakis & Panageas, 2018; Wei et al., 2020), and extension to extensive-form games (Celli et al., 2020; Bai et al., 2022b;a; Song et al., 2022) and Markov games (Song et al., 2021; Jin et al., 2021).\n\nComputational and learning aspect of rationalizability. Despite its conceptual importance, rationalizability and iterative dominance elimination are not well studied from a computational or learning perspective. For iterative strict dominance elimination in two-player games, Knuth et al. (1988) provided a cubic-time algorithm and proved that the problem is P-complete. The weak dominance version of the problem is proven to be NP-complete by Conitzer & Sandholm (2005).\n\n1An action is ∆-rationalizable if it survives iterative elimination of ∆-dominated actions; c.f. Definition 1. 2Throughout this paper, we use (cid:101)O to suppress logarithmic factors in N , A, L, 1 3For this equivalence to hold, we need to allow dominance by mixed strategies, and correlated beliefs when\n\nδ , and 1 ε .\n\n∆ , 1\n\nthere are more than two players. These conditions are met in the setting of this work.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nHofbauer & Weibull (1996) showed that in a class of learning dynamics which includes replicator dynamics — the continuous-time variant of Follow-The-Regularzied-Leader (FTRL), all iteratively strictly dominated actions vanish over time, while Mertikopoulos & Moustakas (2010) proved similar results for stochastic replicator dynamics; however, neither work provides finite-time guarantees. Cohen et al. (2017) proved that Hedge eliminates dominated actions in finite time, but did not extend their results to the more challenging case of iteratively dominated actions.\n\nThe most related work in literature is the work on learning rationalizable actions by Wu et al. (2021), who proposed the Exp3-DH algorithm to find a strategy mostly supported on rationalizable actions with a polynomial rate. Our Algorithm 1 accomplishes the same task with a faster rate, while our Algorithms 2 & 3 deal with the more challenging problems of finding ε-CE/CCE that are also rationalizable. Although Exp3-DH is based on a no-regret algorithm, it does not enjoy regret or weighted regret guarantees and thus does not provably find rationalizable equilibria.\n\n2 PRELIMINARY\n\nAn N -player normal-form game involves N players whose action space are denoted by A = A1 × · · · × AN , and is defined by utility functions u1, · · · , uN : A → [0, 1]. Let A = maxi∈[N ] |Ai| denote the maximum number of actions per player, xi denote a mixed strategy of the i-th player (i.e., a distribution over Ai) and x−i denote a (correlated) mixed strategy of the other players (i.e., a distribution over (cid:81) j̸=i Aj). We further denote ui(xi, x−i) := Eai∼xi,a−i∼x−iui(ai, a−i). We use ∆(S) to denote a distribution over the set S.\n\nLearning from bandit feedback We consider the bandit feedback setting where in each round, each player i ∈ [N ] chooses an action ai ∈ Ai, and then observes a random feedback Ui ∈ [0, 1] such that E[Ui|a1, a2, · · · , an] = ui(a1, a2, · · · , an).\n\n2.1 RATIONALIZABILITY\n\nAn action a ∈ Ai is said to be rationalizable if it could be the best response to some (possibly correlated) belief of other players’ strategies, assuming that they are also rational. In other words, the set of rationalizable actions is obtained by iteratively removing actions that could never be a best response. For finite normal-form games, this is in fact equivalent to the iterative elimination of strictly dominated actions4 (Osborne & Rubinstein, 1994, Lemma 60.1). Definition 1 (∆-Rationalizability). 5 Define\n\nE1 := (cid:83)N\n\ni=1 {a ∈ Ai : ∃x ∈ ∆(Ai), ∀a−i,\n\nui(a, a−i) ≤ ui(x, a−i) − ∆} ,\n\nwhich is the set of ∆-dominated actions for all players. Further define\n\nEl := (cid:83)N\n\ni=1 {a ∈ Ai : ∃x ∈ ∆(Ai), ∀a−i s.t. a−i ∩ El−1 = ∅, ui(a, a−i) ≤ ui(x, a−i) − ∆} ,\n\nwhich is the set of actions that would be eliminated by the l-th round. Define L = inf{l : El+1 = El} as the minimum elimination length, and EL as the set of ∆-iteratively dominated actions (∆-IDAs). Actions in ∪n\n\ni=1Ai \\ EL are said to be ∆-rationalizable.\n\nNotice that E1 ⊆ · · · ⊆ EL = EL+1. Here ∆ plays a similar role as the reward gap for best arm identification in stochastic multi-armed bandits. We will henceforth use ∆-rationalizability and survival of L rounds of iterative dominance elimination (IDE) interchangeably6. Since one cannot eliminate all the actions of a player, |EL| ≥ N , which further implies L ≤ N (A − 1) < N A.\n\n2.2 EQUILIBRIA IN GAMES\n\nWe consider three common learning objectives, namely Nash Equilibrium (NE), Correlated Equilibrium (CE) and Coarse Correlated Equilibrium (CCE).\n\n4See, e.g., the Diamond-In-the-Rough (DIR) games in Wu et al. (2021, Definition 2) for a concrete example\n\nof iterative dominance elimination.\n\n5Here we slightly abuse the notation and use ∆ to refer to both the gap and the probability simplex. 6Alternatively one can also define ∆-rationalizability by the iterative elimination of actions that are never\n\n∆-best response, which is mathematically equivalent to Definition 1 (see Appendix A.1).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nDefinition 2 (Nash Equilibrium). A strategy profile (x1, · · · , xN ) is an ε-Nash equilibrium if\n\nui(xi, x−i) ≥ ui(a, x−i) − ε, ∀a ∈ Ai, ∀i ∈ [N ].\n\nDefinition 3 (Correlated Equilibrium). A correlated strategy Π ∈ ∆(A) is an ε-correlated equilibrium if ∀i ∈ [N ], ∀φ : Ai → Ai,\n\n(cid:80)\n\nai∈Ai,a−i∈A−i\n\nΠ(ai, a−i)ui(ai, a−i) ≥ (cid:80)\n\nai∈Ai,a−i∈A−i\n\nΠ(ai, a−i)ui(φ(ai), a−i) − ε.\n\nDefinition 4 (Coarse Correlated Equilibrium). A correlated strategy Π ∈ ∆(A) is an ε-CCE if ∀i ∈ [N ], ∀a′ ∈ Ai,\n\n(cid:80)\n\nai∈Ai,a−i∈A−i\n\nΠ(ai, a−i)ui(ai, a−i) ≥ (cid:80)\n\nai∈Ai,a−i∈A−i\n\nΠ(ai, a−i)ui(a′, a−i) − ε.\n\nWhen ε = 0, the above definitions give exact Nash equilibrium, correlated equilibrium, and coarse correlated equilibrium, respectively. It is well known that ε-NE are ε-CE, and ε-CE are ε-CCE. Furthermore, we call an ε-CCE/CE that only plays ∆-rationalizable actions a.s. a ∆-rationalizable ε-CCE/CE.\n\n2.3 CONNECTION BETWEEN EQUILIBRIA AND RATIONALIZABILITY\n\nIt is known that all actions in the support of an exact CE are rationalizable (Osborne & Rubinstein, 1994, Lemma 56.2). However, one can easily construct an exact CCE that is supported on dominated (hence, unrationalizable) actions (see e.g. Viossat & Zapechelnyuk (2013, Fig. 3)). One might be tempted to suggest that running a CE solver immediately finds a CE (and hence CCE) that is also rationalizable. However, the connection between CE and rationalizability becomes quite different when it comes to approximate equilibria, which are inevitable in the presence of noise. As shown by Wu et al. (2021, Theorem 1), an ε-CE can be entirely supported on iteratively dominated action, unless ε = O(2−A). In other words, rationalizability is not guaranteed by running an approximate CE solver unless with an extremely high accuracy. Therefore, finding ε-CE and CCE that are simultaneously rationalizable remains a challenging open problem.\n\nSince NE is a subset of CE, all actions in the support of an (exact) NE would also be rationalizable. Unlike approximate CE, for ε < poly(∆, 1/N, 1/A)), one can show that any ε-Nash equilibrium is still mostly supported on rationalizable actions.\n\nProposition 1. If x∗ = (x∗\n\nN ) is an ε-Nash with ε < ∆2 Therefore, for two-player zero-sum games, it is possible to run an approximate NE solver and automatically find a rationalizable ε-NE. However, this method will induce a rather slow rate7, and we will provide a much more efficient algorithm for finding rationalizable ε-NE in Section 4.\n\n[a ∈ EL] ≤ 2Lε ∆ .\n\n24N 2A , ∀i, Pra∼x∗\n\n1, · · · , x∗\n\ni\n\n3 LEARNING RATIONALIZABLE ACTION PROFILES\n\nIn order to learn a rationalizable CE/CCE, one might suggest identifying the set of all rationalizable actions, and then learn CE or CCE on this subgame. Unfortunately, as shown by Proposition 2, even the simpler problem of deciding whether one single action is rationalizable is statistically hard. Proposition 2. For ∆ < 0.1, any algorithm that correctly decides whether an action is ∆- rationalizable with 0.9 probability needs Ω(AN −1∆−2) samples.\n\nThis negative result motivates us to consider an easier task: can we at least find one rationalizable action profile sample-efficiently? Formally, we say a action profile (a1, . . . , aN ) is rationalizable if for all i ∈ [N ], ai is a rationalizable action. This is arguably one of the most fundamental tasks regarding rationalizability. For mixed-strategy dominance solvable games (Alon et al., 2021), the unique rationalizable action profile will be the unique NE and also the unique CE of the game. Therefore this easier task per se is still of practical importance.\n\nIn this section we answer this question in the affirmative. We provide a sample-efficient algorithm (cid:1) samples. This algorithm will also which finds a rationalizable action profile using only (cid:101)O (cid:0) LN A serve as an important subroutine for algorithms finding rationalizable CCE/CE in the later sections.\n\n∆2\n\n7For two-player zero-sum games, the marginals of any CCE is an NE so NE can be found efficiently. This is\n\nnot true for general games, where finding NE is computationally hard and takes Ω(2N ) samples.\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Iterative Best Response\n\n1: Initialization: choose a(0) 2: for l = 1, · · · , L do 3:\n\nfor i ∈ [N ] do\n\ni ∈ Ai arbitrarily for all i ∈ [N ]\n\n4:\n\nFor all a ∈ Ai, play (a, a(l−1)\n\n−i\n\n) for M times, compute player i’s average payoff ˆui(a, a(l−1)\n\n−i\n\n)\n\nSet a(l) 5: 6: return (a(L)\n\n1\n\n, · · · , a(L) N )\n\ni ← arg maxa∈Ai ˆui(a, a(l−1)\n\n−i\n\n)\n\n// Computing the empirical best response\n\nThe intuition behind this algorithm is simple: if an action profile a−i can survive l rounds of IDE, then its best response ai (i.e., arg maxa∈Ai ui(a, a−i)) can survive at least l + 1 rounds of IDE, since the action ai can only be eliminated after some actions in a−i are eliminated. Concretely, we start from an arbitrary action profile (a(0) N ). In each round l ∈ [L], we compute the (empirical) best response of a(l−1) for each i ∈ [N ], and use those best responses to construct a new action profile (a(l) N ). By constructing iterative best responses, we will end up with an action profile that can survive L rounds of IDE, which means surviving any number of rounds of IDE according to the definition of L. The full algorithm is presented in Algorithm 1, for which we have the following theoretical guarantee.\n\n1 , . . . , a(0)\n\n1 , . . . , a(l)\n\n−i\n\nTheorem 3. With M = that is ∆-rationalizable using a total of (cid:101)O (cid:0) LN A\n\n(cid:108) 16 ln(LN A/δ) ∆2\n\n(cid:109)\n\n∆2\n\n(cid:1) samples.\n\n, with probability 1 − δ, Algorithm 1 returns an action profile\n\nWu et al. (2021) provide the first polynomial sample complexity results for finding rationalizable action profiles. They prove that the Exp3-DH algorithm is able to find a distribution with 1 − ζ samples under bandit feedback8. fraction supported on ∆-rationalizable actions using (cid:101)O Compared to their result, our sample complexity bound (cid:101)O (cid:0) LN A (cid:1) has more favorable dependence on all problem parameters, and our algorithm will output a distribution that is fully supported on rationalizable actions (thus has no dependence on ζ).\n\n(cid:16) L1.5N 3A1.5\n\nζ3∆3\n\n∆2\n\n(cid:17)\n\nWe further complement Theorem 3 with a sample complexity lower bound showing that the linear dependency on N and A are optimal. This lower bound suggests that the (cid:101)O (cid:0) LN A (cid:1) upper bound is tight up to logarithmic factors when L = O(1), and we conjecture that this is true for general L.\n\n∆2\n\nTheorem 4. Even for games with L ≤ 2, any algorithm that returns a ∆-rationalizable action profile with 0.9 probability needs Ω (cid:0) N A Conjecture 5. The minimax optimal sample complexity for finding a ∆-rationalizable action profile is Θ (cid:0) LN A\n\n(cid:1) for games with minimum elimination length L.\n\n(cid:1) samples.\n\n∆2\n\n∆2\n\n4 LEARNING RATIONALIZABLE COARSE CORRELATED EQUILIBRIA (CCE)\n\nIn this section we introduce our algorithm for efficiently learning rationalizable CCEs. The high-level idea is to run no-regret Hedge-style algorithms for every player, while constraining the strategy inside the rationalizable region. Our algorithm is motivated by the fact that the probability of playing a dominated action will decay exponentially over time in the Hedge algorithm for adversarial bandit under full information feedback (Cohen et al., 2017). The full algorithm description is provided in Algorithm 2, and here we explain several key components in our algorithm design.\n\nCorrelated Exploration Scheme. In the bandit feedback setting, standard exponential weights algorithms such as EXP3.IX require importance sampling and biased estimators to derive a highprobability regret bound (Neu, 2015). However, such bias could cause a dominating strategy to lose its advantage. In our algorithm we adopt a correlated exploration scheme, which essentially simulates full information feedback by bandit feedback using N A samples. Specifically, at every time step t,\n\n8Wu et al. (2021)’s result allows trade-off between variables via different choice of algorithmic parameters.\n\nHowever, a ζ −1∆−3 factor is unavoidable regardless of choice of parameters.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 2 Hedge for Rationalizable ε-CCE\n\nN ) ← Algorithm 1\n\n1, · · · , a⋆\n\n1: (a⋆ 2: For all i ∈ [N ], initialize θ(1) 3: for t = 1, · · · , T do 4:\n\nfor i = 1, · · · , N do\n\ni\n\n(·) ← 1[· = a⋆ i ]\n\n5:\n\nFor all a ∈ Ai, play (a, θ(t) −i) for Mt times, compute player i’s average payoff u(t) (cid:80)t Set θ(t+1) (·) ∝ exp\n\n(·)\n\n(cid:16)\n\n(cid:17)\n\ni (a)\n\nηt 6: 7: For all t ∈ [T ] and i ∈ [N ], eliminate all actions in θ(t)\n\ni\n\nτ =1 u(τ )\n\ni\n\ni with probability smaller than p, then\n\nrenormalize the vector to simplex as ̄θ(t)\n\ni\n\n8: output:\n\n(cid:16)(cid:80)T\n\nt=1 ⊗n\n\ni=1\n\n ̄θ(t)\n\ni\n\n(cid:17)\n\n/T\n\nthe players take turn to enumerate their action set, while the other players fix their strategies according to Hedge. For i ∈ [N ] and t ≥ 2, we denote θ(t) the strategy computed using Hedge for player i in round t. Joint strategy (a, θ(t) −i) is played to estimate player i’s payoff u(t) i (a). It is important to note that such correlated scheme does not require any communication between the players—the players can schedule the whole process before the game starts.\n\ni\n\nRationalizable Initialization and Variance Reduction. We use Algorithm 1, which learns a rationalizable action profile, to give the strategy for the first round. By carefully preserving the disadvantage of any iteratively dominated action, we keep the iterates inside the rationalizable region throughout the whole learning process. To ensure this for every iterate with high probability, a minibatch is used to reduce the variance of the estimator.\n\nClipping. In the final step, we clip all actions with small probabilities, so that iteratively dominated actions do not appear in the output. The threshold is small enough to not affect the ε-CCE guarantee.\n\n4.1 THEORETICAL GUARANTEE\n\nIn Algorithm 2, we choose parameters in the following manner:\n\nηt = max\n\n(cid:26)(cid:113) ln A\n\nt\n\n, 4 ln(1/p)\n\n∆t\n\n(cid:27)\n\n, Mt =\n\n(cid:108) 64 ln(AN T /δ) ∆2t\n\n(cid:109)\n\n, and p = min{ε,∆} 8AN .\n\n(1)\n\nNote that our learning rate can be bigger than the standard learning rate in FTRL algorithms when t is small. The purpose is to guarantee the rationalizability of the iterates from the beginning of the learning process. As will be shown in the proof, this larger learning rate will not hurt the final rate. We now state the theoretical guarantee for Algorithm 2. (cid:1) rounds, with probability Theorem 6. With parameters chosen as in Eq.(1) , after T = (cid:101)O (cid:0) 1 1 − 3δ, the output strategy of Algorithm 2 is a ∆-rationalizable ε-CCE.The total sample complexity is\n\nε2 + 1\n\nε∆\n\n(cid:101)O (cid:0) LN A\n\n∆2 + N A\n\nε2\n\n(cid:1) .\n\nRemark 7. Due to our lower bound (Theorem 4), an (cid:101)O( N A ∆2 ) term is unavoidable since learning a rationalizable action profile is an easier task than learning rationalizable CCE. Based on our Conjecture 5, the additional L dependency is also likely to be inevitable. On the other hand, learning an ε-CCE alone only requires (cid:101)O( A ε2 ) term. The extra N factor is a consequence of our correlated exploration scheme in which only one player explores at a time. Removing this N factor might require more sophisticated exploration methods and utility estimators, which we leave as future work.\n\nε2 ) samples, where as in our bound we have a larger (cid:101)O( N A\n\nRemark 8. Evoking Algorithm 1 requires knowledge of L, which may not be available in practice. In that case, an estimate L′ may be used in its stead. If L′ ≥ L (for instance when L′ = N A), we can recover the current rationalizability guarantee, albeit with a larger sample complexity scaling with L′. If L′ < L, we can still guarantee that the output policy avoids actions in EL′, which are, informally speaking, actions that would be eliminated with L′ levels of reasoning.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\n4.1.1 OVERVIEW OF THE ANALYSIS\n\nWe give an overview of our analysis of Algorithm 2 below. The full proof is deferred to Appendix C.\n\nStep 1: Ensure rationalizability. We will first show that rationalizability is preserved at each iterate, i.e., actions in EL will be played with low probability across all iterates. Formally, Lemma 9. With probability at least 1 − 2δ, for all t ∈ [T ] and all i ∈ [N ], ai ∈ Ai ∩ EL, we have θ(t) i (ai) ≤ p.\n\nHere p is defined in (1). Lemma 9 guarantees that, after the clipping in Line 7 of Algorithm 2, the output correlated strategy be ∆-rationalizable.\n\nWe proceed to explain the main idea for proving Lemma 9. A key observation is that the set of rationalizable actions, ∪n i=1Ai \\ EL, is closed under best response—for the i-th player, as long as the other players continue to play actions in ∪j̸=iAj \\EL, actions in Ai∩EL will suffer from excess losses each round in an exponential weights style algorithm. Concretely, for any a−i ∈ ((cid:81) j̸=i Aj) \\ EL and any iteratively dominated action ai ∈ Ai ∩ EL, there always exists xi ∈ ∆(Ai) such that\n\nui(xi, a−i) ≥ ui(ai, a−i) + ∆. With our choice of p in Eq. (1), if other players choose their actions from ∪j̸=iAj \\EL with probability 1 − pAN , we can still guarantee an excess loss of Ω(∆). It follows that (xi) − (cid:80)t\n\n(ai) ≥ Ω(t∆) − Sampling Noise.\n\n(cid:80)t\n\nτ =1 u(τ )\n\ni\n\nτ =1 u(τ )\n\ni\n\nHowever, this excess loss can be obscured by the noise from bandit feedback when t is small. Note that it is crucial that the statement of Lemma 9 holds for all t due to the inductive nature of the proof. As a solution, we use a minibatch of size Mt = (cid:101)O (cid:0)⌈ 1 (cid:7)) in the t-th round to reduce the variance of the payoff estimator u(t)\n\n. The noise term can now be upper-bounded with Azuma-Hoeffding by\n\n∆2t\n\ni\n\nSampling Noise ≤ (cid:101)O\n\n(cid:16)(cid:113)(cid:80)t\n\nτ =1\n\n(cid:17)\n\n1 Mt\n\n≤ O(t∆),\n\nCombining this with our choice of the learning rate ηt gives τ =1 u(τ )\n\n(xi) − (cid:80)t\n\nτ =1 u(τ )\n\n(cid:16)(cid:80)t\n\nηt\n\ni\n\ni\n\n(cid:17)\n\n(ai)\n\n≫ 1.\n\n(2)\n\nBy the update rule of the Hedge algorithm, this implies that θ(t+1) complete the proof of Lemma 9 via induction on t.\n\ni\n\n(ai) ≤ p, which enables us to\n\nStep 2: Combine with no-regret guarantees. Next, we prove that the output strategy is an ε-CCE. For a player i ∈ [N ], the regret is defined as Regreti i ⟩. We can obtain the following regret bound by standard analysis of FTRL with changing learning rates. (cid:16)√ Lemma 10. For all i ∈ [N ], Regreti\n\nT = maxθ∈∆(Ai)\n\nt=1⟨u(t)\n\n, θ − θ(t)\n\n(cid:80)T\n\n(cid:17)\n\n.\n\ni\n\nT ≤ (cid:101)O\n\nT + 1 ∆\n\nHere the additive 1/∆ term is the result of our larger (cid:101)O(∆−1t−1) learning rate for small t. It (cid:1) suffices to guarantee that the correlated strategy follows from Lemma 10 that T = (cid:101)O (cid:0) 1 (cid:17)\n\nε2 + 1\n\nis an (ε/2)-CCE. Since pN A = O(ε), the clipping step only minorly affects\n\n(cid:16)(cid:80)T\n\n∆ε\n\nt=1 ⊗n\n\ni=1θ(t)\n\ni\n\n1 T\n\nthe CCE guarantee and the clipped strategy 1\n\nT\n\n(cid:16)(cid:80)T\n\nt=1 ⊗n\n\ni=1\n\n(cid:17)\n\n ̄θ(t)\n\ni\n\nis an ε-CCE.\n\n4.2 APPLICATION TO LEARNING RATIONALIZABLE NASH EQUILIBRIUM\n\nAlgorithm 2 can also be applied to two-player zero-sum games to learn a rationalizable ε-NE efficiently. Note that in two-player zero-sum games, the marginal distribution of an ε-CCE is guaranteed to be a 2ε-Nash (see, e.g., Proposition 9 in Bai et al. (2020)). Hence direct application of Algorithm 2 to a zero-sum game gives the following sample complexity bound. Corollary 11. In a two-player zero-sum game, the sample complexity for finding a ∆-rationalizable ε-Nash with Algorithm 2 is (cid:101)O (cid:0) LA\n\n(cid:1).\n\n∆2 + A\n\nε2\n\nThis result improves over a direct application of Proposition 1, which gives (cid:101)O complexity and produces an ε-Nash that could still take unrationalizable actions with positive probability.\n\nsample\n\nε2\n\n(cid:16) A3\n\n∆4 + A\n\n(cid:17)\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 3 Adaptive Hedge for Rationalizable ε-CE\n\nN ) ← Algorithm 1\n\n1, · · · , a⋆\n\n1: (a⋆ 2: For all i ∈ [N ], initialize θ(1) 3: for t = 1, 2, . . . , T do 4:\n\nfor i = 1, 2, . . . , N do\n\ni ← (1 − |Ai|p)1[· = a⋆\n\ni ] + p1\n\n5:\n\n6:\n\nFor all a ∈ Ai, play (a, θ(t) For all b ∈ Ai, set ˆθ(t+1) Find θ(t+1)\n\n−i) for M (t) i\n(cid:16) (·|b) ∝ exp ∈ ∆(Ai) such that θ(t+1)\n\ni\n\ntimes, compute player i’s average payoff u(t) ηb t,i (a) = (cid:80)\n\n(a|b)θ(t+1)\n\nτ =1 u(τ )\n\n(·)θ(τ )\n\nˆθ(t+1)\n\n(cid:80)t\n\n(b)\n\n(b)\n\n(cid:17)\n\ni\n\ni\n\ni (a)\n\ni\n\n7: 8: For all t ∈ [T ] and i ∈ [N ], eliminate all actions in θ(t)\n\nb∈Ai\n\ni\n\ni\n\ni\n\ni with probability smaller than p, then\n\nrenormalize the vector to simplex as ̄θ(t)\n\ni\n\n9: output:\n\n(cid:16)(cid:80)T\n\nt=1 ⊗n\n\ni=1\n\n ̄θ(t)\n\ni\n\n(cid:17)\n\n/T\n\n5 LEARNING RATIONALIZABLE CORRELATED EQUILIBRIUM\n\nIn order to extend our results on ε-CCE to ε-CE, a natural approach would be augmenting Algorithm 2 with the celebrated Blum-Mansour reduction (Blum & Mansour, 2007) from swap-regret to external regret. In this reduction, one maintains A instances of a no-regret algorithm {Alg1, · · · , AlgA}. In iteration t, the player would stack the recommendations of the A algorithms as a matrix, denoted by ˆθ(t) ∈ RA×A, and compute its eigenvector θ(t) as the randomized strategy in round t. After observing the actual payoff vector u(t), it will pass the weighted payoff vector θ(t)(a)u(t) to algorithm Alga for each a. In this section, we focus on a fixed player i, and omit the subscript i when it’s clear from the context.\n\nApplying this reduction to Algorithm 2 directly, however, would fail to preserve rationalizability since the weighted loss vector θ(t)(a)u(t) admit a smaller utility gap θ(t)(a)∆. Specifically, consider an action b dominated by a mixed strategy x. In the payoff estimate of instance a,\n\n(cid:80)t\n\nτ =1 θ(τ )(a) (cid:0)u(τ )(b) − u(τ )(x)(cid:1) ≳ ∆ (cid:80)t\n\nτ =1 θ(τ )(a) −\n\n(cid:113)(cid:80)t\n\nτ =1\n\n1 M (τ )\n\n≱ 0,\n\n(3)\n\nwhich means that we cannot guarantee the elimination of IDAs every round as in Eq (2). In Algorithm 3, we address this by making (cid:80)t τ =1 θ(τ )(a) play the role as t, tracking the progress of each no-regret instance separately. In time step t, we will compute the average payoff vector u(t) based on M (t) samples; then as in the Blum-Mansour reduction, we will update the A instances of Hedge with weighted payoffs θ(t)(a)u(t) and will use the eigenvector of ˆθ as the strategy for the next round. The key detail here is our choice of parameters, which adapts to the past strategies {θ(τ )}t τ =1:\n\n(cid:24)\n\nM (t)\n\ni\n\n:=\n\nmaxa\n\n64θ(t)\n\ni\n\n(a) τ =1 θ(τ )\n\ni\n\n∆2·(cid:80)t\n\n(a)\n\n(cid:25)\n\n, ηa\n\nt,i := max\n\n(cid:26)\n\n2 ln(1/p) τ =1 θ(τ )\n\ni\n\n∆ (cid:80)t\n\n(cid:113) A ln A\n\nt\n\n,\n\n(a)\n\n(cid:27)\n\n, p = min{ε,∆} 8AN .\n\n(4)\n\nCompared to Eq (1), we are essentially replacing t with an adaptive (cid:80)t improve (3) to\n\nτ =1 θ(τ )(a). We can now\n\n(cid:80)t\n\nτ =1 θ(τ )(a) (cid:0)u(τ )(b) − u(τ )(x)(cid:1) ≳ ∆ (cid:80)t\n\nτ =1 θ(τ )(a) −\n\n(cid:113)(cid:80)t\n\nτ =1\n\nθ(τ )(a)2 M (τ )\n\n≳ ∆ (cid:80)t\n\nτ =1 θ(τ )(a).\n\n(5)\n\nt allows us to ensure the rationalizability of every iterate. The full\n\nThis together with our choice of ηa algorithm is presented in Algorithm 3. We proceed to our theoretical guarantee for Algorithm 3. The analysis framework is largely similar to that of Algorithm 2. Our choice of M (t) is sufficient to ensure ∆-rationalizability via AzumaHoeffding inequality, while swap-regret analysis of the algorithm proves that the average (clipped) strategy is indeed an ε-CE. The full proof is deferred to Appendix D. (cid:1) rounds, with probability 1 − 3δ, Theorem 12. With parameters in Eq. (4), after T = (cid:101)O (cid:0) A the output strategy of Algorithm 3 is a ∆-rationalizable ε-CE . The total sample complexity is (cid:101)O\n\nε2 + A\n\n(cid:16) LN A\n\nN A2 min{∆2,ε2}\n\n∆2 +\n\n∆2\n\n(cid:17)\n\n.\n\ni\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 4 Rationalizable ε-CCE via Black-box Reduction\n\n1, · · · , a⋆\n\nN ) ← Algorithm 1\n\n1: (a⋆ 2: For all i ∈ [N ], initialize A(1) 3: for t = 1, 2, . . . do\n\ni ← {a⋆\n\ni }\n\n4: 5: 6: 7:\n\n8:\n\n9: 10:\n\nFind an ε′-CCE Π with black-box algorithm O in the sub-game Πi∈[N ]A(t) ∀i ∈ [N ], a′ for i ∈ [N ] do\n\ni, Π−i) for M times and compute average ˆui(a′\n\ni ∈ Ai, evaluate ui(a′\n\ni\n\ni, Π−i)\n\n// Computing the empirical best response\n\nLet a′ A(t+1)\n\nif A(t)\n\ni ← A(t) i = A(t+1)\n\ni return Π\n\ni ← arg maxa∈Ai ˆui(a, Π−i)\n\ni ∪ {a′\n\ni}\n\nfor all i ∈ [N ] then\n\nCompared to Theorem 6, our second term has an additional A factor, which is quite reasonable considering that algorithms for learning ε-CE take (cid:101)O(A2ε−2) samples, also A-times larger than the ε-CCE rate.\n\n6 REDUCTION-BASED ALGORITHMS\n\nWhile Algorithm 2 and 3 make use of one specific no-regret algorithm, namely Hedge (Exponential Weights), in this section, we show that arbitrary algorithms for finding CCE/CE can be augmented to find rationalizable CCE/CE. The sample complexity obtained via this reduction is comparable with those of Algorithm 2 and 3 when L = Θ(N A), but slightly worse when L ≪ N A. Moreover, this black-box approach would enable us to derive algorithms for rationalizable equilibria with more desirable qualities, such as last-iterate convergence, when using equilibria-finding algorithms with these properties.\n\nSuppose that we are given a black-box algorithm O that finds ε-CCE in arbitrary games. We can then use this algorithm in the following “support expansion” manner. We start with a subgame of only rationalizable actions, which can be identified efficiently with Algorithm 1, and call O to find an ε-CCE Π for the subgame. Next, we check for each i ∈ [N ] if the best response to Π−i is contained in Ai. If not, this means that the subgame’s ε-CCE may not be an ε-CCE for the full game; in this case, the best response to Π−i would be a rationalizable action that we can safely include into the action set. On the other hand, if the best response falls in Ai for all i, we can conclude that Π is also an ε-CCE for the original game. The details are given by Algorithm 4, and our main theoretical guarantee is the following. Theorem 13. Algorithm 4 outputs a ∆-rationalizable ε-CCE with high probability, using at most N A calls to the black-box CCE algorithm and (cid:101)O\n\nadditional samples.\n\n(cid:16) N 2A2\n\n(cid:17)\n\nmin{ε2,∆2}\n\nUsing similar algorithmic techniques, we can develop a reduction scheme for rationalizable ε-CE. The detailed description for this algorithm is deferred to Appendix E. Here we only state its main theoretical guarantee. Theorem 14. There exists an algorithm that outputs a ∆-rationalizable ε-CE with high probability, using at most N A calls to a black-box CE algorithm and (cid:101)O\n\nadditional samples.\n\n(cid:16) N 2A3\n\n(cid:17)\n\nmin{ε2,∆2}\n\n7 CONCLUSION\n\nIn this paper, we consider two tasks: (1) learning rationalizable action profiles; (2) learning rationalizable equilibria. For task 1, we propose a conceptually simple algorithm whose sample complexity is significantly better than prior work (Wu et al., 2021). For task 2, we develop the first provably efficient algorithms for learning ε-CE and ε-CCE that are also rationalizable. Our algorithms are computationally efficient, enjoy sample complexity that scales polynomially with the number of players and are able to avoid iteratively dominated actions completely. Our results rely on several new techniques which might be of independent interests to the community. There remains a gap between our sample complexity upper bounds and the available lower bounds for both tasks, closing which is an important future research problem.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nThis work is supported by Office of Naval Research N00014-22-1-2253. Dingwen Kong is partially supported by the elite undergraduate training program of School of Mathematical Sciences in Peking University.\n\nREFERENCES\n\nNoga Alon, Kirill Rudov, and Leeat Yariv. Dominance solvability in random games. arXiv preprint\n\narXiv:2105.10743, 2021.\n\nYu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. Advances in\n\nneural information processing systems, 33:2159–2170, 2020.\n\nYu Bai, Chi Jin, Song Mei, Ziang Song, and Tiancheng Yu. Efficient Φ-regret minimization in\n\nextensive-form games via online mirror descent. arXiv preprint arXiv:2205.15294, 2022a.\n\nYu Bai, Chi Jin, Song Mei, and Tiancheng Yu. Near-optimal learning of extensive-form games with\n\nimperfect information. arXiv preprint arXiv:2202.01752, 2022b.\n\nPierpaolo Battigalli and Marciano Siniscalchi. Rationalizable bidding in first-price auctions. Games\n\nand Economic Behavior, 45(1):38–72, 2003.\n\nDirk Bergemann, Stephen Morris, and Olivier Tercieux. Rationalizable implementation. Journal of\n\nEconomic Theory, 146(3):1253–1274, 2011.\n\nB Douglas Bernheim. Rationalizable strategic behavior. Econometrica: Journal of the Econometric\n\nSociety, pp. 1007–1028, 1984.\n\nOlivier Binette. A note on reverse pinsker inequalities. IEEE Transactions on Information Theory,\n\n65(7):4094–4096, 2019.\n\nAvrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning\n\nResearch, 8(6), 2007.\n\nColin F Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton university\n\npress, 2011.\n\nAndrea Celli, Alberto Marchesi, Gabriele Farina, and Nicola Gatti. No-regret learning dynamics for extensive-form correlated equilibrium. Advances in Neural Information Processing Systems, 33: 7722–7732, 2020.\n\nNicolo Cesa-Bianchi and G ́abor Lugosi. Prediction, learning, and games. Cambridge university\n\npress, 2006.\n\nXi Chen and Binghui Peng. Hedging in games: Faster convergence of external and swap regrets.\n\nAdvances in Neural Information Processing Systems, 33:18990–18999, 2020.\n\nJohanne Cohen, Am ́elie H ́eliou, and Panayotis Mertikopoulos. Hedging under uncertainty: Regret minimization meets exponentially fast convergence. In International Symposium on Algorithmic Game Theory, pp. 252–263. Springer, 2017.\n\nVincent Conitzer and Tuomas Sandholm. Complexity of (iterated) dominance. In Proceedings of the\n\n6th ACM Conference on Electronic Commerce, pp. 88–97, 2005.\n\nConstantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and\n\nconstrained min-max optimization. arXiv preprint arXiv:1807.04252, 2018.\n\nConstantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret learning in general games. Advances in Neural Information Processing Systems, 34:27604–27616, 2021.\n\nSergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium.\n\nEconometrica, 68(5):1127–1150, 2000.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJosef Hofbauer and J ̈orgen W Weibull. Evolutionary selection against dominated strategies. Journal\n\nof economic theory, 71(2):558–573, 1996.\n\nChi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, efficient, decentralized\n\nalgorithm for multiagent rl. arXiv preprint arXiv:2110.14555, 2021.\n\nDonald E Knuth, Christos H Papadimitriou, and John N Tsitsiklis. A note on strategy elimination in\n\nbimatrix games. Operations Research Letters, 7(3):103–107, 1988.\n\nR.D. Luce and H. Raiffa. Games and decisions: introductions and critical survey. Wiley, 1957.\n\nPanayotis Mertikopoulos and Aris L Moustakas. The emergence of rational behavior in the presence\n\nof stochastic perturbations. The Annals of Applied Probability, 20(4):1359–1388, 2010.\n\nGergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.\n\nAdvances in Neural Information Processing Systems, 28, 2015.\n\nFrancesco Orabona. A modern introduction to online learning. arXiv preprint arXiv:1912.13213,\n\n2019.\n\nMartin J Osborne and Ariel Rubinstein. A course in game theory. MIT press, 1994.\n\nDavid G Pearce. Rationalizable strategic behavior and the problem of perfection. Econometrica:\n\nJournal of the Econometric Society, pp. 1029–1050, 1984.\n\nZiang Song, Song Mei, and Yu Bai. When can we learn general-sum markov games with a large\n\nnumber of players sample-efficiently? arXiv preprint arXiv:2110.04184, 2021.\n\nZiang Song, Song Mei, and Yu Bai. Sample-efficient learning of correlated equilibria in extensive-\n\nform games. arXiv preprint arXiv:2205.07223, 2022.\n\nGilles Stoltz. Incomplete information and internal regret in prediction of individual sequences. PhD\n\nthesis, Universit ́e Paris Sud-Paris XI, 2005.\n\nVasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regularized learning in games. Advances in Neural Information Processing Systems, 28, 2015.\n\nYannick Viossat and Andriy Zapechelnyuk. No-regret dynamics and fictitious play. Journal of\n\nEconomic Theory, 148(2):825–842, 2013.\n\nChen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence\n\nin constrained saddle-point optimization. arXiv preprint arXiv:2006.09517, 2020.\n\nJibang Wu, Haifeng Xu, and Fan Yao. Multi-agent learning for iterative dominance elimination:\n\nFormal barriers and new algorithms. arXiv preprint arXiv:2111.05486, 2021.\n\nH. Peyton Young. Strategic Learning and its Limits. Oxford University Press, 11 2004. ISBN\n\n9780199269181.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA FURTHER DETAILS ON RATIONALIZABILITY\n\nA.1 EQUIVALENCE OF NEVER-BEST-RESPONSE AND STRICT DOMINANCE\n\nIt is known that for finite normal form games, rationalizable actions are given by iterated elimination of never-best-response actions, which is in fact equivalent to the iterative elimination of strictly dominated actions (Osborne & Rubinstein, 1994, Lemma 60.1). Here, for completeness, we include a proof that the iterative elimination of of actions that are never ∆-best-response gives the same definition as Definition 1. Notice that it suffices to show that for every subgame, the set of never ∆-best response actions and the set of ∆-dominated actions are the same.\n\nProposition A.1. Suppose that an action a ∈ Ai is never a ∆-best response, i.e. ∀Π−i ∈ ∆((cid:81)\n\nj̸=i Ai), ∃u ∈ ∆(Ai) such that\n\nThen a is also ∆-dominated, i.e. ∃u ∈ ∆(Ai), ∀Π−i ∈ ∆((cid:81)\n\nj̸=i Ai)\n\nui (a, Π−i) ≤ ui (u, Π−1) − ∆.\n\nui (a, Π−i) ≤ ui (u, Π−1) − ∆.\n\nProof. That a is never a ∆-best response is equivalent to\n\nmin Π−1\n\nmax u\n\n{ui (a, Π−i) − ui (u, Π−1)} ≤ −∆.\n\nThat a is ∆-dominated is equivalent to\n\nmax u\n\nmin Π−1\n\n{ui (a, Π−i) − ui (u, Π−1)} ≤ −∆.\n\nEquivalence immediately follows from von Neumman’s minimax theorem.\n\nA.2 PROOF OF PROPOSITION 1\n\nProof. We prove this inductively with the following hypothesis:\n\n∀l ≥ 1, ∀i ∈ [N ],\n\n(cid:88)\n\na∈Ai\n\nx∗\n\ni (a) · 1[a ∈ El] ≤\n\n2lε ∆\n\n.\n\nBase case: By the definition of ε-NE, ∀i ∈ [N ], ∀x′ ∈ ∆(Ai),\n\nui(x∗\n\ni , x∗\n\n−i) ≥ ui(x′, x∗\n\n−i) − ε.\n\nNote that if (cid:101)a ∈ E1 ∩ Ai, ∃x ∈ ∆(Ai) such that ∀a−i,\n\nui((cid:101)a, a−i) ≤ ui(x, a−i) − ∆.\n\nTherefore if we choose\n\nx′ := x∗\n\ni −\n\n(cid:88)\n\na∈Ai\n\n1[a ∈ E1]x∗\n\ni (a)ea +\n\n(cid:88)\n\na∈Ai\n\n1[a ∈ E1]x∗\n\ni (a) · x(a),\n\nthat is if we play the dominating strategy instead of the dominated action in x∗\n\ni , then\n\nui(x′, x∗\n\n−i) ≥ ui(x∗\n\ni , x∗\n\n−i) +\n\n(cid:88)\n\na∈Ai\n\nx∗\n\ni (a) · 1[a ∈ E1]∆.\n\nIt follows that\n\n(cid:88)\n\na∈Ai\n\nx∗\n\ni (a) · 1[a ∈ E1] ≤\n\nε ∆\n\n.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nInduction step: By the induction hypothesis, ∀i ∈ [N ],\n\n(cid:88)\n\na∈Ai\n\nx∗\n\ni (a) · 1[a ∈ El] ≤\n\n2lε ∆\n\n.\n\nNow consider\n\n(cid:101)xi :=\n\ni − (cid:80) x∗ 1 − (cid:80)\n\na∈Ai\n\na∈Ai\n\n1[a ∈ El] · x∗ 1[a ∈ El] · x∗\n\ni (a)ea i (a)\n\n,\n\n(∀i ∈ [N ])\n\nwhich is supported on actions on in El. The induction hypothesis implies ∥(cid:101)xi − x∗ Therefore ∀i ∈ [N ], ∀a ∈ Ai,\n\ni ∥1 ≤ 6lε/∆.\n\n(cid:12) (cid:12)ui(a, (cid:101)x−i) − ui(a, x∗\n\n−i)(cid:12)\n\n(cid:12) ≤\n\n6N lε ∆\n\n.\n\nNow if (cid:101)a ∈ (El+1 \\ El) ∩ Ai, since (cid:101)x−i is not supported on El, ∃x ∈ ∆(Ai) such that\n\nui((cid:101)a, (cid:101)x−i) ≤ ui(x, (cid:101)x−i) − ∆.\n\nIt follows that\n\nui((cid:101)a, x∗\n\n−i) ≤ ui(x, x∗\n\n−i) − ∆ +\n\n12N lε ∆\n\n≤ ui(x, x∗\n\n−i) −\n\n∆ 2\n\n.\n\nUsing the same arguments as in the base case,\n\n(cid:88)\n\na∈Ai\n\nx∗\n\ni (a) · 1[a ∈ El+1 \\ El] ≤\n\nε ∆ − 12N lε\n\n∆\n\n≤\n\n2ε ∆\n\n.\n\nIt follows that ∀i ∈ [N ],\n\n(cid:88)\n\na∈Ai\n\nx∗\n\ni (a) · 1[a ∈ El+1] ≤\n\n2(l + 1)ε ∆\n\n.\n\nThe statement is thus proved via induction on l.\n\nB FIND ONE RATIONALIZABLE ACTION PROFILE\n\nB.1 PROOF OF PROPOSITION 2\n\nProof. Consider the following N -player game denoted by G0 with action set [A]:\n\nui (·) = 0\n\nuN (aN ) = ∆ · 1[aN > 1].\n\n(1 ≤ i ≤ N − 1)\n\nSpecifically, a payoff with mean u is realized by a skewed Rademacher random variable with 1+u probability on +1 and 1−u 2 on −1. In game G0, clearly for player N , the action 1 is ∆-dominated. However, consider the following game, denoted by Ga∗ (where a∗ ∈ [A]N −1)\n\n2\n\nui (·) = 0, uN (aN ) = ∆,\n\nuN (1, a−N ) = 2∆ · 1[a−N = a∗].\n\n(1 ≤ i ≤ N − 1) (aN > 1)\n\nIt can be seen that in game Ga∗ , for player N , the action 1 is not dominated or iteratively strictly dominated. Therefore, suppose that an algorithm O is able to determine whether an action is rationalizable (i.e. not iteratively strictly dominated) with 0.9 accuracy, then its output needs to be False with at least 0.9 probability in game G0, but True with at least 0.9 probability in game Ga∗ . By Pinsker’s inequality,\n\nKL(O(G0)||O(Ga∗ )) ≥ 2 · 0.82 > 1,\n\nwhere we used O(G) to denote the trajectory generated by running algorithm O on game G. Meanwhile, notice that G0 and Ga∗ is different only when the first N − 1 players play a∗. Denote\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nthe number of times where the first N − 1 players play a∗ by n(a∗). Using the chain rule of KL-divergence,\n\nKL(O(G0)||O(Ga∗ )) ≤ EG0 [n(a∗)] · KL\n\n(cid:18)\n\nBer\n\n(cid:18) 1 2\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)\n\nBer\n\n(cid:19)(cid:19)\n\n(cid:18) 1 + 2∆ 2\n\n(a)\n\n≤ EG0 [n(a∗)] ·\n\n1 1−2∆ 2\n\n· (2∆)2\n\n(b)\n\n≤ 10∆2EG0 [n(a∗)] .\n\nHere (a) follows from reverse Pinsker’s inequality (see e.g. Binette (2019)), while (b) uses the fact that ∆ < 0.1. This means that for any a∗ ∈ [A]N −1,\n\nIt follows that the expected number of samples when running O on G0 is at least\n\nEG0 [n(a∗)] ≥\n\n1\n\n10∆2 .\n\n\n\n\n\nEG0\n\n\n\n(cid:88)\n\nn(a∗)\n\n ≥\n\na∗∈[A]N −1\n\nAN −1 10∆2 .\n\nB.2 PROOF OF THEOREM 3\n\nProof. We first present the concentration bound. For l ∈ [L], i ∈ [N ], and a ∈ Ai, by Hoeffding’s inequality we have that with probability at least 1 − δ\n\n(cid:12) (cid:12)\n\n(cid:12)ui(a, a(l−1)\n\n−i\n\n) − ˆui(a, a(l−1)\n\n−i\n\n(cid:12) (cid:12) (cid:12) ≤ )\n\n4 ln(AN L/δ) M\n\n≤\n\n∆ 4\n\n.\n\nLN A , (cid:114)\n\nTherefore by a union bound we have that with probability at least 1 − δ, for all l ∈ [L], i ∈ [N ], and a ∈ Ai,\n\nWe condition on this event for the rest of the proof.\n\n(cid:12) (cid:12)\n\n(cid:12)ui(a, a(l−1)\n\n−i\n\n) − ˆui(a, a(l−1)\n\n−i\n\n(cid:12) (cid:12) (cid:12) ≤ )\n\n∆ 4\n\n.\n\nWe use induction on l to prove that for all l ∈ [L] ∪ {0}, (a(l) N ) can survive at least l rounds of IDE. The base case for l = 0 directly holds. Now we assume that the case for 1, 2, . . . , l − 1 holds and consider the case of l. For any i ∈ [N ], we show that a(l) empirical best response, i.e.\n\ncan survive at least l rounds of IDE. Recall that a(l)\n\n1 , · · · , a(l)\n\nis the\n\ni\n\ni\n\na(l)\n\ni = arg max\n\nˆui(a, a(l−1)\n\n−i\n\n).\n\na∈Ai For any mixed strategy xi ∈ ∆(Ai), we have that ) − ui(xi, a(l−1) ) − ˆui(xi, a(l−1)\n\nui(a(l) ≥ˆui(a(l)\n\n, a(l−1)\n\n, a(l−1)\n\n) −\n\n−i\n\n−i\n\n(cid:12) (cid:12)\n\n)\n\ni\n\ni\n\n−i\n\n−i\n\n(cid:12)ui(a(l)\n\ni\n\n, a(l−1)\n\n−i\n\n) − ˆui(a(l)\n\ni\n\n, a(l−1)\n\n−i\n\n(cid:12) (cid:12) )\n(cid:12) −\n\n(cid:12) (cid:12)\n\n(cid:12)ui(xi, a(l−1)\n\n−i\n\n) − ˆui(xi, a(l−1)\n\n−i\n\n(cid:12) (cid:12) )\n(cid:12)\n\n.\n\n−\n\n= −\n\n∆ 4\n\n∆ 4\n\n≥0 −\n\n∆ 2\nSince actions in a(l−1) in rounds 1, · · · , l. Since xi can be arbitrarily chosen, a(l) can now ensure that the output (a(L) ∆-rationalizability (see Definition 1).\n\n, · · · , a(L)\n\n−i\n\n1\n\ni\n\ncan survive at least l − 1 rounds of ∆-IDE, a(l)\n\ncannot be ∆-dominated by xi can survive at least l rounds of ∆-IDE. We N ) survives L rounds of ∆-IDE, which is equivalent to\n\ni\n\nThe total number of samples used is\n\nLN A · M = (cid:101)O\n\n(cid:18) LN A ∆2\n\n(cid:19)\n\n.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nB.3 PROOF OF THEOREM 4\n\nProof. Without loss of generality, assume that ∆ < 0.1. Consider the following instance where A1 = · · · = AN = [A]:\n\nui(ai) = ∆ · 1[ai = 1], (cid:26)∆ · 1[aj = 1]\n\nuj(aj, a−j) =\n\n∆ · 1[aj = 1] + 2∆ · 1[aj = a]\n\n(i ̸= j)\n\n(a−j ̸= {1}N −1) (a−j = {1}N −1)\n\n.\n\nDenote this instance by Gj,a. Additionally, define the following instance G0:\n\nui(ai) = ∆ · 1[ai = 1].\n\n(∀i ∈ [N ])\n\nAs before, a payoff with expectation u is realized as a random variable with distribution 2Ber( 1+u 2 )−1. It can be seen that the only difference between G0 and Gj,a lies in uj(a, {1}N −1). By the KLdivergence chain rule, for any algorithm O,\n\nKL ( O(G0)∥ O(Gj,a)) ≤ 10∆2 · EG0\n\n(cid:2)n(aj = a, a−j = {1}N −1)(cid:3) ,\n\nwhere n(aj = a, a−j = {1}N −1) denotes the number of times the action profile (a, 1N −1) is played. Note that in G0, the only action profile surviving two rounds of ∆-IDE is (1, · · · , 1), while in , a, 1, · · · , 1). To guarantee 0.9 accuracy, by Gj,a, the only rationalizable action profile is (1, · · · , 1 (cid:124) (cid:123)(cid:122) (cid:125) j−1\n\nPinsker’s inequality,\n\nKL (O(G0)||O(Gj,a)) ≥\n\n1 2\n\n|O(G0) − O(Gj,a)|2 > 1.\n\nIt follows that ∀j ∈ [N ], a > 1,\n\nEG0\n\n(cid:2)n(aj = a, a−j = {1}N −1)(cid:3) ≥\n\n1\n\n10∆2 .\n\nThus the total expected sample complexity is at least\n\n(cid:88)\n\nEG0\n\n(cid:2)n(aj = a, a−j = {1}N −1)(cid:3) ≥\n\na>1,j∈[N ]\n\nN (A − 1) 10∆2\n\n.\n\nC OMITTED PROOFS IN SECTION 4\n\nWe start our analysis by bounding the sampling noise. For player i ∈ [N ], action ai ∈ Ai, and τ ∈ [T ], we denote the sampling noise as\n\nξ(τ )\n\ni\n\n(ai) := u(τ )\n\ni\n\n(ai) − ui(ai, θ(τ )\n\n−i ).\n\nWe have the following lemma. Lemma C.1. Let Ω1 denote the event that for all t ∈ [T ], i ∈ [N ], and ai ∈ Ai,\n\nt (cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) τ =1\n\nξ(τ )\n\ni\n\n(ai)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:118) (cid:117) (cid:117) (cid:116)ln(AN T /δ)\n\n≤ 2\n\nt (cid:88)\n\nτ =1\n\n1 Mτ\n\n.\n\nThen Pr[Ω1] ≥ 1 − δ.\n\nProof. Note that(cid:80)t By Azuma-Hoeffding inequality, with probability at least 1 − δ ai ∈ Ai,\n\n(ai) can be written as the sum of (cid:80)t\n\nτ =1 ξ(τ )\n\ni\n\nτ =1 Mτ mean-zero bounded terms. AN T , for a fixed i ∈ [N ], t ∈ [T ],\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nt (cid:88)\n\nτ =1\n\nξ(τ )\n\ni\n\n(cid:12) (cid:12) (cid:12) (ai) (cid:12) (cid:12)\n\n≤ 2\n\n(cid:118) (cid:117) (cid:117) (cid:116)ln(AN T /δ)\n\nt (cid:88)\n\nτ =1\n\nMτ ·\n\n(cid:18) 1 Mτ\n\n(cid:19)2\n\n.\n\n(6)\n\nA union bound over i ∈ [N ], t ∈ [T ], ai ∈ Ai proves the statement.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nLemma C.2. With probability at least 1 − 2δ, for all t ∈ [T ] and all i ∈ [N ], ai ∈ Ai ∩ EL,\n\nθ(t) i (ai) ≤ p.\n\nProof. We condition on the event Ω1 defined in Lemma C.1 and the success of Algorithm 1. We prove the claim by induction in t. The base case for t = 1 holds directly by initialization. Now we assume the case for 1, 2, . . . , t holds and consider the case of t + 1.\n\nConsider a fixed player i ∈ [N ] and iteratively dominated action ai ∈ Ai ∩ EL. By definition there exists a mixed strategy xi such that for all a−i ∩ EL = ∅,\n\nTherefore for τ ∈ [t], by the induction hypothesis for τ ,\n\nui(xi, a−i) ≥ ui(ai, a−i) + ∆.\n\nui(xi, θ(τ )\n\n−i ) ≥ ui(ai, θ(τ ) ≥ ui(ai, θ(τ )\n\n−i ) + ∆/2.\n\n−i ) + (1 − AN p) · ∆ − AN p\n\nConsequently,\n\nt (cid:88)\n\n(u(τ )\n\ni\n\n(xi) − u(τ )\n\ni\n\n(ai))\n\nτ =1\n\nt (cid:88)\n\n≥\n\n(ui(xi, θ(τ )\n\n−i ) − ui(ai, θ(τ )\n\n−i )) − 4 ·\n\n(cid:118) (cid:117) (cid:117) (cid:116)ln(AN T /δ)\n\nt (cid:88)\n\nτ =1\n\n1 Mτ\n\nτ =1\n\n≥\n\nt∆ 2\n\n− 4 ·\n\n≥\n\nt∆ 4\n\n.\n\nTherefore by our choice of learning rate,\n\n(cid:118) (cid:117) (cid:117) (cid:116)ln(AN T /δ)\n\nt (cid:88)\n\nτ =1\n\n1 Mτ\n\n(7)\n\n(By (6))\n\n(By (7))\n\n(cid:32)\n\nθ(t+1)\n\ni\n\n(ai) ≤ exp\n\n−ηt ·\n\nt (cid:88)\n\n(cid:16)\n\nu(τ )\n\ni\n\n(xi) − u(τ )\n\ni\n\n(ai)\n\n(cid:33)\n\n(cid:17)\n\n(cid:18)\n\n≤ exp\n\n−\n\nτ =1 4 ln(1/p) ∆t\n\n(cid:19)\n\n·\n\n∆t 4\n\n= p.\n\nθ(t+1)\n\ni\n\n(ai) ≤ p\n\nTherefore\n\nas desired.\n\nNow we turn to the ε-CCE guarantee. For a player i ∈ [N ], recall that the regret is defined as\n\nRegreti\n\nT = max\n\nθ∈∆(Ai)\n\nT (cid:88)\n\n⟨u(t)\n\ni\n\nt=1\n\n, θ − θ(t)\n\ni ⟩.\n\nLemma C.3. The regret can be bounded as\n\nRegreti\n\nT ≤ O\n\n(cid:18)√\n\nln A · T +\n\nln(1/p) ln T ∆\n\n(cid:19)\n\n.\n\nProof. Note that apart from the choice of θ(1), we are exactly running FTRL with learning rates\n\nηt = max\n\n(cid:26)\n\n(cid:112)ln A/t,\n\n4 ln(1/p) ∆t\n\n(cid:27)\n\n,\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nwhich are monotonically decreasing. Therefore following the standard analysis of FTRL (see, e.g., Orabona (2019, Corollary 7.9)), we have\n\nmax θ∈∆(Ai)\n\nT (cid:88)\n\n⟨u(t)\n\ni\n\nt=1\n\n, θ − θ(t)\n\ni ⟩ ≤ 2 +\n\nln A ηT\n\n+\n\n1 2\n\nT (cid:88)\n\nt=1\n\nηt\n\n√\n\n≤ 2 +\n\nln A · T +\n\n(cid:32)(cid:114)\n\n1 2\n\nT (cid:88)\n\nt=1\n\n(cid:33)\n\nln A t\n\n+\n\n4 ln(1/p) ∆t\n\n(cid:18)√\n\n= O\n\nln A · T +\n\nln(1/p) ln T ∆\n\n(cid:19)\n\n.\n\nHowever, this form of regret cannot directly imply approximate CCE. We define the following expected version regret\n\nRegreti,⋆\n\nT = max\n\nθ∈∆(Ai)\n\nT (cid:88)\n\n⟨ui(·, θ(t)\n\n−i), θ − θ(t)\n\ni ⟩.\n\nt=1\n\nThe next lemma bound the difference between these two types of regret\n\nLemma C.4. The following event Ω2 holds with probability at least 1 − δ: for all i ∈ [N ]\n\n(cid:12) (cid:12)\n\n(cid:12)Regreti,⋆\n\nT − Regreti\n\nT\n\n(cid:12) (cid:12) (cid:12) ≤ O\n\n(cid:16)(cid:112)T · ln(N A/δ)\n\n(cid:17)\n\n.\n\nProof. We denote\n\nTherefore we have\n\nΘi := {e1, e2, . . . , e|Ai|}\n\n(cid:12) (cid:12)\n\n(cid:12)Regreti,⋆\n\nT − Regreti\n\nT\n\n(cid:12) (cid:12) (cid:12)\n\n=\n\n=\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nmax θ∈∆(Ai)\n\nT (cid:88)\n\nt=1\n\n⟨ui(·, θ(t)\n\n−i), θ − θ(t)\n\ni ⟩ − max\n\nθ∈∆(Ai)\n\nT (cid:88)\n\nt=1\n\n⟨u(t)\n\ni\n\n, θ − θ(t) i ⟩\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\n⟨ui(·, θ(t)\n\n−i), θ − θ(t)\n\ni ⟩ − max θ∈Θi\n\nT (cid:88)\n\nt=1\n\n⟨u(t)\n\ni\n\n(cid:12) (cid:12) , θ − θ(t) (cid:12) i ⟩ (cid:12) (cid:12)\n\n⟨ui(·, θ(t)\n\n−i), θ − θ(t)\n\ni ⟩ −\n\nT (cid:88)\n\n⟨u(t)\n\ni\n\n⟨ui(·, θ(t)\n\n−i) − u(t)\n\ni\n\nt=1\n\n(cid:12) (cid:12) , θ − θ(t) (cid:12) i ⟩ (cid:12) (cid:12)\n\n, θ − θ(t) i ⟩\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nmax θ∈Θi\n\n= max θ∈Θi\n\n= max θ∈Θi\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nNote that ⟨ui(·, θ(t) Hoeffding’s inequality, for a fixed θ ∈ Θi, with probability at least 1 − δ\n\n−i) − u(t)\n\n, θ − θ(t)\n\ni ⟩ is a bounded martingale difference sequence. By Azuma-\n\ni\n\nAN ,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nT (cid:88)\n\n⟨ui(·, θ(t)\n\n−i) − u(t)\n\ni\n\nt=1\n\n(cid:12) (cid:12) , θ − θ(t) (cid:12) i ⟩ (cid:12) (cid:12)\n\n(cid:16)(cid:112)T · ln(N A/δ)\n\n(cid:17)\n\n≤ O\n\nThus we complete the proof by a union bound.\n\nProof of Theorem 6. We condition on event Ω1 defined Lemma C.1, event Ω2 defined in Lemma C.4, and the success of Algorithm 1.\n\nCoarse Correlated Equilibria. By Lemma C.3 and Lemma C.4 we know that for all i ∈ [N ],\n\nRegreti,⋆\n\nT ≤ O\n\n(cid:18)√\n\nln A · T +\n\nln(1/p) ln T ∆\n\n+ (cid:112)T · ln(N A/δ)\n\n(cid:19)\n\n.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nTherefore choosing\n\nT = Θ\n\n(cid:18) ln(N A/δ) ε2\n\n+\n\nln2(N A/∆εδ) ∆ε\n\n(cid:19)\n\nwill guarantee that Regreti,⋆ ((cid:80)T\n\ni=1θ(t)\n\nt=1 ⊗N\n\ni )/T would be an (ε/2)-CCE.\n\nT is at most εT /2 for all i ∈ [N ]. In this case the average strategy\n\nFinally, in the clipping step, ∥ ̄θ(t) i=1θ(t) we have ∥ ⊗n\n\n ̄θ(t) i − ⊗n\n\ni − θ(t) i ∥1 ≤ ε\n\ni=1\n\n4 , which further implies\n\ni ∥1 ≤ 2pA ≤ ε\n\n4N for all i ∈ [N ], t ∈ [T ]. Thus for all t ∈ [T ],\n\n(cid:13) T\n(cid:13) (cid:88) (cid:13) (\n(cid:13) (cid:13)\n\nt=1\n\n ̄θ(t) i )/T − (\n\nT (cid:88)\n\n⊗n\n\ni=1\n\nt=1\n\n⊗n\n\ni=1θ(t)\n\ni )/T\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n≤\n\nε 4\n\n.\n\nTherefore the output strategy Π = ((cid:80)T\n\nt=1 ⊗N\n\ni=1\n\n ̄θ(t) i )/T is an ε-CCE.\n\nRationalizability. By Lemma C.2, if a ∈ EL ∩ Ai, θ(t) i (a) = 0, i.e., the action would not be the support in the output strategy Π = ((cid:80)T ̄θ(t) Sample complexity. The total number of full-information queries is\n\ni (a) ≤ p for all t ∈ [T ]. It follows that ̄θ(t) i )/T .\n\nt=1 ⊗N\n\ni=1\n\nT (cid:88)\n\nt=1\n\nMt ≤ T +\n\nT (cid:88)\n\nt=1\n\n64 ln(AN T /δ) ∆2t\n\n≤ T + (cid:101)O\n\n(cid:19)\n\n(cid:18) 1 ∆2\n\n= (cid:101)O\n\n(cid:18) 1\n\n∆2 +\n\n(cid:19)\n\n.\n\n1 ε2\n\nThe total sample complexity for CCE learning would then be\n\nN A ·\n\nT (cid:88)\n\nt=1\n\nMt = (cid:101)O\n\n(cid:18) N A\n\nε2 +\n\n(cid:19)\n\n.\n\nN A ∆2\n\nFinally consider the cost of finding one IDE-surviving action profile ( (cid:101)O (cid:0) LN A claimed rate.\n\n∆2\n\n(cid:1)) and we get the\n\nD OMITTED PROOFS IN SECTION 5\n\nSimilar to the CCE case we first bound the sampling noise. For action ai ∈ Ai, and τ ∈ [T ], we denote the sampling noise as\n\nξ(τ )\n\ni\n\n(ai) := u(τ )\n\ni\n\n(ai) − ui(ai, θ(τ )\n\n−i ).\n\nIn the CE case, we are interested in the weighted sum of noise (cid:80)t bounded in the following lemma.\n\nτ =1 ξ(τ )\n\ni\n\n(ai)θ(τ )\n\ni\n\n(bi), which is\n\nLemma D.1. The following event Ω3 holds with probability at least 1 − δ: for all t ∈ [T ], i ∈ [N ], and ai ∈ Ai,\n\nt (cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) τ =1\n\nξ(τ )\n\ni\n\n(ai)θ(τ )\n\ni\n\n(bi)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n∆ 4\n\nt (cid:88)\n\nτ =1\n\nθ(τ )\n\ni\n\n(bi).\n\nProof. Note that (cid:80)t terms. Precisely, there are M τ i\n\nτ =1 ξ(τ )\n\ni\n\n(ai)θ(τ )\n\ni\n\n(bi) can be written as the sum of (cid:80)t\n\nτ =1 M τ\n\ni mean-zero bounded\n\nterms bounded by θ(τ )\n\ni\n\n(bi)\n\nM τ i\n\n. By the Azuma-Hoeffding inequality, we\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nhave that with probability at least 1 − δ\n\nA2N T ,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nt (cid:88)\n\nτ =1\n\nξ(τ )\n\ni\n\n(ai)θ(τ )\n\ni\n\n(cid:12) (cid:12) (cid:12) (bi) (cid:12) (cid:12)\n\n(cid:118) (cid:117) (cid:117) (cid:116)ln(AN T /δ)\n\n≤ 2 ·\n\n(cid:118) (cid:117) (cid:117) (cid:116)ln(AN T /δ)\n\n= 2 ·\n\nt (cid:88)\n\nτ =1\n\nM τ\n\ni ·\n\n(cid:32)\n\nθ(τ )\n\ni\n\n(bi)\n\n(cid:33)2\n\nM τ i\n\nt (cid:88)\n\n(θ(τ )\n\ni\n\n(bi))2\n\nτ =1\n\nM τ i\n\n≤\n\n≤\n\n∆ 4\n\n∆ 4\n\n(cid:118) (cid:117) (cid:117) (cid:116)\n\n·\n\nt (cid:88)\n\nτ =1\n\nθ(τ )\n\ni\n\n(bi)\n\nτ (cid:88)\n\nj=1\n\nθ(j)\n\ni\n\n(bi)\n\nt (cid:88)\n\nτ =1\n\nθ(τ )\n\ni\n\n(bi)\n\nTherefore by a union bound we complete the proof.\n\nLemma D.2. With probability at least 1 − 2δ, for all t ∈ [T ], all i ∈ [N ], and all ai ∈ Ai ∩ EL,\n\nθ(t) i (ai) ≤ p\n\nProof. We condition on the event Ω3 defined in Lemma D.1 and the success of Algorithm 1. We prove the claim by induction in t. The base case for t = 1 holds directly by initialization. Now we assume the case for 1, 2, . . . , t holds and consider the case of t + 1.\n\nConsider a fixed player i ∈ [N ], an iteratively dominated action ai ∈ Ai ∩ EL, and an expert bi. By definition there exists a mixed strategy xi such that for all a−i ∩ EL = ∅,\n\nui(xi, a−i) ≥ ui(ai, a−i) + ∆\n\nTherefore for τ ∈ [t], by induction hypothesis we have ui(xi, θ(τ )\n\n−i ) ≥ ui(ai, θ(τ ) ≥ ui(ai, θ(τ )\n\n−i ) + ∆/2\n\n−i ) + (1 − AN p) · ∆ − AN p\n\nThus we have\n\nt (cid:88)\n\n(u(τ )\n\ni\n\n(xi) − u(τ )\n\ni\n\n(ai)) · θ(τ )\n\ni\n\n(bi)\n\nτ =1\n\n(bi) −\n\n∆ 4\n\nt (cid:88)\n\nτ =1\n\nθ(τ )\n\ni\n\n(bi)\n\n≥\n\n≥\n\n=\n\nt (cid:88)\n\n(ui(xi, θ(τ )\n\n−i ) − ui(ai, θ(τ )\n\n−i )) · θ(τ )\n\ni\n\nτ =1\n\n∆ 2\n\n∆ 4\n\nt (cid:88)\n\nτ =1\n\nt (cid:88)\n\nτ =1\n\nθ(τ )\n\ni\n\n(bi) −\n\n∆ 4\n\nt (cid:88)\n\nτ =1\n\nθ(τ )\n\ni\n\n(bi)\n\nθ(τ )\n\ni\n\n(bi)\n\nBy our choice of learning rate,\n\nˆθ(t+1)\n\ni\n\n(ai|bi) ≤ exp\n\n−ηbi\n\nt,i ·\n\n(cid:32)\n\nt (cid:88)\n\nτ =1\n\n(cid:16)\n\nθ(τ )\n\ni\n\n(bi)\n\nu(τ )\n\ni\n\n(xi) − u(τ )\n\ni\n\n(cid:33)\n\n(cid:17)\n\n(ai)\n\n(cid:32)\n\n≤ exp\n\n−\n\n4 ln(1/p) τ =1 θ(τ )\n\ni\n\n∆ (cid:80)t\n\n·\n\n∆ 4\n\nt (cid:88)\n\ni=1\n\n(b)\n\n(cid:33)\n\nθ(τ )\n\ni\n\n(b)\n\n= p.\n\nTherefore we conclude\n\nθ(t+1)\n\ni\n\n(ai) =\n\n(cid:88)\n\nbi∈Ai\n\nˆθ(t+1)\n\ni\n\n(ai|bi)θ(t+1)\n\ni\n\n(bi) ≤ p\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nNow we turn to the ε-CE guarantee. For a player i ∈ [N ], recall that the swap-regret is defined as\n\nSwapRegreti\n\nT := sup\n\nφ:Ai→Ai\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\nb∈Ai\n\ni (b)u(t) θ(t)\n\ni (φ(b)) −\n\nT (cid:88)\n\nt=1\n\n(cid:68)\n\nθ(t)\n\ni\n\n, u(t)\n\ni\n\n(cid:69)\n\n.\n\nLemma D.3. For all i ∈ [N ], the swap-regret can be bounded as\n\nSwapRegreti\n\nT ≤ O\n\n(cid:18)\n\n(cid:112)A ln(A)T +\n\nA ln(N AT /∆ε)2 ∆\n\n(cid:19)\n\n.\n\nProof. For i ∈ [N ], recall that the regret for an expert b ∈ Ai is defined as\n\nRegreti,b\n\nT := max a∈Ai\n\nT (cid:88)\n\nt=1\n\ni (b)u(t) θ(t)\n\ni (a) −\n\nT (cid:88)\n\nt=1\n\n(cid:68)ˆθ(t)\n\ni (·|b), θ(t)\n\ni (b)u(t)\n\ni\n\n(cid:69)\n\n.\n\nSince θ(t)\n\ni (a) = (cid:80)\n\nb∈Ai\n\ni (a|b)θ(t) ˆθ(t)\n\ni (b) for all a and all t > 1,\n\n(cid:88)\n\nb∈Ai\n\nRegreti,b\n\nT =\n\n(cid:88)\n\nb∈Ai\n\nmax ab∈Ai\n\nT (cid:88)\n\nt=1\n\ni (b)u(t) θ(t)\n\ni (ab) −\n\n(cid:88)\n\nT (cid:88)\n\nb∈Ai\n\nt=1\n\n(cid:68)ˆθ(t)\n\ni (·|b)θ(t)\n\ni (b), u(t)\n\ni\n\n(cid:69)\n\n(cid:43)\n\n= max\n\nφ:Ai→Ai\n\n≥ max\n\nφ:Ai→Ai\n\n(cid:88)\n\nT (cid:88)\n\nb∈Ai\n\nt=1\n\nT (cid:88)\n\n(cid:88)\n\nt=1\n\nb∈Ai\n\ni (b)u(t) θ(t)\n\ni (φ(b)) −\n\n(cid:42)\n\nT (cid:88)\n\n(cid:88)\n\ni (·|b)θ(t) ˆθ(t)\n\ni (b), u(t)\n\ni\n\ni (b)u(t) θ(t)\n\ni (φ(b)) −\n\nt=1\n\nT (cid:88)\n\nt=2\n\nb∈Ai\n\n(cid:68)\n\nθ(t)\n\ni\n\n(cid:69)\n\n, u(t)\n\ni\n\n− 1 ≥ SwapRegreti\n\nT − 1.\n\nIt now suffices to control the regret of each individual expert. For expert b, we are essentially running FTRL with learning rates\n\n(cid:40)\n\n√\n\n(cid:41)\n\n4 ln(1/p) τ =1 θ(τ ) which are clearly monotonically decreasing. Therefore using standard analysis of FTRL (see, e.g., Orabona (2019, Corollary 7.9)),\n\nt,i := max\n\nA ln A √\n\n∆ (cid:80)t\n\n(b)\n\nηb\n\nt\n\n,\n\n,\n\ni\n\nRegreti,b\n\nT ≤\n\nln A ηb\n\nT,i\n\n+\n\nT (cid:88)\n\nt=1\n\nt,i · θ(t) ηb\n\ni (b)2\n\n(cid:114)\n\n(cid:114)\n\n≤\n\n≤\n\nT ln A A\n\n+\n\nT ln A A\n\n+\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\n(cid:114)\n\n(cid:114)\n\nθ(t) i (b) ·\n\nθ(t) i (b) ·\n\nA ln A t\n\n+\n\n4 ln(1/p) ∆\n\n·\n\nT (cid:88)\n\nt=1\n\n(cid:80)t\n\nA ln A t\n\n+\n\n4 ln(1/p) ∆\n\n(cid:18)\n\n1 + ln\n\nθ(t) i (b) τ =1 θ(τ ) i\n(cid:19)(cid:19) (cid:18) T p\n\n(b)\n\n.\n\nHere we used the fact that ∀b ∈ Ai, θ(1)\n\ni\n\n(b) ≥ p, and\n\nT (cid:88)\n\nt=1\n\nθ(t) i (b) i=1 θ(τ )\n\ni\n\n(cid:80)τ\n\n≤ 1 +\n\n(b)\n\n(cid:90) (cid:80)T\n\nt=1 θ(t)\n\ni\n\n(b)\n\ni\n\nθ(1) (cid:18) T p\n\n(cid:19)\n\n.\n\n≤ 1 + ln\n\n(b)\n\nds s\n\n= 1 + ln\n\n(cid:33)\n\n(cid:32) (cid:80)T\n\nt=1 θ(t) i (b) θ(1) (b)\n\ni\n\nNotice that (cid:80)\n\nb∈Ai\n\n(cid:80)T\n\nt=1 θ(t)\n\ni (b) ·\n\n(cid:113) A ln A\n\nt ≤ O((cid:112)A ln(A)T ). Therefore\n\nSwapRegreti\n\nT ≤ O(1) +\n\nRegreti,b\n\nT ≤ O\n\n(cid:18)\n\n(cid:112)A ln(A)T +\n\nA ln(N AT /∆ε)2 ∆\n\n(cid:19)\n\n.\n\n(8)\n\n(cid:88)\n\nb∈Ai\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nSimilar to the CCE case,, this form of regret can not directly imply approximate CE. We define the following expected version regret\n\nSwapRegreti,⋆\n\nT := sup\n\nφ:Ai→Ai\n\nT (cid:88)\n\n(cid:68)\n\nt=1\n\nφ ◦ θ(t)\n\ni\n\n, ui(·, θ(t) −i)\n\nT (cid:88)\n\n(cid:68)\n\n(cid:69)\n\n−\n\nθ(t)\n\ni\n\n, ui(·, θ(t) −i)\n\n(cid:69)\n\nt=1\n\nThe next lemma bound the difference between these two types of regret Lemma D.4. The following event Ω4 has probability at least 1 − δ: for all i ∈ [N ],\n\n(cid:12) (cid:12)\n\n(cid:12)SwapRegreti,⋆\n\nT − SwapRegreti\n\nT\n\n(cid:32)(cid:115)\n\n(cid:12) (cid:12) (cid:12) ≤ O\n\nAT ln\n\n(cid:19)(cid:33)\n\n.\n\n(cid:18) AN δ\n\nProof. Note that\n\n(cid:12) (cid:12)\n\n(cid:12)SwapRegreti,⋆\n\nT − SwapRegreti\n\nT\n\n(cid:12) (cid:12) (cid:12)\n\n=\n\nT (cid:88)\n\n(cid:12) (cid:12) (cid:12) sup (cid:12) (cid:12) φ:Ai→Ai (cid:12) T\n(cid:12) (cid:88) (cid:12) (cid:12) (cid:12)\n\nφ:Ai→Ai\n\nt=1\n\nt=1\n\n(cid:68)\n\n(cid:68)\n\n≤ sup\n\nφ ◦ θ(t)\n\ni − θ(t)\n\ni\n\n, ui(·, θ(t) −i)\n\n(cid:69)\n\n− sup\n\n(cid:68)\n\nφ ◦ θ(t)\n\ni − θ(t)\n\ni\n\nT (cid:88)\n\nt=1\n\n, u(t)\n\ni\n\n(cid:69)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nφ ◦ θ(t)\n\ni − θ(t)\n\ni\n\n, ui(·, θ(t)\n\n−i) − u(t)\n\ni\n\nφ:Ai→Ai (cid:12) (cid:12) (cid:69) (cid:12) (cid:12) (cid:12)\n\n.\n\nNotice that E[u(t)\n\ni\n\n(cid:68)\n\n] = ui (cid:16)\n\n(cid:17)\n\n(cid:16)\n\n·, θ(t) −i (cid:17)\n\n, and that u(t)\n\ni\n\n(cid:69)\n\ni\n\n−i\n\n, ui\n\n·, θ(t)\n\n− u(t)\n\nφ ◦ θ(t)\n\ni − θ(t)\n\nξφ t := Hoeffding inequality, for a fixed φ : Ai → Ai, with probability 1 − δ′, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:18) 2 δ′\n\n2T ln\n\nT (cid:88)\n\n(cid:19) .\n\n≤ 2\n\nξφ\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:115)\n\nt\n\ni\n\nt=1\n\n∈ [−1, 1]A. Therefore, ∀φ : Ai → Ai,\n\nis a bounded martingale difference sequence. By Azuma-\n\nBy setting δ′ = δ/(N AA), we get with probability 1 − δ/N , ∀φ : Ai → Ai,\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nT (cid:88)\n\nt=1\n\nξφ\n\nt\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:115)\n\n≤ 2\n\n2AT ln\n\n(cid:18) 2AN δ\n\n(cid:19) .\n\nTherefore we complete the proof by a union bound over i ∈ [N ].\n\nProof of Theorem 12. We condition on event Ω3 defined Lemma D.1, event Ω4 defined in Lemma D.4, and the success of Algorithm 1.\n\nCorrelated Equilibrium. By Lemma D.3 and Lemma D.4 we know that for all i ∈ [N ], (cid:19)(cid:33)\n\n(cid:115)\n\n(cid:32)\n\nSwapRegreti,⋆\n\nT ≤ O\n\n(cid:112)A ln(A)T +\n\nA ln(N AT /∆ε)2 ∆\n\n+\n\nAT ln\n\n.\n\n(cid:18) AN δ\n\nTherefore choosing\n\nT = Θ\n\n(cid:32)\n\n(cid:1)\n\nA ln (cid:0) AN ε2\n\nδ\n\nA ln3 (cid:0) N A ∆ε\n\n∆εδ\n\n+\n\n(cid:33)\n\n(cid:1)\n\nwill guarantee that SwapRegreti,⋆ ((cid:80)T\n\ni )/T would be an ε/2-CE.\n\ni=1θ(t)\n\nt=1 ⊗N\n\nT is at most εT /2 for all i ∈ [N ]. In this case the average strategy\n\ni ∥1 ≤ 2pA ≤ ε\n\n4N for all i ∈ [N ], t ∈ [T ]. Thus for all t ∈ [T ],\n\nFinally, in the clipping step, ∥ ̄θ(t) i=1θ(t) ̄θ(t) we have ∥ ⊗n i − ⊗n (cid:13) T\n(cid:13) (cid:88) (cid:13) (\n(cid:13) (cid:13)\n\ni=1\n\nt=1\n\ni − θ(t) i ∥1 ≤ ε\n\n⊗n\n\ni=1\n\n4 , which further implies\n\n⊗n\n\ni=1θ(t)\n\ni )/T\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)1\n\n≤\n\nε 4\n\n.\n\n ̄θ(t) i )/T − (\n\nT (cid:88)\n\nt=1\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nTherefore the output strategy Π = ((cid:80)T\n\nt=1 ⊗N\n\ni=1\n\n ̄θ(t) i )/T is an ε-CE.\n\nRationalizability. By Lemma D.2, if a ∈ EL ∩ Ai, θ(t) i (a) = 0, i.e., the action would not be the support in the output strategy Π = ((cid:80) ̄θ(t) Sample complexity. The total number of queries is\n\ni (a) ≤ p for all t ∈ [T ]. It follows that\n\nt ⊗i\n\n ̄θ(t) i )/T .\n\n(cid:88)\n\nT (cid:88)\n\ni∈[N ]\n\nt=1\n\nAM (t)\n\ni ≤ N AT +\n\n(cid:88)\n\n(cid:88)\n\nT (cid:88)\n\nt=1\n\n16θ(t)\n\ni (b) τ =1 θ(τ )\n\ni\n\n∆2 · (cid:80)t\n\n(b)\n\n≤ N AT +\n\n· ln(T /p)\n\nb∈Ai\n\ni∈[N ] 16N A2 ∆2\n\n(cid:18) N A2\n\nε2 +\n\n(cid:19)\n\n,\n\nN A2 ∆2\n\nwhere we used the fact that\n\n≤ (cid:101)O\n\nT (cid:88)\n\nt=1\n\nθ(t) i (a) i=1 θ(τ )\n\ni\n\n(cid:80)τ\n\n≤ 1 + ln\n\n(a)\n\n(cid:19)\n\n.\n\n(cid:18) T p\n\nFinally consider the cost of finding one IDE-surviving action profile ( (cid:101)O (cid:0) LN A claimed rate.\n\n∆2\n\n(cid:1)) and we get the\n\nE DETAILS FOR REDUCTION ALGORITHMS\n\nIn this section, we present the details for the reduction based algorithm for finding rationalizable CE (Algorithm 5) and analysis of both Algorithm 4 and 5.\n\nE.1 RATIONALIZABLE CCE VIA REDUCTION\n\nWe will choose ε′ = min{ε,∆}\n\n3\n\n, M =\n\n(cid:108) 4 ln(2N A/δ) ε′2\n\n(cid:109) .\n\nLemma E.1. With probability 1 − δ, throughout the execution of Algorithm 4, for every t and i ∈ [N ], a′\n\ni ∈ Ai,\n\n|ˆui(a′\n\ni, Π−i) − ui (a′\n\ni, Π−i)| ≤ ε′.\n\nProof. First, observe that during every iterate of t before the algorithm returns, the total support size (cid:80)t\n\n| is increased by at least 1. It follows that the algorithm returns before t = N A.\n\ni=1 |A(t)\n\ni\n\nBy Hoeffding’s inequality,\n\nPr [|ˆui(a′\n\ni, Π−i) − ui (a′\n\ni, Π−i)| > ε′] ≤ 2 exp\n\n(cid:19)\n\n(cid:18)\n\n−\n\nnε′2 2\n\n≤\n\nδ\n\nN 2A2 .\n\nApplying union bound over t, i and a′\n\ni proves the statement.\n\nProof of Theorem 13. Correctness. Since Π is an ε-CCE in the subgame ΠN ∀a ∈ A(t)\n\ni\n\ni=1A(t)\n\ni\n\n, ∀i ∈ [N ],\n\nBecause arg maxa∈Ai ˆui(a, Π−i) ∈ A(t)\n\ni\n\n, ∀i ∈ [N ], ∀a ∈ Ai\n\nui (a, Π−i) ≤ ui (Π) + ε′.\n\nui (a, Π−i) ≤ ˆui (a, Π−i) + ε′ ≤ max a′∈A(t) ui (a′, Π−i) + 2ε′\n\ni\n\n≤ max a′∈A(t)\n\ni\n\nˆui (a′, Π−i) + ε′\n\n≤ ui(Π) + 3ε′ ≤ ui(Π) + ε.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTherefore Π is an ε-CCE in the full game.\n\nMoreover, we claim that for any t, A(t) i only contains ∆-rationalizable actions. This is true for t = 1 with high probability due to our initialization. Suppose that this is true for t. Notice that the only way for an action a′ is to be an empirical best response, which means i, Π−i) − ε′ ≥ max i, Π−i) ≥ ˆui(a′\n\nˆui(a, Π−i) − ε′\n\ni ∈ A(t+1)\n\nui(a′\n\ni\n\na∈Ai\n\nSince ε′ < ∆/2, this means that a′ therefore ∆-rationalizable. Therefore A(t+1) can be thus proven via induction, and it follows that the output strategy is also ∆-rationalizable.\n\n≥ max a∈Ai i is the ∆-best response to a ∆-rationalizable strategy, and is also only contains ∆-rationalizable actions. Our claim\n\nui(a, Π−i) − 2ε′.\n\ni\n\nWe conclude that the output strategy is a ∆-rationalizable ε-CCE with probability 1 − 2δ (assuming the event in Lemma E.1 as well as the rationalizability of the initialization). (cid:1) samples. Since the algorithm returns Sample complexity. By Theorem 3, Line 1 needs (cid:101)O (cid:0) LN A before t = N A, the total number of calls to the black-box oracle O is N A. For each t, the number of samples required is\n\n∆2\n\nN AM = (cid:101)O\n\n(cid:18)\n\nN A min{∆, ε}2\n\n(cid:19)\n\n.\n\nCombining this with the upper bound on t, and the cost for Algorithm 1 gives the total sample complexity bound\n\n(cid:18) N 2A2\n\nmin{∆2, ε2}\n\n(cid:19)\n\n.\n\n(cid:101)O\n\nE.2 RATIONALIZABLE CE VIA REDUCTION\n\nThe algorithm for CE is quite similar to the one for CCE, except now when testing whether a subgame ε-CE is an actual ε-CE, we need to use the conditional distribution Π|ai, which is the conditional distribution of the other players’ actions given that player i is told to play ai. The detailed description is given in Algorithm 5. Similar to the CCE case, we will choose ε′ = min{ε,∆} ,\n\n3\n\nM =\n\n(cid:24) 4 ln(2N A2/δ)\n\n(cid:25) .\n\nε′2\n\nAlgorithm 5 Rationalizable ε-CE via Black-box Reduction\n\n1, · · · , a⋆\n\nN ) ← Algorithm 1\n\n1: (a⋆ 2: For all i ∈ [N ], initialize A(1) 3: for t = 1, 2, . . . do\n\ni ← {a⋆\n\ni } for all i ∈ [N ]\n\n4: 5:\n\n6:\n\n7: 8:\n\n9:\n\n10: 11:\n\nFind an ε′-CE, Π, in the sub-game supported on Πi∈[N ]A(t) ∀i ∈ [N ], ai, a′\n\ni ∈ Ai, sample ui(a′\n\ni, Π−i|ai) for M times and compute average ˆui(a′\n\ni\n\ni, Π−i|ai)\n\nfor i ∈ [N ] do for ai ∈ A(t)\n\ni do\n\nLet\n\na′\n\ni ← arg max\n\na∈Ai\n\nˆui(a, Π|ai)// Computing the empirical best response\n\ni ← A(t)\n\nA(t+1) i = A(t+1)\n\ni ∪ {a′ i} for all i ∈ [N ] then\n\nif A(t)\n\ni return Π\n\nLemma E.2. With probability 1 − δ, throughout the execution of Algorithm 4, for every t and i ∈ [N ], a′\n\ni ∈ Ai, ai ∈ Ai,\n\n|ˆui(a′\n\ni, Π|ai) − ui (a′\n\ni, Π|ai)| ≤ ε′.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nProof. First, observe that during every iterate of t before the algorithm returns, the total support size (cid:80)t\n\n| is increased by at least 1. It follows that the algorithm returns before t = N A.\n\ni=1 |A(t)\n\ni\n\nBy Hoeffding’s inequality,\n\nPr [|ˆui(a′\n\ni, Π|ai) − ui (a′\n\ni, Π|ai)| > ε′] ≤ 2 exp\n\n(cid:19)\n\n(cid:18)\n\n−\n\nnε′2 2\n\n≤\n\nδ\n\nN 2A3 .\n\nApplying union bound over t, i, ai, and a′\n\ni proves the statement.\n\nProof. Note that with high probability, the empirical estimates ˆU are at most ε/4 away from the true value U . Since a′\n\ni is the empirical best response, we have\n\nUi(a′\n\ni, Π|ai) ≥ arg max\n\nUi(a, Π|ai) − ε.\n\na∈Ai\n\nNote that Π|ai is supported on actions that can survive any rounds of ε-IDE. Therefore it serves as a certificate that a′\n\ni will never be ε-eliminated as well.\n\nLemma E.3. The returned strategy Π is an ε-CE with probability 1 − δ.\n\nProof. When the algorithm terminates, for all i ∈ [N ],\n\n(cid:32)\n\n(cid:88)\n\nΠi(ai) ·\n\nai∈A(t)\n\ni\n\nmax a∈Ai\n\nˆui(a, Π|ai) − max a∈A(t)\n\ni\n\nˆui(a, Π|ai)\n\n= 0.\n\n(cid:33)\n\nTherefore\n\n(cid:88)\n\nΠi(ai) ·\n\nai∈A(t)\n\ni\n\n(cid:32)\n\nmax a∈Ai\n\nui(a, Π|ai) − max a∈A(t)\n\ni\n\n(cid:33)\n\nui(a, Π|ai)\n\n≤ 2ε′.\n\nSince Π is an ε′-CE in the reduced game,\n\n(cid:88)\n\nΠi(ai) ·\n\nai∈A(t)\n\ni\n\n(cid:32)\n\nmax a∈A(t)\n\ni\n\nui(a, Π|ai) − ui(ai, Π|ai)\n\n≤ ε′.\n\n(cid:33)\n\nSumming the two inequalities above gives\n\n(cid:88)\n\nΠi(ai) ·\n\nai∈A(t)\n\ni\n\n(cid:18)\n\nmax a∈Ai\n\nui(a, Π|ai) − ui(ai, Π|ai)\n\n(cid:19)\n\n≤ 3ε′,\n\nwhich proves the statement.\n\nLemma E.4. For any t, A(t)\n\ni only contains ∆-rationalizable actions with probability 1 − 2δ.\n\nProof. We prove this inductively. This is true for t = 1 with probability 1 − δ due to our initialization. Suppose that this is true for t. Notice that the only way for an action a′ is to be an empirical best response, which means for some ai\n\ni ∈ A(t+1)\n\ni\n\nui(a′\n\ni, Π|ai) ≥ ˆui(a′\n\ni, Π|ai) − ε′ ≥ max\n\na∈Ai\n\nˆui(a, Π|ai) − ε′\n\n≥ max a∈Ai\n\nui(a, Π|ai) − 2ε′.\n\nSince ε′ < ∆/2, this means that a′ therefore ∆-rationalizable. Therefore A(t+1) can be thus proven via induction, and it follows that the output strategy is also ∆-rationalizable.\n\ni is the ∆-best response to a ∆-rationalizable strategy, and is also only contains ∆-rationalizable actions. Our claim\n\ni\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nProof of Theorem 14. Correctness. By Lemma E.3 and E.4, the output strategy is a ∆-rationalizable ε-CE with probability 1 − 2δ (assuming that the event in Lemma E.2 holds and the rationalizability of the initialization).\n\nSample complexity. The total sample complexity is\n\n(cid:19)\n\n(cid:18) LN A ∆2\n\n(cid:101)O\n\n+ N A × N A2M = (cid:101)O\n\n(cid:18) N 2A3\n\nmin{∆, ε}2\n\n(cid:19)\n\n.\n\n25",
    "reference": "# Summary Of The Paper\n\nThe paper presents algorithms for computing approximately rationalizable correlated equilibria (CE) and coarse correlated equilibria (CCE) in general games. Although these bounds are not shown to be optimal (except of the case where strategies are rationalizable after a constant number of iterated elimination rounds) they are significantly improved over the recent work of Wu et al. Finally the paper provides reduction schemes that find approximately-rationalizable epsilon-CCE/CE using black-box algorithms for epsilon-CE/CCE.\n\n# Strength And Weaknesses\n\nStrengths\n1) The paper makes recent progress on a paper about learning in games\n2) The writing is relatively clear\n\nWeaknesses\n1) The motivation behind finding rationalizable CCE does not seem to be particularly strong. Why is so important for the players to converge to a rationalizable CE/CCE? The proposed algorithms are rather unnatural and although can be used to compute CCE do not provide no-regret guarantees themselves. Hence, one would need a very strong justification of why this specific class of equilibria are particularly desirable. Also, if \"natural\" learning algorithms like Hedge and variants do not reproduce iteratively rationalizable CCE fast isn't this an indication that maybe iterative rationalizability is not a \"natural\" property?\n2) The algorithms are rather impractical to use and result in significant blow-ups e.g. the sampling complexity grows linearly in the number of agents whereas without it there is no dependence on the number of agents.\n3) There are no proposed interesting practical applications of this idea (also no experimental section)\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI believe the paper is relatively well written. \nThe question that is examined here was introduced recent in the work of Wu et al. This work is effectively a direct follow-up.\n\n# Summary Of The Review\n\nA theoretical paper that makes progress on the subject of rationalizability in equilibria. The motivation/applications are somewhat lacking.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nETSFORMER: EXPONENTIAL SMOOTHING TRANSFORMERS FOR TIME-SERIES FORECASTING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTransformers have recently been actively studied for time-series forecasting. While often showing promising results in various scenarios, traditional Transformers are not designed to fully exploit the characteristics of time-series data and thus suffer some fundamental limitations, e.g., they are generally not decomposable or interpretable, and are neither effective nor efficient for long-term forecasting. In this paper, we propose ETSformer, a novel time-series Transformer architecture, which exploits the principle of exponential smoothing methods in improving Transformers for time-series forecasting. Specifically, ETSformer leverages a novel level-growth-seasonality decomposed Transformer architecture which leads to more interpretable and disentangled decomposed forecasts. We further propose two novel attention mechanisms – the exponential smoothing attention and frequency attention, which are specially designed to overcome the limitations of the vanilla attention mechanism for time-series data. Extensive experiments on the long sequence time-series forecasting (LSTF) benchmark validates the efficacy and advantages of the proposed method. Code is attached in the supplementary material, and will be made publicly available.\n\n1\n\nINTRODUCTION\n\nTransformer models have achieved great success in the fields of natural language processing (Vaswani et al., 2017; Devlin et al., 2019), computer vision (Carion et al., 2020; Dosovitskiy et al., 2021), and even more recently, time-series (Li et al., 2019; Wu et al., 2021; Zhou et al., 2021; Zerveas et al., 2021; Zhou et al., 2022). While the success of Transformer models have been widely attributed to the self-attention mechanism, alternative forms of attention, infused with the appropriate inductive biases, have been introduced to tackle the unique properties of their underlying task or data (You et al., 2020; Raganato et al., 2020). In time-series forecasting, decomposition-based architectures such as Autoformer and FEDformer models (Wu et al., 2021; Zhou et al., 2022) have incorporated time-series specific inductive biases, leading to increased accuracy, and more interpretable forecasts (by decomposing forecasts into seasonal and trend components). Their success has been motivated by: (i) disentangling seasonal and trend representations via seasonal-trend decomposition (Cleveland & Tiao, 1976; Cleveland et al., 1990; Woo et al., 2022), and (ii) replacing the vanilla pointwise dot-product attention which handle time-series patterns such as seasonality and trend inefficiently, with time-series specific attention mechanisms such as the Auto-Correlation mechanism and Frequency-Enhanced Attention. While these existing work introduce the promising direction of interpretable and decomposed time-series forecasting for Transformer-based architectures, they suffer from two drawbacks.\n\nFirstly, they suffer from entangled seasonal-trend representations, evidenced in Figure 1, where the trend forecasts exhibit periodical patterns which should only be present in the seasonal component, and the seasonal component does not accurately track the (multiple) periodicities present in the ground truth seasonal component. This arises due to their decomposition mechanism which detects trend via a simple moving average over the input signal and detrends the signal by removing the detected trend component – an arguably naive approach. This method has many known pitfalls (Hyndman & Athanasopoulos, 2018), such as the trend-cycle component not being available for the first and last few observations, and over-smoothing rapid rises and falls.\n\nSecondly, their proposed replacements for the vanilla attention mechanism are not human interpretable – demonstrated in Section 3.3. Model inspection and diagnosis allows us to better understand the fore-\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Seasonal-trend decomposed forecasts on synthetic data with ground truth seasonal and trend components. Top row: combined forecast. Middle row: trend component forecast. Bottom row: season component forecast. ETSformer is compared to two competing decomposed Transformer baselines, Autoformer, and FEDformer. Seen in the visualization, ETSformer exhibits a more disentangled seasonal-trend decomposition which accurately tracks the ground truth components. Not visualized here is ETSformer’s unique ability to further separate trend into level and growth components.\n\ncasts generated by our models, attributing predictions to each component to make better downstream decisions. For an attention mechanism focusing on seasonality, we would expect the cross-attention map visualization to produce clear periodic patterns which shift smoothly across decoder time steps. Yet, the Auto-Correlation mechanism from Autoformer does not exhibit this property, yielding similar attention weights across decoder time steps, while the Frequency-Enhanced Attention from FEDformer does not have such model interpretability capabilities due to its complicated frequency domain attention.\n\nTo address these limitations, we look towards the more principled approach of level-growth-season decomposition from ETS methods (Hyndman et al., 2008) (further introduced in Appendix A). This principle further deconstructs trend into level and growth components. To extract the level and growth components, we also look at the idea of exponential smoothing, where more recent data gets weighted more highly than older data, reflecting the view that the more recent past should be considered more relevant for making new predictions or identifying current trends, to replace the naive moving average. At the same time, we leverage the idea of extracting the most salient periodic components in the frequency domain via the Fourier transform, to extract the global seasonal patterns present in the signal. These principles help yield a stronger decomposition strategy by first extracting global periodic patterns as seasonality, and subsequently extracting growth as the change in level in an exponentially smoothed manner.\n\nMotivated by the above, we propose ETSformer, an interpretable and efficient Transformer architecture for time-series forecasting which yields disentangled seasonal-trend forecasts. Instead of reusing the moving average operation for detrending, ETSformer overhauls the existing decomposition architecture by leveraging the level-growth-season principle, embedding it into a novel Transformer framework in a non-trivial manner. Next, we introduce interpretable and efficient attention mechanisms – Exponential Smoothing Attention (ESA) for trend, and Frequency Attention (FA) for seasonality. ESA assigns attention weights in an exponentially decreasing manner, with high values to nearby time steps and low values to far away time steps, thus specialising in extracting growth representations. FA leverages frequency domain representations to extract dominating seasonal patterns by selecting the Fourier bases with the K largest amplitudes. Both mechanisms have efficient implementations with O(L log L) complexity. Furthermore, we demonstrate human interpretable visualizations of both mechanisms in Section 3.3. To summarize, our key contributions are as follows:\n\n• We introduce a novel decomposition Transformer architecture, incorporating the timetested level-growth-season principle for more disentangled, human-interpretable time-series forecasts.\n\n• We introduce two new attention mechanisms, ESA and FA, which incorporate stronger time-series specific inductive biases. They achieve better efficiency than vanilla attention, and yield interpretable attention weights upon model inspection.\n\n• The resulting method is a highly effective, efficient, and interpretable deep forecasting model. We show this via extensive empirical analysis, that ETSformer achieves performance competitive with state-of-the-art methods over 6 real world datasets on both multivariate and univariate settings, and is highly efficient compared to competing methods.\n\n2\n\n6507007508008509000.750.500.250.000.250.500.75ETSformerLookbackGround TruthETSformer (MSE: 0.0029)6507007508008509000.750.500.250.000.250.500.75AutoformerLookbackGround TruthAutoformer (MSE: 0.0109)6507007508008509000.750.500.250.000.250.500.751.00FEDformerLookbackGround TruthFEDformer (MSE: 0.0175)6507007508008509000.80.60.40.20.00.20.4ETSformer TrendLookbackGround Truth TrendETSformer Trend (MSE: 0.0015)6507007508008509000.80.60.40.20.00.20.40.6Autoformer TrendLookbackGround Truth TrendAutoformer Trend (MSE: 0.0137)6507007508008509000.80.60.40.20.00.20.4FEDformer TrendLookbackGround Truth TrendFEDformer Trend (MSE: 0.0296)6507007508008509000.60.40.20.00.2ETSformer SeasonLookbackGround Truth SeasonETSformer Season (MSE: 0.0018)6507007508008509000.60.40.20.00.2Autoformer SeasonLookbackGround Truth SeasonAutoformer Season (MSE: 0.0120)6507007508008509000.80.60.40.20.00.20.40.6FEDformer SeasonLookbackGround Truth SeasonFEDformer Season (MSE: 0.0329)Under review as a conference paper at ICLR 2023\n\n2 ETSFORMER\n\nFigure 2: ETSformer model architecture.\n\nProblem Formulation Let xt ∈ Rm denote an observation of a multivariate time-series at time step t. Given a lookback window Xt−L:t = [xt−L, . . . , xt−1], we consider the task of predicting future values over a horizon, Xt:t+H = [xt, . . . , xt+H−1]. We denote ˆXt:t+H as the point forecast of Xt:t+H . Thus, the goal is to learn a forecasting function ˆXt:t+H = f (Xt−L:t) by minimizing some loss function L : RH×m × RH×m → R.\n\nIn the following, we explain how ETSformer infuses level-growth-seasonal decomposition the the classical encoder-decoder Transformer architecture, specializing for interpretable time-series forecasting. Our architecture design methodology relies on three key principles: (1) the architecture leverages the stacking of multiple layers to progressively extract a series of level, growth, and seasonal representations from the intermediate latent residual; (2) performs level-growth-seasonal decomposition of latent representations, by extracting salient seasonal patterns while modeling level and growth components following an exponential smoothing formulation; (3) the final forecast is a composition of level, growth, and seasonal components making it human interpretable.\n\n2.1 OVERALL ARCHITECTURE\n\nFigure 2 illustrates the overall encoder-decoder architecture of ETSformer. At each layer, the encoder is designed to iteratively extract growth and seasonal latent components from the lookback window. The level is then extracted in a similar fashion to classical level smoothing in Equation (3). These extracted components are then fed to the decoder to further generate the final H-step ahead forecast via a composition of level, growth, and seasonal forecasts, which is defined:\n\nˆXt:t+H = Et:t+H + Linear\n\n(cid:16) N (cid:88)\n\n(B(n)\n\nt:t+H + S(n)\n\nt:t+H )\n\n(cid:17) ,\n\n(1)\n\nn=1\n\nwhere Et:t+H ∈ RH×m, and B(n) t:t+H ∈ RH×d represent the level forecasts, and the growth and seasonal latent representations of each time step in the forecast horizon, respectively. The superscript represents the stack index, for a total of N encoder stacks. Note that Linear(·) : Rd → Rm operates element-wise along each time step, projecting the extracted growth and seasonal representations from latent to observation space.\n\nt:t+H , S(n)\n\n2.1.1\n\nINPUT EMBEDDING\n\nRaw signals from the lookback window are mapped to latent space via the input embedding module, defined by Z(0) t−L:t = Conv(Xt−L:t), where Conv is a temporal convolutional filter with kernel size 3, input channel m and output channel d. In contrast to prior work (Li et al., 2019; Wu et al., 2020; 2021; Zhou et al., 2021), the inputs of ETSformer do not rely on any other manually\n\nt−L:t = E(0)\n\n3\n\nInputEmbeddingLookback Window: X!\"#:!GrowthDampingFrequencyAttention+B!%S!\"#:!%B!:!&’%+S!:!&’%Forecast Horizon: XE!:!&’Layer 2Layer 1E!\"EncoderG+S Stack 2G+S Stack NG+S Stack 1+Level Stack+LinearDecoderLayer NLinearConcatDifferenceLinearExponential Smoothing AttentionZ!\"#:!%\"(B!\"#:!%(Growth)DFTTop-K AmplitudeiDFTZ!\"#:!%\"(S!\"#:!%(Season)+FeedforwardLayerNormMulti-Head ES AttentionLayerNorm-FrequencyAttention-LevelZ!\"#:!%Z!\"#:!%\"(E!\"#:!%\"(E!\"#:!%(Level)B!\"#:!%S!\"#:!%Under review as a conference paper at ICLR 2023\n\ndesigned dynamic time-dependent covariates (e.g. month-of-year, day-of-week) for both the lookback window and forecast horizon. This is because the proposed Frequency Attention module (details in Section 2.2.2) is able to automatically uncover these seasonal patterns, which renders it more applicable for challenging scenarios without these discriminative covariates and reduces the need for feature engineering.\n\n2.1.2 ENCODER\n\nThe encoder focuses on extracting a series of latent growth and seasonality representations in a cascaded manner from the lookback window. To achieve this goal, traditional methods rely on the assumption of additive or multiplicative seasonality which has limited capability to express complex patterns beyond these assumptions. Inspired by (Oreshkin et al., 2019; He et al., 2016), we leverage residual learning to build an expressive, deep architecture to characterize the complex intrinsic patterns. Each layer can be interpreted as sequentially analyzing the input signals. The extracted growth and seasonal signals are then removed from the residual and undergo a nonlinear transformation before moving to the next layer. Each encoder layer takes as input the residual from the previous encoder layer Z(n−1) t−L:t, the residual, latent growth, and seasonal representations for the lookback window via the Multi-Head Exponential Smoothing Attention (MH-ESA) and Frequency Attention (FA) modules (detailed description in Section 2.2). The following equations formalizes the overall pipeline in each encoder layer, and for ease of exposition, we use the notation := for a variable update.\n\nt−L:t and emits Z(n)\n\nt−L:t, B(n)\n\nt−L:t, S(n)\n\nSeasonal: S(n)\n\nt−L:t = FAt−L:t(Z(n−1) t−L:t ) t−L:t − S(n) t−L:t := Z(n−1)\n\nZ(n−1)\n\nt−L:t\n\nGrowth: B(n)\n\nZ(n−1)\n\nt−L:t = MH-ESA(Z(n−1) t−L:t ) t−L:t − B(n) t−L:t := LN(Z(n−1) t−L:t + FF(Z(n−1) t−L:t = LN(Z(n−1) Z(n)\n\nt−L:t)\n\nt−L:t ))\n\nLN is layer normalization (Ba et al., 2016), FF(x) = Linear(σ(Linear(x))) is a position-wise feedforward network (Vaswani et al., 2017) and σ(·) is the sigmoid function.\n\nLevel Module Given the latent growth and seasonal representations from each layer, we extract the level at each time step t in the lookback window in a similar way as the level smoothing equation in Equation (3). Formally, the adjusted level is a weighted average of the current (de-seasonalized) level and the level-growth forecast from the previous time step t − 1. It can be formulated as:\n\nE(n)\n\nt = α ∗\n\n(cid:16)\n\nE(n−1)\n\nt\n\n− Linear(S(n)\n\nt\n\n(cid:17)\n\n)\n\n+ (1 − α) ∗\n\n(cid:16)\n\nE(n)\n\nt−1 + Linear(B(n)\n\nt−1)\n\n(cid:17)\n\n,\n\nwhere α ∈ Rm is a learnable smoothing parameter, ∗ is an element-wise multiplication term, and Linear(·) : Rd → Rm maps representations to observation space. Finally, the extracted level in the last layer E(N ) t−L:t can be regarded as the corresponding level for the lookback window. We show in Appendix B.3 that this recurrent exponential smoothing equation can also be efficiently evaluated using the efficient AES algorithm (Algorithm 1) with an auxiliary term.\n\n2.1.3 DECODER\n\nThe decoder is tasked with generating the H-step ahead forecasts. As shown in Equation (1), the final forecast is a composition of level forecasts Et:t+H , growth representations B(n) t:t+H and seasonal representations S(n) t:t+H in the forecast horizon. It comprises N Growth + Seasonal (G+S) Stacks, and a Level Stack. The G+S Stack consists of the Growth Damping (GD) and FA blocks, which leverage B(n)\n\nt−L:t to predict B(n)\n\nt:t+H , respectively.\n\nt:t+H , S(n)\n\n, S(n)\n\nt\n\nGrowth: B(n)\n\nt:t+H = GD(B(n)\n\nt\n\n)\n\nSeasonal: S(n)\n\nt:t+H = FAt:t+H (S(n)\n\nt−L:t)\n\nTo obtain the level in the forecast horizon, the Level Stack repeats the level in the last time step t along the forecast horizon. It can be defined as Et:t+H = RepeatH (E(N ) , . . . , E(N ) ], with RepeatH (·) : R1×m → RH×m.\n\n) = [E(N )\n\nt\n\nt\n\nt\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Full Attention (2017)\n\n(b) Sparse Attention (2020; 2021)\n\n(c) Log-sparse Attention (2019)\n\n(d) Auto-Correlation Mechanism (2021)\n\n(e) Exponential Smoothing Attention (Ours)\n\n(f) Frequency Attention (Ours)\n\nFigure 3: Comparison between different attention mechanisms. (a) Full, (b) Sparse, and (c) Log-sparse Attentions are adaptive mechanisms, where the green circles represent the attention weights adaptively calculated by a point-wise dot-product query, and depends on various factors including the time-series value, additional covariates (e.g. positional encodings, time features, etc.). (d) Auto-Correlation mechanism considers sliding dot-product queries to construct attention weights for each rolled input series. We introduce (e) Exponential Smoothing Attention (ESA) and (f) Frequency Attention (FA). ESA directly computes attention weights based on the relative time lag, without considering the input content, while FA attends to patterns which dominate with large magnitudes in the frequency domain.\n\nGrowth Damping To obtain the growth representation in the forecast horizon, we follow the idea of trend damping in Equation (4) to make robust multi-step forecast. Thus, the trend representations can be formulated as:\n\nGD(B(n)\n\nt\n\n)j =\n\nγiB(n)\n\nt\n\n,\n\nj (cid:88)\n\ni=1\n\nGD(B(n)\n\nt−L:t) = [GD(B(n)\n\nt\n\n)t, . . . , GD(B(n)\n\nt\n\n)t+H−1],\n\nwhere 0 < γ < 1 is the damping parameter which is learnable, and in practice, we apply a multi-head version of trend damping by making use of nh damping parameters. Similar to the implementation for level forecast in the Level Stack, we only use the last trend representation in the lookback window B(n)\n\nto forecast the trend representation in the forecast horizon.\n\nt\n\n2.2 EXPONENTIAL SMOOTHING ATTENTION AND FREQUENCY ATTENTION MECHANISM\n\nConsidering the ineffectiveness of existing attention mechanisms in handling time-series data, we develop the Exponential Smoothing Attention (ESA) and Frequency Attention (FA) mechanisms to extract latent growth and seasonal representations. ESA is a non-adaptive, learnable attention scheme with an inductive bias to attend more strongly to recent observations by following an exponential decay, while FA is a non-learnable attention scheme, that leverages Fourier transformation to select dominating seasonal patterns. A comparison between existing work and our proposed ESA and FA is illustrated in Figure 3.\n\n2.2.1 EXPONENTIAL SMOOTHING ATTENTION\n\nVanilla self-attention can be regarded as a weighted combination of an input sequence, where the weights are normalized alignment scores measuring the similarity between input contents (Tsai et al., 2019). Inspired by the exponential smoothing in Equation (3), we aim to assign a higher weight to recent observations. It can be regarded as a novel form of attention whose weights are computed by the relative time lag, rather than input content. Thus, the ESA mechanism can be defined as AES : RL×d → RL×d, where AES(V )t ∈ Rd denotes the t-th row of the output matrix, representing the token corresponding to the t-th time step. Its exponential smoothing formula can be further written as:\n\nAES(V )t = αVt + (1 − α)AES(V )t−1 =\n\nt−1 (cid:88)\n\nj=0\n\nα(1 − α)jVt−j + (1 − α)tv0,\n\nwhere 0 < α < 1 and v0 are learnable parameters known as the smoothing parameter and initial state respectively.\n\n5\n\nTimeTimeTimeTimePeriod 1Period 2TimeTimeHigh Amplitude FrequenciesPeriod 1Period 2ExtrapolatedPatternUnder review as a conference paper at ICLR 2023\n\nEfficient AES algorithm The straightforward implementation of the ESA mechanism by constructing the attention matrix, AES and performing a matrix multiplication with the input sequence (detailed algorithm in Appendix B.4) results in an O(L2) computational complexity.\n\nAES(V ) =\n\n\n\n \n\nAES(V )1 ... AES(V )L\n\n\n\n  = AES ·\n\n(cid:20)vT 0\nV\n\n(cid:21)\n\n,\n\nYet, we are able to achieve an efficient algorithm by exploiting the unique structure of the exponential smoothing attention matrix, AES, which is illustrated in Appendix B.1. Each row of the attention matrix can be regarded as iteratively right shifting with padding (ignoring the first column). Thus, a matrix-vector multiplication can be computed with a cross-correlation operation, which in turn has an efficient fast Fourier transform implementation (Mathieu et al., 2014). The full algorithm is described in Algorithm 1, Appendix B.2, achieving an O(L log L) complexity.\n\nMulti-Head Exponential Smoothing Attention (MH-ESA) We use AES as a basic building block, and develop the Multi-Head Exponential Smoothing Attention to extract latent growth representations. Formally, we obtain the growth representations by taking the successive difference of the residuals.\n\n ̃Z(n)\n\nt−L:t ),\n\nt−L:t = Linear(Z(n−1) t−L:t = MH-AES( ̃Z(n) t−L:t := Linear(B(n)\n\nt−L:t),\n\nB(n)\n\nB(n)\n\nt−L:t − [ ̃Z(n)\n\nt−L:t−1, v(n)\n\n0\n\n]),\n\nwhere MH-AES is a multi-head version of AES and v(n)\n\n0\n\nis the initial state from the ESA mechanism.\n\n2.2.2 FREQUENCY ATTENTION\n\nThe goal of identifying and extracting seasonal patterns from the lookback window is twofold. Firstly, it can be used to perform de-seasonalization on the input signals such that downstream components are able to focus on modeling the level and growth information. Secondly, we are able to extrapolate the seasonal patterns to build representations for the forecast horizon. The main challenge is to automatically identify seasonal patterns. Fortunately, the use of power spectral density estimation for periodicity detection has been well studied (Vlachos et al., 2005). Inspired by these methods, we leverage the discrete Fourier transform (DFT, details in Appendix C) to develop the FA mechanism to extract dominant seasonal patterns.\n\nSpecifically, FA first decomposes input signals into their Fourier bases via a DFT along the temporal dimension, F(Z(n−1) t−L:t ) ∈ CF ×d where F = ⌊L/2⌋ + 1, and selects bases with the K largest amplitudes. An inverse DFT is then applied to obtain the seasonality pattern in time domain. Formally, this is given by the following equations:\n\nΦk,i = φ\n\n(cid:16)\n\nF(Z(n−1)\n\nt−L:t )k,i\n\n(cid:17)\n\n, Ak,i =\n\nκ(1)\n\ni\n\n, . . . , κ(K)\n\ni = arg Top-K\n\nk∈{2,...,F }\n\n(cid:12) (cid:12)\n\n(cid:110)\n\n(cid:12)F(Z(n−1)\n\nt−L:t )k,i (cid:111) ,\n\nAk,i\n\n(cid:12) (cid:12) (cid:12),\n\nS(n)\n\nj,i =\n\nK (cid:88)\n\nk=1\n\n(cid:104)\n\nAκ(k)\n\ni\n\n,i\n\ncos(2πfκ(k)\n\ni\n\nj + Φκ(k)\n\ni\n\n,i) + cos(2π ̄fκ(k)\n\ni\n\nj + ̄Φκ(k)\n\ni\n\n(cid:105) ,i)\n\n,\n\n(2)\n\nwhere Φk,i, Ak,i are the phase/amplitude of the k-th frequency for the i-th dimension, arg Top-K returns the arguments of the top K amplitudes, K is a hyperparameter, fk is the Fourier frequency of the corresponding index, and ̄fk, ̄Φk,i are the Fourier frequency/amplitude of the corresponding conjugates.\n\nt−L:t,i = [S(n)\n\nFinally, the latent seasonal representation of the i-th dimension for the lookback window is formulated as S(n) t−L,i, . . . , S(n) t−1,i]. For the forecast horizon, the FA module extrapolates beyond the lookback window via, S(n) t:t+H,i = [S(n) t+H−1,i]. Since K is a hyperparameter typically chosen for small values, the complexity for the FA mechanism is similarly O(L log L).\n\nt,i , . . . , S(n)\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Multivariate forecasting results over various forecast horizons. Best results are bolded, and second best results are underlined.\n\nMethods\n\nETSformer\n\nFEDformer\n\nAutoformer\n\nInformer\n\nLogTrans\n\nReformer\n\nLSTnet\n\nES-RNN\n\nMetrics MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n2 m\nT T\nE\n\nL C\nE\n\n96 192 336 720\n\n96 192 336 720\n\ne 96 192 336 720\n\ng n\na h\nc x\nE\n\nc fi\n\nf a\nr\n\nT\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\nI\n\nL\n\nI\n\n24 36 48 60\n\n0.189 0.253 0.314 0.414\n\n0.187 0.199 0.212 0.233\n\n0.085 0.182 0.348 1.025\n\n0.607 0.621 0.622 0.632\n\n0.197 0.237 0.298 0.352\n\n2.527 2.615 2.359 2.487\n\n0.280 0.319 0.357 0.413\n\n0.304 0.315 0.329 0.345\n\n0.204 0.303 0.428 0.774\n\n0.392 0.399 0.396 0.396\n\n0.281 0.312 0.353 0.388\n\n1.020 1.007 0.972 1.016\n\n0.203 0.269 0.325 0.421\n\n0.183 0.195 0.212 0.231\n\n0.139 0.256 0.426 1.090\n\n0.562 0.562 0.570 0.596\n\n0.217 0.276 0.339 0.403\n\n2.203 2.272 2.209 2.545\n\n0.287 0.328 0.366 0.415\n\n0.297 0.308 0.313 0.343\n\n0.276 0.369 0.464 0.800\n\n0.349 0.346 0.323 0.368\n\n0.296 0.336 0.380 0.428\n\n0.963 0.976 0.981 1.061\n\n0.255 0.281 0.339 0.422\n\n0.201 0.222 0.231 0.254\n\n0.197 0.300 0.509 1.447\n\n0.613 0.616 0.622 0.660\n\n0.266 0.307 0.359 0.419\n\n3.483 3.103 2.669 2.770\n\n0.339 0.340 0.372 0.419\n\n0.317 0.334 0.338 0.361\n\n0.323 0.369 0.524 0.941\n\n0.388 0.382 0.337 0.408\n\n0.336 0.367 0.359 0.419\n\n1.287 1.148 1.085 1.125\n\n0.365 0.533 1.363 3.379\n\n0.274 0.296 0.300 0.373\n\n0.847 1.204 1.672 2.478\n\n0.719 0.696 0.777 0.864\n\n0.300 0.598 0.578 1.059\n\n5.764 4.755 4.763 5.264\n\n0.453 0.563 0.887 1.388\n\n0.368 0.386 0.394 0.439\n\n0.752 0.895 1.036 1.310\n\n0.391 0.379 0.420 0.472\n\n0.384 0.544 0.523 0.741\n\n1.677 1.467 1.469 1.564\n\n0.768 0.989 1.334 3.048\n\n0.258 0.266 0.280 0.283\n\n0.968 1.040 1.659 1.941\n\n0.684 0.685 0.733 0.717\n\n0.458 0.658 0.797 0.869\n\n4.480 4.799 4.800 5.278\n\n0.642 0.757 0.872 1.328\n\n0.357 0.368 0.380 0.376\n\n0.812 0.851 1.081 1.127\n\n0.384 0.390 0.408 0.396\n\n0.490 0.589 0.652 0.675\n\n1.444 1.467 1.468 1.560\n\n0.658 1.078 1.549 2.631\n\n0.312 0.348 0.350 0.340\n\n1.065 1.188 1.357 1.510\n\n0.732 0.733 0.742 0.755\n\n0.689 0.752 0.639 1.130\n\n4.400 4.783 4.832 4.882\n\n0.619 0.827 0.972 1.242\n\n0.402 0.433 0.433 0.420\n\n0.829 0.906 0.976 1.016\n\n0.423 0.420 0.420 0.423\n\n0.596 0.638 0.596 0.792\n\n1.382 1.448 1.465 1.483\n\n3.142 3.154 3.160 3.171\n\n0.680 0.725 0.828 0.957\n\n1.551 1.477 1.507 2.285\n\n1.107 1.157 1.216 1.481\n\n0.594 0.560 0.597 0.618\n\n6.026 5.340 6.080 5.548\n\n1.365 1.369 1.369 1.368\n\n0.645 0.676 0.727 0.811\n\n1.058 1.028 1.031 1.243\n\n0.685 0.706 0.730 0.805\n\n0.587 0.565 0.587 0.599\n\n1.770 1.668 1.787 1.720\n\n0.204 0.351 0.476 0.623\n\n0.922 0.499 0.760 -\n\n0.096 0.214 0.469 1.997\n\n1.315 0.727 -\n-\n\n0.585 0.381 0.628 0.711\n\n5.393 6.478 7.160 5.801\n\n0.323 0.405 0.485 0.561\n\n0.666 0.479 0.570 -\n\n0.221 0.360 0.537 1.143\n\n0.546 0.373 -\n-\n\n0.507 0.397 0.533 0.545\n\n1.561 1.751 1.963 1.711\n\n3 EXPERIMENTS\n\nThis section presents extensive empirical evaluations on the LSTF task over 6 real world multivariate datasets, ETT, ECL, Exchange, Traffic, Weather, and ILI, coming from a variety of application areas (details in Appendix E). Performance is evaluated via the mean squared error (MSE) and mean absolute error (MAE) metrics. For the main benchmark, datasets are split into train, validation, and test sets chronologically, following a 60/20/20 split for the ETT datasets and 70/10/20 split for other datasets. The multivariate benchmark makes use of all dimensions, while univariate benchmark selects the last dimension of the datasets as the target variable, following previous work (Zhou et al., 2021; Wu et al., 2021). Data is pre-processed by performing standardization based on train set statistics. Further details on implementation and hyperparameters can be found in Appendix D. This is followed by an ablation study of the various contributing components, and interpretability experiments of our proposed model, and finally an analysis on computational efficiency.\n\n3.1 RESULTS For the multivariate benchmark, baselines include recently proposed time-series/efficient Transformers – FEDformer, Autoformer, Informer, LogTrans (Li et al., 2019), and Reformer (Kitaev et al., 2020), and RNN variants – LSTnet (Lai et al., 2018), and ES-RNN (Smyl, 2020). Univariate baselines further include N-BEATS (Oreshkin et al., 2019), DeepAR (Salinas et al., 2020), ARIMA, Prophet (Taylor & Letham, 2018), and AutoETS (Bhatnagar et al., 2021). We obtain baseline results from the following papers: (Wu et al., 2021; Zhou et al., 2021), and further run AutoETS from the Merlion library (Bhatnagar et al., 2021). Table 1 summarize the results of ETSformer against top performing baselines on a selection of datasets, for the multivariate setting, and Table 6 in Appendix G for space. Results for ETSformer are averaged over three runs (standard deviation in Appendix H). Overall, ETSformer achieves competitive performance, achieving best performance on 14 out of 24 datasets/settings on MSE for the multivariate case, and within top 2 performance across all 24 datasets/settings.\n\n3.2 ABLATION STUDY We study the contribution of each major component which the final forecast is composed of level, growth, and seasonality. Table 2 first presents the performance of the full model, and subsequently, the performance of the resulting model by removing each component. We observe that the composition of level, growth, and season provides the most accurate forecasts across a variety of application areas, and removing any one component results in a deterioration. In particular, estimation of the level of the time-series is critical. We also analyse the design of the MH-ESA in Section 3.2, replacing it with a vanilla multi-head attention and an FC layer performing token mixing – we observe that our trend attention formulation indeed is more effective.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Ablation study on the various components (Level, Growth, Season) of ETSformer, averaged over multiple horizons {24, 96, 192, 336, 720} for ETTm2, ECL, and Traffic, {24, 36, 48, 60} for ILI.\n\nDatasets\n\nETTm2\n\nECL\n\nTraffic\n\nILI\n\nETSformer\n\nw/o Level\n\nw/o Season\n\nw/o Growth\n\nMSE MAE\n\nMSE MAE\n\nMSE MAE\n\nMSE MAE\n\n0.256 0.318\n\n2.426 1.146\n\n0.302 0.348\n\n0.261 0.321\n\n0.199 0.316\n\n0.306 0.396\n\n0.819 0.745\n\n0.202 0.317\n\n0.611 0.391\n\n0.683 0.412\n\n1.393 0.792\n\n0.619 0.397\n\n2.570 1.029\n\n4.994 1.628\n\n4.110 1.437\n\n2.642 1.101\n\nTable 3: Ablation study on the effectiveness of the MH-ESA design.\n\nDatasets\n\nETTm2\n\nECL\n\nTraffic\n\nILI\n\nETSformer\n\nMH-ESA → MHA\n\nMH-ESA → FC\n\nMSE MAE\n\nMSE MAE\n\nMSE MAE\n\n0.256 0.318\n\n0.548 0.570\n\n0.342 0.394\n\n0.199 0.316\n\n0.239 0.262\n\n0.235 0.348\n\n0.611 0.391\n\n0.632 0.591\n\n0.626 0.395\n\n2.570 1.029\n\n3.408 2.485\n\n2.779 1.062\n\nTable 4: MSE of decomposed forecasts over the synthetic dataset’s test set (1000 samples).\n\nCombined Trend\n\nSeason\n\nETSformer Autoformer FEDformer\n\n0.009 0.037 0.012\n\n0.003 0.046 0.201\n\n0.005 0.008 0.198\n\n(a) FA weights\n\n(b) ESA weights\n\n(c) Learned dependencies\n\nFigure 4: ETSformer attention weights visualization and learned seasonal dependencies on the ECL dataset. For weights visualizations, each row represents the attention weights a time step in the forecast horizon places on each time step in the lookback window. FA learns a clear periodicity, which is highlighted in the learned dependencies, where the top 6 time steps being attended to by the query time step are highlighted in red. ESA displays exponentially decaying weights representing growth.\n\nFigure 5: Autoformer Auto-Correlation mechanism weights on ECL dataset.\n\nINTERPRETABILITY\n\n3.3 ETSformer generates interpretable forecasts which can be decomposed into disentangled level, growth, and seasonal components. We showcased this ability compared to baselines in Figure 1 on synthetic data containing (nonlinear) trend and seasonality patterns (details in Appendix F) , since we are not able to obtain ground truth decomposition from real-world data. Forecast decompositions (without component ground truth) can be found in Appendix J. Furthermore, we report quantitative results over the test set in Table 4. ETSformer successfully forecasts interpretable level, trend (level + growth), and seasonal components, as observed in the trend and seasonality components closely tracking the ground truth patterns. Despite obtaining a good combined forecast, competing decomposition based approaches, struggles to disambiguate between trend and seasonality.\n\nFurthermore, ETSformer produces human interpretable attention weights for both the FA and ESA mechanisms, visualized in Figure 4. The FA weights visualized exhibit clear periodicity which can be used to identify the dominating seasonal patterns, while ESA weights exhibit exponentially decaying property as per the inductive biases. This is contrasted to Autoformer’s Auto-Correlation visualization in Figure 5 which does not follow periodicity properties despite being specialized to handle seasonality.\n\n3.4 COMPUTATIONAL EFFICIENCY Figure 6 charts ETSformer’s empirical efficiency with that of competing Transformer-based approaches. ETSformer maintains competitive efficiency with competing quasilinear and linear complexity Transformers. This is especially so when forecast horizon increasese, due to ETSformer’s unique decoder architecture which relies on its Trend Damping and Frequency Attention modules rather than relying on a cross attention mechanism. Of note, while FEDformer claims linear complexity, our empirical results show that it incurs significant overhead especially in terms of runtime efficiency. This slowdown arises from their (official) implementation still relying on the straightfor-\n\n8\n\n020406080020406080020406080100120Ground TruthETSformerattendquery0204060Under review as a conference paper at ICLR 2023\n\n(a) Runtime Efficiency Analysis\n\n(b) Memory Efficiency Analysis\n\nFigure 6: Computational Efficiency Analysis. Values reported are based on the training phase of ETTm2 multivariate setting. Horizon is fixed to 48 for lookback window plots, and lookback is fixed to 48 for forecast horizon plots. For runtime efficiency, values refer to the time for one iteration. The “ ” marker indicates an out-of-memory error for those settings.\n\nward FFT operation, incurring O(L log L) complexity, as well as their Frequency Enhanced Modules requiring a large number of trainable parameters.\n\n4 RELATED WORK\n\nDeep Forecasting LogTrans (Li et al., 2019) and AST (Wu et al., 2020) first introduced Transformer based methods to reduce computational complexity of attention. The LSTF benchmark was first introduced by Informer (Zhou et al., 2021), extending the Transformer architecture by proposing the ProbSparse attention and distillation operation to achieve O(L log L) complexity. Similar to our work that incorporates prior knowledge of time-series structure, Autoformer (Wu et al., 2021) introduces the Auto-Correlation attention mechanism which focuses on sub-series based similarity and is able to extract periodic patterns. FEDformer (Zhou et al., 2022) extends this line of work by incorporating Frequency Enhanced structures. N-HiTS (Challu et al., 2022) introduced hierarchical interpolation and multi-rate data sampling by building on top of N-BEATS (Oreshkin et al., 2019) for the LSTF task. ES-RNN (Smyl, 2020) has explored combining ETS methods with neural networks. However, they treat ETS as a pre and post processing step, rather than baking it into the model architecture. Furthermore, their method requires prior knowledge on seasonality patterns, and they were not proposed for LSTF, leading to high computation costs over long horizons.\n\nAttention Mechanisms The self-attention mechanism in Transformer models has recently received much attention, its necessity has been greatly investigated in attempts to introduce more flexibility and reduce computational cost. Synthesizer (Tay et al., 2021) empirically studies the importance of dot-product interactions, and show that a randomly initialized, learnable attention mechanisms with or without token-token dependencies can achieve competitive performance with vanilla self-attention on various NLP tasks. You et al. (2020) utilizes an unparameterized Gaussian distribution to replace the original attention scores, concluding that the attention distribution should focus on a certain local window and can achieve comparable performance. Raganato et al. (2020) replaces attention with fixed, non-learnable positional patterns, obtaining competitive performance on NMT tasks. Lee-Thorp et al. (2021) replaces self-attention with a non-learnable Fourier Transform and verifies it to be an effective mixing mechanism.\n\n5 DISCUSSION\n\nInspired by the classical exponential smoothing methods and emerging Transformer approaches for time-series forecasting, we propose ETSformer, a novel level-growth-season decomposition Transfomer. ETSformer leverages the novel Exponential Smoothing Attention and Frequency Attention mechanisms which are more effective at modeling time-series than vanilla self-attention, and at the same time achieves O(L log L) complexity, where L is the length of lookback window. We performed extensive empirical evaluation, showing that ETSformer has extremely competitive accuracy and efficiency, while being highly interpretable.\n\nLimitations & Future Work ETSformer currently only produces point forecasts. Probabilistic forecasting would be a valuable extension of our current work due to it’s importance in practical applications. Other future directions which ETSformer does not currently consider but would be useful are additional covariates such as holiday indicators and other dummy variables to consider holiday effects which cannot be captured by the FA mechanism.\n\n9\n\n4896168336720144028805670Lookback Window0.00.51.01.52.0Time (s)ETSformer (K=1)ETSformer (K=2)ETSformer (K=3)FEDformerAutoformerInformerTransformer4896168336720144028805670Forecast Horizon0.00.20.40.60.81.0Time (s)4896168336720144028805670Lookback Window051015202530Memory (GB)ETSformer (K=1)ETSformer (K=2)ETSformer (K=3)FEDformerAutoformerInformerTransformer4896168336720144028805670Forecast Horizon05101520253035Memory (GB)Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\n\nAadyot Bhatnagar, Paul Kassianik, Chenghao Liu, Tian Lan, Wenzhuo Yang, Rowan Cassius, Doyen Sahoo, Devansh Arpit, Sri Subramanian, Gerald Woo, et al. Merlion: A machine learning library for time series. arXiv preprint arXiv:2109.09265, 2021.\n\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pp. 213–229. Springer, 2020.\n\nCristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. arXiv preprint arXiv:2201.12886, 2022.\n\nRobert B Cleveland, William S Cleveland, Jean E McRae, and Irma Terpenning. Stl: A seasonal-trend\n\ndecomposition. J. Off. Stat, 6(1):3–73, 1990.\n\nWilliam P Cleveland and George C Tiao. Decomposition of seasonal time series: a model for the census x-11 program. Journal of the American statistical Association, 71(355):581–587, 1976.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n\nbidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=YicbFdNTTy.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nCharles C Holt. Forecasting seasonals and trends by exponentially weighted moving averages.\n\nInternational journal of forecasting, 20(1):5–10, 2004.\n\nRob Hyndman, Anne B Koehler, J Keith Ord, and Ralph D Snyder. Forecasting with exponential\n\nsmoothing: the state space approach. Springer Science & Business Media, 2008.\n\nRob J Hyndman and George Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\n\nIn International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=rkgNKkHtvB.\n\nGuokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp. 95–104, 2018.\n\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with\n\nfourier transforms. arXiv preprint arXiv:2105.03824, 2021.\n\nShiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. ArXiv, abs/1907.00235, 2019.\n\nMicha ̈el Mathieu, Mikael Henaff, and Yann LeCun. Fast training of convolutional networks through\n\nffts. CoRR, abs/1312.5851, 2014.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nEddie McKenzie and Everette S Gardner Jr. Damped trend exponential smoothing: a modelling\n\nviewpoint. International Journal of Forecasting, 26(4):661–665, 2010.\n\nBoris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis In International Conference on\n\nexpansion analysis for interpretable time series forecasting. Learning Representations, 2019.\n\nAlessandro Raganato, Yves Scherrer, and J ̈org Tiedemann. Fixed encoder self-attention patterns in transformer-based machine translation. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 1620 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 556–568. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.49. URL https: //doi.org/10.18653/v1/2020.findings-emnlp.49.\n\nDavid Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181– 1191, 2020. ISSN 0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2019.07.001. URL https: //www.sciencedirect.com/science/article/pii/S0169207019301888.\n\nSlawek Smyl. A hybrid method of exponential smoothing and recurrent neural networks for time\n\nseries forecasting. International Journal of Forecasting, 36(1):75–85, 2020.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Journal of Machine Dropout: A simple way to prevent neural networks from overfitting. Learning Research, 15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html.\n\nIvan Svetunkov. Complex exponential smoothing. Lancaster University (United Kingdom), 2016.\n\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In International Conference on Machine Learning, pp. 10183–10192. PMLR, 2021.\n\nSean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45,\n\n2018.\n\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformer’s attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pp. 4344–4353, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1443. URL https://aclanthology.org/D19-1443.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz In Advances in neural information\n\nKaiser, and Illia Polosukhin. Attention is all you need. processing systems, pp. 5998–6008, 2017.\n\nMichail Vlachos, Philip Yu, and Vittorio Castelli. On periodicity detection and structural periodic similarity. In Proceedings of the 2005 SIAM international conference on data mining, pp. 449–460. SIAM, 2005.\n\nPeter R Winters. Forecasting sales by exponentially weighted moving averages. Management science,\n\n6(3):324–342, 1960.\n\nGerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. CoST: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=PilZY3omXV2.\n\nHaixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers In Advances in Neural Information\n\nwith Auto-Correlation for long-term series forecasting. Processing Systems, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSifan Wu, Xi Xiao, Qianggang Ding, Peilin Zhao, Ying Wei, and Junzhou Huang. Adversarial sparse\n\ntransformer for time series forecasting. In NeurIPS, 2020.\n\nWeiqiu You, Simeng Sun, and Mohit Iyyer. Hard-coded gaussian attention for neural machine\n\ntranslation. arXiv preprint arXiv:2005.00742, 2020.\n\nGeorge Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 2114–2124, 2021.\n\nHaoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of AAAI, 2021.\n\nTian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. arXiv preprint arXiv:2201.12740, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA CLASSICAL EXPONENTIAL SMOOTHING\n\nWe instantiate exponential smoothing methods (Hyndman et al., 2008) in the univariate forecasting setting. They assume that time-series can be decomposed into seasonal and trend components, and trend can be further decomposed into level and growth components. Specifically, a commonly used model is the additive Holt-Winters’ method (Holt, 2004; Winters, 1960), which can be formulated as:\n\nLevel : et = α(xt − st−p) + (1 − α)(et−1 + bt−1)\n\nGrowth : bt = β(et − et−1) + (1 − β)bt−1\n\nSeasonal : st = γ(xt − et) + (1 − γ)st−p\n\nForecasting : ˆxt+h|t = et + hbt + st+h−p\n\n(3)\n\nwhere p is the period of seasonality, and ˆxt+h|t is the h-steps ahead forecast. The above equations state that the h-steps ahead forecast is composed of the last estimated level et, incrementing it by h times the last growth factor, bt, and adding the last available seasonal factor st+h−p. Specifically, the level smoothing equation is formulated as a weighted average of the seasonally adjusted observation (xt − st−p) and the non-seasonal forecast, obtained by summing the previous level and growth (et−1 + bt−1). The growth smoothing equation is implemented by a weighted average between the successive difference of the (de-seasonalized) level, (et − et−1), and the previous growth, bt−1. Finally, the seasonal smoothing equation is a weighted average between the difference of observation and (de-seasonalized) level, (xt − et), and the previous seasonal index st−p. The weighted average of these three equations are controlled by the smoothing parameters α, β and γ, respectively.\n\nA widely used modification of the additive Holt-Winters’ method is to allow the damping of trends, which has been proved to produce robust multi-step forecasts (Svetunkov, 2016; McKenzie & Gardner Jr, 2010). The forecast with damping trend can be rewritten as:\n\nˆxt+h|t = et + (φ + φ2 + · · · + φh)bt + st+h−p, where the growth is damped by a factor of φ. If φ = 1, it degenerates to the vanilla forecast. For 0 < φ < 1, as h → ∞ this growth component approaches an asymptote given by φbt/(1 − φ).\n\n(4)\n\nB EXPONENTIAL SMOOTHING ATTENTION\n\nB.1 EXPONENTIAL SMOOTHING ATTENTION MATRIX\n\nAES =\n\n\n\n \n \n\n\n(1 − α)1 (1 − α)2 (1 − α)3 ...\n\nα α(1 − α) α(1 − α)2 ...\n\n(1 − α)L α(1 − α)L−1\n\n0 α\nα(1 − α)\n\n0 0\nα\n\n...\n\n. . .\n\n... α(1 − α)j\n\n\n\n \n \n\n\n0 0\n0\n\n. . . . . . . . . . . . . . . α\n\n...\n\nB.2 EFFICIENT EXPONENTIAL SMOOTHING ATTENTION ALGORITHM\n\nAlgorithm 1 PyTorch-style pseudocode of efficient AES conv1d fft: efficient convolution operation implemented with fast Fourier transform (Appendix B, Algorithm 3), outer: outer product\n\n# V: value matrix, shape: L x d # v0: initial state, shape: d # alpha: smoothing parameter, shape: 1\n\n# obtain exponentially decaying weights # and compute weighted combination powers = arange(L) # L weight = alpha ∗ (1 − alpha) ∗∗ flip(powers) # L output = conv1d fft(V, weight, dim=0) # L x d\n\n# compute contribution from initial state init weight = (1 − alpha) ∗∗ (powers + 1) # L init output = outer(init weight, v0) # L x d return init output + output\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB.3 LEVEL SMOOTHING VIA EXPONENTIAL SMOOTHING ATTENTION\n\nE(n)\n\nt = α ∗ (E(n−1) = α ∗ (E(n−1)\n\nt\n\nt\n\n− S(n)\n\nt\n\n− S(n)\n\nt\n\n) + (1 − α) ∗ (E(n)\n\nt−1 + B(n)\n\nt−1)\n\n+ (1 − α) ∗ [α ∗ (E(n−1)\n\n) + (1 − α) ∗ B(n) t−1 − S(n)\n\nt−1\n\nt−1) + (1 − α) ∗ (E(n) t−1 − S(n) t−1)\n\nt−2 + B(n)\n\nt−2)]\n\n= α ∗ (E(n−1)\n\nt\n\n− S(n)\n\nt\n\n+ (1 − α) ∗ B(n)\n\n) + α ∗ (1 − α) ∗ (E(n−1) t−1 + (1 − α)2 ∗ B(n) t−2 − S(n)\n\nt−2\n\n+ (1 − α)2[α ∗ (E(n−1)\n\nt−2) + (1 − α) ∗ (E(n)\n\nt−3 + B(n)\n\nt−3)]\n\n...\n\n= (1 − α)t(E(n)\n\n0 − S(n)\n\n0\n\n) +\n\n= AES(E(n−1)\n\nt−L:t − S(n)\n\nt−L:t) +\n\nt−1 (cid:88)\n\nj=0\n\nt (cid:88)\n\nα ∗ (1 − α)j ∗ (E(n−1)\n\nt−j − S(n)\n\nt−j) +\n\nt (cid:88)\n\n(1 − α)k ∗ B(n)\n\nt−k\n\nk=1\n\n(1 − α)k ∗ B(n)\n\nt−k\n\nk=1\n\nBased on the above expansion of the level equation, we observe that E(t) n can be computed by a sum of two terms, the first of which is given by an AES term, and we finally, we note that the second term can also be calculated using the conv1d fft algorithm, resulting in a fast implementation of level smoothing.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nB.4 FURTHER DETAILS ON ESA IMPLEMENTATION\n\nAlgorithm 2 PyTorch-style pseudocode of naive AES mm: matrix multiplication, outer: outer product repeat: einops style tensor operations, gather: gathers values along an axis specified by dim\n\n# V: value matrix, shape: L x d # v0: initial state, shape: d # alpha: smoothing parameter, shape: 1\n\nL, d = V.shape\n\n# obtain exponentially decaying weights powers = arange(L) # L weight = alpha ∗ (1 − alpha).pow(flip(powers)) #\n\nL\n\n# perform a strided roll operation # rolls a matrix along the columns in a strided\n\nmanner\n\n# i.e. first row is shifted right by L−1\n\npositions,\n\n# second row is shifted L−2, ..., last row is\n\nshifted by 0.\n\nweight = repeat(weight, 'L −> T L', T=L) # L x L indices = repeat(arange(L), 'L −> T L', T=L) indices = (indices − (arange(L) + 1).unsqueeze(1)\n\n) % L\n\nweight = gather(weight, dim=−1, index=indices)\n\n# triangle masking to achieve the exponential\n\nsmoothing attention matrix weight = triangle causal mask(weight)\n\noutput = mm(weight, V)\n\ninit weight = (1 − alpha) ∗∗ (powers + 1) init output = outer(init weight, v0)\n\nreturn init output + output\n\nAlgorithm 3 PyTorch-style pseudocode of conv1d fft next_fast_len: find the next fast size of input data to fft, for zero-padding, etc. rfft: compute the one-dimensional discrete Fourier Transform for real input x.conj(): return the complex conjugate, element-wise irfft: computes the inverse of rfft roll: roll array elements along a given axis index select: returns a new tensor which index es the input tensor along dimension dim using the entries in index\n\n# V: value matrix, shape: L x d # weight: exponential smoothing attention vector,\n\nshape: L\n\n# dim: dimension to perform convolution on\n\n# obtain lengths of sequence to perform\n\nconvolution on\n\nN = V.size(dim) M = weight.size(dim)\n\n# Fourier transform on inputs fast len = next fast len(N + M − 1) F V = rfft(V, fast len, dim=dim) F weight = rfft(weight, fast len, dim=dim)\n\n# multiplication and inverse F V weight = F V ∗ F weight.conj() out = irfft(F V weight, fast len, dim=dim) out = out.roll(−1, dim=dim)\n\n# select the correct indices idx = range(fast len − N, fast len) out = out.index select(dim, idx)\n\nreturn out\n\nAlgorithm 2 describes the naive implementation for ESA by first constructing the exponential smoothing attention matrix, AES, and performing the full matrix-vector multiplication. Efficient AES relies on Algorithm 3, to achieve an O(L log L) complexity, by speeding up the matrix-vector multiplication. Due to the structure lower triangular structure of AES (ignoring the first column), we note that performing a matrix-vector multiplication with it is equivalent to performing a convolution with the last row. Algorithm 3 describes the pseudocode for fast convolutions using fast Fourier transforms.\n\nC DISCRETE FOURIER TRANSFORM\n\nThe DFT of a sequence with regular intervals, x = (x0, x1, . . . , xN −1) is a sequence of complex numbers,\n\nck =\n\nN −1 (cid:88)\n\nn=0\n\nxn · exp(−i2πkn/N ),\n\nfor k = 0, 1, . . . , N − 1, where ck are known as the Fourier coefficients of their respective Fourier frequencies. Due to the conjugate symmetry of DFT for real-valued signals, we simply consider the first ⌊N/2⌋ + 1 Fourier coefficients and thus we denote the DFT as F : RN → C⌊N/2⌋+1. The DFT maps a signal to the frequency domain, where each Fourier coefficient can be uniquely represented\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nby the amplitude, |ck|, and the phase, φ(ck),\n\n|ck| = (cid:112)R{ck}2 + I{ck}2\n\nφ(ck) = tan−1\n\n(cid:19)\n\n(cid:18) I{ck} R{ck}\n\nwhere R{ck} and I{ck} are the real and imaginary components of ck respectively. Finally, the inverse DFT maps the frequency domain representation back to the time domain,\n\nxn = F −1(c)n =\n\n1 N\n\nN −1 (cid:88)\n\nk=0\n\nck · exp(i2πkn/N ),\n\nD IMPLEMENTATION DETAILS\n\nD.1 HYPERPARAMETERS\n\nFor all experiments, we use the same hyperparameters for the encoder layers, decoder stacks, model dimensions, feedforward layer dimensions, number of heads in multi-head exponential smoothing attention, and kernel size for input embedding as listed in Table 5. We perform hyperparameter tuning via a grid search over the number of frequencies K, lookback window size, and learning rate, selecting the settings which perform the best on the validation set based on MSE (on results averaged over three runs). The search range is reported in Table 5, where the lookback window size search range was decided to be set as the values for the horizon sizes for the respective datasets.\n\nTable 5: Hyperparameters used in ETSformer.\n\nHyperparameter\n\nValue\n\nEncoder layers Decoder stacks Model dimension Feedforward dimension Multi-head ESA heads Input embedding kernel size K\nLookback window size Lookback window size (ILI) L ∈ {24, 36, 48, 60} Learning rate\n\n2 2\n512 2048 8\n3 K ∈ {0, 1, 2, 3} L ∈ {96, 192, 336, 720}\n\nlr ∈ {1e−3, 3e−4, 1e−4, 3e−5, 1e−5}\n\nD.2 OPTIMIZATION\n\nWe use the Adam optimizer (Kingma & Ba, 2015) with β1 = 0.9, β2 = 0.999, and ε = 1e − 08, and a batch size of 32. We schedule the learning rate with linear warmup over 3 epochs, and cosine annealing thereafter for a total of 15 training epochs for all datasets. The minimum learning rate is set to 1e-30. For smoothing and damping parameters, we set the learning rate to be 100 times larger and do not use learning rate scheduling. Training was done on an Nvidia A100 GPU.\n\nD.3 REGULARIZATION\n\nWe apply two forms of regularization during the training phase.\n\nDropout We apply dropout (Srivastava et al., 2014) with a rate of p = 0.2 across the model. Dropout is applied on the outputs of the Input Embedding, Frequency Self-Attention and Multi-Head ES Attention blocks, in the Feedforward block (after activation and before normalization), on the attention weights, as well as damping weights.\n\nNoise Injection We utilize a composition of three noise distributions, applied in the following order - scale, shift, and jitter, activating with a probability of 0.5.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\n1. Scale – The time-series is scaled by a single random scalar value, obtained by sampling\n\nε ∼ N (0, 0.2), and each time step is ̃xt = εxt.\n\n2. Shift – The time-series is shifted by a single random scalar value, obtained by sampling\n\nε ∼ N (0, 0.2) and each time step is ̃xt = xt + ε.\n\n3. Jitter – I.I.D. Gaussian noise is added to each time step, from a distribution εt ∼ N (0, 0.2),\n\nwhere each time step is now ̃xt = xt + εt.\n\nE DATASETS\n\nETT1 Electricity Transformer Temperature (Zhou et al., 2021) is a multivariate time-series dataset, comprising of load and oil temperature data recorded every 15 minutes from electricity transformers. ETT consists of two variants, ETTm and ETTh, whereby ETTh is the hourly-aggregated version of ETTm, the original 15 minute level dataset.\n\nECL2 Electricity Consuming Load measures the electricity consumption of 321 households clients over two years, the original dataset was collected at the 15 minute level, but is pre-processed into an hourly level dataset.\n\nExchange3 Exchange (Lai et al., 2018) tracks the daily exchange rates of eight countries (Australia, United Kingdom, Canada, Switzerland, China, Japan, New Zealand, and Singapore) from 1990 to 2016.\n\nTraffic4 Traffic is an hourly dataset from the California Department of Transportation describing road occupancy rates in San Francisco Bay area freeways.\n\nWeather5 Weather measures 21 meteorological indicators like air temperature, humidity, etc., every 10 minutes for the year of 2020.\n\nILI6 Influenza-like Illness records the ratio of patients seen with ILI and the total number of patients on a weekly basis, obtained by the Centers for Disease Control and Prevention of the United States between 2002 and 2021.\n\nF SYNTHETIC DATASET\n\nThe synthetic dataset is constructed by a combination of trend and seasonal component. Each instance in the dataset has a lookack window length of 720 and forecast horizon length of 192. The trend 1+exp β0(t−β1) , where β0 = −0.2, β1 = 720. pattern follows a nonlinear, saturating pattern, b(t) = The seasonal pattern follows a complex periodic pattern formed by a sum of sinusoids. Concretely, s(t) = A1 cos(2πf1t) + A2 cos(2πf2t, where f1 = 1/10, f2 = 1/13 are the frequencies, A1 = A2 = 0.15 are the amplitudes. During training phase, we use an additional noise component by adding i.i.d. gaussian noise with 0.05 standard deviation. Finally, the i-th instance of the dataset is xi = [xi(1), xi(2), . . . , xi(720 + 192)], where xi(t) = b(t) + s(t + i).\n\n1\n\n1https://github.com/zhouhaoyi/ETDataset 2lhttps://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 3https://github.com/laiguokun/multivariate-time-series-data 4https://pems.dot.ca.gov/ 5https://www.bgc-jena.mpg.de/wetter/ 6https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nG UNIVARIATE FORECASTING BENCHMARK\n\nTable 6: Univariate forecasting results over various forecast horizons. Best results are bolded, and second best results are underlined.\n\nMethods\n\nETSformer\n\nFEDformer\n\nAutoformer\n\nInformer\n\nN-BEATS\n\nDeepAR\n\nProphet\n\nARIMA\n\nAutoETS\n\nMetrics MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n2 m\nT T\nE\n\n96 192 336 720\n\ne 96 192 336 720\n\ng n\na h\nc x\nE\n\n0.080 0.150 0.175 0.224\n\n0.099 0.223 0.421 1.114\n\n0.212 0.302 0.334 0.379\n\n0.230 0.353 0.497 0.807\n\n0.063 0.102 0.130 0.178\n\n0.131 0.277 0.426 1.162\n\n0.189 0.245 0.279 0.325\n\n0.284 0.420 0.511 0.832\n\n0.065 0.118 0.154 0.182\n\n0.241 0.273 0.508 0.991\n\n0.189 0.256 0.305 0.335\n\n0.299 0.665 0.605 0.860\n\n0.088 0.132 0.180 0.300\n\n0.591 1.183 1.367 1.872\n\n0.225 0.283 0.336 0.435\n\n0.615 0.912 0.984 1.072\n\n0.082 0.120 0.226 0.188\n\n0.156 0.669 0.611 1.111\n\n0.219 0.268 0.370 0.338\n\n0.299 0.665 0.605 0.860\n\n0.099 0.154 0.277 0.332\n\n0.417 0.813 1.331 1.890\n\n0.237 0.310 0.428 0.468\n\n0.515 0.735 0.962 1.181\n\n0.287 0.312 0.331 0.534\n\n0.828 0.909 1.304 3.238\n\n0.456 0.483 0.474 0.593\n\n0.762 0.974 0.988 1.566\n\n0.211 0.261 0.317 0.366\n\n0.112 0.304 0.736 1.871\n\n0.362 0.406 0.448 0.487\n\n0.245 0.404 0.598 0.935\n\n0.794 1.078 1.279 1.541\n\n0.192 0.355 0.577 1.242\n\n0.617 0.740 0.822 0.924\n\n0.316 0.442 0.578 0.865\n\nH ETSFORMER STANDARD DEVIATION\n\nTable 7: ETSformer main benchmark results with standard deviation. Experiments are performed over three runs.\n\n(a) Multivariate benchmark.\n\n(b) Univariate benchmark.\n\nMetrics\n\nMSE (SD)\n\nMAE (SD)\n\nMetrics\n\nMSE (SD)\n\nMAE (SD)\n\n2 m\nT T\nE\n\nL C\nE\n\n96 192 336 720\n\n96 192 336 720\n\ne 96 192 336 720\n\ng n\na h\nc x\nE\n\nc fi\n\nf a\nr\n\nT\n\n96 192 336 720\n\ne h\n\nr 96 192 336 720\n\nt a\ne\n\nW\n\nI\n\nL\n\nI\n\n24 36 48 60\n\n2 m\nT T\nE\n\n96 192 336 720\n\ne 96 192 336 720\n\ng n\na h\nc x\nE\n\n0.080 (0.001) 0.150 (0.024) 0.175 (0.012) 0.224 (0.008)\n\n0.099 (0.003) 0.223 (0.015) 0.421 (0.002) 1.114 (0.049)\n\n0.212 (0.001) 0.302 (0.026) 0.334 (0.014) 0.379 (0.006)\n\n0.230 (0.003) 0.353 (0.009) 0.497 (0.000) 0.807 (0.016)\n\n0.189 (0.002) 0.253 (0.002) 0.314 (0.001) 0.414 (0.000)\n\n0.187 (0.001) 0.199 (0.001) 0.212 (0.001) 0.233 (0.006)\n\n0.085 (0.000) 0.182 (0.003) 0.348 (0.004) 1.025 (0.031)\n\n0.607 (0.005) 0.621 (0.015) 0.622 (0.003) 0.632 (0.004)\n\n0.197 (0.007) 0.237 (0.005) 0.298 (0.003) 0.352 (0.007)\n\n2.527 (0.061) 2.615 (0.103) 2.359 (0.056) 2.487 (0.006)\n\n0.280 (0.001) 0.319 (0.001) 0.357 (0.001) 0.413 (0.001)\n\n0.304 (0.001) 0.315 (0.002) 0.329 (0.002) 0.345 (0.006)\n\n0.204 (0.001) 0.303 (0.002) 0.428 (0.003) 0.774 (0.014)\n\n0.392 (0.005) 0.399 (0.013) 0.396 (0.003) 0.396 (0.004)\n\n0.281 (0.008) 0.312 (0.004) 0.353 (0.003) 0.388 (0.002)\n\n1.020 (0.021) 1.007 (0.013) 0.972 (0.011) 1.016 (0.007)\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nI LAYER ANALYSIS\n\nTable 8: Analysis on the number of layers and stacks of ETSformer for ETTm2 and ECL datasets. Obs Space refers to a variation of ETSformer which removes the embedding projection layer, and performs operations in observation space.\n\nNum Layers\n\nObs. Space\n\nLayers=1\n\nLayers=2\n\nLayers=3\n\nLayers=4\n\nLayers=5\n\nMetrics\n\nMSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\n\n2 m\nT T\nE\n\nL C\nE\n\n96 192 336 720\n\n96 192 336 720\n\n0.685 0.758 0.833 0.946\n\n0.204 0.215 0.227 0.273\n\n0.705 0.736 0.766 0.808\n\n0.318 0.328 0.339 0.373\n\n0.190 0.256 0.320 0.424\n\n0.190 0.204 0.216 0.254\n\n0.284 0.325 0.364 0.423\n\n0.309 0.320 0.332 0.360\n\n0.189 0.253 0.314 0.414\n\n0.187 0.199 0.212 0.233\n\n0.280 0.319 0.357 0.413\n\n0.304 0.315 0.329 0.345\n\n0.188 0.252 0.313 0.412\n\n0.190 0.199 0.212 0.248\n\n0.279 0.317 0.354 0.410\n\n0.308 0.315 0.330 0.356\n\n0.189 0.252 0.313 0.413\n\n0.194 0.202 0.216 0.248\n\n0.279 0.317 0.354 0.411\n\n0.312 0.319 0.334 0.355\n\n0.192 0.254 0.314 0.413\n\n0.194 0.202 0.217 0.248\n\n0.282 0.319 0.355 0.411\n\n0.311 0.319 0.335 0.356\n\nWe provide additional analysis on the number of layers, and also ablations on the observation space (meaning that there is no projection into representation space by removing the embedding layer). We observe that learning deep representations lead to a significant increase in performance, and the optimal number of layers is around 2 3, before overfitting occurs.\n\nJ REAL-WORLD DECOMPOSED FORECASTS\n\nFigure 7: Visualization of decomposed forecasts from ETSformer on real world datasets, ETTh1, ECL, and Weather. Note that season is zero-centered, and trend successfully tracks the level of the time-series. Due to the long sequence forecasting setting and with a damping, the growth component is not visually obvious, but notice for the Weather dataset, the trend pattern is has a strong downward slope initially (near time step 0), and is quickly damped.\n\n19\n\n0102030401.00.80.60.40.20.00.2ETTh1Ground TruthForecastTrendSeason0204060801.51.00.50.00.51.01.5ECLGround TruthForecastTrendSeason0204060801.00.80.60.40.20.00.20.4weatherGround TruthForecastTrendSeason",
    "reference": "# Summary Of The Paper\n\nThis paper presents exponential smoothing transformers for long-range time series forecasting. The key idea is to leverage a level-growth-seasonality decomposed transformer architecture and employ both exponential smoothing attention and frequency attention to reduce computational complexity. The experiment results showed the effectiveness of the proposed method.\n\n# Strength And Weaknesses\n\nStrength:\n\n1. This paper is well-written and organized.\n2. Several transformer-based baselines have been used in empirical studies.\n\nWeaknesses:\n\n1. Combining exponential smoothing with neural networks to perform time series forecasting is not a new idea, for example, \n\n[1] A hybrid method of Exponential Smoothing and Recurrent Neural Networks for time series forecasting International Journal of Forecasting 2019\n\n[2] A Hybrid Residual Dilated LSTM and Exponential Smoothing Model for Midterm Electric Load Forecasting. IEEE TNNLS 2021\n\n[3] Time‑series analysis with smoothed Convolutional Neural Network\n\nEspecially In [2], the authors have thoroughly assessed various approaches (including LSTM, GNNs)+ ETS for time series forecasting. However, none of them were mentioned or compared in this paper. Also, applying ETS to the transformer seems straightforward to me and thus the technical novelty here is limited.\n\n2. FEDformer has already shown that properly considering frequency-based attention in the transformer is useful for long-range forecasting. In the paper, I did not observe how the proposed FA is superior to FEDformer.\n\n3. Overall, I feel this paper is a little bit add-hoc, i.e., combining the benefit of ETS (which has been validated in earlier works) and FA (similar ideas have been validated in FEDformer).  It is not clear which component contributes more to the performance boost.\n\n4. The experiment results do not show the obvious superior performance of the proposed ETSformer over FEDformer and other baselines.\n\n5. The discussion of the results in Table 1 is limited, especially for the cases when ETSformer cannot outperform baselines.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe overall clarify is OK. However, the technical quality and novelty is limited. Reproducibility is OK since code is available.\n\n# Summary Of The Review\n\nOverall, I have concerns over the novelty, related works, and experiment results.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Details Of Ethics Concerns\n\nNo."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSCALING CONVEX NEURAL NETWORKS WITH BURERMONTEIRO FACTORIZATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRecently, it has been demonstrated that the training problem for a wide variety of (non) linear two-layer neural networks (such as two-layer perceptrons, convolutional networks, and self-attention) can be posed as equivalent convex optimization problems, with an induced regularizer which encourages low rank. However, this regularizer becomes prohibitively expensive to compute at moderate scales, impeding training convex neural networks. To this end, we propose applying the Burer-Monteiro factorization to convex neural networks, which for the first time enables a Burer-Monteiro perspective on neural networks with non-linearities. This factorization leads to an equivalent yet computationally tractable non-convex alternative with no spurious local minima. We develop a novel relative optimality bound of stationary points of the Burer-Monteiro factorization, thereby providing verifiable conditions under which any stationary point is a global optimum. Further, for the first time, we show that linear self-attention with sufficiently many heads has no spurious local minima. Our experiments demonstrate the implications of the relative optimality bound for stationary points of the Burer-Monteiro factorization.\n\n1\n\nINTRODUCTION\n\nIt has been demonstrated that the training problem for (non-linear) two-layer neural networks are equivalent to convex programs (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020; Sahiner et al., 2021b; Ergen et al., 2021; Sahiner et al., 2021a). This has been observed for a variety of architectures, including multi-layer perceptrons (MLPs) (Pilanci & Ergen, 2020; Sahiner et al., 2021b), convolutional neural networks (CNNs) (Ergen & Pilanci, 2020; Sahiner et al., 2021c), and self-attention based transformers (Sahiner et al., 2022). A major benefit of convex training of neural networks is that global optimality is guaranteed, which brings transparency to training neural networks.\n\nThe convex formulation of neural networks induces biases by regularization of the network weights. For linear activation, the convex model directly imposes nuclear-norm regularization which is wellknown to encourage low-rank solutions (Recht et al., 2010). For ReLU activation, however, the convex model induces a type of nuclear norm which promotes sparse factorization while the left factor is constrained to an affine space (Sahiner et al., 2021b). This constrained nuclear-norm is NP-hard to compute. This impedes the utility of convex neural networks for ReLU activation.\n\nTo address this computational challenge, we seek a method which (i) inherits the per-iteration complexity of non-convex training of neural network, and (ii) inherits the optimality guarantees and transparency of convex training. To find a solution, we leverage the well-studied Burer-Monterio (BM) factorization (Burer & Monteiro, 2003), which was originally proposed as a heuristic method to improve the complexity of convex semi-definite programs (SDPs).\n\nBM has been applied as an efficient solution strategy for problems ranging from matrix factorization (Zheng & Lafferty, 2016; Park et al., 2017; Ge et al., 2017; Gillis, 2017) to rank minimization (Mardani et al., 2013; Recht et al., 2010; Wang et al., 2017) and matrix completion (Mardani et al., 2015; Ge et al., 2017). BM has also been used for over-simplified neural networks such as (Kawaguchi, 2016; Haeffele & Vidal, 2017; Du & Lee, 2018), where optimality conditions for local minima are provided. However, no work has deployed BM factorization for practical non-linear neural networks, and no guarantees are available about the optimality of stationary points. This is likely because BM theory is not applicable to the standard non-convex ReLU networks due to non-linearity between layer weights.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nThus, our focus in this work is to adapt BM for practical two-layer (non-linear) convex neural networks. We consider three common architectures, namely MLPs, CNNs, and self-attention networks. For these scenarios, we develop verifiable relative optimality bounds for all local minima and stationary points, which are easy and interpretable. In light of these conditions, we identify useful insights about the nature of neural networks contributing to optimality. In particular, we observe that for self-attention networks all local minima coincide with the global optima if there are sufficiently many heads. The optimality guarantees also provide useful algorithmic insights, allowing one to verify whether the light-weight first-order methods such as SGD achieve the global optimum for the non-convex training of neural networks. Our experiments with image classification task indicate that this BM factorization enables layerwise training of convex CNNs, which allows for convex networks for the first time to match the performance of multi-layer end-to-end trained non-convex CNNs.\n\n1.1 CONTRIBUTIONS\n\nAll in all, our contributions are summarized as follows:\n\n• We propose the BM factorization for efficiently solving convex neural networks with ReLU activation for moderate and large scales. This is the first time BM theory has been applied to the non-linear neural network setting.\n\n• We derive a novel bound on the relative optimality of the stationary points of the BM\n\nfactorization for neural networks.\n\n• Accordingly, we identify simple and verifiable conditions which guarantee a stationary point of the non-convex BM formulation achieves the global optimum of the convex neural network.\n\n• We yield basic insights into the fundamental nature of neural networks that contribute to optimality; e.g. that linear self-attention has no spurious local minima if it has sufficiently many heads.\n\n• Our experiments verify the proposed relative optimality bound of stationary points from the BM factorization, and uncovers cases where SGD finds saddle points, even in two-layer neural networks.\n\n1.2 RELATED WORK\n\nBurer-Monteiro factorization. The Burer-Monteiro (BM) factorization was first introduced in (Burer & Monteiro, 2003; 2005). There has been a long line of work studying the use of this factorization for solving SDPs (Boumal et al., 2016; Cifuentes & Moitra, 2019; Waldspurger & Waters, 2020; Erdogdu et al., 2021). In the rectangular matrix case, gradient descent converges to a global optimum of the matrix factorization problem with high probability for certain classes of matrices (Zheng & Lafferty, 2016). The BM factorization has been also studied in the rectangular case in more generic settings (Bach et al., 2008; Haeffele et al., 2014; Haeffele & Vidal, 2017).\n\nNuclear norm and rank minimization. The ability of nuclear norm regularization to induce low rank has been studied extensively in compressed sensing (Candès & Recht, 2009; Recht et al., 2010; Candès & Tao, 2010). BM factorization has been applied to scale up nuclear-norm minimization (Mardani et al., 2015; 2013). It has also been deployed for low-rank matrix factorization (Cabral et al., 2013; Zhu et al., 2017; Park et al., 2017; Ge et al., 2017). The results show that all second-order critical points of the BM factorization are global optima if certain qualification conditions are met.\n\nSGD for non-convex neural networks. It has been shown that for over-parameterized two-layer linear networks, all local minima are global minima (Kawaguchi, 2016). Accordingly, a line of work has attempted to show that gradient descent or its modifications provably find local minima and escape saddle points (Ge et al., 2015; Lee et al., 2016; Jin et al., 2017; Daneshmand et al., 2018). However, these works assume Lipschitz gradients and Hessians of the non-convex objective, which is not typically satisfied. Another line of work shows that gradient descent converges to global optima for sufficiently highly over-parameterized neural networks, with either the parameter count being a high-order polynomial of the sample count (Du et al., 2018; 2019; Arora et al., 2019), or the network architecture being simple (Du & Lee, 2018). In practice, it has been empirically observed that SGD\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ncan converge to local maxima, or get stuck in saddle points (Du et al., 2017; Ziyin et al., 2021). For unregularized matrix factorization, it has also recently been shown that randomly initialized gradient descent on BM factorization provably converges to global minima (Ye & Du, 2021).\n\nConvex neural networks. ReLU neural networks have equivalent convex programs for training, such as networks with scalar outputs (Pilanci & Ergen, 2020), vector-outputs (Sahiner et al., 2021b), convolutional networks (Ergen & Pilanci, 2020; Sahiner et al., 2021c), polynomial-activation networks (Bartan & Pilanci, 2021), batch-norm based networks (Ergen et al., 2021), Wasserstein GANs (Sahiner et al., 2021a), and self-attention networks (Sahiner et al., 2022). Despite efforts in developing efficient solvers, convex networks are only effectively trainable at small scales (Bai et al., 2022; Mishkin et al., 2022). Our novelty is to adapt BM factorization as a fast and scalable solution for training convex networks, with simple, verifiable conditions for global optimality.\n\n2 PRELIMINARIES\n\nWe denote (·)+ := max{0, ·} as the ReLU non-linearity. We use superscripts, say A(ii,i2), to denote blocks of matrices, and brackets, say A[i1, i2], to denote elements of matrices. We let 1 be the vector of ones of appropriate size, and BH be the unit H-norm ball, {u : ∥u∥H ≤ 1}. Unless otherwise stated, let F be a convex, differentiable function. We use n to denote the number of samples, and c to denote the output dimension of each network. All proofs are presented in the Appendix.\n\n2.1 TWO-LAYER NEURAL NETWORKS AS CONVEX PROGRAMS\n\nA line of work has demonstrated that two-layer neural networks are equivalent to convex optimization problems. We consider a data matrix X ∈ Rn×d and consider two-layer σ-activation network with c outputs, m neurons, weight-decay parameter β > 0 :\n\np∗\n\nM LP := min\n\nW1∈Rd×m W2∈Rc×m\n\nF (σ(XW1)W⊤\n\n2 ) +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥w1j∥2\n\n2 + ∥w2j∥2 2.\n\n(1)\n\nWhen σ is a linear activation and m ≥ m∗ for some m∗ ≤ min{d, c}, this problem is equivalent to ((Rennie & Srebro, 2005), Section 2.2)\n\np∗\n\nLM LP = min\n\nZ∈Rd×c\n\nF (XZ) + β∥Z∥∗,\n\n(2)\n\nwhereas for a ReLU activation and m ≥ m∗ for some unknown, problem-dependent m∗ ≤ nc ((Sahiner et al., 2021b), Thm. 3.1),\n\np∗\n\nRM LP = min\n\nZj ∈Rd×c\n\nF (\n\nP (cid:88)\n\nj=1\n\nDjXZj) + β\n\nP (cid:88)\n\nj=1\n\n∥Zj∥∗,Kj ,\n\n(3)\n\nKj := (2Dj − In)X\n\nwhere {Dj}P\n\nj=1 = {diag (1{Xu ≥ 0}) : u ∈ Rd} enumerates the possible activation patterns , where r := rank(X)\n\ngenerated from X, and the number of such patterns satisfies P ≤ 2r\n\n(cid:17)r\n\n(cid:16) e(n−1) r\n\n(Stanley et al., 2004; Pilanci & Ergen, 2020). The expression (3) also involves a constrained nuclear norm expression, which is defined as\n\n∥Z∥∗,K := min t≥0\n\nt s.t. Z ∈ tC\n\nC := conv{Z = uv⊤ : Ku ≥ 0, ∥u∥2 ≤ 1, ∥v∥2 ≤ 1}.\n\n(4)\n\nThis norm is a quasi-nuclear norm, which differs from the standard nuclear norm in that the factorization upon which it relies imposes a constraint on its left factors. In convex ReLU neural networks, this norm enforces the existence of {uk, vk} such that Z = (cid:80) k(Xuk)+v⊤ k , and penalizes (cid:80) k ∥∗. This norm is NP-hard to compute. A variant of these ReLU activations, called gated ReLU activations, achieves the piecewise linearity of ReLU activations without enforcing the constraints (Fiat et al., 2019). Specifically, the ReLU gates are fixed to some {hj}P\n\nk and DjXZ = (cid:80)\n\nk ∥ukv⊤\n\nk ukv⊤\n\nj=1 to form\n\nσ(Xw1j) := diag (1{Xhj ≥ 0}) (Xw1j) = DjXw1j.\n\n(5)\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nWith gated ReLU activation, the equivalent convex program is given by ((Mishkin et al., 2022), Thm. 2.2; (Sahiner et al., 2022), e.q. (8))\n\np∗\n\nGM LP = min\n\nZj ∈Rd×c\n\nP (cid:88)\n\nF (\n\nj=1\n\nDjXZj) + β\n\nP (cid:88)\n\nj=1\n\n∥Zj∥∗,\n\n(6)\n\nwhich thereby converts the constrained nuclear norm penalty to a standard nuclear norm penalty, thereby improving the complexity of the ReLU network. In addition to the multi-layer perceptron (MLP) formulation, two-layer ReLU-activation convolutional neural networks (CNNs) with global average pooling have been demonstrated to be equivalent to convex programs as well (Sahiner et al., 2021b;c; Ergen & Pilanci, 2020). The non-convex formulation is given by\n\np∗\n\nRCN N := min\n\nw1j ∈Rh w2j ∈Rc\n\nn (cid:88)\n\nm (cid:88)\n\nF (\n\ni=1\n\nj=1\n\nw2j1⊤(Xiw1j)+) +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥w1j∥2\n\n2 + ∥w2j∥2 2,\n\n(7)\n\nwhere samples Xi ∈ RK×h are represented by patch matrices, which hold a convolutional patch of size h in each of their K rows. In particular, each row of Xi contains the data a convolutional kernel would perform an inner-product with, and h is the product of kernel dimensions while K is the number of patches each kernel passes over. It has been shown (Sahiner et al., 2021b) that as long as m ≥ m∗ where m∗ ≤ nc, this is equivalent to a convex program ((Sahiner et al., 2021b), Cor. 5.1)\n\np∗\n\nRCN N = min\n\nZj ∈Rh×c\n\nn (cid:88)\n\nP (cid:88)\n\nF ((\n\ni=1\n\nj=1\n\n1⊤D(i)\n\nj XiZj)⊤) + β\n\nP (cid:88)\n\nj=1\n\n∥Zj∥∗,Kj\n\n(8)\n\nKj := (2Dj − InK)X, X :=\n\n(cid:35)\n\n(cid:34)X1 · · · Xn\n\nwhere {Dj}P (cid:16) e(n−1) (cid:17)r r\n\n2r\n\nj=1 = (cid:8)diag (1{Xu ≥ 0}) : u ∈ Rh(cid:9) and D(i) j ∈ RK×K. Noting that P ≤ and r ≤ h, the only exponential dependence of P is on h, which is typically fixed.\n\nLastly, we review existing convexity results for self-attention transformers (Sahiner et al., 2022). We have the following non-convex objective for a single block of multi-head self-attention with m heads, where Xi ∈ Rs×d with s tokens and d features\n\np∗\n\nSA := min\n\nW1j ∈Rd×d W2j ∈Rd×c\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\nσ (cid:0)XiW1jX⊤\n\ni\n\n(cid:1) XiW2j\n\n +\n\n\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥W1j∥2\n\nF + ∥W2j∥2 F ,\n\n(9)\n\nfor which a variety of objectives F can be posed, including classification (e.g. F incorporates global average pooling followed by softmax-cross-entropy with labels) or denoising (e.g. F is a squared loss against a label matrix). In the linear activation case, as long as m ≥ m∗, where m∗ ≤ min{d2, dc}, this is equivalent to ((Sahiner et al., 2022), Thm. 3.1)\n\np∗\n\nLSA = min\n\nZ∈Rd2×dc\n\nn (cid:88)\n\ni=1\n\nF\n\n(cid:32) d\n\n(cid:88)\n\nd (cid:88)\n\nk=1\n\nl=1\n\n(cid:33)\n\nGi[k, l]XiZ(k,l)\n\n+ β∥Z∥∗,\n\n(10)\n\ni Xi, Gi[k, l] ∈ R, and {Z(k,l) ∈ Rd×c} are block matrices which form Z. A where Gi := X⊤ similar formulation can be posed for ReLU and Gated ReLU activations. In this work, we show that these network architectures are amenable to the BM factorization.\n\n2.2 THE BURER-MONTEIRO FACTORIZATION\n\nFirst proposed by Burer & Monteiro (2003), the Burer-Monteiro (BM) factorization proposes to solve SDPs over some square matrix Q in terms of rectangular factors R where Q is substituted by RR⊤. It was first demonstrated that solving over R does not introduce spurious local minima, given rank(R) ≥ rank(Q∗) for optimal solution to the original SDP Q∗ (Burer & Monteiro, 2005). In general, we seek applications where we optimize over a non-square matrix Z, i.e.\n\np∗\n\nCV X := min\n\nZ∈Rd×c\n\nF (Z)\n\n(11)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nfor a convex, differentiable function F . One may approach this by factoring Z = UV⊤, where U ∈ Rd×m, V ∈ Rc×m for some arbitrary choice m. Then, we have an equivalent non-convex\n\nproblem over R :=\n\n, for f (R) = F (UV⊤):\n\n(cid:21)\n\n(cid:20)U V\n\np∗\n\nCV X = min\n\nR\n\nf (R).\n\n(12)\n\nNoting that (11) is convex over RR⊤ =\n\n(cid:20)UU⊤ UV⊤ VU⊤ VV⊤\n\n(cid:21)\n\n, one may apply directly the result of\n\nBurer & Monteiro (2005) to conclude that as long as m ≥ rank(Z∗), all local minima of (12) are global minima of (11) (see Appendix A.1). A major issue with these results is that rank(Z∗) is not known a priori. Naively, one may simply choose m ≥ min{d, c} and be assured that m ≥ rank(Z∗), but this approach is not satisfactory if further under-parameterization is desired. To address this issue, work from Bach et al. (2008) and Haeffele et al. (2014) demonstrates that all rank-deficient local minimizers of (12) achieve the global minimum p∗ CV X (under mild conditions, see Appendix A.2).\n\nA long line of work has analyzed the conditions where known non-convex optimization algorithms will converge to second-order critical points (local minima) (Ge et al., 2015; Jin et al., 2017; Daneshmand et al., 2018). Under the assumption of a bounded f and its Hessian, a second-order critical point can be found by noisy gradient descent (Ge et al., 2015), or other second-order algorithms (Sun et al., 2015). Even vanilla gradient descent with random initialization has been demonstrated to almost surely converge to a local minimum for f with Lipschitz gradient (Lee et al., 2016). However, if the gradient of f is not Lipschitz-continuous, there are no guarantees that gradient descent will find a second-order critical point of (12): one may encounter a stationary point which is a saddle. For example, in the linear regression setting, i.e.\n\nf (R) = ∥XUV⊤ − Y∥2\n\nF ,\n\n(13)\n\nthe gradient of f is Lipschitz continuous with respect to U when V is fixed and vice-versa, but not Lipschitz continuous with respect to R (Mukkamala & Ochs, 2019). Thus, one may not directly apply the results of Ge et al. (2015); Sun et al. (2015); Lee et al. (2016) in this case. Instead, we seek to understand the conditions under which stationary points to (12) correspond to global optima of (11). One such condition is given in Mardani et al. (2013; 2015). Theorem 2.1 (From (Mardani et al., 2013)). Stationary points ˆU, ˆV of the optimization problem\n\np∗ := min\n\nU,V\n\n1 2\n\n∥UV⊤ − Y∥2\n\nF +\n\nβ 2\n\n(cid:0)∥U∥2\n\nF + ∥V∥2\n\nF\n\n(cid:1)\n\ncorrespond to global optima Z∗ = ˆU ˆV⊤ of the equivalent convex optimization problem 1\n2\n\nF + β∥Z∥∗\n\n∥Z − Y∥2\n\np∗ = min\n\nZ\n\n(14)\n\n(15)\n\nprovided that ∥Y − ˆU ˆV⊤∥2 ≤ β.\n\n3 BURER-MONTEIRO FACTORIZATION FOR CONVEX NEURAL NETWORKS\n\n3.1 MLPS\n\nWe first seek to compare the convex formulations of the MLP training problem (2), (3), and (6) to their BM factorizations. We describe how to find the BM factorization for any convex MLP. Lemma 3.1. For any matrix M ∈ Rn×dc, let f (U, V) := F (MUV⊤) be a differentiable function. For any β > 0 and arbitrary vector norms ∥·∥R and ∥·∥C, we define the Burer-Monteiro factorization\n\np∗ := min\n\nU∈Rdc×m V∈Rdr ×m\n\nf (U, V) +\n\nβ 2\n\nFor the matrix norm ∥ · ∥D defined as\n\n\n\n\n\nm (cid:88)\n\n∥uj∥2\n\nC + ∥vj∥2\n\nR\n\n\n\n .\n\nj=1\n\n∥Z∥D := max\n\nR\n\ntrace(R⊤Z) s.t. u⊤Rv ≤ 1 ∀u ∈ BC, ∀v ∈ BR,\n\n5\n\n(16)\n\n(17)\n\nUnder review as a conference paper at ICLR 2023\n\nthe problem (16) is equivalent to the convex optimization problem\n\np∗ = min\n\nZ∈Rdc×dr\n\nF (MZ) + β∥Z∥D.\n\n(18)\n\nRemark 3.2. In the case of a linear MLP, M = X, dc = d, dr = c, and ∥ · ∥D = ∥ · ∥∗, so using the definition of ∥ · ∥D, in the corresponding BM factorization, R = 2 and C = 2 (Bach et al., 2008). For a gated ReLU network, the regularizer is still the nuclear norm, and thus the same R = C = 2 regularization appears in the BM factorization. In the case of the ReLU MLP, the nuclear norm is replaced by ∥ · ∥D = (cid:80)P j=1 ∥ ·j ∥∗,Kj , which in the BM factorization amounts to having the constraint KjUj ≥ 0. We accordingly express the BM factorization of convex MLPs below.\n\np∗\n\nLM LP = min\n\nU∈Rd×m V∈Rc×m\n\nF (XUV⊤) +\n\nβ 2\n\n(cid:0)∥U∥2\n\nF + ∥V∥2\n\nF\n\n(cid:1)\n\np∗\n\nGM LP = min\n\nUj ∈Rd×m Vj ∈Rc×m\n\nF (\n\nP (cid:88)\n\nj=1\n\nDjXUjV⊤\n\nj ) +\n\nβ 2\n\nP (cid:88)\n\nj=1\n\n(cid:0)∥Uj∥2\n\nF + ∥Vj∥2\n\nF\n\n(cid:1)\n\np∗\n\nRM LP =\n\nmin Uj ∈Rd×m:(2Dj −In)XUj ≥0 Vj ∈Rc×m\n\nP (cid:88)\n\nF (\n\nj=1\n\nDjXUjV⊤\n\nj ) +\n\nβ 2\n\nP (cid:88)\n\nj=1\n\n(cid:0)∥Uj∥2\n\nF + ∥Vj∥2\n\nF\n\n(19)\n\n(20)\n\n(cid:1)\n\n(21)\n\nTo the best of our knowledge, (21) presents the first application of BM factorization to a non-linear neural network, which is enabled by the convex model (3).\n\nIn the linear case, the BM factorization (19) is identical to the original non-convex formulation of a linear MLP with m neurons. Furthermore, in the case of gated ReLU, the BM factorization when m = 1 is equivalent to the original non-convex formulation. However, for ReLU activation two-layer networks, the BM factorization even when m = 1 corresponds to a different (i.e. constrained, rather than ReLU activation) model than the non-convex formulation. While the original convex program is NP-hard, the computation of the cost function of the BM factorization is very simple. Thus, the per-iteration complexity of the BM factorization is much lower than for the convex ReLU MLP.\n\nThe BM factorizations of these convex MLPs are non-convex, hence finding a global minimum appears intractable. However, the following theorem demonstrates that as long as a rank-deficient local minimum to the BM factorization is obtained, it corresponds to a global optimum. Theorem 3.3. If m ≥ rank(Z∗), where Z∗ is a minimizer of (18), all local minima of the BM factorization (16) are global minima. Furthermore, if F is twice-differentiable, any rank-deficient\n\nlocal minimum ˆR :=\n\nof (16) corresponds to a global minimizer Z∗ = ˆU ˆV⊤ of (18).\n\n(cid:21)\n\n(cid:20) ˆU ˆV\n\nThis result demonstrates that these two-layer convex MLPs have no spurious local minima under mild conditions. However, there remains an algorithmic challenge: it is not straightforward to obtain a guaranteed local minima when the gradients of f are not Lipschitz continuous. The following result provides a general condition under which stationary points of the (16) are global optima of (18). Theorem 3.4. For any non-negative objective function F , for a stationary ( ˆU, ˆV) of (16) with corresponding ˆZ = ˆU ˆV⊤ with objective ˆp for (18), the relative optimality gap ˆp−p∗\n\nsatisfies\n\np∗\n\nˆp − p∗\n\np∗ ≤\n\n(cid:32)\n\n∥∇ZF (M ˆZ)∥∗ β\n\nD\n\n(cid:33)\n\n− 1\n\n+\n\n(22)\n\nwhere ∥ · ∥∗\n\nD is the dual norm of ∥ · ∥D.\n\nIn particular, this bound can be calculated by taking the gradient of the unregularized objective function, evaluated at candidate solution ˆZ to the convex problem (18), which is formed by the stationary point of BM problem (16). Intuitively, if the ∥ · ∥∗ D norm of this solution is less than β, then by the subgradient condition, ˆZ is an optimal solution of (18). In the case of a linear MLP with X = Id, F a squared-loss objective, and ∥∇ZF (M ˆZ)∥∗ D ≤ β, our result exactly replicates the result\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nof Theorem 2.1 from Mardani et al. (2013). Furthermore, when this condition is not exactly satisfied, (22) provides a novel result in the form of an optimality gap bound. To our knowledge, this is the first result that generalizes the optimality conditions for stationary points from any BM factorization of a neural network. This provides an easily computable bound after solving (16) which quantifies how close a solution is to the global minimum. In the case of a ReLU MLP, the relative optimality gap is given by\n\n\n\n\n\nˆp − p∗\n\np∗ ≤\n\n1 β\n\n \n \n\nmax j∈[P ] u∈B2 Kj u≥0\n\nP (cid:88)\n\n∥∇Zj F (\n\nj′=1\n\nDj′X ˆZj′)u∥2 − 1\n\n \n \n\n+\n\n.\n\n(23)\n\nComputing this quantity amounts to solving a cone-constrained PCA problem (Deshpande et al., 2014), which can be done in polynomial-time when d is constant. We should note that some stationary points are clearly present in any problem, such as ( ˆU, ˆV) = (0, 0), so we cannot conclude that all stationary points are global optima. However, in certain cases, the optimality gap of stationary points (22) is always zero as we show next. Theorem 3.5. A stationary point ( ˆU, ˆV) of (16) is a global minimizer of (18) if R = C = 2 and\n\nrank( ˆU) = rank( ˆV) = min{dc, dr}.\n\n(24)\n\nThus, for linear and gated ReLU MLPs, we can ensure that if the Burer-Monteiro factorization achieves a stationary point with full rank, it is corresponds with the global optimum of the convex program. We now can further extend these results to CNNs and self-attention architectures.\n\n3.2 CNNS\n\nBefore proceeding to explore the BM factorization in the context of two-layer CNNs, we first provide a new result on an equivalent convex program for two-layer ReLU CNNs with arbitrary linear pooling operations, which extends the results of Sahiner et al. (2021b); Ergen & Pilanci (2020) on Global Average Pooling CNNs. Define Pa ∈ Ra×K to be a linear pooling matrix which pools the K spatial dimensions to an arbitrary size a. Then, we express the non-convex two-layer CNN problem as\n\np∗\n\nCN N := min\n\nw1j ∈Rh W2j ∈Rc×a\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\n\n\nW2jPaσ(Xiw1j)\n\n +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥w1j∥2\n\n2 + ∥W2j∥2 F .\n\n(25)\n\nTheorem 3.6. For β > 0 and ReLU activation σ(·) = (·)+, if m ≥ m∗ where m∗ ≤ nac, then (25) is equivalent to a convex optimization problem, given by\n\np∗\n\nCN N = min\n\nZk∈Rh×ac\n\nn (cid:88)\n\ni=1\n\nF\n\n\n\n \n\nP (cid:88)\n\nk=1\n\n\n\n \n\ntrace(PaD(i)\n\nk XiZ(1) k )\n\n...\n\ntrace(PaD(i)\n\nk XiZ(c) k )\n\n\n\n\n\n \n\n  + β\n\nP (cid:88)\n\nk=1\n\n∥Zk∥∗,Kk ,\n\n(26)\n\nKk := (2Dk − InK)\n\n(cid:35)\n\n(cid:34)X1 · · · Xn\n\n, Z(c′)\n\nk ∈ Rh×a ∀c′ ∈ [c].\n\nThus, we provide a novel result which characterizes two-layer CNNs with arbitrary linear pooling operations as a convex program. Similar results can be shown for the linear and gated-ReLU activation cases1. With this established, we present our main results on the BM factorization for CNNs. Lemma 3.7. The BM factorization of the convex CNN problem with ReLU activation is given as follows.\n\np∗\n\nRCN N =\n\nmin {{ujk∈Rh}m {{Vjk∈Rc×a}m\n\nj=1}P k −I)Xiujk≥0\n\nk=1\n\n(2D(i)\n\nj=1}P\n\nk=1\n\n\n\nF\n\n\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\nn (cid:88)\n\ni=1\n\n\n\nVjkPaD(i)\n\nk Xiujk\n\n +\n\nβ 2\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\n(cid:0)∥ujk∥2\n\nF + ∥Vjk∥2\n\nF\n\n(cid:1)\n\n1We examine linear and gated ReLU activations for CNNs in the Appendix.\n\n(27)\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nThe BM factorization closely resembles the original non-convex formulation (25). Generally, (27) inherits the results of Theorems (3.3), (3.4), and (3.5); we present one such corollary here. Corollary 3.7.1. A stationary point ((ˆujk, ˆVjk)m ˆZk = (cid:80)m\n\nk=1 of (27) corresponds to a global minimizer\n\nof (26) provided that\n\n(cid:16) ˆVjk\n\nj=1)P\n\n(cid:17)⊤\n\nj=1 ˆujkvec\n\n∥\n\nn (cid:88)\n\ni=1\n\n∇Zk F\n\n\n\n \n\nP (cid:88)\n\nk′=1\n\n\n\n \n\ntrace(PaD(i)\n\nk′ XiZ(1) k′ )\n\n\n\n\n\ntrace(PaD(i)\n\nk′ XiZ(c) k′ )\n\n...\n\n \n\n\n\n u∥2 ≤ β, ∀k ∈ [P ], ∀u ∈ B2 : (2D(i)\n\nk −I)Xiu ≥ 0.\n\n(28)\n\n3.3 MULTI-HEAD SELF-ATTENTION\n\nWe now for the first time extend BM factorization theory to self-attention networks. Lemma 3.8. The BM factorization of the convex self-attention problem with linear activation2 is given as follows.\n\np∗\n\nLSA = min\n\nUj ∈Rd×d Vj ∈Rd×c\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\n\n\nXiUjX⊤\n\ni XiVj\n\n +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥Uj∥2\n\nF + ∥Vj∥2\n\nF\n\n(29)\n\nIn addition to inheriting all of the results of Theorems 3.3, 3.4, and 3.5, noting the equivalence of the BM factorization with the original non-convex program (9), we are the first to show conditions under which there are no spurious local minima for self-attention networks.\n\nCorollary 3.8.1. The linear-activation self-attention network (29) has no spurious local minima as long as the number of heads satisfies m ≥ m∗ where m∗ ≤ min{d2, dc}. Furthermore, for any twice-differentiable objective F , if for any local minimum ( ˆUj, ˆVj)m\n\nj=1 of (29), the matrix\n\nˆR :=\n\n(cid:20)vec( ˆU1) vec( ˆV1)\n\n· · · · · ·\n\n(cid:21)\n\nvec( ˆUm) vec( ˆVm)\n\n∈ Rd(d+c)×m\n\n(30)\n\nis rank-deficient, then this local minimum is also a global minimum of (10).\n\n4 EXPERIMENTAL RESULTS: THE RELATIVE OPTIMALITY GAP BOUND\n\n(a) n = 45\n\n(b) n = 150\n\nFigure 1: Example of three-class spiral dataset, with different number of samples n.\n\nIn this section, we illustrate the utility of our proposed relative optimality bound for stationary points in the setting of two-layer fully-connected networks. We also seek to examine how this bound changes with respect to the number of samples n, the regularization parameter β (which controls the sparsity of the convex solution), and the number of factors in the BM factorization\n\n2We examine gated ReLU and ReLU activations for self-attention in the Appendix.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a) n = 15\n\n(b) n = 45\n\n(c) n = 75\n\n(d) n = 150\n\nFigure 2: Relative optimality gap of the non-convex BM factorization of a gated-ReLU two-layer MLP for three-class spiral data classification (d = 2, c = 3). For fixed values of n, we demonstrate how β and m affect relative optimality gap, both in in terms of the proposed bound and the actual gap, where the global minimum is determined by convex optimization.\n\nm. We initialize a class-balanced three-class spiral data set with varied number of samples n (see Figure 1 for examples). For this dataset, we then train the gated ReLU MLP BM factorization (20) with varying number of factors m. We then compare the stationary points of these BM factorizations found by gradient descent (GD) to the global optimum, which we compute from (6).\n\nFor each stationary point of the BM factorization, we compute the relative optimality gap bound provided in our result in Theorem 3.4. We note that since d = 2, c = 3 in this case, for all j, rank(Z∗ j ) ≤ 2, so as long as m ≥ 2 all local minima of the BM factorization are global minima (Burer & Monteiro, 2005; Haeffele et al., 2014). While Lee et al. (2016) demonstrated that gradient descent with a random initialization converges to a local optimum almost surely for losses f whose gradient is Lipschitz continuous, we use squared loss with one-hot-encoded class labels, for which f is not Lipschitz continuous (Mukkamala & Ochs, 2019). Thus, there is no guarantee that GD will find the global minimum. We display results over β for each fixed n in Figure 2. For larger values of β, it becomes much easier for GD to find an optimal solution. We nevertheless find that our bound gives a useful proxy for whether the BM factorization has converged to the global minimum.\n\nWe find that GD applied to the BM factorization finds “subtle\" saddle points: not quite local minima, but close. Interestingly, there is only a minor relationship between the optimality gap and the rank of the BM factorization m. While our optimality gap bound for m = 1 is larger than larger values of m for small β, the actual optimality gap is nearly identical across m. This experiment further validates the need to consider stationary points of the BM factorization, rather than just local minima, to fully characterize the BM factorization for efficient solutions to convex problems.\n\n5 CONCLUSION\n\nWe are the first to adapt the Burer-Monteiro (BM) factorization for two-layer convex neural networks with linear and ReLU activations, which offers new insights on their global optima. We provide a novel relative optimality bound on stationary point of the BM factorization, which provides a condition whose satisfaction guarantees a globally optimal solution.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nErling D Andersen and Knud D Andersen. The mosek interior point optimizer for linear programming: an implementation of the homogeneous algorithm. In High performance optimization, pp. 197–232. Springer, 2000.\n\nSanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International Conference on Machine Learning, pp. 322–332. PMLR, 2019.\n\nFrancis Bach, Julien Mairal, and Jean Ponce. Convex sparse matrix factorizations. arXiv preprint\n\narXiv:0812.1869, 2008.\n\nYatong Bai, Tanmay Gautam, and Somayeh Sojoudi. Efficient global optimization of two-layer relu networks: Quadratic-time algorithms and adversarial training. arXiv preprint arXiv:2201.01965, 2022.\n\nBurak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex optimization of polynomial activation neural networks in fully polynomial-time. arXiv preprint arXiv:2101.02429, 2021.\n\nEugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale\n\nto imagenet. In International conference on machine learning, pp. 583–593. PMLR, 2019.\n\nShobhit Bhatnagar, Deepanway Ghosal, and Maheshkumar H Kolekar. Classification of fashion article images using convolutional neural networks. In 2017 Fourth International Conference on Image Information Processing (ICIIP), pp. 1–6. IEEE, 2017.\n\nNicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex burer-monteiro approach works on smooth semidefinite programs. Advances in Neural Information Processing Systems, 29, 2016.\n\nSamuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite\n\nprograms via low-rank factorization. Mathematical Programming, 95(2):329–357, 2003.\n\nSamuel Burer and Renato DC Monteiro. Local minima and convergence in low-rank semidefinite\n\nprogramming. Mathematical programming, 103(3):427–444, 2005.\n\nRicardo Cabral, Fernando De la Torre, João P Costeira, and Alexandre Bernardino. Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition. In Proceedings of the IEEE international conference on computer vision, pp. 2488–2495, 2013.\n\nEmmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foun-\n\ndations of Computational mathematics, 9(6):717–772, 2009.\n\nEmmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix\n\ncompletion. IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.\n\nDiego Cifuentes and Ankur Moitra. Polynomial time guarantees for the burer-monteiro method.\n\narXiv preprint arXiv:1912.01745, 2019.\n\nHadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with stochastic gradients. In International Conference on Machine Learning, pp. 1155–1164. PMLR, 2018.\n\nYash Deshpande, Andrea Montanari, and Emile Richard. Cone-constrained principal component\n\nanalysis. Advances in Neural Information Processing Systems, 27, 2014.\n\nSteven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex\n\noptimization. The Journal of Machine Learning Research, 17(1):2909–2913, 2016.\n\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\n\npreprint arXiv:1605.08803, 2016.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nSimon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic\n\nactivation. In International conference on machine learning, pp. 1329–1338. PMLR, 2018.\n\nSimon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on Machine Learning, pp. 1339–1348. PMLR, 2018.\n\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675–1685. PMLR, 2019.\n\nSimon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient descent can take exponential time to escape saddle points. Advances in neural information processing systems, 30, 2017.\n\nMurat A Erdogdu, Asuman Ozdaglar, Pablo A Parrilo, and Nuri Denizcan Vanli. Convergence rate of block-coordinate maximization burer–monteiro method for solving large sdps. Mathematical Programming, pp. 1–39, 2021.\n\nTolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization of two-and three-layer networks in polynomial time. In International Conference on Learning Representations, 2020.\n\nTolga Ergen, Arda Sahiner, Batu Ozturkler, John M Pauly, Morteza Mardani, and Mert Pilanci. Demystifying batch normalization in relu networks: Equivalent convex optimization models and implicit regularization. In International Conference on Learning Representations, 2021.\n\nJonathan Fiat, Eran Malach, and Shai Shalev-Shwartz. Decoupling gating from linearity. arXiv\n\npreprint arXiv:1906.05032, 2019.\n\nRong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. In Conference on learning theory, pp. 797–842. PMLR, 2015.\n\nRong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In International Conference on Machine Learning, pp. 1233–1242. PMLR, 2017.\n\nNicolas Gillis. Introduction to nonnegative matrix factorization. arXiv preprint arXiv:1703.00663,\n\n2017.\n\nBenjamin Haeffele, Eric Young, and Rene Vidal. Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing. In International conference on machine learning, pp. 2007–2015. PMLR, 2014.\n\nBenjamin D Haeffele and René Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331–7339, 2017.\n\nChi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International Conference on Machine Learning, pp. 1724–1732. PMLR, 2017.\n\nKenji Kawaguchi. Deep learning without poor local minima. Advances in neural information\n\nprocessing systems, 29, 2016.\n\nSerhat Kiliçarslan and Mete Celik. Rsigelu: A nonlinear activation function for deep neural networks.\n\nExpert Systems with Applications, 174:114805, 2021.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nJason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only\n\nconverges to minimizers. In Conference on learning theory, pp. 1246–1257. PMLR, 2016.\n\nJan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in statistics and\n\neconometrics. John Wiley & Sons, 2019.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMorteza Mardani, Gonzalo Mateos, and Georgios B Giannakis. Decentralized sparsity-regularized rank minimization: Algorithms and applications. IEEE Transactions on Signal Processing, 61(21): 5374–5388, 2013.\n\nMorteza Mardani, Gonzalo Mateos, and Georgios B Giannakis. Subspace learning and imputation for streaming big data matrices and tensors. IEEE Transactions on Signal Processing, 63(10): 2663–2677, 2015.\n\nAaron Mishkin, Arda Sahiner, and Mert Pilanci. Fast convex optimization for two-layer relu networks: Equivalent model classes and cone decompositions. In Proceedings of the 39th International Conference on Machine Learning, 2022.\n\nMahesh Chandra Mukkamala and Peter Ochs. Beyond alternating updates for matrix factorization with inertial bregman proximal gradient algorithms. Advances in Neural Information Processing Systems, 32, 2019.\n\nDohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the burer-monteiro approach. In Artificial Intelligence and Statistics, pp. 65–74. PMLR, 2017.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32: 8026–8037, 2019.\n\nMert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks. In International Conference on Machine Learning, pp. 7695–7705. PMLR, 2020.\n\nBenjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear\n\nmatrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.\n\nJasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pp. 713–719, 2005.\n\nArda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John M Pauly, Morteza Mardani, and Mert Pilanci. Hidden convexity of wasserstein gans: Interpretable generative models with closed-form solutions. In International Conference on Learning Representations, 2021a.\n\nArda Sahiner, Tolga Ergen, John M Pauly, and Mert Pilanci. Vector-output relu neural network problems are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms. In ICLR, 2021b.\n\nArda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John M Pauly. Convex regulariza-\n\ntion behind neural reconstruction. In ICLR, 2021c.\n\nArda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Unraveling attention via convex duality: Analysis and interpretations of vision transformers. In Proceedings of the 39th International Conference on Machine Learning, 2022.\n\nAlexander Shapiro. Semi-infinite programming, duality, discretization and optimality conditions.\n\nOptimization, 58(2):133–161, 2009.\n\nRichard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics, 13\n\n(389-496):24, 2004.\n\nJu Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint\n\narXiv:1510.06096, 2015.\n\nIrene Waldspurger and Alden Waters. Rank optimality for the burer–monteiro factorization. SIAM\n\njournal on Optimization, 30(3):2577–2602, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nLingxiao Wang, Xiao Zhang, and Quanquan Gu. A unified computational and statistical framework for nonconvex low-rank matrix estimation. In Artificial Intelligence and Statistics, pp. 981–990. PMLR, 2017.\n\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking\n\nmachine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\n\nTian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix\n\nfactorization. Advances in Neural Information Processing Systems, 34, 2021.\n\nQinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using\n\nburer-monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016.\n\nZhihui Zhu, Qiuwei Li, Gongguo Tang, and Michael B Wakin. The global optimization geometry of\n\nlow-rank matrix optimization. arXiv preprint arXiv:1703.01256, 2017.\n\nLiu Ziyin, Botao Li, James B Simon, and Masahito Ueda. Sgd can converge to local maxima. In\n\nInternational Conference on Learning Representations, 2021.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS\n\nA.1 RESULT OF (BURER & MONTEIRO, 2005) AND ITS APPLICATIONS TO RECTANGULAR\n\nMATRICES\n\nIn this subsection, we outline the precise theoretical statement of (Burer & Monteiro, 2005) and describe exactly how it corresponds to our summary in Section 2.2, and thus the application to the later derivations in our work. We first describe the following result, without proof, from (Burer & Monteiro, 2005). Lemma A.1 (Lemma 2.1 of (Burer & Monteiro, 2005)). Suppose R ∈ R(d+c)×r, S ∈ R(d+c)×r satisfy RR⊤ = SS⊤. Then, S = RQ for some orthogonal Q ∈ Rr×r.\n\nNow, we proceed to prove analogs of Theorem 2.3 of (Burer & Monteiro, 2005) for general SDPs with a rank constraint.\n\nLemma A.2 (Analog of Lemma 2.2 of (Burer & Monteiro, 2005)). Consider the problem\n\nmin R∈R(d+c)×r\n\nF ′(RR⊤).\n\n(31)\n\nˆR is a local minimum of (31) if and only if ˆRQ is a local minimum of (31) for all orthogonal Q ∈ Rr×r.\n\nProof. Since Q is orthogonal, ( ˆRQ)( ˆRQ)⊤ = RQQ⊤R⊤ = RR⊤. Thus, ˆR′ := ˆRQ attains the same objective value, gradients, and higher order derivatives as ˆR. Thus, ˆR is a local minimum if and only if ˆR′ is a local minimum.\n\nTheorem A.3 (Analog of Theorem 2.3 of (Burer & Monteiro, 2005)). Consider the following two problems.\n\nF ′(X),\n\nmin X∈R(d+c)×(d+c) X⪰0 rank(X)≤r\n\nmin R∈R(d+c)×r\n\nF ′(RR⊤).\n\n(32)\n\n(33)\n\nThen, for any continuous function F ′, a feasible solution ˆX is a local minimizer of (32) if and only if, for ˆX = ˆR ˆR⊤, ˆR is a local minimizer of (33).\n\nProof. We follow the exact same lines as (Burer & Monteiro, 2005). By continuity of the map R → RR⊤, we know that if ˆX is a local minimizer of (32), then ˆR is a local minimizer of (33). Now, we must prove the other direction, namely that if ˆX = ˆR ˆR⊤ is not local minimizer of (32), then ˆR is not a local minimizer of (33).\n\nSuppose that ˆX is not a local minimum. By continuity of F ′, there must be a sequence of feasible solutions {Xk} of (32) converging to ˆX such that F ′(Xk) < F ′( ˆX) for all k. For each k, choose Rk such that Xk = RkRk⊤ . Since {Xk} is bounded, it follows that {Rk} is bounded and hence has a subsequence {Rk}k∈K converging to some R such that ˆX = RR⊤. Since F ′(RkRk⊤ ) = F ′(Xk) < F ′( ˆX) = F ′(RR⊤), we see that R is not a local minimum of (33). Using the fact that ˆX = ˆR ˆR⊤ = RR⊤ together with Lemmas A.1 and A.2, we conclude that ˆR is not a local minimum of (33).\n\nthen,\n\nWith this established, we now describe how this theorem applies to the setting described in Section 2.2, i.e. the rectangular matrix, non-SDP case. Lemma A.4. Consider the solution X∗ to\n\np∗\n\n1 :=\n\nmin X∈R(d+c)×(d+c) X⪰0\n\nF ′(X),\n\n14\n\n(34)\n\nUnder review as a conference paper at ICLR 2023\n\nand define m∗ := rank(X∗). Then, for any m ≥ m∗, (34) is equivalent to\n\np∗\n\n2 :=\n\nmin X∈R(d+c)×(d+c) X⪰0 rank(X)≤m\n\nF ′(X),\n\n(35)\n\ni.e. p∗\n\n1 = p∗\n\n2, and the solutions to (35) and (34) are identical.\n\nProof. Clearly, adding a rank constraint to (34) can only increase the objective value, so p∗ 2 ≥ p∗ 1. However, since the optimal solution to (34) satisfies the rank constraint for any m ≥ m∗, every optimal solution of (34) maps to a feasible point for (35) that obtains the same objective, so p∗ 2 ≤ p∗ 1. Putting these together, we have p∗\n\n1, and the solutions are identical.\n\n2 = p∗\n\nLemma A.5. Consider the solution X∗ to (34) with m∗ := rank(X∗). Then, for any convex, continuous function F ′, the global minimizer X∗ = ˆR ˆR⊤ corresponds to a local minimizer ˆR of\n\nmin R∈R(d+c)×m\n\nF ′(RR⊤),\n\n(36)\n\nas long as m ≥ m∗.\n\nProof. We simply use the result from Lemma A.4 and apply the equivalence to Theorem A.3, noting that if F ′ is convex, all local minimizers of (34) are global optimizers.\n\nLemma A.6. Consider the optimization problem\n\np∗\n\n3 := min\n\nZ∈Rd×c\n\nF (Z).\n\nDefine F ′ : R(d+c)×(d+c) → R such that\n\nF ′(\n\n(cid:20)X1 X2 X3 X4\n\n(cid:21)\n\n) := F (X2)\n\n(37)\n\n(38)\n\nfor X1 ∈ Rd×d, X2 ∈ Rd×c, X3 ∈ Rc×d, X4 ∈ Rc×c. Then, (37) is equivalent to (34), meaning that p∗\n\n3 and their solutions map to each other.\n\n1 = p∗\n\nProof. For any solution X∗ to (34), we can simply form a solution Z∗ to (37) by letting Z∗ = X∗ 2, 3. For any solution Z∗ to (37), factor Z∗ = U∗V∗⊤ e.g. with SVD. Then, let so clearly p∗ 1 ≥ p∗ (cid:21) and form a solution to (34) as X∗ = R∗R∗⊤. Clearly, X∗ ⪰ 0, so X∗ is feasible, and\n\nR∗ =\n\n(cid:20)U∗ V∗\n\nthe objective value is given by F ′(X∗) = F (U∗V∗⊤) = F (Z∗) = p∗ two together, we conclude p∗\n\n3 and the solutions of the two problems map to each other.\n\n1. Thus, p∗\n\n3 ≤ p∗\n\n1 = p∗\n\n1. Putting these\n\nLemma A.7 (Used in Section 2.2). Consider the optimization problem (37), with optimal solution Z∗ with m∗ := rank(Z∗). Then, for any convex, continuous function F ′, the solution Z∗ = ˆU ˆV⊤ corresponds to a local minimum ( ˆU, ˆV) of\n\nmin U∈Rd×m,V∈Rc×m\n\nF (UV⊤),\n\nas long as m ≥ m∗.\n\nProof. Define F ′ : R(d+c)×(d+c) → R such that (cid:20)X1 X2 X3 X4\n\nF ′(\n\n(cid:21)\n\n) := F (X2)\n\n(39)\n\n(40)\n\nfor X1 ∈ Rd×d, X2 ∈ Rd×c, X3 ∈ Rc×d, X4 ∈ Rc×c. Then, let R =\n\n(cid:21)\n\n(cid:20)U V\n\n∈ R(d+c)×m. One can\n\nre-write F (UV⊤) as F ′(RR⊤). Then, we see that (39) can be expressed as (36), and any local minimizer to (36) is a local minimizer to (39).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFurthermore, noting from Lemma A.6, any optimal solution Z∗ = ˆU ˆV⊤ of (37) is equiva-\n\nlent to any optimal solution (34) by constructing R∗ =\n\nm∗ = rank(Z∗) = rank(R∗) = X∗.\n\n(cid:21)\n\n(cid:20) ˆU ˆV\n\n, X∗ = R∗R∗⊤. Furthermore,\n\nPutting everything together, we have that a rank-m∗ global optimum Z∗ = ˆU ˆV⊤ to (37) corresponds to a rank-m∗ global optimum X∗ to (34) (Lemma A.6), which, as long as m ≥ m∗,\n\n(Lemma A.5), which is exactly the local minimum\n\ncorresponds to a local minimum ˆR =\n\n( ˆU, ˆV) of (39).\n\n(cid:21)\n\n(cid:20) ˆU ˆV\n\nA.2 RESULT OF (HAEFFELE ET AL., 2014) AND ITS APPLICATION TO RECTANGULAR\n\nMATRICES\n\nIn this subsection, we outline the precise theoretical statement of (Haeffele et al., 2014) and describe exactly how it corresponds to our summary in Section 2.2, and thus the application to the later derivations in our work. We first describe the following result, without proof, from (Haeffele et al., 2014). n → R be of the form such that Theorem A.8 (Theorem 2 of (Haeffele et al., 2014)). Let F ′ : S+ F ′(X) = G′(X) + H ′(X), where G′ is convex, twice differentiable with compact level sets, and H ′ is a proper convex function such that F ′ is lower semi-continuous. Then, if ˆR is a rank-deficient local minimizer of\n\nthen X∗ = ˆR ˆR⊤ is a global minimizer of\n\nmin R∈R(d+c)×m\n\nF ′(RR⊤),\n\nmin X∈R(d+c)×(d+c) X⪰0\n\nF ′(X).\n\n(41)\n\n(42)\n\nWe now describe how this theorem applies to the setting described in Section 2.2, i.e. the rectangular matrix, non-SDP case. Lemma A.9 (Used in Section 2.2). Let F : Rd×c → R be of the form such that F (Z) = G(Z) + H(Z), where G is convex, twice differentiable with compact level sets, and H is a proper convex\n\n(cid:21)\n\n(cid:20) ˆU ˆV\n\nfunction such that F is lower semi-continuous. Then, if ˆR =\n\nof\n\nF (UV⊤),\n\nmin U∈Rd×m V∈Rd×m\n\nthen Z∗ = ˆU ˆV⊤ is a global minimizer of\n\nmin Z∈Rd×c\n\nF (Z).\n\nProof. Define F ′ : R(d+c)×(d+c) → R such that (cid:20)X1 X2 X3 X4\n\nF ′(\n\n(cid:21)\n\n) := F (X2)\n\nis a rank-deficient local minimizer\n\n(43)\n\n(44)\n\n(45)\n\nfor X1 ∈ Rd×d, X2 ∈ Rd×c, X3 ∈ Rc×d, X4 ∈ Rc×c. Clearly, if F = G + H, where G is twice-differentiable and H is proper convex, then, F ′ = G′ + H ′ where G′ is twicedifferentiable and H ′ is proper convex. From the proof of Lemma A.7, we know that (43) is\n\nexactly the same as (41) for R =\n\n. Furthermore, we know from Lemma A.6 that any\n\nglobal minimum Z∗ of (44) corresponds to a global minimum X∗ of (42). Lastly, we know from Theorem A.8 that any rank-deficient local minimum of (41) corresponds to a global minimizer of (42).\n\n16\n\n(cid:21)\n\n(cid:20)U V\n\nUnder review as a conference paper at ICLR 2023\n\nPutting it all together, we have that a rank-deficient local minimizer ˆR =\n\n(cid:21)\n\n(cid:20) ˆU ˆV\n\nof (43) is a\n\nrank-deficient local minimizer of (41), which corresponds to a global optimizer X∗ = ˆR ˆR⊤ of (42) (Theorem A.8), which corresponds to a global optimizer Z∗ = ˆU ˆV⊤ of (44) (Lemma A.6).\n\nA.3 PROOF OF LEMMA 3.1\n\nProof. We first analyze the solution of the following optimization problem \n\n∥uj∥2\n\nC + ∥vj∥2\n\nR\n\n  s.t. UV⊤ = Z.\n\nf ∗ = min uj ,vj\n\n1 2\n\n\n\nm (cid:88)\n\nj=1\n\nWe can write this as an equivalent problem here (Bach et al., 2008; Pilanci & Ergen, 2020):\n\nf ∗ = min\n\nuj ∈BC ,vj\n\n  s.t. UV⊤ = Z.\n\n∥vj∥R\n\n\n\n\n\nm (cid:88)\n\nj=1\n\nWe can form the Lagrangian of this as\n\nf ∗ = min\n\nuj ∈BC ,vj\n\nmax R\n\n\n\n\n\nm (cid:88)\n\n  − trace (cid:0)R⊤UV⊤(cid:1) + trace (cid:0)R⊤Z(cid:1) .\n\n∥vj∥R\n\nj=1\n\nBy Sion’s minimax theorem, we can switch the maximum over R and minimum over V and minimize over V to obtain\n\nf ∗ = min u∈BC\n\nmax R\n\ntrace (cid:0)R⊤Z(cid:1) s.t.∥u⊤R∥∗\n\nR ≤ 1.\n\nAs long as m ≥ rank(Z), by Slater’s condition, we can switch the minimum and maximum (Shapiro, 2009) to obtain\n\nf ∗ = max\n\ntrace (cid:0)R⊤Z(cid:1) s.t.∥u⊤R∥∗\n\nR ≤ 1 ∀u ∈ BC.\n\nR\n\nBy the definition of dual norm, we can also write this as\n\nf ∗ = max\n\nR\n\ntrace (cid:0)R⊤Z(cid:1) s.t.u⊤Rv ≤ 1 ∀u ∈ BC, ∀v ∈ BR = ∥Z∥D.\n\nThus, with this result, we have\n\np∗ := min\n\nU∈Rdc×m V∈Rdr ×m\n\nf (U, V) +\n\nβ 2\n\nequivalently as\n\nor also as\n\np∗ = min\n\nU∈Rdc×m V∈Rdr ×m\n\nF (MUV⊤) +\n\nβ 2\n\np∗ =\n\nmin Z:rank(Z)≤m\n\nF (MZ) + min\n\nU∈Rdc×m V∈Rdr ×m UV⊤=Z\n\nwhere we now apply our previous result to obtain\n\n\n\n\n\nm (cid:88)\n\n∥uj∥2\n\nC + ∥vj∥2\n\nR\n\n\n\n ,\n\nj=1\n\n\n\n\n\nm (cid:88)\n\n∥uj∥2\n\nC + ∥vj∥2\n\nR\n\n\n\n ,\n\nj=1\n\n\n\n\n\nm (cid:88)\n\nj=1\n\nβ 2\n\n∥uj∥2\n\nC + ∥vj∥2\n\nR\n\n\n\n ,\n\np∗ =\n\nmin Z:rank(Z)≤m\n\nF (MZ) + β∥Z∥D,\n\nwhich if rank(Z∗) ≥ m is equivalent to\n\np∗ = min\n\nZ\n\nF (MZ) + β∥Z∥D.\n\n17\n\n(46)\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\n(52)\n\n(53)\n\n(54)\n\n(55)\n\n(56)\n\nUnder review as a conference paper at ICLR 2023\n\nA.4 PROOF OF THEOREM 3.3\n\nProof. We simply note that (16) is the Burer-Monteiro factorization of (18). Thus, from Lemma A.7, as long as m ≥ rank(Z∗), all local minima of (16) are global minima. Furthermore, note that (18) is composed of two components, one of which is a twice-differentiable function, and the other is a proper convex function. Thus, by Lemma A.9, all rank-deficient local minima are global minima.\n\nA.5 PROOF OF THEOREM 3.4\n\nProof. Stationary points of (16) satisfy\n\nwhere we define\n\n0 ∈ ∇Uf ( ˆU, ˆV) + β∥ ˆU∥C∂∥ ˆU∥C 0 ∈ ∇Vf ( ˆU, ˆV) + β∥ ˆV∥R∂∥ ˆV∥R,\n\n∥ ˆU∥C :=\n\nm (cid:88)\n\nj=1\n\n∥ˆuj∥C\n\nand the same for ∥ ˆV∥R. By the definition of the subgradient, this stationarity condition can be written as\n\n∃U′ s.t. trace( ˆU⊤U′) = ∥ ˆU∥C, ∥U′∥∗ ∃V′ s.t. trace( ˆV⊤V′) = ∥ ˆV∥R, ∥V′∥∗\n\nC ≤ 1, 0 = ∇Uf ( ˆU, ˆV) + β∥ ˆU∥CU′ R ≤ 1, 0 = ∇Vf ( ˆU, ˆV) + β∥ ˆV∥RV′.\n\nBy the chain rule, we have that\n\n0 = ∇ZF (M ˆZ) ˆV + β∥ ˆU∥CU′ 0 = ∇ZF (M ˆZ)⊤ ˆU + β∥ ˆV∥RV′\n\nWe now right-multiply the top equation by ˆU⊤ and the bottom equation by ˆV⊤ to obtain\n\n0 = ∇ZF (M ˆZ) ˆV ˆU⊤ + β∥ ˆU∥CU′ ˆU⊤ 0 = ∇ZF (M ˆZ)⊤ ˆU ˆV⊤ + β∥ ˆV∥RV′ ˆV⊤.\n\n(57)\n\n(58)\n\nTaking the trace, we have\n\ntrace\n\n(cid:16)\n\n∇ZF (M ˆZ)⊤ ˆZ\n\n(cid:17)\n\n= ∥ ˆU∥Ctrace\n\n(cid:16)\n\nU′ ˆU⊤(cid:17)\n\n= ∥ ˆU∥Rtrace\n\n(cid:16)\n\nV′ ˆV⊤(cid:17)\n\n.\n\n(59)\n\n−\n\n1 β\n\nNoting the definitions of U′ and V′, we have\n\n(cid:16)\n\ntrace\n\n−\n\n1 β\n\n∇ZF (M ˆZ)⊤ ˆZ\n\n(cid:17)\n\n= ∥ ˆU∥2\n\nC = ∥ ˆV∥2\n\nR\n\nFurthermore, since clearly F (M ˆZ) = f ( ˆU, ˆV), we have that\n\n(cid:16)\n\ntrace\n\n−\n\n1 β\n\n∇ZF (M ˆZ)⊤ ˆZ\n\n(cid:17)\n\n= ∥ ˆU∥2\n\nC = ∥ ˆV∥2\n\nR =\n\n1 2\n\n(∥ ˆU∥2\n\nC + ∥ ˆV∥2\n\nR) = ∥ ˆZ∥D.\n\nNow, we examine the optimality conditions for (18). In particular, we have\n\nwhich by definition of the dual norm, is equivalent to\n\np∗ = min\n\nZ\n\nF (MZ) + β∥Z∥D,\n\np∗ = min\n\nZ\n\nmax Z′∈BD∗\n\nF (MZ) + βtrace(Z⊤Z′).\n\n(60)\n\n(61)\n\n(62)\n\n(63)\n\nNow suppose we have an approximate saddle point ( ˆZ, ˆZ′) with objective value ˆp such that ∇ZF ( ˆZ)+ β ˆZ′ = 0, trace( ˆZ⊤ ˆZ′) = ∥ ˆZ∥D, and ˆZ′ ∈ (1 + ε)BD∗ for some ε ≥ 0. Let ̃Z = 1 ˆZ′. By strong\n\n1+ε\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nduality and non-negativity of F , we have\n\np∗ = max Z′∈BD∗\n\nmin Z\n\nF (MZ) + βtrace(Z⊤Z′)\n\n≥ min\n\nZ\n\nF (MZ) + βtrace(Z⊤ ̃Z)\n\n= min\n\nF (MZ) +\n\nβ 1 + ε\n\ntrace(Z⊤ ˆZ′)\n\nF (MZ) + βtrace(Z⊤ ˆZ′)\n\n(cid:17)\n\n(cid:16)\n\nmin Z\n\nˆp\n\nZ 1\n1 + ε 1\n1 + ε\n\n≥\n\n=\n\nRearranging, we have that\n\n(64)\n\n(65)\n\n(66)\n\n(67)\n\n(68)\n\nˆp − p∗\n\np∗ ≤ ε. For the assumptions of this inequality to hold, for any candidate solution ˆZ, one must satisfy ∇ZF (M ˆZ) + β ˆZ′ = 0 and trace( ˆZ⊤ ˆZ′) = ∥ ˆZ∥D. Solving the former equality for ˆZ′ = − 1 β ∇ZF (M ˆZ), we have by (61) that the latter equality is satisfied at any stationary point of the BM factorization. Lastly, for (69) to hold for a particular ε, one must have\n\n(69)\n\nso ε =\n\n(cid:16) ∥∇ZF (M ˆZ)∥∗\n\nD\n\nβ\n\nˆZ′ = −\n\n1 β\n\n∇ZF (M ˆZ) ∈ (1 + ε)BD∗ ,\n\n(cid:17)\n\n− 1\n\n, i.e.\n\n+\n\nˆp − p∗\n\np∗ ≤\n\n(cid:32)\n\n∥∇ZF (M ˆZ)∥∗ β\n\nD\n\n(cid:33)\n\n− 1\n\n+\n\nA.6 PROOF OF THEOREM 3.5\n\nProof. From the stationary point condition, we have\n\n0 = ∇ZF (M ˆZ) ˆV + β ˆU 0 = ∇ZF (M ˆZ)⊤ ˆU + β ˆV.\n\nˆU⊤∇ZF (M ˆZ) = −β ˆV⊤.\n\nFrom (73) we can obtain\n\nSubstituting this into (72), we have\n\n0 = −β ˆV⊤ ˆV + β ˆU⊤ ˆU\n\nˆV⊤ ˆV = ˆU⊤ ˆU.\n\n(70)\n\n(71)\n\n(72)\n\n(73)\n\n(74)\n\n(75)\n\n(76)\n\nThus, let r := rank( ˆV) = rank( ˆU) ≤ min{dc, dr}. We can write the compact SVD of the stationary point as\n\nˆU = LUΛR⊤ ˆV = LVΛR⊤, where LU ∈ Rdc×r, LV ∈ Rdr×r. Assume without loss of generality that dc > dr, so dr = min{dc, dr}. We have\n\n0 = ∇ZF (M ˆZ) ˆV + β ˆU 0 = ∇ZF (M ˆZ)LVΛR⊤ + βLUΛR⊤\n\n−βLU = ∇ZF (M ˆZ)LV.\n\n(77)\n\n(78)\n\n(79)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nIf r = b, LV is square and therefore unitary, so\n\n∥∇ZF (M ˆZ)∥2 = ∥∇ZF (M ˆZ)LV∥2\n\n= ∥ − βLU∥2 = β.\n\nThus, we satisfy (22) with equality.\n\nIn general, note that when r < min{dc, dr}, we have\n\n∥∇ZF (M ˆZ)∥2 = ∥∇ZF (M ˆZ)∥2∥LV∥2 ≥ ∥∇ZF (M ˆZ)LV∥2\n\n= ∥ − βLU∥2 = β,\n\n(80)\n\n(81)\n\n(82)\n\n(83)\n\n(84) (85)\n\nso (22) is a lower bound, which depends on how ∇ZF ( ˆZ) behaves when operating on vectors in null(L⊤\n\nV).\n\nA.7 PROOF OF THEOREM 3.6\n\nProof. We begin with the non-convex objective (25)\n\np∗\n\nCN N := min\n\nw1j ∈Rh W2j ∈Rc×a\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\n\n\nW2jPa(Xiw1j)+\n\n +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥w1j∥2\n\n2 + ∥W2j∥2\n\nF .\n\n(86)\n\nWe can re-write this as (Bach et al., 2008; Pilanci & Ergen, 2020)\n\np∗\n\nCN N = min\n\nw1j ∈B2 W2j ∈Rc×a\n\nWe can also re-write this as\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\n\n\nW2jPa(Xiw1j)+\n\n + β\n\nm (cid:88)\n\nj=1\n\n∥W2j∥F .\n\n(87)\n\np∗\n\nCN N = min\n\nw1j ∈B2 W2j ∈Rc×a ri\n\nn (cid:88)\n\ni=1\n\nF (ri) + β\n\nm (cid:88)\n\nj=1\n\n∥W2j∥F s.t.\n\nm (cid:88)\n\nj=1\n\nW2jPa(Xiw1j)+ = ri\n\n(88)\n\nForming the Lagrangian, we have\n\nmax vi\n\nn (cid:88)\n\ni=1\n\nF (ri) + β\n\nm (cid:88)\n\nj=1\n\n∥W2j∥F +\n\nn (cid:88)\n\ni=1\n\nv⊤\n\ni\n\n\n\n\n\nm (cid:88)\n\nW2jPa(Xiw1j)+ − ri\n\n\n\n\n\nj=1\n\np∗\n\nCN N = min\n\nw1j ∈B2 W2j ∈Rc×a ri\n\n(89) By Sion’s minimax theorem, we can swap the minimum over W2j, ri and maximum over vi, and minimize over W2j, ri to obtain\n\np∗\n\nCN N = min u∈B2\n\nmax vi\n\n−\n\nn (cid:88)\n\ni=1\n\n−F ∗(vi)\n\ns.t. ∥\n\nn (cid:88)\n\ni=1\n\nPa(Xiu)+v⊤\n\ni ∥F ≤ β\n\n(90)\n\nwhere F ∗ is the Fenchel conjugate of F . Now, as long as β > 0 and m ≥ m∗ where m∗ ≤ nac, we can switch the order of max and min by Slater’s condition (Shapiro, 2009; Sahiner et al., 2021b) to obtain\n\np∗\n\nCN N = max\n\nvi\n\nn (cid:88)\n\n−\n\n−F ∗(vi)\n\ns.t. max u∈B2\n\ni=1 n\n(cid:88)\n\n∥\n\ni=1\n\nPa(Xiu)+v⊤\n\ni ∥F ≤ β.\n\n(91)\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nEnumerating over the hyperplane arrangements {Dk}P\n\nk=1, we can further write this as\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\n−F ∗(vi)\n\ns.t.\n\nmax k∈[P ] u∈B2\n\n(2D(i)\n\nk −I)Xiu≥0\n\n∥\n\nn (cid:88)\n\ni=1\n\nPaD(i)\n\nk Xiuv⊤\n\ni ∥F ≤ β.\n\n(92)\n\nNow, noting that vec(ABC) = (C⊤ ⊗ A)vec(B) (Magnus & Neudecker, 2019), this is equivalent to\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\nF ∗(vi)\n\ns.t.\n\nmax k∈[P ] u∈B2\n\n(2D(i)\n\nk −I)Xiu≥0\n\n∥\n\nn (cid:88)\n\ni=1\n\n(vi ⊗ PaD(i)\n\nk Xi)u∥2 ≤ β\n\n(93)\n\nThis may also be written further as\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\nF ∗(vi)\n\ns.t.\n\nand thereby as\n\nmax k∈[P ] u∈B2\n\ng∈B2\n\n(2D(i)\n\nk −I)Xiu≥0\n\ng⊤\n\nn (cid:88)\n\ni=1\n\n(vi ⊗ PaD(i)\n\nk Xi)u ≤ β,\n\n(94)\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\nF ∗(vi)\n\n(cid:32) n\n\n(cid:88)\n\n(vi ⊗ PaD(i)\n\nk Xi)ug⊤\n\n(cid:33)\n\n≤ β.\n\n(95)\n\ntrace\n\ni=1\n\ns.t.\n\nmax k∈[P ] u∈B2\n\ng∈B2\n\n(2D(i)\n\nk −I)Xiu≥0\n\nNow, we let Z = ug⊤ to obtain\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\nF ∗(vi)\n\n(cid:32) n\n\n(cid:88)\n\n(vi ⊗ PaD(i)\n\nk Xi)Z\n\n(cid:33)\n\n≤ β.\n\n(96)\n\ntrace\n\ni=1\n\ns.t.\n\nmax k∈[P ] Z=ug⊤ u∈B2\n\n(2D(i)\n\nk −I)Xiu≥0\n\ng∈B2\n\nWe let Ck := conv k − I)Xiu ≥ 0, u ∈ B2, g ∈ B2 is linear we can take the convex hull of the constraints without changing the objective, to obtain\n\nand note that since our objective\n\n(cid:110)\n\nug⊤ : (2D(i)\n\n(cid:111)\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\nF ∗(vi)\n\ntrace\n\ns.t. max k∈[P ] Z∈Ck\n\n(cid:32) n\n\n(cid:88)\n\ni=1\n\n(vi ⊗ PaD(i)\n\nk Xi)Z\n\n≤ β.\n\n(cid:33)\n\n(97)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nNote that the constraint Z ∈ Ck is equivalent to stating that ∥Z∥∗,Kk ≤ 1 for the constrained nuclear norm definition with Kk = (2Dk − I)X. Then, we have\n\np∗\n\nCN N = max\n\nvi\n\n−\n\nn (cid:88)\n\ni=1\n\nF ∗(vi)\n\ns.t. max k∈[P ] ∥Z∥∗,Kk ≤1\n\ntrace\n\nNow, we form the Lagrangian, given by\n\n(cid:32) n\n\n(cid:88)\n\n(vi ⊗ PaD(i)\n\nk Xi)Z\n\n(cid:33)\n\n≤ β.\n\n(98)\n\ni=1\n\np∗\n\nCN N = max\n\nvi\n\nmin ∥Zk∥∗,Kk ≤1\n\nmin λk≥0\n\n−\n\nn (cid:88)\n\nF ∗(vi)+\n\nP (cid:88)\n\n(cid:32)\n\nλk\n\nβ −\n\nn (cid:88)\n\nvec(Zk)⊤vec\n\n(cid:16)\n\nv⊤\n\ni ⊗ (PaD(i)\n\nk Xi)⊤(cid:17)\n\ni=1\n\nk=1\n\ni=1\n\n(99) By Sion’s minimax theorem, we are permitted to change the order of the maxima and minima, to obtain\n\np∗\n\nCN N = min λk≥0\n\nmin ∥Zk∥∗,Kk ≤1\n\nmax vi\n\n−\n\nn (cid:88)\n\nF ∗(vi)+\n\nP (cid:88)\n\n(cid:32)\n\nλk\n\nβ −\n\nn (cid:88)\n\nvec(Zk)⊤vec\n\n(cid:16)\n\nv⊤\n\ni ⊗ (PaD(i)\n\nk Xi)⊤(cid:17)\n\ni=1\n\nk=1\n\ni=1\n\n(100) Now, defining Ka,1 as the (a, 1) commutation matrix we have the following identity from (Magnus & Neudecker, 2019):\n\n(cid:33)\n\n(cid:33)\n\n.\n\n.\n\n(cid:16)\n\nvec\n\nv⊤\n\ni ⊗ (PaD(i)\n\nk Xi)⊤(cid:17)\n\n(cid:16)\n\n=\n\nIc ⊗\n\n(cid:16)\n\n(Ka,1 ⊗ Ih)(vec(X⊤\n\ni D(i)\n\nk P⊤ a )\n\n(cid:17)(cid:17)\n\nvec(vi).\n\nUsing this identity and maximizing over vi, we obtain\n\np∗\n\nCN N = min\n\n∥Zk∥∗,Kk ≤1\n\nn (cid:88)\n\nF\n\n(cid:32) P\n\n(cid:88)\n\ni=1\n\nk=1\n\nmin λk≥0\n\n(cid:16)\n\nIc ⊗\n\n(cid:16)\n\nvec(X⊤\n\ni D(i)\n\nk P⊤\n\na )⊤(K1,a ⊗ Ih)\n\n(cid:17)(cid:17)\n\nRescaling such that ̃Zk = λkZk, we obtain\n\n(cid:33)\n\nvec(Zk)\n\n+β\n\nP (cid:88)\n\nλk.\n\nk=1\n\n(101)\n\np∗\n\nCN N = min\n\nZk∈Rh×ac\n\nn (cid:88)\n\nF\n\n(cid:32) P\n\n(cid:88)\n\n(cid:16)\n\n(cid:16)\n\nIc ⊗\n\ni=1\n\nk=1\n\nvec(X⊤\n\ni D(i)\n\nk P⊤\n\na )⊤(K1,a ⊗ Ih)\n\n(cid:17)(cid:17)\n\n(cid:33)\n\nvec(Zk)\n\n+β\n\nP (cid:88)\n\n∥Zk∥∗,Kk .\n\nk=1\n\n(102)\n\nSimplifying further, we can write this as\n\np∗\n\nCN N = min\n\nZk∈Rh×ac\n\n\n\n \n\nP (cid:88)\n\nk=1\n\nn (cid:88)\n\ni=1\n\nF\n\n\n\n \n\ntrace(PaD(i)\n\nk XiZ(1) k )\n\n\n\n\n\ntrace(PaD(i)\n\nk XiZ(c) k )\n\n \n\n  + β\n\nP (cid:88)\n\nk=1\n\n∥Zk∥∗,Kk ,\n\n(103)\n\nwhere Z(c′)\n\nk ∈ Rh×a.\n\nA.8 PROOF OF LEMMA 3.7\n\nProof. We start with the convex formulation\n\np∗\n\nRCN N = min\n\nZk∈Rh×ac\n\nn (cid:88)\n\ni=1\n\nF\n\n\n\n \n\nP (cid:88)\n\nk=1\n\n\n\n \n\n...\n\n...\n\ntrace(PaD(i)\n\nk XiZ(1) k )\n\n\n\n\n\ntrace(PaD(i)\n\nk XiZ(c) k )\n\n22\n\n \n\n  + β\n\nP (cid:88)\n\nk=1\n\n∥Zk∥∗,Kk .\n\n(104)\n\nUnder review as a conference paper at ICLR 2023\n\nIn order to compute the Burer-Monteiro factorization, we factor Zk = UkV⊤ Vk ∈ Rac×m, and (2D(i)\n\nk − I)XiUk ≥ 0. Then, with V(c′)\n\nk ∈ Ra×m. Then for each k,\n\nk , where Uk ∈ Rh×m,\n\n\n\n \n\ntrace(PaD(i)\n\nk XiZ(1) k )\n\n\n\n...\n\n  =\n\ntrace(PaD(i)\n\nk XiZ(c) k )\n\nk XiUkV(1)\n\nk\n\n⊤\n\n⊤\n\n )\n \n\n\n)\n\n\n\n \n\n\n\n\ntrace(PaD(i) ... trace(PaD(i)\n\ntrace(V(1)\n\nk\n\n=\n\n \n\n\n=\n\n \n\n\ntrace(V(c)\n\nk\n\n\n\n(cid:80)m\n\nj=1 v(1)\n\njk\n\nk XiUk)\n\n⊤\n\nk\n\nk XiUkV(c) PaD(i) ... PaD(i)\n\n⊤\n\n\n\n \n\n\nk XiUk) \n\nk Xiujk\n\n⊤\n\n⊤\n\nPaD(i) ... PaD(i)\n\n \n\n\n(cid:80)m\n\nj=1 v(c)\n\njk\n\nk Xiujk\n\n=\n\nm (cid:88)\n\nj=1\n\nV⊤\n\njkPaD(i)\n\nk Xiujk,\n\nwhere\n\nV⊤\n\njk :=\n\n\n\n\n\n ∈ Rc×a.\n\n(105)\n\n\n\n \n\n⊤\n\n⊤\n\nv(1) jk · · · v(c)\n\njk\n\nThe equivalent Burer-Monteiro formulation thus is given by\n\np∗\n\nRCN N =\n\nmin {{ujk∈Rh}m {{Vjk∈Rc×a}m\n\nj=1}P k −I)Xiujk≥0\n\nk=1\n\n(2D(i)\n\nj=1}P\n\nk=1\n\n\n\nF\n\n\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\nn (cid:88)\n\ni=1\n\n\n\nVjkPaD(i)\n\nk Xiujk\n\n+\n\nβ 2\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\n(cid:0)∥ujk∥2\n\nF + ∥Vjk∥2\n\nF\n\n(cid:1) .\n\n(106)\n\nA.9 PROOF OF COROLLARY 3.7.1\n\nProof. We simply apply the result of Theorem 3.4, noting that stationary points correspond to global minima if the norm of the gradient is less than β. Thus, this condition is equivalent to\n\n∥\n\nn (cid:88)\n\ni=1\n\n∇Zk F\n\n\n\n \n\nP (cid:88)\n\nk′=1\n\n\n\n \n\ntrace(PaD(i)\n\nk′ XiZ(1) k′ )\n\n\n\n\n\ntrace(PaD(i)\n\nk′ XiZ(c) k′ )\n\n...\n\n \n\n\n\n u∥2 ≤ β, ∀k ∈ [P ], ∀u ∈ B2 : (2D(i)\n\nk −I)Xiu ≥ 0.\n\n(107)\n\nA.10 PROOF OF LEMMA 3.8\n\nProof. We begin from the convex formulation (10):\n\np∗\n\nLSA = min\n\nZ∈Rd2×dc\n\nn (cid:88)\n\ni=1\n\nF\n\n(cid:32) d\n\n(cid:88)\n\nd (cid:88)\n\nk=1\n\nl=1\n\n(cid:33)\n\nGi[k, l]XiZ(k,l)\n\n+ β∥Z∥∗.\n\n(108)\n\nNow, we seek to find the Burer-Monteiro factorization. We let Z = UV⊤, where U ∈ Rd2×m and V ∈ Rdc×m. Let vec−1(uj) ∈ Rd×d be the result of taking chunks of d-length vectors from uj for j ∈ [m] and stacking them in columns. Similarly, let vec−1(vj) ∈ Rc×d be the result of\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\ntaking chunks of c-length vectors from vj and stacking them in columns. Furthermore, we will let vec−1(uj)k be the kth column of vec−1(uj). Then, recognize that\n\nZ(k,l) =\n\nm (cid:88)\n\nj=1\n\nvec−1(uj)kvec−1(vj)⊤ l .\n\nThus,\n\nd (cid:88)\n\nd (cid:88)\n\nk=1\n\nl=1\n\nGi[k, l]XiZ(k,l) =\n\nd (cid:88)\n\nd (cid:88)\n\nm (cid:88)\n\nk=1\n\nl=1\n\nj=1\n\nGi[k, l]Xivec−1(uj)kvec−1(vj)⊤\n\nl\n\n= Xi\n\nd (cid:88)\n\nd (cid:88)\n\nm (cid:88)\n\nk=1\n\nl=1\n\nj=1\n\nvec−1(uj)kGi[k, l]vec−1(vj)⊤\n\nl\n\n= Xi\n\nm (cid:88)\n\nj=1\n\nvec−1(uj)Givec−1(vj)⊤\n\n=\n\nm (cid:88)\n\nj=1\n\nXivec−1(uj)X⊤\n\ni Xivec−1(vj)⊤.\n\nNow, overloading notation, let vec−1(uj) = Uj and vec−1(vj)⊤ = Vj. We have clearly that the Burer-Monteiro factorization of (10) is given by\n\np∗\n\nLSA = min\n\nUj ∈Rd×d Vj ∈Rd×c\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\n\n\nXiUjX⊤\n\ni XiVj\n\n +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n∥Uj∥2\n\nF + ∥Vj∥2 F .\n\n(109)\n\nA.11 PROOF OF COROLLARY 3.8.1\n\nProof. We simply nee dto apply the result of 3.3 to this setting. In this case, the non-convex linear selfattention network is equivalent to the Burer-Monteiro factorization of the convex form. To obtain this Burer-Monteiro factorization, we factorize convex weights Z ∈ Rd2×dc, so rank(Z∗) ≤ min{d2, dc}. Thus, letting m∗ = rank(Z∗) ≤ min{d2, dc}, we can observe that as long as the number of heads m\n\nexceeds m∗, from Lemma A.7, all local optima are global. Further, we can form ˆR =\n\n, and as\n\n(cid:21)\n\n(cid:20) ˆU ˆV\n\nlong as this is a rank-deficient local minimum, it also corresponds to a global minimum when F is twice-differentiable by Lemma A.9.\n\nB ADDITIONAL THEORETICAL RESULTS\n\nB.1 MLPS\n\nThe following theorem demonstrates that we can extend the results of (Sahiner et al., 2021b) beyond simply weight-decay regularization, and to arbitrary regularization. Theorem B.1. The non-convex ReLU training objective\n\np∗ := min\n\nw1j ,w2j\n\nm (cid:88)\n\nF (\n\nj=1\n\n(Xw1j)+w2j) +\n\nβ 2\n\nis equivalent to the convex training objective\n\np∗ = min Zj\n\nP (cid:88)\n\nF (\n\nj=1\n\nDjXZj) + β\n\n\n\n\n\nm (cid:88)\n\n∥w1j∥2\n\nC + ∥w2j∥2\n\nR\n\nj=1\n\nP (cid:88)\n\nj=1\n\n∥Zj∥Dj ,\n\n\n\n\n\n(110)\n\n(111)\n\nas long as β > 0 and m ≥ m∗ where m∗ ≤ nc, where\n\n∥Z∥Dj := max\n\nR\n\ntrace(R⊤Z) s.t. u⊤Rv ≤ 1 ∀u ∈ BC : (2Dj − In)Xu ≥ 0, ∀v ∈ BR.\n\n(112)\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We start by re-stating the convex objective (Bach et al., 2008; Pilanci & Ergen, 2020)\n\np∗ := min\n\nw1j ∈BC w2j\n\nm (cid:88)\n\nF (\n\nj=1\n\n(Xw1j)+w2j) + β\n\nm (cid:88)\n\nj=1\n\n∥w2j∥R.\n\n(113)\n\nThen, we can re-write this in a constrained form\n\np∗ = min\n\nw1j ∈BC w2j R\n\nF (R) + β\n\nm (cid:88)\n\nj=1\n\n∥w2j∥R s.t.\n\nm (cid:88)\n\n(Xw1j)+w2j = R,\n\n(114)\n\nj=1\n\nand then the Lagrangian\n\np∗ = min\n\nw1j ∈BC w2j R\n\nmax V\n\nF (R) + β\n\nm (cid:88)\n\nj=1\n\n∥w2j∥R + trace\n\n V⊤\n\n\n\n\n\nm (cid:88)\n\n(Xw1j)+w2j − R\n\n\n\n\n\n\n\n .\n\n(115)\n\nj=1\n\nBy Sion’s minimax theorem, we can swap the order of the maximization over V and minimization over w2j and R. Then, minimizing over these two, we have\n\np∗ = min u∈BC\n\nmax V\n\n−F ∗(V) s.t.∥V⊤(Xu)+∥∗\n\nR ≤ β.\n\n(116)\n\nBy Slater’s condition, which holds when β > 0 and m ≤ m∗ where m∗ ≤ nc (Shapiro, 2009; Sahiner et al., 2021c), we can switch the order of minimum and maximum to obtain\n\np∗ = max\n\nV\n\n−F ∗(V) s.t. max u∈BC\n\n∥V⊤(Xu)+∥∗\n\nR ≤ β.\n\nIntroducing hyperplane arrangements, we have\n\np∗ = max\n\nV\n\n−F ∗(V) s.t.\n\nmax j∈[P ] u∈BC (2Dj −I)Xu≥0\n\n∥V⊤DjXu∥∗\n\nR ≤ β.\n\n(117)\n\n(118)\n\nBy the concept of dual norm, this is equivalent to\n\np∗ = max\n\nV\n\n−F ∗(V) s.t.\n\nmax j∈[P ] u∈BC (2Dj −I)Xu≥0 g∈Br\n\nDefine\n\ntrace (cid:0)V⊤DjXug⊤(cid:1) ≤ β.\n\n(119)\n\n∥Z∥j := max t≥0\n\nt s.t. Z ∈ tconv{ug⊤ : u ∈ BC, (2Dj − I)Xu ≥ 0, g ∈ Br}.\n\n(120)\n\nThen, we can write our problem as\n\np∗ = max\n\nV\n\n−F ∗(V) s.t. max\n\nj∈[P ] ∥Z∥j ≤1\n\ntrace (cid:0)V⊤DjXZ(cid:1) ≤ β.\n\n(121)\n\nNow, observe that ∥Z∥j = ∥Z∥Dj . In particular, let us examine (112), which we can re-write as\n\n∥Z∥Dj = max\n\nR\n\ntrace(R⊤Z) s.t. max ∥Z∥j ≤1\n\ntrace(Z⊤R) ≤ 1\n\ntrace(R⊤Z) s.t. ∥R∥∗\n\nj ≤ 1\n\n= max\n\nR = ∥Z∥j,\n\n(122)\n\n(123)\n\n(124)\n\nwhere the simplifications are made noting the definition of the dual norm. Now, we can write our objective as\n\ntrace (cid:0)V⊤DjXZ(cid:1) ≤ β.\n\n(125)\n\np∗ = max\n\nV\n\n−F ∗(V) s.t. max\n\nj∈[P ] ∥Z∥Dj ≤1\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nForming the Largrangian, we have\n\np∗ = max\n\nV\n\nmin ∥Zj ∥Dj ≤1 λj ≥0\n\n−F ∗(V) +\n\nP (cid:88)\n\nj=1\n\n(cid:0)β − trace (cid:0)V⊤DjXZj\n\n(cid:1)(cid:1) .\n\nλj\n\n(126)\n\nBy Sion’s minimax theorem, we can switch max and min and solve over V to obtain\n\np∗ = min\n\n∥Zj ∥Dj ≤1 λj ≥0\n\nP (cid:88)\n\nF (\n\nj=1\n\nλjDjXZj) + β\n\nP (cid:88)\n\nj=1\n\nλj.\n\nLastly, we can combine Zjλj into one variable to obtain\n\np∗ = min Zj\n\nP (cid:88)\n\nF (\n\nj=1\n\nDjXZj) + β\n\nP (cid:88)\n\nj=1\n\n∥Zj∥Dj ,\n\n(127)\n\n(128)\n\nas desired.\n\nB.2 CNNS\n\nLemma B.2. The Burer-Monteiro factorization of the convex CNN problem with linear and gated ReLU activation are given as follows.\n\np∗\n\nLCN N =\n\np∗\n\nGCN N =\n\nn (cid:88)\n\nF\n\n\n\n\n\nm (cid:88)\n\ni=1\n\nj=1\n\n\n\nVjPaXiuj\n\n +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\n(cid:0)∥uj∥2\n\n2 + ∥Vj∥2\n\nF\n\n(cid:1)\n\n(129)\n\nmin {uj ∈Rh}m {Vj ∈Rc×a}m\n\nj=1\n\nj=1\n\nmin {{ujk∈Rh}m {{Vjk∈Rc×a}m\n\nj=1}P\n\nk=1\n\nj=1}P\n\nk=1\n\n\n\nF\n\n\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\nn (cid:88)\n\ni=1\n\n\n\nVjkPaD(i)\n\nk Xiujk\n\n +\n\nβ 2\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\n(cid:0)∥ujk∥2\n\nF + ∥Vjk∥2\n\nF\n\n(cid:1)\n\nProof. The proofs follow almost identically from the proof of 3.7. In the linear case, the convex objective is given by\n\n(130)\n\np∗\n\nLCN N = min\n\nZ∈Rh×ac\n\nn (cid:88)\n\ni=1\n\nF\n\n\n\n\n\n\n\n\n\n \n\n \n\ntrace(PaXiZ(1)) ... trace(PaXiZ(c))\n\n \n\n  + β∥Z∥∗.\n\n(131)\n\nIn order to compute the Burer-Monteiro factorization, we factor Z = UV⊤, where U ∈ Rh×m, V ∈ Rac×m. Then, with V(c′) ∈ Ra×m. Then,\n\n\n\n \n\ntrace(PaXiZ(1)) ... trace(PaXiZ(c))\n\n\n\n  =\n\n=\n\n\n\n \n\n\n\n\n \n\n\ntrace(PaXiUV(1)⊤ ... trace(PaXiUV(c)⊤ trace(V(1)⊤ ... trace(V(c)⊤\n\nPaXiU)\n\n )\n \n\n\n)\n\n\n\n \n\n\nPaXiU) \n\nPaXiuj\n\n \n\n\nPaXiuj\n\n\n\n(cid:80)m\n\n⊤\n\n⊤\n\nj\n\nj=1 v(1) ... j=1 v(c)\n\nj\n\n(cid:80)m\n\n=\n\n \n\n\nV⊤\n\nj PaXiuj,\n\n=\n\nm (cid:88)\n\nj=1\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nV⊤\n\nj :=\n\n\n\n\n\n ∈ Rc×a.\n\n\n\n \n\n⊤\n\n⊤\n\nv(1) j\n· · · v(c)\n\nj\n\nThe equivalent Burer-Monteiro formulation thus is given by\n\np∗\n\nLCN N =\n\nmin {uj ∈Rh}m {Vj ∈Rc×a}m\n\nj=1\n\nj=1\n\n\n\nF\n\n\n\nm (cid:88)\n\nj=1\n\nn (cid:88)\n\ni=1\n\n\n\nVjPaXiuj\n\n +\n\nβ 2\n\nm (cid:88)\n\nj=1\n\nIn the gated ReLU case, the convex program is given by\n\n(cid:0)∥uj∥2\n\n2 + ∥Vj∥2\n\nF\n\n(132)\n\n(cid:1) .\n\n(133)\n\np∗\n\nGCN N = min\n\nZk∈Rh×ac\n\nn (cid:88)\n\ni=1\n\nF\n\n\n\n \n\nP (cid:88)\n\nk=1\n\n\n\n \n\ntrace(PaD(i)\n\nk XiZ(1) k )\n\n\n\n\n\n...\n\n \n\n  + β\n\nP (cid:88)\n\nk=1\n\n∥Zk∥∗.\n\n(134)\n\ntrace(PaD(i)\n\nk XiZ(c) k )\n\nIn order to compute the Burer-Monteiro factorization, we factor Zk = UkV⊤ Vk ∈ Rac×m. Then, with V(c′)\n\nk ∈ Ra×m. Then for each k,\n\nk , where Uk ∈ Rh×m,\n\n\n\n \n\ntrace(PaD(i)\n\nk XiZ(1) k )\n\n\n\n...\n\n  =\n\ntrace(PaD(i)\n\nk XiZ(c) k )\n\nk XiUkV(1)\n\nk\n\n⊤\n\n⊤\n\n )\n \n\n\n)\n\n\n\n \n\n\n\n\ntrace(PaD(i) ... trace(PaD(i)\n\ntrace(V(1)\n\nk\n\n=\n\n \n\n\n=\n\n \n\n\ntrace(V(c)\n\nk\n\n\n\n(cid:80)m\n\nj=1 v(1)\n\njk\n\nk XiUk)\n\n⊤\n\nk\n\nk XiUkV(c) PaD(i) ... PaD(i)\n\n⊤\n\n\n\n \n\n\nk XiUk) \n\nk Xiujk\n\n⊤\n\n⊤\n\nPaD(i) ... PaD(i)\n\n \n\n\n(cid:80)m\n\nj=1 v(c)\n\njk\n\nk Xiujk\n\n=\n\nm (cid:88)\n\nj=1\n\nV⊤\n\njkPaD(i)\n\nk Xiujk,\n\nwhere\n\nV⊤\n\njk :=\n\n\n\n\n\n ∈ Rc×a.\n\n(135)\n\n\n\n \n\n⊤\n\n⊤\n\nv(1) jk · · · v(c)\n\njk\n\nThe equivalent Burer-Monteiro formulation thus is given by\n\np∗\n\nGCN N =\n\nmin {{ujk∈Rh}m {{Vjk∈Rc×a}m\n\nj=1}P\n\nk=1\n\nj=1}P\n\nk=1\n\n\n\nF\n\n\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\nn (cid:88)\n\ni=1\n\n\n\nVjkPaD(i)\n\nk Xiujk\n\n+\n\nβ 2\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\n(cid:0)∥ujk∥2\n\nF + ∥Vjk∥2\n\nF\n\n(cid:1) .\n\n(136)\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nB.3 SELF-ATTENTION\n\nLemma B.3. The Burer-Monteiro factorization of the convex self-attention problem with gated ReLU and ReLU activations are given as follows.\n\np∗\n\nGSA = min Ujk Vjk\n\n\n\nF\n\n\n\nP (cid:88)\n\nm (cid:88)\n\n(cid:16)\n\nk=1\n\nj=1\n\nn (cid:88)\n\ni=1\n\ndiag−1(D(i)\n\nk ) ⊙ (XiUjkX⊤ i )\n\nP (cid:88)\n\nm (cid:88)\n\n∥Ujk∥2\n\nF + ∥Vjk∥2\n\nF\n\n+\n\nβ 2\n\nk=1\n\nj=1 \n\np∗\n\nRSA = min Ujk Vjk\n\nn (cid:88)\n\ni=1\n\nP (cid:88)\n\nm (cid:88)\n\n(cid:16)\n\ndiag−1(D(i)\n\nk ) ⊙ (XiUjkX⊤ i )\n\nF\n\n\n\nk=1\n\nj=1\n\n(cid:17)\n\n(cid:17)\n\n\n\nXiVjk\n\n\n\n\n\nXiVjk\n\n\n\n(137)\n\n(138)\n\n+\n\nβ 2\n\nP (cid:88)\n\nm (cid:88)\n\nk=1\n\nj=1\n\n∥Ujk∥2\n\nF + ∥Vjk∥2\n\nF\n\ns.t. (2diag−1(D(i)\n\nk ) − 11⊤) ⊙ (XiUjkX⊤\n\ni ) ≥ 0,\n\nwhere diag−1(D(i) form.\n\nk ) ∈ Rs×s takes elements along the diagonal of D(i)\n\nk and places them in matrix\n\nProof. Courtesy of (Sahiner et al., 2022), we first present the equivalent convex models for gated ReLU and ReLU activation self attention. First, define\n\n(cid:35)\n\nX :=\n\n(cid:34)X1 ⊗ X1 · · · Xn ⊗ Xn j=1 := {diag (1{Xu ≥ 0})}P\n\nj=1,\n\n{Dj}P\n\nthen, we have\n\np∗\n\nGSA = min\n\nZj ∈Rd2×dc\n\np∗\n\nRSA = min\n\nZj ∈Rd2×dc\n\nn (cid:88)\n\ni=1\n\nn (cid:88)\n\nL\n\nL\n\n\n\n\n\nP (cid:88)\n\nd (cid:88)\n\nd (cid:88)\n\nG(k,l)\n\ni,j XiZ(k,l)\n\nj\n\nj=1\n\nk=1\n\nl=1\n\n\n\n\n\nP (cid:88)\n\nd (cid:88)\n\nd (cid:88)\n\nG(k,l)\n\ni,j XiZ(k,l)\n\nj\n\ni=1\n\nj=1\n\nk=1\n\nl=1\n\n\n\n, Yi\n\n + β\n\n\n\n, Yi\n\n + β\n\nP (cid:88)\n\nj=1\n\nP (cid:88)\n\nj=1\n\n∥Zj∥∗,\n\n(139)\n\n∥Zj∥Kj ,∗,\n\n(140)\n\nwhere\n\nfor G(k,l)\n\ni,j ∈ Rs×s and Z(k,l)\n\nj\n\nGi,j := (Xi ⊗ Is)⊤D(i)\n\nj (Xi ⊗ Is),\n\n∈ Rd×c.\n\nWe then proceed to take the Burer-Monteiro factorization of these models. Here, we will show the Burer-Monteiro factorization of the ReLU model, noting that the proof is the same for the Gated ReLU model sans the constraints.\n\nj , where Uj ∈ Rd2×m and Vj ∈ Rdc×m, where KjUj ≥ 0. Let We let Zj = UjV⊤ vec−1(ujx) ∈ Rd×d be the result of taking chunks of d-length vectors from ujx for j ∈ [m] and stacking them in columns. Similarly, let vec−1(vjx) ∈ Rc×d be the result of taking chunks of c-length vectors from vjx and stacking them in columns. Furthermore, we will let vec−1(ujx)k be the kth column of vec−1(ujx). Then, recognize that\n\nZj\n\n(k,l) =\n\nm (cid:88)\n\nx=1\n\nvec−1(ujx)kvec−1(vjx)⊤ l .\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nThus, for each j,\n\nd (cid:88)\n\nd (cid:88)\n\nk=1\n\nl=1\n\nG(k,l)\n\ni,j XiZ(k,l)\n\nj\n\n=\n\n=\n\n=\n\n=\n\n=\n\nm (cid:88)\n\nd (cid:88)\n\nd (cid:88)\n\nG(k,l)\n\ni,j Xivec−1(ujx)kvec−1(vjx)⊤\n\nl\n\nx=1\n\nk=1\n\nl=1\n\nm (cid:88)\n\nd (cid:88)\n\nd (cid:88)\n\n(cid:104)\n\n(Xi ⊗ Is)⊤D(i)\n\nj (Xi ⊗ Is)\n\n(cid:105)(k,l)\n\nXivec−1(ujx)kvec−1(vjx)⊤\n\nl\n\nx=1\n\nk=1\n\nl=1\n\nm (cid:88)\n\nd (cid:88)\n\nd (cid:88)\n\n(Xi[·, k] ⊗ Is)⊤D(i)\n\nj (Xi[·, l] ⊗ Is)Xivec−1(ujx)kvec−1(vjx)⊤\n\nl\n\nx=1\n\nk=1\n\nl=1\n\ns (cid:88)\n\nm (cid:88)\n\nd (cid:88)\n\nd (cid:88)\n\ny=1\n\nx=1\n\nk=1\n\nl=1\n\nD(i)(y,y)\n\nj\n\nXivec−1(ujx)kvec−1(vjx)⊤\n\nl Xi[y, k]Xi[y, l]\n\nm (cid:88)\n\n(cid:16)\n\nx=1\n\ndiag−1(D(i)\n\nj ) ⊙ (XiUjxX⊤ i )\n\n(cid:17)\n\nXiVjx,\n\nwhere the constraint that (2D(i) (XiUjxX⊤\n\ni ) ≥ 0 for all x ∈ [m]. Thus, we have proven the statement.\n\nj − I)XUj ≥ 0 can also be re-written as (2diag−1(D(i)\n\nk ) − 11⊤) ⊙\n\nC EXPERIMENTAL DETAILS\n\nIn all cases, we solve the BM factorization with GD using Pytorch (Paszke et al., 2019) on a CPU with a momentum parameter of 0.9, a learning rate of 1.0 which decays by a factor of 0.9 whenever the training loss plateaus, and train for 20000 epochs such that GD always converges. For convex optimization, to determine the global optimum of each problem, we use the MOSEK interior point solver (Andersen & Andersen, 2000) with CVXPY (Diamond & Boyd, 2016). The parameters we evaluate are n ∈ [3, 15, 45, 75, 150], β ∈ [10−4, 10−3, 10−2, 10−1] and m ∈ [1, 2, 5]. We use a randomly subsampled set of ˆP = 100 hyperplane arrangements. We perform this experiment over three random seeds, which are used to generate the hyperplane arrangements as well as the random Gaussian initializations of the weights.\n\nD ADDITIONAL EXPERIMENTAL RESULTS: BM ENABLES LAYERWISE\n\nTRAINING OF CONVEX CNNS\n\n(a) CIFAR-10\n\n(b) Fashion-MNIST\n\nFigure 3: BM enables layerwise training of convex gated ReLU CNNs, which are competitive with end-to-end ReLU networks of similar depth. For CIFAR-10, we achieve a test accuracy of 80.5% compared to 81.6% for end-to-end non-convex training, and for Fashion-MNIST we achieve a test accuracy of 91.5% compared to 91.2% for end-to-end non-convex training (Kiliçarslan & Celik, 2021; Bhatnagar et al., 2017).\n\nWe also provide some additional experimental results not presented in the main paper in order to supplement our submission.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nWe consider the task of leveraging the theory of two-layer convex ReLU neural networks for training deep image classifiers. In particular, following the approach of (Belilovsky et al., 2019), we seek to train two-layer convex CNNs greedily to mimick the performance of a deep network. In the non-convex setting, the greedy approach proceeds by training a single two-layer CNN, then freezing the weights of this CNN, using the latent representation of this CNN as the input features for another two-layer CNN, and repeating this process for a specified number of stages. We leverage the result of Theorem 3.6 to convert this non-convex layerwise training procedure to a convex one, training stages of convex two-layer gated ReLU CNNs with average pooling. We apply this procedure to the image classification datasets of CIFAR-10 (Krizhevsky et al., 2009) and Fashion-MNIST (Xiao et al., 2017), following all general architecture choices from (Belilovsky et al., 2019) (see Appendix for details). We model the scenario in which we are in a memory-limited setting, restricting our memory to a single 12GB GPU.\n\nIn this memory-limited setting, layerwise training with the full convex model is impossible, because the nuclear norm penalty requires an SVD at each iteration, and further because the latent representation to be used as input for the second stage, given by {{D(i) c′=1, has P ac channels, which for reasonable choices of P = 256, a = 4, c = 10 yields upwards of 104 channels for the input to the second CNN stage. Accordingly, for this model, we employ the BM factorization with m = 1, allowing for a latent representation for the second stage consisting of only P channels.\n\nj XiZ(c′)\n\nj=1}c\n\n)}P\n\nj\n\nAs we show in Figure 3, this BM scheme for layerwise training allows for both test and train accuracies to improve one stage the next of the layerwise training procedure, reaching the performance of much deeper networks while enabling a convex optimization procedure. In particular, training five stages of a BM factorized convex two-layer gated ReLU CNN on CIFAR-10 resulted in a final test accuracy of 80.5%. Previously, it has been demonstrated that a six-layer ReLU CNN achieves 81.6% on CIFAR-10 when trained end-to-end (Kiliçarslan & Celik, 2021). In addition, the three-stage trained BM factorized convex two-layer gated ReLU CNN on Fasion-MNIST achieved a final test accuracy of 91.5%, compared to 91.2% for a four-layer ReLU CNN trained end-to-end (Bhatnagar et al., 2017). These results demonstrate that the BM factorization is essential for convex neural networks to match the performance of deep end-to-end trained ReLU networks.\n\nOur layerwise training procedure was trained on a single NVIDIA 1080 Ti GPU using the Pytorch deep learning library (Paszke et al., 2019). In particular, we follow the implementation of (Belilovsky et al., 2019), who proposed greedily, sequentially training two-layer CNNs. At each stage, a two-layer CNN (convolutional layer + average pooling + fully connected layer) is trained, and then the weights are frozen, the fully connected layer and average pooling are discarded, and the trained convolutional layer is used as a feature-generator for the following stage. At certain stages, before the CNN is applied, an invertible downsampling operation (Dinh et al., 2016) is used to reduce the spatial dimensions of the image. In (Belilovsky et al., 2019), m = 128 convolutional features per stage are used, with ReLU activations as well as an average pooling operation to spatial dimensions of 2 × 2 (i.e. a = 4) is used, followed by a flattening operation and a fully connected layer. They also use a softmax cross-entropy loss, a batch size of 128, weight decay parameter of β = 5e − 4, along with stochastic gradient descent (SGD) with momentum fixed to 0.9, 50 epochs per stage, and learning rate decay by a factor of 0.2 every 15 epochs.\n\nIn our experiments, we keep all network and optimization parameters the same, aside from replacing the non-convex CNN at each stage with our convex CNN objective (26). We then apply the Burer-Monteiro factorization with m = 1 to this architecture to make it tractable for layerwise learning as described in the main paper. At each stage, we randomly subsample ˆP = 256 hyperplane arrangements, rather than enumerate over all P , which is a high-order polynomial in terms of n. We further use gated ReLU rather than ReLU activations for simplicity, which can work as well as ReLU in practice (Fiat et al., 2019). These techniques have been used effectively for convex learning to exceed the performance of two-layer non-convex neural networks (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020; Ergen et al., 2021).\n\nFor the CIFAR-10 experiment, we use 5 stages (following (Belilovsky et al., 2019)), whereas for the Fashion-MNIST experiment, we use 3 stages, since the training accuracy saturates after 3 stages. We\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nchoose learning rates per stage from {10−1, 10−2, 10−3, 10−4} per stage based on training accuracy for CIFAR-10. The chosen learning rates were [10−1, 10−2, 10−3, 10−2, 10−2] for CIFAR-10. For Fashion-MNIST, we empirically observed the training loss was better optimized with slightly higher learning rates, so we used [2 × 10−1, 5 × 10−2, 5 × 10−3]. All code used to run our experiments is provided. Ultimately, our CIFAR-10 network with 5 stages took 9163 seconds to train, and the Fashion-MNIST network with 3 stages took 4931 seconds to train. In (Kiliçarslan & Celik, 2021), it is shown that an end-to-end 6-layer CNN with ReLU activations takes 640 seconds to train on CIFAR-10 and 285 seconds to train on Fasion-MNIST. We note that the purpose of this experiment is not to advocate for the use of layerwise BM networks over end-to-end trained networks, but simply to demonstrate the utility of the BM network in enabling convex neural networks to scale to the performance of end-to-end deep networks by using layerwise learning.\n\n31",
    "reference": "# Summary Of The Paper\n\nThis work studies the problem of formulating several (non)linear two-layer neural networks (NNs) as equivalent convex optimization problems, where existing results of equivalent convex optimization problems are computationally expensive.\n\nThis work proposes to use Burer-Monteiro (BM) factorization to obtain equivalent and computationally tractable non-convex alternative (rather than convex optimization problems) with no spurious local minima.\n\nSection 3.1 first calculates BM factorization for convex MLP with linear, gated ReLU, and ReLU activations, as shown in Eqs. (19) - (21). Theorem 3.4 characterize the relative optimality gap in and show that under some rank conditions the stationary points of BM factorization  in Eq. (16) are global minimizers of the equivalent convex optimization problem of Eq. (18), meaning that solving stationary points Eqs. (19) - (21) for those NNs could obtain global minimizers of their existing equivalent convex optimization problems.\n\nSection 3.2 first characterizes two-layer convolutional neural networks (CNNs) with arbitrary linear pooling operations as a convex program (Theorem 3.6). Lemma 3.7 gives BM factorization of the convex CNN problem with ReLU activation. Corollary 3.7.1 gives a result for stationary points of Eq. (27) are global minimizers of Eq. (26), under the condition of Eq. (28).\n\nSection 3.3 then studies self-attention networks. Lemma 3.8 shows the BM factorization of the the convex self-attention problem with linear activation (gated ReLU and ReLU activations are also calculated in the appendix), and Corollary 3.8.1 shows that Eq (29) has no spurious local minima as long as the number of heads is large enough.\n\nSection 4 uses experimental results on synthetic dataset, with a gated-ReLU two-layer MLP; as well as (shown in the appendix) on CIFAR-10 and Fashion-MNIST datasets, with  two-layer gated ReLU CNNs. The results show that BM factorization enables layer-wise training of two-layered MLPs and CNNs, achieving comparable results to end-to-end training of networks.\n\n# Strength And Weaknesses\n\n**Strength**:\n\n1. The idea of using BM factorization to improve the computational efficiency of equivalent convex problems of two-layered NNs is interesting, and the results seem novel.\n2. The presentation of results is clear and easy to follow.\n3. Experimental results can verify the proposed BM factorizations.\n\n**Weaknesses**:\n\n1. The neural networks studied in this work are not practical, mainly two-layered MLPs and CNNs. This weakness is from existing equivalent convex optimization problems are for two-layered models.\n2. The results for self-attention networks are interesting. However, it has a similar problem of using linear activation, which makes it questionable how useful those results are in practice.\n3. The characterizations of relative gaps and conditions for stationary points being global minimizers are presented without explanation, making the implications not clear to me, e.g., the gradient norm in Eq. (23), and the trace conditions in Eq. (28). Could the authors provide some intuitions or detailed explanations for the audience to better understand the meaning of those results?\n4. The comments of BM factorization finds \"subtle\" saddle points at the end of Section 4 is interesting. However, it is not clear to me how is it concluded that from the results in Figure 2 that the found solutions are saddle points rather than local minima, and is there a calculation to verify that?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe clarity is good, the problems and results are clearly presented, and relate works and techniques are also discussed well enough. The quality and originality are OK, since most results are from using BM factorization on existing equivalent convex optimization problems of NNs, and some results are not known before.\n\n# Summary Of The Review\n\nThe idea of using BM factorization to improve the computational efficiency of equivalent convex problems of two-layered NNs is reasonable and novel to me. However, most of the results are for two-layered NNs and restricted models like linear activations in self-attention networks, which makes it questionable how useful those results could be in practical NNs.\n\n=====UPDATE=====\n\nI would like to thank the authors for the feedback. I increased my score since it addressed my questions well.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "TOWARDS BETTER SELECTIVE CLASSIFICATION\n\nLeo Feng Mila – Université de Montréal & Borealis AI leo.feng@mila.quebec\n\nMohamed Osama Ahmed Borealis AI mohamed.o.ahmed@borealisai.com\n\nHossein Hajimirsadeghi Borealis AI hossein.hajimirsadeghi@borealisai.com\n\nAmir Abdi Borealis AI amir.abdi@borealisai.com\n\nABSTRACT\n\nWe tackle the problem of Selective Classification where the objective is to achieve the best performance on a predetermined ratio (coverage) of the dataset. Recent state-of-the-art selective methods come with architectural changes either via introducing a separate selection head or an extra abstention logit. In this paper, we challenge the aforementioned methods. The results suggest that the superior performance of state-of-the-art methods is owed to training a more generalizable classifier rather than their proposed selection mechanisms. We argue that the best performing selection mechanism should instead be rooted in the classifier itself. Our proposed selection strategy uses the classification scores and achieves better results by a significant margin, consistently, across all coverages and all datasets, without any added compute cost. Furthermore, inspired by semi-supervised learning, we propose an entropy-based regularizer that improves the performance of selective classification methods. Our proposed selection mechanism with the proposed entropy-based regularizer achieves new state-of-the-art results.\n\n1\n\nINTRODUCTION\n\nA model’s ability to abstain from a decision when lacking confidence is essential in mission-critical applications. This is known as the Selective Prediction problem setting. The abstained and uncertain samples can be flagged and passed to a human expert for manual assessment, which, in turn, can improve the re-training process. This is crucial in problem settings where confidence is critical or an incorrect prediction can have significant consequences such as in the financial, medical, or autonomous driving domains. Several papers have tried to address this problem by estimating the uncertainty in the prediction. Gal & Ghahramani (2016) proposed using MC-dropout. Lakshminarayanan et al. (2017) proposed to use an ensemble of models. Dusenberry et al. (2020) and Maddox et al. (2019) are examples of work using Bayesian deep learning. These methods, however, are either expensive to train or require lots of tuning for acceptable results.\n\nIn this paper, we focus on the Selective Classification problem setting where a classifier has the option to abstain from making predictions. Models that come with an abstention option and tackle the selective prediction problem setting are naturally called selective models. Different selection approaches have been suggested such as incorporating a selection head Geifman & El-Yaniv (2019) or an abstention logit (Huang et al., 2020; Ziyin et al., 2019). In either case, a threshold is set such that selection and abstention values above or below the threshold decide the selection action. SelectiveNet Geifman & El-Yaniv (2019) proposes to learn a model comprising of a selection head and a prediction head where the values returned by the selection head determines whether the datapoint is selected for prediction or not. Huang et al. (2020) and Ziyin et al. (2019) introduced an additional abstention logit for classification settings where the output of the additional logit determines whether the model abstains from making predictions on the sample. The promising results of these works suggest that the selection mechanism should focus on the output of an external head/logit.\n\nOn the contrary, in this work, we argue that the selection mechanism should be rooted in the classifier itself. The results of our rigorously conducted experiments show that (1) the superior\n\n1\n\nperformance of the state-of-the-art methods is owed to training a more generalizable classifier rather than their proposed external head/logit selection mechanisms. These results suggest that future work in selective classification (i) should aim to learn a more generalizable classifier and (ii) the selection mechanism should be based on the classifier itself rather than the recent research directions of architecture modifications for an external logit/head. (2) We highlight a connection between selective classification and semi-supervised learning. To the best of our knowledge, this has has not been explored before. We show that entropy-minimization regularization, a common technique in semi-supervised learning, significantly improves the performance of the state-of-the-art selective classification method. The promising results suggest that additional research is warranted to explore the relationship between these two research directions.\n\nFrom a practical perspective, (3) we propose a selection mechanism that outperforms the original selection mechanism of state-of-the-art methods. Furthermore, this method can be immediately applied to an already deployed selective classification model and instantly improve performance at no additional cost. (4) We show a selective classifier trained with the entropy-regularised loss and with selection according to the classification scores achieves new state-of-the-art results by a significant margin (up to 80% relative improvement). (5) Going beyond the already-saturated datasets often used for Selective Classification research, we include results on larger datasets: StanfordCars, Food101, Imagenet, and Imagenet100 to test the methods on a wide range of coverages and ImagenetSubset to test the scalability of the methods.\n\n2 RELATED WORK\n\nThe option to reject a prediction has been explored in depth in various learning algorithms not limited to neural networks. Primarily, Chow (Chow, 1970) introduced a cost-based rejection model and analysed the error-reject trade-off. There has been significant study in rejection in Support Vector Machines (Bartlett & Wegkamp, 2008; Fumera & Roli, 2002; Wegkamp, 2007; Wegkamp & Yuan, 2011). The same is true for nearest neighbours (Hellman, 1970) and boosting (Cortes et al., 2016).\n\nIn 1989, LeCun et al. (1989) proposed a rejection strategy for neural networks based on the most activated output logit, second most activated output logit, and the difference between the activated output logits. Geifman & El-Yaniv (2017) presented a technique to achieve a target risk with a certain probability for a given confidence-rate function. As examples of confidence-rate functions, the authors suggested selecting according to Softmax Response and MC-Dropout as selection mechanisms for a vanilla classifier. We build on this idea to demonstrate that Softmax Response, if utilized correctly, is the highest performing selection mechanism in the selective classification settings. Beyond selective classification, max-logit (Softmax Response) has also been used in anomaly detection (Hendrycks & Gimpel, 2016; Dietterich & Guyer, 2022).\n\nFuture work focused on architectural changes and selecting according to a separately computed head/logit with their own parameters. The same authors, Geifman & El-Yaniv (2019) later proposed SelectiveNet (see Section 3.2.1), a three-headed model, comprising of heads for selection, prediction, and auxiliary prediction. Deep Gamblers (Ziyin et al., 2019) (see Appendix A.1) and Self-Adaptive Training (Huang et al., 2020) (see Section 3.3.1) propose a (C + 1)-way classifier, where C is the number of classes and the additional logit represents abstention. In contrast, in this work, we explain how selecting via entropy and max-logit can work as a proxy to select samples which could potentially minimise the cross entropy loss. In general, we report the surprising results that the selection head of the SelectiveNet and the abstention logits in Deep Gamblers and Self-Adaptive Training are suboptimal selection mechanisms. Furthermore, their previously reported good performance is rooted in their optimization process converging to a more generalizable model.\n\nAnother line of work which tackles the selective classification is that of cost-sensitive classification (Charoenphakdee et al., 2021). However, the introduction of the target coverage adds a new variable and changes the mathematical formulation. Other works have proposed to perform classification in conjunction with expert decision makers (Mozannar & Sontag, 2020).\n\nIn this work, we also highlight a connection between semi-supervised learning and selective classification, which, to the best of our knowledge, has not been explored before. As a result, we propose an entropy-regularized loss function in the Selective Classification settings to further improve the performance of the Softmax Response selection mechanism. However, entropy minimization\n\n2\n\nobjectives have been widely used for Unsupervised Learning (Long et al., 2016), Semi-Supervised Learning (Grandvalet & Bengio, 2004), and Domain Adaptation (Vu et al., 2019; Wu et al., 2020).\n\n3 BACKGROUND\n\nIn this section, we introduce the Selective Classification problem. Additionally, we describe the top methods for Selective Classification. To the best of our knowledge, Self-Adaptive Training (Huang et al., 2020) achieves the best performance on the selective classification datatests.\n\n3.1 PROBLEM SETTING: SELECTIVE CLASSIFICATION\n\nThe selective prediction task can be formulated as follows. Let X be the feature space, Y be the label space, and P (X , Y) represent the data distribution over X × Y. A selective model comprises of a prediction function f : X → Y and a selection function g : X → {0, 1}. The selective model decides to make predictions when g(x) = 1 and abstains from making predictions when g(x) = 0. The objective is to maximise the model’s predictive performance for a given target coverage ctarget ∈ [0, 1], where coverage is the proportion of the selected samples. The selected set is defined as {x : g(x) = 1}. Formally, an optimal selective model, parameterised by θ∗ and ψ∗, would be the following:\n\nθ∗, ψ∗ = argminθ,ψ\n\n(1) where EP [l(fθ(x), y) · gψ(x)] is the selective risk. Naturally, higher coverages are correlated with higher selective risks.\n\nEP [l(fθ(x), y) · gψ(x)],\n\ns.t. EP [gψ(x)] ≥ ctarget,\n\nIn practice, instead of a hard selection function gψ(x), existing methods aim to learn a soft selection function ̄gψ : X → R such that larger values of ̄gψ(x) indicate the datapoint should be selected for prediction. At test time, a threshold τ is selected for a coverage c such that\n\ngψ(x) =\n\n(cid:26)1 0\n\nif ̄gψ(x) ≥ τ otherwise\n\n,\n\ns.t. E[gψ(x)] = ctarget\n\n(2)\n\nIn this setting, the selected (covered) dataset is defined as {x : ̄gψ(x) ≥ τ }. The process of selecting the threshold τ is known as calibration.\n\n3.2 APPROACH: LEARN TO SELECT\n\n3.2.1 SELECTIVENET\n\nSelectiveNet (Geifman & El-Yaniv, 2019) is a three-headed network proposed for selective learning. A SelectiveNet model has three output heads designed for selection ̄g, prediction f , and auxiliary prediction h. The selection head infers the selective score of each sample, as a value between 0 to 1, and is implemented with a sigmoid activation function. The auxiliary prediction head is trained with a standard (non-selective) loss function. Given a batch {(xi, yi)}m i=1, where yi is the label, the model is trained to minimise the loss L where it is defined as:\n\nL = α (Lselective + λLc) + (1 − α)Laux,\n\nLselective =\n\n1 m\n\n(cid:80)m\n\n,\n\n1 m\n\n(cid:80)m\n\ni=1 l(f (xi), yi) ̄g(xi) i=1 ̄g(xi) 1\nm\n\nm (cid:88)\n\ni=1\n\nLc = max(0, (ctarget −\n\n ̄g(xi))2), Laux =\n\n(3)\n\n(4)\n\n(5)\n\n1 m\n\nm (cid:88)\n\ni=1\n\nl(h(xi), yi),\n\nwhere l is any standard loss function. In Selective Classification, l is the Cross Entropy loss function. The coverage loss Lc encourages the model to achieve the desired coverage and ensures ̄g(xi) > 0 for at least ctarget proportion of the batch samples. The selective loss Lselective discounts the weight of difficult samples via the soft selection value ̄g(x) term encouraging the model to focus more on easier samples which the model is more confident about.\n\nThe auxiliary loss Laux ensures that all samples, regardless of their selective score ( ̄g(x)), contribute to the learning of the feature model. λ and α are hyper-parameters controlling the trade-off of\n\n3\n\ndifferent terms. Unlike Deep Gamblers and Self-Adaptive Training, SelectiveNet trains a separate model for each target coverage ctarget. In the SelectiveNet paper (Geifman & El-Yaniv, 2019), it has been suggested that the best performance is achieved when the training target coverage is equal to that of the evaluation coverage.\n\n3.3 APPROACH: LEARN TO ABSTAIN\n\nSelf-Adaptive Training (Huang et al., 2020) and Deep Gamblers (Ziyin et al., 2019) propose to tackle the selective classification problem by introducing a (C + 1)-th class logit where the extra class logit represents abstention. Let pθ(·|x) represent the prediction network with softmax as the last layer. This family of methods abstain if pθ(C + 1|x) is above a threshold. Here, we unify the notations of these abstention and selection methods under the same Selective Classification framework (See Section 3.1) with the following soft selection function: ̄g(x) = 1 − pθ(C + 1|x). Due to the space limitation, the formulation for Deep Gamblers is included in the Appendix.\n\n3.3.1 SELF-ADAPTIVE TRAINING\n\nIn addition to learning a logit that represents abstention, Self-Adaptive Training (Huang et al., 2020) proposes to use a convex combination of labels and predictions as a dynamically moving training target instead of the fixed labels. Let yi be the one-hot encoded vector representing of the label for a datapoint (xi, yi) where yi is the label.\n\nInitially, the model is trained with a cross-entropy loss for a series of pre-training steps. Afterwards, the model is updated according to a dynamically moving training target. The training target ti is initially set equal to the label ti ← yi such that the training target is updated according to ti ← α × ti + (1 − α) × pθ(·|xi) s.t. α ∈ (0, 1) after each model update. Similar to Deep Gamblers, the model is trained to optimise a loss function that allows the model to also choose to abstain on hard samples instead of making a prediction:\n\nL = −\n\n1 m\n\nm (cid:88)\n\ni=1\n\n[ti,yi log pθ(yi|xi) + (1 − ti,yi) log pθ(C + 1|xi)],\n\n(6)\n\nwhere m is the number of datapoints in the batch. As training progresses, ti approaches pθ(·|xi). The first term is similar to the Cross Entropy Loss and encourages the model to learn a good classifier. The second term encourages the model to abstain from making predictions for samples that the model is uncertain about. This use of dynamically moving training target ti allows the model to avoid fitting on difficult samples as the training progresses.\n\n4 METHODOLOGY\n\nWe motivate an alternative selection mechanism Softmax Response for Selective Classification models. We explain how the known state-of-the-art selective methods can be equipped with the proposed selection mechanism and why it further improves performance. Inspired by semi-supervised learning, we also introduce an entropy-regularized loss function.\n\n4.1 MOTIVATION\n\nRecent state-of-the-art methods have proposed to learn selective models with architecture modifications such as an external logit/head. These architecture modifications, however, act as regularization mechanisms that allow the method to train more generalizable classifiers (see Table 1). As a result, the claimed improved results from these models could actually be attributed to their classifiers being more generalizable. For these selective models to have strong performance in selective classification they require the external logit/head to generalise in these sense that the external logit/head must select samples for which the classifier is confident of its prediction. Since the logit/head has its own set of learned model parameters, this adds another potential mode of failure for a selective model. Specifically, the learned parameters can fail to generalise and the logit/head may (1) suggest samples for which the classifier is not confident about and (2) reject samples for which the classifier is confident about. In the appendix (See Figure 4 and 5), we include examples of images that fail due\n\n4\n\nModel Vanilla Classifier SelectiveNet Deep Gamblers Self-Adaptive Training\n\nAccuracy 85.68 ± 0.14 86.23 ± 0.14 86.51 ± 0.52 86.40 ± 0.30\n\nTable 1: Results at 100% coverage on Imagenet100.\n\nto this introduced mode of failure. As such, we propose that selective mechanisms should stem from the classifier itself instead of an external logit/head, avoiding this extra mode of failure.\n\n4.2 SELECTING ACCORDING TO THE CLASSIFIER\n\nThe cross entropy loss function is a popular loss function for classification due to its differentiability. However, during evaluation, the most utilized metric is accuracy, i.e., whether a datapoint is predicted correctly. In the cross-entropy objective of the conventional classification settings, p(c|xi) is a one-hot encoded vector; therefore, the the cross-entropy loss can be simplified as CE (p(·|xi), pθ(·|xi)) = − (cid:80)C u=1 p(u|xi) log pθ(u|xi) = − log pθ(yi|xi), i.e., during optimization, the logit of the correct class is maximised. Accordingly, the maximum value of logits can be interpreted as the model’s relative confidence of its prediction. Therefore, a simple selection mechanism for a model would be to select according to the maximum predictive class score, ̄g(x) = maxu∈{1,...C}pθ(u|xi) (aka Softmax Response (Geifman & El-Yaniv, 2017)). Alternatively, a model can also select according to its predictive entropy ̄g(x) = −H(pθ(·|x)), a metric of the model’s uncertainty. An in-depth discussion is included in in the Appendix B.\n\n4.3 RECIPE FOR BETTER SELECTIVE CLASSIFICATION\n\nThe recipe that we are providing for better selective classification is as follows:\n\n1. Train a selective classifier (e.g., SelectiveNet, Self-Adaptive Training, or Deep Gamblers).\n\n2. Discard its selection mechanism:\n\n• For SelectiveNet: Ignore the selection head • For Self-Adaptive Training and Deep Gambler: Ignore the additional abstain logit and\n\ncompute the final layer’s softmax on the original C class logits.\n\n3. Use a classifier-based selection mechanism (e.g., Softmax Response) to rank the samples.\n\n4. Calculate the threshold value τ , based on the validation set, to achieve the desired target\n\ncoverage and select samples with max logit greater than τ .\n\nEmpirically, we show that selecting via entropy or Softmax Response both outperform selecting according to the external head/logit. From these results, we can conclude that the strong performance of these recent state-of-the-art methods were due to learning a more generalizable classifier rather than their proposed selection mechanisms. In Step 3, we experimented with both an entropy-based selection mechanism and Softmax Response but we found that Softmax Response performed better. Notably, Softmax Response does not require retraining and can be immediately applied to already deployed models for significant performance improvement at negligible cost.\n\n4.4 ENTROPY-REGULARIZED LOSS FUNCTION\n\nHere, we highlight a similarity between semi-supervised learning and selective classification, which to our knowledge has not been explored before. In the semi-supervised learning setting the training dataset consists of labelled and unlabelled data. A simple approach is to train solely on the labelled data and ignore the unlabelled data, i.e., training a model via supervised learning to the labelled data. This is equivalent to having a weight of 1 for the labelled samples and 0 for the unlabelled samples. However, this is suboptimal because it does not use any information from the unlabelled samples. Similarly, in Selective Classification, samples that are selected tend to have a high weight close to 1 (see, for example, in Section 3.2.1, the ̄g(x) term in the objective) and samples that are not selected\n\n5\n\nhave a low weight close to 0. One way that semi-supervised learning have proposed to tackle this is via an entropy minimization term.\n\nEntropy minimization is one of the most standard, well-studied, and intuitive methods for semisupervised learning. It uses the information of all the samples and increases the model’s confidence in its predictions, including on the unlabelled samples, resulting in a better classifier. Inspired by the similarity in the objective of selective classification and the setting of semi-supervised learning, we propose an entropy-minimisation term for the objective function of selective classification methods:\n\nLnew = L + β H(pθ(·|x)),\n\n(7)\n\nwhere β is a hyperparameter that controls the impact. In our experiments, we found β = 0.01 to perform well in practice. The entropy minimization term encourages the model to be more confident in its predictions, i.e., increasing the confidence of the predicted class and decreasing the predictive entropy during training. Thus, it allows for better disambiguation between sample predictions. The larger coefficient on the cross-entropy term compared to that of the entropy-minimization term ensures that increasing the confidence of correct predictions are prioritised, benefitting Softmax Response. In Section 5, we show that this proposed loss function based on semi-supervised learning improves the performance in Selective Classification by a significant margin. These results opens the door to future exploration of the connection between Selective Classification and semi-supervised learning.\n\n5 EXPERIMENTS\n\nFor the following experiments, we evaluate the following state-of-the-art methods (1) SelectiveNet (SN), (2) Self-Adaptive Training (SAT), and (3) Deep Gamblers. Furthermore, we compare the performance of these methods with the following selection mechanisms (1) original selection mechanism and (2) SR: Softmax Response (our proposed method). Due to space limitations, the table results for a vanilla classifier is included in the Appendix with several additional results.\n\nThe goal of our experimental evaluation is to answer the following questions: (1) Is the superior performance of recent state-of-the-art methods due to their proposed external head/logit selection mechanisms? Taking this further, what is the state-of-the-art selection mechanism? (2) Does the proposed entropy-regularized loss function improve the effectiveness of Softmax Response for Selective Classification? (3) What is the new state-of-the-art method for Selective Classification? (4) How scalable are selective methods for larger datasets with larger number of classes?\n\n5.1 DATASETS\n\nWe introduce new datasets: StanfordCars, Food101, Imagenet, Imagenet100 and ImagenetSubset, for the selective classification problem setting and benchmark the existing state-of-the-art methods. We propose StanfordCars, Food101, Imagenet, and Imagenet100, as realistic non-saturated datasets that can be evaluated at a wide range of coverages (10 − 100%). In addition, we propose ImagenetSubset as a collection of datasets to evaluate the scalability of the methods for different number of classes. This is in contrast to the existing Selective Classification research which mainly have focused on small datasets such as CIFAR-10 with 10 or less classes, low resolution images (64x64 or less), and very low error (The error at 80% coverage is already lower than 1%) so this dataset is limited to high coverages (70%+). The results of the previously introduced datasets indicate saturation, e.g., 0.3% error at 70% coverage, discouraging experiments with lower coverages, which, in turn, prevents researchers from achieving conclusive results.\n\nImagenet/Imagenet100/ImagenetSubset. Imagenet (Deng et al., 2009) comprises of 1,300 images per class and evaluation data comprising of 50,000 images split into 1,000 classes. Imagenet100 (Tian et al., 2020) is a subset of Imagenet which comprising of 100 classes. ImagenetSubset is created as a collection of datasets with varying number of classes (difficulty) from 25 to 175 in increments of 25. The classes are sampled randomly such that datasets with less classes are subsets of those with more classes. The complete list of selected classes in each dataset subset is available in the Appendix1. ImagenetSubset evaluates the models’ performance with respect to the difficulty (scalability) of the task.\n\n1Note that the created dataset of ImagenetSubset with 100 classes is different than that of Imagenet100.\n\n6\n\nCoverage 100 90 80 70 60 50 40 30 20 10\n\nSelectiveNet (SN) SN 13.77 ± 0.14 9.44 ± 0.28 6.00 ± 0.22 3.38 ± 0.21 1.99 ± 0.15 1.05 ± 0.17 0.58 ± 0.08 1.04 ± 0.37 48.87 ± 6.15 99.00 ± 0.00\n\nSN+SR 13.77 ± 0.14 7.89 ± 0.10 4.47 ± 0.19 2.21 ± 0.37 1.57 ± 0.06 0.85 ± 0.02 0.53 ± 0.03 0.64 ± 0.10 47.10 ± 3.83 99.00 ± 0.00\n\nDeep Gamblers (DG) DG+SR 13.49 ± 0.52 8.11 ± 0.48 4.52 ± 0.38 2.58 ± 0.21 1.71 ± 0.32 1.31 ± 0.22 1.07 ± 0.19 0.96 ± 0.21 0.90 ± 0.22 0.53 ± 0.25\n\nDG 13.49 ± 0.52 8.42 ± 0.44 5.21 ± 0.32 3.30 ± 0.40 2.14 ± 0.37 1.55 ± 0.27 1.23 ± 0.38 1.09 ± 0.31 1.03 ± 0.31 0.80 ± 0.28\n\nSelf-Adaptive Training (SAT)\n\nSAT 13.58 ± 0.30 8.80 ± 0.41 5.20 ± 0.29 2.71 ± 0.29 1.72 ± 0.11 1.18 ± 0.14 0.82 ± 0.06 0.67 ± 0.06 0.48 ± 0.18 0.32 ± 0.10\n\nSAT+SR 13.58 ± 0.30 8.04 ± 0.25 4.46 ± 0.13 2.33 ± 0.19 1.37 ± 0.12 0.88 ± 0.07 0.60 ± 0.11 0.59 ± 0.11 0.46 ± 0.22 0.12 ± 0.16\n\nTable 2: Comparison of the selective classification error between SelectiveNet (SN), Deep Gamblers (DG), Self-Adaptive Training (SAT) with their original selection mechanisms vs. using Softmax Response (SR) as the selection mechanisms on ImageNet100.\n\nFood101. The Food dataset (Bossard et al., 2014) contains 75750 training images and 25250 testing images split into 101 food categories.\n\nStanfordCars. The Cars dataset (Krause et al., 2013) contains 8,144 training images and 8,041 testing images split into 196 classes of cars. Unlike prior works which typically evaluate StanfordCars for transfer learning, in this work, the models are trained from scratch.\n\nCIFAR-10. The CIFAR-10 dataset (Krizhevsky, 2009) comprises of small images: 50,000 images for training and 10,00 images for evaluation split into 10 classes. Each image is of size 32 × 32 × 3.\n\n5.2 EXPERIMENT DETAILS\n\nFor our experiments, we adapted the publicly available official implementations of Deep Gamblers and Self-Adaptive Training 2. Experiments on SelectiveNet were conducted with our Pytorch implementation of the method which follow the details provided in the original paper (Geifman & El-Yaniv, 2019). For the StanfordCars, Food101, Imagenet100, and ImagenetSubset datasets, we use a ResNet34 architecture for Deep Gamblers, Self-Adaptive Training, and the main body block of SelectiveNet. Following prior work, we use a VGG16 architecture for the CIFAR-10 experiments.\n\nWe tuned the entropy minimization loss function hyperparameter with the following values: β ∈ {0.1, 0.01, 0.001, 0.0001}. CIFAR10, Food101, and StanfordCars experiments were run with 5 seeds. Imagenet-related experiments were run with 3 seeds. Additional details regarding hyperparameters are included in the Appendix.\n\n5.3 RESULTS\n\n5.3.1 CORRECTING THE MISCONCEPTION ABOUT THE SELECTION MECHANISM\n\nIn Table 2, we compare the different selection mechanisms for a given selective classification method (SelectiveNet, Deep Gamblers, and Self-Adaptive Training). The results show that for each of these trained selective classifiers, their original selection mechanisms are suboptimal; in fact, selecting via Softmax Response outperforms their original selection mechanism. These results suggest that (1) the strong performance of these methods were due to them learning a more generalizable model rather than their proposed external head/logit selection mechanisms and (2) the selection mechanism should stem from the classifier itself rather than a separate head/logit. We see that Softmax Response is the state-of-the-art selection mechanism. It is important to note that this performance gain is achieved by simply changing the selection mechanism of the pre-trained selective model without any additional computational cost. This observation applies to SN, DG, and SAT models.\n\nAn interesting result from this experiment is that at low coverages (30%, 20%, and 10%), SelectiveNet’s performance progressively gets worse. We hypothesize that this is due to the optimisation process of SelectiveNet that allows the model to disregard (i.e., assign lower weight to their loss) a vast majority of samples during training at little cost, i.e., ̄g(x) ≈ 0, especially when the target\n\n2The code is available at https://github.com/BorealisAI/towards-better-sel-cls.\n\n7\n\nCov. 100 90 80 70 60 50 40 30 20 10\n\nSAT 37.68 ± 1.11 32.34 ± 1.19 26.86 ± 1.15 21.34 ± 1.20 16.21 ± 1.10 11.59 ± 0.74 7.76 ± 0.43 4.56 ± 0.35 2.42 ± 0.36 1.49 ± 0.00\n\nStanfordCars\n\nSAT+SR 37.68 ± 1.11 32.04 ± 1.18 26.39 ± 1.13 20.70 ± 1.23 14.92 ± 1.03 10.25 ± 0.97 6.32 ± 0.69 3.54 ± 0.36 1.93 ± 0.09 1.20 ± 0.21\n\nSAT+EM+SR 32.49 ± 2.33 26.60 ± 2.39 20.87 ± 2.33 15.84 ± 1.98 11.09 ± 1.50 7.00 ± 1.13 4.00 ± 0.87 2.20 ± 0.44 1.17 ± 0.28 0.80 ± 0.22\n\nSAT 16.41 ± 0.10 11.87 ± 0.13 7.99 ± 0.12 4.89 ± 0.11 2.73 ± 0.09 1.38 ± 0.09 0.79 ± 0.05 0.48 ± 0.07 0.25 ± 0.01 0.15 ± 0.07\n\nFood101 SAT+SR 16.41 ± 0.10 10.84 ± 0.17 6.57 ± 0.13 3.52 ± 0.05 1.95 ± 0.08 1.06 ± 0.06 0.56 ± 0.08 0.32 ± 0.04 0.15 ± 0.01 0.09 ± 0.02\n\nSAT+EM+SR 16.32 ± 0.35 10.77 ± 0.36 6.57 ± 0.21 3.52 ± 0.19 1.75 ± 0.17 0.96 ± 0.14 0.49 ± 0.08 0.19 ± 0.03 0.09 ± 0.05 0.03 ± 0.02\n\nTable 3: Comparison of the selective classification error between Self-Adaptive Training (SAT) with the original selection mechanisms vs. using Softmax Response (SR) and the proposed entropy minimization loss function (EM) on StanfordCars and Food101\n\nImagenet\n\nCov. 100 90 80 70 60 50 40 30 20 10\n\nSAT 27.41 ± 0.08 22.67 ± 0.24 18.14 ± 0.28 13.88 ± 0.14 10.11 ± 0.15 6.82 ± 0.07 4.32 ± 0.33 2.68 ± 0.14 1.82 ± 0.13 1.27 ± 0.34\n\nSAT+EM+SR 27.27 ± 0.05 21.57 ± 0.19 16.83 ± 0.06 12.34 ± 0.11 8.45 ± 0.05 5.57 ± 0.17 3.77 ± 0.00 2.32 ± 0.15 1.35 ± 0.20 0.55 ± 0.05\n\nImagenet100\n\nSAT 13.58 ± 0.30 8.80 ± 0.41 5.20 ± 0.29 2.71 ± 0.29 1.72 ± 0.11 1.18 ± 0.14 0.82 ± 0.06 0.67 ± 0.06 0.48 ± 0.18 0.32 ± 0.10\n\nSAT + SR 13.58 ± 0.30 8.04 ± 0.25 4.46 ± 0.13 2.33 ± 0.19 1.37 ± 0.12 0.88 ± 0.07 0.60 ± 0.11 0.59 ± 0.11 0.46 ± 0.22 0.12 ± 0.16\n\nSAT + EM SAT+EM+SR 13.18 ± 0.24 8.69 ± 0.32 5.03 ± 0.36 2.61 ± 0.22 1.59 ± 0.19 1.02 ± 0.21 0.81 ± 0.12 0.61 ± 0.14 0.52 ± 0.16 0.32 ± 0.20\n\n13.18 ± 0.24 7.73 ± 0.22 3.90 ± 0.34 1.81 ± 0.27 0.95 ± 0.13 0.62 ± 0.09 0.34 ± 0.06 0.25 ± 0.10 0.15 ± 0.08 0.12 ± 0.07\n\nTable 4: Results on Imagenet and demonstration of the impact of our SAT+EM+SR method over using SR alone or EM alone.\n\ncoverage is as low as 10%. In contrast, Deep Gamblers and Self-Adaptive Training models are equally optimised over all samples regardless of their selection.\n\n5.3.2 POWER OF SOFTMAX RESPONSE SELECTION WITH ENTROPY MINIMIZATION\n\nCIFAR10\n\nCoverage 100 95 90 85 80 75 70\n\nSAT 5.91 ± 0.04 3.73 ± 0.13 2.18 ± 0.11 1.26 ± 0.09 0.69 ± 0.04 0.37 ± 0.01 0.27 ± 0.02\n\nIn these experiments, we focus on Self-Adaptive Training as it is the state-of-the-art selective model. In Table 3, 4, and 5, we compare Self-Adaptive Training (SAT), SAT with SR (Softmax Response) selection mechanism, and SAT with SR and EM (Entropy-Minimization) on the Imagenet, StanfordCars, Food101, and CIFAR10 datasets. The results show that SAT+EM+SR achieves state-of-the-art performance across all coverages. For example, in StanfordCars, at 70% coverage, we see a raw 5.5% absolute improvement (25% relative reduction) in selective classification error by using our proposed method EM+SR. In Food101, at 70% coverage, we see a raw 1.37% absolute reduction (28% relative reduction) in selective classification error. The clear and considerable improvement across all coverages when using Softmax Response selection mechanism rather than the original selection mechanism. These results further confirm our surprising finding that existing selection mechanisms are suboptimal. In the Appendix we further include (1) risk-coverage curves and (2) results for several network architectures. The results of those experiments show that our proposed methodology generalises across different network architectures.\n\nSAT+EM+SR 5.91 ± 0.04 3.63 ± 0.10 2.11 ± 0.06 1.18 ± 0.07 0.64 ± 0.04 0.36 ± 0.03 0.23 ± 0.05\n\nTable 5: Results on CIFAR10\n\nFor the CIFAR-10 experiments (Table 5), the results for the different methods are within confidence intervals. Since the selective classification errors are very small, it is difficult to draw conclusions from such results. On CIFAR-10, SAT achieves 99+% accuracy at 80% coverage. In contrast, on\n\n8\n\n30% Coverage\n\n50% Coverage\n\n70% Coverage\n\n# Classes 175 150 125 100 75 50 25\n\nSAT 0.69 ± 0.12 0.44 ± 0.13 0.44 ± 0.07 0.71 ± 0.11 0.50 ± 0.15 0.76 ± 0.06 0.53 ± 0.00\n\nSAT+EM+SR 0.46 ± 0.05 0.16 ± 0.02 0.14 ± 0.09 0.15 ± 0.06 0.09 ± 0.00 0.16 ± 0.05 0.08 ± 0.11\n\nSAT 1.27 ± 0.12 0.81 ± 0.11 0.93 ± 0.11 1.11 ± 0.10 1.01 ± 0.08 1.44 ± 0.30 0.64 ± 0.23\n\nSAT+EM+SR 0.91 ± 0.16 0.47 ± 0.05 0.52 ± 0.07 0.56 ± 0.06 0.40 ± 0.03 0.37 ± 0.10 0.21 ± 0.08\n\nSAT 3.03 ± 0.13 2.23 ± 0.16 2.32 ± 0.25 2.65 ± 0.19 2.60 ± 0.15 2.86 ± 0.34 1.79 ± 0.53\n\nSAT+EM+SR 2.73 ± 0.07 1.71 ± 0.15 1.84 ± 0.14 1.81 ± 0.08 1.68 ± 0.27 1.47 ± 0.12 1.14 ± 0.25\n\nTable 6: Comparison between the Selective classification error for Self-Adaptive Training (SAT) and SAT with Entropy Minimization (EM) and Softmax Response (SR) on ImagenetSubset.\n\nImagenet100, SAT achieves 95% at 80% coverage. The saturation of CIFAR-10 is further highlighted in previous works which show improvements on the dataset (Geifman & El-Yaniv, 2019; Ziyin et al., 2019; Huang et al., 2022) on the scale of 0.1 − 0.2%.\n\n5.3.3 SCALABILITY WITH THE NUMBER OF CLASSES: IMAGENETSUBSET\n\nTo evaluate the scalability of the proposed methodology with respect to the number of classes, we evaluate our proposed method SAT+EM+SR with the previous state-of-the-art SAT on ImagenetSubset. In Table 6, we see once again that Self-Adaptive Training with our proposed entropy-regularised loss function and selecting according to Softmax Response outperforms the previous state-of-the-art (vanilla Self-Adaptive Training) by a very significant margin (up to 85% relative improvement) across all sizes of datasets. Due to the space limitations, the results for the other coverages of Table 6 are included in the Appendix.\n\n5.3.4 ENTROPY-MINIMIZATION ONLY, SOFTMAX RESPONSE SELECTION ONLY, OR BOTH?\n\nIn this experiment, we show that applying EM or SR alone provide gains. However, to achieve state-of-the-art results by a large margin, it is crucial to use the combination of both SR and EM. Table 4 shows that using only the entropy-minimization (SAT-EM) slightly improves the performance of SAT. However, SAT+EM+SR (SAT+EM in conjunction with SR selection mechanism) improves upon SAT+SR and SAT+EM significantly, achieving new state-of-the art results for selective classification.\n\n6 CONCLUSION\n\nIn this work, we analysed the state-of-the-art Selective Classification methods and concluded that their strong performance is owed to learning a more generalisable classifier rather, yet their suggested selective solutions are suboptimal. Accordingly, we showed that selection mechanisms based on the classifier itself outperforms the state-of-the-art selection methods. These results suggest that future work in selective classification should explore selection mechanisms based on the classifier itself rather than following recent works which proposed architecture modifications. Moreover, we also highlighted a connection between selective classification and semi-supervised learning, which to our knowledge has not been explored before. We show that a common technique in semi-supervised learning, namely, entropy-minimization, greatly improves performance in selective classification, opening the door to further exploration of the relationship between these two fields.\n\nFrom a practical perspective, we showed that selecting according to classification scores is the SOTA selection mechanism for comparison. Importantly, this method can be applied to an already deployed trained selective classification model and instantly improve performance at negligible cost. In addition, we showed a selective classifier trained with the entropy-regularised loss and with selection according to Softmax Response achieves new state-of-the-art results by a significant margin.\n\nREPRODUCIBILITY STATEMENT\n\nIn our experiments, we build on the official ing available at https://github.com/LayneH/SAT-selective-cls.\n\nimplementations of Self-Adaptive TrainOur code is available at\n\n9\n\nhttps://github.com/BorealisAI/towards-better-sel-cls. The experiments with Deep Gamblers (Link: https://github.com/Z-T-WANG/NIPS2019DeepGamblers) are run using the official implementaiton. Our Pytorch implementation of SelectiveNet follows the details in the original paper. The implementation details are available in Section 4. The hyperparameters are available in Section 5 and Appendix C.\n\nACKNOWLEDGEMENTS\n\nThe authors acknowledge funding from the Quebec government.\n\nREFERENCES\n\nPeter L. Bartlett and Marten H. Wegkamp. Classification with a reject option using a hinge loss. J.\n\nMach. Learn. Res., 9:1823–1840, jun 2008. ISSN 1532-4435.\n\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative compo-\n\nnents with random forests. In European Conference on Computer Vision, 2014.\n\nNontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification with rejection based on cost-sensitive classification. In International Conference on Machine Learning, pp. 1507–1517. PMLR, 2021.\n\nC. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information\n\nTheory, 16(1):41–46, 1970. doi: 10.1109/TIT.1970.1054406.\n\nCorinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. Advances in Neural\n\nInformation Processing Systems, 29, 2016.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nThomas G Dietterich and Alex Guyer. The familiarity hypothesis: Explaining the behavior of deep\n\nopen set methods. Pattern Recognition, 132:108931, 2022.\n\nMichael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran. Efficient and scalable Bayesian neural nets with rank-1 factors. In International Conference on Machine Learning, 2020.\n\nGiorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In Interna-\n\ntional Workshop on Support Vector Machines, pp. 68–82. Springer, 2002.\n\nYarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model\n\nuncertainty in deep learning. In International Conference on Machine Learning, 2016.\n\nYonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. Advances in\n\nneural information processing systems, 30, 2017.\n\nYonatan Geifman and Ran El-Yaniv. Selectivenet: A deep neural network with an integrated reject\n\noption. In International Conference on Machine Learning, pp. 2151–2159. PMLR, 2019.\n\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances\n\nin neural information processing systems, 17, 2004.\n\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\n\nnetworks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017.\n\nMartin E. Hellman. The nearest neighbor classification rule with a reject option. IEEE Trans. Syst.\n\nSci. Cybern., 6:179–185, 1970.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\n\nexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\n10\n\nLang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk\n\nminimization. Advances in neural information processing systems, 33:19365–19376, 2020.\n\nLang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: Bridging supervised and self-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–17, 2022. doi: 10.1109/TPAMI.2022.3217792.\n\nErik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification can magnify disparities across groups. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=N0M_4BkQ05i.\n\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained In 4th International IEEE Workshop on 3D Representation and Recognition\n\ncategorization. (3dRR-13), Sydney, Australia, 2013.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017.\n\nYann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back-propagation network. Advances in neural information processing systems, 2, 1989.\n\nJoshua K Lee, Yuheng Bu, Deepta Rajan, Prasanna Sattigeri, Rameswar Panda, Subhro Das, and Gregory W Wornell. Fair selective classification via sufficiency. In Marina Meila and Tong Zhang (eds.), International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 6076–6086. PMLR, 18–24 Jul 2021.\n\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with residual transfer networks. Advances in neural information processing systems, 29, 2016.\n\nWesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information Processing Systems, 2019.\n\nMatthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nHussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In\n\nInternational Conference on Machine Learning, pp. 7076–7087. PMLR, 2020.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding.\n\nIn European\n\nconference on computer vision, pp. 776–794. Springer, 2020.\n\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Pérez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2517–2526, 2019.\n\nMarten Wegkamp. Lasso type classifiers with a reject option. Electronic Journal of Statistics, 1:\n\n155–168, 2007.\n\nMarten Wegkamp and Ming Yuan. Support vector machines with a reject option. Bernoulli, 17(4):\n\n1368–1385, 2011.\n\nXiaofu Wu, Quan Zhou, Zhen Yang, Chunming Zhao, Longin Jan Latecki, et al. Entropy minimization\n\nvs. diversity maximization for domain adaptation. arXiv preprint arXiv:2002.01690, 2020.\n\nLiu Ziyin, Zhikang T Wang, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency, and Masahito Ueda. Deep gamblers: learning to abstain with portfolio theory. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 10623–10633, 2019.\n\n11\n\nA APPENDIX: ADDITIONAL BACKGROUND AND BROADER IMPACT\n\nA.1 DEEP GAMBLERS\n\nInspired by portfolio theory, Deep Gamblers proposes to train the model using the following loss function:\n\nL = −\n\n1 m\n\nm (cid:88)\n\ni=1\n\n(cid:18)\n\n(cid:19)\n\np(yi|x) log\n\npθ(yi|x) +\n\npθ(C + 1|x)\n\n,\n\n1 o\n\nwhere m is the number of datapoints in the batch and o is a hyperparameter controlling the impact of the abstain logit. Smaller values of o encourages the model to abstain more often. However, o ≤ 1 makes it ideal to abstain for all datapoints and o > C makes it ideal to predict for all datapoints. As a result, o is restricted to be between 1 and C. Note that the corresponding loss function with large values of o is approximately equivalent to the Cross Entropy loss.\n\nA.2 BROADER IMPACT\n\nThe broader impact of this work depends on the application of the selective model. In terms of the societal impact, fairness in selection remains a concern as lowering the coverage can magnify the difference in recall between groups and increase unfairness (Jones et al., 2021; Lee et al., 2021).\n\nThe calibration step performed on the validation set assumes the validation and test data are sampled from the same distribution. Hence, in the case of out-of-distribution test data, a selective classifier calibrated to, for example, 70\n\nWhen evaluating, Selective Classifiers may choose to predict samples with easier classes more than hard to predict classes. Thus, it would be undesirable in fairness applications that require equal coverage amongst the different classes.\n\nB APPENDIX: ALTERNATIVE MOTIVATION\n\nIn the selective classification problem setting, the objective is to select ctarget proportion of samples for prediction according to the value outputted by a selection function, ̄g(x). Since each datapoint (xi, yi) is an i.i.d. sample, it is optimal to iteratively select from the dataset D the sample x∗ that maximizes the selection function, i.e., x∗ ∈ argmaxx∈D ̄g(x)., until the target coverage ctarget proportion of the dataset is reached. In other words, to select ctarget proportion of samples (coverage = ctarget), it is sufficient to define the criterion ̄g and select a threshold τ such that exactly ctarget proportion of samples satisfy ̄g(x) > τ .\n\nB.1 SELECTING VIA PREDICTIVE ENTROPY\n\nAt test time, given a dataset of datapoints D, if the labels were available, the optimal criterion to select the datapoint x ∈ D that minimise the loss function would be according to:\n\nargminx∈D CE (p(·|x), pθ(·|x)) .\n\nHowever, at test time, the labels are unavailable. Instead, we can use the model’s belief over what the label is, i.e., the learned approximation pθ(·|x) ≈ p(·|x). We know CE(pθ(·|x), pθ(·|x)) = H(pθ(·|x)) where H is the entropy function. As such, we can select samples according to\n\nargminx∈D CE (p(·|x), pθ(·|x)) ≈ argminx∈D H (pθ(·|x)) .\n\nIn other words, entropy is an approximation for the unknown loss function. Accordingly, with respect to the discussed selection framework (Section 3.1), the samples with the largest negative entropy value, i.e., ̄g(x) = −H(pθ(·|x)) are best nominees for selection.\n\nIn Figure 1a, we show the distribution of entropy for a trained vanilla classifier, empirically showing entropy to be strongly inversely correlated with the model’s ability to correctly predict the labels. As a result, entropy is a good selection mechanism. We include results on CIFAR-10 and Imagenet100 for a vanilla classifier in Table 8 and Table 9.\n\n12\n\n(a) Entropy\n\n(b) Max Class Logit\n\nFigure 1: A histogram of the number of datapoints according to a vanilla classifier trained on Imagenet100. The orange bar indicates the samples for which the model correctly predicts the class of the sample. The blue bar represents the samples for which the model incorrectly predicted the class. In the case of entropy, a lower value correponds to higher model confidence. In contrast, in the case of max class logit, a higher value correponds to higher model confidence.\n\nFigure 2: Entropy Comparison. SelectiveNet trained on Imagenet100 for a target coverage of 0.8 and evaluated on a coverage of 0.8. In the case of entropy, a lower value correponds to higher model confidence. The histogram represents the counts of samples that were incorrectly predicted by the model. The left image indicates datapoints that were not selected by the selection head, i.e., datapoints with low selection value h(x) < τ . The right image indicates datapoints that were selected by the selection head, i.e., h(x) ≥ τ .\n\nB.2 SELECTING VIA MAXIMUM PREDICTIVE CLASS LOGIT (SOFTMAX RESPONSE)\n\nGiven a model with well-calibrated confidences (Guo et al., 2017; Minderer et al., 2021), an interpretation of pθ(u|x) is a probability estimate of the true correctness likelihood, i.e., pθ(u|x) is the likelihood that u is the correct label of x. Let yi be the correct label for xi. For example, given 100 samples {x1, . . . , x100} with pθ(u|xi) = 0.8, we would expect approximately 80% of the samples to have u as its label. As a result, pθ(yi|xi) is the model’s probability estimate that the correct label is yi. In classification, the probability that the calibrated model predicts a datapoint x correctly is equivalent to the value of the max class logit, i.e., maxu∈{1,...C} pθ(u|x). Logically, the sample xi that should be selected for clasification is the sample the model’s most likely to predict the sample correctly, (cid:0)maxu∈{1,...C}pθ(u|xj)(cid:1). This selection is equivalent to selecting according to i.e., i = argmaxj the following soft selection function ̄g(x) = maxu∈{1,...C} pθ(u|x). Simply put, this is equivalent to selecting according to the maximum predictive class logit (aka Softmax Response (Geifman & El-Yaniv, 2017)).\n\nIn practice, neural network models are not guaranteed to have well-calibrated confidences. In Selective Classification, however, we threshold according to τ and select samples above the threshold τ for classification, so we do not use the exact values of the confidence (max class logit). As a result, we do not need the model to necessarily have well-calibrated confidences. Instead, it suffices if samples with higher confidences (max class logit) have a higher likelihood of being correct. In Figure 1b, we show the distribution of max class logit for a trained vanilla classifier, empirically showing larger max class logit to be strongly correlated with model’s ability to correctly predict the label.\n\n13\n\nAs a result, max class logit is a good selection mechanism. We include results on CIFAR-10 and Imagenet100 for a vanilla classifier in Table 8 and Table 9.\n\nFigure 3: Max Class Logit Comparison. SelectiveNet trained on Imagenet100 for a target coverage of 0.8 and evaluated on a coverage of 0.8. In the case of max class logit, a higher value correponds to higher model confidence. The histogram represents the counts of samples that were incorrectly predicted by the model. The left image indicates datapoints that were not selected by the selection head, i.e., datapoints with low selection value h(x) < τ . The right image indicates datapoints that were selected by the selection head, i.e., h(x) ≥ τ\n\nB.3 RECIPE FOR BETTER SELECTIVE CLASSIFICATION\n\nIn this section, we further illustrate how SelectiveNet’s original selection mechanism is suboptimal. The optimisation of SelectiveNet’s selective loss Lselective (See Section 3.2.1) aims to learn a selection head (soft selection model) ̄g that outputs a low selection value for inputs with large cross-entropy loss and high selection value for inputs with low cross-entropy loss. At test time, good performance of SelectiveNet depends on the generalisation of both the prediction and selection heads. However, learned models can at times fail to generalise. In Figure 2 and Figure 3, we show the distribution of entropy and max class logit for selected and not-selected samples according to a SelectiveNet model. In the plots, we see SelectiveNet’s original selection mechanism selects several samples with large entropy and low max class logit. In Table 2, we see that the selection mechanisms based on entropy and max class logit outperforms the original selection mechanism. This comparison further supports our argument that the selection mechanism should be rooted in the objective function instead of a separately calculated score.\n\nC APPENDIX: ADDITIONAL EXPERIMENTAL DETAILS\n\nC.1 HYPERPARAMETERS\n\nFollowing (Geifman & El-Yaniv, 2019), SelectiveNet was trained with a target coverage rate and evaluated on the same coverage rate. As a result, there are different models for each experimental coverage rate. In contrast, target coverage does not play a role in the optimization process of Deep Gamblers and Self-Adaptive Training, hence, the results for different experimental coverages are computed with the same models.\n\nAll CIFAR-10 experiments were performed with 5 seeds. All Imagenet-related experiments were performed with 3 seeds. For hyperparameter tuning, we split Imagenet100’s training data into 80% training data and 20% validation data evenly across the different classes. We tested the following values for the entropy minimization coefficient β ∈ {0.1, 0.01, 0.001, 0.0001}. For the final evaluation, we trained the model on the entire training data.\n\nSelf-Adaptive Training models are trained using SGD with an initial learning rate of 0.1 and a momentum of 0.9.\n\nFood101/Imagenet100/ImagenetSubset. The models were trained for 500 epochs with a mini-batch size of 128. The learning rate was reduced by 0.5 every 25 epochs. The entropy-minimization term was β = 0.01.\n\n14\n\nCIFAR-10. The models were trained for 300 epochs with a mini-batch size of 64. The learning rate was reduced by 0.5 every 25 epochs. The entropy-minimization term was β = 0.001.\n\nStanfordCars. The models were trained for 300 epochs with a mini-batch size of 64. The learning rate was reduced by 0.5 every 25 epochs. The entropy-minimization term was β = 0.01.\n\nImagenet. The models were trained for 150 epochs with a mini-batch size of 256. The learning rate was reduced by 0.5 every 10 epochs. The entropy-minimization term was β = 0.001.\n\nC.2 COMPUTE\n\nThe experiments were primarily run on a GTX 1080 Ti. The CIFAR10 experiments took 1.5 hours for Self-Adaptive Training and Deep Gamblers. SelectiveNet experiments took 3 hours each. The Imagenet100 experiments took 2 days for Self-Adaptive Training and Deep Gamblers. SelectiveNet experiments took 2.75 days each. The ImagenetSubset experiments took 0.5-4.5 days each for Self-Adaptive Training and Deep Gamblers, depending on the number of classes. SelectiveNet experiments took 0.75-5.5 days each, depending on the number of classes.\n\nC.3\n\nIMAGENETSUBSET: CLASSES\n\nImagenetSubset comprises of multiple datasets ranging from 25 to 175 classes in increments of 25, i.e., {D25, D50, D75, D100, D125, D150, D175}. Let C25, C50, . . . , C175 represent the classes of the respective datasets. The classes for ImagenetSubset are uniform randomly sampled from the classes of Imagenet such that the classes of the smaller datasets are subsets of the classes of the larger datasets, i.e. D25 ⊂ D50 ⊂ D75 ⊂ · · · ⊂ D175 and C25 ⊂ C50 . . . C175. The list of Imagenet classes in each dataset is included below for reproducibility.\n\nC.3.1 C25\n\nn03133878 n03983396 n03995372 n03776460 n02730930 n03814639 n03666591 n03110669 n04442312 n02017213 n04265275 n01774750 n03709823 n09256479 n07715103 n04560804 n02120505 n04522168 n04074963 n02268443 n03291819 n02091467 n02486261 n03180011 n02100236\n\nC.3.2 C50 − C25\n\nn02106662 n01871265 n12057211 n04579432 n07734744 n02408429 n02025239 n03649909 n03041632 n02484975 n02097209 n03854065 n03476684 n04579145 n01739381 n02319095 n01843383 n02229544 n09288635 n02138441 n02119022 n07583066 n03534580 n02817516 n04356056\n\nC.3.3 C75 − C50\n\nn03424325 n04507155 n02112350 n03450230 n01616318 n01641577 n03630383 n01530575 n02102973 n04310018 n02134084 n01729322 n03250847 n02099849 n03544143 n03871628 n03777754 n04465501 n01770081 n03255030 n01910747 n03016953 n03485407 n03998194 n02129604\n\nC.3.4 C100 − C75\n\nn02128757 n03763968 n01677366 n03483316 n02177972 n03814906 n01753488 n02116738 n01755581 n02264363 n03290653 n13133613 n03929660 n04040759 n02317335 n02494079 n02865351 n03134739 n02102177 n04192698 n02814533 n04090263 n01818515 n01748264 n04328186\n\nC.3.5 C125 − C100\n\nn03930313 n02422106 n07714571 n02111277 n03706229 n03729826 n03344393 n07831146 n02090379 n06596364 n03187595 n04317175 n11939491 n04277352 n01807496 n02804610\n\n15\n\nCoverage 100 95 90 85 80 75 70\n\nSelf-Adaptive Training Deep Gamblers\n\n5.91 ± 0.04 3.73 ± 0.13 2.18 ± 0.11 1.26 ± 0.09 0.69 ± 0.04 0.37 ± 0.01 0.27 ± 0.02\n\n6.08 ± 0.00 3.71 ± 0.00 2.27 ± 0.00 1.29 ± 0.00 0.81 ± 0.00 0.44 ± 0.00 0.30 ± 0.00\n\nSelectiveNet MC-Dropout 6.47 ± 0.22 4.07 ± 0.12 2.49 ± 0.13 1.42 ± 0.08 0.86 ± 0.05 0.53 ± 0.06 0.42 ± 0.04\n\n6.79 ± 0.03 4.58 ± 0.05 2.92 ± 0.01 1.82 ± 0.09 1.08 ± 0.05 0.66 ± 0.05 0.43 ± 0.05\n\nTable 7: Comparison of existing Selective Classification baselines with MC-Dropout. The results of MC-Dropout are originally from Geifman & El-Yaniv (2017). For a given coverage, the bolded result indicate the lowest selective risk (i.e. best result) and underlined result indicate the second lowest selective risk.\n\nn02093991 n09428293 n03207941 n02132136 n04548280 n02793495 n03924679 n02112137 n02107312\n\nC.3.6 C150 − C125\n\nn03376595 n03467068 n02837789 n04467665 n04243546 n03530642 n04398044 n02113624 n13044778 n03188531 n01729977 n01980166 n02101388 n01629819 n01773157 n01689811 n02109525 n03938244 n02123045 n04548362 n04612504 n04264628 n02108551 n04311174 n02276258\n\nC.3.7 C175 − C150\n\nn03724870 n02087046 n09421951 n02799071 n07717410 n02906734 n02206856 n03877472 n01740131 n04523525 n03496892 n04116512 n03743016 n03759954 n04462240 n03788195 n02137549 n03866082 n02233338 n02219486 n02445715 n02974003 n01924916 n12620546 n02992211\n\nD APPENDIX: ADDITIONAL RESULTS\n\nBriefly summarised, the additional interesting results found are as follows: (1) In low coverage settings, selecting based on Softmax Response and Entropy on a vanilla classifier trained via the cross entropy loss outperform both SelectiveNet and Deep Gamblers. (2) SelectiveNet outperforms Deep Gamblers on moderate coverages (60%, 50%, and 40%) which is not in par with the previously reported results. We attribute the interesting results to our work being the first to evaluate these methods on large datasets at a wide range of coverages. Since previous works have mainly focused on toy datasets and high coverages (70 + %), they failed to capture these patterns. The main takeaway of these results, however, is that, across all the reported methods, selecting via Softmax Response is best.\n\nD.1 ANALYSIS OF SELECTED IMAGES: SELECTIVENET\n\nWe include in Figure 4 and Figure 5 examples of images where the selection head of SelectiveNet fails to generalise.\n\nD.2 SELECTION MECHANISMS: MC-DROPOUT\n\nIn Table 7, we see that MC-Dropout performs worse than the existing state-of-the-art methods for Selective Classification.\n\n16\n\nFigure 4: SelectiveNet (at 80% coverage) on Imagenet100: Incorrect and unconfident predictions according to its classifier but selected images according to its Selective Head. The selection score threshold for selecting images for prediction is 0.93. Included are the selection score according to the Selection Head, the image’s label, and the top-3 predicted classes according to the classifier and their respective classifier scores.\n\n17\n\nFigure 5: SelectiveNet (at 80% coverage) on Imagenet100: Correct and confident predictions according to its classifier but rejected images according to its Selective Head.\n\n18\n\nDataset\n\nCIFAR-10\n\nCoverage 100 95 90 85 80 75 70\n\nVanilla Classifier\n\nEntropy 6.61 ± 0.25 4.30 ± 0.19 2.63 ± 0.12 1.62 ± 0.10 1.01 ± 0.13 0.72 ± 0.08 0.57 ± 0.08\n\nSoftmax Response 6.61 ± 0.25 4.35 ± 0.17 2.63 ± 0.11 1.63 ± 0.12 0.99 ± 0.10 0.72 ± 0.08 0.55 ± 0.07\n\nTable 8: Comparison of selection based on Entropy and Softmax Response for a vanilla classifier trained with cross-entropy loss on CIFAR-10.\n\nDataset\n\nImagenet100\n\nCoverage 100 90 80 70 60 50 40 30 20 10\n\nVanilla Classifier\n\nEntropy 14.32 ± 0.14 9.14 ± 0.05 5.34 ± 0.12 3.04 ± 0.14 1.80 ± 0.14 1.22 ± 0.31 0.82 ± 0.32 0.63 ± 0.33 0.60 ± 0.28 0.30 ± 0.14\n\nSoftmax Response 14.32 ± 0.14 8.96 ± 0.13 4.99 ± 0.05 2.83 ± 0.12 1.70 ± 0.19 1.08 ± 0.28 0.77 ± 0.39 0.60 ± 0.28 0.60 ± 0.28 0.20 ± 0.28\n\nTable 9: Comparison of selection based on Entropy and Softmax Response for a vanilla classifier trained with cross-entropy loss on Imagenet100.\n\nD.3 SELECTION MECHANISMS: VANILLA CLASSIFIER\n\nD.3.1 CIFAR-10\n\nIn Table 8, the difference in performance between selecting according to entropy and selecting according to Softmax Response is not significant. We attribute this marginal difference to the saturatedness of the CIFAR-10 dataset.\n\nD.3.2\n\nIMAGENET100\n\nIn Table 9, we see that selecting according to Softmax Response clearly outperforms selecting according to entropy. We see that Softmax Response learns a less generalizeable clasifier (See performance on 100% coverage) than Self-Adaptive Training, Deep Gamblers, and SelectiveNet. However, interestingly, we found that Softmax Response outperforms both Deep Gamblers and SelectiveNet on low coverages (10%, 20%, 30%). Previous works failed to capture this pattern due to lack of evaluation on larger datasets and lower coverages.\n\nD.4 SELECTION MECHANISMS: DEEP GAMBLERS\n\nCIFAR-10. In these results (Table 10), we see that the difference in performance between the various selection mechanisms is marginal. Due to the marginal difference between errors, it is difficult to make conclusions from these results.\n\nImagenet100. In Table 10, we see that selecting according to Softmax Response and Entropy clearly outperforms the original selection mechanism.\n\nImagenetSubset. In Table 11, similar to Imagenet100, we see a clear substantial improvement when using Softmax Response as the selection mechanism instead of the original selection mechanism. Furthermore, we see that Entropy also outperforms the original selection mechanism.\n\n19\n\nDataset\n\nCIFAR-10\n\nImagenet100\n\nCoverage 100 95 90 85 80 75 70 100 90 80 70 60 50 40 30 20 10\n\nDG 6.08 ± 0.00 3.71 ± 0.00 2.27 ± 0.00 1.29 ± 0.00 0.81 ± 0.00 0.44 ± 0.00 0.30 ± 0.00 13.49 ± 0.52 8.42 ± 0.44 5.21 ± 0.32 3.30 ± 0.40 2.14 ± 0.37 1.55 ± 0.27 1.23 ± 0.38 1.09 ± 0.31 1.03 ± 0.31 0.80 ± 0.28\n\nDeep Gamblers DG + Entropy 6.08 ± 0.00 3.79 ± 0.00 2.14 ± 0.00 1.31 ± 0.00 0.84 ± 0.00 0.57 ± 0.00 0.41 ± 0.00 13.49 ± 0.52 8.25 ± 0.43 4.76 ± 0.37 2.70 ± 0.21 1.86 ± 0.32 1.35 ± 0.25 1.20 ± 0.11 1.00 ± 0.19 0.97 ± 0.21 0.73 ± 0.25\n\nDG + SR 6.08 ± 0.00 3.81 ± 0.00 2.16 ± 0.00 1.35 ± 0.00 0.85 ± 0.00 0.56 ± 0.00 0.43 ± 0.00 13.49 ± 0.52 8.11 ± 0.48 4.52 ± 0.38 2.58 ± 0.21 1.71 ± 0.32 1.31 ± 0.22 1.07 ± 0.19 0.96 ± 0.21 0.90 ± 0.22 0.53 ± 0.25\n\nTable 10: Deep Gamblers Results on CIFAR-10 and Imagenet100. Comparison of Selection Mechanism Results.\n\nDataset\n\nImagenetSubset\n\n# of Classes 175 150 125 100 75 50 25\n\nDG 3.77 ± 0.10 2.62 ± 0.03 2.58 ± 0.25 2.57 ± 0.04 2.60 ± 0.20 2.63 ± 0.12 1.60 ± 0.28\n\nDeep Gamblers DG + Entropy 3.75 ± 0.14 2.65 ± 0.26 2.40 ± 0.19 2.30 ± 0.01 2.29 ± 0.00 2.15 ± 0.05 1.22 ± 0.30\n\nDG + SR 3.62 ± 0.11 2.54 ± 0.24 2.22 ± 0.17 2.20 ± 0.04 2.22 ± 0.05 2.08 ± 0.07 1.30 ± 0.19\n\nTable 11: Deep Gamblers Results on ImagenetSubset (70% coverage) with various selective mechanisms.\n\nD.5 SELECTION MECHANISMS: SELF-ADAPTIVE TRAINING\n\nImagenetSubset. In addition to the Imagenet100 experiments, we also evaluate Self-Adaptive Training trained with the proposed entropy-regularised loss function on ImagenetSubset.\n\nIn Figure 6 (and Table 12 and Table 13), we see that training with the entropy-regularised loss function improves the scalability of Self-Adaptive Training when selecting according to Softmax Response.\n\nIn Figure 6, the results for SelectiveNet, Deep Gamblers, and Self-Adaptive Training on 70% coverage. Consistent with previous experiments, we see that both the selection mechanisms based on the classifier itself (predictive entropy and Softmax Response) significantly outperform the original selection mechanisms of the proposed methods. These results further support our conclusion that (1) the strong performance of these methods were due to them learning a more generalizable model and (2) the selection mechanism should stem from the classifier itself rather than a separate head/logit. Similarly, we see that Softmax Response is the state-of-the-art selection mechanism. In the experiments, we see that SelectiveNet struggles to scale to harder tasks. Accordingly, the achieved improvement in selective accuracy with Softmax Response (SR) increases as the number of classes increase. This suggests that the proposed selection mechanism is more beneficial for SelectiveNet as the difficulty of the task increases, i.e., improves scalability.\n\n20\n\n(a) SelectiveNet\n\n(b) Deep Gamblers\n\n(c) Self-Adaptive Training\n\n(d) SAT with Entropy Minimization\n\nFigure 6: ImagenetSubset at 70% coverage. (a), (b), and (c) Various Selective Models (d) Comparison of Self-Adaptive Training trained with the proposed entropy-regularised loss and without. The loss function improves the scalability of Self-Adaptive Training, particularly when using Softmax Response as the selection mechanism.\n\n21\n\nSAT\n\nSAT + Entropy\n\n# Classes 175 150 125 100 75 50 25\n\nSAT 3.03 ± 0.13 2.23 ± 0.16 2.32 ± 0.25 2.65 ± 0.19 2.60 ± 0.15 2.86 ± 0.34 1.79 ± 0.53\n\nSAT + EM 3.16 ± 0.15 2.20 ± 0.20 2.24 ± 0.22 2.52 ± 0.27 2.65 ± 0.40 2.34 ± 0.05 1.87 ± 0.05\n\nSAT + E 3.01 ± 0.09 2.01 ± 0.07 2.00 ± 0.19 2.30 ± 0.27 1.98 ± 0.14 1.98 ± 0.24 1.14 ± 0.19\n\nSAT+EM+E 2.80 ± 0.07 1.73 ± 0.19 1.90 ± 0.12 1.87 ± 0.06 1.73 ± 0.29 1.47 ± 0.16 1.14 ± 0.25\n\nSAT + Softmax Response SAT + SR 2.88 ± 0.15 1.89 ± 0.15 1.89 ± 0.16 2.19 ± 0.26 1.88 ± 0.19 1.98 ± 0.27 1.18 ± 0.11\n\nSAT+EM+SR 2.73 ± 0.07 1.71 ± 0.15 1.84 ± 0.14 1.81 ± 0.08 1.68 ± 0.27 1.47 ± 0.12 1.14 ± 0.25\n\nTable 12: Self Adaptive Training with Entropy Minimization Loss Function on ImagenetSubset at 70% coverage. We see that SAT+EM+SR performs the best and outperforms SAT by a statistically significant margin.\n\nD.6 ABLATION: VARYING ARCHITECTURE\n\nIn these experiments, we show generalizability across architectures of our proposed entropyminimization and softmax response methodology. In Tables 14, 15, and 16, we see that applying the entropy-minimization and Softmax Response methodology improves upon the state-of-the-art method’s performance significantly.\n\nD.7 RISK-COVERAGE PLOTS\n\nFigure 7 shows the risk coverage plots for Imagenet100, Food101, and StanfordCars results.\n\nD.8 LEARNING CURVES PLOTS\n\nFigure 8 shows that SAT and SAT+EM models have converged on StanfordCars.\n\n22\n\nDataset\n\nCoverage\n\n30\n\n40\n\n50\n\nImagenetSubset\n\n60\n\n70\n\n80\n\n90\n\n# of Classes 175 150 125 100 75 50 25 175 150 125 100 75 50 25 175 150 125 100 75 50 25 175 150 125 100 75 50 25 175 150 125 100 75 50 25 175 150 125 100 75 50 25 175 150 125 100 75 50 25\n\nSelf-Adaptive Training SAT 0.69 ± 0.12 0.44 ± 0.13 0.44 ± 0.07 0.71 ± 0.11 0.50 ± 0.15 0.76 ± 0.06 0.53 ± 0.00 0.94 ± 0.06 0.64 ± 0.03 0.76 ± 0.06 0.90 ± 0.15 0.84 ± 0.14 1.17 ± 0.39 0.67 ± 0.25 1.27 ± 0.12 0.81 ± 0.11 0.93 ± 0.11 1.11 ± 0.10 1.01 ± 0.08 1.44 ± 0.30 0.64 ± 0.23 1.77 ± 0.12 1.21 ± 0.10 1.34 ± 0.17 1.67 ± 0.07 1.51 ± 0.18 1.78 ± 0.16 0.93 ± 0.19 3.03 ± 0.13 2.23 ± 0.16 2.32 ± 0.25 2.65 ± 0.19 2.60 ± 0.15 2.86 ± 0.34 1.79 ± 0.53 5.85 ± 0.13 4.46 ± 0.05 4.78 ± 0.26 4.94 ± 0.41 4.91 ± 0.28 5.05 ± 0.14 4.13 ± 0.34 10.14 ± 0.32 8.30 ± 0.20 8.87 ± 0.04 8.90 ± 0.54 8.57 ± 0.44 8.79 ± 0.17 7.79 ± 0.43\n\nSAT + EM + SR 0.46 ± 0.05 0.16 ± 0.02 0.14 ± 0.09 0.15 ± 0.06 0.09 ± 0.00 0.16 ± 0.05 0.08 ± 0.11 0.59 ± 0.14 0.34 ± 0.06 0.25 ± 0.04 0.30 ± 0.00 0.23 ± 0.03 0.27 ± 0.13 0.07 ± 0.09 0.91 ± 0.16 0.47 ± 0.05 0.52 ± 0.07 0.56 ± 0.06 0.40 ± 0.03 0.37 ± 0.10 0.21 ± 0.08 1.44 ± 0.20 0.87 ± 0.04 0.95 ± 0.01 0.93 ± 0.03 0.78 ± 0.02 0.69 ± 0.08 0.49 ± 0.17 2.73 ± 0.07 1.71 ± 0.15 1.84 ± 0.14 1.81 ± 0.08 1.68 ± 0.27 1.47 ± 0.12 1.14 ± 0.25 5.37 ± 0.15 3.88 ± 0.19 3.94 ± 0.34 3.96 ± 0.02 3.78 ± 0.35 3.35 ± 0.36 2.80 ± 0.16 9.69 ± 0.17 8.08 ± 0.16 8.18 ± 0.59 8.18 ± 0.23 7.78 ± 0.43 6.96 ± 0.64 6.84 ± 0.19\n\nTable 13: ImagenetSubset Results\n\n23\n\nCoverage 100 90 80 70 60 50 40 30 20 10\n\nSAT 37.68 ± 1.11 32.34 ± 1.19 26.86 ± 1.15 21.34 ± 1.20 16.21 ± 1.10 11.59 ± 0.74 7.76 ± 0.43 4.56 ± 0.35 2.42 ± 0.36 1.49 ± 0.00\n\nResNet34 SAT+SR 37.68 ± 1.11 32.04 ± 1.18 26.39 ± 1.13 20.70 ± 1.23 14.92 ± 1.03 10.25 ± 0.97 6.32 ± 0.69 3.54 ± 0.36 1.93 ± 0.09 1.20 ± 0.21\n\nSAT+EM+SR 32.49 ± 2.33 26.60 ± 2.39 20.87 ± 2.33 15.84 ± 1.98 11.09 ± 1.50 7.00 ± 1.13 4.00 ± 0.87 2.20 ± 0.44 1.17 ± 0.28 0.80 ± 0.22\n\nTable 14: ResNet34: StanfordCars results\n\nCoverage 100 90 80 70 60 50 40 30 20 10\n\nSAT 31.78 ± 2.44 26.35 ± 2.43 21.20 ± 2.40 16.45 ± 2.14 12.13 ± 1.64 8.60 ± 1.27 5.94 ± 1.06 3.99 ± 0.60 2.55 ± 0.33 1.66 ± 0.26\n\nRegNetX SAT+SR 31.78 ± 2.44 25.68 ± 2.44 20.07 ± 2.54 14.77 ± 2.23 10.07 ± 1.58 6.43 ± 1.46 4.04 ± 0.88 2.47 ± 0.44 1.55 ± 0.00 0.91 ± 0.15\n\nSAT+EM+SR 27.75 ± 1.81 21.72 ± 1.90 16.21 ± 1.79 11.22 ± 1.54 7.39 ± 1.21 4.55 ± 0.96 2.88 ± 0.61 1.74 ± 0.34 1.10 ± 0.34 0.70 ± 0.26\n\nTable 15: RegNetX: StanfordCars results\n\nCoverage 100 90 80 70 60 50 40 30 20 10\n\nSAT 34.10 ± 0.73 28.61 ± 0.72 23.16 ± 0.47 17.94 ± 0.27 13.00 ± 0.24 9.23 ± 0.10 6.31 ± 0.22 3.81 ± 0.39 2.07 ± 0.34 1.08 ± 0.26\n\nShuffleNet SAT+SR 34.10 ± 0.73 28.27 ± 0.80 22.72 ± 0.63 17.14 ± 0.46 12.10 ± 0.46 7.68 ± 0.10 4.77 ± 0.24 2.97 ± 0.25 1.70 ± 0.30 0.99 ± 0.18\n\nSAT+EM+SR 32.90 ± 1.29 26.94 ± 1.33 21.13 ± 1.40 15.70 ± 1.42 10.89 ± 1.19 7.11 ± 0.87 4.49 ± 0.51 2.83 ± 0.28 1.43 ± 0.05 0.66 ± 0.31\n\nTable 16: ShuffleNet: StanfordCars results\n\n24\n\n(a) Imagenet100 Results: High Coverages\n\n(b) Imagenet100 Results: Low Coverages\n\n(c) Food101 Results: High Coverages\n\n(d) Food101 Results: Low Coverages\n\n(e) StanfordCars Results: High Coverages\n\n(f) StanfordCars Results: Low Coverages\n\nFigure 7: Risk Coverage Plots for Imagenet10, Food101, and StanfordCars. All plots show that SAT+EM+SR outperform SAT across all coverages, achieving state-of-the-art results.\n\n(a) StanfordCars: SAT\n\n(b) StanfordCars: SAT+EM\n\nFigure 8: Learning Curve (Convergence) Plots for StanfordCars.\n\n25",
    "reference": "# Summary Of The Paper\n\nThe paper tackles an interesting question regarding model decision making. Specifially, the authors are looking into the ability of a model to abstain from making a decision, when it has low confidence scores: The “Selective Classification problem”. \n\nThe authors demonstrate that: \n1) the current SOA models’s performance can be attributed to their ability to train a more classifier than is more generalizable (than the logit selection mechanism).\n2) semi-supervised learning-based models have better selective classification ability. \n3) the new selective classifier (SR: SoftMax response) could improve current SOA models\n\n# Strength And Weaknesses\n\nPros:\nI found the paper to be generally very well written and the experiments well motivated. The most interesting part of the paper to me was the connection with self-supervised models.\nI also appreciated the authors reporting the results with errorbar statistics. I wish more CS/AI researchers emulate this\nI found the results to be quite thorough. I liked seeing the results generalize to different tests and reported across the number of classes (where selective decision becomes more of a problem). \n\nCons:\nIn some instances, I felt the results were slightly overstated. The difference between the entropy loss and the SR loss is relatively subtle across the board. \nThe relation with semi-supervised learning-based models could be better explored. At the moment it is an observation. Readers would like to get an intuition for why this happens.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI found the paper well written. The experiments were thorough and the relation with semi-supervised learning is indeed novel, albeit less well fleshed out.\n\n# Summary Of The Review\n\nOverall the paper addresses an important question about Selective Classification. I was wondering if there are practical applications of this specific decision making approach. It would be a good addition to articulate the utility of this approach, especially in CV related tasks. The relation with the semi-supervised learning approaches is also interesting but less well understood. I encourage the authors to spend more time fleshing that section out in the paper (or in future work).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNDERSTANDING SELF-SUPERVISED PRETRAINING WITH PART-AWARE REPRESENTATION LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn this paper, we are interested in understanding self-supervised pretraining through studying the capability that self-supervised representation pretraining methods learn part-aware representations. The study is mainly motivated by that random views, used in contrastive learning, and random masked (visible) patches, used in masked image modeling, are often about object parts. We explain that masked image modeling is a part-to-part task: the masked patches of the object are hallucinated from the visible patches, and that contrastive learning is a part-to-whole task: the projection layer hallucinates the whole object representation from the object part representation learned from the encoder. The explanation suggests that the self-supervised pretrained encoder is required to understand the object part. We empirically compare the off-the-shelf encoders pretrained with several representative methods on object-level recognition and part-level recognition. The results show that the fully-supervised model outperforms self-supervised models for object-level recognition, and most self-supervised contrastive learning and masked image modeling methods outperform the fully-supervised method for part-level recognition. It is observed that the combination of contrastive learning and masked image modeling further improves the performance.\n\n1\n\nINTRODUCTION\n\nSelf-supervised representation pretraining has been attracting a lot of research efforts recently. The goal is to train an encoder that maps an image to a representation from visual contents without the necessity of human annotation, expecting that the encoder benefits the downstream tasks, e.g., segmentation and detection.\n\nThere are two main frameworks: contrastive learning1 and masked image modeling. Contrastive learning aims to maximize the agreement of the embeddings of random augmented views from the same image. Masked image modeling partitions an image into masked patches and visible patches, and makes predictions for masked patches from visible patches. Figure 1 gives examples of random views for contrastive learning and masked and visible patches for masked image modeling.\n\nWe observe that a random view and a set of masked (visible) patches usually contain a portion of an object. It is also reported in self-supervised learning methods, e.g., DINO (Caron et al., 2021) and iBOT (Zhou et al., 2021), that different attention heads in ViTs can attend to different semantic regions or parts of an object. In light of this, we attempt to understand self-supervised pretraining by studying the capability that the pretrained encoder learns part representations.\n\nWe present a part-to-whole explanation for typical contrastive learning methods (e.g., SimCLR (Chen et al., 2020), MoCo (Chen et al., 2021), and BYOL (Grill et al., 2020)): the embedding of the whole object is hallucinated from the embedding of the part of the object contained in the random crop through a projection layer. In this way, embeddings of random crops from the same image naturally agrees with each other. Masked image modeling is a part-to-part process: the embeddings of the masked patches of the object (a part of the object), are hallucinated from the visible patches (the other part of the object).\n\n1In this paper, we use contrastive learning to refer to the methods that compare random views, e.g., SimCLR,\n\nMoCo, and BYOL.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\nFigure 1: (a) original image, (b-c) two random crops, and (d-e) masked and visible patches.\n\nDeiT\n\nMoCo v3\n\nCAE\n\nFigure 2: Top-24 patch retrieval results with three frozen encoders of DeiT, MoCo v3, and CAE, by taking the patch in the red box as the query. It can be seen that the retrieved results from CAE and MoCo v3 are about the object part (wing and dog mouth) and more precise than DeiT (about the whole object) implying that self-supervised pretraining methods, CAE and MoCo v3 are stronger at learning part-aware representations than the fully-supervised method DeiT.\n\nWe empirically compare the supervised model DeiT (Touvron et al., 2020) and typical self-supervised representation pretraining methods, including MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), CAE (Chen et al., 2022a), MAE (He et al., 2021), BEiT (Bao et al., 2021), and iBOT (Zhou et al., 2021), on object-level recognition (image classification and object segmentation) and part-level recognition (patch retrieval, patch classification, and part segmentation). Figure 2 presents patch retrieval results using the encoders learned through CAE, MoCo v3, and DeiT, implying that the encoders pretrained by CAE and MoCo v3 are able to learn part-aware representations.\n\nThrough extensive studies and comparisons, we make the following observations. 1) DeiT outperforms contrastive learning and MIM methods except iBOT in object-level recognition tasks, which may benefit from its explicit object-level supervision. 2) In contrast, self-supervised methods learn better part-aware representations than DeiT. For example, while DeiT is superior to DINO and CAE by 0.4% and 2.3% on ADE20K object segmentation, DINO and CAE outperform DeiT by 1.6% and 1.1% on ADE20K part segmentation, respectively. 3) In contrastive learning, the encoder can learn part-aware information, while the projected representation tends to be more about the whole object. The evidence could be found in part retrieval experiments on MoCo v3, DINO, and iBOT. 4) The MIM method CAE shows good potential in part-aware representation learning. Interestingly, the method combines contrastive learning and MIM is promising, e.g., iBOT learns better representations at both object and part levels.\n\nTo summarize, this paper presents the following contributions:\n\n• We study the capability of learning part-aware representations as a way of understanding\n\nself-supervised representation pretraining.\n\n• We explain masked image modeling as a part-to-part task and contrastive learning as a partto-whole task, and speculate that self-supervised pretraining has the potential for learning part-aware representations.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n• We empirically compare several pretrained models on object-level and part-level recognition tasks, showing interesting findings with supporting evidence of the capability of part-aware representation learning for self-supervised learning.\n\n2 RELATED WORK\n\nContrastive learning. Contrastive pretraining has been an intense academic field in the CNN era. In this work, we use it to refer to methods for comparing random views (Caron et al., 2020; Chen et al., 2020; Zbontar et al., 2021; Xie et al., 2021a; Chen et al., 2021; Caron et al., 2021), including some instance discrimination work such as (Grill et al., 2020; Chen & He, 2021; Bardes et al., 2021). As one of the representative works, SimCLR (Chen et al., 2020) learns representations through maximizing agreement between different views of the same image in the latent space. BYOL (Grill et al., 2020) uses two asymmetrical networks to bootstrap latent representation without negative samples involved during the interaction. As vision transformer (ViT) (Dosovitskiy et al., 2021) shows excellent performance via supervised learning, it is adopted subsequently in contrastive pertaining, and numerous outstanding works are proposed. For example, MoCo v3 (Chen et al., 2021) observes the hidden instability while training self-supervised ViT and solves it by using a fixed random patch projection. DINO (Caron et al., 2021) explores new properties derived from self-supervised ViT and accordingly designs a learning strategy interpreted as a form of self-distillation with no labels.\n\nMasked image modeling (MIM). Masked image modeling is another self-supervised pretraining paradigm that attracts much attention recently. BEiT (Bao et al., 2021) follows masked language modeling in the natural language process (NLP) area and predicts tokens via mapping image patches by d-VAE (Ramesh et al., 2021). PeCo (Dong et al., 2021) boosts BEiT by taking into consideration more semantic information in visual tokens. MAE (He et al., 2021) learns rich hidden information by directly performing masked image reconstruction in RGB color space using ViT while SimMIM (Xie et al., 2021b) uses Swin-transformer (Liu et al., 2021). CAE (Chen et al., 2022a) adds a regressor between encoder and decoder, which is designed to align unmasked patches with masked ones, leading to a pure context encoder. Recently, a trend that combines MIM with siamese frameworks has surfaced and showed encouraging results including MST (Li et al., 2021), SplitMask (El-Nouby et al., 2021), iBOT (Zhou et al., 2021), dBOT (Liu et al., 2022), and SIM (Tao et al., 2022).\n\nUnderstanding self-supervised contrastive pretraining. The studies on understanding contrastive pretraining (Saunshi et al., 2022; Chen et al., 2022b; Zhong et al., 2022; Wei et al., 2022) mainly focus on random augmentations (views), contrastive loss function and its variants under the assumption that: the augmentations of inputs from the same class have significant overlap in the representation space, but there is little overlap for inputs from different classes. Our work is complementary to these studies. Inspired by the observation that random views usually contain a portion of an object, and methods (Caron et al., 2021; Zhou et al., 2021) show that different attention heads in ViTs can attend to different semantic regions of an object, we investigate what the encoder and the projector do in typical self-supervised contrastive pretraining. We speculate that the pretraining task is a part-to-whole problem, predicting the representation of the whole object through the projector from the representation (obtained from the encoder) of the part of an object. We use empirical results to verify our analysis.\n\nUnderstanding self-supervised masked image modeling. The comparison of attention in different layers between the pretrained models from MIM and the supervised approach is conducted: MIM pretraining brings locality to the trained model with sufficient diversity on the attention heads (Xie et al., 2022a). Consistent with the analysis in NLP, empirical studies are conducted in Xie et al. (2022b) to verify that MIM benefits from larger models, more data, and longer training. CAE (Chen et al., 2022a) gives the comparison between contrastive and MIM and shows MIM cares about all patches and thus achieves better results for fine-tuning. Cao et al. (2022) provides a mathematical understanding of MIM. Kong & Zhang (2022) points out that the learned occlusion invariant feature contributes to the success of MIM. In this work, we speculate that masked image modeling is a partto-part process: the embeddings of the masked part of the object are hallucinated from the visible part using the position information of the masked patches, leading to better part-aware representation than the supervised model DeiT (Touvron et al., 2020).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: The pipeline of a typical contrastive learning approach. Two augmented views, red box and blue box, are generated from the original image. The augmented view in red is fed into the encoder and the projector, and then the predictor (which does not appear in earlier works like MoCo (Chen et al., 2021) and SimCLR (Chen et al., 2020)), and the view in blue is fed into the encoder and the projector. The two outputs are expected to be aligned. The gradient is stopped for the bottom stream.\n\n3 UNDERSTANDING CONTRASTIVE LEARNING AND MASK IMAGE MODELING\n\n3.1 CONTRASTIVE LEARNING\n\nContrastive learning aims to learn the encoder through maximizing the agreement between differently augmented views of the same image in the representation space. An example pipeline is depicted in Figure 3. Given an image I, the augmentations, e.g., random cropping, random color distortion, and random Gaussian blur, are applied to generate a set of N augmented views, {V1, V2, · · · , VN }. An augmented view Vn is fed into an encoder Encoder, generating the encoded representation xn, and followed by a projector, generating the projection zn. The basic goal is to maximize the agreement between the projections {z1, z2, · · · , zN }, i.e., minimize the loss\n\nLCPT =\n\n(cid:88)N\n\n(cid:88)N\n\ni=1\n\nj=1\n\nl(Projector(Encoder(Vi)), Projector(Encoder(Vj))).\n\n(1)\n\nIn the formulation with a contrastive loss, the agreement between the projections of random augmentations from different images is minimized.\n\nPart-to-whole prediction explanation. Let us consider two crops randomly sampled from the original image (see the examples given in Figure 1(b-c)). The encoded representation of the first crop is expected to describe a part of the object dog; the encoded representation of the second crop is expected to describe another part of the object dog2. The two representations are related but different. Contrastive learning methods project the two encoded representations into two projected representations that are expected to agree. We hypothesize that the projection process maps the encoded part representation to the representation of the whole object3. Through this way, the projected representations will agree to different views from the same image. It is assumed that the part-to-whole projection is more reliable if the encoded representation is semantically richer and is able to describe the part information. The part-to-whole process suggests that the encoder pretrained by contrastive learning methods is potentially capable of learning part-aware representations.\n\nFigure 4 provides patch search results of a representative contrastive learning method MoCo v3 (Chen et al., 2021) based on the encoded representations before and after the projections. One can see that the results through the encoded representations are mainly about the local part, and the results through the projections tend to include the other parts of the same object. In other words, the projections tend to be about the whole object. Similar observations are also shown in Chen et al. (2022b). The search results verify the part-to-whole hypothesis.\n\n3.2 MASKED IMAGE MODELING\n\nMask image modeling is the task of predicting some parts of an image from the remaining parts. An augmented view of an image is partitioned into patches, R = {R1, R2, . . . , RM }. The task is\n\n2In this paper, we mainly study the capability of learning representations about objects and parts, and leave\n\nthe study of representations of the background as the future work.\n\n3It is said that different parts have common causes in the external world (Becker & Hinton, 1992). Our\n\nhypothesis is that the common cause is the whole object.\n\n4\n\nAlignmentProjectorEncoderEncoderPredictorXizi(a)(b)XjProjectorzjUnder review as a conference paper at ICLR 2023\n\nEncoded representation\n\nProjected representation\n\nFigure 4: Illustration of patch search results using encoded representations and projections (pretrained with MoCo v3 as). Left: patch search results with encoded representations. Right: patch search results with projections. In each result, the small patch encircled by the red box is taken as the query. It can be seen that for encoded representations, the returned patches are about the same part, and for projections, the result patches are about the same object, verifying the part-to-whole hypothesis.\n\nto predict a subset of patches Rm, named masked patches, from the remaining patches Rv, named visible patches. Considering contrastive learning that explicitly compares representations of random views, we take context autoencoder (CAE) (Chen et al., 2022a) as an example that explicitly predicts the encoded representations of the masked patches from the encoded representations of the visible patches4.\n\nOne goal of CAE (illustrated in Figure 5), which we call masked representation modeling (MRM), is to maximize the agreement between the predictions of the representations of masked patches (through a regressor) and the representation of masked patches computed from the encoder by minimizing the loss\n\nlMRM(Regressor(Encoder(Pv)), Encoder(Pm)).\n\n(2)\n\nHere, we do not include the positional embeddings of masked and visible patches for clarity. It is noted that MRM differs from contrastive learning: MRM does not compare multiple random views, but compares the regressed representations for masked patches and the encoded representations of masked patches. In addition, there is another loss for target prediction (reconstruction) for the masked patches, which is commonly used in masked image modeling (MIM) methods:\n\nlMIM(Decoder(Regressor(Encoder(Pv))), Target(Pm)),\n\n(3)\n\nwhere Target(Pm) is a function to map the masked patches to the targets, e.g., d-VAE (Ramesh et al., 2021) token used in CAE and BeiT (Bao et al., 2021), or normalized RGB values used in MAE (He et al., 2021).\n\nPart-to-part prediction explanation. The masked image modeling approaches, including CAE, MAE, and BEiT, make use of the positions of masked patches for making predictions for masked patches from visible patches. The visible patches and masked patches often contain different parts of an object. In other words, MIM aims to predict the masked part of an object from the visible part. We name this a part-to-part process. There are two part-to-part tasks: one is to reconstruct the part targets from the visible part representations (MAE and CAE) or from the visible part raw pixels (BEiT), and the other one is to regress the masked part representations (CAE). The part-to-part process suggests that the encoder pretrained by MIM methods is potentially capable of learning part-aware representations. Figure 2 illustrates the capability with the patch retrieval results.\n\n4Some MIM methods, such as BEiT (Bao et al., 2021) and MAE (Masked Autoencoder) (He et al., 2021) do not have an explicit process to predict the encoded representations of masked patches, instead, directly reconstruct the targets.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: The pipeline of an MIM approach, context autoencoder (CAE). An augmented view (in blue) of the image is partitioned into visible and masked patches. The CAE approach feeds visible patches into the encoder and extracts their representations Zv and then completes the pretext task by predicting the representations Zm of the masked patches from the visible patches in the encoded representation space with latent contextual regressor and alignment constraint, and mapping predicted representations Zm of masked patches to the targets. The pretrained encoder in (a) is applied to downstream tasks by simply replacing the pretext task part (b, c) with the downstream task completion part.\n\nTable 1: Top-1 accuracy with linear probing, and attentive probing (Chen et al., 2022a), on the ImageNet classification benchmark (Deng et al., 2009).\n\nTable 2: Linear evaluation of ADE20K (Zhou et al., 2019) object-level semantic segmentation (150 classes) using 4× upsampling and a single 1 × 1 convolutional layer on frozen backbones.\n\nMethod\n\nLinear\n\nAttentive\n\nMethod\n\nmIoU\n\nmAcc\n\naAcc\n\n81.8\n\n76.2 77.3\n\nSupervised Model: DeiT Contrastive Learning: MoCo v3 DINO Masked Image Modeling (MIM): BEiT MAE CAE Contrastive Learning + MIM: iBOT\n\n41.8 67.8 70.4\n\n79.5\n\n81.8\n\n77.0 77.8\n\n51.9 74.2 77.1\n\n79.8\n\n44.2\n\n43.9 43.5\n\nSupervised Model: DeiT 34.9 Contrastive Learning: 34.7 MoCo v3 DINO 34.5 Masked Image Modeling (MIM): BEiT MAE CAE Contrastive Learning + MIM: iBOT\n\n17.8 27.1 32.6\n\n38.3\n\n23.7 34.8 42.2\n\n47.4\n\n75.4\n\n75.9 76.1\n\n64.9 71.6 75.2\n\n78.1\n\n4 EXPERIMENTS\n\nWe study seven representative methods with the same ViT-B encoder, including a supervised method DeiT (Touvron et al., 2020); contrastive learning methods MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021); masked image modeling (MIM) methods BEiT (Bao et al., 2021), MAE (He et al., 2021), and CAE (Chen et al., 2022a); and iBOT (Zhou et al., 2021) that combines contrastive learning and MIM. We take the training epochs specified in each work to ensure that all compared models are properly trained: 300 for DeiT, 300 (6005) for MoCo v3, 400 (16005) for DINO and iBOT, 800 for BEiT, and 1600 for MAE and CAE. Frozen encoders are used in all experiments to understand what these different representation pretraining methods learn. More details can be found in Appendix A.1.\n\n4.1 OBJECT-LEVEL RECOGNITION\n\nWe benchmark two widely-studied object-level recognition, i.e., image classification and semantic segmentation to show the capability that the pretrained encoder learns object-level representations.\n\n5The number of effective training epochs introduced in Zhou et al. (2021).\n\n6\n\nMaskqueryMaskedpatchAlignmentLatentcontextualregressorEncoderEncoderDecoderTargetXvXm ̄Xm(a)(b)(c)VisiblepatchUnder review as a conference paper at ICLR 2023\n\nTable 3: Part retrieval (AP, %) and classification (accuracy, %) results on the cropped part patches of CUB-200-2011 and COCO. The “Encoded\" and “Projected\" refer to the encoded and projected representations. “Linear\" and “Attentive\" columns denote the linear probing and attentive probing accuracy, respectively.\n\nMethods\n\nCUB-200-2011 Encoded Projected\n\nCOCO Encoded Projected\n\nCUB-200-2011 Linear Attentive\n\nCOCO Linear Attentive\n\nPart Retrieval\n\nPart Classification\n\n–\n\n35.0\n\n28.4 31.7\n\n50.8 48.9\n\nSupervised Model: DeiT Contrastive Learning: MoCo v3 DINO Masked Image Modeling (MIM): –\nBEiT –\nMAE CAE –\nContrastive Learning + MIM: 31.2 iBOT\n\n27.9 28.5 58.0\n\n49.3\n\n44.1\n\n52.3 51.8\n\n35.3 37.1 57.0\n\n59.2\n\n–\n\n90.9\n\n92.9\n\n88.5\n\n91.4\n\n36.8 41.2\n\n– –\n–\n\n93.8 93.2\n\n55.4 86.9 89.5\n\n96.0 95.2\n\n86.5 92.8 95.8\n\n92.4 91.7\n\n69.3 88.0 91.1\n\n95.3 94.5\n\n86.5 93.9 95.5\n\n41.5\n\n93.8\n\n95.8\n\n92.1\n\n95.1\n\nImage classification. We report the linear probing, and attentive probing results of the selected models on ImageNet (Deng et al., 2009). For attentive probing, we follow the protocol in CAE (Chen et al., 2022a) that append a cross-attention layer together with a batch normalization layer and a linear classifier.\n\nWe have the following observations from Table 1. 1) The supervised model, DeiT performs better than self-supervised models at object-level recognition. 2) The models that leverage contrastive learning, i.e., MoCo, DINO, and iBOT, show superior linear probing performance than MIM-based models, demonstrating they contain more object-aware high-level semantics. 3) MIM-based models, e.g., CAE, show inferior results in linear probing while competitive results with contrastive-based methods in attentive probing. The reason might be that MIM is capable of attending to all the regions, including non-object regions in an image, thus needs a spatial feature selection step to attend to the object part, which is pointed out in Chen et al. (2022a). BEiT and MAE perform inferior, implying that the two methods are less capable of learning semantics.\n\nObject-level semantic segmentation. We perform linear evaluation on ADE20K (Zhou et al., 2019) to show the object-level semantic capabilities of the pretrained models. A 4× bilinear interpolation and a single 1 × 1 convolutional layer for pixel labeling are attached to the frozen encoder.\n\nWe can see from Table 2 that the supervised model DeiT outperforms all self-supervised models except iBOT, including contrastive learning and MIM methods on ADE20K object-level segmentation. This implies that in general the self-supervised models are not strong at object-level understanding, which is consistent with the observations for image classification. iBOT (Zhou et al., 2021), as a combination of contrastive learning and MIM, shows surprisingly better performance than the supervised model DeiT on ADE20K, implying the power of combining contrastive learning and masked image modeling for downstream tasks.\n\n4.2 PART-LEVEL RECOGNITION\n\nSelf-supervised methods like iBOT (Zhou et al., 2021) and DINO (Caron et al., 2021) qualitatively show that different attention heads in ViTs can attend to different semantic regions of an object. We conduct the quantitative evaluation for part-aware representation obtained by pretrained models that is not well explored before, through three part-level recognition tasks, part retrieval, part classification, and part segmentation.\n\nPart retrieval. We conduct part retrieval experiments on two datasets, CUB-200-2011 (Wah et al., 2011) and COCO (Lin et al., 2014). We build the part patch databases by cropping the patches centered at the keypoint. We consider four and three keypoints from the two datasets, respectively. For each keypoint, we find the minimum L2 distance (d) from the distances between it and all the\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Part-level linear semantic segmentation results on ADE20K-Part, Pascal-Part, and LIP datasets.\n\nMethods\n\nADE20K-Part 209 Part Classes mAcc\n\nmIoU\n\naAcc\n\nPascal-Part 193 Part Classes\n\nLIP 19 Part Classes\n\nmIoU mAcc\n\naAcc\n\nmIoU mAcc\n\naAcc\n\n34.7\n\n27.3\n\n34.7 36.8\n\n27.1 28.9\n\nSupervised Model: DeiT Contrastive Learning: MoCo v3 DINO Masked Image Modeling (MIM): 25.8 BEiT 35.0 MAE CAE 36.9 Contrastive Learning + MIM: 40.0 iBOT\n\n18.6 26.3 28.4\n\n32.2\n\n69.2\n\n27.4\n\n36.1\n\n65.8\n\n41.4\n\n52.6\n\n73.5\n\n70.1 70.3\n\n58.2 67.3 71.1\n\n27.1 27.8\n\n14.8 24.3 27.8\n\n35.8 36.5\n\n21.4 32.9 37.0\n\n66.0 66.4\n\n47.0 61.5 66.3\n\n41.9 41.0\n\n27.2 38.2 43.7\n\n53.0 51.9\n\n36.5 48.7 55.1\n\n74.5 74.0\n\n60.1 71.3 75.9\n\n73.4\n\n30.7\n\n40.0\n\n69.7\n\n44.6\n\n55.7\n\n76.6\n\nother keypoints in the same image, then crop a d × d patch centered at this keypoint and resize it to 224 × 224. We use the cosine distance as the patch distance and evaluate the retrieval performance using average precision (AP) as the retrieval metric.\n\nThe results are provided in Table 3. We have the following observations. 1) Self-supervised models except BEiT and MAE outperform the supervised model DeiT, indicating the capability that contrastive learning and CAE learn part-aware representations. BEiT and MAE perform inferior, consistent to the observations in ImageNet classification in Table 1. 2) iBOT performs the best, and the reason might be that the capability of learning part-aware representations is boosted by making use of both contrastive learning and masked image modeling.\n\nWe also report the part retrieval performance of the projected representations of contrastive learning methods in Table 3. The performance is much lower than the encoded representations. This provides an extra evidence for the part-to-whole hypothesis of contrastive learning: the projected representations are more about the whole object.\n\nPart classification. We further conduct part classification experiments on the datasets used for part retrieval. We consider two kinds of extra learnable layers, linear probing and attentive probing, for classification. The results in Table 3 show that: 1) While DeiT performs the best in the image classification task (see Table 1), for part classification, contrastive-based methods like MoCo v3, DINO, and iBOT outperform DeiT by more than 2% under both linear and attentive probing settings. 2) Though MIM-based models CAE and MAE are inferior to DeiT in object-level classification (e.g., more than 10% and 4% lower in linear and attentive probing), they show competitive performance in linear probing and higher results than DeiT in attentive probing, demonstrating they learn better partaware representations. 3) BEiT is inferior to other works, and iBOT has good performance, implying that the probing quality of pretrained encoders is a good indicator for downstream performance.\n\nPart segmentation. We perform part-level linear semantic segmentation to study the finer-grained part representation modeling capability of different pretraining paradigms on three widely used datasets: ADE20K-Part (Zhou et al., 2019) containing 209 parts from the ADE20K dataset (Zhou et al., 2019), Pascal-Part (Chen et al., 2014) including 193 part categories, and LIP (Gong et al., 2017) consisting of 19 semantic human part labels. Similar to the object-level semantic segmentation experiments, linear evaluation is employed here. We maintain the same training protocols for all methods for fair comparisons. See Appendix A.2 and A.4 for dataset and training details.\n\nThe results are reported in Table 4 with the following observations. 1) Contrastive learning models, i.e., MoCo v3 and DINO, achieve competitive performance with the supervised model DeiT: DINO outperforms DeiT on ADE20K-Part and Pascal-Part, and MoCo v3 outperforms DeiT on LIP. 2) The MIM model CAE, outperforms DeiT by large margins on all three datasets, e.g., 1.1% on ADE20K-Part and 2.3% on LIP, indicating CAE learns good part-aware representations. Similar to part retrieval, possibly due to pretraining quality in representation encoding, BEiT and MAE perform\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Comparisons between object-level and part-level semantic segmentation on ADE20K and Pascal-Part datasets. Though the supervised DeiT is superior over self-supervised models (i.e., MoCo v3, DINO, MAE, CAE) on object-level segmentation, it is generally inferior to self-supervised models on part segmentation, demonstrating self-supervised methods learn good part-aware representations. iBOT enjoys the benefits of contrastive learning and MIM. See Appendix A.3 for detailed results.\n\ninferior. 3) Compared with object-level segmentation results in Table 2, DeiT learns better object-level semantics by explicit supervision than both contrastive learning and MIM, however, it is generally inferior to self-supervised models on part segmentation. 4) The model iBOT, which leverages both contrastive learning and MIM, outperforms all other works on three datasets, demonstrating its powerful capability in learning finer part-level semantics. Combining the two self-supervised learning techniques is thus a promising direction.\n\nIn summary, we show that self-supervised methods are potentially capable of learning part-aware representations. Among them, CAE is a representative MIM work, showing good performance by explicitly predicting the encoded representations of the masked patches in the encoding space; contrastive learning methods MoCo v3 and DINO outperform BEiT and MAE; and iBOT performs the best by combining contrastive learning and MIM. The observations are evidenced by three part-based segmentation benchmarks consistently.\n\n4.3 OBSERVATION SUMMARY BETWEEN OBJECT-LEVEL AND PART-LEVEL SEGMENTATION\n\nWe conduct both object-level and part-level linear semantic segmentation on different hierarchies of the same dataset. Considering that the 209 classes in ADE20K-Part are basically chosen from 59 object classes, we denote the 59-object dataset as ADE20K-Object. Similarly, Pascal-Object consists of 16 object categories, corresponding to the 193 part categories in Pascal-Part.\n\nThe results in Figure 6 show that: although the supervised DeiT is superior over contrastive learning and masked image modeling methods on ADE20K-Object and Pascal-Object except iBOT, it is generally inferior to self-supervised models on ADE20K-Part and Pascal-Part, demonstrating selfsupervised methods can learn good part-aware representations. Similar observations could be found from the object classification in Table 1 and part classification in Table 3.\n\nIn comparison to contrastive learning, CAE shows a stronger capability of learning part-aware representations, and a weaker capability of learning object-level semantics. The superiority of iBOT, a combination of contrastive learning and masked image modeling, demonstrates that it enjoys the benefits of contrastive learning and masked image modeling.\n\n5 CONCLUSION\n\nWe attempt to study the capability of learning part-aware representations of self-supervised representation pretraining methods. We provide speculations for contrastive learning and masked image modeling: part-to-whole and part-to-part, with empirical results justifying the speculations. Our study presents an aspect to understand what self-supervised representation pretraining methods learn.\n\nFuture work. The strong capability of part-aware representation learning is one of the properties of self-supervised pretraining. There should be other characteristics that are leaved as the future work.\n\n9\n\nDeiTMoCo v3DINOMAECAEiBOT3036424854mIoU (%)ADE20K-ObjectDeiTMoCo v3DINOMAECAEiBOT151821242730ADE20K-PartDeiTMoCo v3DINOMAECAEiBOT5060708090Pascal-ObjectDeiTMoCo v3DINOMAECAEiBOT151821242730Pascal-PartUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nHangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of image transformers.\n\narXiv:2106.08254, 2021.\n\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization\n\nfor self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.\n\nSuzanna Becker and Geoffrey E. Hinton. A self-organizing neural network that discovers surfaces in\n\nrandom-dot stereograms. Nature, 355(6356):161–163, 1992.\n\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context.\n\nIn CVPR, pp. 1209–1218, 2018.\n\nShuhao Cao, Peng Xu, and David A Clifton. How to understand masked autoencoders. arXiv preprint\n\narXiv:2202.03670, 2022.\n\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.\n\nMathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and\n\nArmand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 1597–1607. PMLR, 2020.\n\nXianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. In CVPR, pp. 1971–1978, 2014.\n\nXiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin, Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo, Gang Zeng, and Jingdong Wang. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026, 2022a.\n\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, pp.\n\n15750–15758, 2021.\n\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision\n\ntransformers. In ICCV, pp. 9640–9649, 2021.\n\nYubei Chen, Adrien Bardes, Zengyi Li, and Yann LeCun. Intra-instance vicreg: Bag of self-supervised\n\nimage patch embedding. arXiv preprint arXiv:2206.08954, 2022b.\n\nMMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and\n\nbenchmark. https://github.com/open-mmlab/mmsegmentation, 2020.\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\n\nhierarchical image database. In CVPR, pp. 248–255. Ieee, 2009.\n\nXiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. Peco: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR. OpenReview.net, 2021.\n\nAlaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan Laptev, Hervé Jegou, and Edouard arXiv preprint\n\nGrave. Are large-scale datasets necessary for self-supervised pre-training? arXiv:2112.10740, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.\n\nPASCAL Visual Object Classes Challenge 2010 (VOC2010) Results. network.org/challenges/VOC/voc2010/workshop/index.html, 2010.\n\nThe http://www.pascal-\n\nKe Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, and Liang Lin. Look into person: Selfsupervised structure-sensitive learning and a new benchmark for human parsing. In CVPR, July 2017.\n\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked\n\nautoencoders are scalable vision learners. arXiv preprint arXiv:2111.06377, 2021.\n\nXiangwen Kong and Xiangyu Zhang. Understanding masked image modeling via learning occlusion\n\ninvariant feature. arXiv preprint arXiv:2208.04164, 2022.\n\nZhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, et al. Mst: Masked self-supervised transformer for visual representation. Advances in Neural Information Processing Systems, 34:13165–13176, 2021.\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pp. 740–755. Springer, 2014.\n\nXingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, and Rongrong Ji. Exploring target representa-\n\ntions for masked autoencoders. arXiv preprint arXiv:2209.03917, 2022.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pp. 10012–10022, 2021.\n\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pp. 8821–8831. PMLR, 2021.\n\nNikunj Saunshi, Jordan T. Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev Arora, Sham M. Kakade, and Akshay Krishnamurthy. Understanding contrastive learning requires incorporating inductive biases. CoRR, abs/2202.14037, 2022. URL https://arxiv.org/abs/2202. 14037.\n\nChenxin Tao, Xizhou Zhu, Gao Huang, Yu Qiao, Xiaogang Wang, and Jifeng Dai. Siamese image modeling for self-supervised vision representation learning. arXiv preprint arXiv:2206.01204, 2022.\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.\n\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\n\nbirds-200-2011 dataset. 2011.\n\nYixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning via feature distillation. arXiv preprint arXiv:2205.14141, 2022.\n\nZhenda Xie, Yutong Lin, Zhuliang Yao, Zheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-supervised\n\nlearning with swin transformers. arXiv preprint arXiv:2105.04553, 2021a.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. arXiv preprint arXiv:2111.09886, 2021b.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nZhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han Hu, and Yue Cao. Revealing the dark secrets of masked image modeling. CoRR, abs/2205.13543, 2022a. doi: 10.48550/arXiv.2205. 13543. URL https://doi.org/10.48550/arXiv.2205.13543.\n\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Yixuan Wei, Qi Dai, and Han Hu. On data scaling in masked image modeling. CoRR, abs/2206.04664, 2022b. doi: 10.48550/arXiv.2206.04664. URL https://doi.org/10.48550/arXiv.2206.04664.\n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised\n\nlearning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021.\n\nYuanyi Zhong, Haoran Tang, Junkun Chen, Jian Peng, and Yu-Xiong Wang. Is self-supervised\n\nlearning more robust than supervised learning? arXiv preprint arXiv:2206.05259, 2022.\n\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\n\nSemantic understanding of scenes through the ade20k dataset. IJCV, 127(3):302–321, 2019.\n\nJinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Ibot:\n\nImage bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 MODEL DESCRIPTION\n\nFor all the models involved in the experiments including DeiT (Touvron et al., 2020), MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), BEiT (Bao et al., 2021), MAE (He et al., 2021), CAE (Chen et al., 2022a), and iBOT (Zhou et al., 2021), we use their official code to implement the encoders. It is worth noticing that for DINO and iBOT, we choose the checkpoint of the teacher models as they have been reported to perform better than the student models in their papers (Caron et al., 2021; Zhou et al., 2021).\n\nA.2 DATASETS\n\nADE20K (Zhou et al., 2019) is one of the most challenging benchmarks, containing 150 fine-grained semantic concepts and a variety of scenes with 1,038 image-level labels. There are 20,210 images in the training set and 2,000 images in the validation set. We choose 59 out of total 150 semantic concepts that are concrete objects containing parts (Zhou et al., 2019), termed ADE20K-Object. We also select 209 part categories that emerge both in the training set and the validation set, called ADE20K-Part.\n\nPascal-Part (Chen et al., 2014) is a set of additional annotations for PASCAL VOC 2010 (Everingham et al., 2010), thereby holding the same statistics as those of PASCAL VOC 2010. It provides segmentation masks for each part of objects. Concretely, the dataset includes 20 object-level categories and 193 part-level categories. In our experiments, we remove 4 object categories that do not contain parts including boat, table, chair, and sofa.\n\nLIP (Gong et al., 2017) is a large-scale benchmark for human parsing research, which includes 50,462 images with pixel-wise annotations on 19 semantic part labels. In detail, it includes 19,081 full-body images, 13,672 upper-body images, 403 lower-body images, 3,386 head-missed images, 2,778 back-view images and 21,028 images with occlusions. There are 30,462 images in the training set and 10,000 images in the validation set. The rest 10,000 images are served as the test set with missing labels for competition evaluation.\n\nCUB-200-2011 (Wah et al., 2011) is a popular benchmark for fine-grained image classification, and also provides bounding box and part location annotations. It contains 11,788 images of 200 bird species and 15 part keypoint annotations per bird. In this work, we mainly leverage its part keypoint annotations. And only 4 part categories (right eye, right leg, left wing, and tail) are chosen to be considered in our experiments, to make sure that the selected keypoints are far enough away from each other and enough context information can be contained in the cropped patches. (We also tried using all keypoints and the conclusion is consistent.)\n\nCOCO (Caesar et al., 2018), as one of the most widely-used human pose estimation datasets, contains more than 200,000 images and 250,000 labeled person instances. Similar to CUB-200-2011 mentioned above, only 3 (nose, right wrist, and left ankle) of its 17 keypoint categories are considered in our experiments.\n\nA.3 DETAILED RESULTS FOR OBJECT-LEVEL AND PART-LEVEL SEGMENTATION\n\nIn this section, we provide detailed comparisons between object-level and part-level semantic segmentation in Table 5 and Table 6. Similar observations as in Figure 6 in the main paper are found: although the supervised DeiT is superior over self-supervised methods on ADE20K-Object and Pascal-Object except iBOT, it is generally inferior to self-supervised models on ADE20K-Part and Pascal-Part, demonstrating self-supervised methods can learn good part-aware representations. BEiT and MAE perform inferior, perhaps because the two methods do not have an explicit process to predict the encoded representations of masked patches, instead, directly reconstruct the targets.\n\nA.4 EXPERIMENT DETAILS\n\nPart retrieval. In our part retrieval experiments, we directly use the pretrained encoders to extract features, without additional training process. For each method, we take the better one from the class\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Linear semantic segmentation results on ADE20K-Object and ADE20K-Part.\n\nMethods\n\nObject Seg on ADE20K-Object 59 Object Classes\n\nmIoU\n\nmAcc\n\naAcc\n\nPart Seg on ADE20K-Part 209 Part Classes mAcc\n\nmIoU\n\naAcc\n\n62.9\n\n52.6\n\n50.2 50.8\n\nSupervised Model: DeiT Contrastive Learning: MoCo v3 DINO Masked Image Modeling (MIM): BEiT MAE CAE Contrastive Learning + MIM: iBOT\n\n28.6 41.0 47.4\n\n55.2\n\n60.4 60.8\n\n37.2 50.6 58.4\n\n65.1\n\n83.8\n\n83.6 83.9\n\n73.4 79.9 82.9\n\n85.6\n\n27.3\n\n34.7\n\n69.2\n\n27.1 28.9\n\n18.6 26.3 28.4\n\n34.7 36.8\n\n25.8 35.0 36.9\n\n70.1 70.3\n\n58.2 67.3 71.1\n\n32.2\n\n40.0\n\n73.4\n\nTable 6: Linear semantic segmentation results on Pascal-Object and Pascal-Part.\n\nMethods\n\nObject Seg on Pascal-Object 16 Object Classes mAcc\n\nmIoU\n\naAcc\n\nPart Seg on Pascal-Part 193 Part Classes mAcc\n\nmIoU\n\naAcc\n\n92.2\n\n89.4 88.0\n\nSupervised Model: DeiT Contrastive Learning: MoCo v3 DINO Masked Image Modeling (MIM): BEiT MAE CAE Contrastive Learning + MIM: iBOT\n\n56.4 76.1 83.3\n\n92.1\n\n95.3\n\n93.7 92.7\n\n69.0 84.6 89.7\n\n95.3\n\n96.8\n\n95.7 95.3\n\n76.8 89.5 93.2\n\n97.1\n\n27.4\n\n36.2\n\n65.8\n\n27.1 27.8\n\n14.8 24.3 27.8\n\n35.8 36.5\n\n21.4 32.9 37.0\n\n66.0 66.4\n\n47.0 61.5 66.3\n\n30.7\n\n40.0\n\n69.7\n\ntoken or the average embedding of all patch tokens as the extracted representation. With each patch as the query patch, we calculate the cosine similarity between its representation and all the other patches’ in the dataset and utilize the average precision (AP) as the retrieval metric. Finally, we average all the obtained AP scores (with all patches respectively taken as the query patch for retrieval) as the final retrieval score of the method.\n\nApart from the part retrieval experiments shown in Table 3, the visualized patch retrieval results in Figures 2 and 4 are obtained based on ImageNet (Deng et al., 2009) validation set. Concretely, from each pre-processed 224×224 validation image in ImageNet, we uniformly crop 49 patches sized 56×56 using a stride of 28. With all the cropped patches from the validation set, we select one patch as a query and find top 24 patches with the highest cosine similarity with it.\n\nPart classification. For linear probing, we learn a supervised linear classification layer on the extracted class token of the frozen encoders. While for attentive probing, following Chen et al. (2022a), a cross attention module and a batch normalization layer without affine transformation are additionally inserted between the encoder and the linear classifier. And a new learnable class token is taken as the query of the cross attention module, to replace the original class token extracted by the frozen encoder. We use SGD optimizer with a learning rate of 0.4 and 0.04 for linear probing and attentive probing, respectively. For both linear probing and attentive probing, the models are trained for 90 epochs. And the momentum of SGD is set to 0.9, the weight decay is set to 0, and the batch size is set to 1024.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nDeiT\n\nMoCo v3\n\nDINO\n\nMAE\n\nCAE\n\niBOT\n\nDeiT\n\nMoCo v3\n\nDINO\n\nMAE\n\nCAE\n\niBOT\n\nFigure 7: Patch retrieval comparisons of encoded representations on cropped patches from ImageNet.\n\nSegmentation. We use the same model structure that contains a parameter-fixed pretrained encoder (e.g., MAE and DeiT) and a simple learnable 1 × 1 convolutional layer for object-level and part-level segmentation tasks. Note that the learning rate (4e − 4), training iterations (160k), and batch size (16) among all the experiments maintain the same during training for fair comparisons. For ADE20K, the input size is set to 512 × 512 following previous works (Bao et al., 2021; He et al., 2021; Chen et al., 2022a; Zhou et al., 2021). For Pascal-Part, we adopt 480 × 480 as image input resolution following Contributors (2020). As for LIP, we use the same input size (320 × 320) proposed in LIP (Gong et al., 2017).\n\nA.5\n\nIMAGENET PATCH RETRIEVAL VISUALIZATION\n\nWe visualize more patch retrieval results of the encoded representations on the ImageNet validation set in Figures 7 and 8. It is observed that the retrieved patches of self-supervised methods are generally more about the semantics of the query part than that of DeiT. The results demonstrate that the encoded representations of DeiT focus more on object-level semantics, while the encoded representations of these self-supervised methods are more about part-level semantics. Among these methods, the retrieved patches of MAE have less semantic correlation but often share similar hues.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nDeiT\n\nMoCo v3\n\nDINO\n\nMAE\n\nCAE\n\niBOT\n\nDeiT\n\nMoCo v3\n\nDINO\n\nMAE\n\nCAE\n\niBOT\n\nDeiT\n\nMoCo v3\n\nDINO\n\nMAE\n\nCAE\n\niBOT\n\nDeiT\n\nMoCo v3\n\nDINO\n\nMAE\n\nCAE\n\niBOT\n\nFigure 8: Patch retrieval comparisons of encoded representations on cropped patches from ImageNet.\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper is a study on what is the different when learning with random crops and contrastive learning (CL) in a supervised learning (SL) or self-supervised learning (SSL) way, versus random patch masking via self-supervised learning (SSL) as in the Masked image modeling (MIM) variants (MAE/BeiT/CAE). The authors claim that the former is learning a \"part-to-whole\" task while the latter a \"part-to-part\" task; both can lead to strong part-aware representations, while it seems that SL excels more on object-level tasks, while SSL methods like CAE or MoCo-v3 are able to learn part-aware representations.\n\n# Strength And Weaknesses\n\n### Strengths\n\nS1: the paper is studying part-level recognition, a very interesging and important task\n\nS2: The paper features an extensive empirical study on many object-level recognition and part-level recognition tasks. \n\n### Weaknesses\n\nW1: It is unclear to me what the part-to-whole vs  part-to-part analysis offers, especially with respect to part-aware representations: Sections 3.1 and 3.2 seem to suggest the same thing: Both CL and MIM, \"[are] potentially capable of learning part-aware representations.\". Are there any insights or interesting experiments that are derived from that analysis and distinction? Cause what i see in Sec 4 is an empirical analysis on which losses can learn part-aware representation, that can stand without section 3, really.\n\nW2: It is unclear to me if the \"part-to-whole\" effect is only a function of the input (random crops) and the contrastive aspect of the loss; I think that what really matters is what is the contrastive loss appled on: If the loss is on aggregated features from the whole crop, I see this making sense. But what is a contrastive loss is applied at the token level?  How would for example DenseCL (Wang, Xinlong, et al. \"Dense contrastive learning for self-supervised visual pre-training.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.) fits in this framework? \n\nW3: The authors are focusing on random crops, which is only one of the augmentations used for CL - yet there are other augmentations at play. A more fair comparison would use an identical augmentation setup apart from either adding random crops or masking. Are all the other augmentations (color jittering, etc) shared across compared methods?\n\nW4: it is unclear to me, eg from the abstract what is the focus of the study, as I see two  axes that are convoluted: MIM vs CL and SL versus SSL methods. The abstract highlights the first axis, so does Sec 3, but then Figure 2 shows that both CAE (MIM) and MoCov3 (Contrastive) can give part-aware representations, while DeiT (SL) cannot. I think that leaving SL out of the study would make it clearer.\n\nW5: the insights from the empirical study are not analysed enough and can in places seems exargerated:, eg \"This implies that in general the self-supervised models are not strong at object-level understanding,\" this is a very generic statement to make from the results in Table 2. \n\nQ1: Are you retraining all the models under some otherwise identical setup, or using the publicly available models out of the box? Overall, three are many differences between the models compared beyond the loss (from batchsizes, to augmentations, to optimizers to other ViT-related tricks used in different papers). \n\nQ2: Although figure 1 and 3 is mostly figurative it seems to me that it exaggerates the effects of the random crops used in practice. Apart from methods that use local crops as in multi-crop, the scale parameter for the random crops is usually not as large as the one used in this figure. What is the crop scale used in all methods, ie what is the percentage of the max width/height that each readnom crop uses?\n\nQ3: Where are the patches seen in Fig 2 and 4 from? Imagenet?\n\nN1: I wonder how things would change for SL if a more recent variant of the DeiT series was used, e.g. Deit III from \nTouvron, Hugo, Matthieu Cord, and Hervé Jégou. \"Deit iii: Revenge of the vit.\" arXiv preprint arXiv:2204.07118 (2022).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written, but the usefulness oft the analysis of Section 3 is to me unclear. The experimental evaluation is exhaustive and in theory reproducible. There seems to be no technical novelty, the paper is a study on SSL/SL and MIM /CL, starting from pretrained models from other papers.\n\n# Summary Of The Review\n\nThis paper is at its core an empirical study on part-aware representations; the usefulness of the analysis in section 3 is unclear to me, looking forward for some clarifications from the authors on this. There is no technical novelty, while the insights from the study are also not strong, basically verifying the superiority of iBoT over other SSL methods tested on both object and part tasks. I am looking forward to the author's responses.\n\n--- Post rebuttal update:\n\nI want to thank the authors for constructive discussion. I think iff the authors make the edit they promise to the text  and clarify their contributions, I think I can raise my score to borderline accept. Although limited in novelty, the study in this paper can be valuable to the community, if the contributions are not over-claimed.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nSTOCHASTIC CONSTRAINED DRO WITH A COMPLEXITY INDEPENDENT OF SAMPLE SIZE\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDistributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback–Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an ε-stationary solution for non-convex losses and an optimal complexity for finding an ε-optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.\n\n1\n\nINTRODUCTION\n\nLarge-scale optimization of DRO has recently garnered increasing attention due to its promising performance on handling noisy labels, imbalanced data and adversarial data (Namkoong & Duchi, 2017; Zhu et al., 2019; Qi et al., 2020a; Chen & Paschalidis, 2018). Various primal-dual algorithms can be used for solving various DRO problems (Rafique et al., 2021; Nemirovski et al., 2009). However, primal-dual algorithms inevitably suffer from additional overhead for handling a n dimensionality dual variable, where n is the sample size. This is an undesirable feature for large-scale deep learning, where n could be in the order of millions or even billions. Hence, a recent trend is to design dual-free algorithms for solving various DRO problems (Qi et al., 2021; Jin et al., 2021; Levy et al., 2020).\n\nIn this paper, we provide efficient dual-free algorithms solving the following constrained DRO problem, which are still lacking in the literature,\n\nmin w∈W\n\nmax {p∈∆n:D(p,1/n)≤ρ}\n\ni=1\n\nn (cid:88)\n\npili(w) − λ0D(p, 1/n),\n\n(1)\n\nwhere w denotes the model parameter, W is closed convex set, ∆n = {p ∈ Rn : (cid:80)n i=1 pi = 1, pi ≥ 0} denotes a n-dimensional simplex, li(w) denotes a loss function on the i-th data, D(p, 1/n) = (cid:80)n i=1 pi log(pin) represents the Kullback–Leibler (KL) divergence measure between p and uniform probabilities 1/n ∈ Rn, and ρ is the constraint parameter, and λ0 > 0 is a small constant. A small KL regularization on p is added to ensure the objective in terms of w is smooth for deriving fast convergence.\n\nThere are several reasons for considering the above constrained DRO problem. First, existing dualfree algorithms are not satisfactory (Qi et al., 2021; Jin et al., 2021; Levy et al., 2020; Hu et al., 2021). They are either restricted to problems with no additional constraints on the dual variable p except for the simplex constraint (Qi et al., 2021; Jin et al., 2021), or restricted to convex analysis or have a requirement on the batch size that depends on accuracy level (Levy et al., 2020; Hu et al., 2021). Second, the Kullback–Leibler divergence measure is a more natural metric for measuring the distance between two distributions than other divergence measures, e.g., Euclidean distance. Third, compared with KL-regularized DRO problem without constraints, the above KL-constrained DRO formulation allows it to automatically decide a proper regularization effect that depends on the optimal solution by tuning the constraint upper bound ρ. The question to be addressed is the following:\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nCan we develop stochastic algorithms whose oracle complexity is optimal for both convex and non-convex losses, and its per-iteration complexity is independent of sample size n without imposing any requirements on the (large) batch size in the meantime?\n\nWe address the above question by (i) deriving an equivalent primal-only formulation that is of a compositional form; (ii) designing two algorithms for non-convex losses and extending them for convex losses; (iii) establishing an optimal complexity for both convex and non-convex losses. In particular, for a non-convex and smooth loss function li(w), we achieve an oracle complexity of (cid:101)O(1/ε3)1 for finding an ε-stationary solution; and for a convex and smooth loss function, we achieve an oracle complexity of O(1/ε2) for finding an ε-optimal solution. We would like to emphasize that these results are on par with the best complexities that can be achieved by primal-dual algorithms (Huang et al., 2020; Namkoong & Duchi, 2016). But our algorithms have a per-iteration complexity of O(d), which is independent of the sample size n. The convergence comparison of different methods for solving (1) is shown in Table 1. To achieve these results, we first convert the problem (1) into an equivalent problem:\n\nmin w∈W\n\nmin λ≥λ0\n\nF (w, λ) := λ log\n\n(cid:32)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19)(cid:33)\n\n+ (λ − λ0)ρ.\n\n(2)\n\nBy considering x = (w⊤, λ)⊤ ∈ Rd+1 as a single variable to be optimized, the objective function is ∈ R2 a compositional function of x in the form of f (g(x)), where g(x) =\n\n(cid:16) li(w)\n\n(cid:80)n\n\n(cid:17)(cid:105)\n\n(cid:104)\n\nλ, 1 n\n\ni=1 exp\n\nλ\n\nand f (g) = g1 log(g2) + g1ρ. However, there are several challenges to be addressed for achieving optimal complexities for both convex and non-convex loss functions li(w). First, the problem F (x) is non-smooth in terms of x given the domain constraint w ∈ W and λ ≥ λ0. Second, the outer function f (g)’s gradient is non-Lipschtiz continuous in terms of the second coordinate g2 if λ is unbounded, which is essential for all existing stochastic compositional optimization algorithms. Third, to the best of our knowledge, no optimal complexity in the order of O(1/ε2) has been achieved for a convex compositional function except for Zhang & Lan (2021), which assumes f is convex and component-wisely non-decreasing and hence is not applicable to (2). To address the first two challenges, we derive an upper bound for the optimal λ assuming that li(w) is bounded for w ∈ W, i.e., λ ∈ [λ0, ̃λ], which allows us to establish the smoothness condition of F (x) and f (g). Then we consider optimizing ̄F (x) = F (x) + δX (x), where δX (x) = 0 if x ∈ X = {x = (w⊤, λ)⊤ : w ∈ W, λ ∈ [λ0, ̃λ]}. By leveraging the smoothness conditions of F and f , we design stochastic algorithms by utilizing a recursive variance-reduction technique to compute a stochastic estimator of the gradient of F (x), which allows us to achieve a complexity of (cid:101)O(1/ε3) for finding a solution ̄x such that E[dist(0, ˆ∂ ̄F ( ̄x))] ≤ ε. To address the third challenge, we consider optimizing ̄Fμ(x) = ̄F (x) + μ∥x∥2/2 for a small μ. We prove that ̄Fμ(x) satisfies a Kurdyka-Łojasiewicz inequality, which allows us to boost the convergence of the aforementioned algorithm to enjoy an optimal complexity of O(1/ε2) for finding an ε-optimal solution to ̄F (x). Besides the optimal algorithms, we also present simpler algorithms with worse complexity, which are more practical for deep learning applications without requiring two backpropagations at two different points per iteration as in the optimal algorithms.\n\n2 RELATED WORK\n\nDRO springs from the robust optimization literature (Bertsimas et al., 2018; Ben-Tal et al., 2013) and has been extensively studied in machine learning and statistics (Namkoong & Duchi, 2017; Duchi et al., 2016; Staib & Jegelka, 2019; Deng et al., 2020; Qi et al., 2020b; Duchi & Namkoong, 2021), and operations research (Rahimian & Mehrotra, 2019; Delage & Ye, 2010). Depending on how to constrain or regularize the uncertain variables, there are constrained DRO formulations that specify a constraint set for the uncertain variables, and regularized DRO formulations that use a regularization term in the objective for regularizing the uncertain variables (Levy et al., 2020). Duchi et al. (2016) showed that minimizing constrained DRO with f -divergence including a χ2divergence constraint and a KL-divergence constraint, is equivalent to adding variance regularization for the Empirical Risk Minimization (ERM) objective, which is able to reduce the uncertainty and\n\n1 (cid:101)O omits a logarithmic dependence over ε.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Summary of algorithms solving KL-constrained DRO problem. Complexity represents the oracle complexity for achieving E[dist(0, ˆ∂ ̄F (x))] ≤ ε or other first-order stationarity convergence for the non-convex setting and E[F (x) − F (x∗)] ≤ ε for the convex setting. Per Iter Cost denotes the per-iteration computational complexity. The algorithm styles include primal-dual (PD), primal only (P), and compositional (COM). “-\" means not available in the original paper.\n\nSetting\n\nNon-convex\n\nConvex\n\nAlgorithms PG-SMD22 AccMDA Dual SGM SCDRO ASCDRO FastDRO3 SPD Dual SGM RSCDRO RASCDRO\n\nReference (Rafique et al., 2021) (Huang et al., 2020) (Levy et al., 2020)\n\nThis work\n\n(Levy et al., 2020) (Namkoong & Duchi, 2016) (Levy et al., 2020)\n\nThis work\n\nComplexity Batch Size\n\nO(1/ε4) O(1/ε3) -\nO(1/ε4) (cid:101)O(1/ε3) O(1/ε3) O(1/ε2) O(1/ε2) O(1/ε3) O(1/ε2)\n\nO(1) O(1) O(1) O(1) O(1) O(1/ε) O(1) O(1) O(1) O(1)\n\nPer Iter Cost O(n + d) O(n + d) O(d) O(d) O(d) O( d ε ) O(n + d) O(d) O(d) O(d)\n\nStyle PD PD P\nCOM COM P\nPD P\nCOM COM\n\nimprove the generalization performance of the model. Primal-Dual Algorithms. Many primal-dual algorithms designed for the min-max problems can be directly applied to optimize the constrained DRO problem. The algorithms proposed in (Nemirovski et al., 2009; Juditsky et al., 2011; Yan et al., 2019; Namkoong & Duchi, 2016; Yan et al., 2020; Song et al., 2021; Alacaoglu et al., 2022) are applicable to solving (1) when l is a convex function. Recently, Rafique et al. (2021) and Yan et al. (2020) proposed non-convex stochastic algorithms for solving non-convex strongly convex min-max problems, which are applicable to solving (1) when l is a weakly convex function or smooth. Many primal-dual stochastic algorithms have been proposed for solving non-convex strongly concave problems with a state of the art oracle complexity of O(1/ε3) for finding a stationary solution (Huang et al., 2020; Luo et al., 2020; Tran-Dinh et al., 2020). However, the primal-dual algorithms require maintaining and updating an O(n) dimensional vector for updating the dual variable. Constrained DRO. Recently, Levy et al. (2020) proposed sample independent algorithms based on gradient estimators for solving a group of DRO problems in the convex setting. To be more specific, they achieved a convergence rate of (cid:101)O(1/ε2) for the χ2-constrained/regularized and CVaRconstrained convex DRO problems and the batch size of logarithmically dependent on the inverse accuracy level O(log(1/ε)) with the help of multi-level Monte-Carlo (MLMC) gradient estimator. For the KL-constrained DRO objective and other more general setting, they achieve a convergence rate of O(1/ε3) under a Lipschitz continuity assumption on the inverse CDF of the loss function and a mini-batch gradient estimator with a batch size in the order O(1/ε) (please refer to Table 3 in Levy et al. (2020)). In addition, Levy et al. (2020) also proposed a simple stochastic gradient method for solving the dual expression of the DRO formulation, which is called Dual SGM. In terms of convergence, they only discussed the convergence guarantee for the χ2-regularized and CVaR penalized convex DRO problems (cf. Claim 3 in their paper). However, there is still gap for proving the convergence rate of Dual SGM for non-convex KL-constrained DRO problems due to similar challenges mentioned in the previous section, in particular establishing the smoothness condition in terms of the primal variable and the Lagrangian multipliers (denoted as x, ν, η respectively in their paper). This paper makes unique contributions for addressing these challenges by (i) removing η in Dual SGM and deriving the box constraint for our Lagrangian multiplier λ for proving the smoothness condition; (ii) establishing an optimal complexity in the order of O(1/ε3) in the presence of non-smooth box constraints, which, to the best of our knowledge, is the first time for solving a non-convex constrained compositional optimization problem. Regularized DRO. DRO with KL divergence regularization objective has shown superior performance for addressing data imbalanced problems (Qi et al., 2021; 2020a; Li et al., 2020; 2021). Jin et al. (2021) proposed a mini-batch normalized gradient descent with momentum that can find a first-order ε stationary point with an oracle complexity of O(1/ε4) for KL-regularized DRO and χ2 regularized DRO with a non-convex loss. They solve the challenge that the loss function could be unbounded. Qi et al. (2021) proposed online stochastic compositional algorithms to solve KLregularized DRO. They leveraged a recursive variance reduction technique (STORM (Cutkosky & Orabona, 2019)) to compute a gradient estimator for the model parameter w only. They derived a complexity of (cid:101)O(1/ε3) for a general non-convex problem and improved it to O(1/(με)) for a\n\n2PG-SMD2 refers to PG-SMD algorithm under Assumption D2 in Rafique et al. (2021). 3FastDRO is name of the GitHub repository of Levy et al. (2020), and we use the name “FastDRO” to refer\n\nto the algorithm based on mini-batch gradient estimator in Levy et al. (2020).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nproblem that satisfies an μ-PL condition. Qi et al. (2020a) reports a worse complexity for a simpler algorithm for solving KL-regularized DRO. Li et al. (2020; 2021) studied the effectiveness of KL regularized objective on different applications, such as enforcing fairness between subgroups, and handling the class imbalance.\n\nMore related works are included in the appendix due to limit of space, which will not affect the discussion of results in this paper.\n\n3 PRELIMINARIES\n\nIn this section, we introduce notations, definitions and assumptions. We show that (1) is equivalent to (2) in Section G in Appendix.\n\nλ ) and g(x) = Ei∼D[exp( li(w)\n\nNotations: Let ∥ · ∥ denotes the Euclidean norm of a vector or the spectral norm of a matrix. And x = (w⊤, λ)⊤ ∈ Rd+1, gi(x) = exp( li(w) λ )] where D denotes the training set and i denotes the index of the sample randomly generated from D. Let fλ(·) = λ log(·) + λρ, and ∇fλ(g) = λ g denotes the gradient of f in terms of g. Let ΠX (·) denote an Euclidean projection onto the domain X . Let [T ] = {1, . . . , T } and τ ∼ [T ] denotes a random selected index. We make the following standard assumptions regarding to the problem (2). Assumption 1. There exists R, G, C, and L such that\n\n(a) The domain of model parameter W is bounded by R, i.e., for all w ∈ W, we have ∥w∥ ≤ R.\n\n(b) li(w) is G-Lipschitz continuous function and bounded by C, i.e., ∥∂li(w)∥ ≤ G and\n\n|li(w)| ≤ C for all w ∈ W and i ∼ D.\n\n(c) li(w) is L-smooth, i.e., ∥∇li(w1) − ∇li(w2)∥ ≤ L∥w1 − w2∥, ∀w1, w2 ∈ W, i ∼ D. (d) There exists a positive constant ∆ < ∞ and an initial solution (w1, λ1) such that\n\nF (w1, λ1) − min w∈W\n\nmin λ≥λ0\n\nF (w, λ) ≤ ∆.\n\nAssumption 2. Let σg, σ∇g be positive constants and σ2 = max{σg, σ∇g}. For i ∼ D, assume that E[∥gi(x) − g(x)∥2] ≤ σ2\n\ng, E[∥∇gi(x) − ∇g(x)∥2] ≤ σ2\n\n.\n\n∇g\n\nRemark: Assumption 1 (a), i.e., the boundness condition of W is also assumed in Levy et al. (2020), which is mainly used for convex analysis. Assumption 1(b), (c), i.e., the Lipstchiz continuity and smoothness of loss function, and the variance bounds for gi and its gradient in Assumption 2 can be derived from Assumption 1 (b), such that E[∥gi(x) − g(x)∥2] ≤ E[∥gi(x)∥2] ≤ exp( 2C ), and E[∥∇gi(x) − ∇g(x)∥2] ≤ E[∥∇gi(x)∥2] ≤ exp( 2C\n\n)(G2 + C2\n\n)4\n\nλ0\n\nλ0\n\nλ0\n\nHowever, F (w, λ) is not necessarily smooth in terms of x = (w⊤, λ)⊤ if λ is unbounded. To address this concern, we prove that optimal λ is indeed bounded. Lemma 1. The optimal solution of the dual variable λ∗ to the problem (2) is upper bounded by ̃λ = λ0 + C/ρ, where C is the upper bound of the loss function and ρ is the constraint parameter. Thus, we could constrain the domain of λ in the DRO formulation (2) with the upper bound ̃λ , and obtain the following equivalent formulation:\n\nmin w∈W\n\nmin λ0≤λ≤ ̃λ\n\nλ log\n\n(cid:32)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19)(cid:33)\n\n+ λρ.\n\n(3)\n\nThe upper bound ̃λ guarantees the smoothness of F (w, λ) and the smoothness of fλ(·), which are critical for the proposed algorithms to enjoy fast convergence rates. Lemma 2. F (w, λ) is LF -smooth for any w ∈ W and λ ∈ [λ0, ̃λ], where LF = ̃λL2 g + 2Lg + ̃λL∇g + 1 + ̃λ. Lg and L∇g are constants independent of sample size n and explicitly derived in Lemma 7 . Below, we let X = {x|w ∈ W, λ0 ≤ λ ≤ ̃λ}, δX (x) = 0 if x ∈ X , and δX (x) = ∞ if x /∈ X . The problem (3) is equivalent to :\n\n(4)\n\nmin x∈Rd+1\n\n ̄F (x) := F (x) + δX (x),\n\nSince ̄F is non-smooth, we define the regular subgradient as follows.\n\n4We would like to point out that the variance bound and the smoothness constant LF are exponentially dependent on the problem parameters, so are these constants in some other stochastic methods solving constrained DRO, like Dual SGM in Levy et al. (2020).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 1 (Regular Subgradient). Consider a function Φ : Rn → R and Φ( ̄x) is finite at a point ̄x. For a vector v ∈ Rn, v is a regular subgradient of Φ at ̄x, written v ∈ ˆ∂Φ( ̄x), if\n\nlim inf x→ ̄x\n\nΦ(x) − Φ( ̄x) − v⊤(x − ̄x) ∥x − ̄x∥ Since F (x) is differentiable, we use ˆ∂ ̄F (x) = ∇F (x) + ˆ∂δX (x) (see Exercise 8.8 in Rockafellar & Wets (1998)) in the analysis. Recall the definition of subgradient of a convex function ̄F which is denoted by ∂ ̄F . When ̄F (x) is convex, we have ˆ∂ ̄F (x) = ∂ ̄F (x) (see Proposition 8.2 in Rockafellar & Wets (1998)). The dist(0, ˆ∂ ̄F (x)) measures the distance between the origin and the regular subgradient set of ̄F at x. The oracle complexity is defined below:\n\n≥ 0.\n\nDefinition 2 (Oracle Complexity). Let ε > 0 be a small constant, the oracle complexity is defined as the number of processing samples z in order to achieve E[dist(0, ˆ∂ ̄F (x))] ≤ ε for a non-convex loss function or E[F (x) − F (x∗)] ≤ ε for a convex loss function.\n\n4 STOCHASTIC CONSTRAINED DRO WITH NON-CONVEX LOSSES\n\nIn this section, we present two stochastic algorithms for solving (4). The first algorithm is simpler yet practical for deep learning applications. The second algorithm is an accelerated one with a better complexity, which is more complex than the first algorithm.\n\nAlgorithm 1 SCDRO(x1, v1, u1, s1, η1, T1)\n\nAlgorithm 2 ASCDRO(x1, v1, u1, s1, η1, T1)\n\n1: Input: w1 ∈ W, λ1 ≥ λ0, x1 = (w⊤ 1 , λ1)⊤ 2: Initialization: Draw a sample ξ1 ∼ D, and\n\n1: Input: w1 ∈ W, λ1 ≥ λ0, x1 = (w⊤ 1 , λ1)⊤ 2: Initialization: Draw a sample ξ1 ∼ D, and\n\ncalculate s1 = exp(li(w1)/λ1),\n\nv1 = ∇fλ1(s1)∂wgi(x1)) ∈ Rd u1 = ∇fλ1 (s1)∂λgi(x1) + log(s1) + ρ ∈ R\n\ncalculate s1 = exp(li(w1)/λ1), v1 = ∂wgi(x1) ∈ Rd u1 = ∂λgi(x1) ∈ R\n\nUpdate xt+1 = ΠX (xt − ηzt) Draw a sample ξi ∼ D Let st+1 = (1 − β)st + βgi(xt+1) Update vt+1, ut+1 according to (6)\n\n3: for t = 1, · · · , T do 4: 5: 6: 7: 8: end for 9: return: (xτ , vτ , uτ , sτ ), where τ ∼ [T ]\n\n3: for t = 1, · · · , T do 4:\n\nUpdate xt+1 = ΠX (xt − ηzt), where zt is given in (8) Draw a sample ξi ∼ D Update st+1, vt+1, ut+1 according to (7)\n\n5: 6: 7: end for 8: return: (xτ , vτ , uτ , sτ ), where τ ∼ [T ]\n\n4.1 BASIC ALGORITHM: SCDRO A major concern of the algorithm design is to compute a stochastic gradient estimator of the gradient of F (x). At iteration t, the gradient of F (xt) is given by\n\n∂wF (xt) = ∇fλt(g(xt))∇wg(xt) ∂λF (xt) = ∇fλt(g(xt))∇λg(xt) + log(g(xt)) + ρ. Both ∇λg(xt) and ∇wg(xt) can be estimated by unbiased estimator denoted by ∇gi(xt). The concern lies at how to estimate g(xt) inside ∇fλt(·). The first algorithm SCDRO is applying existing techniques for two-level compositional function. In particular, we estimate g(xt) by a sequence of st, which is updated by moving average st = (1 − β)st−1 + βgi(xt). Then we substitute g(xt) in ∂wF (xt) and ∂λF (xt) with st, and invoke the following moving average to obtain the gradient estimators in terms of wt and λt, respectively,\n\n(5)\n\nvt = (1 − β)vt−1 + β∇fλt(st)∇wgi(xt) ut = (1 − β)ut−1 + β(∇fλt(st)∇λgi(xt) + log(st) + ρ).\n\n(6)\n\nFinally we complete the update step of xt by xt+1 = ΠX (xt − ηzt), where zt = (v⊤\n\nt , ut)⊤.\n\nWe would like to point out the moving average estimator for tracking the inner function g(w) is widely used for solving compositional optimization problems (Wang et al., 2017; Qi et al., 2021; Zhang & Xiao, 2019; Zhou et al., 2019). Using the moving average for computing a stochastic gradient estimator of a compositional function was first used in the NASA method proposed in Ghadimi et al. (2020). The proposed method SCDRO is presented in Algorithm 1. It is similar to NASA but with\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nT\n\n20L2 F\n\na simpler design on the update of xt+1. We directly use projection after an SGD-tyle update. In contrast, NASA uses two steps to update xt+1. As a consequence, NASA has two parameters for updating xt+1 while SCDRO only has one parameter η for updating xt+1. It is this simple change that allows us to extend SCDRO for convex problems in the next section. Below, we present the convergence rate of our basic algorithm SCDRO for a non-convex loss function. , η = β Theorem 1. Suppose the Assumption 1 and 2 hold, and set β = 1√ Algorithm 1 T iterations, we have E[dist(0, ˆ∂ ̄F (xτ ))2] ≤ (624σ2 + 280∆) L2 Remark: Theorem 1 shows that SCDRO achieves a complexity of O(1/ε4) for finding an ε-stationary point, i.e., E[dist(0, ˆ∂ ̄F (xR))] ≤ ε for a non-convex loss function. Note that NASA (Ghadimi et al., 2020) enjoys the same oracle complexity but for a different convergence measure, i.e., E[∥y(x, z) − x∥2 + ∥z − ∇F (x)∥2] ≤ ε for a returned primal-dual pair (x, z), where y(x, z) = (cid:81) X [x − z]. We can see that our convergence measure is more intuitive. In addition, we are able to leverage our convergence measure to establish the convergence for convex functions by using Kurdyka-Łojasiewicz (KL) inequality and the restarting trick as shown in next section. In contrast, such convergence for NASA is missing in their paper. Compared with stochastic primal-dual methods (Rafique et al., 2021; Yan et al., 2020) for the min-max formulation (1), their algorithms are double looped and have the same oracle complexity for a different convergence measure, i.e., E[dist(0, ˆ∂ ̄F (x∗))2] ≤ γ2∥x − x∗∥2] ≤ ε for some returned solution x, where x∗ is a reference point that is not computable. Our convergence measure is stronger as we directly measure E[dist(0, ˆ∂ ̄F (xτ ))2] on a returned solution xτ . This is due to that we leverage the smoothness of F (·).\n\n. Then after running + 20L2\n\nF√ T\n\nF ∆\n\nT\n\n.\n\n4.2 ACCELERATED ALGORITHM: ASCDRO Our second algorithm presented in Algorithm 2 is inspired by Qi et al. (2021) for solving the KLregularized DRO by leveraging a recursive variance reduced technique (i.e., STORM) to estimate g(wt) and ∇g(wt) for computing ∂wF (xt) and ∂λF (xt) in (5). In particular, we use vt for tracking ∇wg(xt), use ut for tracking ∇λg(xt), and use st for tracking g(xt), which are updated by:\n\nvt = ∇wgi(xt) + (1 − β)(vt−1 − ∇wgi(xt−1)) ut = ∇λgi(xt) + (1 − β)(ut−1 − ∇λgi(xt−1)) st = gi(xt) + (1 − β)(st−1 − gi(xt−1)).\n\n(7)\n\nA similar update to st has been used in Chen et al. (2021) for tracking the inner function values for two-level compositional optimization. However, they do not use similar updates for tracking the gradients as vt, ut. Hence, their algorithm has a worse complexity.\n\nThen we invoke these estimators into ∂wF (xt) and ∂λF (xt) to obtain the gradient estimator\n\nzt = (∇fλt(st)v⊤\n\nt , ∇fλt(st)ut + log(st) + ρ)⊤.\n\n(8)\n\nLF\n\n14LF k3 + 130L4\n\nBelow, we show ASCDRO can achieve a better convergence rate in the non-convex loss function. Theorem 2. Under Assumption 1 and 2, for any α > 1, let k = ασ2/3 and c = σ2\n\n, w = max(2σ2, (16L2 k\n(w+tσ2)1/3\n\nF . Then after running Algorithm 2 for T iterations with ηt = (cid:17)\n\nt , we have E[dist(0, ˆ∂ ̄F (xτ ))2] ≤ O\n\nand βt = cη2 Remark: Theorem 2 implies that with a polynomial decreasing step size, ASCDRO is able to find an ε-stationary solution such that E[dist(0, ˆ∂ ̄F (xR))] ≤ ε with a near-optimal complexity (cid:101)O(1/ε3). Note that the complexity (cid:101)O(1/ε3) is optimal up to a logarithmic factor for solving non-convex smooth optimization problems (Arjevani et al., 2019). State-of-the-art primal-dual methods with variance-reduction for min-max problems (Huang et al., 2020) have the same complexity but for a different convergence measure, i.e, E[ 1\n\nX [x − γ∇F (x)]∥] ≤ ε for a returned solution x.\n\n(cid:16) log T T 2/3\n\nγ ∥x − (cid:81)\n\nF k)3)\n\n.\n\n5 STOCHASTIC ALGORITHMS FOR CONVEX PROBLEMS\n\nIn this section, we presented restarted algorithms for solving (3) with a convex loss function li(w). The key is to restart SCDRO and ASCDRO by using a stagewise step size scheme. We define a new objective Fμ(x) = F (x) + μ∥x∥2/2 and correspondingly ̄Fμ(x) = Fμ(x) + δX (x), where μ is a constant to be determined later. With this new objective, we have the following lemma. Lemma 3. Suppose that li(w) is convex for all i, then for all x ∈ X , ̄Fμ(x) satisfies the following Kurdyka-Łojasiewicz (KL) inequality dist(0, ∂ ̄Fμ(x))2 ≥ 2μ( ̄Fμ(x) − inf\n\n ̄Fμ(x)).\n\nx∈X\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 3 RSCDRO or RASCDRO\n\n1 , λ1)⊤\n\n1: Input: w1 ∈ W, λ1 ∈ R+, x1 = (w⊤ 2: Initialization: The same as in SCDRO or ASCDRO 3: Let Λk = (xk, vk, uk, sk) 4: for k = 1, · · · , K do 5: 6: 7: end for 8: return: xK\n\nΛk+1 = SCDRO(Λk, ηk, Tk) or Λk+1 = ASCDRO(Λk, ηk, Tk) Change ηk, Tk according to Lemma 4 or Lemma 5\n\nLemma 3 allows us to obtain the convergence guarantee for convex losses. The idea of the restarted algorithm is to apply SCDRO and ASCDRO to the new objective ̄Fμ(x) by adding μxt to (∇fλt(st)∇wgi(xt)⊤, ∇fλt(st)∇λgi(xt) + log(st) + ρ)⊤ in Eq. (6) of Algorithm 1 and substituting t , ∇fλt(st)ut + log(st) + ρ)⊤ + μxt, and restarting zt in (8) of Algorithm 2 by zt = (∇fλt(st)v⊤ SCDRO or ASCDRO with a stagewise step size to enjoy the benefit of KL inequality of ̄Fμ(x). It is notable that a stagewise step size is widely and commonly used in practice. The multi-stage restarted version of SCDRO and ASCDRO are shown Algorithm 3, to which we refer as restarted-SCDRO (RSCDRO) and restarted-ASCDRO (RASCDRO).\n\n5.1 RESTARTED SCDRO FOR CONVEX PROBLEMS In this subsection, we present the convergence rate of RSCDRO for convex losses. We first present a lemma that states Fμ(xk) is stagewisely decreasing. Lemma 4. Suppose Assumptions 1 and 2 hold, li(w) is convex for all i, and Fμ(x1) − inf x∈X Fμ(x) ≤ ∆μ < ∞. Let ε1 = ∆μ, εk = εk−1/2, βk = min{ μεk c }, ηk = F . Run RSCDRO, min{ then we have E[Fμ(xk) − inf\n\nμ2εk Fμ(x)] ≤ εk for each stage k.\n\n} and Tk = max{ 384cL2\n\n}, where c = 384L2\n\n, 384cL2\n\n1 12cL2 F\n\ncσ2 , 1\n\nμεk 12cL2\n\nF σ2 ,\n\nF σ2\n\nμ\n\nF\n\nx∈X\n\nThe above lemma implies that the objective gap E[Fμ(xk) − inf x∈X Fμ(x)] is decreased by a factor of 2 after each stage. Based on the above lemma, RSCDRO has the following convergence rate\n\nTheorem 3. Under the same assumptions and parameter settings as Lemma 4, after K = O(log2(ε1/ε)) stages, the output of RSCDRO satisfies E[Fμ(xK) − inf x∈X Fμ(x)] ≤ ε, and the oracle complexity is O(1/μ2ε).\n\nThe following corollary follows from the above theorem (please see Appendix F.5 for proof). Corollary 1. Let μ = ε/(2(R2 + ̃λ2)). Then under the same assumptions and parameter settings as Lemma 4, after K = O(log2(ε1/ε)) stages, the output of RSCDRO satisfies E[F (xK) − inf x∈X F (x)] ≤ ε and the oracle complexity is O(1/ε3).\n\n5.2 RESTARTED ASCDRO FOR CONVEX PROBLEMS In this subsection, we establish a better convergence rate of RASCDRO for convex losses.\n\nLemma 5. Suppose Assumptions 1 and 2 hold, li(w) is convex for all i, and Fμ(x1) − inf x∈X Fμ(x) ≤ ∆μ < ∞. Let ε1 = ∆μ, εk = εk−1/2, βk = min{ μεk cσ2 , 1 c }, ηk = } and Tk = max{ 192cLF σ }, where c = 768L2 F . Run RASCmin{ εk DRO, then we have E[Fμ(xk) − inf x∈X Fμ(x)] ≤ εk for each stage k.\n\n24cLF σ2 ,\n\n, 192cL2\n\n, 192cL2\n\n1 24cL2 F\n\nμ3/2√\n\nF σ2\n\nμεk\n\nμεk\n\n√\n\nμ\n\nF\n\nThe above lemma implies that the objective gap E[Fμ(xk) − inf x∈X Fμ(x)] is decreased by a factor of 2 after each stage. Hence we have the following convergence rate for the RASCDRO.\n\nTheorem 4. Under the same assumptions and parameter settings as Lemma 5, after K = O(log2(ε1/ε)) stages, the output of RASCDRO satisfies E[Fμ(xK) − inf x∈X Fμ(x)] ≤ ε, and the oracle complexity is O (cid:0)max (cid:0)1/με, 1/μ3/2√\n\nε(cid:1)(cid:1).\n\nBy the same method of derivation of Corollary 1, the following corollary of Theorem 4 holds. Corollary 2. Let μ = ε/(2(R2 + ̃λ2)). Then under the same assumptions and parameter settings as Lemma 5, after K = O(log2(ε1/ε)) stages, the output of RASCDRO satisfies E[F (xK) − inf x∈X F (x)] ≤ ε and the oracle complexity is O(1/ε2).\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Training accuracy (%) vs # of processed training samples for the convex setting. ρ is fixed to 0.5 on CIFAR10-ST and CIFAR100-ST, and 0.1 on ImageNet-LT and iNaturalist2018. The results are averaged over 5 independent runs.\n\nFigure 2: Training accuracy vs # of processed training samples for the non-convex setting. ρ is fixed to 0.5 on all datasets. The results are averaged over 5 independent runs.\n\nRemark: Corollary 2 shows that RASCDRO achieves the claimed oracle complexity O(1/ε2) for finding an ε-optimal solution, which is optimal for solving convex smooth optimization problems (Nemirovsky & Yudin, 1983). Finally, we note that a similar complexity was established in (Zhang & Lan, 2021) for constrained convex compositional optimization problems. However, their analysis requires each level function to be convex, which does not apply to our case as the outer function fλ(·) is non-convex.\n\n6 EXPERIMENTS\n\nIn this section, we verify the effectiveness of the proposed algorithms in solving imbalanced classification problems. We show that the proposed methods outperform baselines under both the convex and non-convex settings in terms of convergence speed, and generalization performance. In addition, we study the influence of ρ to the robustness of different optimization methods in supplement. Baselines. For the comparison of convergence speed, we compare with different algorithms for optimizing the same objective (1), including, stochastic primal-dual algorithms, namely PGSMD2 (Rafique et al., 2021) for a non-convex loss, and SPD (Namkoong & Duchi, 2016) for a convex loss, Dual SGM (Levy et al., 2020) and mini-batch based SGD named FastDRO (Levy et al., 2020) for both convex and non-convex losses . For the comparison of generalization performance, we compare with different methods for optimizing different objectives, including the traditional ERM with CE loss by SGD with momentum (SGDM), KL-regularized DRO solved by RECOVER (Qi et al., 2021), and CVaR-constrained, χ2-regularized/-constrained DRO optimized by FastDRO. Datasets. We conduct experiments on four imbalanced datasets, namely CIFAR10-ST, CIFAR100-ST (Qi et al., 2020b), ImageNet-LT (Liu et al., 2019), and iNaturalist2018 (iNaturalist 2018 competition dataset). The original CIFAR10, CIFAR100 are balanced data, where CIFAR10 (resp. CIFAR100) has 10 (resp. 100) classes and each class has 5K (resp. 500) training images. For constructing CIFAR10-ST and CIFAR100-ST, we artificially construct imbalanced training data, where we only keep the last 100 images of each class for the first half classes, and keep other classes and the test data unchanged. ImageNet-LT is a long-tailed subset of the original ImageNet-2012 by sampling a subset following the Pareto distribution with the power value 6. It has 115.8K images from 1000 categories, which include 4980 for head class and 5 images for tail class. iNaturalist 2018 is a real-world dataset whose class-frequency follows a heavy-tail distribution. It contains 437K images from 8142 classes. Models. For a non-convex setting (deep model), we learn ResNet20 for CIFAR10-ST, CIFAR100-ST, and ResNet50 for ImageNet-LT and iNaturalist2018, respectively. On CIFAR10-ST, CIFAR100-ST, we optimize the network from scratch by different algorithms. For the large-scale ImageNet-LT and iNaturalist2018 datasets, we optimize the last block of the feature layers and the classifier weight with other layers frozen of a pretrained ResNet50 model. This is a common training strategy in the literature (Kang et al., 2019; Qi et al., 2020a). For a convex setting (linear model), we freeze the feature layers of the pretrained models, and only fine-tune the last classifier weight. The pretrained models for ImageNet-LT, CIFAR10-ST, CIFAR100-ST are trained from scratch by optimizing the standard cross-entropy (CE) loss using SGD with momentum 0.9 for 90 epochs. The pretrained ResNet50 model for iNaturalist2018 is from the released model by Kang et al. (2019).\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nParameters and Settings. For all experiments, the batch size is 128 for CIFAR10-ST and CIFAR100ST, and 512 for ImageNet-LT and iNaturalist2018. The loss function is the CE loss. The λ0 is set to 1e-3. The (primal) learning rates for all methods are tuned in {0.01, 0.05, 0.1, 0.5, 1}. The learning rate for updating the dual variable in PG_SMD2 and SPD is tuned in {1e-5, 5e-5, 1e-4, 5e-4)}. The momentum parameter β in our proposed algorithms and RECOVER are tuned {0.1 : 0.1 : 0.9}. For RECOVER, the hyper-parameter λ is tuned in {1, 50, 100}. The constrained parameter ρ is tuned in {0.1, 0.5, 1} for the comparison of generalization performance unless specified otherwise. The initial λ and Larange multiplier in Dual SGM are both tuned in {0.1, 1, 10}. Convergence comparison between different baselines. In the convex setting, we compare RSCDRO and RASCDRO with SPD, FastDRO and Dual SGM baselines. We report the training accuracy and testing accuracy in terms of the number (#) of processing samples. We denote 1 pass of training data by 1 epoch. We run a total of 3 epochs for CIFAR10-ST and CIFAR100-ST and decay the learning rate by a factor of 10 at the end of 2nd epoch. Similarly, we run 60 epochs and decay the learning rate at the 30th epochs for the ImageNet-LT, and run 30 epochs and decay the learning rate at the 20th epoch for iNaturalist2018. In the nonconvex setting, we compare SCDRO with two baselines, PG-SMD2 and FastDRO. We run 120 epochs for CIFAR10-ST and CIFAR100-ST, and decay the learning rate by a factor of 10 at the 90th epoch. And we run 30 epochs for ImageNet-LT and iNaturalist2018, and decay the learning rate at the 20th epoch. Results. We first report the results for convex setting in Figures 1 and 3. It is obvious to see that RSCDRO and RASCDRO are consistently better than baselines on CIFAR10-ST, CIFAR100-ST, and ImageNet-LT. PD-SMD2 and Dual SGM have comparable results with our proposed algorithms on the iNaturalist2018 in terms of training accuracy, but is worse in terms of testing accuracy. FastDRO has the worst performance on all the datasets. RSCDRO and RASCDRO achieve comparable results on all datasets, however, the stochastic estimator in RASCDRO requires two gradient computations per iteration, which incurs more computational cost than RSCDRO. Hence, in the non-convex setting, we focus on SCDRO. Figure 2 and 4 report the results for non-convex setting. We can see that SCDRO achieves the best performance on all the datasets. The margin increases on the large scale ImageNet-LT and iNaturalist2018 datasets. For the three baselines, Dual SGM has better testing performance than FastDRO and PD-SGM2 on CIFAR10-ST and CIFAR100-ST. On the large scale data ImageNet-LT and iNaturalist2018, however, Dual SGM has the worst performance in terms of the testing accuracy. Furthermore, SCDRO is more stable than FastDRO and Dual SGM in different settings as the training of Dual SGM and FastDRO is comparable to SCDRO in convex settings and much worse than SCDRO in non-convex settings. Comparison with ERM and KL-regularized DRO. Next, we compare our method for solving KL-constrained DRO (KL-CDRO) with 1) ERM+SGDM, and KL-regularized DRO (KL-RDRO) optimized by RECOVER in the non-convex setting 2) CVaR-constrained DRO, χ2-regularized DRO χ2-constrained DRO optimized by FastDRO in the convex setting. We conduct the experiments on the large-scale ImageNet-LT and iNaturalist2018 datasets. The results shown in Table 2 and 3 vividly demonstrate that our method for constrained DRO outperforms the ERM-based method and other popular f -divergence constrained/regularized DRO in different settings.\n\nTable 2: Testing Accuracy in Convex Setting\n\nTable 3: Testing Accuracy in Non-Convex Setting\n\nImageNet-LT\n\niNaturalist2018\n\nImageNet-LT iNaturalist2018\n\nKL-Constraint + SCDRO\n\n24.08 (± 0.01)\n\n55.63 (± 0.03)\n\nKL-Constraint + SCDRO\n\nCVaR-Constraint + FastDRO 17.23 (± 0.03)\n\n54.52 (± 0.11)\n\nERM+SGDM\n\nχ2-Regularization + FastDRO 23.98 (± 0.01)\n\n55.03 (± 0.03)\n\nKL-Regularization + RECOVER\n\n43.74\n\n43.36\n\n42.68\n\n65.59\n\n64.42\n\n64.57\n\nχ2-Constraint + FastDRO\n\n23.61 (± 0.01)\n\n53.71 (± 0.05)\n\n7 CONCLUSIONS\n\nIn this paper, we proposed dual-free stochastic algorithms for solving KL-constrained distributionally robust optimization problems for both convex and non-convex losses. The proposed algorithms have nearly optimal complexity in both settings. Empirical studies vividly demonstrate the effectiveness of the proposed algorithm for solving non-convex and convex constrained DRO problems.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAhmet Alacaoglu, Volkan Cevher, and Stephen J Wright. On the complexity of a practical primal-dual\n\ncoordinate method. arXiv preprint arXiv:2201.07684, 2022.\n\nYossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.\n\nAharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341–357, 2013.\n\nDimitris Bertsimas, Vishal Gupta, and Nathan Kallus. Data-driven robust optimization. Mathematical\n\nProgramming, 167(2):235–292, 2018.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge\n\nuniversity press, 2004.\n\nRuidi Chen and Ioannis C Paschalidis. A robust learning approach for regression models based on\n\ndistributionally robust optimization. Journal of Machine Learning Research, 19(13), 2018.\n\nTianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. IEEE Transactions on Signal Processing, 69:4937–4948, 2021. doi: 10.1109/tsp.2021.3092377. URL https://doi.org/10.1109%2Ftsp.2021. 3092377.\n\nAshok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.\n\nAdvances in Neural Information Processing Systems, 32:15236–15245, 2019.\n\nErick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with\n\napplication to data-driven problems. Operations research, 58(3):595–612, 2010.\n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Distributionally robust federated\n\naveraging. Advances in Neural Information Processing Systems, 33, 2020.\n\nDarinka Dentcheva, Spiridon Penev, and Andrzej Ruszczynski. Statistical estimation of composite risk functionals and risk optimization problems. Annals of the Institute of Statistical Mathematics, 69(4):737–760, 2017. URL https://EconPapers.repec.org/RePEc:spr:aistmt: v:69:y:2017:i:4:d:10.1007_s10463-016-0559-8.\n\nC. John Duchi, W. Peter Glynn, and Hongseok Namkoong. Statistics of robust optimization: A\n\ngeneralized empirical likelihood approach. Mathematics of Operations Research, 2016.\n\nJohn C Duchi and Hongseok Namkoong. Learning models with uniform performance via distribu-\n\ntionally robust optimization. The Annals of Statistics, 49(3):1378–1406, 2021.\n\nSaeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang. A single timescale stochastic approximation method for nested stochastic optimization. SIAM Journal on Optimization, 30(1):960–979, 2020.\n\nYifan Hu, Xin Chen, and Niao He. On the bias-variance-cost tradeoff of stochastic optimization. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 22119–22131. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ b986700c627db479a4d9460b75de7222-Paper.pdf.\n\nFeihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Accelerated zeroth-order momentum\n\nmethods from mini to minimax optimization. arXiv e-prints, pp. arXiv–2008, 2020.\n\niNaturalist 2018 competition dataset. iNaturalist 2018 competition dataset. https://github.\n\ncom/visipedia/inat_comp/tree/master/2018, 2018.\n\nJikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang. Non-convex distributionally robust optimization: Non-asymptotic analysis. Advances in Neural Information Processing Systems, 34, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAnatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with\n\nstochastic mirror-prox algorithm. Stochastic Systems, 1(1):17–58, 2011.\n\nBingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217, 2019.\n\nDaniel Levy, Yair Carmon, John C Duchi, and Aaron Sidford. Large-scale methods for distributionally\n\nrobust optimization. Advances in Neural Information Processing Systems, 33, 2020.\n\nTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. Tilted empirical risk minimization. In\n\nInternational Conference on Learning Representations, 2020.\n\nTian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith. On tilted losses in machine learning:\n\nTheory and applications. arXiv preprint arXiv:2109.06141, 2021.\n\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Largescale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2537–2546, 2019.\n\nLuo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems. Advances in Neural Information Processing Systems, 33, 2020.\n\nHongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust\n\noptimization with f-divergences. In NIPS, volume 29, pp. 2208–2216, 2016.\n\nHongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In\n\nAdvances in neural information processing systems, pp. 2971–2980, 2017.\n\nAngelia Nedi ́c and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of\n\noptimization theory and applications, 142(1):205–228, 2009.\n\nArkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on optimization, 19(4):1574– 1609, 2009.\n\nA. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. A Wiley-Interscience publication. Wiley, 1983. ISBN 9780471103455. URL https://books. google.com/books?id=6ULvAAAAMAAJ.\n\nQi Qi, Yi Xu, Rong Jin, Wotao Yin, and Tianbao Yang. Attentional biased stochastic gradient for\n\nimbalanced classification. arXiv preprint arXiv:2012.06951, 2020a.\n\nQi Qi, Yan Yan, Zixuan Wu, Xiaoyu Wang, and Tianbao Yang. A simple and effective framework for pairwise deep metric learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVII 16, pp. 375–391. Springer, 2020b.\n\nQi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. An online method for a class of distributionally robust optimization with non-convex objectives. Advances in Neural Information Processing Systems, 34, 2021.\n\nHassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex–concave min–max optimization: provable algorithms and applications in machine learning. Optimization Methods and Software, pp. 1–35, 2021.\n\nHamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv\n\npreprint arXiv:1908.05659, 2019.\n\nRT Rockafellar and RJB Wets. Variational analysis springer. MR1491362, 1998.\n\nChaobing Song, Stephen J Wright, and Jelena Diakonikolas. Variance reduction via primal-dual accelerated dual averaging for nonsmooth convex finite-sums. In International Conference on Machine Learning, pp. 9824–9834. PMLR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nMatthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel\n\nmethods. Advances in Neural Information Processing Systems, 32:9134–9144, 2019.\n\nQuoc Tran-Dinh, Deyi Liu, and Lam M Nguyen. Hybrid variance-reduced sgd algorithms for\n\nminimax problems with nonconvex-linear function. In NeurIPS, 2020.\n\nMadeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, and Stephen Boyd. Convex optimization in julia. In 2014 First Workshop for High Performance Technical Computing in Dynamic Languages, pp. 18–28. IEEE, 2014.\n\nJie Wang, Rui Gao, and Yao Xie. Sinkhorn distributionally robust optimization. arXiv preprint\n\narXiv:2109.11926, 2021.\n\nMengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2): 419–449, 2017.\n\nYi Xu, Rong Jin, and Tianbao Yang. Non-asymptotic analysis of stochastic methods for non-smooth non-convex regularized problems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 2630–2640, 2019.\n\nYan Yan, Yi Xu, Qihang Lin, Lijun Zhang, and Tianbao Yang. Stochastic primal-dual algorithms T ) for problems without bilinear structure. arXiv preprint\n\n√\n\nwith faster convergence than O(1/ arXiv:1904.10112, 2019.\n\nYan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. In Conference on Neural Information Processing Systems, 2020.\n\nJunyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance\n\nreduction. In Advances in Neural Information Processing Systems, pp. 9075–9085, 2019.\n\nZhe Zhang and Guanghui Lan. Optimal algorithms for convex nested stochastic composite optimiza-\n\ntion. ArXiv e-prints, arXiv:2011.10076, 2021.\n\nYi Zhou, Zhe Wang, Kaiyi Ji, Yingbin Liang, and Vahid Tarokh. Momentum schemes with stochastic variance reduction for nonconvex composite optimization. arXiv preprint arXiv:1902.02715, 2019.\n\nDixian Zhu, Zhe Li, Xiaoyu Wang, Boqing Gong, and Tianbao Yang. A robust zero-sum game framework for pool-based active learning. In The 22nd international conference on artificial intelligence and statistics, pp. 517–526. PMLR, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA MORE RELATED WORK\n\nWang et al. (2021) studies the Sinkhorn distance constraint, a variant of Wasserstein distance based on entropic regularization. An efficient batch gradient descent with a bisection search algorithm has been proposed to obtain a near-optimal solution with an arbitrarily small sub-optimality gap. However, no non-asymptotic convergence results are established in their paper. Duchi & Namkoong (2021) developed a convex DRO framework with f -divergence constraints to improve model robustness. The author developed the finite-sample minimax upper and lower bounds and the non-asymptotic n), and provided the empirical studies on real distributional shifts tasks convergence rate of O(1/ with existing interior point solver (Udell et al., 2014) and gradient descent with backtracking Armijo line-searches (Boyd et al., 2004). However, no stochastic algorithms that directly optimize the considered constrained DRO with non-asymptotic convergence rates are provided in their paper.\n\n√\n\nCompositional Functions and DRO. The connection between compositional functions and DRO formulations have been observed and leveraged in the literature. Dentcheva et al. (2017) studied the statistical estimation of compositional functionals with applications to estimating conditionalvalue-at-risk measures, which is closely related to the CVaR constrained DRO. However, they do not consider stochastic optimization algorithms. To the best of our knowledge, Qi et al. (2021) was the first to use stochastic compositional optimization algorithms to solve KL-regularized DRO problems. Our work is different in that we solve KL-constrained DRO problems, which is more challenging than KL-regularized DRO problems. The benefits of using compositional optimization for solving DRO include (i) we do not need to maintain and update a high dimensional dual variable as in the primal-dual methods (Rafique et al., 2021); (ii) we do not need to worry about the batch size as in MLMC-based stochastic methods (Levy et al., 2020; Hu et al., 2021).\n\nB MORE EXPERIMENTAL RESULTS\n\nGPU Setting: All our results are conducted on Tesla V100.\n\nTesting convergence curves are presented in the Figure 3 and 4 for the convex and non-convex setting respectively.\n\nFigure 3: Testing accuracy (%) vs # of processed training samples for the convex setting. ρ is fixed to 0.5 on CIFAR10-ST and CIFAR100-ST, and 0.1 on ImageNet-LT and iNaturalist2018. The results are averaged over 5 independent runs.\n\nFigure 4: Testing accuracy (%) vs # of processed training samples for the non-convex setting. ρ is fixed to 0.5 on all datasets. The results are averaged over 5 independent runs.\n\nSensitivity to ρ. We study the sensitivity of different methods to ρ. The results on CIFAR10-ST and CIFAR100-ST are shown in Table 4 in the supplement, which demonstrates that the testing performance is sensitive to ρ. However, our method SCDRO is better than baselines PG-SMD2 and FastDRO for different values of ρ.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Test accuracy (%) of different methods for different constraint parameter ρ in the non-convex setting. The results are averaged over 5 independent runs.\n\nCIFAR10-ST\n\nCIFAR100-ST\n\nρ PG-SMD2 FastDRO SCDRO\n\nPG-SMD2 FastDRO SCDRO\n\n0.01 67.09 (± 0.59) 65.41 (± 0.33) 67.73 (± 0.39)\n\n57.31 (± 0.09) 57.60 (± 0.32) 57.84 (± 0.15)\n\n0.05 66.96 (± 0.71) 66.15 (± 0.09) 67.58 (± 0.48)\n\n56.44 (± 0.17) 57.20 (± 0.42) 57.60 (± 0.15)\n\n0.1 67.12 (± 0.61) 66.24 (± 0.63) 67.71 (± 0.43)\n\n55.85 (± 0.19) 56.78 (± 0.40) 58.32 (± 0.43)\n\n0.5 67.36 (± 0.36) 65.98 (± 0.45) 67.57 (± 0.28)\n\n52.68 (± 0.40) 55.58 (± 0.62) 57.90 (± 0.26)\n\n1 67.10 (± 0.61) 65.68 (± 0.52) 67.96 (± 0.50)\n\n48.72 (± 0.25) 52.39 (± 0.31) 57.71 (± 0.24)\n\nFigure 5: Running time comparison between PG-SMD2 and SCDRO\n\nPer Iteration Cost We report the per iteration cost between the non-convex primal-dual algorithm PG-SMD2 and SCDRO on a single Tesla V100 GPU in Figure 5. It is clear to see that the primal-dual algorithm incurs significantly overhead time due to the updates of dual variables. By comparing the large-scale datasets, ImageNet-LT and iNaturalist2018, with the medium datasets, CIFAR10-ST and CIFAR100-ST, the increased dataset size leads to amplified running time gap as the dual variables dependent on the dataset size O(n). For the largest iNaturalist2018 dataset, SCDRO could save days of training time.\n\nC PRELIMINARY LEMMAS\n\nLemma 6. For q ≥ 1, fλ(q) = λ log(q) + λρ is Lfλ-Lipschitz continuous and L∇fλ-smooth, where L∇fλ = Lfλ = λ.\n\nRemark: gi(w, λ) = exp( li(w) g(x) = 1 n\n∇fλ(g(x2))∥ ≤ λ∥g(x1) − g(x2)∥ for x, x1, x2 ∈ X .\n\nλ ) ≥ 1 as λ ≥ λ0 ∈ R+ and li(w) ≥ 0 in problem (3). Thus i=1 gi(w, λ) ≥ 1. Then by this lemma we have ∥∇fλ(g(x))∥ ≤ λ and ∥∇fλ(g(x1)) −\n\n(cid:80)n\n\nProof. For any q ≥ 1, we have\n\n∇fλ(q) =\n\nλ q\n\n≤ λ\n\nAnd for any q1, q2 ≥ 1, we have\n\n∥∇fλ(q1) − ∇fλ(q2)∥ ≤\n\nThis complete the proof.\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nλ q1\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nλ q2\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(q1 − q2)λ q1q2\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ λ∥q1 − q2∥\n\nLemma 7. Let LA = exp( C λ0 )( C2+2λ0C and LD = exp( C λ4 λ0 0\n(w, λ), where Lg = exp( C λ0\n\n)( G λ0\n\n)( G2 λ2 0\n\n+ L λ0\n\n), LB = exp( C λ0\n\n)( CG λ3 0\n\n+ G λ2 0\n\n), LC = exp( C λ0\n\n)( CG+λ0G λ3 0\n\n)\n\n). gi(w, λ) is Lg-Lipschtz continuous and L∇g-smooth in terms of\n\n+ C λ2 0\n\n) and L∇g = (cid:112)L2\n\nA + L2\n\nB + L2\n\nC + L2\n\nD,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nProof. The gradient of gi(w, λ) is given as\n\n∇w,λgi(w, λ)⊤ = (∂wgi(w, λ)⊤, ∂λgi(w, λ)) (cid:19) ∇wli(w) λ\n\n(cid:18) li(w) λ\n\nexp\n\n(cid:32)\n\n=\n\n⊤\n\n, − exp\n\n(cid:18) li(w) λ\n\n(cid:19) li(w) λ2\n\n(cid:33)\n\n.\n\nThen by Assumption 1, we have\n\n∥∇w,λgi(w, λ)∥ ≤ exp\n\n∇wli(w) λ\n\n(cid:19)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\nli(w) λ2\n\n(cid:18) li(w) λ\n(cid:18) C λ0\n\n(cid:19) (cid:18)(cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:18) G λ0\n\nλ≥λ0\n\n≤ exp\n\n+\n\n(cid:19)\n\n.\n\nC λ2 0\n\nThus, Lg = exp\n\n(cid:16) C λ0\n\n(cid:17) (cid:16) G λ0\n\n(cid:17)\n\n.\n\n+ C λ2 0\n\nFor for all (w, λ), (w′, λ′) ∈ X , we have\n\n∥∇w,λgi(w, λ) − ∇w,λgi(w′, λ′)∥2\n\n+ exp\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n+\n\nexp\n\n≤\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n(cid:18) li(w) λ\n(cid:18) li(w) λ\n\n(cid:19) ∇wli(w) λ\n(cid:19) li(w)\n\nλ2 − exp\n\n(cid:19) ∇wli(w) λ\n\n− exp\n\n+\n\n+\n\n+\n\nexp\n\nexp\n\nexp\n\n(cid:18) li(w′) λ\n\n(cid:18) li(w) λ\n(cid:18) li(w′) λ\n\n(cid:19) ∇wli(w′) λ\n(cid:19) li(w)\n\nλ2 − exp\n\n(cid:19) li(w′)\n\nλ2 − exp\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(cid:19) ∇wli(w′) λ\n\n(cid:19) ∇wli(w′) λ′ (cid:13) (cid:19) li(w′) 2\n(cid:13) (cid:13) λ′2 (cid:13)\n\n(cid:18) li(w′) λ′ (cid:18) li(w′) λ′ (cid:18) li(w′) λ\n(cid:18) li(w′) λ′ (cid:13) (cid:19) li(w′) 2\n(cid:13) (cid:13) λ2 (cid:13) (cid:19) li(w′) λ′2\n\n(cid:18) li(w′) λ\n(cid:18) li(w′) λ′\n\n− exp\n\n(cid:19) ∇wli(w′) λ′\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) λ ) ∇wli(w)\n\nλ\n\nTo bound the first term, we first check the Lipschitz continuous of exp( li(w) to w,\n\nwith respect\n\nλ\n\n∂w\n\nλ\n\n(cid:17)\n\n(cid:17) ∇wli(w)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:18) ∇wli(w)\n\n(cid:16)\n\n∂\n\nexp\n\n(cid:16) li(w)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\nexp\n\n(a) =\n\nexp\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:18) li(w) λ\n\n(cid:18) li(w) λ\n\n(cid:19) (cid:18) ∇wli(w)\n\nλ\n\nλ\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19) ∇2\n\nwli(w) λ\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:19) (cid:18) ∇wli(w)\n\n(cid:19)⊤ (cid:18) ∇wli(w)\n\nλ\n\nλ\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19) ∇2\n\nwli(w) λ\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:19)⊤(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(b) ≤ exp\n\n≤ exp\n\n(cid:19) (cid:13) (cid:18) li(w) (cid:18) ∇wli(w) (cid:13) (cid:13) λ\nλ (cid:13) (cid:19) (cid:19) (cid:18) G2 λ2 0\n\n(cid:18) C λ0\n\nL λ0\n\n+\n\n:= LA.\n\n(cid:19)(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19) ∇2\n\nwli(w) λ\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nwhere equality (a) is due to the property of the norm of rank-one symmetric matrix and inequality (b) is due to Cauchy-Schwarz inequality.\n\nTherefore, we have\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp(\n\nli(w) λ\n\n)\n\n∇wli(w) λ\n\n− exp(\n\nli(w′) λ\n\n)\n\n∇wli(w′) λ\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n≤ LA ∥w − w′∥2\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n∂\n\n(cid:16)\n\nexp\n\nFurthermore, it holds that (cid:16) li(w) (cid:17) ∇wli(w) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∂λ\n\nλ\n\nλ\n\n(cid:17)\n\n(cid:16)\n\n∂\n\nexp\n\n(cid:17)\n\n(cid:17) li(w) λ2\n\n(cid:16) li(w)\n\nλ\n\n∂w\n\n(cid:16)\n\n∂\n\nexp\n\n(cid:17)\n\n(cid:17) li(w) λ2\n\n(cid:16) li(w)\n\nλ\n\n∂λ\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nAs a result, we obtain\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n=\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19) li(w)∇wli(w) λ3\n\n+ exp\n\n(cid:18) li(w) λ\n\n(cid:19) (cid:18) ∇wli(w)\n\nλ2\n\n(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ exp\n\n(cid:18) C λ0\n\n(cid:19) (cid:18) CG λ3 0\n\n+\n\nG λ2 0\n\n(cid:19)\n\n:= LB\n\n=\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19) li(w)∇wli(w) λ3\n\n+ exp\n\n(cid:18) li(w) λ\n\n(cid:19) ∇wli(w) λ2\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ exp\n\n(cid:18) C λ0\n\n(cid:19) (cid:18) CG + λ0G\n\n(cid:19)\n\nλ3 0\n\n:= LC\n\n=\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19) l2\n\ni (w) λ4 + exp\n\n(cid:18) li(w) λ\n\n(cid:19) 2li(w) λ3\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤ exp\n\n(cid:18) C λ0\n\n(cid:19) (cid:18) C 2 + 2λ0C\n\n(cid:19)\n\nλ4 0\n\n:= LD.\n\n∥∇w,λgi(w, λ) − ∇w,λgi(w′, λ′)∥2 ≤ L2\n\nA ∥w − w′∥2 + L2 A + L2 A + L2\n\nC) ∥w − w′∥2 + (L2 C + L2 B + L2\n\nB ∥λ − λ′∥2 + L2 B + L2\n\nC ∥w − w′∥2 + L2 D) ∥λ − λ′∥2 (cid:13)(w⊤, λ) − (w′⊤, λ′)(cid:13) 2\n(cid:13)\n\nD) (cid:13)\n\n.\n\n= (L2\n\n≤ (L2\n\nD ∥λ − λ′∥2\n\nThus L∇g = (cid:112)L2\n\nA + L2\n\nB + L2\n\nC + L2\n\nD.\n\nLemma 8. F (w, λ) is LF -smooth, where LF = ̃λL2\n\ng + 2Lg + ̃λL∇g + 1 + ̃λ.\n\nRemark: Lemma 6, 7 and Lemma 8 imply that L∇fλ = Lfλ ≤ LF , Lg ≤ LF and LF ≥ 1.\n\n1 , λ1)⊤, x2 = (w⊤\n\n2 , λ2)⊤ ∈ X , and let d(x) = (0, · · · , 0, log(g(x)) +\n\nProof. For all x1 = (w⊤ ρ)⊤ ∈ Rd+1, by expansion we have ∥∇F (x1) − ∇F (x2)∥ = ∥∇fλ1(g(x1))∇g(x1) + d(x1) − ∇fλ2 (g(x2))∇g(x2) − d(x2)∥ ≤ ∥∇fλ1(g(x1))∇g(x1) − ∇fλ2 (g(x2))∇g(x2)∥ + | log(g(x1)) − log(g(x2))| ≤ ∥∇fλ1(g(x1))∇g(x1) − ∇fλ1 (g(x2))∇g(x1)∥ + ∥∇fλ1(g(x2))∇g(x1) − ∇fλ2 (g(x2))∇g(x1)∥\n\n+ ∥∇fλ2(g(x2))∇g(x1) − ∇fλ2(g(x2))∇g(x2)∥ + |g(x1) − g(x2)|.\n\nNoting the Lipschtiz continuous of g(x) and ∇g(x), we obtain\n\n∥∇F (x1) − ∇F (x2)∥\n\n≤ (L∇fλ1\n\nLg + 1)|g(x1) − g(x2)| +\n\n∥∇g(x1)∥ g(x2)\n\n∥λ1 − λ2∥ + Lfλ2\n\n∥∇g(x1) − ∇g(x2)∥\n\n(a)\n\n≤ (L∇fλ1 ≤ (L∇fλ1\n\nL2\n\ng + Lg)∥x1 − x2∥ + ∥∇g(x1)∥∥λ1 − λ2∥ + Lfλ2\n\nL∇g ∥x1 − x2∥\n\nL2\n\ng + 2Lg + Lfλ2\n\nL∇g )∥x1 − x2∥\n\n(b)\n\n≤ ( ̃λL2\n\ng + 2Lg + ̃λL∇g + 1 + ̃λ)∥x1 − x2∥.\n\nwhere the inequality (a) is due to g(x2) ≥ 1 and the inequality (b) is due to the upper bound of λ. Thus, LF = ̃λL2\n\ng + 2Lg + ̃λL∇g + 1 + ̃λ.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nC.1 PROOF OF LEMMA 1\n\nProof. Recall the primal problem:\n\np∗ =\n\nmax {p∈∆n,D(p,1/n)≤ρ}\n\nn (cid:88)\n\ni=1\n\npili(w) + λ0D(p, 1/n).\n\nInvoking dual variable ̄λ, we obtain the dual problem:\n\nq∗ = min ̄λ≥0\n\nmax p∈∆n\n\nn (cid:88)\n\ni=1\n\npili(w) − ̄λ(D(p, 1/n) − ρ) − λ0D(p, 1/n).\n\n(9)\n\nSet ̄p = (1/n, . . . , 1/n), which is a Slater vector satisfying D( ̄p, 1/n) − ρ < 0. Applying Lemma 3 in (Nedi ́c & Ozdaglar, 2009), we have\n\n| ̄λ∗| ≤\n\nq∗ −\n\n ̄pili(w) − λ0D( ̄p, 1/n)\n\n.\n\n(cid:32)\n\n1 ρ\n\nn (cid:88)\n\n(cid:33)\n\ni=1 Since the primal problem is concave in term of p given w, we have p∗ = q∗. Therefore,\n\n(cid:32)\n\n(cid:33)\n\np∗ −\n\n ̄pili(w)\n\nn (cid:88)\n\ni=1\n\n(cid:32) n\n\n(cid:88)\n\ni=1\n\ni li(w) − λ0D(p∗, 1/n) − ̄p∗\n\n(cid:33)\n\n ̄pili(w)\n\nn (cid:88)\n\ni=1\n\n1 ρ\n\n1 ρ\n\n| ̄λ∗| ≤\n\n=\n\n≤\n\nC ρ\n\n,\n\n(10)\n\nwhere the last inequality is because |li(w)| ≤ C for w ∈ W. Let λ = ̄λ + λ0, we have\n\nq∗ = min λ≥λ0\n\nmax p∈∆n\n\nn (cid:88)\n\ni=1\n\npili(w) − λ(D(p, 1/n) − ρ) − λ0ρ.\n\nSection G will also show\n\nq∗ = min λ≥λ0\n\nλ log\n\n(cid:32)\n\n1 n\n\nn (cid:88)\n\ni=1\n\nexp\n\n(cid:18) li(w) λ\n\n(cid:19)(cid:33)\n\n+ λ(ρ − ρ0).\n\nBy Eq. (10), we have the optimal solution of above optimization problem |λ∗| ≤ | ̄λ∗| + λ0 ≤ λ0 + C ρ , which complete the proof\n\nD PROOFS IN SECTION 4\n\nD.1 TECHNICAL LEMMAS\n\nLemma 9. Suppose Assumption 2 holds and i ∼ D and s are initialized with s1 = exp( li(w1) Then for every t ∈ {1, · · · T } we have (cid:34)\n\nλ1\n\n(cid:35)\n\n).\n\nE[∥g(xt+1) − st+1∥2] ≤ E\n\n(1 − β)∥g(xt) − st∥2 +\n\n2L2\n\ng∥xt+1 − xt∥2\n\nβ\n\n+ β2σ2\n\n.\n\n(cid:35)\n\n∥xt+1 − xt∥2 + βT σ2\n\n.\n\n(11)\n\nTaking summation of E[∥g(xt+1) − st+1∥2] from 1 to T , we have\n\nT (cid:88)\n\nt=1\n\nE[∥g(xt) − st∥2] ≤ E\n\n(cid:34)\n\n∥g(x1) − s1∥2 β\n\n+\n\n2L2 g\nβ2\n\nT (cid:88)\n\nt=1\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Note that st+1 = (1 − β)st + βgi(xt+1) and E[g(xt+1) − gi(xt+1)]=0, then by simple expansion we have\n\nE[∥g(xt+1) − st+1∥2]\n\n= E[∥β(g(xt+1) − gi(xt+1)) + (1 − β)(g(xt+1) − st)∥2] = E[β2∥g(xt+1) − gi(xt+1)∥2 + (1 − β)2∥g(xt+1) − st∥2]\n\n+ 2 E[⟨g(xt+1) − gi(xt+1), g(xt+1) − st⟩] (cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) 0\n\n= E[β2∥g(xt+1) − gi(xt+1)∥2 + (1 − β)2∥g(xt+1) − g(xt) + g(xt) − st∥2].\n\n(12)\n\nInvkoing Lemma 7 to Eq. (12) and recalling Assumption 2 , we obtain\n\nE[∥g(xt+1) − st+1∥2]\n\n(a)\n\n≤ E[β2∥g(xt+1) − gi(xt+1)∥2 + (1 − β)2(1 + β)∥g(xt) − st∥2\n\n+ (1 +\n\n1 β\n\n)(1 − β)2∥g(xt+1) − g(xt)∥2\n\n(b)\n\n≤ E\n\n(c)\n\n≤ E\n\n(cid:34)\n\nβ2∥g(xt+1) − gi(xt+1)∥2 + (1 − β)∥g(xt) − st∥2 +\n\n(cid:34)\n\n(1 − β)∥g(xt) − st∥2 +\n\n2L2\n\ng∥xt+1 − xt∥2\n\nβ\n\n(cid:35)\n\n+ β2σ2\n\n.\n\n2L2\n\ng∥xt+1 − xt∥2\n\n(cid:35)\n\nβ\n\nwhere the inequality (a) is due to (a + b)2 ≤ (1 + β)a2 + (1 + 1 (1 − β)2 ≤ 1, (1 + 1\n\nβ ) ≤ 2\n\nβ and the Lemma 7 and the inequality (c) is from Assumption 2.\n\nβ )b2, the inequality (b) is because of\n\nLemma 10. Under Assumption 1, run Algorithm 1 with ηLF ≤ 1/4, and then the output xR of Algorithm 1 satisfies\n\nER[dist(0, ˆ∂ ̄F (xR))2] ≤\n\n2 + 40LF η T\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt)∥2 +\n\n2∆ ηT\n\n+\n\n40LF ∆ T\n\n.\n\n(13)\n\nProof. The proof of this lemma follow the proof of Theorem 2 in (Xu et al., 2019).\n\nRecall the update of xt+1 is\n\nxt+1 = ΠX (xt − ηzt)\n\n= arg min x∈Rd+1\n\n{δX (x) + ⟨zt, x − xt⟩ +\n\n1 2η\n\n∥x − xt∥2}.\n\nthen by Exercise 8.8 and Theorem 10.1 of (Rockafellar & Wets, 1998) we know\n\n−zt −\n\n1 η\n\n(xt+1 − xt) ∈ ˆ∂δX (xt+1) ,\n\nwhich implies that\n\n∇F (xt+1) − zt −\n\n1 η\n\n(xt+1 − xt) ∈ ∇F (xt+1) + ˆ∂δX (xt+1) = ˆ∂ ̄F (xt+1) .\n\n(14)\n\nBy the update of xt+1, we also have,\n\nδX (xt+1) + ⟨zt, xt+1 − xt⟩ +\n\n1 2η\n\n∥xt+1 − xt∥2 ≤ δX (xt).\n\nSince F (x) is smooth with parameter LF , then\n\nF (xt+1) ≤ F (xt) + ⟨∇F (xt), xt+1 − xt⟩ +\n\nCombing the above two inequalities, we get\n\nLF 2\n\n∥xt+1 − xt∥2.\n\n⟨zt − ∇F (xt), xt+1 − xt⟩ +\n\n1 2\n\n(1/η − L)∥xt+1 − xt∥2 ≤ ̄F (xt) − ̄F (xt+1).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nThat is\n\n1 2\n\n(1/η − LF )∥xt+1 − xt∥2 ≤ ̄F (xt) − ̄F (xt+1) − ⟨zt − ∇F (xt), xt+1 − xt⟩\n\n≤ ̄F (xt) − ̄F (xt+1) + η∥zt − ∇F (xt)∥2 +\n\n1 4η\n\n∥xt − xt+1∥2,\n\nwhere the last inequality uses Young’s inequality ⟨a, b⟩ ≤ ∥a∥2 + ∥b∥2 above inequality and summing it across t = 1, · · · , T , we have\n\n4 . Then by rearranging the\n\nT (cid:88)\n\nt=1\n\n1 − 2ηLF 4η\n\n∥xt+1 − xt∥2 ≤ ̄F (x1) − ̄F (xT +1) +\n\nT (cid:88)\n\nη∥zt − ∇F (xt)∥2\n\n≤ ̄F (x1) − inf\n\nx∈X\n\n ̄F (x) +\n\nt=1\n\nT (cid:88)\n\nt=1\n\nη∥zt − ∇F (xt)∥2\n\nT (cid:88)\n\n≤ ∆ +\n\nη∥zt − ∇F (xt)∥2.\n\n(15)\n\nt=1 By the same method used in the proof of Theorem 2 in Xu et al. (2019), we have the following inequality,\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt+1) +\n\n1 η\n\n(xt+1 − xt)∥2 ≤ 2\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt)∥2 +\n\n2∆ η\n\n+ (2L2\n\nF +\n\n3LF η\n\n)\n\nT (cid:88)\n\nt=1\n\n∥xt+1 − xt∥2.\n\nRecalling ηLF ≤ 1\n\n4 and combining Eq. (15) and Eq. (16), we obtain\n\nT (cid:88)\n\nt=1\n\n(a) ≤ 2\n\n∥zt − ∇F (xt+1) +\n\n1 η\n\n(xt+1 − xt)∥2\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt)∥2 +\n\n(cid:32)\n\n2∆ η\n\n+\n\n5LF η\n\n1 1/4 − η1LF /2\n\n(cid:32)\n\nη1∆ + η1\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt)∥2\n\n(b) ≤ 2\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt)∥2 +\n\n2∆ η\n\n+ 40LF ∆ + 40ηLF\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt)∥2.\n\n(16)\n\n(cid:33)(cid:33)\n\n(17)\n\nwhere inequality (a) is due to (2L2\n\nF + 3LF\n\nη ) ≤ 5LF\n\nη\n\nand inequality (b) is due to\n\n1\n\n1/4−ηLF /2 ≤ 8.\n\nRecalling Eq. (14) and the output rule of Algorithm 1, we have\n\nER[dist(0, ˆ∂ ̄F (xR))2] ≤\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt+1) +\n\n1 η\n\n(xt+1 − xt)∥2.\n\n(18)\n\nThen by combining Eqs. (17,18) together we have the Lemma.\n\nLemma 11. Under Assumption 1, 2, run Algorithm 1 with η ≤\n\n√\n\nβ 4+20L2 g\n\n≤ 1 4LF\n\n4LF\n\n, and then we\n\nhave T\n(cid:88)\n\n1 T\n\nt=1\n\nE[∥zt − ∇F (xt)∥2] ≤\n\n2E[∥z1 − ∇F (x1)∥2] βT\n\n+\n\n∆ ηT\n\n+\n\n20LF E[∥g(x1) − s1∥2] βT\n\n+ 24βL2\n\nF σ2.\n\nProof. To facilitate our proof statement, we define the following notations: ∇F (xt)⊤ = (∂wF (xt)⊤, ∂λF (xt)) = (∇fλt(g(xt))∂wg(xt)⊤, ∇fλt(g(xt))∂λg(xt) + log(g(xt)) + ρ) (cid:101)∇F (xt)⊤ = (∇fλt (g(xt))∂wgi(xt)⊤, ∇fλt(g(xt))∂λgi(xt) + log(g(xt)) + ρ) G(xt)⊤ = (Gwt(xt)⊤, Gλt (xt)) = (∇fλt(st)∂wgi(xt)⊤, ∇fλt(st)∂λgi(xt) + log(st) + ρ). It is worth to notice that E[ (cid:101)∇F (xt)] = ∇F (xt).\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nFor every iteration t, by simple expansion we have It = E[∥∇F (xt) − zt∥2]\n\n= E[∥∇F (xt) − (1 − β)zt−1 − βG(xt)∥2] = E[∥(1 − β)(∇F (xt) − ∇F (xt−1)) + (1 − β)∇F (xt−1) − (1 − β)zt−1 + β∇F (xt) − βG(xt)∥2] = E[∥(1 − β) (∇F (xt) − ∇F (xt−1) (cid:125)\n\n) + (1 − β) (∇F (xt−1) − zt−1) (cid:125)\n\n∥2]\n\n(cid:124)\n\n(cid:124)\n\n(cid:123)(cid:122) A\n\n(cid:123)(cid:122) B\n\n+ E[∥β( (cid:101)∇F (xt) − G(xt) ) + β (∇F (xt) − (cid:101)∇F (xt)) (cid:125) (cid:125)\n\n(cid:124)\n\n(cid:124)\n\n∥2]\n\n(cid:123)(cid:122) C\n\n(cid:123)(cid:122) D\n\n= E[(1 − β)2∥A∥2 + (1 − β)2∥B∥2 + β2∥C∥2 + β2∥D∥2 + 2(1 − β)(1 − β)⟨A, B⟩\n\n+ 2β(1 − β)⟨A, C⟩ + 2β(1 − β)⟨A, D⟩ + 2(1 − β)β⟨B, C⟩ + 2(1 − β)β⟨B, D⟩ + 2β2⟨C, D⟩]\n\n(a)\n\n= E[(1 − β)2∥A∥2 + (1 − β)2∥B∥2 + β2∥C∥2 + β2∥D∥2\n\n+ 2(1 − β)2⟨A, B⟩ + 2(1 − β)β⟨C, B⟩ + 2β(1 − β)⟨A, C⟩ + 2β2⟨C, D⟩],\n\nwhere the equality (a) is due to E⟨∇F (xt) − ∇F (xt−1), ∇F (xt) − (cid:101)∇F (xt)⟩ = 0 and E⟨zt−1 − ∇F (xt−1), ∇F (xt) − (cid:101)∇F (xt)⟩ = 0.\n\nBy Young’s inequality, we have (1 − β)2⟨A, B⟩ ≤ (1 − β)⟨A, B⟩ ≤ 2 2β(1 − β)⟨C, B⟩ ≤ (1−β)2β 2β2⟨C, D⟩ ≤ β2∥C∥2 + β2∥D∥2. Therefore, noting (1 − β) < 1 and 1/β > 1, we can obtain It ≤ E[(1 − β)2∥A∥2 + (1 − β)2∥B∥2 + β2∥C∥2 + β2∥D∥2\n\n∥B∥2 + 2β∥C∥2, 2β(1 − β)⟨A, C⟩ ≤ (1 − β)2∥A∥2 + β2∥C∥2 and\n\nβ ∥A∥2 + (1−β)2β\n\n∥B∥2,\n\n2\n\n8\n\n+\n\n2 β\n\n∥A∥2 +\n\n(1 − β)2β 2\n+ (1 − β)2∥A∥2 + β2∥C∥2 + β2∥C∥2 + β2∥D∥2]\n\n(1 − β)2β 2\n\n∥B∥2 + 2β∥C∥2 +\n\n∥B∥2\n\n≤ E[(1 − β)∥B∥2 +\n\n4 β\n\n∥A∥2 + 5β∥C∥2 + 2β2∥D∥2].\n\n(19)\n\nThus recalling the defintion of G(xt), (cid:101)∇F (xt), ∇F (xt) and applying the smoothness and Lipschitz continuity of fλ and g, we have C = ∥ (cid:101)∇F (xt) − G(xt)∥2\n\n= ∥∇fλt(g(xt))∂wgi(xt) − ∇fλt(st)∂wtgi(xt)∥2\n\n+ ∥∇fλt(g(xt))∂λgi(xt) + log(g(xt)) − ∇fλt(st)∂λgi(xt) − log(st)∥2\n\n≤ ∥∇fλt(g(xt))∂wgi(xt) − ∇fλt(st)∂wtgi(xt)∥2 + 2∥∇fλt(g(xt))∂λgi(xt) − ∇fλt(st)∂λgi(xt)∥2\n\n+ 2∥ log(g(xt)) − log(st)∥2\n\n(a)\n\n≤ 2L2\n\ngL2\n\n∇fλt\n\n∥st − g(xt)∥2 + 2∥st − g(xt)∥2\n\n(b)\n\n≤ 2L2\n\nF ∥st − g(xt)∥2,\n\n(20) where the inequality (a) is due to | log(g(xt)) − log(st)| ≤ |st − g(xt)| since g(xt) ≥ 1, st ≥ 1 for all t = {1, · · · , T } by the definition and initialzation of gi(xt), st, and the inequality (b) is due to L2\n\n+ 1 ≤ L2\n\ngL2\n\n∇fλt\n\nF .\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nAnd by the similar method, we also have\n\nD = ∥∇F (xt) − (cid:101)∇F (xt)∥2\n\n= ∥∇fλt(g(xt))∂wg(xt) − ∇fλt (g(xt))∂wgi(xt)∥2\n\n+ ∥∇fλt(g(xt))∂λg(xt) + log(g(xt)) + ρ − ∇fλt(g(xt))∂λgi(xt) − log(g(xt)) − ρ∥2\n\n= ∥∇fλt(g(xt))∂wg(xt) − ∇fλt(g(xt))∂wgi(xt)∥2 + ∥∇fλt(g(xt))∂λg(xt) − ∇fλt(g(xt))∂λgi(xt)∥2\n\n≤ L2\n\nfλt\n\n∥∇g(xt) − ∇gi(xt)∥2 ≤ L2\n\nF ∥∇g(xt) − ∇gi(xt)∥2.\n\nThus combining the Eqs. (19, 20, 21) and applying Assumption 2, we can obtain E[∥zt − ∇F (xt)∥2]\n\n= E[(1 − β)∥zt−1 − ∇F (xt−1)∥2 +\n\n4 β\n\n∥∇F (xt) − ∇F (xt−1)∥2\n\n+ 5β∥ (cid:101)∇F (xt) − G(xt)∥2 + 2β2∥∇F (xt) − (cid:101)∇F (xt)∥2]\n\n(21)\n\n≤ E[(1 − β)∥zt−1 − ∇F (xt−1)∥2 +\n\n4 β\n\nL2\n\nF ∥xt − xt−1∥2 + 10L2\n\nF β∥g(xt) − st∥2] + 2β2L2\n\nF σ2.\n\nTaking summation of E[∥zt+1 − ∇F (xt+1)∥2] from 1 to T and invoking Lemma 9, we have\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2]\n\n≤\n\n≤\n\nE[∥∇F (x1) − z1∥2] β\n\nE[∥∇F (x1) − z1∥2] β\n\n+\n\n+\n\n4L2 F\nβ2\n\n4L2 F\nβ2\n\nT (cid:88)\n\nt=1\n\nT (cid:88)\n\nt=1\n\nE[∥xt+1 − xt∥2] + 10L2\n\nF β\n\nT (cid:88)\n\nt=1\n\nE[∥xt+1 − xt∥2]\n\nE[∥g(xt) − st∥2] + 2β2LF σ2\n\n(cid:32)\n\n(cid:34)\n\nE\n\n+ 10L2\n\nF\n\n∥g(x1) − s1∥2 β\n\n+\n\n2L2 g\nβ2\n\nT (cid:88)\n\nt=1\n\n(cid:35)\n\n(cid:33)\n\n∥xt+1 − xt∥2\n\n+ βT σ2\n\n+ 2βL2\n\nF T σ2.\n\nTaking Eq. (15) into the above inequality, we have\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2]\n\n≤\n\n+ (\n\nE[∥∇F (x1) − z1∥2] β\n(cid:18) E[∥g(x1) − s1∥2] β\n\n+ 10L2\n\nF\n\n4L2 F\nβ2 +\n\n(cid:19)\n\n+ βT σ2\n\n+ 2βL2\n\nF T σ2\n\n(cid:32)\n\nF L2 20L2 β2\n\ng\n\n)\n\nη 1/4 − ηLF /2\n\n(cid:32)\n\n∆ + η\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2]\n\n(cid:33)(cid:33)\n\n(a) ≤\n\nE[∥∇F (x1) − z1∥2] β\n\n+ (\n\n4L2 F\nβ2 +\n\n20L2 F L2 β2\n\ng\n\n(cid:32)\n\n(cid:32)\n\n)\n\n8η\n\n∆ + η\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2]\n\n(cid:33)(cid:33)\n\n+ 10L2\n\nF\n\n(cid:18) E[∥g(x1) − s1∥2] β\n\n(cid:19)\n\n+ βT σ2\n\n+ 2βL2\n\nF T σ2\n\n(b) ≤\n\nE[∥z1 − ∇F (x1)∥2] β\n\n∆ 2η (cid:18) E[∥g(x1) − s1∥2] β\n\n+ 10L2\n\nF\n\n+\n\n+\n\n1 2\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2]\n\n(cid:19)\n\n+ βT σ2\n\n+ 2βL2\n\nF T σ2,\n\n(22)\n\nwhere the inequality (a) is due to ηLF ≤ 1/4 and the inequality (b) is due to 8(4L2 β2 2 . Rearranging terms and dividing T on both sides of Eq. (22), we compelte the proof.\n\nF + 20L2\n\nF L2\n\ng)η2 ≤\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nD.2 PROOF OF THEOREM 1\n\nProof. Since η = β\n\n20L2 F\n\n, LF ≥ 1 and LF ≤ Lg, it holds that η ≤\n\n√\n\nβ 4+20L2 g\n\n≤ 1 4LF\n\n4LF\n\nwhich satisfy\n\nthe assumptions of η in Lemma 10 and Lemma 11. Therefore, combining Lemma 10 and Lemma 11, we have E[dist(0, ˆ∂ ̄F (xR))2]\n\n≤\n\n≤\n\n≤\n\n2 + 40LF η T\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2] +\n\n2∆ ηT\n\n+\n\n40LF ∆ T\n\n12 T\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt)∥2] +\n\n2∆ ηT\n\n+\n\n20LF ∆ T\n\n24E[∥z1 − ∇F (x1)∥2] βT\n\n+\n\n12∆ ηT\n\n+\n\n240L2 F\n\nE[∥g(x1) − s1∥2]\n\nβT\n\n+ 288L2\n\nF βσ2 +\n\n2∆ ηT\n\n+\n\n20L2 F ∆ T\n\n.\n\nBy the definition of s1 and Assumption 2, it holds that\n\nE[∥s1 − g(x1)∥2] ≤ E[∥gi(x1) − g(x1)∥2] ≤ σ2.\n\n(23)\n\n(24)\n\n∇fλ1\n\n≤ L2\n\nF and 2L2\n\nSince L2 gL2 ≤ L2 E[∥z1 − ∇F (x1)∥2] = ∥∇fλ1(gi(x1))∇gi(x1) − ∇fλ1(g(x1))∇g(x1)∥2 = ∥∇fλ1(gi(x1)∇gi(x1)) − ∇fλ1(g(x1))∇gi(x1) + ∇fλ1(g(x1))∇gi(x1) − ∇fλ1 (g(x1)∇g(x1))∥2\n\nF , we have\n\nfλ1\n\n(a)\n\n≤ 2∥∇fλ1(gi(x1)) − ∇fλ1(g(x1))∥2∥∇gi(x1)∥2 + 2∥∇fλ1(gi(x1))∥2∥∇gi(x1) − ∇g(x1)∥2 ≤ (2L2\n\n)σ2 ≤ 4L2\n\n+ 2L2\n\n(25)\n\nF σ2,\n\ngL2\n\n∇fλ1\n\nfλ1\n\nwhere the inequality (a) is due to ∥a + b∥2 ≤ 2∥a∥2 + 2∥b∥2.\n\nCombining Eqs. (23,24,25), we obtain E[dist(0, ˆ∂ ̄F (xR))2]\n\n24E[∥z1 − ∇F (x1)∥2] βT\n\n+\n\n240L2 F\n\nE[∥g(x1) − s1∥2]\n\nβT\n\n+ 288L2\n\nF βσ2 +\n\n2∆ ηT\n\n+\n\n20L2 F ∆ T\n\n≤\n\n≤\n\n≤\n\n96L2 βT 96L2 √\n\nF σ2\n\nF σ2 T\n\n+\n\n+\n\n≤ (624σ2 + 280∆)\n\n+\n\n12∆ ηT 240∆L2 F√ T\nL2 F√ T\n\n+\n\n12∆ ηT F σ2\n\n240L2 βT\n\n+ 288L2\n\nF βσ2 +\n\n2∆ ηT\n\n+\n\n20L2 F ∆ T\n\n+\n\n40∆L2 F√ T\n\n+\n\n20L2 F ∆ T\n\n+\n\n+\n\n528L2 √\n\nF σ2 T\n20L2 F ∆ T\n\n.\n\nThis complete the proof.\n\nE PROOFS IN SECTION 4.2\n\nE.1 TECHNICAL LEMMAS\n\nt , ut)⊤, qλt = (0⊤, log(st) + ρ)⊤ and Lemma 12. Let zt = ∇fλt(st)qt + qλt, where qt = (v⊤ 0 ∈ Rd. Let ∥κt∥2 = ∥st − g(xt)∥2 + ∥vt − ∂wg(xt)∥2 + |ut − ∂λg(xt)|2. Under Assumption 1, run Algorithm 2, and then for every t ∈ {1, · · · T } we have ∥zt − ∇F (xt)∥2 ≤ 4L2\n\nF ∥κt∥2.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nProof. By simple expansion, it holds that\n\n∥zt − ∇F (xt)∥2 = ∥∇fλt(g(xt))∂wg(xt) − ∇fλt(st)vt∥2\n\n+ ∥∇fλt(g(xt))∂λg(xt) − ∇fλt(st)vt + log(g(xt)) − log(st)∥2\n\n(a)\n\n≤ 2∥∇fλt(g(xt))∂wg(xt) − ∇fλt(st)vt∥2 + 2∥∇fλt(g(xt))∂λg(xt) − ∇fλt(st)ut∥2\n\n+ 2∥g(xt) − st∥2\n\n= 2∥∇fλt(g(xt))∇g(xt) − ∇fλt(st)qt∥2 + 2∥g(xt) − st∥2,\n\n(26) where the inequality (a) is because ∥a + b∥2 ≤ 2∥a∥2 + 2∥b∥2, and | log(x) − log(y)| ≤ |x − y| for all x, y ≥ 1.\n\nApplying the smoothness and Lipschitz continuity of fλ and g, we obtain\n\n∥∇fλt(g(xt))∇g(xt) − ∇fλt(st)qt∥2 = ∥∇fλt(g(xt))∇g(xt) − ∇fλt(st)∇g(xt) + ∇fλt(st)∇g(xt) − ∇fλt (st)qt∥2 ≤ 2∥∇fλt(g(xt))∇g(xt) − ∇fλt(st)∇g(xt)∥2 + 2∥∇fλt (st)∇g(xt) − ∇fλt(st)qt∥2 ≤ 2L2\n\n∥qt − ∇g(xt)∥2 + 2∥g(xt) − st∥2.\n\n(27) Noting ∥qt − ∇g(xt)∥2] = ∥vt − ∂wg(xt)∥2 + |ut − ∂λg(xt)|2 and combining Eqs. (26, 27), we have\n\n∥st − g(xt)∥2 + 2Lfλt\n\ngL2\n\n∇fλt\n\n∥zt − ∇F (xt)∥2 gL2 ≤ (4L2 F ∥st − g(xt)∥2 + 4L2 F (∥st − g(xt)∥2 + ∥vt − ∂wg(xt)∥2 + |ut − ∂λg(xt)|2).\n\n+ 2)∥st − g(xt)∥2 + 4L2\n\nF ∥qt − ∇g(xt)∥2\n\n∥qt − ∇g(xt)∥2\n\n∇fλt\n\nfλt\n\n≤ 4L2 = 4L2 This complete the proof.\n\nLemma 13. Under Assumption 1, 2, run Algorithm 2, and then for every t ∈ {1, · · · T } we have\n\nE[∥κt+1∥2] ≤ (1 − βt)2E[∥κt∥2] + 8(1 − βt)2L2\n\nF\n\nE[∥xt+1 − xt∥2] + 6β2\n\nt σ2.\n\nProof. Since st+1 = (gi(xt+1) + (1 − β)(st − gi(xt)), it holds that\n\nE[∥st+1 − g(xt+1)∥2] = E[∥gi(xt+1) + (1 − βt)(st − gi(xt)) − g(xt+1)∥2] ≤ E[∥(1 − βt)(st − g(xt)) + βt(gi(xt+1) − g(xt+1))\n\n+ (1 − βt)(gi(xt+1) − gi(xt) − (g(xt+1) − g(xt)))∥2] = E[(1 − βt)2∥st − g(xt)∥2] + E[∥βt(gi(xt+1) − g(xt+1)) + (1 − βt)(gi(xt+1) − gi(xt) − (g(xt+1) − g(xt)))∥2],\n\n(28)\n\nwhere the last inequality is due to E[gi(xt+1) − g(xt+1)] = 0. Noting E[⟨gi(xt+1) − gi(xt+1), g(xt+1) − g(xt)⟩] = E[∥(g(xt+1) − g(xt))∥2] and applying the Lipschitz continuty of gi(x), we have E[∥gi(xt+1) − gi(xt+1) − (g(xt+1) − g(xt))∥2] = E[∥(gi(xt+1) − gi(xt+1)∥2 + ∥(g(xt+1) − g(xt))∥2 − 2 ⟨gi(xt+1) − gi(xt+1), g(xt+1) − g(xt)⟩] = E[∥(gi(xt+1) − gi(xt+1)∥2 − ∥(g(xt+1) − g(xt))∥2] ≤ E[∥(gi(xt+1) − gi(xt+1)∥2] E[∥xt+1 − xt∥2]. ≤ L2\n\n(29)\n\ng\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nCombining Eqs. (28, 29) and invoking the Lipschitz continuty of gi(x), under Assumption 2, we have E[∥st+1 − g(xt+1)∥2] ≤ (1 − βt)2E[∥st − g(xt)∥2]\n\n+ 2β2\n\nt\n\nE[∥gi(xt+1) − g(xt)∥2] + 2(1 − βt)2E[∥gi(xt+1) − gi(xt+1) − (g(xt+1) − g(xt))∥2]\n\n≤ (1 − βt)2E[∥st − g(xt)∥2] + 2β2\n\nt σ2 + 2(1 − βt)2L2\n\ng\n\nE[∥xt+1 − xt∥2].\n\n(30)\n\nIn the same way, we also have E[∥vt+1 − ∂wg(xt+1)∥2] ≤ (1 − βt)2E[∥vt − ∂wg(xt)∥2] + 2β2\n\nt σ2 + 2(1 − βt)2L2\n\n∇g\n\nE[∥xt+1 − xt∥2], (31)\n\nE[|ut+1 − ∂λg(xt+1)|2] ≤ (1 − βt)2E[|ut − ∂λg(xt)|2] + 2β2\n\nt σ2 + 2(1 − βt)2L2\n\n∇g\n\nE[∥xt+1 − xt∥2].\n\n(32)\n\nTherefore, combining Eqs. (30, 32, 31), we obtain\n\nE[∥κt+1∥2] ≤ (1 − βt)2E[∥κt∥2] + 6β2\n\nt σ2 + 4(1 − βt)2(L2\n\nwhere the last inequality applies (L2\n\n≤ (1 − βt)2E[∥κt∥2] + 8(1 − βt)2L2 ∇g + L2\n\ng) ≤ 2L2\n\nF\n\nF . This complete the proof.\n\n∇g + L2 E[∥xt+1 − xt∥2] + 6β2\n\ng)∥xt+1 − xt∥2) t σ2,\n\nLemma 14. Under Assumption 1 and 2, for any α > 1, let k = ασ2/3 and c = σ2 Algrithm 2 satisfies T\n(cid:88)\n\n(w+tσ2)1/3 , βt = cη2\n\n14LF k3 + 130L4\n\nF . Then with ηt =\n\nT (cid:88)\n\nk\n\nE[∥κ1∥2] η0\n\n−\n\nE[∥κT +1∥2] ηT\n\n+\n\n4L4\n\nF\n\nηtE[∥κt∥2] ≤\n\nt=1\n\nt=1\n\n, w = max(2σ2, (16L2\n\nF k)3) LF t and after running T iterations,\n\n6c2η3\n\nt σ2 + 64L2\n\nF ∆.\n\nProof. Since w ≥ (16L2\n\nF k)3, it is easy to note that 1\n16L2 F\n\nηt ≤ η0 ≤\n\n≤\n\n1 4LF\n\n.\n\nIn addition,\n\nβt = cη2\n\nt ≤ cη2\n\n0 ≤ (\n\nσ2\n\n14LF k3 + 130L4 F )\n\n1 256L4 F\n\n=\n\nσ2L3 F\n14LF α3σ2\n\n1 256L4 F\n\n+\n\n65 128\n\n=\n\n1 14α3\n\n1 2556L2 F\n\n+\n\n65 128\n\n≤ 1.\n\nWith ηt =\n\nk\n\n(w+tσ2)1/3 , we obtain 1\nηt\n\n1 ηt−1\n\n−\n\n=\n\n(w + tσ2)1/3 − (w + (t − 1)σ2)1/3 k\n\n(a) ≤\n\nσ2 3k(w + (t − 1)σ2)2/3\n\n(b) ≤\n\n=\n\nσ2 3k(w/2 + tσ2)2/3 22/3σ2 3k3 η2\n\n(c) ≤\n\nt\n\n22/3\n\n12LF k3 ηt ≤\n\nσ2 7Lk3 ηt,\n\n≤\n\nσ2 3k(w/2 + tσ2/2)2/3\n\n=\n\n22/3σ2 3k(w + tσ2)2/3\n\nwhere the inequality (a) uses the inequality (x + y)1/3 − x1/3 ≤ yx−2/3 w ≥ 2σ2, and the inequality (c) is due to ηt ≤ 1 4LF\n\n3\n\n.\n\n, the inequality (b) is due to\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nNoting βt = cη2\n\nt and 0 ≤ (1 − βt) ≤ 1, by Lemma 13 we have\n\n−\n\nE[∥κt+1∥2] ηt (1 − βt)2 ηt\n\n≤ (\n\nE[∥κt∥2] ηt−1 1\nηt−1\n\n−\n\n)E[∥κt∥2] + 6c2η3\n\nt σ2 +\n\n≤ (η−1\n\nt − η−1\n\nt−1 − 2cηt)E[∥κt∥2] + 6c2η3\n\nt σ2 +\n\nF\n\n8(1 − βt)2L2 ηt 8(1 − βt)2L2 ηt\n\nF\n\nE[∥xt+1 − xt∥2]\n\nE[∥xt+1 − xt∥2]\n\n≤ −260L4\n\nF ηtE[∥κt∥2] + 6c2η3\n\nt σ2 +\n\nE[∥xt+1 − xt∥2],\n\n(33)\n\nF\n\n8(1 − βt)2L2 ηt t−1 − 2cηt ≤ σ2\n\n7LF k3 ηt − 2(\n\nσ2\n\n14LF k3 + 130L4\n\nF )ηt ≤\n\nwhere the last inequality is due to η−1 −260L4\n\nF ηt.\n\nt − η−1\n\nTaking summation of Eq. (33) from 1 to T , we have E[∥κT +1∥2] ηT\n\nE[∥κ1∥2] η0\n\nηtE[∥κt∥2] ≤\n\n260L4\n\nT (cid:88)\n\n−\n\nF\n\nt=1\n\n+\n\nT (cid:88)\n\nt=1\n\n6c2η3\n\nt σ2 + 8L2\n\nF\n\nT (cid:88)\n\nt=1\n\n1 ηt\n\nE[∥xt+1 − xt∥2].\n\n(34)\n\nIn the same way with Eq. (15) and ηt ≤ η1, ∀t ≥ 1, we could also have\n\n1 − 2η1LF 4\n\nT (cid:88)\n\nt=1\n\n1 ηt\n\n∥xt+1 − xt∥2 ≤\n\nT (cid:88)\n\nt=1\n\n1 − 2ηtLF 4ηt\n\n∥xt+1 − xt∥2 ≤ ∆ +\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt)∥2.\n\n(35)\n\nNoting η1LF ≤ 1\n\n4 and invoking Lemma 12, we obtain\n\nT (cid:88)\n\nt=1\n\n1 ηt\n\nE[∥xt+1 − xt∥2] ≤\n\n4 1 − 2η1LF\n\n(∆ +\n\nT (cid:88)\n\nt=1\n\nηtE[∥zt − ∇F (xt)∥2])\n\n≤ 8∆ + 8\n\nT (cid:88)\n\nt=1\n\n≤ 8∆ + 32L2\n\nF\n\nηtE[∥zt − ∇F (xt)∥2]\n\nT (cid:88)\n\nt=1\n\nηtE[∥κt∥2].\n\nE[∥κ1∥2] η0\n\n−\n\nE[∥κT +1∥2] ηT\n\n+\n\nT (cid:88)\n\nt=1\n\n6c2η3\n\nt σ2 + 64L2\n\nF ∆.\n\n(36)\n\n(37)\n\nCombining Eqs. (34, 36), we have T\n(cid:88)\n\nηtE[∥κt∥2] ≤\n\n4L4\n\nF\n\nThis complete the proof.\n\nt=1\n\nE.2 PROOF OF THEOREM 2\n\nProof. Noting the monotonity of ηt and dividing\n\nT (cid:88)\n\nt=1\n\n∥xt+1 − xt∥2 ≤\n\n1 1/4 − η1LF /2\n\nη1\n\n1/4−η1LF /2 on both sides of Eq. (35), we have (cid:32)\n\n(cid:33)\n\nη1∆ + η1\n\nηt∥zt − ∇F (xt)∥2\n\n.\n\n(38)\n\nT (cid:88)\n\nt=1\n\nBy the same method used in the proof of Theorem 2 in Xu et al. (2019), we have the following inequality,\n\n∥zt − ∇F (xt+1) +\n\n1 ηt\n\n(xt − xt+1)∥2 ≤ 2∥zt − ∇F (xt)∥2 +\n\n2 (cid:0) ̄F (xt+1) − ̄F (xt)(cid:1) ηt\n\n+ (2L2\n\nF +\n\n3LF ηt\n\n)∥xt+1 − xt∥2.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nMultiplying ηt on both sides of the above inequality and taking summation from 1 to T , we have\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt+1) +\n\n1 ηt\n\n(xt+1 − xt)∥2\n\n(a) ≤ 2\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt)∥2 + 2∆ + 5LF\n\n(cid:32)\n\n1 1/4 − η1LF /2\n\n(cid:32)\n\nη1∆ + η1\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt)∥2\n\n(cid:33)(cid:33)\n\n(b) ≤ 12\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt)∥2 + 12∆,\n\nwhere inequality (a) is due to (2L2 1/4−η1LF /2 ≤ 8.\n\n1\n\nF + 3LF\n\nηt\n\n) ≤ 5LF ηt\n\n, inequality (b) is due to η1LF ≤ 1\n\n4 and\n\n(39)\n\nCombining Eqs. (37, 39) and invoking Lemma 12 we have\n\nT (cid:88)\n\nt=1\n\nηt∥zt − ∇F (xt+1) +\n\n1 ηt\n\n(xt+1 − xt)∥2\n\n≤ 48L2\n\nF\n\nT (cid:88)\n\nt=1\n\nηtE[∥κt∥2] + 12∆\n\n≤ 12\n\n(cid:32) E[∥κ1∥2] η0\n\n−\n\nE[∥κT +1∥2] ηT\n\n+\n\nT (cid:88)\n\nt=1\n\n6c2η3\n\nt σ2 + 64L2\n\nF ∆\n\n(cid:33)\n\n+ 12∆.\n\n(40)\n\nNoting the monotonity of ηt and dividing T ηT on both sides of Eq. (40), we obtain\n\n1 T\n\nT (cid:88)\n\nt=1\n\n∥zt − ∇F (xt+1) +\n\n1 ηt\n\n(xt+1 − xt)∥2\n\nE[∥κT +1∥2] T η2 T\nCombining Eqs. (18, 41) and noting (cid:80)T\n\n(cid:32) E[∥κ1∥2] T ηT η0\n\n≤ 12\n\n−\n\nt=1 η3\n\n+\n\n1 T ηT\n\nT (cid:88)\n\nt=1\n\n6c2η3\n\nt σ2 +\n\n(cid:33)\n\nF ∆\n\n64L2 T ηT\n\n+\n\n12∆ T ηT\n\n.\n\n(41)\n\nt ≤ O(log T ), we get the conclusion that\n\nE[dist(0, ˆ∂ ̄F (xR))2] ≤\n\n1 T\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇F (xt+1) +\n\n1 ηt\n\n(xt+1 − xt)∥2]\n\n+\n\n1 T ηT\n\nT (cid:88)\n\nt=1\n\n6c2η3\n\nt σ2 +\n\n(cid:33)\n\nF ∆\n\n64L2 T ηT\n\n+\n\n12∆ T ηT\n\n≤ 12\n\n(cid:32) E[∥κ1∥2] T ηT η0 (cid:19)\n\n(cid:18) log T T 2/3\n\n.\n\nThis complete the proof.\n\n≤ O\n\nF PROOFS IN SECTION 5\n\nF.1 TECHNICAL LEMMAS\n\nLemma 15. (w, λ).\n\nIf li(w) is convex for all i, we can show that F (w, λ) is jointly convex in terms of\n\nProof. We have\n\nF (w, λ) = max p∈∆n\n\nn (cid:88)\n\ni=1 (cid:124)\n\npili(w) − λ(\n\nn (cid:88)\n\npi log(npi) − ρ) − λ0ρ\n\n.\n\ni=1\n\n(cid:123)(cid:122) G(w,λ,p)\n\n(cid:125)\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nSince G(w, λ, p) is jointly convex in terms of (w, λ) for every fixed p, F (w, λ) is jointly convex in terms of (w, λ).\n\nLemma 16. Under Assumption 1, 2, run Algorithm 1 with η ≤ and apply SCDRO to the new objective ̄Fμ(x) by adding μxt to (∇fλt(st)∇wgi(xt)⊤, ∇fλt(st)∇λgi(xt) + log(st) + ρ)⊤ in Eq. (6) of Algorithm 1, where μ is a small constant to be determined later. Without loss of the generality, we assume 0 < μ ≤ 1\n\n4LF\n\n√\n\nβ 9+20L2 g\n\n≤ 1 6LF\n\n1 T\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2] ≤\n\n2 and then we have ∆μ ηT\n\n2E[∥z1 − ∇Fμ(x1)∥2] βT\n\n+\n\n+\n\n20LF E[∥g(x1) − s1∥2] βT\n\n+ 24βL2\n\nF σ2.\n\nProof. To facilitate our proof statement, we define the following notations: ∇Fμ(xt)⊤ = (∂wFμ(xt)⊤, ∂λFμ(xt)) = (∇fλt(g(xt))∂wg(xt)⊤ + μw⊤\n\nt , ∇fλt(g(xt))∂λg(xt) + log(g(xt)) + ρ + μλt)\n\n(cid:101)∇Fμ(xt)⊤\n\n= (∇fλt (g(xt))∂wgi(xt)⊤ + μw⊤\n\nt , ∇fλt(g(xt))∂λgi(xt) + log(g(xt)) + ρ + μλt)\n\nGμ(xt)⊤\n\n= (Gwt(xt)⊤, Gλt(xt)) = (∇fλt(st)∂wgi(xt)⊤ + μw⊤\n\nt , ∇fλt(st)∂λgi(xt) + log(st) + ρ + μλt).\n\nIt is worth to notice that E[ (cid:101)∇Fμ(xt)] = ∇Fμ(xt).\n\nSince F (x) is LF -smooth, then we have Fμ(x) is LFμ-smooth, where LFμ = (LF + μ). Noting LF > 1 and μ ≤ 1 2 LF . For every iteration t, by simple expansion we have It = E[∥∇Fμ(xt) − zt∥2]\n\n2 , we obtain LF + μ ≤ 3\n\n= E[∥∇Fμ(xt) − (1 − β)zt−1 − βGμ(xt)∥2] = E[∥(1 − β)(∇Fμ(xt) − ∇Fμ(xt−1)) + (1 − β)∇Fμ(xt−1) − (1 − β)zt−1 + β∇Fμ(xt) − βGμ(xt)∥2] = E[∥(1 − β)(∇Fμ(xt) − ∇Fμ(xt−1)) + (1 − β)(∇Fμ(xt−1) − zt−1)∥2]\n\n+ E[∥β( (cid:101)∇Fμ(xt) − Gμ(xt)) + β(∇Fμ(xt) − (cid:101)∇Fμ(xt))∥2]\n\n= E[∥(1 − β) (∇Fμ(xt) − ∇Fμ(xt−1) (cid:125)\n\n(cid:124)\n\n) + (1 − β) (∇F (xt−1) − zt−1) (cid:125)\n\n(cid:124)\n\n∥2]\n\n(cid:123)(cid:122) B\n\n(cid:123)(cid:122) A\n\n+ E[∥β( (cid:101)∇F (xt) − G(xt) ) + β (∇F (xt) − (cid:101)∇F (xt)) (cid:125) (cid:125)\n\n(cid:124)\n\n(cid:124)\n\n∥2].\n\n(cid:123)(cid:122) C\n\n(cid:123)(cid:122) D\n\nThe above inequality shows that the only difference between It in the proof of Lemma 11 and It in the proof of Lemma 16 is term A.\n\nTherefore, by the same method used in the proof of Lemma 11, we have\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n≤\n\nE[∥∇Fμ(x1) − z1∥2] β\n\n+ (\n\nFμ\n\n4L2 β2 +\n\nF L2 20L2 β2\n\ng\n\n(cid:18) )\n\nη 1/4 − ηLFμ/2\n\n(∆μ + η\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2])\n\n(cid:19)\n\n+ 10L2\n\nF\n\n(cid:18) E[∥g(x1) − s1∥2] β\n\n(cid:19)\n\n+ βT σ2\n\n+ 2βL2\n\nF T σ2.\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nBy LFμ ≤ 3\n\n2 LF and ηLFμ ≤ 3\n\n2 ηLF ≤ 1/4, it holds that\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n≤\n\nE[∥∇Fμ(x1) − z1∥2] β\n\n+ (\n\n9L2 F\nβ2 +\n\n20L2 F L2 β2\n\ng\n\n(cid:32)\n\n(cid:32)\n\n)\n\n8η\n\n∆μ + η\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n(cid:33)(cid:33)\n\n+ 10L2\n\nF\n\n(cid:18) E[∥g(x1) − s1∥2] β\n\n(cid:19)\n\n+ βT σ2\n\n+ 2βL2\n\nF T σ2\n\n≤\n\nE[∥z1 − ∇Fμ(x1)∥2] β\n\n∆μ 2η (cid:18) E[∥g(x1) − s1∥2] β\n\n+ 10L2\n\nF\n\n+\n\n+\n\n1 2\n\nT (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n+ βT σ2\n\n(cid:19)\n\n+ 2βL2\n\nF T σ2,\n\n(42)\n\nwhere the last inequality is due to 8(9L2\n\nF + 20L2\n\nF L2\n\ng)η2 ≤ β2 2 .\n\nRearranging terms and dividing T on both sides of Eq. (42), we complete the proof of this Lemma.\n\nLemma 17. At the k-th stage of RASCDRO, let βk = cη2 E[∥κk∥2] βkTk\n\nE[∥zt − ∇Fμ(xt)∥2] ≤\n\n1 8L2 F Tk\n\nTk(cid:88)\n\nt=1\n\nk and c = 512L4\n\nF we have E[∆μ 64L2 F\nβkTk\n\nk ]ηk\n\n+ 6βkσ2 +\n\n,\n\n(43)\n\nwhere ∆μ\n\nk = Fμ(xk) − inf x∈X Fμ(x).\n\nProof. Recall the definition of ∥κt∥2 and by the same proof of Lemma 12 we have\n\nDenote κt at kth-stage as κt\n\nk, and by Lemma 13, at the kth-stage in RASCDRO we have\n\n∥zt − ∇Fμ(xt)∥2 ≤ 4L2\n\nF ∥κt∥2.\n\n(44)\n\nE[∥κt+1\n\nk ∥2] ≤ (1 − βk)2∥κt\n\nk∥2 + 6β2\n\nkσ2 + 8L2\n\nF (1 − βk)2∥xt+1 − xt∥2\n\n≤ (1 − βk)2t∥κk∥2 + 6β2\n\nkσ2\n\nt (cid:88)\n\n(1 − βk)2(t−i)\n\ni=1\n\n+ 8L2\n\nF (1 − βk)2\n\nt (cid:88)\n\ni=1\n\n(1 − βk)2(t−i)∥xi+1 − xi∥2\n\n≤ (1 − βk)2t∥κk∥2 + 6βkσ2\n\n(45)\n\n+ 8L2\n\nF (1 − βk)2\n\nt (cid:88)\n\ni=1\n\n(1 − βk)2(t−i)∥xi+1 − xi∥2.\n\nCombining Eqs. (44,45), we obtain\n\n1 4L2 F Tk\n\nTk(cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n≤\n\n≤\n\n1 Tk\n\n1 Tk\n\nTk(cid:88)\n\nt=1\n\nE[(1 − βk)2∥κt\n\nk∥2 + 6β2\n\nkσ2 + 8L2\n\nF (1 − βk)2∥xt+1 − xt∥2]\n\nTk(cid:88)\n\n(1 − βk)2t−2E[∥κk∥2] + 6βkσ2 +\n\nt=1\n\n8L2\n\nF (1 − βk)2\n\nTk(cid:88)\n\nt−1 (cid:88)\n\nTk\n\nt=1\n\ni=1\n\n(1 − βk)2(t−i)E[∥xi+1 − xi∥2].\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nNoting (cid:80)Tk Tk(cid:88)\n\n1 4L2 F Tk\n\nt=1\n\n≤\n\n≤\n\n≤\n\nE[∥κk∥2] βkTk\n\nE[∥κk∥2] βkTk\n\nE[∥κk∥2] βkTk\n\nt=1(1 − βk)2t−2 ≤ 1/βk and invoking Eq. (38), we have\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n+ 6βkσ2 +\n\n8L2\n\nF (1 − βk)2\n\nTk(cid:88)\n\nβkTk\n\nE[∥xt+1 − xt∥2]\n\n+ 6βkσ2 +\n\n8L2\n\nF (1 − βk)2\n\nβkTk\n\nt=1 (cid:32)\n\nηk 1/4 − ηkLFμ/2\n\n(cid:32)\n\nE[∆μ\n\nk ] + ηk\n\nTk(cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n(cid:33)(cid:33)\n\n+ 6βkσ2 +\n\nE[∆μ 64L2 F\nβkTk\n\nk ]ηk\n\n+\n\n64L2\n\nF η2\n\nk\n\nβkTk\n\nTk(cid:88)\n\nt=1\n\n∥zt − ∇Fμ(xt)∥2],\n\nwhere the last inequality is due to 1/(1/4 − ηkLFμ/2) ≤ 8, (1 − βk)2 ≤ 1, L2\n\n∇g + L2\n\ng ≤ 2L2 F .\n\nInvoking βk = cη2\n\nk and c = 576L4\n\nF to above inequality, we get the conclusion that E[∆μ 64L2 F\nβkTk\n\nE[∥κk∥2] βkTk\n\n+ 6βkσ2 +\n\nk ]ηk\n\n.\n\nE[∥zt − ∇Fμ(xt)∥2] ≤\n\n1 8L2 F Tk\n\nTk(cid:88)\n\nt=1\n\nF.2 PROOF OF LEMMA 3\n\nProof. Since li(w) is convex for all i, by Lemma 15 we know F (x) is convex. And thus by the definition of ̄Fμ(x) we have ̄Fμ(x) is a strongly convex function. Then by strong convexity, we have μ\n2\n\n∥y − x∥2, ∀x, y ∈ X , v ∈ ∂ ̄Fμ(x).\n\n ̄Fμ(y) ≥ ̄Fμ(x) + v⊤(y − x) +\n\nThen\n\ninf x∈X\n\n ̄Fμ (x) ≥ min\n\ny∈X\n\n ̄Fμ(x) + v⊤(y − x) +\n\n≥ min\n\ny\n\n ̄Fμ(x) + v⊤(y − x) +\n\nμ 2\nμ 2\n\n∥y − x∥2\n\n∥y − x∥2\n\n= ̄Fμ(x) −\n\n∥v∥2 2μ\n\n,\n\n∀v ∈ ∂ ̄Fμ(x).\n\nHence, ∥v∥2\n\n2μ ≥ ̄Fμ(x) − inf x∈X ̄Fμ (x) , ∀v ∈ ∂ ̄Fμ(x), which implies dist(0, ∂ ̄Fμ(x))2 ≥ 2μ (cid:0) ̄Fμ(x) − ̄Fμ (x∗)(cid:1) .\n\nF.3 PROOF OF LEMMA 4\n\nProof. We use inductions to prove E[∥zk − ∇Fμ(xk)∥2] ≤ μεk/4, E[∥g(xk) − sk∥2] ≤ μεk/4 and E[Fμ(xk) − inf\n\nFμ(x)] ≤ εk. Let’s consider the first stage in the beginning.\n\nx∈X\n\nLet ε1 = ∆μ, thus E[Fμ(x1) − inf x∈X Fμ(x)] ≤ ε1. And we can use a batch size of 4/με1 for initialization.to make sure E[∥∇Fμ(x1) − z1∥2] ≤ με1/4,E[∥s1 − g(x1)∥2] ≤ με1/4.\n\nSuppose that E[∥g(xk−1) − sk−1∥2] ≤ μεk−1/4, E[∥zk−1 − ∇Fμ(xk−1)∥2] ≤ μεk−1/4 and E[Fμ(xk−1) − inf x∈X Fμ(x)] ≤ εk−1 . By setting βk−1 = min{ μεk−1 }, ηk−1 = min{ μεk−1 4608L4\n\n} and Tk−1 = max{ 147456L4\n\n}, it is easy to obtain that ηk−1 ≤\n\n, 147456L4\n\n1 384L2 F\n\nF σ2 ,\n\n384L2\n\nF σ2\n\nμ2εk−1\n\nμ\n\nF\n\nF σ2 ,\n\n1 4608L4 F\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nβk−1 √\n\n9+20L2 g\n\n4LF\n\n. Therefore, invoking Lemma 16 we have\n\nE[∥zk − ∇Fμ(xk)∥2]\n\n≤\n\n≤\n\n≤\n\n1 Tk−1\n\nTk−1 (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\nE[2∥zk−1 − ∇Fμ(xk−1)∥2] βk−1Tk−1\n\n+\n\nE[∆μ k−1] ηk−1Tk−1\n\n+\n\n20LF E[∥g(xk−1) − sk−1∥2] βk−1Tk−1\n\n+ 24βkL2\n\nF σ2\n\nμεk−1 2βk−1Tk−1\n\n+\n\nεk−1 ηk−1Tk−1\n\n+\n\n5LF μεk−1 βk−1Tk−1\n\n+ 24βk−1L2\n\nF σ2.\n\nWithout loss of the generality, we consider the case μεk−1/σ2 ≤ 1. By definition have βk−1 = μεk−1/(384L2 F σ2/(μ2εk−1), which imply\n\nF σ2), ηk−1 = μεk−1/(4608L4\n\nF σ2) and Tk−1 = 147456L4\n\n1 βk−1Tk−1\n\n≤\n\nμ 384L2 F\n\n,\n\n1 ηk−1Tk−1\n\n≤\n\nμ 32\n\nand 24βk−1L2\n\nF σ2 ≤\n\nμεk−1 16\n\n.\n\nThen, note LF ≥ 1, μ < 1 and εk = εk−1/2 we have\n\nE[∥zk − ∇Fμ(xk)∥2] ≤\n\n≤\n\n=\n\n≤\n\n+\n\nμεk−1 16 μεk−1 32\n\n+\n\nμεk 16\n\nμ2εk−1 768L2 F\nμεk−1 768\n\nμεk 192 μεk 4\n\n+\n\n.\n\n+\n\nμεk 40\n\n+\n\nμεk 8\n\n+\n\n5μ2εk−1 384LF\n\n+\n\nμεk−1 8\n\n+\n\n5μεk 192\n\n+\n\nμεk−1 16\n\nNext we need to show E[∥g(xk)−sk∥2] ≤ μεk/4 under the assumption that E[∥g(xk−1)−sk−1∥2] ≤ μεk−1/4.\n\nBy Lemma 9, we have E[∥g(xk) − sk∥2]\n\n1 Tk−1\n\nTk−1 (cid:88)\n\nt=1\n\nE[∥g(xt) − st∥2]\n\n=\n\n≤\n\nβ2 \n\n\n\n≤\n\nμεk−1 4βk−1Tk−1\n\n+\n\n2L2 g\nk−1Tk−1\n\nβ2\n\n+ βk−1σ2,\n\nE[∥g(xk−1) − sk−1∥2] βk−1Tk−1\n\n+\n\n2L2 g\nk−1Tk−1\n\nTk−1 (cid:88)\n\nt=1\n\nE[∥xt+1 − xt∥2] + βk−1σ2\n\nηk−1 1/4 − ηk−1LFμ/2\n\n E[∆μ\n\nk−1] + ηk−1\n\nTk−1 (cid:88)\n\nt=1\n\n\n\n\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n\n\n\n\nwhere ∆μ sk−1∥2] ≤ μεk−1/4 and E[Fμ(xk−1) − inf x∈X Fμ(x)] ≤ εk−1, it holds that\n\nk−1 = Fμ(xk−1) − inf x∈X Fμ(x). With 1/(1/4 − ηk−1LFμ /2) ≤ 8, E[∥g(xk−1) −\n\nE[∥g(xk) − sk∥2] ≤\n\nμεk−1 2βk−1Tk−1\n\n+\n\n16L2 β2\n\ngηk−1εk−1 k−1Tk−1\n\n+\n\n4L2\n\ngη2 k−1μεk−1 β2\n\nk−1\n\n+ βk−1σ2\n\n+\n\nL2 gμεk−1 288L4 F\n\n+\n\nL2\n\ngμεk−1 36L4 F\n\n+\n\nμεk−1 192L2 F\n\n≤\n\n≤\n\nμεk 384L2 F\nμεk 2\n\n.\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\nInvoking Lemma 10, at (k − 1)-th stage (k > 1) we have\n\nE[dist(0, ˆ∂ ̄Fμ(xk))2]\n\n≤\n\n≤\n\n≤\n\n2 + 40LFμηk−1 Tk−1\n\nTk−1 (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2] +\n\n2E[∆μ k−1] ηk−1Tk−1\n\n+\n\n40LFμ\n\nE[∆μ\n\nk−1]\n\nTk−1\n\n(2 + 40LFμηk−1)μεk−1 4\nμεk 8\n\n197μεk 192\n\n+\n\n+\n\n40LFμμεk−1 147456L4 F\n\n+\n\n2εk−1 ηk−1Tk−1\n\n+\n\n40LFμεk−1 Tk−1\n\n≤ 2μεk,\n\nwhere the second inequality is due to LFμηk−1 ≤ (3/2)LF ηk−1 ≤ 1/1536. Since Fμ(xk) ≤ ̄Fμ(xk) and inf x∈X Fμ(x) = inf x∈X ̄Fμ(x), applying Lemma 3 we have 2μεk 2μ\n\nFμ(x)] ≤ E[ ̄Fμ(xk) − inf\n\nE[dist(0, ˆ∂ ̄Fμ(xk))2] ≤\n\nE[Fμ(xk) − inf\n\n ̄Fμ(x)] ≤\n\n1 2μ\n\nx∈X\n\nx∈X\n\n= εk.\n\nThis complete the proof of this Lemma.\n\nF.4 PROOF OF THEOREM 3\n\nProof. Invoking Lemma 4, then after K = O(log2(ε1/ε)) stages, we have 2K−1 = ε.\n\nE[Fμ(xK) − inf\n\nFμ(x)] ≤ εK =\n\nx∈X\n\nε1\n\nSince (cid:80)K\n\nk=1 2k = O(1/ε), the overall oracle complexity is\n\nK (cid:88)\n\nk=1\n\nTk +\n\n4 με1\n\nK (cid:88)\n\nk=2\n\nK (cid:88)\n\nk=1\n\n1 μ2εk\n\n+\n\n4 με1\n\n1\n\n2k +\n\n4 με1\n\n≤ 36864σ2L4\n\nF\n\n≤\n\n36864σ2L4 F\nμ2ε\n\n≤ O(\n\n1 μ2ε\n\n).\n\nF.5 PROOF OF COROLLARY 1\n\nIt is easy to note that Fμ(xK)−Fμ(x∗) ≤ Fμ(xK)−inf x∈X Fμ(x), where x∗ = arg minx∈X F (x). Therefore, if after K stages it holds that E[Fμ(xK)−inf x∈X Fμ(x)] ≤ ε/2 with an oracle complexity of O(1/μ2ε), we have E[Fμ(xK) − Fμ(x∗)] ≤ ε/2 , i.e., E[F (xK) + μ∥xK∥2/2 − F (x∗) − μ∥x∗∥2/2] ≤ ε/2. By Assumption 1(a) W is bounded by R, and then by setting μ = ε/(2(R2 + ̃λ2)), with ∥x∥2 ≤ (R2 + ̃λ2) we have\n\nε 2\n\n+ (2(R2 + ̃λ2))\n\n≤\n\n+\n\n≤ ε\n\nμ 2\n\nε 2\n\nε 2\n\nE[F (xK) − F (x∗)] ≤\n\nwith an oracle complexity of O(1/ε3).\n\nF.6 PROOF OF LEMMA 5\n\nProof. We use inductions to prove E[∥κk∥2] ≤ μεk/16L2 consider the first stage in the beginning. Let ε1 = ∆μ, thus E[Fμ(x1) − inf x∈X Fμ(x)] ≤ ε1. And we can use a batch size of 48L2 F /με1 for initialization to make sure E[∥κ1∥2] = E[∥s1 − g(x1)∥2 + ∥v1 − ∂wg(x1)∥2 + |u1 − ∂λg(x1)|2] ≤ με1/16L2\n\nF and E[Fμ(xk) − inf\n\nFμ(x)] ≤ εk. Let’s\n\nx∈X\n\nF .\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nSuppose that E[∥κk−1∥2] ≤ μεk−1/16L2 setting βk−1 = min{ μεk−1 max{ 147456L3\n\nF σ2 , , 147456L4\n\n, 147456L4\n\n768L2 F σ2\n\n1 768L2 F\n\n}.\n\nF\n\nμ3/2√\n\nF σ εk−1\n\nμεk−1\n\nμ\n\nF and E[Fμ(xk−1) − inf x∈X Fμ(x)] ≤ εk−1. By } and Tk−1 = F σ2 ,\n\n}, ηk−1 = min{\n\n1 18432L4 F\n\n18432L3\n\nμεk−1\n\n√\n\nThen following the above Lemma 17, for k ≥ 1,\n\nE[∥κk∥2] ≤\n\n≤\n\n≤\n\n1\n\nTk−1 (cid:88)\n\nt=1\n\n4L2\n\nF Tk−1 2E[∥κk−1∥2] βk−1Tk−1 μεk−1\n\n4L2\n\nF βk−1Tk−1\n\nE[∥zt − ∇Fμ(xt)∥2]\n\n+ 12βk−1σ2 +\n\nE[∆μ 128L2 F\nβk−1Tk−1\n\nk−1]ηk−1\n\n+ 12βk−1σ2 +\n\n128L2\n\nF εk−1ηk−1\n\nβk−1Tk−1\n\n.\n\nWithout loss of the generality, we consider the case μεk−1/σ2 ≤ 1. By definition we have βk−1 = μεk−1/(768L2\n\nF σ2), ηk−1 =\n\n√\n\n1 βk−1Tk−1\n\nμεk−1/(9216L3 F σ), which imply 1\nηk−1Tk−1\n\n1 96L2 F\n\nμ 8\n\n≤\n\n,\n\n≤\n\nand 12βkσ2 ≤\n\nμεk−1 64L2 F\n\n.\n\nThen, noting LF ≥ 1, μ < 1 and εk = εk−1/2 we have\n\nE[∥κk∥2] ≤\n\n≤\n\n≤\n\nμεk−1 384L4 F\nμεk 192L2 F\nμεk 16L2 F\n\n.\n\n+\n\n+\n\nμεk−1 64L2 F\nμεk 32L2 F\n\n+\n\n+\n\nμεk−1 6912L4 F\nμεk 3456L2 F\n\nThen by Eq. (44), we have ∥zk − ∇Fμ(xk)∥2 ≤ 4L2 (k − 1)-th stage (k > 1) we have\n\nF ∥κk∥2 ≤ μεk/4. Invoking Lemma 10, at\n\nE[dist(0, ˆ∂ ̄Fμ(xk))2]\n\n≤\n\n≤\n\n≤\n\n2 + 40LFμηk−1 Tk−1\n\nTk−1 (cid:88)\n\nt=1\n\nE[∥zt − ∇Fμ(xt)∥2] +\n\n2E[∆μ k−1] ηk−1Tk−1\n\n+\n\n40LFμ\n\nE[∆μ\n\nk−1]\n\nTk−1\n\n(2 + 40LFμηk−1)μεk−1 2\nμεk 2\n\n773μεk 768\n\n+\n\n+\n\n40LFμμεk−1 73728L4 F\n\n+\n\n2εk−1 ηk−1Tk−1\n\n+\n\n40LFμεk−1 Tk−1\n\n≤ 2μεk,\n\nwhere the second inequality is due to LFμηk−1 ≤ (3/2)LF ηk−1 ≤ 1/3072. Since Fμ(xk) ≤ ̄Fμ(xk) and inf x∈X Fμ(x) = inf x∈X ̄F μ(x), applying Lemma 3 we have 2μεk 2μ\n\nFμ(x)] ≤ E[ ̄Fμ(xk) − inf\n\nE[dist(0, ˆ∂ ̄Fμ(xk))2] ≤\n\nE[Fμ(xk) − inf\n\n ̄Fμ(x)] ≤\n\n1 2μ\n\nx∈X\n\nx∈X\n\n= εk.\n\nThis complete the proof of this Lemma.\n\nF.7 PROOF OF THEOREM 4\n\nProof. Invoking Lemma 5, then after K = O(log2(ε1/ε)) stages, we have 2K−1 = ε.\n\nE[Fμ(xK) − inf\n\nFμ(x)] ≤ εK =\n\nx∈X\n\nε1\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nSince (cid:80)K\n\nk=1 2k = O(1/ε), the overall oracle complexity is\n\nK (cid:88)\n\nk=1\n\nTk +\n\n48L2 F\nμε1\n\n≤ O\n\n≤ O\n\n(cid:32) K\n\n(cid:88)\n\nk=2\n\n(cid:32) K\n\n(cid:88)\n\nk=2\n\n(cid:18)\n\nmax\n\nmax\n\n(cid:18) 1 μεk\n\n(cid:32)\n\n2k μ\n\n,\n\n(cid:18) 1 με\n\n,\n\n1 μ3/2\n\n,\n\n1 μ3/2√ √\n2 μ3/2√ (cid:19)(cid:19)\n\nk\n\nεk\n\n√\n\n.\n\nε\n\n(cid:19)(cid:33)\n\nεk\n\n(cid:33)(cid:33)\n\n+\n\n48L2 F\nμε1\n\n+\n\n48L2 F\nμε1\n\nThis complete the proof.\n\n≤ O\n\nmax\n\nG DERIVATION OF THE COMPOSITIONAL FORMULATION\n\nRecall the original KL-constrained DRO problem: n\n(cid:88)\n\nmin w∈W\n\nmax {p∈∆n:D(p,1/n)≤ρ}\n\ni=1\n\nwhere ∆n = {p ∈ Rn : (cid:80)n small positive constant.\n\npili(w) − λ0D(p, 1/n),\n\ni=1 pi = 1, 0 ≤ pi ≤ 1}, D(p, 1/n) is the KL divergence and λ0 is a\n\nIn order to tackle this problem, let us first consider the robust loss\n\nmax {p∈∆n:D(p,1/n)≤ρ}\n\nn (cid:88)\n\npili(w) − λ0D(p, 1/n).\n\ni=1 And then we invoke the dual variable λ to transform this primal problem to the following form\n\nmax p∈∆n\n\nmin ̄λ≥0\n\nn (cid:88)\n\ni=1\n\npili(w) − ̄λ(D(p, 1/n) − ρ) − λ0D(p, 1/n).\n\nSince this problem is concave in term of p given w, by strong duality theorem, we have\n\nmax p∈∆n\n\nmin ̄λ≥0\n\nn (cid:88)\n\ni=1\n\npili(w) − ̄λ(D(p, 1/n) − ρ) − λ0D(p, 1/n)\n\n= min ̄λ≥0 Let λ = ̄λ + λ0, we have\n\nmax p∈∆n\n\nn (cid:88)\n\ni=1\n\npili(w) − ̄λ(D(p, 1/n) − ρ) − λ0D(p, 1/n).\n\nmin ̄λ≥0\n\nmax p∈∆n\n\nn (cid:88)\n\ni=1\n\n= min λ≥λ0\n\nmax p∈∆n\n\npili(w) − ̄λ(D(p, 1/n) − ρ) − λ0D(p, 1/n)\n\nn (cid:88)\n\ni=1\n\npili(w) − λ(D(p, 1/n) − ρ) − λ0ρ.\n\nThen the original problem is equivalent to the following problem\n\nmin w∈W\n\nmin λ≥λ0\n\nmax p∈∆n\n\nn (cid:88)\n\ni=1\n\npili(w) − λ(D(p, 1/n) − ρ) − λ0ρ,\n\nNext we fix x = (w⊤, λ)⊤ and derive an optimal solution p∗(x) which depends on x and solves the inner maximization problem. We consider the following problem\n\nmin p∈∆n\n\n−\n\nn (cid:88)\n\ni=1\n\npili(w) + λD(p, 1/n).\n\nwhich has the same optimal solution p∗(x) with our problem. There are three constraints to handle, i.e., pi ≥ 0, ∀i and pi ≤ 1, ∀i and (cid:80)n i=1 pi = 1. Note that the constraint pi ≥ 0 is enforced by the term pi log(pi), otherwise the above objective will become infinity. As a result, the constraint pi < 1 is automatically satisfied due to (cid:80)n i=1 pi = 1 and pi ≥ 0. Hence, we only need to explicitly tackle the constraint (cid:80)n i=1 pi = 1. To this end, we define the\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nfollowing Lagrangian function\n\nLx(p, μ) = −\n\nn (cid:88)\n\n(cid:32)\n\npili(w) + λ\n\nlog n +\n\nn (cid:88)\n\n(cid:33)\n\npi log(pi)\n\nwhere μ is the Lagrangian multiplier for the constraint (cid:80)n the KKT conditions:\n\ni=1\n\ni=1\n\nn (cid:88)\n\n+ μ(\n\ni=1\n\npi − 1),\n\ni=1 pi = 1. The optimal solutions satisfy\n\n− li(w) + λ (log(p∗\n\ni (x)) + 1) + μ = 0 and\n\nn (cid:88)\n\ni=1\n\np∗\n\ni (x) = 1.\n\nFrom the first equation, we can derive p∗ conclude that p∗ problem, we have\n\ni (x) = exp(li(w)/λ)\n\n(cid:80)n\n\ni (x) ∝ exp(li(w)/λ). Due to the second equation, we can i=1 exp(li(w)/λ) . Plugging this optimal p∗(w) into the inner maximization\n\nn (cid:88)\n\n(cid:32)\n\nn (cid:88)\n\np∗\n\ni (x)li(w) − λ\n\nlog n +\n\ni (w) log(p∗ p∗\n\ni (w))\n\n= λ log\n\n(cid:33)\n\n(cid:32)\n\nn (cid:88)\n\nexp\n\n1 n\n\n(cid:18) li(w) λ\n\n(cid:19)(cid:33)\n\n,\n\ni=1\n\ni=1 Therefore, we get the following equivalent problem to the original problem\n\ni=1\n\nmin w∈W\n\nmin λ≥λ0\n\nλ log\n\nwhich is Eq. (2) in the paper.\n\n(cid:32)\n\n(cid:19)(cid:33)\n\n+ λρ.\n\n1 n\n\nn (cid:88)\n\ni=1\n\nexp\n\n(cid:18) li(w) λ\n\n34",
    "reference": "# Summary Of The Paper\n\nThis paper aims to develop new stochastic algorithms for solving the popular KL-divergence-constrained distributionally robust optimization (DRO) problem for both non-convex and convex losses. The proposed method, SCDRO or its variants, establishes (near-)optimal oracle complexities for both convex and non-convex losses. Furthermore, the said complexities are independent of per-iterate sample and batch sizes. Empirical studies further validate the effectiveness of SCDRO variants.\n\n# Strength And Weaknesses\n\nStrength\n\nI find this paper interesting and well-written. To me, the methodology behind SCDRO is highly innovative in deriving a primal-only formulation linked with existing compositional optimization works. The developed theory for SCDOR was designed for non-convex loss functions and further extended to convex loss functions, enjoying optimal complexities for finding $\\epsilon$-accurate solutions for both convex ($O(1/\\epsilon^2)$) and non-convex ($\\tilde{O}(1/\\epsilon^3)$) smooth loss functions $\\ell_i(\\mathbf{w})$. Both of these complexities are first in literature in the given settings.\n\nWeaknesses\n\nOn pp. 3 ending part, the authors essentially indicate that DRO with KL divergence regularization enjoys shown superior performance for distributional shift. End of pp. 1, \"They are either restricted to problems with no additional constraints on the dual variable p except for the simplex constraint (Qi et al., 2021; Jin et al., 2021), ...\" On pp. 13 middle part the authors indicate that Qi et al. (2021) firstly cast the KL-regularized DRO problems into stochastic compositional optimization. They also indicate the difference with this work that they \"solve KL-constrained DRO problems, which is more challenging than KL-regularized DRO problems.\"\n\nAs indicated in the paper, the hardness of their analyses lies on the non-smoothness of $F(\\mathbf{x})$ and the non-Lipschitzness of outer function in the given unbounded regime, which breaks the assumptions of stochastic compositional optimization theory, hindering the achievement of optimal complexities for both convex and non-convex loss functions. I am wondering if the said challenge bring essential new difficulties for this change? From the perspectives of Lagrangian's multiplier these two formulations are essentially the same. Can the authors bring more details on this comparison?\n\nMinor comments:\n\nPage 2, middle part, the authors said $f$ being \"monotone\" which can confuse readers since this work does not involve variational inequalities.\n\nPage 13, middle part, \"(i) we do not need to maintain and update ... (i) we do not need to worry ... \" the second (i) should be (ii).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI praise the authors for good clarity and quality of this work. The SCDRO framework is innovative but I am concerned on the technical novelty on the relationship with earlier works, including Qi et al. (2021).\n\n# Summary Of The Review\n\nAs indicated above, this work enjoys strength of good clarity and quality but suffer from potential weakness of limited technical novelty. My rating is based on this, barring further rebuttals and revisions from the authors.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTHREE PROBLEM CLASSES THAT MARKOV REWARDS CANNOT EXPRESS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nIn this paper, we study the expressivity of Markovian reward functions, and identify several limitations to what they can express. Specifically, we look at three classes of reinforcement learning tasks (multi-objective reinforcement learning, risk-averse reinforcement learning, and modal reinforcement learning), and then prove mathematically that most of the tasks in each of these classes cannot be expressed using scalar, Markovian reward functions. In the process, we provide necessary and sufficient conditions for when a multi-objective reinforcement learning problem can be reduced to ordinary, scalar reward reinforcement learning. We also call attention to a new class of reinforcement learning problems (namely those we call “modal” problems), which have so far not been given any systematic treatment in the reinforcement learning literature. In addition, we also show that many of these problems can be solved effectively using reinforcement learning. This rules out the possibility that those problems which cannot be expressed using Markovian reward functions also are impossible to learn effectively.\n\n1\n\nINTRODUCTION\n\nTo use reinforcement learning (RL) to solve a task, it is necessary to first encode that task using a reward function (Sutton & Barto, 2018). Usually, these reward functions are Markovian functions from state-action-next-state triples to reals. In this paper, we study the expressivity of Markovian reward functions, and identify several limitations to what they can express. Specifically, we will examine three classes of tasks, all of which are both intuitive to understand, and useful in practical situations. We will then show that almost all tasks in each of these three classes are impossible to express using Markovian reward functions. Moreover, we also show that many of these problems can be solved effectively with RL, either by providing references to existing literature, or by providing an outline of a possible approach. This rules out the possibility that those problems which cannot be expressed using Markovian reward functions also are impossible to learn effectively.\n\nThe first class of problems we look at, in Section 2, is the single-policy version of multi-objective RL (MORL). In such a problem, the agent receives multiple reward signals, and the aim is to learn a single policy that achieves an optimal trade-off of those rewards according to some criterion (Roijers et al., 2013; Liu et al., 2015). For example, a single-policy MORL algorithm might attempt to maximise the rewards lexicographically (Skalse et al., 2022b). We will look at the question of which MORL problems can be reduced to ordinary RL, by providing a scalar reward function that induces the same preferences as the original MORL problem. Moreover, we will provide a complete solution to this problem, in the form of necessary and sufficient conditions. We find that this can only be done for MORL problems that correspond to a linear weighting of the rewards, which means that it cannot be done for the vast majority of all interesting MORL problems.\n\nThe next class of problems we look at, in Section 3, is risks-sensitive RL. There are many contexts where it is desirable to be risk averse. In economics, and related fields, this is often modelled using utility functions U : R → R which are concave in some underlying quantity. Can the same thing be done with reward functions? Is it possible to take a reward function, and then create a version of that reward function which induces more risk-averse behaviour? We show that the answer is no – none of the standard risk-averse utility functions can be expressed using reward functions. This demonstrates another limitation in the expressive power of Markovian rewards.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nThe last class of problems we look at, in Section 4, is something we call modal tasks. These are tasks where the agent is evaluated not only based on what trajectories it generates, but also based on what it could have done along those trajectories. For example, consider the instruction “you should always be able to return to the start state”. We provide a formalisation of such tasks, argue that there are many situations in which these tasks could be useful, and finally prove that these tasks also typically cannot be formalised using ordinary reward functions.\n\nIn Section 5, we discuss how to solve tasks from each of these classes using RL. We provide references to existing literature, and then sketch both an approach for learning a wide class of MORL problems, and an approach for learning a wide class of modal problems. Finally, in Section 6, we discuss the significance and limitations of our results, together with ways to extend them.\n\n1.1 RELATED WORK\n\nThere has been a few recent papers which examine the expressivity of Markovian reward functions. The first of these is the work by Abel et al. (2021), who point to three different ways to formalise the notion of a “task” (namely, as a set of acceptable policies, as an ordering over policies, or as an ordering over trajectories). They then demonstrate that each of these classes contains at least one instance which cannot be expressed using a reward function (by using the fact that the set of all optimal policies forms a convex set, and the fact that the reward function is Markovian). They also provide algorithms which compute reward functions for these types of tasks, by constructing a linear program. We greatly extend their work by providing new results that are significantly stronger.\n\nAnother important paper is the work by Vamplew et al. (2022), who argue that there are many important aspects of intelligence which can be captured by MORL, but not by scalar RL. Like them, we also argue that MORL is a genuine extension of scalar RL, but our approach is quite different. They focus on the question of whether MORL or (scalar) RL is a better foundation for the development of general intelligence (considering feasibility, safety, and etc), and they provide qualitative arguments and biological evidence. By contrast, we are more narrowly focused on what incentive structures can be expressed by MORL and scalar RL, and our results are mathematical.\n\nThere is also other relevant work that is less strongly related. For example, Icarte et al. (2022) point out that there are certain tasks which cannot be expressed using Markovian rewards, and propose a way extend their expressivity by augmenting the reward function with an automaton that they call a reward machine. Similar approaches have also been used by e.g. Hasanbeig et al. (2020); Hammond et al. (2021). There are also other ways to extend Markovian rewards to a more general setting, such as convex RL, as studied by e.g. Hazan et al. (2019); Zhang et al. (2020); Zahavy et al. (2021); Geist et al. (2022); Mutti et al. (2022), and vectorial RL, as studied by e.g. Cheung (2019a;b). Also related is the work by Skalse et al. (2022c), who show that there are certain relationships that are never satisfied by any pair of reward functions. This paper can also be seen as relating to earlier work on characterising what kinds of preference structures can be expressed using utility functions, such as the famous work by von Neumann & Morgenstern (1947), and other work in game theory.\n\nThere is a large literature on (the overlapping topics of) single-policy MORL, constrained RL, and risk-sensitive RL. Some notable examples of this work includes Achiam et al. (2017); Chow et al. (2017); Miryoosefi et al. (2019); Tessler et al. (2019); Skalse et al. (2022b). This existing literature typically focuses on the creation of algorithms for solving particular MORL problems, and has so far not tackled the problem of characterising when MORL problems can be reduced to scalar RL. Modal RL has (to the best of our knowledge) never been discussed explicitly in the literature before. However, it relates to some existing work, such as side-effect avoidance (Krakovna et al., 2018; 2020; Turner et al., 2020), and the work by Wang et al. (2020).\n\n1.2 PRELIMINARIES\n\nThe standard RL setting is formalised using Markov Decision Processes (MDPs), which are tuples ⟨S, A, τ, μ0, R, γ⟩ where S is a set of states, A is a set of actions, τ : S × A ⇝ S is a transition function, μ0 is an initial state distribution over S, R : S × A × S ⇝ R a reward function, where R(s, a, s′) is the reward obtained if the agent moves from state s to s′ by taking action a, and γ ∈ (0, 1) is a discount factor. Here, f : X ⇝ Y denotes a probabilistic mapping f from X to Y . A state is terminal if τ (s, a) = s and R(s, a, s) = 0 for all a. A trajectory ξ is a path s0, a0, s1 . . . in an\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nMDP that is possible according to μ0 and τ . We use G to denote the trajectory return function, where G(ξ) = (cid:80)∞ t=0 γtrt. A policy is a mapping π : S ⇝ A, and Π is the set of all policies. Given a policy π, its value function V π : S → R is the function where V π(s) is the expected future discounted reward when following π from s, and its Q-function Qπ : S × A → R = ES′∼τ (s,a)[R(s, a, S′) + γ · V π(S′)]. The policy evaluation function J : Π → R is J(π) = ES0∼μ0 [V π(So)]. If a policy maximises J, then we say that this policy is optimal. We denote optimal policies by π⋆, and their value function and Q-function by V ⋆ and Q⋆. Moreover, given an MDP M, we say that M’s policy order is the ordering ≺ on Π induced by π1 ≺ π2 ⇐⇒ J(π1) < J(π2) for all π1, π2. For a more comprehensive overview, see Sutton & Barto (2018).\n\nIn this paper, we will say that a reward function R is trivial if J(π1) = J(π2) for all π1, π2. Moreover, we say that R1 and R2 are equivalent if J1(π1) < J1(π2) ⇐⇒ J2(π1) < J2(π2) for all π1, π2, and that they are opposites if J1(π1) < J1(π2) ⇐⇒ J2(π1) > J2(π2) for all π1, π2.\n\nMORL problems are formalised using Multi-Objective MDPs (MOMDPs), which are tuples ⟨S, A, τ, μ0, ⃗R, γ⟩. The only place where MOMDPs differ from MDPs are ⃗R, which is a function ⃗R : S × A × S ⇝ Rk that, for each transition s, a, s′, returns k different rewards (for some k). We denote the reward function that returns the i’th component of ⃗R as Ri, and use V π i , Ji, Gi, etc, to refer to its value functions, Q-functions, evaluation function, return function, etc. Since there may not be any single policy which maximises each component of ⃗R, a MORL problem additionally needs a rule for how to combine and trade off each reward.\n\ni , Qπ\n\n1.3 A REMARK ON “TASKS”\n\nIn order to determine if a given task can be expressed by Markovian reward functions, we must first determine what it means for a reward function to express a task. One answer to this question is to say that a task corresponds to a desired policy π, and that a reward function R expresses the task if π is optimal under R (possibly with the additional requirement that π is the only policy that is optimal under R). With this definition, we find that any task can be expressed as a Markovian reward function, at least as long as π is stationary and deterministic (see Appendix B).\n\nAnother possible definition is to say that a task corresponds to an ordering ≺ on Π, which encodes a preference ordering over all policies, and that a reward function R expresses the task if J orders Π according to ≺. It is primarily this latter definition that we will use in this paper. The main reason for this is that it often is impossible to find the optimal policy in complex environments. This means that it is not enough for R to have the right optimal policy; it must also induce the right preferences between the (sub-optimal) policies that the policy optimisation algorithm actually considers. The only way to robustly ensure that this is the case is if R induces the right policy ordering.\n\nThese are not the only two reasonable definitions. As mentioned previously, more definitions can be found in Abel et al. (2021).\n\n2 MULTI-OBJECTIVE REINFORCEMENT LEARNING\n\nIn this section, we examine the MORL setting. We first need a general definition of what a singlepolicy MORL problem is. Recall that a MOMDP ⟨S, A, τ, μ0, ⃗R, γ⟩ by itself has no one canonical objective to maximise. We therefore introduce the notion of a MORL objective: Definition 1. A MORL objective over k rewards is a function O that takes k policy evaluation functions J1 . . . Jk and returns a (total) ordering ≺O over the set of all policies Π.\n\nGiven a MOMDP M = ⟨S, A, τ, μ0, ⃗R, γ⟩, a MORL objective O gives us an ordering over Π that tells us when a policy is preferred over another. We use ≺M O to denote the policy ordering that is obtained when we apply O to M’s policy evaluation functions. For the purposes of this paper, we will not need to impose any further requirements on ≺O. For example, we will not insist that ≺O must have a greatest element in Π, or that π1 ≺O π2 whenever π2 is a Pareto improvement over π1, etc, even though a reasonable MORL objective presumably would have these properties. We next give a few examples of some interesting MORL objectives: Definition 2. Given J1 . . . Jk, the LexMax objective ≺Lex is given by π1 ≺Lex π2 if and only if there is an i ∈ {1 . . . m} such that Ji(π1) < Ji(π2), and Jj(π1) = Jj(π2) for j < i.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nDefinition 3. Given J1 . . . Jk, the MaxMin objective ≺Min is given by π1 ≺Min π2 ⇐⇒ mini Ji(π1) < mini Ji(π2). Definition 4. Given J1 . . . Jk and some c1 . . . cm ∈ R, the MaxSat objective ≺Sat is given by π1 ≺Sat π2 if and only if the number of rewards that satisfy Ji(π1) ≥ ci is larger than the number of rewards that satisfy Ji(π2) ≥ ci. Definition 5. Given J1, J2 and some c ∈ R, the ConSat objective ≺Con is given by π1 ≺Con π2 if and only if either J1(π1) < c and J1(π1) < J1(π2), or if J1(π1), J1(π2) ≥ c and J2(π1) < J2(π2).\n\nIn other words, the LexMax objective has lexicographic preferences over R1 . . . Rm, so that policies are first ordered by their expected discounted R1-reward, and then policies that obtain the same expected discounted R1-reward are ordered by their expected discounted R2-reward, and so on. The MaxMin objective orders policies by their worst performance according to any of R1 . . . Rm (which could be used to obtain worst-case guarantees). The MaxSat objective only cares about whether a policy reaches a certain threshold for each reward, and ranks policies based on how many thresholds they reach. The ConSat objective wants to maximise J2, but under the constraint that J1 reaches a certain threshold. These MORL objectives are simply a short list of illustrative examples, demonstrating the flexibility of the framework. A few more examples are given in Appendix D. We next need to define what it means to reduce a MORL problem to a (scalar) RL problem: Definition 6. A MOMDP M = ⟨S, A, τ, μ0, ⃗R, γ⟩ with objective O is equivalent to the MDP ̃M = ⟨S, A, τ, μ0, ̃R, γ⟩ if and only if ̃M ’s policy order is ≺M O .\n\nNote that ̃M must have the same states, actions, transition function, initial state distribution, and discount factor, as M. This definition therefore says that M with O is equivalent to ̃M if ̃M is given by replacing ⃗R = ⟨R1 . . . Rk⟩ with a single reward function ̃R, and ̃R induces the same preferences between all policies as O(J1 . . . Jk). We can now derive necessary and sufficient conditions for when a MORL problem can be reduced to a scalar-reward RL problem. Theorem 1. If a MOMDP M = ⟨S, A, τ, μ0, ⃗R, γ⟩ with objective O is equivalent to an MDP ̃M = ⟨S, A, τ, μ0, ̃R, γ⟩, then ̃J(π) = (cid:80)k i=1 wi · Ji(π) for some w1 . . . wk ∈ R. Moreover, M with O is also equivalent to the MDP with reward R(s, a, s′) = (cid:80)k\n\ni=1 wi · Ri(s, a, s′).\n\nProof. Suppose M with O is equivalent to an MDP ̃M = ⟨S, A, τ, μ0, ̃R, γ⟩. First, let m : Π → R|S||A| be the function that maps each policy π to the |S||A|-dimensional vector where\n\nm(π)[s, a] =\n\n∞ (cid:88)\n\nt=0\n\nγtPξ∼π(St = s, At = a).\n\nMoreover, for a reward function R, let ⃗R ∈ R|S||A| be the |S||A|-dimensional vector where\n\n⃗R[s, a] = ES′∼τ (s,a)[R(s, a, S′)]. Note that we now have that J(π) = m(π) · ⃗R, for any reward function R. Recall also that multiplication by an |S||A|-dimensional vector induces a linear function over R|S||A|. This means that, for any reward function R, we can express its policy evaluation function J : Π → R as L ◦ m, where L is a linear function. In particular, ̃J = ̃L ◦ m, and Ji = Li ◦ m for each of Ri ∈ ⃗R. From the definition of MORL objectives, we have that ̃J(π) is a function of J1(π) . . . Jk(π). This, in turn, means that ̃L(v) is a function of L1(v) . . . Lk(v), for any v ∈ Im(m). Let M be the (|S||A| × k)-dimensional matrix that maps each vector v ∈ R|S||A| to ⟨L1(v), . . . , Lk(v)⟩ (in other words, the matrix whose rows are ⃗R1 . . . ⃗Rk). Since ̃L(v) is a function of L1(v) . . . Lk(v), we have that ̃L can be expressed as f ◦ M for some function f . Since ̃L is a linear function, and since M is a linear transformation, we that f must be a linear function as well. This means that there are w1 . . . wk ∈ Rk such that f (x) = (cid:80)k i=1 wi · Li(v), and further that ̃J(π) = (cid:80)k\n\ni=1 wi · xi, which implies that ̃L(v) = (cid:80)m\n\ni=1 wi · Ji(π). This completes the first part.\n\nNext, let R(s, a, s′) = (cid:80) i1 i=1 wi · Ji(π). Now, since J = ̃J, and since M with O is equivalent to ̃M, we have that M with O is equivalent to the MDP with reward R. This completes the second part.\n\nkwi · Ri(s, a, s′). Straightforward algebra shows that J(π) = (cid:80)k\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nThis theorem effectively tells us that only linear MORL objectives can be represented using scalarreward RL! This imposes a harsh limitation on what kinds of tasks can be encoded using scalar rewards. Theorem 1 also has the following corollary, which is useful for demonstrating when some MORL objective cannot be expressed using scalar reward functions. Given an ordering ≺ over Π dependent on some evaluation functions J1 . . . Jk, we say that a function U : Π → R represents ≺ if U (π1) < U (π2) ⇐⇒ π1 ≺ π2. We say that U is a linear representation if U (π) = f ((cid:80)k Corollary 1. If O(J1 . . . Jk) has a non-linear representation U , and M is a MOMDP whose Jfunctions are J1 . . . Jk, then M with O is not equivalent to any MDP.\n\ni=1 wi · Ji(π)) for some w1 . . . wk ∈ R and some f that is strictly monotonic.\n\nProof. Assume for contradiction that M with O is equivalent the MDP ̃M = ⟨S, A, τ, μ0, ̃R, γ⟩. Then ̃J represents O(J1 . . . Jk), and this in turn means that U must be strictly monotonic in ̃J. Moreover, Theorem 1 implies that ̃J = (cid:80)k i=0 wi · Ji for some w1 . . . wk ∈ Rk. However, this contradicts our assumptions.\n\nTherefore, we can prove that M with O is not equivalent to any MDP by finding a non-linear representation of ≺M O . We will now show that none of the MORL objectives given in Definition 2-5 can be expressed using single-objective RL, except in a few degenerate edge cases.\n\nTheorem 2. There is no MDP equivalent to M with LexMax, as long as M has at least two reward functions that are neither trivial, equivalent, or opposites.\n\nProof. Suppose M with LexMax is equivalent to ̃M = ⟨S, A, τ, μ0, ̃R, γ⟩. Let i be the smallest number such that Ri is non-trivial, and let j be the smallest number greater than i such that Rj is non-trivial, and not equivalent to or opposite of Ri. Then there are π1, π2 such that Ji(π1) = Ji(π2) and Jj(π1) < Jj(π2), which means that π1 ≺M Lex, it follows that there are no π, π′ such that Ji(π) < Ji(π′) and ̃J(π) > ̃J(π′). Then Theorem 1 in Skalse et al. (2022c) implies that Ri is equivalent to ̃R. However, then ̃J(π1) = ̃J(π2), which means that ̃J cannot represent ≺M\n\nLex π2. Moreover, since ̃J represents ≺M\n\nLex.\n\nTheorem 3. There is no MDP equivalent to M with MaxMin, unless M has a reward function Ri such that Ji(π) ≤ Jj(π) for all j ∈ {1 . . . k} and all π.\n\nProof. OM Min is represented by the function U (π) = miniJi(π). Moreover, if M has no reward function Ri such that Ji(π) ≤ Jj(π) for all j ∈ {1 . . . k} and all π then this representation is non-linear. Corollary 1 then implies that M with MaxMin is not equivalent to any MDP.\n\nTheorem 4. There is no MDP equivalent to M with MaxSat, as long as M has at least one reward Ri where Ji(π1) < ci and Ji(π2) ≥ ci for some π1, π2 ∈ Π.\n\nProof. Note that MaxSat(M) is represented by the function U (π) = (cid:80)k 1[Ji(π) ≥ ci], where 1[Ji(π) ≥ ci] is the function that is equal to 1 when Ji(π) ≥ ci, and 0 otherwise. Moreover, U is not strictly monotonic in any function that is linear in J1 . . . Jk. Corollary 1 thus implies that M with MaxSat is not equivalent to any MDP.\n\ni=1\n\nTheorem 5. There is no MDP equivalent to M with ConSat, unless either R1 and R2 are equivalent, or maxπ J1(π) ≤ c.\n\nProof. OM Con is represented by U (π) = {J1(π) if J1(π) ≤ c, else J2(π) − minπ J2(π) + c}. Moreover, this representation is non-linear, unless either R1 and R2 are equivalent, or maxπ J1(π) ≤ c. Corollary 1 then implies that M with ConSat is not equivalent to any MDP.\n\nTheorem 2-5 show that none of the MORL objectives given in Definition 2-5 can be expressed using single-objective RL, except in a few degenerate cases where those MORL objectives are uninteresting. This demonstrates that there is no satisfactory way to reduce MORL problems to scalar-reward RL (and hence that scalar RL is unable to express many natural task specifications).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3 RISK-SENSITIVE REINFORCEMENT LEARNING\n\nThe next area we will look at is that of risk-sensitive reinforcement learning. An ordinary RL agent tries to maximise the expectation of its reward function. However, there are many cases where it is natural to want the agent to be risk-averse. In economics, risk-aversion is typically modelled by using utility functions U (c) that are concave in some relevant quantity c (which might be money, for example). A natural question is then whether a similar trick may be used with reward functions? That is, given a reward function R1 and a concave function f , can we construct a reward function R2 such that G2(ξ) = f (G1(ξ)) for all trajectories ξ? We will examine this question.\n\nSome of the most common risk-averse utility functions includes exponential utility, isoelastic utility, and quadratic utility. The exponential utility function is given by U (c) = −eαc, where α > 0 is a parameter controlling the degree of risk aversion. The isoelastic utility function is given by U (c) = c1−α, for α > 0, α ̸= 1, or by U (c) = ln(c) (corresponding to the case when α = 1). The quadratic utility function is given by U (c) = c−αc2, where α > 0. Since this function is decreasing for sufficiently large c, its domain is typically restricted to (−∞, 1/2α]. We will examine each of these, and show that none of them can be expressed using reward functions.\n\nIn this section, we will consider the domain of G to be the set of all coherent trajectories, not the set of trajectories which are possible under some transition function τ . In other words, we consider the set of all trajectories to be (S × A)ω. The reason for this is that we do not want to presume any prior knowledge of the environment. If we restrict the set of trajectories we consider, then some risk-averse utility functions can become possible to express (consider the case of a tree-shaped MDP, for example). Finally, we will say that R is constant if it has a constant value for all s, a, s′.\n\nTo prove our results, we will make use of three lemmas. The proofs of these lemmas are fairly long, but not very illuminating, and so we have relegated them to Appendix A.\n\nLemma 1. If R is non-constant, then for any state s there exists trajectories ζ1, ζ2, ζ3 starting in s such that G(ζ1) ̸= G(ζ2), G(ζ2) ̸= G(ζ3), and G(ζ1) ̸= G(ζ3). Lemma 2. If G2(ξ) = f (G1(ξ)) for all ξ and some f , then for any transition ⟨s, a, s′⟩ and any trajectory ζ starting in s′, R2(s, a, s′) = f (R1(s, a, s′) + γG1(ζ)) − γf (G1(ζ)).\n\nLemma 3. For any non-constant reward R1 and any f that is injective on range(G1), if for any y ∈ range(R1) and any γ ∈ (0, 1) there are at most two distinct x1, x2 such that f (y + γx1) − γf (x1) = f (y + γx2) − γf (x2) then there is no reward R2 such that G2(ξ) = f (G1(ξ)) for all ξ.\n\nUsing these lemmas, we can now derive our main results:\n\nTheorem 6. For any non-constant reward function R1 and any constant α ̸= 0, there is no reward function R2 such that G2(ξ) = −eαG1(ξ) for all valid trajectories ξ.\n\nProof. With f (x) = −eαx, the expression in Lemma 3 becomes −eα(y+γx) + γeαx. The derivative of this expression with respect to x is γα(−eα(y+γx) + eαx), which has only one root when γ ̸= 0 and α ̸= 0. This means that there can be at most two distinct values x1, x2 such that −eα(y+γx1) + γeαx1 = −eα(y+γx2) + γeαx2. Since −eαx is injective, we can thus apply Lemma 3, which completes the proof.\n\nTheorem 7. For any non-constant reward function R1 and any constant α > 0, α ̸= 1, there is no reward function R2 such that G2(ξ) = G1(ξ)1−α for all valid trajectories ξ.\n\nProof. With f (x) = x1−α, the expression in Lemma 3 becomes (y + γx)(1−α) − γx1−α. The derivative of this expression with respect to x is γ(α − 1)(x−α − (γx + y)−α), which has only one root when γ ̸= 0 and α ̸∈ {0, 1}. This means that there can be at most two distinct values x1, x2 such that (y + γx1)(1−α) − γx1−α . Since x1−α is injective, we can thus apply Lemma 3, which completes the proof.\n\n= (y + γx2)(1−α) − γx1−α\n\n1\n\n2\n\nTheorem 8. For any non-constant reward function R1, there is no reward function R2 such that G2(ξ) = ln(G1(ξ)) for all valid trajectories ξ.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nProof. With f (x) = ln(x), the expression in Lemma 3 becomes ln(y+γx)−γ ln(x). The derivative of this expression with respect to x is γ(1/(y + γx) − 1/x), which has only one root when γ ̸= 0. Since ln(x) is injective, we can thus apply Lemma 3, which completes the proof.\n\nTheorem 9. For any non-constant reward function R1 and any α > 0 where maxξ G1(ξ) ≤ 1 there is no reward function R2 such that G2(ξ) = G1(ξ) − αG1(ξ)2 for all ξ.\n\n2α ,\n\nProof. With f (x) = x − αx2, the expression in Lemma 3 becomes y + γx − α(y + γx)2. This is a second-degree polynomial, which means that there can be at most two distinct values x1, x2 such that y + γx1 − α(y + γx1)2 = y + γx2 − α(y + γx2)2. Moreover, if maxξ G1(ξ) ≤ 1 2α then f (x) = x − αx2 is injective on range(G1). We can thus apply Lemma 3.\n\nWe can thus see that Lemma 3 is quite flexible. It allows us to rule out many modifications to G as impossible, including all the standard risk-averse utility functions. It would be desirable to strengthen these results, and provide necessary and sufficient conditions for when it is possible to construct a reward R2 such that G2(ξ) = f (G1(ξ)) for some function f and some (non-constant) reward R1. We consider this to be an important question for further work.\n\n4 MODAL REINFORCEMENT LEARNING\n\nThe final class of tasks we will examine is one which we have decided to refer to as modal tasks. Before we give a formal definition of this class, we will first provide some intuition. In analytic philosophy, a distinction is made between categorical facts and modal facts. In short, categorical facts only concern what is true in actuality, whereas modal facts concern what must be true, could have been true, or cannot be true, etc. For example, it is a categorical fact that the Eiffel Tower is brown, and a modal fact that it could have had a different colour. It is (arguably) a categorical fact that the number 3 is prime, and a modal fact that it could not have been otherwise. To give another example, there is a difference between stating that nothing can travel faster than light and that nothing does travel faster than light – the former statement, which is modal, is stronger than the latter, which is categorical. One can further distinguish between different kinds of possibility (e.g. logical vs physical possibility, etc), and discussions about modality also involves topics such as causality and counterfactuals, etc. A complete treatment of this subject is far beyond the scope of this paper, but for an overview, see e.g. Menzel (2021).\n\nModality does of course relate to modal logic, but it also relates to temporal logic. In particular, computational tree logic (CTL), and its extensions, can express many modal statements.\n\nThe intuition behind this section is that a reward function always is expressed in terms of categorical facts, whereas many tasks are naturally expressed in terms of modal facts. For example, consider an instruction such as “you should always be able to return to the start state”. This instruction seems quite reasonable, but it is not obvious how to translate it into a reward function. Note that this instruction is not telling the agent to actually return to the start state, it merely says that it should maintain the ability to do so. To give a few other examples, consider instructions such as “you should never enter a state from which it is possible to quickly enter an unsafe state”, “you should always be able to press the emergency shutdown button”, or “you should never enter a state where you would be unable to receive a feedback signal”. These instructions all seem very reasonable, and they are expressed in terms of what should be possible or impossible along the trajectory of the agent, rather than in terms of what in fact occurs along that trajectory. Given this background motivation, we can now give a formal definition of modal tasks: Definition 7. Given a set of states S and a set of actions A, a modal reward function R♢ is a function R♢ : S × A × S × (S × A ⇝ S) → R which takes two states s, s′ ∈ S, an action a ∈ A, and a transition function τ over S and A, and returns a real number.\n\nR♢(s, a, s′, τ ) is the reward that is obtained when transitioning from state s to s′ using action a in an environment whose transition function is τ . Here we allow R♢ an unrestricted dependence on τ , to make our results as general as possible, even if a practical algorithm for solving modal tasks presumably would require restrictions on what this dependence can look like (see Appendix E). Modal reward functions can be used to express instructions such as those we gave above. For example, a simple case might be “you get 1 reward if you reach this goal state, and -1 reward if\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nyou ever enter a state from which you cannot reach the initial state”. This reward depends on the transition function, because the transition function determines from which states you can reach the initial state. As usual, R♢ then induces a Q-function Q♢, value function V ♢, and evaluation function J ♢, etc. We say that a modal reward R♢ and an ordinary reward R are contingently equivalent given a transition function τ if J ♢ and J induce the same ordering of policies given τ , and that they are robustly equivalent if J ♢ and J induce the same ordering of policies for all τ . We use R♢ τ to denote the reward function R♢ Definition 8. A modal reward function R♢ is trivial if there is a reward function R such that for all τ , R and R♢\n\nτ (s, a, s′) = R♢(s, a, s′, τ ). We will also use the following definition:\n\nτ have the same policy ordering under τ .\n\nτ is a scaled version of R, or that R♢\n\nThe intuition here is that a trivial modal reward function does not actually depends on τ in any important sense. Note that this is not necessarily to say that R♢ τ = R for all τ . For example, it could be the case that R♢ τ and R differ by potential shaping Ng et al. (1999), or that R♢ τ (s, a, S′)] = ES′∼τ (s,a)[R(s, a, S′)], since none of these differences affect the policy ordering. Theorem 10. For any modal reward R♢ and any transition function τ , there exists a reward function R that is contingently equivalent to R♢ given τ . Moreover, unless R♢ is trivial, there is no reward function that is robustly equivalent to R♢.\n\nτ is modified in a way such that ES′∼τ (s,a)[R♢\n\nProof. This is straightforward. For the first part, simply let R(s, a, s′) = R♢(s, a, s′, τ ). The second part is immediate from the definition of trivial modal reward functions.\n\nIn other words, every modal task can be expressed with ordinary reward function in each particular environment, but no reward function expresses a (non-trivial) modal task in all environments. Is this enough? We argue that it is not, because the construction of R♢ τ will invariably be laborious, and require detailed knowledge of the environment. For example, consider the task “you should always be able to return to the start state”; here, constructing R♢ τ would amount to manually enumerating all the states from which the start state is reachable. This is very much against the spirit of reinforcement learning, where much of the point is that we want to be able to specify tasks which can be pursued in unknown environments. In short, a method which requires a model of the environment is arguably not a reinforcement learning method. We thus argue that reward functions are unable to capture modal tasks in a satisfactory way.\n\nOne remaining question might be why one would want to express instructions for reinforcement learning agents in terms of modal properties. After all, what benefit is there to the instruction “never enter a state from which it is possible to quickly enter an unsafe state” over the instruction “never enter an unsafe state”? One reason is that the former task might lead to behaviour that is more robust to changes in the environment. For example, if an RL agent is trained in a simulated environment, and deployed in the real world, then it seems like it would be preferable to tell the agent to avoid risky states, rather than unsafe states, since imperfections in the simulation could lead to an underestimation of the risk involved. Another example is the existing work on avoiding side effects (Krakovna et al., 2018; 2020; Turner et al., 2020), which it is natural to express in modal terms. This work can be viewed as being aimed at making the behaviour of an RL agent more robust to misspecification of the reward function.\n\n5 SOLVING “INEXPRESSIBLE” TASKS\n\nWe have pointed to three classes of tasks which cannot be expressed using reward functions (namely multi-objective tasks, risk-sensitive tasks, and modal tasks). A natural next question is whether these tasks could be solved using RL, or whether only the tasks which correspond to Markovian reward functions can be effectively learnt? We discuss this issue below.\n\nIn short, it is possible to design RL algorithms for tasks in each of these categories. Multi-objective reinforcement learning is well-explored, with many existing algorithms (see Section 1.1). Most of these algorithms are designed to solve a specific MORL objective; for example, Skalse et al. (2022b) solve the LexMax objective, and Tessler et al. (2019) solve the ConSat objective. There is (to the best of our knowledge) not yet any algorithm for the solving e.g. the MaxMin objective, but there is no good reason to believe that such an algorithm could not be made. Similarly, there are existing\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nalgorithms for risk-sensitive RL (e.g. Chow et al. (2017)), and even algorithms that solve certain modal tasks (Krakovna et al., 2018; 2020; Turner et al., 2020; Wang et al., 2020).\n\nIt should also be possible to design algorithms which can flexibly solve many different tasks from the classes we have discussed (instead of having to be designed for just one particular task). For example, suppose a MORL objective can be represented by a function U : Rk → R, such that π1 ≺ π2 when U (J1(π1) . . . Jk(π1)) < U (J1(π2) . . . Jk(π2)), and that U is differentiable. We give a few examples of such objectives in Appendix D, including e.g. a “soft” version of MaxMin. With such an objective, if we have a policy π that is differentiable with respect to some parameters θ, then it should be possible to compute the gradient of U (J1(π) . . . Jk(π)) with respect to θ, and then use a policy gradient method to increase U . This means that it should be possible to design an actor-critic algorithm which can solve any differentiable MORL objective. We consider the development and evaluation of such methods to be a promising direction for further work.\n\nWe outline a possible approach for solving a wide class of modal tasks in Appendix E.\n\n6 DISCUSSION\n\nIn this paper, we have studied the ability of Markovian reward functions to express different kinds of problems. We have looked at three classes of tasks; multi-objective tasks, risk-sensitive tasks, and modal tasks, and found that Markovian reward functions are unable to express most of the tasks in each of these three classes. We have also provided necessary and sufficient conditions for when a single-policy MORL problem can be expressed using a single reward function (which, as it turns out, is almost never), and also drawn attention to a class of tasks which have just barely been explored previously (namely modal tasks). Finally, we have also shown that many of these problems still can be solved with RL, and even outlined some methods for how to extend these solutions.\n\nThere are several ways to extend our work. First of all, while we have given many examples of tasks which cannot be formalised using Markovian reward functions, we have not given a general characterisation of what reward functions are or are not able to express. It would be very desirable to have a set of intuitive necessary and sufficient conditions, which exactly describe those policy orderings that can be expressed using reward functions, similar to what the VNM axioms provide in the case of utility functions. We outline some initial steps towards such a characterisation in Appendix B. Note that the VNM axioms themselves cannot be directly applied to RL, see Appendix C. Additionally, it would also be desirable to provide necessary and sufficient conditions for when it is possible to construct a reward R2 such that G2(ξ) = f (G1(ξ)) for some function f and some (non-constant) reward R1, as we discussed at the end of Section 3.\n\nOur work also provides a strong motivation for developing more RL algorithms that can learn tasks which cannot be expressed using Markovian reward functions. There are several ways to to this. In section 5, we outline an approach for learning any differentiable MORL objective using policy gradients, and in Appendix E, we outline an approach for learning a large class of modal tasks. It would also be very interesting to explore more general ways to express RL tasks, and study their expressivity. For example, it would be interesting to know if (and to what extent) MORL tasks can be expressed using reward machines (Icarte et al., 2022), and similar.\n\nREFERENCES\n\nDavid Abel, Will Dabney, Anna Harutyunyan, Mark K. Ho, Michael L. Littman, Doina Precup, and Satinder Singh. On the expressivity of markov reward, 2021. URL https://arxiv.org/ abs/2111.00876.\n\nJoshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp. 22–31. JMLR.org, 2017.\n\nWang Chi Cheung. Exploration-exploitation trade-off in reinforcement learning on online markov decision processes with global concave rewards, 2019a. URL https://arxiv.org/abs/ 1905.06466.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nWang Chi Cheung. Regret minimization for reinforcement learning with vectorial feedback and complex objectives. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ́e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b. URL https://proceedings.neurips.cc/paper/2019/file/ a02ffd91ece5e7efeb46db8f10a74059-Paper.pdf.\n\nYinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research, 18(1): 6070–6120, 2017.\n\nMatthieu Geist, Julien P ́erolat, Mathieu Lauri`ere, Romuald Elie, Sarah Perrin, Oliver Bachem, R ́emi Munos, and Olivier Pietquin. Concave utility reinforcement learning: The mean-field game viewIn Proceedings of the 21st International Conference on Autonomous Agents and Mulpoint. tiagent Systems, AAMAS ’22, pp. 489–497, Richland, SC, 2022. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450392136.\n\nL. Hammond, A. Abate, J. Gutierrez, and M. Wooldridge. Multi-agent reinforcement learning with temporal logic specifications. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 583–592, 2021.\n\nM. Hasanbeig, D. Kroening, and A. Abate. Deep reinforcement learning with temporal logics. In\n\nProceedings of FORMATS, LNCS 12288, pp. 1–22, 2020.\n\nElad Hazan, Sham M. Kakade, Karan Singh, and Abby van Soest. Provably efficient maximum entropy exploration. In 36th International Conference on Machine Learning, ICML 2019, 36th International Conference on Machine Learning, ICML 2019, pp. 4774–4786. International Machine Learning Society (IMLS), January 2019. 36th International Conference on Machine Learning, ICML 2019 ; Conference date: 09-06-2019 Through 15-06-2019.\n\nRodrigo Toro Icarte, Toryn Q. Klassen, Richard Valenzano, and Sheila A. McIlraith. Reward machines: Exploiting reward function structure in reinforcement learning. Journal of Artificial Intelligence Research, 73:173–208, jan 2022. doi: 10.1613/jair.1.12440.\n\nVictoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, and Shane Legg. Penalizing side effects using stepwise relative reachability, 2018. URL https://arxiv.org/abs/ 1806.01186.\n\nVictoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, and Shane Legg. Avoiding side effects by considering future tasks, 2020. URL https://arxiv.org/abs/2010.07877.\n\nC. Liu, X. Xu, and D. Hu. Multiobjective reinforcement learning: A comprehensive overview. IEEE\n\nTransactions on Systems, Man, and Cybernetics: Systems, 45(3):385–398, 2015.\n\nChristopher Menzel. Possible Worlds.\n\nIn Edward N. Zalta (ed.), The Stanford Encyclopedia of\n\nPhilosophy. Metaphysics Research Lab, Stanford University, Fall 2021 edition, 2021.\n\nSobhan Miryoosefi, Kiant ́e Brantley, Hal Daum ́e III, Miroslav Dud ́ık, and Robert E. Schapire. Reinforcement learning with convex constraints. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 14070–14079, 2019.\n\nMirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, and Marcello Restelli. Challenging common assumptions in convex reinforcement learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, 2022.\n\nAndrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proceedings of the Sixteenth International Conference on Machine Learning, pp. 278–287, Bled, Slovenia, 1999. Morgan Kaufmann Publishers Inc.\n\nD. M. Roijers, P. Vamplew, S. Whiteson, and R. Dazeley. A survey of multi-objective sequential decision-making. Journal of Artificial Intelligence Research, 48:67–113, 10 2013. ISSN 10769757. doi: 10.1613/jair.3987. URL http://dx.doi.org/10.1613/jair.3987.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nSatinder Singh, Tommi Jaakkola, Michael L. Littman, and Csaba Szepesv ́ari. Convergence results for single-step on-policy reinforcement-learning algorithms. Machine Learning, 38:287–308, 2000.\n\nJoar Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro Abate, and Adam Gleave. Invariance in policy optimisation and partial identifiability in reward learning, 2022a. URL https://arxiv.org/abs/2203.07475.\n\nJoar Skalse, Lewis Hammond, Charlie Griffin, and Alessandro Abate. Lexicographic multi-objective reinforcement learning. In Lud De Raedt (ed.), Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pp. 3430–3436. International Joint Conferences on Artificial Intelligence Organization, 7 2022b. doi: 10.24963/ijcai.2022/476. URL https: //doi.org/10.24963/ijcai.2022/476. Main Track.\n\nJoar Skalse, Niki Howe, Krasheninnikov Dima, and David Krueger. Defining and characterizing In Proceedings of the 33rd International Conference on Neural Information\n\nreward hacking. Processing Systems, 2022c.\n\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nChen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In\n\nProceedings of the 7th International Conference on Learning Representations, 2019.\n\nAlex Turner, Neale Ratzlaff, and Prasad Tadepalli. Avoiding side effects in complex enviIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Adronments. vances in Neural Information Processing Systems, volume 33, pp. 21406–21415. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ f50a6c02a3fc5a3a5d4d9391f05f3efc-Paper.pdf.\n\nPeter Vamplew, Benjamin J. Smith, Johan K ̈allstr ̈om, Gabriel Ramos, Roxana R ̆adulescu, Diederik M. Roijers, Conor F. Hayes, Fredrik Heintz, Patrick Mannion, Pieter J. K. Libin, Richard Dazeley, and Cameron Foale. Scalar reward is not enough: a response to silver, singh, precup and sutton (2021). Autonomous Agents and Multi-Agent Systems, 36(2):41, Jul 2022. ISSN 1573-7454. doi: 10.1007/s10458-022-09575-5. URL https://doi.org/10.1007/ s10458-022-09575-5.\n\nJ. von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University\n\nPress, 1947.\n\nYu Wang, Nima Roohi, Matthew West, Mahesh Viswanathan, and Geir E. Dullerud. Statistically model checking pctl specifications on markov decision processes via reinforcement learning. In 2020 59th IEEE Conference on Decision and Control (CDC), pp. 1392–1397, 2020. doi: 10. 1109/CDC42340.2020.9303982.\n\nTom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough\n\nfor convex mdps, 2021. URL https://arxiv.org/abs/2106.00661.\n\nJunyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesv ́ari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 2020-December, 2020. ISSN 1049-5258.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA PROOFS OF LEMMAS\n\nIn this Appendix, we provide the proofs of the lemmas from Section 3.\n\nLemma 1. If R is non-constant, then for any state s there exists trajectories ζ1, ζ2, ζ3 starting in s such that G(ζ1) ̸= G(ζ2), G(ζ2) ̸= G(ζ3), and G(ζ1) ̸= G(ζ3).\n\nProof. First note that if R is non-constant, then there must be some state s and some trajectories ξ1, ξ2 starting in s such that G(ξ1) ̸= G(ξ2) (this follows from Theorem 3.8 in Skalse et al. (2022a)). We will establish that there is a ξ3 starting in s such that G(ξ3) ̸= G(ξ1) and G(ξ3) ̸= G(ξ2), and then show that this implies that such trajectories exist for all states.\n\nSuppose for contradiction that for any ξ3 starting in s, either G(ξ3) = G(ξ1) or G(ξ3) = G(ξ2). Consider a transition ⟨s, a, s⟩, and let ζ1 = ⟨s, a, s⟩ + ξ1 and ζ2 = ⟨s, a, s⟩ + ξ2; we will do a case enumeration, and show that either G(ζ1) or G(ζ2) must be distinct from both G(ξ1) and G(ξ2). Note that G(ζ1) = R(s, a, s) + γG(ξ1) and G(ζ2) = R(s, a, s) + γG(ξ2).\n\nCase 1: G(ζ1) = G(ξ1), G(ζ2) = G(ξ2). If R(s, a, s) + γG(ξ1) = G(ξ1) then R(s, a, s) = (1 − γ)G(ξ1), and similarly, if R(s, a, s) + γG(ξ2) = G(ξ2) then R(s, a, s) = (1 − γ)G(ξ2). This is a contradiction, since G(ξ1) ̸= G(ξ2) and γ ̸= 1.\n\nCase 2: G(ζ1) = G(ζ2) = G(ξ1). If R(s, a, s) + γG(ξ1) = G(ξ1) then R(s, a, s) = (1 − γ)G(ξ1). Using R(s, a, s) + γG(ξ2) = G(ξ1), we get (1 − γ)G(ξ1) + γG(ξ2) = γG(ξ1). By rearranging, we get γ(G(ξ1) − G(ξ2)) = 0. This is a contradiction, since G(ξ1) ̸= G(ξ2) and γ ̸= 0.\n\nCase 3: G(ζ1) = G(ζ2) = G(ξ2). This is analogous to Case 2.\n\nCase 4: G(ζ1) = G(ξ2), G(ζ2) = G(ξ1). If R(s, a, s) + γG(ξ1) = G(ξ2) then R(s, a, s) = G(ξ2) − γG(ξ2), and similarly, if R(s, a, s) + γG(ξ2) = G(ξ1) then R(s, a, s) = G(ξ1) − γG(ξ2). Combining this, and rearranging, gives (1 + γ)G(ξ1) = (1 + γ)G(ξ2). This is a contradiction, since G(ξ1) ̸= G(ξ2) and γ ̸= −1.\n\nThis exhausts all cases, which means that if R is non-constant, then there must be some state s and some trajectories ζ1, ζ2, ζ3 starting in s such that G(ζ1) ̸= G(ζ2), G(ζ2) ̸= G(ζ3), and G(ζ1) ̸= G(ζ3). Finally, note that this means that we can construct such trajectories for any state s′, by simply composing a transition ⟨s′, a, s⟩ with each of ζ1, ζ2, ζ3.\n\nLemma 2. If G2(ξ) = f (G1(ξ)) for all ξ and some f , then for any transition ⟨s, a, s′⟩ and any trajectory ζ starting in s′, R2(s, a, s′) = f (R1(s, a, s′) + γG1(ζ)) − γf (G1(ζ)).\n\nProof. Suppose that G2(ξ) = f (G1(ξ)) for all trajectories ξ. Let ⟨s, a, s′⟩ be an arbitrary transition, let ζ be an arbitrary trajectory starting in s′, and let ξ = ⟨s, a, s′⟩ + ζ. We have that G2(ξ) = R2(s, a, s′) + γG2(ζ), and also that G2(ξ) = f (G1(ξ)), which implies that\n\nR2(s, a, s′) + γG2(ζ) = f (G1(ξ)).\n\nSince G1(ξ) = R1(s, a, s′) + γG1(ζ), this implies that\n\nR2(s, a, s′) + γG2(ζ) = f (R1(s, a, s′) + γG1(ζ)).\n\nBy using the fact that G2(ζ) = f (G1(ζ)), and rearranging, we get that\n\nR2(s, a, s′) = f (R1(s, a, s′) + γG1(ζ)) − γf (G1(ζ)).\n\nSince ⟨s, a, s′⟩ and ζ were chosen arbitrarily, this completes the proof.\n\nLemma 3. For any non-constant reward R1 and any f that is injective on range(G1), if for any y ∈ range(R1) and any γ ∈ (0, 1) there are at most two distinct x1, x2 such that f (y + γx1) − γf (x1) = f (y + γx2) − γf (x2) then there is no reward R2 such that G2(ξ) = f (G1(ξ)) for all ξ.\n\nProof. Suppose for contradiction that G2(ξ) = f (G1(ξ)) for all ξ. Let ⟨s, a, s′⟩ be an arbitrary transition. Applying Lemma 2, we get that\n\nR2(s, a, s′) = f (R1(s, a, s′) + γG1(ζ)) − γf (G1(ζ))\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nfor all trajectories ζ starting in s′. For clarity, let x = G1(ζ) and y = R1(s, a, s′), so that f (y + γx) − γf (x). By assumption, there can be at most two distinct values x1, x2 such that f (y + γx1) − γf (x1) = f (y + γx2) − γf (x2). However, Lemma 1 implies that there are at least three ζ1, ζ2, ζ3 starting in s′ with distinct values of G1. Since f is injective on range(G1), this means that there are at least three distinct values of x for which f (y + γx) − γf (x) must be constant (and equal to R2(s, a, s′)), which is a contradiction.\n\nB TOWARDS NECESSARY AND SUFFICIENT CONDITIONS\n\nIn this paper, we have provided several examples of “natural” policy orderings which cannot be represented using a reward function. It would be desirable to have a set of necessary and sufficient conditions to characterise those orderings over Π that can be expressed by reward functions, similar to that provided by the VNM axioms (the VNM axioms themselves do not provide this, see Appendix C). We consider this to be an important topic for future work. In this section, we will discuss a few interesting properties which are shared by all policy orderings which can be represented by reward functions. We believe that these examples will help with building an intuition for what reward functions can and cannot express.\n\nWe would first like to point out that, while it seems difficult to characterise the policy orderings which can be expressed by reward functions, it is fairly straightforward to exactly characterise the sets of policies ˆΠ that can be optimal under some reward function: Proposition 1. A set of policies ˆΠ is the optimal policy set for some reward function if and only if there is a function o : S → P(A)\\∅ that maps each state to a (non-empty) set of “optimal actions”, and π ∈ ˆΠ if and only if supp(π(s)) ⊆ o(s).\n\nProof. For the “if” part, consider the reward function R where R(s, a, s′) = 0 if a ∈ o(s), and R(s, a, s′) = −1 otherwise. The “only if” part follows from the fact that the optimal Q-function Q⋆ is the same for all optimal policies, so we can let o(s) = arg maxa Q⋆(s, a).\n\nThis immediately lets us rule out many policy orderings as inexpressible. For example, consider the task “always go in the same direction” — this task cannot be expressed as a reward function, because any policy that mixes the actions of two other optimal policies must itself be optimal. It also shows that Markovian reward functions cannot be used to encourage stochastic policies. For example, there is no Markovian reward function under which “play rock, paper, and scissors with equal probability” is the unique optimal policy.\n\nThe next thing we would like to point out is that no reward function can express an ordering over Π that has a countable number of equivalence classes (except trivial reward functions, which have only one equivalence class). This simple fact also rules out many orderings.\n\nProposition 2. If R is non-trivial then J has an uncountable number of equivalence classes.\n\nProof. This follows from the intermediate value theorem, and the fact that J is continuous in Π.\n\nThis simple observation can be used to e.g. create an alternative proof of Theorem 4, which says that the MaxSat objective cannot be represented as a (scalar) reward function. It also shows that objectives such as e.g. J(π) = minξ∈supp(π) G(ξ), which evaluates policies according to the worst trajectory in their support, cannot be represented (since any policy then has the same value as some deterministic policy, and since there is only a finite number of deterministic policies).\n\nC A DIGRESSION ON THE VON NEUMANN–MORGENSTERN AXIOMS\n\nThe famous VNM axioms, due to von Neumann & Morgenstern (1947), provide necessary and sufficient conditions for when a utility function can be used to represent a preference ordering for lotteries over a finite choice set. In an MDP, a policy induces a distribution over trajectories, and a reward function assigns a value to each trajectory. One might then wonder if the VNM axioms could provide necessary and sufficient conditions for when an ordering over Π can be realised using\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\na reward function. This is not the case, and in this appendix, we briefly point out why. These results are not novel to this paper, but are instead provided to help with intuition building.\n\nFirst of all, the VNM theorem assumes that the choice set is finite, whereas in an MDP, the number of trajectories is (countably) infinite. There are preferences between distributions over countable choice sets which satisfy the VNM axioms, but which can nonetheless not be represented using utility functions.1 Second, not all distributions over trajectories can be represented as a policy (unless we allow both the policy and the transition function to be non-stationary). Third, there is a special structure to how a reward function assigns value to a trajectory, and not all functions Ξ → R can be represented in this way. This means that the VNM axioms are not applicable to RL. However, it may still be possible to provide similar intuitive necessary and sufficient conditions for the RL case. We consider this to be an important topic for future work.\n\nD MORE MORL OBJECTIVES\n\nIn this Appendix, we give even more examples of MORL objectives, and some comments on how to construct them – the purpose of this is mainly just to show how rich this space is. First, similar to the MaxMin objective, we might want to judge a policy according to its best performance:\n\nDefinition 9. Given J1 . . . Jk, the MaxMax objective ≺Max is given by π1 ≺Max π2 ⇐⇒ maxi Ji(π1) < maxi Ji(π2).\n\nWe would next like to point out that it is possible to create smooth versions of almost any MORL objective. In Section 5, we outline an approach for learning any continuous, differentiable MORL objective, so this is quite useful. We begin with a soft version of the MaxMax objective:\n\nDefinition 10. Given J1 . . . Jk and α > 0, the Soft MaxMax objective ≺MaxSoft is given by\n\nJMaxSoft(π) =\n\n(cid:32) k\n\n(cid:88)\n\ni=1\n\nJi(π)eαJi(π)\n\n(cid:33)(cid:44)(cid:32) k\n\n(cid:88)\n\n(cid:33)\n\neαJi(π)\n\n.\n\ni=1\n\nThis is of course not the only way to continuously approximate MaxMax, it is just an example of one way of doing it. Here α controls how “sharp” the approximation is – the larger α is, the closer JMaxSoft gets to the sharp max function, and the smaller α is, the closer it gets to the arithmetic mean function (so by varying α, we can continuously interpolate between them). Similarly, we can also create a smooth version of MaxMin:\n\nDefinition 11. Given J1 . . . Jk and α > 0, the Soft MaxMin objective ≺MinSoft is given by\n\nJMinSoft(π) =\n\n(cid:32) k\n\n(cid:88)\n\ni=1\n\nJi(π)e−αJi(π)\n\n(cid:33)(cid:44)(cid:32) k\n\n(cid:88)\n\n(cid:33)\n\ne−αJi(π)\n\n.\n\ni=1\n\nAs before, the larger α is, the closer JMinSoft gets to the sharp min function, and the smaller α is, the closer it gets to the arithmetic mean function We can also smoothen MaxSat:\n\nDefinition 12. Given J1 . . . Jk, c1 . . . ck, and α > 0, the Soft MaxSat objective ≺SatSoft is\n\nJSatSoft(π) =\n\nk (cid:88)\n\n(cid:18)\n\ni=1\n\n1 1 + e−α(Ji(π)−ci)\n\n(cid:19)\n\n.\n\nThe larger α is, the closer JSatSoft gets to the sharp MaxSat function (and the smaller α gets, the closer JSatSoft gets to a flat 0.5). And, again, this is of course not the only way to create a smooth version of MaxSat. It is unclear if it is possible to create a smooth version of ConSat without having any prior knowledge of (a lower bound of) the value of minπ J1(π), but with this value it should be reasonably straightforward (see the construction in Theorem 5). As for LexMax, we can of course create a smooth approximation of it by taking a linear approximation of the weights, but here we would need some prior knowledge of maxπ J1(π) . . . maxπ Jk(π).\n\n1For example, consider the ordering that prefers all distributions with infinite support over all distributions\n\nwith finite support, and which is indifferent between any two distributions in either of these classes.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nE A METHOD FOR SOLVING MODAL TASKS\n\nIn this Appendix, we give an outline of one possible method for solving modal tasks. We mainly want to show that it is feasible to learn modal tasks, and so we only provide a solution sketch; the task of implementing and evaluating this method is something we leave as a topic for future work.\n\nWe will first define a restricted class of modal tasks, which is both very expressive, and also more amenable to learning than the more general version given in Definition 7:\n\nDefinition 13. An affordance consists of a reward function and a discount factor, ⟨R, γ⟩, and an affordance-based reward is a function R♢ : S × A × S × R2k → R, that is continuous in the last 2k arguments. An affordance-based MDP is a tuple ⟨S, A, τ, μ0, R♢, γ, ⟨R, γ⟩k⟩, where the reward given for transitioning from s to s′ via a is R♢(s, a, s′, V ⋆ k (s′)), where V ⋆ is the optimal value function of the i’th affordance. i\n\n1 (s′) . . . V ⋆\n\n1 (s) . . . V ⋆\n\nk (s), V ⋆\n\nThis definition requires some explanation. In psychology (and other fields, such as user interface design), an affordance is, roughly, a perceived possible action, or a perceived way to use an object. For example, if you see a button, then the fact that you can press that button, and expect something to happen, is part of how you perceive it, in a way that might not be the case if you could somehow show the button to a premodern human. It can also be used to refer to a choice or action that is perceived as available in some context (without being tied to an object). Here, we are using it to refer to a task that could be performed in an MDP. The intuition is that R♢ is allowed to depend on what could be done from s and s′, in addition to the state features of s and s′.\n\nBefore outlining an algorithm, let us first give a few examples of how to formalise modal tasks within this framework. First consider the instruction “you should always be able to return to the start state”. We can formalise this using a reward function R1 that gives 1 reward if the start state is entered, and 0 otherwise, and pair it up with a discount parameter γ that is very close to 1. We could then set R♢ to, for example, R♢(s, a, s′, V ⋆ 1 (s′)), where R describes some base task. In this way, no reward is given if the start state cannot be reached from s′. Next, consider the instruction “never enter a state from which it is possible to quickly enter an unsafe state”. To formalise this, let R1 give 1 reward if an unsafe state is entered, and 0 otherwise, and let γ correspond to a very high discount rate (e.g. 0.7). We could then set R♢ to, for example, R♢(s, a, s′, V ⋆\n\n1 (s′), where R again describes some base task.\n\n1 (s′)) = R(s, a, s′) · tanh(V ⋆\n\n1 (s′)) = R(s, a, s′) − V ⋆\n\n1 (s), V ⋆\n\n1 (s), V ⋆\n\nThese examples show that our “affordance-based” MDPs are quite flexible, and that they should be able to formalise many natural modal tasks in a satisfactory way, including most of our motivating examples.2 However, the definition could of course be made more general. For example, we could allow the affordances to themselves be based on affordance-based reward functions, etc. However, it is not clear if this would bring much benefit in practice.\n\nLet us now outline an approach for solving affordance-based MDPs using reinforcement learning, specifically using an action-value method. First, let the agent maintain k + 1 Q-functions, Q♢, Q1, . . . , Qk, one for R♢ and one for each affordance ⟨Ri, γi⟩. Next, we suppose that the agent updates each of Q1, . . . , Qk using an off-policy update rule, such as Q-learning; this will ensure that Q1, . . . , Qk converge to their true values (i.e. to Q⋆ k), as long as the agent explores infinitely often. Note that the use of an off-policy update rule is crucial. Next, let the agent update Q♢ as if it were an ordinary Markovian reward function, using the reward ˆR(s, a, s′) = R♢(s, a, s′, V1(s) . . . Vk(s), V1(s′) . . . Vk(s′)), where Vi(s) is given by maxa Qi(s, a). In other words, we let it update Q♢ using an estimate of the true value of R♢, expressed in terms of its k . The fact that Q1, . . . , Qk converge to Q⋆ current estimates of V ⋆ k, and the fact that R♢ is continuous in its value function arguments, will ensure that the estimate ˆR also converges to the true value of R♢. The update rule used for Q♢ could be either on-policy or off-policy. We then suppose that the agent selects its actions by applying a Bandit algorithm to Q♢, and that this Bandit algorithm is greedy in the limit, but also explores infinitely often, as usual.\n\n1, . . . , Q⋆\n\n1 . . . V ⋆\n\n1 . . . Q⋆\n\nThis algorithm should be able to learn to optimise the reward in any affordance-based MDP. In the tabular case, it should be possible (and reasonably straightforward) to prove that it always converges to an optimal policy (assuming that appropriate learning rates are used, etc), using Lemma 1 in\n\n2This arguably excludes “you should never enter a state where you would be unable to receive a feedback\n\nsignal”. However, this instruction only makes sense in a multi-agent setting.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nSingh et al. (2000). We would also expect it to perform well in practice, when used with function approximators (such as neural networks). However, we leave the task of implementing and properly evaluating this approach as a topic for future work.\n\nThere are also several ways that this algorithm could be tweaked or improved. For example, the algorithm we have described is an action-value algorithm, but the same approach could of course be used to make an actor-critic algorithm instead. We also suspect that there could be interesting modifications one could make to the exploration strategy of the algorithm. If a standard Bandit algorithm (such as ε-greedy) is used, then the agent will mostly take actions that are optimal under its current estimate of Q♢. In the ordinary case, this is good, because it leads the agent to spend more time in the parts of the MDP that are relevant for maximising the reward. However, in this case, there is a worry that it could lead the agent to neglect the parts of the (affordance-based) MDP that are relevant for learning more about V ⋆ k , which might slow down the learning. Again, we leave such developments for future work, since our aim here only is to show that it is feasible to learn non-trivial modal tasks.\n\n1 . . . V ⋆\n\nWe also want to point out that the work by Wang et al. (2020) could provide another starting point for learning modal tasks using RL. In their work, they present some RL-based methods for determining whether a specification in Probabilistic Computational Tree Logic (PCTL) holds in an MDP. PCTL can be used to specify many kinds of properties of states in MDPs which depend on the transition function, including e.g. what states can and cannot be reached from a particular state, and with what probability, etc. We can therefore specify non-trivial modal tasks by providing a number of PCTL formulas, and allowing the reward function to depend on the truth values of these formulas. That is, we could consider a setup that is analogous to that which we give in Definition 13, but where the “affordances” are replaced by PCTL formulas. It should then be possible to learn tasks specified in this manner by using the techniques of Wang et al. (2020) to learn the values of the PCTL formulas, and then using ordinary RL to train on the resulting reward function.\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper tries to disprove the reward hypothesis mathematically by showing some classes of tasks cannot be expressed using any scalar Markovian reward function. They showcase a set of multi-objective reward functions, risk averse utility functions that can’t be reduced to an equivalent scalar reward function. They also introduce a new class of RL tasks, namely,  modal tasks where the reward is dependent on the transition function\n\n# Strength And Weaknesses\n\nStrengths \nThe paper does provide sound proofs that some of the multi objective and risk averse based reward functions cannot be reduced, ceteris paribus, to an equivalent scalar Markovian reward function.\n\nWeaknesses/Questions\n\n1. The paper states that ‘In other words, it is the hypothesis that any natural task can be expressed as a reward signal’. The definition of a natural task can vary significantly. It would also bring up the question of whether the corresponding objective functions are ‘natural’ or if they are part of ‘what we mean by goals and purposes’.\n\n2. I think there should be clarification on why the state space has to be the same between the original and modified MDPs to showcase that the reward hypothesis holds. \n\n3.This also leads to the question of if there are scalar rewards conditioned on the objectives then would that be equivalent to the original MORL or Risk Sensitive RL reward functions or if there are reward function approximators that approximate such equivalent rewards then would the reward hypothesis hold. Could there even be such approximators?\n\n4. Coming to modal reward function, there isn’t a specific example that showcases how the reward function might be dependent on the transition function.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity - The writing in general is followable and most of the proofs are straightforward.\n\nNovelty - The attempt to disprove the reward hypothesis mathematically is novel.\n\nReproducability - It is reproducible\n\n# Summary Of The Review\n\nThe paper definitely provides more insight into discussing the reward hypothesis and the extent to which it can be flexible but there are other aspects that need to be clarified (mentioned in the strengths and weaknesses section) in order to firmly disprove the reward hypothesis. Even in regards to the modal tasks, additional details are needed to clarify the dependency of the reward function and the transition function. Due to these reasons, I would incline towards not accepting this paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nEVALUATING VISUAL COUNTERFACTUAL EXPLAINERS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nExplainability methods have been widely used to provide insight into the decisions made by statistical models, thus facilitating their adoption in various domains within the industry. Counterfactual explanation methods aim to improve our understanding of a model by perturbing samples in a way that would alter its response in an unexpected manner. This information is helpful for users and for machine learning practitioners to understand and improve their models. Given the value provided by counterfactual explanations, there is a growing interest in the research community to investigate and propose new methods. However, we identify two issues that could hinder the progress in this field. (1) Existing metrics do not accurately reflect the value of an explainability method for the users. (2) Comparisons between methods are usually performed with datasets like CelebA, where images are annotated with attributes that do not fully describe them and with subjective attributes such as “Attractive”. In this work, we address these problems by proposing an evaluation method with a principled metric to evaluate and compare different counterfactual explanation methods. The evaluation is based on a synthetic dataset where images are fully described by their annotated attributes. As a result, we are able to perform a fair comparison of multiple explainability methods in the recent literature, obtaining insights about their performance. We make the code and data public to the research community.\n\n1\n\nINTRODUCTION\n\nThe popularity of deep learning methods is a testament to their effectiveness across a multitude of tasks in different domains. This effectiveness has led to their widespread industrial adoption (e.g., self-driving cars, screening systems, healthcare, etc.), where the need to explain a model’s decision becomes paramount. However, due to the high level of complexity of deep learning models, it is difficult to understand their decision making process (Burkart & Huber, 2021). This ambiguity has slowed down the adoption of these systems in critical domains. Hence, in order to ensure algorithmic fairness in deep learning and to identify potential biases in training data and models, it is key to explore the reasoning behind their decisions (Buhrmester et al., 2021).\n\nIn an attempt to convincingly tackle the why question, there has been a surge of work in the field of explainability for machine learning models (Joshi et al., 2018; Mothilal et al., 2020; Rodríguez et al., 2021). The goal of this field is to provide explanations for the decisions of a classifier, which often come in the form of counterfactuals. These conterfactuals provide insight as to why the output of the algorithms is not any different and how it could be changed (Goyal et al., 2019). Basically a counterfactual explanation answers the question: “For situation X why was the outcome Y and not Z”, describing what changes in a situation would have produced a different decision.\n\nIdeally, counterfactual methods produce explanations that are interpretable by humans while reflecting the factors that influence the decisions of a model (Mothilal et al., 2020). So given an input sample and a model, a counterfactual explainer would perturb certain attributes of the sample, producing a counterexample i.e., counterfactual, that shifts the model’s prediction, thus revealing which semantic attributes the model is sensitive to. In this work, we focus on the image domain given the recent surge of explainability methods for image classifiers (Joshi et al., 2018; Rodríguez et al., 2021; Singla et al., 2019; Chang et al., 2018). A particular challenge of the image domain is that changes in the pixel space are difficult to interpret and resemble adversarial attacks (Goodfellow et al., 2014b) (see Figure 4 for an example), so current explainers tend to search for counterfactuals in a latent space produced\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nby, e.g., a variational autoencoder (VAE) (Kingma & Welling, 2013), or by conditioning on annotated attributes (Denton et al., 2019; Joshi et al., 2018; Singla et al., 2019; Rodríguez et al., 2021).\n\nAlthough explanations produced in the latent space are easier to interpret than in the pixel space, they depend on the chosen or learned decomposition of the input into attributes or latent factors. In the case of VAEs, these factors could be misaligned with the real underlying generating process of the images. Moreover, different methods in the literature rely on different autoencoding architectures or generative models to infer semantic attributes from images, which make their counterfactual search algorithms not comparable. In the case of annotations, since datasets do not provide access to the whole data generating process, they tend to focus on arbitrary aspects of the input (such as facial attributes for CelebA (Liu et al., 2015b)), ignoring other aspects that could influence a classifier’s decision boundaries such as illumination, background color, shadows, etc. This raises the need for evaluating explainers on a known set of attributes that represent the real generative factors of the input. In this work, we propose to fill this gap by introducing a new explainability benchmark based on a synthetic image dataset, where we model the whole data generating process and samples are fully described by a controlled set of attributes.\n\nAn additional challenge when evaluating explainers is that there is no consensus on the metric that should be used. While there has been some effort to provide a general metric to evaluate explainers (Mothilal et al., 2020; Rodríguez et al., 2021), most of the proposed metrics could be easily gamed to maximize the score of a given explainer without actually improving its quality for a user. For example, since current metrics reward producing many explanations, the score can be increased by (i) producing random samples that cannot be related to the ones being explained. This has motivated measuring the proximity of explanations (Mothilal et al., 2020). (ii) Repeating the same explanation many times. This motivates measuring diversity (Mothilal et al., 2020). However, we found that it is possible to maximize existing diversity measures by always performing the same perturbation to a sensitive attribute while performing random perturbations to the rest of attributes that describe a sample. As a result, although one counterfactual changing the sensitive attribute would suffice, an explainer could obtain a higher score by producing more redundant explanations. We argue that instead of providing many explanations, explainers should be designed to produce the minimal set of counterfactuals that represent each of the factors that influence a model’s decision. (iii) Providing uninformative or trivial explanations (Rodríguez et al., 2021). This has motivated us to compare the model’s predictions with those expected from an “oracle classifier”. Model predictions that deviate from the expected value are more informative than those that behave as expected (see A.2. In this work, we address these problems by proposing a fair way to evaluate and compare different counterfactual explanation methods.\n\ncierOur contributions can be summarized as follows: (i) we present a benchmark to evaluate counterfactuals generated by any explainer in a fair way (Section 3); (ii) we offer insights on why existing explainability methods have strong limitations such as an ill-defined oracle (Section 3.3); (iii) we introduce a new set of metrics to evaluate the quality of counterfactuals (Section 3.4); and (iv) we evaluate 6 explainers across different dataset configurations (Section 4).\n\n2 RELATED WORK\n\nExplainability methods. Since most successful machine learning models are uninterpretable (He et al., 2016; Jégou et al., 2017; LeCun et al., 1989), modern explainability methods have emerged to provide explanations for these types of models, which are known as post-hoc methods. An important approach to post-hoc explanations is to establish feature importance for a given prediction. These methods (Guidotti et al., 2018; Ribeiro et al., 2016; Shrikumar et al., 2017; Bach et al., 2015) involve locally approximating the machine learning model being explained with a simpler interpretable model. However, the usage of proxy models hinders the truthfulness of the explanations. Another explainability technique is visualizing the factors that influenced a model’s decision through heatmaps (Fong et al., 2019; Elliott et al., 2021; Zhou et al., 2022). Heatmaps are useful to understand which objects present in the image have contributed to a classification. However, heatmaps do not show how areas of the image should be changed and they cannot explain factors that are not spatially localized (e.g., size, color, brightness, etc).\n\nExplanation through examples or counterfactual explanations addresses these limitations by synthesizing alternative inputs (counterfactuals) where a small set of attributes is changed resulting in a different classification. These counterfactuals are usually created using generative models. A set of\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Comparison of explainers considered in this work. First column indicates whether counterfactuals are found with gradient descent. Second column displays the domain in which we perform the counterfactual search, with z referring to the attribute or latent space. Third column indicates whether the explainer takes into account changes in pixel space during optimization (e.g., visual similarity loss). Last column indicates if the explainer performs feature selection to generate counterfactuals.\n\nMethod\n\nGradient based Domain Optimizes x-space Feature selection\n\nDiCE (Mothilal et al., 2020)\n\nDiVE (Rodríguez et al., 2021)\n\nGS (Laugel et al., 2017)\n\nStylEx (Lang et al., 2021)\n\nLatent-CF (Balasubramanian et al., 2020)\n\nxGEM (Joshi et al., 2018)\n\n✓\n\n✓\n\n✗\n\n✗\n\n✓\n\n✓\n\nz\n\nz\n\nz\n\nz\n\nz\n\nz\n\n✗\n\n✓\n\n✗\n\n✗\n\n✗\n\n✓\n\n✗\n\n✗\n\n✓\n\n✓\n\n✗\n\n✗\n\nmethods condition the generative model on attributes annotated in the dataset by using a conditional Generative Adversarial Network (GAN) (Joshi et al., 2018; Liu et al., 2019; Sauer & Geiger, 2021; Van Looveren et al., 2021; Yang et al., 2021). However, this approach restricts the explanations to the provided attributes which do not reflect the entirety of the image properties, making the applicability of these methods challenging where annotations are scarce. In order to generate counterfactuals without recurring to annotated attributes, another set of methods uses VAEs or unconditional GANs (Goodfellow et al., 2014a) that do not depend on attributes during generation (Rodríguez et al., 2021; Denton et al., 2019; Pawelczyk et al., 2020; Perez et al., 2018; Mothilal et al., 2020). See Table 1 for a comparison of the methods considered in our work.\n\nExplainability Benchmarks. DiVE (Rodríguez et al., 2021) and DiCE (Mothilal et al., 2020) propose metrics that allow researchers to evaluate the quality of an explanation. These metrics evaluate the proximity of explanations to their original sample, and how diverse these are. Unfortunately, they are easy to game. For example, an explainer could maximize diversity by always modifying the same counterfactual attribute but randomly perturbing other non-counterfactual attributes to produce new redundant explanations. We propose a more general, harder to game metric that allows us to evaluate a set of explainers in order to identify their strengths and weaknesses through fair comparisons. Further, the set of attributes of a dataset can influence the evaluation of the explainability methods. CelebA (Liu et al., 2015a) is a common dataset used for generating counterfactual explanations (Rodríguez et al., 2021; Denton et al., 2019), and it is labeled with a series of attributes, such as “Atractive\", that fail to fully describe the true underlying factors that generated the images (e.g, illumination, occlusions, contrast, etc). Likewise, there is no guarantee that unsupervised disentanglement methods such as VAEs identify the true factors of variations without making strong assumptions (Arjovsky et al., 2019). We sidestep these problems by evaluating all explainers in a common latent space with known attributes that fully describe the samples. Recently Pawelczyk et al. (2021) published a benchmark (CARLA) with an extensive comparison of several counterfactual explanation methods across 3 different tabular datasets. Our work differs from CARLA in three important ways: (1) we propose a principled metric to compare counterfactual explanation methods, (2) we introduce a new synthetic benchmark that allows comparing multiple explainers in a fair manner in the same latent space. (3) We focus on counterfactual visual explanations, which require access to a common latent space for fair comparison since pixel-level counterfactuals are difficult to interpret (e.g., adversarial attacks).\n\n3 PROBLEM SETUP\n\nIn the following lines we describe a principled framework to quantify the quality of counterfactual explainers and show how it can be applied to compare multiple methods in the literature. In Section 3.1 we define the data generation process, in Section 3.2 we define the counterfactual generation process, in Section 3.3 we define the concept of optimal classifier used to compare the predictions of a model, and in Section 3.4 we define the metric used to evaluate counterfactual explanation methods. A notation table can be found in Table 3.\n\n3.1 DATA GENERATION\n\nMany explainability methods in the literature are designed for the image domain (Rodríguez et al., 2021; Joshi et al., 2018; Lang et al., 2021; Singla et al., 2019; Chang et al., 2018). In this area, most\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\ndatasets can be described with a data generating process where a set of latent variables (z) result in an image (x) and a corresponding label (y), see Figure 1a. However, not all the latents that generate the image have an impact on the label (zind). For example, the image brightness does not affect the presence of a dog. In addition, some latents can be correlated with the label (zcorr). For instance, whenever there is a dog there is usually a dog collar. Formally, we consider a data generating process where a set of latent variables z ∈ Rd are sampled from a prior p(z), and a generator that produces images p(x|z). Labels are generated using p(y|zcausal), where zcausal is a subset of z containing direct causal parents of y (Figure 1a). We also define zcorr as the set of attributes that are correlated to y but not part of zcausal.1 Sometimes, these correlated attributes may have stronger predictive power, but relying on them would lead to unreliable predictions. For instance using the sky background for classifying airplanes. To generate datasets, we rely on a structural causal model (SCM) (Pearl, 2009), corresponding to a sequence of stochastic equations producing random variables based on the causal parents in the causal graph as described in Figure 1a.\n\nIn order to obtain a known mapping between z and x, we propose to leverage synbols (Lacoste et al., 2020), a synthetic dataset generator with many controllable attributes (font, character, color, rotation, size, etc). In addition, using a synthetic dataset allows us to control the effect of z on x and specify the amount of change in x relative to the amount of change in z (and vice-versa). Using synbols, we train an image generator x = g(z)2, which is used to generate subsequent datasets. The generator g is provided to the explainers to offer a differential mapping from z to x. We believe this is a strength of our benchmark compared to using datasets of natural images.\n\n3.2 COUNTERFACTUAL GENERATION\n\nGiven an image x and a classifier ˆf (x), a counterfactual explanation method (explainer) produces x′, a perturbed version of x that shows some insight about the sensitivity of ˆf to the semantic attributes that describe x. The perturbation is commonly performed on a learned latent space z. In general, explainers are tasked to learn an encoder and find a useful latent space, but this task is hard and still under active research. In order to bring a better comparison between explainers, we provide them access to the generating function g and z so that explanations are generated in the same latent space. This gives us the opportunity to let explainers work directly in latent space by defining ˆh(z) := ˆf (g(z)). In other words, we define an explainer as:\n\n{z′\n\ni}n\n\ni=1 = e(z, ˆh, g),\n\n(1)\n\ni is the ith counterfactual explanation from z found by explainer e on the latent classifier ˆh. where z′ Working in latent spaces greatly simplifies the task of an explainer, but we will see that there are still a variety of challenges to be addressed. Namely, the notion of optimal classifier or stable classifier may be ill defined or may not always exists.\n\n3.3 OPTIMAL CLASSIFIER\n\nCounterfactual explanation methods tend to produce trivial explanations by perturbing the attribute being classified from the input (Rodríguez et al., 2021). A more useful explainer would change a model’s predictions by perturbing non-causal attributes (such as the background of an image). To distinguish between these two kinds of explanations, an “oracle” is required, whose predictions are contrasted with those of the model. If an explanation changes both the oracle and the model’s predictions, the explanation is deemed trivial and discarded. However, if the explanation only changes one of the two, the explanation is non-trivial. In the absence of a human oracle who knows the causal attribute being classified, the authors resort to an optimal predictor or ground truth classifier. However, the concept of optimal predictor is commonly ill defined and application dependant, thus we must proceed with care when assuming the existence of a ground truth classifier. To show that, we next define the concepts of Bayes classifier, causal classifier, and finally the causal classifier with non-reversible generator used in this work.\n\n1z ∈ zcorr could be correlated to y for two different reasons: i) y → z ii) a confounder α such that\n\ny ← α → z. Note that α may be element of zcausal or outside of the scene, such as the photograph.\n\n2In this work, we consider deterministic generators. A more general formulation would be g(x|z)\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure 1: (a) Example of a causal graph satisfying the problem setup of section 3.1. (b) Successful counterfactual explanation as defined in (Mothilal et al., 2020; Joshi et al., 2018). That is, a successful counterfactual changes (gray) the classifier prediction (red) for the sample (point). The dashed square represents the maximum L1 norm of the perturbation performed by an explainer (c) Our definition of successful counterfactual explanation (gray) considers any change where an oracle (green) behaves differently from the classifier (red). EF: estimator flips, NCF: non-causal flips, CF: causal flips.\n\nCausal classifier with reversible generator. The causal classifier makes predictions solely based on the causal parents of y in the causal graph G. In latent space: hcausal(z) = arg maxy p(y|zcausal). (cid:0)g−1(x)(cid:1). InterestWhen the generator x = g(z, εx) is reversible, we obtain fcausal(x) = hcausal ingly, this classifier is robust to changes of p(z) as long as p(y|zcausal) and p(x|z) remain unchanged.\n\nCausal classifier with non-reversible generator. It is worth noting that when the generator is not reversible, a given x can lead to many z, which prevents from directly recovering z from x. A natural choice is to rely on the posterior distribution f (x) = (cid:80) z p(z|x)hcausal(z), where p(z|x) ∝ p(z)p(x|z). However, this posterior now depends on p(z), making the new classifier no longer independent to distribution shift when p(z) is changed to e.g. p′(z). This leads to the following negative result (see A.1 for the proof, along with an example): Proposition 1. If there exists a pair z, z′ s.t. g(z) = g(z′) and hcausal(z) ̸= hcausal(z′), then for any deterministic classifier ˆf (x), there is a prior p′(z) s.t. the accuracy of ˆf is 0 with respect to hcausal.\n\n3.4 EVALUATING COUNTERFACTUAL EXPLANATIONS\n\nThe goal for counterfactual generation methods is to find all the attributes that make a classifier behave differently from a causal classifier (see Figure 1c). Note that Mothilal et al. (2020) only considered counterfactuals that change the predictions of a classifier (Figure 1b), and Rodríguez et al. (2021) only considered the top region in Figure 1c. These definitions do not cover cases such as when the oracle changes its prediction while the classifier’s stay the same. Following Mothilal et al. (2020); Rodríguez et al. (2021); Joshi et al. (2018), we also measure the similarity between the original example and the counterfactuals used to explain it. The reason is that counterfactuals should be relatable to original samples so that a human can interpret what is the sensitive semantic attribute. Next, we define the components of the proposed metric (Eq. 7).\n\nProximal change (Joshi et al., 2018; Mothilal et al., 2020). An explanationmust be relatable to the original sample, thus it needs to be proximal. That is, the change z′ needs to stay within a certain radius r from z. Using L1 norm, the set of proximal z′ is defined as follows:\n\nPr(z) = {z′ | ∥z − z′∥1 ≤ r}\n\n(2)\n\nEstimator Flip (EF) (Joshi et al., 2018; Mothilal et al., 2020). This is defined as a proximal change on z leading to a change in prediction of the estimator ˆh (see Figure 1b).\n\nEF(z) =\n\n(cid:110)\n\nz′ (cid:12)\n\n(cid:12) (cid:12)\n\nˆh(z′) ̸= ˆh(z)\n\n(cid:111)\n\n∩ Pr.\n\n(3)\n\n5\n\nYXOracleZcausalZcorrZindz1z2y = 0 = 1ClassifierSample = 0rz1z2y = 0ClassifierSample = 0rCFEFNCFSCECFEFUnder review as a conference paper at ICLR 2023\n\nNon-Causal Flip (NCF). Counterfactuals obtained by estimator flips (EF) are common in the literature as they do not require the knowledge of hcausal. However, if we have access to hcausal, we can detect a new set of explanations: a proximal change in z′ that changes the prediction of ˆh but not of hcausal:\n\nNCF(z) = {z′ | EF(z) ∧ hcausal(z′) = hcausal(z)} ∩ Pr.\n\n(4)\n\nCausal Flip (CF). Additionally, access to hcausal allows us to detect another new set of explanations: a proximal change in z′ that changes the prediction of hcausal but not ˆh:\n\nCF(z) =\n\n(cid:110)\n\nz′ (cid:12)\n\n(cid:12) (cid:12)\n\nˆh(z′) = ˆh(z) ∧ hcausal(z′) ̸= hcausal(z)\n\n(cid:111)\n\n∩ Pr.\n\nThus, we define the set of successful counterfactual explanation (SCE) as follows:\n\nSCE(z) = (NCF ∪ CF) .\n\n(5)\n\n(6)\n\nIn summary, having knowledge of the causal factors (access to hcausal) allows us to evaluate counterfactuals explanations in a new way as illustrated in the following example. Given a dog classifier and an image of a dog, a counterfactual example that changes the background of the image in a way that alters the classifier’s prediction (NCF) will almost certainly provide valuable insight about the model’s behaviour. The same can be said about a counterfactual example that removes the dog from the image without altering the classifier’s prediction (CF) (see Figure 1c). Note that these counterfactuals cannot be detected without causal knowledge, which is only available if we have access to the entire data generating process i.e., a synthetic dataset.\n\nOrthogonal and complement subset. Note that both EF and SCE are possibly infinite sets and cannot be easily interpreted by humans. We could return the explanation minimizing some notion of distance on z or x, however a good explainer should return a useful and diverse set of explanations.\n\nTo this end, we propose a metric that only takes into account the subset of orthogonal and complementary explanations. Otherwise, it is trivial to report many explanations that are a modification of an existing explanation without being useful. For instance, modifying the hair color to trigger a change in gender classification is a good finding, but changing the hair color again, and removing some clouds in the sky would not constitute a useful explanation. Hence, only admitting orthogonal explanations enforces a useful diversity. However, we also admit complementary explanations. That is, if darker hair triggers a change in gender classification and lighter hair also triggers a change, these are two useful explanations. In short, given two explainers that find counterfactuals by perturbing the most sensitive attribute, the orthogonality and complementary requirements ensure that the one that provides a more diverse set of counterfactuals by also perturbing less sensitive attributes scores higher. This is important because it rewards explainers that give a more complete description of the model to the user. There may be use cases where only the most sensitive attribute matters. However, everything else being equal, we argue that, in general, it is favorable to have access to a diversity of explanations.\n\nThe explainer is responsible for returning explanations produced with orthogonal or complementary perturbation vectors. To verify whether explanations are orthogonal or complementary we use a greedy algorithm. Concretely, we sort the explanations by how proximal they are to the original sample and add the first one to the set. Then we iterate through the rest and sequentially add every subsequent explanation that is orthogonal or complementary to all the explanations currently in the set (see Algorithm 1 for implementation). The resulting orthogonal and complement set is referred to as SCE⊥(z). We use the cardinality of this set to evaluate the performance of explainers:\n\nS# = |SCE⊥(z)|.\n\n(7)\n\nWe consider the proposed setup to be fairer than previous works, since: (1) all explainers are compared in the same latent space, resulting in a fair evaluation, (2) uninformative explanations are discarded leveraging knowledge of the causal factors, (3) it is designed to be more difficult to game by repeating counterfactual explanations, and (4) it rewards explainers that return a more complete set of explanations.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Interpolations produced by our learned generator (g(z)). Images are changed as the attribute’s value changes smoothly from one value to another (left-right). From top to bottom: rotation, scale, h-translation, v-translation, char, font.\n\nFigure 3: Average attribute perturbation for each method/scenario. Gradient based methods perturb almost all attributes while gradientagnostic methods perturb only one or two. DiVE focuses almost solely on font.\n\n4 EXPERIMENTS\n\nIn this section we give an overview of the different methods (see Table1) and datasets that are comprised within our benchmark. Since we provide access to a common interpretable latent space, we evaluate explainers that do not depend on a concrete latent decomposition. The code is written in PyTorch (Paszke et al., 2017) and will be made public. Implementations details can be found in the Appendix.\n\nLatent-CF (Balasubramanian et al., 2020): A simple method that performs a adversarial perturbations in the latent space until a counterfactual with confidence higher than threshold tol is found. DiCE (Mothilal et al., 2020): A method that aims to produce a diverse set of counterfactual examples directly from a series of attributes or latent space by proposing a series of perturbations that change the predictions of a classifier. This is achieved by gradient-based optimization of multiple loss functions with respect to the attributes or latents and the classifier:\n\nL = hinge_loss(ˆh(z′), y, margin)\n\n+ λ1dist(z, z′)\n\n+ λ2dpp_diversity(z′)\n\n,\n\n(8)\n\n(A)\n\n(B)\n\n(C)\n\nwhere optimizing (A) pushes the prediction of the classifier ˆf towards y up to some margin, (B) ensures that counterfactuals (z′) are close to the original samples (z), and (C) maximizes the distance between each pair of counterfactuals. xGEM (Joshi et al., 2018): A method equivalent to DiCE without the diversity term (C). DiVE (Rodríguez et al., 2021): A method similar to DiCE that leverages the Fisher Information (FI) to find non-trivial counterfactuals, i.e. samples that change the classifier prediction without changing the causal attribute zcausal, thus focusing on spurious correlations. This is done by masking out latent dimensions with the highest FI while optimizing a cost equivalent to Eq. 8. Growing Spheres (GS) (Laugel et al., 2017): A method that given a data point z identifies its closest neighbour classified differently e referred to as enemy. This is done by finding the smallest l2-ball around z that contains an enemy. Once e is found the dimensions with small changes in e with respect to z are discarded through a feature selection process, maximizing the sparsity of e − z. StylEx (Lang et al., 2021)3: They find a latent perturbation in a direction that maximizes the difference in the output of the classifier for the original sample and its perturbed counterpart. Informed Search (IS): An explainer that knows about the data generation process in Figure 1a. Thus, IS generates explanations by perturbing the spuriously correlated attributes zcorr.\n\n3Since we already provide an interpretable set of latent attributes we evaluate only the Attribute finding\n\n(AttFind) algorithm from the paper\n\n7\n\nLatent-CF (Grad)6 clusters and 0.95 correlation10 clusters and 0.95 correlationcharacterfontcolorrotationscaletranslation-xtranslation-yDiVE (Grad)6 clusters and 0.5 correlation10 clusters and 0.5 correlationcharacterfontcolorrotationscaletranslation-xtranslation-yDiCE (Grad)characterfontcolorrotationscaletranslation-xtranslation-ycharacterfontcolorrotationscaletranslation-xtranslation-yStylExcharacterfontcolorrotationscaletranslation-xtranslation-yxGEM (Grad)characterfontcolorrotationscaletranslation-xtranslation-yGrowing SpheresUnder review as a conference paper at ICLR 2023\n\nTable 2: Score (Eq. 7) and percentage of trivial counterfactuals () obtained by each explainer for each of the different datasets described in 4.1. Values represent an average score across batches for the entire dataset for 3 different runs.\n\nCorrelation\n\n#Spurious Explainer\n\n6\n\n10\n\nIS (Oracle) 4 DiCE (Mothilal et al., 2020) DiVE (Rodríguez et al., 2021) GS (Laugel et al., 2017) StylEx (Lang et al., 2021) Latent-CF (Balasubramanian et al., 2020) xGEM (Joshi et al., 2018)\n\nIS (Oracle) 4 DiCE (Mothilal et al., 2020) DiVE (Rodríguez et al., 2021) GS (Laugel et al., 2017) StylEx (Lang et al., 2021) Latent-CF (Balasubramanian et al., 2020) xGEM (Joshi et al., 2018)\n\n0.50\n\nS#\n\n2.40 ±0.3 1.18 ±0.01 1.02 ±0.00 1.01 ±0.00 1.04 ±0.00 0.82 ±0.00 1.18 ±0.02\n\n2.80 ±0.04 1.13 ±0.01 1.00 ±0.00 1.01 ±0.00 1.15 ±0.00 0.81 ±0.00 1.15 ±0.00\n\n0.95\n\nS#\n\n2.67 ±0.02 1.17 ±0.01 1.00 ±0.01 1.01 ±0.00 1.17 ±0.00 0.93 ±0.00 1.15 ±0.01\n\n3.63 ±0.02 1.19 ±0.01 1.04 ±0.00 1.00 ±0.01 1.12 ±0.00 0.81 ±0.00 1.16 ±0.00\n\n0.50\n\n0.95\n\nTrivial (%)\n\nTrivial (%)\n\n0.00 ±0.00 9.37 ±0.11 2.51 ±0.09 4.49 ±0.40 2.41 ±0.00 0.00 ±0.00 12.46 ±0.03\n\n0.00 ±0.00 8.62 ±0.46 2.28 ±0.03 4.91 ±0.25 3.37 ±0.00 0.00 ±0.00 10.17 ±0.28\n\n0.00 ±0.00 6.78 ±0.26 1.68 ±0.02 2.34 ±0.15 1.58 ±0.00 0.00 ±0.00 6.45 ±0.07\n\n0.00 ±0.00 6.70 ±0.08 1.44 ±0.04 1.95 ±0.08 1.62 ±0.00 0.00 ±0.00 6.38 ±0.11\n\n4.1 DATASETS\n\nWe design a synthetic benchmark based on the synbols dataset (Lacoste et al., 2020). In this benchmarks images are fully defined by 3 categorical attributes (48 fonts, 48 characters, 2 background colors) and 4 continuous attributes (x-translation, y-translation, rotation, scale), see Figure 2.\n\nAn advantage of synbols is the large amount of values in its categorical attributes such as character and font. This allows us to design different scenarios by introducing spurious correlations based on subsets of these attributes. From now on, we assume zcausal = char ∈ [1..48] and ˆhcausal = zcausal mod 2. Then we leverage the font attribute to introduce spurious correlations (zcorr in Figure 1a). Note that increasing the number of fonts in zcorr (the rest will be in zind) increases the random chance of finding a counterfactual by accidentally switching the font. Likewise, increasing the amount of correlation between zcorr and y makes spurious correlations easier to find since the classifier latches stronger on them. We hypothesize that stronger correlations will benefit gradient-based explainers, which will find higher gradient curvature for highly correlated fonts. To explore how explainers behave under different scenarios, we consider 6 and 10 spurious fonts with 50% and 95% correlation with y, resulting in a total of 4 scenarios. Further, we introduce a 5% of noise the the zcausal attribute (character) to encourage classifiers to also rely on the font.\n\n4.2 RESULTS\n\nWe evaluate the performance of six different methods with the metric defined in Eq. 7. Each method is evaluated in several different datasets with varying levels of difficulty as described in 3.1 and 4.1. Additional results can be found in section A.4.\n\nIt is hard to diversify. A good explainer should be able to predict the behavior of the model with respect to changes in the different attributes that generate the data. In the case of a classifier, finding the attributes that induce it to change its prediction in order to reveal if it is relying on attributes that are independent from the class being predicted is a desirable goal. In the pursuit of this goal, an explainer should ideally populate SCE(z) (see Eq. 6) with explanations altering each of the attributes that are correlated with the data. However, as shown in Table 2 explainers fail to consistently find more than one altering attribute.\n\nPerformance saturates with 6 fonts. We observe that methods do not significantly increase the number of successful counterfactuals when adding 4 more spurious fonts (Table 2). This is, partially, because methods tend to focus on changing the zcausal attribute character as seen in Figure 3, which leads to trivial counterfactuals. When adding more fonts, the font identification task becomes more difficult for the classifier, which makes it more sensitive to characters and exacerbates this problem. For a more extensive ablation illustrating this phenomenon see Figure 5.\n\nGradients tend to perturb most of the attributes. Figure 3 offers insight into how each method perturbs z and we can see that gradient-based methods tend to perturb almost all attributes equally,\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nexploring the perturbation space in many directions. In the extreme, we found that Latent-CF slightly modifies all the latent attributes, producing counterfactuals that resemble adversarial attacks. While modifying all the attributes increases the chances of finding 1 good explanation on average, it also prevents the explainer from finding multiple non-trivial diverse explanations. On the other hand, methods that are gradient-agnostic focus on perturbing one or two attributes, resulting in a more narrow search space. This increases the risk of methods focusing on zcausal (Figure 1a). This is evidenced in Figure 3, where StylEx and GS considerably perturb the character attribute. Interestingly, the perturbation pattern of DiVE shares some similarities with StylEx and GS due to gradient masking.\n\nExplainers exploit bad classifiers. As seen in Table 2 explainers are not significantly affected by the amount of spurious correlation zcorr introduced. This indicates that, in contrast with the oracle (IS), methods produce explanations by changing the font attribute zcausal (as seen in Figure 3) without changing the classifier’s prediction, thus creating a successful counterfactual (Eq. 5). Figure 6 shows some examples of methods achieving this by changing the accent mark, the umlauts and the circumflex of vowel characters (e.g., ô → ö). These counterfactuals expose failure cases of the classifier and are therefore useful, since they show that the classifier is unable to classify some characters. See A.4 (On causal counterfactuals).\n\nDiVE focuses on changing the font. As shown in Figure 3, DiVE perturbs almost exclusively the zcorr attribute (font), specially for high correlation values, this indicates that the method successfully distinguishes between zcausal and zcorr attributes. However, it is not able to consistently perturb the font in the right way to produce a diverse set of counterfactuals as evidenced by its score (Table 2).\n\nNon-triviality is not enough. Table 2 (right) shows the average percentage of trivial counterfactuals found by each method. We observe that methods that tend to produce a higher number of successful explanations (left) tend to also produce a larger number of trivial counterfactuals (right), which are discarded in our metric.\n\n5 DISCUSSION\n\nBenchmark In this work, we have introduced a more comprehensive definition of good counterfactual (Section 3) that we instantiate as a metric (Section 3.4) as well as a fair evaluation setup (Section 3.1) in the form of a benchmark. Previous evaluation setups use datasets like CelebA where the causal data generation process is unknown and use metrics that are easy to game. In contrast, our evaluation setup uses a more comprehensive and fair metric while providing control over the entire data generating process and therefore knowledge of the causal factors. Even though knowledge of causal factors is rare when working in real world scenarios, it is possible to adapt our metric to take only into account an orthogonal and complement set of estimator flips EF (Eq. 3) which do not require causal knowledge. However, any evaluation schema that does not include causal information would be incomplete. Further, if an explainer fails to provide a set of useful and diverse explanations for our synthetic dataset it is very unlikely that it is able to do so for real datasets. Nevertheless, we recommend users to also evaluate explainers using real world data. Oracle We argue that successful counterfactuals should be considered in the perspective of a human. In the absence of a human, we must resort to an optimal classifier, whose task is to contrast the predictions of the model with the optimal prediction and spot unexpected behaviors; acting as an oracle. Without an oracle, it is not clear how we could assess whether a model is working as intended. We show that the optimal classifier is commonly ill-defined in the image domain, because it is not always possible to access an invertible image generator (Section 3.3). Therefore, it cannot be expected that a classifier trained on pixel space achieves optimal performance. Results Our experimental results indicate that the different counterfactual explainers in the literature perform similarly and there has been little improvement in the recent years (Table 2). Although most of them find a single explanation in average, we found that they do it in different ways (Figure 3).\n\n6 CONCLUSION\n\nIn this paper we present a benchmark that provides unified metrics for evaluating different counterfactual explanation methods. The benchmark consists of synthetic images fully described by their annotated attributes which are accessible to the explainers through a differentiable generator. We hope this benchmark serves as an inspiration for developing future explainability methods.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n7 REPRODUCIBILITY\n\nIn an effort to ensure reproducibility we provide implementation details in A.3. The total run-time for all experiments is ~37 hours on a single Titan-X GPU. The code4 is written in PyTorch (Paszke et al., 2017) and is made public along with the datasets and pretrained weights for the models used in this work. Detailed documentation will be released upon publication.\n\nREFERENCES\n\nMartin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\n\narXiv preprint arXiv:1907.02893, 2019.\n\nSebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.\n\nRachana Balasubramanian, Samuel Sharpe, Brian Barr, Jason Wittenbach, and C Bayan Bruss. Latentcf: a simple baseline for reverse counterfactual explanations. arXiv preprint arXiv:2012.09301, 2020.\n\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\n\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\n\nVanessa Buhrmester, David Münch, and Michael Arens. Analysis of explainers of black box deep neural networks for computer vision: A survey. Machine Learning and Knowledge Extraction, 3 (4):966–989, 2021.\n\nNadia Burkart and Marco F Huber. A survey on the explainability of supervised machine learning.\n\nJournal of Artificial Intelligence Research, 70:245–317, 2021.\n\nChun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image\n\nclassifiers by counterfactual generation. arXiv preprint arXiv:1807.08024, 2018.\n\nEmily Denton, Ben Hutchinson, Margaret Mitchell, and Timnit Gebru. Detecting bias with generative\n\ncounterfactual face attribute augmentation. 2019.\n\nAndrew Elliott, Stephen Law, and Chris Russell. Explaining classifiers using adversarial perturbations on the perceptual ball. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10693–10702, 2021.\n\nRuth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal perturbations and smooth masks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2950–2958, 2019.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014a.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. arXiv preprint arXiv:1412.6572, 2014b.\n\nYash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual explanations. In International Conference on Machine Learning, pp. 2376–2384. PMLR, 2019.\n\nRiccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini, and Fosca arXiv preprint\n\nGiannotti. Local rule-based explanations of black box decision systems. arXiv:1805.10820, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\n4https://anonymous.4open.science/r/Bex-15A3\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nSimon Jégou, Michal Drozdzal, David Vazquez, Adriana Romero, and Yoshua Bengio. The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp. 11–19, 2017.\n\nShalmali Joshi, Oluwasanmi Koyejo, Been Kim, and Joydeep Ghosh. xgems: Generating examplars\n\nto explain black-box models. arXiv preprint arXiv:1806.08867, 2018.\n\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence and Statistics, pp. 2207–2217. PMLR, 2020.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\nSébastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Rémi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ica. In Conference on Causal Learning and Reasoning, pp. 428–484. PMLR, 2022.\n\nAlexandre Lacoste, Pau Rodríguez, Frédéric Branchaud-Charron, Parmida Atighehchian, Massimo Caccia, Issam Laradji, Alexandre Drouin, Matt Craddock, Laurent Charlin, and David Vázquez. Synbols: Probing learning algorithms with synthetic datasets. arXiv preprint arXiv:2009.06415, 2020.\n\nOran Lang, Yossi Gandelsman, Michal Yarom, Yoav Wald, Gal Elidan, Avinatan Hassidim, William T Freeman, Phillip Isola, Amir Globerson, Michal Irani, et al. Explaining in style: Training a gan to explain a classifier in stylespace. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 693–702, 2021.\n\nThibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin Detyniecki. Inverse classification for comparison-based interpretability in machine learning. arXiv preprint arXiv:1712.08443, 2017.\n\nYann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.\n\nShusen Liu, Bhavya Kailkhura, Donald Loveland, and Yong Han. Generative counterfactual introspection for explainable deep learning. In 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pp. 1–5. IEEE, 2019.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\n\nProceedings of International Conference on Computer Vision (ICCV), December 2015a.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\n\nProceedings of International Conference on Computer Vision (ICCV), December 2015b.\n\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pp. 4114–4124. PMLR, 2019.\n\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\n\npreprint arXiv:1608.03983, 2016.\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization.\n\narXiv preprint\n\narXiv:1711.05101, 2017.\n\nRamaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 607–617, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nMartin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual explanations for tabular data. In Proceedings of The Web Conference 2020, pp. 3126–3132, 2020.\n\nMartin Pawelczyk, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. Carla: A python library to benchmark algorithmic recourse and counterfactual explanation algorithms, 2021.\n\nJudea Pearl. Causality. Cambridge university press, 2009.\n\nEthan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135–1144, 2016.\n\nPau Rodríguez, Massimo Caccia, Alexandre Lacoste, Lee Zamparo, Issam Laradji, Laurent Charlin, and David Vazquez. Beyond trivial counterfactual explanations with diverse valuable explanations. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1056– 1065, October 2021.\n\nAxel Sauer and Andreas Geiger.\n\nCounterfactual generative networks.\n\narXiv preprint\n\narXiv:2101.06046, 2021.\n\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In International conference on machine learning, pp. 3145– 3153. PMLR, 2017.\n\nSumedha Singla, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich. Explanation by\n\nprogressive exaggeration. arXiv preprint arXiv:1911.00483, 2019.\n\nArnaud Van Looveren, Janis Klaise, Giovanni Vacanti, and Oliver Cobb. Conditional generative\n\nmodels for counterfactual explanations. arXiv preprint arXiv:2101.10123, 2021.\n\nFan Yang, Ninghao Liu, Mengnan Du, and Xia Hu. Generative counterfactuals for neural networks via attribute-informed perturbation. ACM SIGKDD Explorations Newsletter, 23(1):59–68, 2021.\n\nYilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution methods correctly attribute features? In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 9623–9633, 2022.\n\nA APPENDIX\n\nA.1 PROOF OF PROPOSITION 1\n\nProposition 1. If there exists a pair z, z′ s.t. g(z) = g(z′) and hcausal(z) ̸= hcausal(z′), then for any deterministic classifier ˆf (x), there is a prior p′(z) s.t. the accuracy of ˆf is 0 with respect to hcausal.\n\n(cid:110)\n\n(cid:12) (cid:12) (cid:12)\n\n ̃z\n\nˆf (g( ̃z)) ̸= hcausal( ̃z)\n\nand show that |S| ̸= 0. Since g(z) = g(z′), we have Proof. Let S = that ˆf (g(z)) = ˆf (g(z′)). Also, since hcausal(z) ̸= hcausal(z′), we have either ˆf (g(z)) ̸= hcausal(z) or ˆf (g(z′)) ̸= hcausal(z′). Finally, any prior p′(z) with no mass outside of S satisfies the proof.\n\n(cid:111)\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nAs an example, let’s consider a cube classifier from 2d projections of cubes. We can train a classifier to predict the correct class from the 3 visible faces. However, since the back of the cube is not visible there exists datasets such that the back of the cube is distorted in a way that makes it not a cube. The point being made here is that even though hcausal exists, fcausal does not always exists. Further assumptions needs to be made on which priors p(z) are valid for robustness to distribution shift, e.g., as humans, we make a symmetry assumption for classifying cubes. Perhaps future line of research in explainability should seek to find what assumptions are made by classifiers. In the rest of this work, we will make the assumption that if g(z) = g(z′), then hcausal(z) = hcausal(z′) for all pairs z, z′ 5\n\nThe proof is simple but it contrasts with the known result that hcausal is robust to any distribution shift over p(z).\n\nA.2\n\nINFORMATION OF THE EXPLANATIONS\n\nIn our work, we aim to improve the amount of information provided by explanation sets. In terms of information theory, unexpected events tend to be more informative. In this sense, repeating the same explanation multiple times provides little information and increasing diversity increases the amount of information. In addition, the amount of information of an event is usually measured with respect to some underlying probability distribution. If we know that a classifier is trained to detect a certain object, we expect the classifier to learn the conditional probability distribution (label given input) of the training set. Thus, deviations from this distribution are unexpected and informative. However, we do not know the real label for new samples and thus, we need to resort to some form of oracle or ground truth classifier in order to compare it with the model being explained. This ground truth classifier must have access to the real data generating process in order to infer the correct class from images, and the data generating process of image-label distributions is typically a causal process governed by the laws of physics. That is why we refer to this classifier as “causal classifier”.\n\nA.3\n\nIMPLEMENTATION DETAILS\n\nIn this section we will describe the training setting for the encoder q(z|x) the generator g(z) and the classifier to be explained ˆf (x), along with the settings for each of the explainers considered in this work.\n\nA.3.1 TRAINING DETAILS\n\nEncoder The encoder is based on BigGAN’s (Brock et al., 2018; Rodríguez et al., 2021) discriminator architecture with a classifier on top and it is trained on Synbols (Lacoste et al., 2020) dataset. Given an image x we task the encoder with predicting the attributes that describe it. It is trained for 100 epochs with a batch size of 64. We use AdamW (Loshchilov & Hutter, 2017) with a learning rate of 0.001 and a weight decay of 0.0001 with a cosine annealing learning rate scheduler(Loshchilov & Hutter, 2016). Since Synbols contains discrete and continuous attributes, we optimize them separately minimizing:\n\nLdiscrete + Lcontinuous\n\n(9)\n\nwhere Ldiscrete is the average cross entropy loss for each categorical attribute and Lcontinuous is the L1 distance between the original continuous attributes and the ones predicted.\n\nGenerator Similar to the encoder, the generator is also based on a BigGAN (Brock et al., 2018) architecture and trained with the same hyper-parameters and learning rate scheduler. Given a set of attributes z and the image x that they describe, we train the generator to reconstruct x from z. However, in order to provide a high-dimensional input to the generator instead of directly using the 7 Synbols attributes leveraged in our benchmark z ∈ R7 (Section 4.1), we use embedding layers to project each categorical attribute (character and font) into a 3-dimensional space. The embeddings are concatenated with the 5 continuous6 Synbols attributes obtaining z ∈ R11. We train the generator minimizing the following criteria:\n\n5This assumption could be relaxed by saying that the probability of this assertion being violated is unlikely\n\nunder a predefined set of valid distribution shifts.\n\n6We treat the background color as a continuous attribute\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Notation Table.\n\nNotation Description\n\nx y\nz zcausal z′ ˆf (x) ˆh(z)\n\nAn input image The label of the input image A set of latent variables A subset of z containing the causal parents A counterfactual explanation for z The classifier to be explained The latent classifier to be explained\n\nhcausal(z) The latent classifier to that is robust under change of p(z)\n\np(z) g(z) g(x|z) q(z|x)\n\nPrior distribution over z Deterministic image generator Stochastic image generator An encoder\n\ne(z, ˆh, g) A function that generates explanations\n\nLrec = α × ∥ x − x′ ∥1 + (1 − α) × ∥ q(x) − q(x′) ∥1 where α = 0.2, x and x′ are the original and reconstructed image respectively and q(x) are the learned encoder features for image x. Lastly, to reduce the amount of noise in the generated images we use a discriminator network D trained alongside the generator to distinguish between x and x′. Specifically the generator is trained every 3rd iteration. So, the criteria for the generator to optimize becomes:\n\n(10)\n\nLrec + log(1 − D(x′)) ∗ λ where λ = 0.01 and D(x′) is the discriminator’s estimate of the probability that the reconstructed image x′ is real.\n\n(11)\n\nClassifier The classifiers we set to explain are ResNet-18 (He et al., 2016) architectures trained on the different benchmarks described in Section 4.1. All the classifiers are trained for 10 epochs with a batch size of 256. We use AdamW with a learning rate of 0.01 and a weight decay of 0.0001 with a cosine annealing learning rate scheduler.\n\nA.3.2 EXPLAINER EVALUATION\n\nIn this section we will detail the hyper-parameters chosen for each of the methods analyzed in this work as well as some details regarding the benchmark. Given the varied nature of the explainers and their hyper-parameters we will refer to the works where the explainers are introduced when describing the effect the hyper-parameters have on each method. All of the hyper-parameters where chosen through random search.\n\nCommon setup Across our experiments there are a few settings that are common for all methods. To save time, instead of producing counterfactuals for the entire validation dataset we select a balanced subset with a total of 800 correctly and incorrectly classified samples with different levels of confidence. We do this by selecting the 100 samples closest to the required level of confidence ˆf (x) ∈ (±0.1, ±0.4, ±0.6, ±0.9).\n\nThe explainers are required to produce 10 counterfactuals per sample, if a method is originally conceived to produce only one, we follow (Mothilal et al., 2020) to create 10 samples close to the original sample in z space and task the method with producing an explanation for each of them. The attributes in z are standardized using the mean and standard deviation of the attributes in the training set. To prevent explainers from generating counterfactuals that the generator g(z) cannot interpret we clip every coordinate in z to the maximum and minimum values for each attribute found in the training set. We set the batch size to 12 across experiments.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 Generating an orthogonal set of counterfactuals\n\ndef orthogonal_set(z:np.ndarray, e_sc: np.ndarray, tau=0.15: float) -> np.ndarray: # d X n,d X 0 -> n,d\n\n\"\"\" Receives the latents of an original sample (z), and a set of successful counterfactuals (e_sc from Eq. 6) and returns an orthogonal set of counterfactuals (Eq. 7). Two samples z1 and z2 are orthogonal whenever abs(cos(z1, z2)) < tau. Two samples z1 and z2 are complementary whenever cos(z1, z2) + 1 < tau \"\"\" z = z[None, :] # d -> 1,d delta_sc = e_sc - z # compute perturbation vector delta_sc_norm = np.linalg.norm(delta_sc, 1, axis=1) # compute norm of perturbations indices = delta_sc_norm.argsort() delta_orth = delta_sc[None, indices[0]] # initialize set of orthogonal perturbations for i in indices[1:]:\n\nsim = cos(delta_sc[i], delta_orth) # calculate cosine similarity wrt all ellements in a set cond_orth = (np.abs(sim) < tau).all() # orthogonality condition cond_comp = (sim + 1 < tau).any() # complementary condition if cond_orth or cond_comp: # if condition satisfied, add to set\n\ndelta_orth = np.concatenate([delta_orth, delta_sc[None, i]], axis=0)\n\nreturn z + delta_orth # return set of orthogonal counterfactuals\n\nSince the categorical attributes in z space (font and character) are produced by embedding layers and projected into a 3-dimensional space each (Section A.3.1), they must be treated differently from the continuous attributes. When measuring if an explanation z′ is proximal to the original sample z we establish three different radius r values (see Eq. 2), one for the font, one for the character and one for the continuous attributes. For the character and font attributes we set r to be the maximum pairwise distance between the embedding representations of each attribute and for continuous attributes we set r = 1. Making this distinction allows the explainer to change from any given font/character to another while preventing it from modifying every attribute at once. When verifying if counterfactuals fulfill our orthogonality condition we look at perturbations between continuous attributes (z′ cont). However, for embedded attributes (font and character) we transform their embedded representation in z space into:\n\ncat = Softmax(∥ c′ − wi ∥2 / −t), ∀i ∈ [1..48] z′\n\n(12)\n\nwhere c′ is the representation of the categorical attribute character/font for a given counterfactual z′, w are the weights of the embedding layer used to map the 48 characters/fonts to their 3 dimensional representation c′ and t is a temperature factor set to 0.33. We choose the softmax function so that any small change in c′ that maps to a different categorical attribute can be orthogonal. In order to turn z′ cat into a perturbation vector we cancel the perturbation on the original attributes coordinate by setting:\n\ncat = z′ z′\n\ncat × (1 − 1c)\n\n(13)\n\nwhere 1 is a vector of 1s and 1c is a one-hot representation of the categorical attributes for the original sample. Thus given a sample z we verify that two counterfactual explanations z′ and z′′ are orthogonal by measuring cosine similarity: cos(z′ cont − z) for continuous attributes.\n\ncat) for categorical attributes and cos(z′\n\ncont − z, z′′\n\ncat, z′′\n\nGrowing Spheres (GS) (Laugel et al., 2017) For this method we found the best configuration was setting the initial radius η to 10 and the number of candidates n to 50. For a detailed description of the role of these hyper-parameters see (Laugel et al., 2017).\n\nStylEx (Lang et al., 2021) For this method we found the best configuration was setting threshold t to 0.3 using the “Independent” selection strategy and the amount of shift applied to each coordinate to 0.8. This last parameter is not mentioned in the Stylex (Lang et al., 2021) paper, but it can found as a parameter under the name shift_size in the implementation they provide. For a detailed description of the role of these hyper-parameters see (Lang et al., 2021).\n\nDiCE (Mothilal et al., 2020) For this method the best results were obtained by setting to learning rate to 0.1, the reconstruction weight of the loss function λ1 to 1 and the diversity weight of the loss function λ2 to 1. For a detailed description of the role of these hyper-parameters see (Mothilal et al., 2020). We set the maximum number of iterations to 50 in case the algorithm does not converge.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Comparison of perturbations performed in latent and pixel space. Note how the perturbations performed in latent space convey meaningful information (e.g., changes in the font or character), while the ones performed in pixel space resemble adversarial attacks and do not provide any valuable insights into the classifier’s reasoning.\n\nDiVE (Rodríguez et al., 2021) For this method we found the best configuration was setting the learning rate to 0.1, the weight of the proximity loss term to 0.0001, the factor that controls the sparsity of the latent space γ to 0.1 and the weight of the diversity loss to 0.001. For a detailed description of the role of these hyper-parameters see (Rodríguez et al., 2021). We set the maximum number of iterations to 50.\n\nxGEM (Joshi et al., 2018) This method shares some parameters with DiVE. We found the best configuration for this explainer was setting the learning rate to 0.1 and the weight of the proximity loss term to 0.001. We set the maximum number of iterations to 50.\n\nLatent-CF (Balasubramanian et al., 2020) For this method we set the learning rate to 0.1, the probability of target counterfactual class p to 0.1 and the tolerance tol to 0.5. For a detailed description of the role of these hyper-parameters see (Balasubramanian et al., 2020). We set the maximum number of iterations to 50 in case the algorithm does not converge.\n\nA.4 ADDITIONAL RESULTS\n\nIn this section we present some qualitative results in the form of counterfactuals generated by each method, along with extra quantitative results shown in Table 4. We also provide an example illustrating why explanations found in pixel space are uninterpretable (Figure 4).\n\nQuality over quantity As seen in Table 4 some explainers obtain a high percentage of successful counterfactuals SCE, sometimes even higher than the oracle (xGEM, DiVE). However, this is not\n\nTable 4: From left to right: We report the percentage of estimator flips EF (Eq. 3) (Joshi et al., 2018; Mothilal et al., 2020), percentage of successful counterfactuals SCE (Eq. 6) and what percentage of those are Causal Flips and Non-Causal Flips (see Section 3.4 and Eq. 6). Values represent an average score across batches for the entire dataset for 3 different runs.\n\nCorrelation\n\n#Spurious Explainer\n\n0.50\n\n0.95\n\n0.50\n\n0.95\n\n0.50\n\n0.95\n\n0.50\n\n0.95\n\nEF (%)\n\nSCE (%)\n\nCausal Flip Rate (%)\n\nNon-Causal Flip Rate (%)\n\n6\n\n10\n\nIS (Oracle) DiCE (Mothilal et al., 2020) DiVE (Rodríguez et al., 2021) GS (Laugel et al., 2017) Stylex (Lang et al., 2021) Latent-CF (Balasubramanian et al., 2020) xGEM (Joshi et al., 2018)\n\nIS (Oracle) DiCE (Mothilal et al., 2020) DiVE (Rodríguez et al., 2021) GS (Laugel et al., 2017) Stylex (Lang et al., 2021) Latent-CF (Balasubramanian et al., 2020) xGEM (Joshi et al., 2018)\n\n40.55 ±0.16 32.90 ±0.05 25.02 ±0.38 36.14 ±1.22 23.66 ±0.00 20.98 ±0.00 70.61 ±0.28\n\n35.33 ±0.16 31.77 ±0.45 22.39 ±0.31 37.38 ±0.38 24.20 ±0.00 22.00 ±0.00 66.45 ±0.63\n\n76.97 ±0.24 31.96 ±0.11 36.58 ±0.12 34.94 ±0.49 24.84 ±0.00 24.18 ±0.00 75.98 ±0.25\n\n71.19 ±0.08 33.25 ±0.23 31.8 ±0.25 36.38 ±0.54 23.04 ±0.00 23.33 ±0.00 74.95 ±0.30\n\n67.5 ±4.40 55.42 ±2.60 67.5 ±1.25 31.67 ±7.10 23.07 ±0.00 20.96 ±0.00 76.67 ±9.21\n\n54.50 ±2.43 45.00 ±4.33 60.00 ±1.25 44.58 ±12.52 23.65 ±0.00 21.98 ±0.00 61.67 ±5.20\n\n62.25 ±4.45 56.67 ±2.88 60.83 ±2.60 31.67 ±15.63 25.80 ±0.00 24.18 ±0.00 78.33 ±1.90\n\n51.25 ±1.25 39.17 ±0.72 54.58 ±0.72 40.42 ±5.90 23.77 ±0.00 23.24 ±0.00 62.92 ±5.90\n\n0.00 ±0.00 26.46 ±1.56 6.55 ±0.28 0.00 ±0.00 17.02 ±0.00 0.00 ±0.00 4.18 ±0.69\n\n0.00±0.00 26.16 ±3.43 6.83 ±0.57 0.00 ±0.00 21.60 ±0.00 0.00 ±0.00 4.51 ±0.35\n\n0.00 ±0.00 29.22 ±1.20 3.44 ±0.15 0.00 ±0.00 21.40 ±0.00 0.00 ±0.00 2.96 ±0.22\n\n0.00 ±0.00 27.65 ±0.79 2.25 ±0.01 0.00 ±0.00 20.35 ±0.00 0.00 ±0.00 2.15 ±0.22\n\n100 ±0.00 73.54 ±1.56 93.45 ±0.28 100 ±0.00 82.98 ±0.00 100 ±0.00 95.82 ±0.00\n\n100 ±0.00 73.84 ±3.43 93.17 ±0.57 100 ±0.00 78.40 ±0.00 100 ±0.00 95.49 ±0.00\n\n100 ±0.00 70.78 ±1.20 96.56 ±0.15 100 ±0.00 78.60 ±0.00 100 ±0.00 97.04 ±0.00\n\n100 ±0.00 72.35 ±0.79 97.75 ±0.01 100 ±0.00 79.65 ±0.00 100 ±0.00 97.85 ±0.27\n\n16\n\nLatent SpaceOriginalPixel SpaceUnder review as a conference paper at ICLR 2023\n\nreflected in their score S# (Table 2), which is considerably lower than the oracle’s. This is because even though the explainers can find a high number of counterfactuals they are discarded by our metric since they are not orthogonal or complementary and thus redundant. Further, note that the score measured using estimator flips EF (Eq. 3) (Joshi et al., 2018; Mothilal et al., 2020) is not correlated with our score S# (Table 2). For example, xGEM obtains a higher score than the oracle (IS) despite the latter returning a more complete set of explanations. This shows how previously proposed metrics (Joshi et al., 2018; Mothilal et al., 2020) can be gamed by explainers by generating many redundant explanations that fail to fully the describe the model’s behaviour. Figure 5, also supports this finding, showing how the performance of the oracle (IS) is the only one affected by the amount of correlated attributes and their level of correlation (see Section 4.1).\n\nOn causal counterfactuals As seen in Table 4, some methods produce a high amount of causal change counterfactuals i.e., they change zcausal and create a counterfactual that does not change the classifier’s prediction whilst changing the oracle’s prediction, exposing failure cases in the classifier. We find that DiCE (Mothilal et al., 2020) and StylEx (Lang et al., 2021) produce a high amount of these counterfactuals, while GS (Laugel et al., 2017) and Latent-CF (Balasubramanian et al., 2020) always change the classifiers prediction and thus produce none. The oracle (IS) is not designed to perturb zcausal in any way so it cannot produce any causal counterfactuals. Figure 6 shows two ways in which explainers achieve these counterfactuals. We can see that explainers that generate a high amount of causal counterfactuals (DiCE, StylEx) can be very “creative”, when modifying the character. Note that besides confusing the classifier by modifying the character’s diacritic, explainers can create new characters entirely by merging two letters together or even adding an accent mark to a consonant. Note that this behaviour is unavoidable in the absence of an optimal classifier.\n\nAdditional insights As seen in Table 2 explainers are unable to generate a diverse set of counterfactual explanations. However, Table 4 highlights some differences between methods when it comes to other metrics. If the objective is to maximize the number of estimator flips EF or the number of successful counterfactuals SCE we recommend using xGEM. If the objective is to maximize the number of causal flips we recommend using StylEx or DiCE. Note that explainers generate a high amount of redundant counterfactuals, and choosing them based on how they maximize these individual metrics is of little use.\n\nA.5 LIMITATIONS\n\nAs discussed in Section 3.3, the core limitation of explaining image classifiers via latent perturbations is the lack of accurate reversible generators. If the generator is not reversible, a given x can lead to many z, which prevents the direct recovery of z from x. It might be a good idea to bypass the\n\nFigure 5: Sensitivity of every explainer to varying amount of correlation levels (left) and number of spuriously correlated attributes (right) (see Section 4.1) measured with our score (Eq. 7). Note how the performance of the explainers, excluding the oracle (IS), does not scale and is not sensitive to the level of correlation and the amount of correlated variables. This supports our findings that explainers are unable to provide a diverse set of explanations and focus on changing the causal attribute zcausal.\n\n17\n\n46810# Spurious1.01.52.02.53.0CardinalityexplainerISdicedivegslcfstylexxgem0.100.250.500.700.95Correlation1.01.52.02.53.03.5CardinalityexplainerISdicedivegslcfstylexxgemUnder review as a conference paper at ICLR 2023\n\nFigure 6: Some of the Causal Flip counterfactuals (see Section 3.4 and Eq. 6) obtained by each method separated into two different subcategories.\n\npixel space completely and work directly on z, this however, would produce explanations outside the image domain and therefore, uninterpretable by humans, which is ultimately not very useful. It could be argued that the generator used in this work could be modified to yield better image reconstructions given any z, however this will always be hindered by the aforementioned limitation. More generally, most methods rely on some some sort of latent decomposition in order to search for counterfactuals in a latent space. However, it is still not clear how the true latent variables of the data generating process are not identifiable (Locatello et al., 2019). In this work we circumvent this problem by using a synthetic dataset. On the other hand, Khemakhem et al. (2020) showed that, with further assumptions, it is possible to identify the latent variables (Khemakhem et al., 2020). Moreover, in a temporal setup, it is possible to identify which of these latent variables are the causal ones (Lachapelle et al., 2022). Finally, in a multi-task setup where distribution shift occurs, it is possible to identify which variables are robust to distributions shift and hence, likely to be the causal ones. In summary, although it would be possible to approximate the true latent factors in some cases, it would require making additional assumptions about the data.\n\nWe make an effort to establish a fair, principled metric that is useful. However, this metric does not depict all the properties of an explainer such as fragility or speed. It is possible, albeit unlikely, that our definition of useful/informative explanation might not always align with that of the user. Lastly, the generator we use in this work can generate images with certain implicit biases. However, please note that our benchmark is a tool to evaluate properties of explainers that are impossible to evaluate without a synthetic setup.\n\nA.6 ETHICAL CONCERNS\n\nResearch in explainable AI is crucial for safe deployment of machine learning solutions in real life. In this work, we have shown that the lack of a principled evaluation of such systems has slowed-down advancements in the field. These findings should not discourage future research in explainable AI, on the contrary, we hope that our work unblocks the current state of the art and spurs further progress.\n\nOn the other hand, the research described in this paper does not (i) directly facilitate injury to living beings, (ii) raise safety, privacy or security concerns, (iii) raise human rights concerns, (iv) have a\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\ndetriment effect on people’s livelihood or economic security, (v) develop or extend harmful forms of surveillance, (vi) severely damage the environment, or (vii) deceive people in ways that cause harm.\n\nLastly, our research does not use human-derived data, or has involved extensive annotation by human research participants. The synthetic dataset used in this paper has not been deprecated for technical, legal, or ethical reasons.\n\n19",
    "reference": "# Summary Of The Paper\n\nThe authors propose a novel way to compare counterfactual explanation methods, which \"explain\" why a classifier made a certain prediction by highlighting nearby points in the domain with a different outcome. Their proposal involves a synthetic dataset in which the latent causal drivers of the outcome are \"known,\" and greedily constructing a set of explanations which are \"orthogonal\" or \"complementary\" to each other. This method is built from the premise that \"The goal for counterfactual generation methods is to find all the attributes that make a classifier behave differently from a causal classifier\" and \"a good explainer should return a ... diverse set of explanations.\" They use their method to benchmark existing methods for generating counterfactuals.\n\n# Strength And Weaknesses\n\nStrengths: The authors provide lots of helpful exposition on why they proposed the method in the way that they did, and are clear about their stance on what counterfactual explanations should be. The insights they draw at the end are interesting; for instance, \"explainers fail to consistently find more than one [attribute to alter to generate interesting counterfactuals].\"\n\nWeaknesses: The premise of the paper posits VERY strong assumptions on the *purpose* of counterfactual explanations: to be diverse, and to illustrate tensions with an optimal causal model. Though I don't deny these are *nice* qualities, I think their introduction could be clarified to assert *why* those properties are desirable (and why they did not consider other qualities). It may seem self explanatory, but really break it down: Why is it useful to know when a model is not acting \"causally\"??\n\nFurther, although their conclusions from applying their benchmark to explanation methods are interesting and believable, they seem to involve a bit of speculation, sometimes providing \"because\" explanations for the results they see that don't seem to have been directly experimentally validated. Sometimes, I felt the authors went off on tangents not totally necessary for the paper; for instance I am not sure why Proposition 1 is important.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe work is quite clear and novel. I did not attempt to use the code they say they posted.\n\n# Summary Of The Review\n\nI believe this work is interesting and worthy of publication, however I think it could use some polishing, and perhaps a redistribution of emphasis (i.e. providing more results in the body of the paper rather than delegating to the appendix and briefly summarizing them at the end)\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTHE ULTIMATE COMBO: BOOSTING ADVERSARIAL EXAMPLE TRANSFERABILITY BY COMPOSING DATA AUGMENTATIONS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nTransferring adversarial examples (AEs) from surrogate machine-learning (ML) models to evade target models is a common method for evaluating adversarial robustness in black-box settings. Researchers have invested substantial efforts to enhance transferability. Chiefly, attacks leveraging data augmentation have been found to help AEs generalize better from surrogates to targets. Still, prior work has explored a limited set of augmentation techniques and their composition. To fill the gap, we conducted a systematic study of how data augmentation affects transferability. Particularly, we explored ten augmentation techniques of six categories originally proposed to help ML models generalize to unseen benign samples, and assessed how they influence transferability, both when applied individually and when composed. Our extensive experiments with the ImageNet and CIFAR-10 dataset showed that simple color-space augmentations (e.g., color to greyscale) outperform the state of the art when combined with standard augmentations, such as translation and scaling. Additionally, except for two methods that may harm transferability, we found that composing augmentation methods impacts transferability monotonically (i.e., more methods composed → ≥transferability)—the best composition we found significantly outperformed the state of the art (e.g., 95.6% vs. 92.0% average transferability on ImageNet from normally trained surrogates to other normally trained models). We provide intuitive, empirically supported explanations for why certain augmentations fail to improve transferability.\n\n1\n\nINTRODUCTION\n\nAdversarial examples (AEs)—variants of benign inputs minimally perturbed to induce misclassification at test time—have emerged as a profound challenge to machine learning (ML) (Biggio et al., 2013; Szegedy et al., 2014), calling its use in security- and safety-critical systems into question (e.g., Eykholt et al. (2018)). Many attacks have been proposed to generate AEs in white-box settings, where adversaries are familiar with all the particularities of the attacked model (Papernot et al., 2016). By contrast, black-box attacks enable evaluating the vulnerability of ML in realistic settings, without access to the model (Papernot et al., 2016).\n\nAttacks exploiting the transferability-property of AEs (Szegedy et al., 2014) have received special attention. Namely, as AEs produced against one model are often misclassified by others, transferability-based attacks produce AEs against surrogate (a.k.a. substitute) white-box models to mislead black-box ones. To measure the risk of AEs in black-box settings accurately, researchers have proposed varied methods to enhance transferability (e.g., Lin et al. (2020); Liu et al. (2017)).\n\nNotably, attacks using data augmentation, such as translations (Dong et al., 2019) and scaling of pixel values (Lin et al., 2020), as a means to improve the generalizability of AEs across models have accomplished state-of-the-art transferability rates. Still, previous transferability-based attacks have studied only four augmentation methods (see Section 3.1), out of many proposed in the dataaugmentation literature (Shorten & Khoshgoftaar, 2019), primarily for reducing model overfitting. Hence, the extent to which different data-augmentation types boost transferability, either individually or when combined, remains largely unknown.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nTo fill the gap, we conducted a systematic study of how augmentation methods influence transferability. Specifically, alongside techniques considered in previous work, we studied how ten augmentation techniques pertaining to six categories impact transferability when applied individually or composed (Section 3). Integrating augmentation methods into attacks via a flexible framework we propose (Algorithm 1), we conducted extensive experiments using an ImageNet-compatible dataset, CIFAR-10 (Krizhevsky, 2009), and 16 models, and measured transferability in diverse settings, including with and without defenses (Sections 4 and 5). Our results offer several interesting insights:\n\n• Simple color-space augmentations outperform state-of-the-art transferability-based attacks when\n\ncomposed with standard augmentations (Section 5.1).\n\n• Transferability has a mostly monotonic relationship with data-augmentation techniques. Except for two augmentation methods that may harm transferability, composing additional augmentation methods either improves of preserves transferability (Section 5.2).\n\n• Out of 27 compositions explored, the best composition we found, ULTIMATECOMBO, outperforms\n\nstate-of-the-art attacks by a large margin (Section 5.3).\n\n• We show empirical support to conjectures we raise concerning when data-augmentation tech-\n\nniques may be counterproductive to transferability (Section 5.4).\n\n2 BACKGROUND AND RELATED WORK\n\nEvasion Attacks Many evasion attacks assume adversaries have white-box access to models—i.e., adversaries know models’ architectures and weights (e.g., Goodfellow et al. (2015); Szegedy et al. (2014); Carlini & Wagner (2017)). These typically leverage first- or second-order optimizations to generate AEs models would misclassify. For example, given an input x of class y, model weights θ, and a loss function J, the Fast Gradient Sign method (FGSM) of Goodfellow et al. (2015), crafts an AE ˆx using the loss gradients ∇xJ(x, y, θ):\n\nˆx = x + (cid:15) ∗ sign(∇xJ(x, y, θ))\n\nwhere sign(·) maps real numbers to -1, 0, or 1, depending on their sign. Following FGSM, researchers proposed numerous advanced attacks. Notably, iterative FGSM (I-FGSM) of Kurakin et al. (2017b) performs multiple gradient-ascent steps, updating ˆx iteratively to evade models:\n\nˆxt+1 = Proj(cid:15)\n\nx\n\n(cid:18)\n\nˆxt + α · sign\n\n(cid:16)\n\n∇xJ (ˆxt, y, θ)\n\n(cid:17)(cid:19)\n\nwhere Proj(cid:15) ˆx0 = x. The attacks we study in this work are based on I-FGSM.\n\nx(·) projects the perturbation into (cid:96)∞-norm (cid:15)-ball centered at x, α is the step size, and\n\nIn practice, adversaries often lack white-box access to victim models. Hence, researchers studied black-box attacks in which adversaries may only query models. Certain attack types, such as scoreand boundary-based attacks perform multiple queries, often around several thousands, to produce AEs (e.g., Brendel et al. (2018); Ilyas et al. (2019)). By contrast, attacks leveraging transferability (e.g., Goodfellow et al. (2015); Szegedy et al. (2014)) avoid querying victim models, and use surrogate white-box models to create AEs that are likely misclassified by other black-box ones.\n\nAttempts to explain the transferability phenomenon attribute it to gradient norm of the target model (i.e., its susceptibility to attacks), the smoothness of classification boundaries, and, primarily, the alignment of gradient directions between the surrogate and target models (Demontis et al., 2019; Yang et al., 2021). Said differently, for AEs to transfer, the gradient directions of surrogates need to be similar to those of target models (i.e., attain high cosine similarity).\n\nEnhancing transferability is an active research area. Some methods integrate momentum into attacks such as I-FGSM to avoid surrogate-specific optima and saddle points that may hinder transferability (e.g., Dong et al. (2018); Wang & He (2021)). Others employ specialized losses, such as reducing the variance of intermediate activations (Huang et al., 2019) or the mean loss of model ensembles (Liu et al., 2017), to enhance transferability. Lastly, a prominent family of attacks leverages data augmentation to enhance AEs’ generalizability between models. For instance, Dong et al. (2019) boosted transferability by integrating random translations into I-FGSM. Evasion attacks incorporating data augmentation attain state-of-the-art transferability rates (Lin et al., 2020; Wang et al.,\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 MI-FGSM with data augmentation Input: Benign sample x; ground-truth label y; loss function J(·); model parameters θ; iterations # T ; momentum parameter μ; perturbation size (cid:15); data-augmentation method D(·). Output: Adversarial example ˆx\n\n1: α = (cid:15)/T 2: ˆx0 = x 3: g0 = 0 4: for t = 0 to T − 1 do (cid:80)m−1 ̄gt+1 = 1 5: gt+1 = μ · gt + ̄gt+1 ˆxt+1 = Proj(cid:15)\n\n7:\n\n6:\n\nm\n\nx\n\ni=0 ∇x (J (D(ˆxt)i, y, θ))\n\n(cid:107) ̄gt+1(cid:107)1\n\n(cid:0)ˆxt + α · sign (gt+1) (cid:1)\n\n# Initialize adversarial example # Initialize momentum\n\n# Expected loss gradient on augmented samples\n\n# Gradient with momentum\n\n# Update adversarial example\n\n8: return ˆx = ˆxT\n\n2021a). Nonetheless, prior work has only considered a restricted set of augmentation methods for boosting transferability. By contrast, we aim to investigate the role of data augmentation at enhancing transferability more systematically, by exploring how a more comprehensive set of augmentation types and their compositions affect transferability.\n\nDefenses Various defenses have been proposed to mitigate evasion attacks. Adversarial training—a procedure integrating correctly labeled AEs in training—is one of the most practical and effective methods for enhancing adversarial robustness (e.g., Goodfellow et al. (2015); Tram`er et al. (2018)). Other defense methods sanitize inputs prior to classification (e.g., Guo et al. (2018)); attempt to detect attacks (see Tramer (2022)); or seek to certify robustness in (cid:15)-balls around inputs (e.g., Cohen et al. (2019); Salman et al. (2019)). Following standard practices in the literature (Wang et al., 2021a), we evaluate transferability-based attacks against a representative set of these defense.\n\n3 DATA AUGMENTATION FOR ENHANCING TRANSFERABILITY\n\nData augmentation is traditionally used in training, to reduce overfitting and improve generalizability (Shorten & Khoshgoftaar, 2019). Inspired by this use, transferability-based attacks adopted data augmentation to limit overfitting to surrogate models and produce AEs likely to generalize and be misclassified by victim models. Algorithm 1 depicts a general framework for integrating data augmentation into I-FGSM with momentum (MI-FGSM). In the framework, a method D(·) augments the attack with m variants of the estimated AE at each iteration. Consequently, the adversarial perturbation found by the attack increases the expected loss over transformed counterparts of the benign sample x (i.e., the distribution set by D(·) given x). Note that D(·)’s output may include x.\n\nThe framework in Algorithm 1 is flexible, and can admit any data-augmentation method. We use it to describe previous attacks employing data augmentation and to systematically explore new ones. Next, we detail previous attacks, describe data augmentation methods we adopt for the first time to enhance transferability, and explain how these can be combined for best performance.\n\n3.1 PREVIOUS ATTACKS LEVERAGING DATA AUGMENTATION\n\nPrevious work explored the following augmentation methods to set D(·).\n\nTranslations Using random translations of inputs, Dong et al. (2019) proposed a translationinvariant attack to promote transferability. They also offered an optimization to reduce the attack’s time and space complexity by simply convolving the model’s gradients (w.r.t. non-translated inputs) with a Gaussian kernel. While we use this optimization in the implementation for the interest of efficiency, we highlight that the attack can be well-captured by our framework.\n\nDiverse Inputs Xie et al. (2019) proposed a size-invariant attack. Their augmentation procedure samples random crops from ˆxt that are later resized per the model’s input dimensionality.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nScaling Pixels Lin et al. (2020) showed that adversarial perturbations invariant to scaling pixel In their case, D(·) values transfer with higher success between deep neural networks (DNNs). produces m samples such that D(x)i = x\n\n2i for i ∈ {0, 1, ..., m − 1}, where m=5 by default.\n\nAdmix Wang et al. (2021a) assumed that the adversary has a gallery of images from different classes and adopted augmentations similar to MixUp (Zhang et al., 2018a). For each sample x(cid:48) from the gallery, Admix augments attacks with m (typically set to 5) samples, such that D(x, x(cid:48))i = 1 2i · (ˆxt + η · x(cid:48)), where i ∈ {0, 1, ..., m − 1}, and η ∈ [0, 1] is set to 0.2 by default. Notably, Admix degenerates to pixel scaling when η = 0.\n\nThe leading transferability-based attacks compose (1) diverse inputs, scaling, and translations (Lin et al.’s (2020) DST-MI-FGSM attack, and Wang & He’s (2021) DST-VMI-FGSM attack that also tunes the gradients’ variance); or (2) Admix, diverse inputs, and translation (Wang et al.’s (2021a) Admix-DT-MI-FGSM attack). We describe how the compositions operate in Section 3.3.\n\n3.2 NEW AUGMENTATIONS FOR ENHANCING TRANSFERABILITY\n\nWhile prior work studied the effect of spatial transformations (i.e., translations and diverse inputs), pixel scaling, and mixing on transferability, a substantially wider range of data-augmentation methods exist. Yet, the impact of these on transferability remains unknown. To fill the gap, we examined Shorten & Khoshgoftaar’s (2019) survey on data augmentation for reducing overfitting in deep learning and identified ten representative methods of six categories that may boost transferability. We present them in what follows, one category at a time.\n\nColor-space Transformations Potentially the simplest of all augmentation types are those applied in color-space. Given images represented as three-channel tensors, methods in this category manipulate pixel values only based on information encoded in the tensors. We evaluate four color-space transformations. First, we consider color jitter (CJ), which applies random color manipulation (Wu et al., 2015). Specifically, we consider random adjustments of pixel values within a pre-defined range in terms of hue, contrast, saturation, and brightness around original values. Second, we evaluate fancy principle component analysis (fPCA). Used in AlexNet (Krizhevsky et al., 2017), fPCA adds noise to the image proportionally to the variance in each channel. Given an RGB image, fPCA adds the following quantity to each image pixel:\n\n[p1, p2, p3] [α1λ1, α2λ2, α3λ3]T ,\n\nwhere pi and λi are the ith eigenvector and eigenvalue of the of 3× 3 covariance matrix of RGB pixels, respectively, and αi is sampled once per image from Gaussian distribution N (0, 0.1). Third, we test channel shuffle (CS). Included in ShuffleNet training (Zhang et al., 2018b), CS simply swaps the orders of the image’s RGB channels at random. Last, but not least, we consider greyscale (GS) augmentations. This simple augmentation converts images into greyscale (replicating it three times to obtain an RGB representation). Mathematically, the conversion is calculated by ωR · xR + ωG · xG + ωB · xB, where xR, xG, and xB, correspond to the RGB channels, respectively, and ωR, ωG, and ωB, all ∈ [0, 1], denote the channel weights, and sum up to 1.\n\nRandom Erasing Inspired by dropout regularization, random erasing (RE) helps ML models focus on descriptive features of images and promote robustness to occlusions (Zhong et al., 2020). To do so, randomly selected rectangular regions in images are replaced by masks composed of random pixel values. Similarly to RE, CutOut masks out regions of inputs to improve DNNs’ accuracy (DeVries & Taylor, 2017). The main difference from e is that CutOut uses fixed masking values, and may perform less aggressive masking when selected regions lie outside the image.\n\nKernel Filters Convolving images with kernels of different types can produce certain effects, such as blurring (via Gaussian kernels), sharpening (via edge filters), or edge enhancement. We study the effect of sharpening (Sharp) on transferability with edge-enhancement filters.\n\nMixing Images As a form of vicinal risk minimization, some augmentation methods mix images together, creating virtual examples for training. MixUp, the cornerstone behind Admix, computes weighted sums of images (Zhang et al., 2018a). By contrast, we consider CutMix, which replaces a region within one image with a region from another image picked from a gallery (Yun et al., 2019).\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nNeural Transfer Augmentations using neural transfer (NeuTrans) preserve image semantics while changing their style. We use Gatys et al.’s (2015) generative model to transfer image styles to that of Picasso’s 1907 self-portrait.\n\nMeta-learning-inspired Augmentations Meta-learning is a subfield of ML studying how ML algorithms can optimize other learning algorithms (Hospedales et al., 2021). In the context of data augmentation, algorithms such as AutoAugment have been proposed to train controllers to select an appropriate augmentation method to avoid overfitting (Cubuk et al., 2019). We use the pre-trained AutoAugment controller, encoded as a recurrent neural network, to select augmentation methods and their magnitude from a set of 13 augmentation methods.\n\n3.3 COMPOSING AUGMENTATIONS\n\nThere are two ways to compose data-augmentation methods in attacks, namely: parallel and serial composition. Figure 1 in Appendix A illustrates both. In parallel composition, each augmentation method is applied independently on the input, and their outputs are aggregated by taking their union to augment attacks (i.e., as D(·)’s output). By contrast, serial composition applies augmentation methods sequentially, one after the other, where the first method operates on the original sample, and each subsequent augmentation function operates on its predecessor’s outputs. Consequently, serial composition leads to an exponential growth in the number of samples, while parallel composition leads to a linear growth. DST-MI-FGSM and Admix-DT-MI-FGSM use serial composition. By contrast, we consider a substantially larger number of augmentation methods, which may lead to prohibitive memory and time requirements in the case of serial composition. Additionally, because the order of applying certain augmentations matters (e.g., GS then CutMix leads to different outcome that CutMix followed by GS), exploring a meaningful number of serial compositions (out of an order of 10! possibilities) becomes virtually impossible. Accordingly, we mainly consider parallel composition between data-augmentation methods. We only serially compose translations, scaling, and diverse inputs, for consistency with prior work (e.g., Wang et al. (2021a)). We tested a few serial compositions between new augmentation methods we consider and found they were significantly outperformed by their parallel counterparts. While non-exhaustive, this hints that serially composing augmentations may not be a promising direction for enhancing transferability.\n\n4 EXPERIMENTAL SETUP\n\nNow we turn to the setup of our experiments, including the data, models, and attack configurations.\n\nData We used an ImageNet-compatible dataset1 and CIFAR-10 for evaluation, per common practice (e.g., (Dong et al., 2019; Yang et al., 2021)). The former contains 1,000 images, originally collected for the NeurIPS 2017 adversarial ML competition. For the latter, we sampled 1,000 images, roughly balanced between classes, from the test set.\n\nModels We used 16 DNNs to transfer attacks from (as surrogates) and to (as targets)—six for CIFAR-10 and ten for ImageNet. All CIFAR-10 models and six of the ImageNet models were normally trained, while the other four ImageNet models were adversarially trained. To facilitate comparison with prior work, we included models that are widely used for assessing transferability (e.g., (Wang et al., 2021a; Yang et al., 2021)). Furthermore, to ensure that our findings are general, we included models covering varied architectures, including Inception, ResNet, VGG, DenseNet, and MobileNet. Appendix B provides more details about the models.\n\nAttack Parameters We tested standard attack configurations, in line with prior work (Wang et al., 2021a; Yang et al., 2021). Namely, we evaluated untargeted MI-FGSM-based attacks, bounded in (cid:96)∞-norm. We validated findings with varied perturbation norms. For ImageNet, unless stated otherwise, we tested (cid:15) = 16 255 , but also experimented with (cid:15) ∈ { 8 255 }. For CIFAR-10, we experimented with (cid:15) ∈ {0.02, 0.04}. We quantified attack success via transferability rates—the percentages of attempts at which AEs created against surrogates were misclassified by victims. As baselines, we used three state-of-the-art transferability-based attacks: DST-MI-FGSM, DST-VMIFGSM, and Admix-DT-MI-FGSM (see Section 3.1). Appendix C reports the parameters used in\n\n255 , 24\n\n1https://bit.ly/3fq4pN6\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nattacks and augmentation methods. Appendix D discusses attacks we considered but excluded from experiments.\n\n5 EXPERIMENTAL RESULTS\n\nThis section summarizes our findings. We start by evaluating individual augmentation methods and standard combinations with scaling, diverse inputs, and translations (Section 5.1). We then turn to analyzing all possible compositions between different augmentation types to assess whether transferability typically improves when considering additional augmentations (Section 5.2). Our analysis helped us identify the best performing composition for boosting transferability, denoted by ULTIMATECOMBO, outperforming state-of-the-art attacks. Section 5.3 reports rigorous comparisons between ULTIMATECOMBO and the baselines, including against defended models. Finally, we help develop intuition for when augmentations may or may not help improve transferability (Section 5.4).\n\n5.1 COLOR-SPACE AUGMENTATIONS SIGNIFICANTLY ADVANCE THE STATE OF THE ART\n\nInitially, we evaluated transferability integrating a single augmentation at a time in attacks, or when composing individual augmentations with diverse inputs, scaling, and translation (DST), as is standard (Lin et al., 2020; Wang et al., 2021a). We found that considering each of the ten augmentations individually does not lead to competitive performance with the baselines (Table 9 in Appendix E). However, composing individual augmentations with DST enhanced transferability markedly (Table 10 in Appendix E). Surprisingly, augmentations in color-space fared particularly well, outperforming the baselines and advanced augmentation methods (e.g., AutoAugment) in most cases.\n\nComposing GS with DST (GS-DST-MI-FGSM attack) performed best in this setting. Table 1 reports the transferability rates from four normally trained models to other models on ImageNet (see It can be immediately seen that GS-DST-MI-FGSM Table 11 in Appendix F for more details). attains higher transferability than the baselines (93.6% vs. ≤92.0%, on avg.). This held also when considering different perturbation norms on ImageNet, where GS-DST-MI-FGSM outperformed the baselines with sometimes higher margin (e.g., 75.9% vs. ≤70.8% on avg. with (cid:15) = 8 255 ; see Table 13 in Appendix F). GS-DST-MI-FGSM also outperformed the baselines on CIFAR-10, when transferring AEs to normally trained DNNs of different architectures, with perturbation norms (cid:15)=0.02 (74.9% vs. ≤71.5% avg. transferability rate) and (cid:15)=0.04 (92.1% vs. ≤89.6% avg. transferability rate). Tables 14 and 15 in Appendix G show the detailed CIFAR-10 results.\n\nModel\n\nAttack\n\nInc-v3\n\nInc-v4 Res-50 Res-101 Res-152\n\nIncRes-v2\n\nInc-v3\n\nInc-v4\n\nRes-101\n\nIncRes-v2\n\nMAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO MAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO MAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO MAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO\n\n100.0 100.0 100.0 95.3 96.5 98.1 88.3 89.0 93.0 95.8 96.5 98.2\n\n94.7 95.6 98.0 100.0 100.0 99.9 85.0 84.8 90.4 94.7 95.6 97.1\n\n90.7 93.7 95.1 91.0 94.1 94.8 97.6 97.6 98.1 94.0 95.5 96.3\n\n88.9 91.8 94.3 89.9 92.5 95.0 99.9 99.8 99.7 92.9 94.2 96.5\n\n89.1 90.9 92.7 88.4 93.0 94.6 96.9 97.7 97.8 92.9 94.7 95.7\n\n92.6 94.9 97.1 93.5 95.4 96.8 87.2 87.6 91.8 99.8 100.0 100.0\n\nTable 1: Transferability rates (%) on ImageNet, from normally trained surrogates (rows) to normally trained targets (columns). All attacks are black-box, except for when the surrogate and target models are the same. MAXBASELINE is the best performing of the three baselines.\n\nThe same trends held when transferring AEs to adversarially trained models. Here, we transferred AEs from individual, normally trained DNNs (Table 2 reports a summary, and Table 12 shows complete results), as well as an ensemble of DNNs (Table 3) used to boost transferability further (Liu et al., 2017), finding that GS-DST-MI-FGSM attained better transferability than the baselines. Overall, according to a paired t-test, the differences between GS-DST-MI-FGSM and the baselines across different surrogate and target models were statistically significant (p <0.01).\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nAttack\n\nInc-v3adv\n\nInc-v3ens3\n\nInc-v3ens4\n\nIncRes-v2ens\n\nInc-v3\n\nInc-v4\n\nRes-101\n\nIncRes-v2\n\nMAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO MAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO MAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO MAXBASELINE GS-DST-MI-FGSM ULTIMATECOMBO\n\n84.6 87.3 88.2 84.3 87.6 88.6 82.0 82.3 83.5 89.0 91.1 92.2\n\n84.8 88.5 88.7 86.0 89.5 89.4 83.0 83.7 86.7 89.0 92.2 92.6\n\n83.5 85.5 86.7 83.0 87.2 88.4 80.9 81.1 82.8 88.7 90.0 92.0\n\n70.8 72.2 72.6 74.6 78.0 78.2 72.5 73.2 76.8 87.1 88.2 88.5\n\nTable 2: Transferability rates (%) on ImageNet, from normally trained surrogates (rows) to adversarially trained targets (columns). MAXBASELINE is the best performing of the three baselines.\n\nAttack\n\nInc-v3adv\n\nInc-v3ens3\n\nInc-v3ens4\n\nIncRes-v2ens\n\nDST-MI-FGSM Admix-DT-MI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n89.0 90.1 92.4 93.6\n\n90.0 90.5 93.5 95.2\n\n87.6 89.4 92.5 93.7\n\n82.4 84.7 88.7 91.2\n\nTable 3: Transferability rates (%) on ImageNet, from an ensemble of normally trained surrogates (containing Inc-v4, Res-50, Res-101 and Res-152) to adversarially trained target models. DST-VMIFGSM was excluded due to resource constrains.\n\nFinally, we evaluated attack run-times, finding that, despite investing no effort to improve its efficiency, GS-DST-MI-FGSM is at least ×1.14 more time-efficient than Admix-DT-MI-FGSM and DST-VMI-FGSM, on avg. (Table 16 in Appendix I). Still, we denote that, since transferabilitybased attacks generate AEs offline, and only once per surrogate model, as long as an attack is not prohibitively slow, attack run-time is a marginal consideration for selecting an attack compared to transferability rates.\n\n5.2 THE MONOTONICITY OF TRANSFERABILITY WHEN ADDING AUGMENTATIONS\n\nWe wanted to evaluate whether transferability is monotonic in the number of augmentation types considered—i.e., whether composing more techniques increases, or at least does not harm, transferability. To this end, we selected the best performing augmentation method of each of the six categories presented in Section 3.2 as well as DST-MI-FGSM, and evaluated all 27 (=128) compositions possible (per Section 3.3). More precisely, we tested every possible combination of GS, CutOut, Sharp, NeuTrans, AutoAugment, Admix, and DST-MI-FGSM. Given a composition, we produced AEs against the Inc-v3 ImageNet DNN as surrogate, and computed the expected transferability rate against all other nine ImageNet DNNs, both normally and adversarially trained. Then, for every pair of attacks differing only in whether a single augmentation method was incorporated in the composition, we tested whether adding the augmentation method improved transferability.\n\nThe results reflected a mostly monotonic relationship between transferability and augmentations. Except for NeuTrans and Sharp, which sometimes harmed transferability when considered within a composition, adding augmentation method increased or preserved transferability. Figure 2 in Appendix H summarizes the results. Notably, comparing all compositions enabled us to find that a composition of all seven augmentation methods except for NeuTrans attained the best transferability. We call this composition the ULTIMATECOMBO.\n\n5.3 THE MOST EFFECTIVE COMBINATION\n\nWe evaluated ULTIMATECOMBO extensively, testing transferability to normally and adversarially trained DNNs. As shown in Table 1, ULTIMATECOMBO obtained higher transferability to normally\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\ntrained models than the baselines (95.6% vs. ≤92.0% avg. transferability) and GS-DST-MI-FGSM, when normally trained models were used as surrogates. This holds across different values of (cid:15) (Table 13), and on the CIFAR-10 dataset with different architectures (Tables 14 and 15).\n\nFurthermore, ULTIMATECOMBO achieved the best performance also when transferring attacks from normally trained to adversarially trained DNNs (Table 2; 86.0% vs. ≤82.7% avg. transferability). Transferring AEs crafted by ULTIMATECOMBO using an ensemble of models increased transferability further (Table 3; 93.4% avg. transferability). Per a paired t-test, the differences between ULTIMATECOMBO and the baselines over all pairs of surrogates and targets considered are statistically significant (p <0.01).\n\nBesides adversarially trained models, we evaluated ULTIMATECOMBO’s transferability against five defenses. Two defenses, bit reduction (Bit-Red) (Xu et al., 2018) and neural representation purification (NRP) (Naseer et al., 2020), transform inputs to sanitize adversarial perturbations. Two others, randomized smoothing (RS) (Cohen et al., 2019) and randomized smoothing with adversarial training (ARS) (Salman et al., 2019) offer provable robustness guarantees. Finally, TRS leverages an ensemble of smooth DNNs trained to have misaligned gradients, to defend attacks (Yang et al., 2021). We evaluated all defenses except for TRS on ImageNet. We used the defenses with default parameters (see Appendix J), and transferred AEs crafted against an ensemble of normally trained models. Results are shown in Table 4. Similar to other settings, here too, ULTIMATECOMBO outperformed the baselines (66.8% vs. ≤63.9% avg. transferability). Following Yang et al. (2021), we tested TRS on CIFAR-10 with adversarial perturbation norms (cid:15) ∈ {0.02, 0.04}. ULTIMATECOMBO did best against this defense as well (Table 5).\n\nAttack\n\nBit-Red NRP\n\nRS ARS\n\nDST-MI-FGSM Admix-DT-MI-FGSM ULTIMATECOMBO\n\n85.3 86.4 87.5\n\n40.7 39.4 47.7\n\n84.2 86.6 88.4\n\n39.8 43.0 43.5\n\nTable 4: Transferability rates (%) from an ensemble of normally trained surrogates (Inc-v4, Res50, Res-101 and Res-152) to models defended by provable methods or input transformations. DSTVMI-FGSM was excluded due to resource constrains.\n\nEpsilon\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n0.02 0.04\n\n21.5 36.2\n\n23.1 41.3\n\n18.9 36.3\n\n25.1 47.8\n\n27.4 49.4\n\nTable 5: Transferability rates (%) on CIFAR-10 from a normally trained VGG surrogate DNN to an ensemble of Res DNNs trained via TRS.\n\nLastly, due to composing more augmentations, ULTIMATECOMBO is slower than DST-MI-FGSM, GS-DST-MI-FGSM, and Admix-DT-MI-FGSM. However, it is ×2.44 faster than DST-VMI-FGSM at producing AEs (Table 16).\n\n5.4 WHEN DO AUGMENTATIONS FAIL TO IMPROVE TRANSFERABILITY?\n\nWhile augmentation methods mostly increased transferability, in some cases they were counterproductive. Particularly, NeuTrans and Sharp decreased transferability when composed with certain methods. We conducted simple experiments as a preliminary assessment of two conjectures we had concerning when augmentations may harm transferability.\n\nFirst, we expected augmentation methods that harm model accuracy on benign samples to be less conducive for transferability. As DNNs do not generalize well to benign samples produced by these augmentation methods, we anticipated that adversarial perturbations relying on the augmented samples would also have limited generalizability across models. To support the conjecture, we tested the normally trained DNNs’ accuracy on benign samples transformed by each augmentation method. As can be seen from Table 6, NeuTrans and Sharp, which often decrease transferability (Section 5.2 and Figure 2), harmed the DNN accuracy the most (6.5%–58.7% lower accuracy than other methods), supporting our conjecture.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nAugmentation\n\nInc-v3\n\nInc-v4 Res-50 Res-101 Res-152\n\nIncRes-v2 Avg.\n\nNone CS fPCA CJ Admix CutOut GS AutoAugment Sharp NeuTrans\n\n96.2 94.0 91.6 90.0 86.7 86.5 86.6 82.9 69.5 24.4\n\n97.4 95.7 96.8 92.3 91.6 89.2 90.3 86.2 87.3 25.4\n\n94.5 95.3 89.9 90.3 86.8 85.7 84.7 82.1 71.5 24.2\n\n96.3 94.6 92.8 90.3 88.9 87.2 87.6 84.3 76.7 27.0\n\n95.8 95.4 93.6 91.4 89.7 88.6 86.5 84.4 75.5 24.0\n\n99.8 99.5 99.4 96.8 94.6 92.3 92.7 89.8 90.6 32.7\n\n96.7 95.8 94.0 91.9 89.7 88.2 88.1 85.0 78.5 26.3\n\nTable 6: Benign accuracy (%) after applying data augmentation methods. Rows are sorted in a descending order of average transferability.\n\nAugmentation\n\nInc-v4 Res-50 Res-101 Res-152\n\nIncRes-v2\n\nAvg.\n\nCutOut CS None Admix CJ GS AutoAugment fPCA NeuTrans Sharp\n\n0.568 0.565 0.564 0.563 0.560 0.559 0.558 0.560 0.546 0.548\n\n0.583 0.578 0.575 0.575 0.575 0.572 0.569 0.568 0.556 0.548\n\n0.581 0.576 0.573 0.573 0.573 0.569 0.567 0.566 0.554 0.545\n\n0.574 0.570 0.568 0.567 0.568 0.563 0.562 0.561 0.549 0.540\n\n0.591 0.590 0.586 0.586 0.584 0.582 0.579 0.578 0.565 0.558\n\n0.579 0.576 0.573 0.573 0.572 0.569 0.567 0.567 0.554 0.548\n\nTable 7: Cosine similarities between gradients of benign images computed on Inc-v3 after applying augmentation methods composed with DST-MI-FGSM, and gradients of other normally trained models on benign images. Rows are sorted in a descending order of average cosine similarity.\n\nPrior work demonstrated that gradient alignment between surrogates and targets is needed for transferability (Demontis et al., 2019). Thus, we expected augmentation methods that estimate target model gradients more accurately to increase transferability further. To assess this conjecture, we evaluated the cosine similarity between the gradients of the Inc-v3 model while using augmentations composed with DST applied to benign samples, and the gradients of other normally trained models on (untransformed) benign samples. The results (Table 7) show some support to the conjecture— NeuTrans and Sharp led to lower cosine similarities with target models’ gradients. Yet, the differences in cosine similarities between augmentation methods were small (≤0.031, on avg.).\n\n6 CONCLUSION AND FUTURE WORK\n\nOur study uncovered a mostly monotonic relationship between data-augmentation methods and transferability, and helped us identify a simple yet effective composition of data-augmentation methods, ULTIMATECOMBO, that outperforms previously proposed methods when integrated into attacks. The resulting attack should be considered as a standard baseline in follow-up work on transferability. Our work also puts forward conjectures for when augmentation techniques are expected to improve transferability, and offers some empirical support.\n\nIn the future, it would be informative to develop a theory that formally explains why augmentation methods help increase transferability. Furthermore, instead of relying on existing augmentation methods originally developed to improve DNN generalizability, an intriguing research direction would be to develop augmentation techniques tailored specifically for improving transferability. Lastly, in addition for assessing the vulnerability of ML models in black-box settings, it would be interesting to evaluate whether the ULTIMATECOMBO-based attack advances methods leveraging AEs for defensive purposes, by deceiving adversaries (e.g., to attain privacy (Cherepanova et al., 2021; Shetty et al., 2018)).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nIn the interest of reproducibility, we make our code publicly available at the following repository: https://tinyurl.com/UltimateComboICLR.\n\nREFERENCES Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim ˇSrndi ́c, Pavel Laskov, GiorIn Proc.\n\ngio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. ECML/PKDD, 2013.\n\nWieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable\n\nattacks against black-box machine learning models. In Proc. ICLR, 2018.\n\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Proc.\n\nIEEE S&P, 2017.\n\nValeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John Dickerson, Gavin Taylor, and Tom Goldstein. Lowkey: Leveraging adversarial attacks to protect social media users from facial recognition. In Proc. ICLR, 2021.\n\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized\n\nsmoothing. In Proc. ICML, 2019.\n\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. AutoAugment:\n\nLearning augmentation policies from data. In Proc. CVPR, 2019.\n\nAmbra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. Why do adversarial attacks transfer? Explaining transferability of evasion and poisoning attacks. In Proc. USENIX Security, 2019.\n\nTerrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks\n\nwith cutout. arXiv preprint 1708.04552, 2017.\n\nYinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boost-\n\ning adversarial attacks with momentum. In Proc. CVPR, 2018.\n\nYinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial\n\nexamples by translation-invariant attacks. In Proc. CVPR, 2019.\n\nKevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In Proc. CVPR, 2018.\n\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv\n\npreprint 1508.06576, 2015.\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\n\nexamples. In Proc. ICLR, 2015.\n\nChuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. Countering adversarial\n\nimages using input transformations. In Proc. ICLR, 2018.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\nIdentity mappings in deep residual\n\nnetworks. In Proc. ECCV, 2016.\n\nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural\n\nnetworks: A survey. IEEE PAMI, 44(9):5149–5169, 2021.\n\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\n\nconvolutional networks. In Proc. CVPR, 2017.\n\nQian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. Enhancing\n\nadversarial example transferability with an intermediate level attack. In Proc. ICCV, 2019.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAndrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial\n\nattacks with bandits and priors. In Proc. ICLR, 2019.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University\n\nof Toronto, 2009.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convo-\n\nlutional neural networks. CACM, 60(6):84–90, 2017.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2017a.\n\nAlexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\n\nIn Proc. ICLRW. 2017b.\n\nJiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft. Nesterov accelerated\n\ngradient and scale invariance for adversarial attacks. In Proc. ICLR, 2020.\n\nYanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial exam-\n\nples and black-box attacks. In Proc. ICLR, 2017.\n\nMuzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. A\n\nself-supervised approach for adversarial robustness. In Proc. CVPR, 2020.\n\nNicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram\n\nSwami. The limitations of deep learning in adversarial settings. In Proc. Euro S&P, 2016.\n\nHadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In Proc. NeurIPS, 2019.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\n\nbilenetv2: Inverted residuals and linear bottlenecks. In Proc. CVPR, 2018.\n\nRakshith Shetty, Bernt Schiele, and Mario Fritz. A4NT: Author attribute anonymity by adversarial\n\ntraining of neural machine translation. In Proc. USENIX Security, 2018.\n\nConnor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.\n\nJournal of big data, 6(1):1–48, 2019.\n\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-\n\ntion. In Proc. ICLR, 2015.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,\n\nand Rob Fergus. Intriguing properties of neural networks. In Proc. ICLR, 2014.\n\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proc. CVPR, 2015.\n\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking\n\nthe inception architecture for computer vision. In Proc. CVPR, 2016.\n\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi.\n\nInception-v4,\n\nInception-ResNet and the impact of residual connections on learning. In Proc. AAAI, 2017.\n\nFlorian Tramer. Detecting adversarial examples is (nearly) as hard as classifying them. In Proc.\n\nICML, 2022.\n\nFlorian Tram`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-\n\nDaniel. Ensemble adversarial training: Attacks and defenses. In Proc. ICLR, 2018.\n\nXiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance\n\ntuning. In Proc. CVPR, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiaosen Wang, Xuanran He, Jingdong Wang, and Kun He. Admix: Enhancing the transferability of\n\nadversarial attacks. In Proc. ICCV, 2021a.\n\nXiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, and Kun He. Boosting adversarial transfer-\n\nability through enhanced momentum. In Proc. BMVC, 2021b.\n\nRen Wu, Shengen Yan, Yi Shan, Qingqing Dang, and Gang Sun. Deep image: Scaling up image\n\nrecognition. arXiv preprint 1501.02876, 2015.\n\nWeibin Wu, Yuxin Su, Michael R Lyu, and Irwin King. Improving the transferability of adversarial\n\nsamples with adversarial transformations. In Proc. CVPR, 2021.\n\nCihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. Improving transferability of adversarial examples with input diversity. In Proc. CVPR, 2019.\n\nWeilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep\n\nneural networks. In Proc. NDSS, 2018.\n\nZhuolin Yang, Linyi Li, Xiaojun Xu, Shiliang Zuo, Qian Chen, Pan Zhou, Benjamin Rubinstein, Ce Zhang, and Bo Li. TRS: Transferability reduced ensemble via promoting gradient diversity and model smoothness. In Proc. NeurIPS, 2021.\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. In Proc.\n\nCutMix: Regularization strategy to train strong classifiers with localizable features. ICCV, 2019.\n\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\n\nrisk minimization. In Proc. ICLR, 2018a.\n\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShuffleNet: An extremely efficient\n\nconvolutional neural network for mobile devices. In Proc. CVPR, 2018b.\n\nZhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmen-\n\ntation. In Proc. AAAI, 2020.\n\nA PARALLEL AND SERIAL COMPOSITIONS OF AUGMENTATIONS\n\nFigure 1 illustrates how parallel and serial compositions work.\n\nB DNNS USED IN THE EXPERIMENTS\n\nWe tested transferability using ten ImageNet DNNs and six CIFAR-10 DNNs. Of the ten ImageNet models, six were normally trained, while others were adversarially trained. Specifically, for normally trained models, we selected: Inception-v3 (Inc-v3) (Szegedy et al., 2016); Inception-v4 (Incv4); Inception-ResNet-v2 (IncRes-v2) (Szegedy et al., 2017)); ResNet-v2-50 (Res-50); ResNet-v2101 (Res-101); and ResNet-v2-152 (Res-152) (He et al., 2016). For adversarially trained models, we selected: Inception-v3-adv (Inc-v3adv ) (Kurakin et al., 2017a); ens3-Inception-v3 (Inc-v3ens3 ); ens4-Inception-v3 (Inc-v3ens4 ); and ens-adv-Inception-ResNet-v2 (IncRes-v2ens) (Tram`er et al., 2018). We obtained the models’ PyTorch implementations and weights from a public GitHub repository.2 All six CIFAR-10 DNNs were normally trained. For this dataset, we used pretrained VGG-11 (VGG) (Simonyan & Zisserman, 2015)), ResNet-50 (Res) (He et al., 2016), DenseNet-121 (DenseNet) (Huang et al., 2017), MobileNet-v2 (MobileNet) (Sandler et al., 2018), GoogleNet (Szegedy et al., 2015), and an Inception-v3 (Inc) DNNs (Szegedy et al., 2016), also implemented in PyTorch.3\n\n2https://github.com/ylhz/tf_to_pytorch_model 3https://github.com/huyvnphan/PyTorch_CIFAR10\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: An illustration of serial and parallel compositions. When serially composing augmentations, each augmentation method operates on the output of the previous one. By contrast, in parallel composition, each augmentation method operates independently on the input (or set of inputs). The number of samples grows exponentially in serial composition, whereas it grows linearly in parallel composition. We use serial composition when composing diverse inputs (DI), scaling (Sc.), and translations (Tr.). Other augmentation methods are composed in parallel.\n\nC ATTACK AND AUGMENTATION METHOD PARAMETERS\n\nSimilarly to Wang et al. (2021b), we set the MI-FGSM decay factor μ=1.0, and the number of iterations T =10.\n\nWe mostly used default or commonly used parameters of augmentation methods. For CJ, we performed random adjustments of image hue ∈ [−0.5, 0.5], contrast ∈ [0.5, 1.5], saturation ∈ [0.5, 1.5], and brightness∈ [0.5, 1.5]. For CutOut, we replaced values in selected regions with zeros, and the portion of masked areas compared to image dimensions lied in [0.02, 0.4], with aspect ratios ∈ [0.4, 2.5]. In comparison, for RE, the dimension of masked areas relatively to the image dimensions lied in [0.02, 0.2], with aspect ratios ∈ [0.3, 3.3]. For Sharp, we used the following edgeenhancement mask:\n\n(cid:34) −0.5 −0.5 −0.5 5.0 −0.5 −0.5 −0.5 −0.5 −0.5\n\n(cid:35)\n\n.\n\nFor diverse inputs, images were transformed with probability 0.5. For the Admix operation, consistently with Wang et al. (2021a), we randomly sampled three images from other categories for mixing as part of the Admix-DT-MI-FGSM attack. However, for the interest of computational efficiency, we use only one image for mixing when composing Admix with other augmentation methods. We did not find that mixing with fewer images harmed performance. In fact, it even improved transferability in some cases. Finally, in CutMix, we picked the top left coordinate (rx, ry), the width, rw, and height, rh, of the region to be replaced, using the formulas:\n\nrx ∼ U(0, W ),\n\nrw = W\n\n1 − λ,\n\nry ∼ U(0, H),\n\nrh = H\n\n1 − λ,\n\n√\n\n√\n\nwhere U is the uniform distribution, W is the image width, H is the image height, and λ is a parameter set to 0.5.\n\nIn an attempt to enhance transferability further, we optimized the parameters of a few augmentation methods we considered via grid search. Except for the Gaussian kernel’s size used in translationinvariant attacks (Dong et al., 2019), we found that the selected parameters had little impact on\n\n13\n\nTr.DI.Sc.CutOutSerial Composition Parallel Composition Under review as a conference paper at ICLR 2023\n\ntransferability. Specifically, for translations, after considering Gaussian kernels of sizes ∈ {5 × 5, 7 × 7, 9 × 9}, we set the default to 7 × 7, except for Admix-DT-MI-FGSM, for which the 9 × 9 kernel performed best. Table 8 shows that our choice of Admix parameters (m=1 and Gaussian kernel size of 9 × 9) improves its performance. For GS, we found ωR, ωG, and ωB had little impact on transferability, as long as the weight assigned to each channel was >0.1. Accordingly, we set ωR, ωG, and ωB to 0.299, 0.587, and 0.114, respectively, per commonly used values (e.g., in the Python PyTorch package4). Finally, for CS, we only swapped the blue and green channels, as this led to a minor improvement compared to swapping all three channels.\n\nAttack\n\nInc-v3\n\nInc-v4 Res-50\n\nRes-101 Res-152\n\nIncRes-v2\n\nAdmix-DT-MI-FGSM (original) Admix-DT-MI-FGSM (ours)\n\n99.5 100.0\n\n92.3 94.7\n\n88.5 91.9\n\n87.0 90.3\n\n85.3 88.7\n\n90.9 93.3\n\nTable 8: Transferability rates (%) of AEs crafted via Admix-DT-MI-FGSM against an Inc-v3 surrogate. Our variant sets m=1 and the translation’s Gaussian kernel to 7 × 7 and include original images when calculating gradients, whereas the original work uses m=3 and a 9 × 9 kernel.\n\nFinally, we clarify that each of our attack combinations emits the original image once, alongside the transformed images. Moreover, when aggregating the gradients, the gradients of the original and transformed images are assigned equal weights. We tested whether weighting the gradients differently (e.g., assigning higher or lower weight to the original sample) can help improve transferability using the GS method. However, we found that equal weights attained the best results.\n\nD ATTACKS CONSIDERED BUT EXCLUDED\n\nBesides the three state-of-the-art baselines we experimented with, we considered including two other attacks in the evaluation. Wu et al.’s (2021) attack uses a neural network to create adversarial perturbations robust against transformations for enhanced transferability, and achieves competitive transferability rates. However, unfortunately, we were unable to find a publicly available implementation of the attack. Huang et al.’s (2019) intermediate level attack improve AE transferability by reducing the variance of intermediate activations. We used the official implementation5 to test the attack on CIFAR-10 with (cid:15)=0.02 and the VGG or DenseNet models as surrogates. The results showed that the transferability rates were much less competitive that the three baselines we considered (50.34% vs. >54.00% average transferability with a VGG surrogate, and 45.68% vs. >56.56% average transferability with a DenseNet surrogate). Therefore, we removed the intermediate level attack for the remaining experiments.\n\nE INDIVIDUAL AUGMENTATIONS\n\nTable 9 presents the transferability rates when integrating individual augmentation methods into MI-FGSM. Table 10 presents the transferability when composing individual augmentation methods with DST. Trasnferability rates were computed on ImageNet, using the Inc-v3 DNN as a surrogate and the other normally trained DNNs as victims ((cid:15) = 16 255 ). Notice how composing color-space augmentations (specifically, CS, CJ, and GS) with DST helps improve transferability over the baselines (Table 10).\n\nF TRANSFERABILITY RATES ON IMAGENET\n\nTables 11 and 12 detail the trasferability rates on ImageNet, from all ten DNNs to normally and adversarially trained models, respectively. Here, we also consider transferring AEs from adversarially trained surrogates. Table 13 shows the transferability rates on ImageNet from Inc-v3 to other normally traiend models with varied perturbation norms (i.e., values of (cid:15)).\n\n4https://bit.ly/3ynCyUD 5https://github.com/CUAI/Intermediate-Level-Attack\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nG TRANSFERABILITY RATES ON CIFAR-10\n\nTables 14 and 15 report attack tranferability rates from all six normally trained DNNs to all other victim DNNs for (cid:15)=0.02 and (cid:15)=0.04, respectively.\n\nH THE MONOTONICITY OF TRANSFERABILITY WHEN ADDING\n\nAUGMENTATIONS\n\nFigure 2 depicts a visual summary of the experiment presented in Section 5.2, demonstrating how the relationship between augmentation methods and transferability is mostly monotonic.\n\nI ATTACK RUN-TIME\n\nMI-FGSM’s time complexity is predominated by the gradient computation steps. Accordingly, the attacks’ run-times are directly affected by the number of samples the augmentation methods create (i.e., samples emitted by D(·) in Algorithm 1): The more samples emitted by the augmentation method, the more back-propagation would be required to compute gradients for updating the adversarial examples in each iteration, thus increasing the AE-generation time. The empirical measurements corroborate this intuition (Table 16). Overall, we can see that DST augments MI-FGSM with the least samples, leading to the fastest attack (DST-MI-FGSM). GS-DST-MI-FGSM is the second fastest attack, while ULTIMATECOMBO is slower than Admix-DT-MI-FGSM but substantially faster than DST-VMI-FGSM. We note that no particular effort was invested to make GS-DST-MI-FGSM and ULTIMATECOMBO more time-efficient (e.g., stacking augmented samples for parallel computation, similarly to Admix-DT-MI-FGSM). Moreover, since transferability-based attacks generate AEs offline, and only once per surrogate model, as long as an attack is not prohibitively slow, attack run-time is a marginal consideration for selecting an attack compared to transferability rates.\n\nJ DEFENSE PARAMETERS\n\nWe used standard parameters when attacking defenses. For RS, we used a normally trained ResNet50 and set σ to 0.25, following Cohen et al. (2019). For ARS, the target model was ResNet-50 trained with isotropic Gaussian-noise augmentations (sampled from N (0, 0.25)), and σ was set to 0.25 during prediction, per Salman et al. (2019). In both cases, we used 10,000 noisified samples during inference. For Bit-Red, we used a squeezer with bit-depth of one, in accordance with Xu et al. (2018). Finally, we used the default NRP parameters and pre-trained model from the official GitHub repository (Naseer et al., 2020).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nAttack\n\nInc-v3\n\nInc-v4 Res-50 Res-101 Res-152\n\nIncRes-v2\n\nMI-FGSM fPCA-MI-FGSM CS-MI-FGSM CJ-MI-FGSM GS-MI-FGSM RE-MI-FGSM CutMix-MI-FGSM CutOut-MI-FGSM NeuTrans-MI-FGSM Sharp-MI-FGSM AutoAugment-MI-FGSM\n\n100.0 100.0 100.0 100.0 100.0 100.0 63.0 100.0 96.4 99.3 100.0\n\n54.7 70.7 57.5 66.4 62.9 55.1 34.7 58.2 44.4 44.7 61.1\n\n48.9 65.4 54.7 61.8 61.4 52.1 33.9 54.4 39.5 41.0 56.6\n\n43.5 59.6 49.9 57.5 56.5 46.6 30.0 49.6 35.6 34.3 50.1\n\n41.1 57.6 46.3 54.7 51.8 43.9 33.1 45.7 33.6 34.4 48.5\n\n50.6 69.2 56.7 65.4 62.4 52.2 31.2 55.9 38.2 40.2 58.4\n\nTable 9: Transferability rates (%) on ImageNet from a normally trained Inc-v3 surrogate to normally trained target models (columns) when integrating individual augmentation methods into MI-FGSMbased attacks.\n\nAttack\n\nInc-v3\n\nInc-v4 Res-50 Res-101 Res-152\n\nIncRes-v2\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM fPCA-DST-MI-FGSM CS-DST-MI-FGSM CJ-DST-MI-FGSM GS-DST-MI-FGSM RE-DST-MI-FGSM CutMix-DST-MI-FGSM CutOut-DST-MI-FGSM NeuTrans-DST-MI-FGSM Sharp-DST-MI-FGSM AutoAugment-DST-MI-FGSM\n\n99.5 100.0 100.0 100.0 100.0 100.0 100.0 100.0 69.5 100.0 97.8 99.9 100.0\n\n92.3 92.9 94.7 94.3 94.7 94.9 95.6 95.3 61.8 95.0 87.2 95.6 94.1\n\n88.5 89.5 90.7 90.3 92.8 92.3 93.7 91.5 59.6 91.4 82.5 91.3 91.1\n\n87.0 87.2 88.9 88.6 90.3 90.9 91.8 89.7 58.1 89.8 79.8 89.6 88.6\n\n85.3 86.4 89.1 87.3 88.7 90.2 90.9 88.2 56.4 89.1 80.7 89.4 87.5\n\n90.9 91.2 92.6 90.8 93.3 94.1 94.9 93.5 58.6 93.4 84.4 92.9 92.9\n\nTable 10: Transferability rates (%) on ImageNet from a normally trained Inc-v3 surrogate to normally trained target models (columns) when integrating individual augmentation methods composed with DST into MI-FGSM-based attacks. Admix-DT-MI-FGSM, DST-MI-FGSM, and DST-VMIFGSM are baseline attacks from prior work.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nAttack\n\nInc-v3\n\nInc-v4 Res-50 Res-101 Res-152\n\nIncRes-v2\n\nInc-v3\n\nInc-v4\n\nRes-101\n\nIncRes-v2\n\nRes-50\n\nRes-152\n\nInc-v3adv\n\nInc-v3ens3\n\nInc-v3ens4\n\nIncRes-v2ens\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n99.5 100.0 100.0 100.0 100.0 93.7 94.4 95.3 96.5 98.1 82.6 86.7 88.3 89.0 93.0 93.8 95.8 95.8 96.5 98.2 83.9 89.1 88.7 90.3 94.1 82.9 86.7 88.4 87.7 91.8 92.5 93.6 95.9 95.2 97.1 87.9 90.2 90.4 93.2 94.5 86.0 88.8 89.8 92.8 94.5 82.8 85.5 87.2 90.1 91.7\n\n92.3 92.9 94.7 95.6 98.0 99.3 100.0 99.9 100.0 99.9 78.1 83.2 85.0 84.8 90.4 91.9 94.2 94.7 95.6 97.1 80.0 84.1 84.7 85.7 91.6 79.0 83.6 85.3 84.3 90.4 87.9 90.9 91.7 92.8 95.5 83.4 85.9 87.1 90.4 92.1 80.3 85.6 84.8 88.4 90.2 80.4 82.9 84.3 88.2 90.9\n\n88.5 89.5 90.7 93.7 95.1 86.7 90.2 91.0 94.1 94.8 93.5 97.3 97.6 97.6 98.1 91.1 93.5 94.0 95.5 96.3 97.8 99.9 100.0 99.9 99.9 92.5 95.5 95.6 96.7 97.0 89.0 91.2 91.3 93.8 95.3 85.3 86.7 88.9 90.7 93.2 81.4 83.4 85.6 90.2 92.0 81.8 84.8 85.3 91.0 90.9\n\n87.0 87.2 88.9 91.8 94.3 84.9 88.1 89.9 92.5 95.0 97.4 99.9 99.9 99.8 99.7 90.6 92.0 92.9 94.2 96.5 93.4 97.3 98.1 98.3 99.0 92.3 97.1 97.2 97.6 97.3 88.2 90.6 91.7 92.8 94.6 84.5 86.0 88.0 90.0 92.3 82.3 85.9 86.2 89.5 91.0 79.5 84.5 85.7 89.7 90.9\n\n85.3 86.4 89.1 90.9 92.7 84.7 88.2 88.4 93.0 94.6 93.9 96.6 96.9 97.7 97.8 89.5 92.4 92.9 94.7 95.7 93.2 96.8 97.1 97.9 98.2 96.5 99.8 99.7 99.8 99.5 86.8 89.5 90.3 92.2 94.0 83.9 85.3 87.1 90.1 92.1 81.1 83.7 85.2 87.8 91.1 80.3 83.9 85.6 88.6 89.7\n\n90.9 91.2 92.6 94.9 97.1 89.6 92.8 93.5 95.4 96.8 79.3 84.9 87.2 87.6 91.8 98.9 99.8 99.7 100.0 100.0 78.9 84.3 86.7 87.5 90.9 77.9 83.1 85.6 85.3 90.3 88.3 91.7 92.6 94.2 95.8 85.3 87.3 88.7 90.7 92.9 80.7 84.4 85.3 88.5 91.5 83.5 88.1 88.6 91.6 93.3\n\nTable 11: Transferability rates (%) on ImageNet from ten surrogates (rows) to normally trained target models (columns). All attacks are black-box, except for when the surrogate and target models are the same.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nAttack\n\nInc-v3adv\n\nInc-v3ens3\n\nInc-v3ens4\n\nIncRes-v2ens\n\nInc-v3\n\nInc-v4\n\nRes-101\n\nIncRes-v2\n\nRes-50\n\nRes-152\n\nInc-v3adv\n\nInc-v3ens3\n\nInc-v4ens4\n\nIncRes-v2ens\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n84.6 81.3 84.4 87.3 88.2 82.7 80.6 84.3 87.6 88.6 79.5 78.9 82.0 82.3 83.5 89.0 87.2 88.8 91.1 92.2 79.1 76.1 80.7 81.4 85.1 77.7 75.0 79.1 79.6 83.6 98.3 99.7 99.6 99.9 99.9 88.3 87.8 90.3 91.6 94.0 88.3 88.2 89.7 91.8 93.4 86.2 88.2 89.4 92.0 92.5\n\n84.3 81.2 84.8 88.5 88.7 83.3 81.8 86.0 89.5 89.4 80.4 78.7 83.0 83.7 86.7 89.0 89.2 90.6 92.2 92.6 78.0 77.7 80.7 83.6 85.7 78.2 78.4 81.0 82.1 85.0 93.0 93.0 94.1 95.1 96.1 98.2 99.9 99.7 99.9 99.9 90.3 91.8 91.7 94.1 95.4 87.7 89.4 90.5 93.4 93.5\n\n83.5 77.7 82.9 85.5 86.7 81.3 80.8 83.0 87.2 88.4 78.6 76.7 80.9 81.1 82.8 88.7 86.4 87.8 90.0 92.0 77.1 75.6 79.5 78.9 83.4 75.9 75.7 79.5 79.0 82.5 92.2 93.5 93.7 94.1 96.2 91.6 91.9 92.6 93.4 94.7 98.7 99.8 100.0 99.9 99.9 88.6 89.8 91.3 92.7 93.1\n\n70.8 61.4 69.2 72.2 72.6 73.7 70.5 74.6 78.0 78.2 71.2 68.7 72.5 73.2 76.8 87.1 82.9 85.5 88.2 88.5 68.0 63.6 69.7 70.4 74.0 71.9 70.1 72.7 71.3 74.8 87.4 85.4 88.1 88.5 91.0 84.6 83.0 84.8 85.8 89.6 83.4 83.8 85.6 86.9 90.2 96.3 98.7 99.3 99.1 99.3\n\nTable 12: Transferability rates on ImageNet (%) from ten surrogates (rows) to adversarially trained target models (columns).\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nEpsilon Attack\n\nInc-v3\n\nInc-v4 Res-152\n\nIncRes-v2\n\nRes-50\n\nRes-101\n\n8/255\n\n24/255\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n98.3 99.7 99.5 99.5 99.7 99.8 100.0 100.0 100.0 100.0\n\n67.8 77.1 77.9 81.3 86.0 94.9 97.8 97.9 98.5 99.3\n\n50.0 62.9 64.1 71.1 75.0 88.2 93.6 95.0 96.7 97.4\n\n59.1 70.9 72.9 77.6 81.8 93.1 96.8 97.1 98.7 99.2\n\n57.4 69.8 72.2 77.3 81.2 89.4 95.1 96.2 97.0 97.5\n\n53.5 64.8 66.7 72.0 76.3 88.4 93.9 95.6 97.0 98.4\n\nTable 13: Transferability rates (%) on ImageNet, from a Inc-v3 surrogate to other normally trained models, with perturbation norms (cid:15) ∈ { 8\n\n255 } other than the default (cid:15) = 16 255 .\n\n255 , 24\n\nModel\n\nAttack\n\nVGG Res DenseNet MobileNet GoogleNet\n\nVGG\n\nRes\n\nDenseNet\n\nMobileNet\n\nGoogleNet\n\nInc\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n92.1 93.5 68.9 94.5 95.0 47.1 52.4 40.9 58.7 60.8 50.8 56.3 40.6 61.5 64.3 34.6 37.3 32.6 42.0 44.2 42.8 45.7 40.5 47.7 49.2 46.8 50.9 39.0 50.8 52.6\n\n67.8 73.5 52.3 76.5 77.0 94.9 98.5 75.2 98.4 98.3 78.4 86.9 62.2 88.8 90.0 54.5 60.0 50.7 63.6 67.9 63.0 65.8 56.2 65.3 67.9 65.4 68.1 51.1 69.0 72.2\n\n66.6 71.5 50.5 75.7 77.7 74.2 83.1 59.2 86.9 88.5 94.7 98.5 77.6 98.4 98.6 50.8 54.4 46.5 62.2 62.5 59.0 62.5 55.2 62.5 65.0 63.3 70.1 48.6 68.0 70.0\n\n78.8 80.4 61.6 85.8 86.5 78.9 86.5 64.7 90.3 91.0 81.4 86.8 67.1 89.1 92.2 99.8 100.0 90.6 100.0 100.0 88.2 92.7 77.7 93.4 94.0 89.6 93.1 64.1 92.6 95.3\n\n69.9 71.6 54.0 78.3 80.2 68.1 75.9 56.1 80.3 80.7 70.8 76.5 58.6 81.1 82.4 70.3 76.8 64.8 83.1 86.4 99.9 100.0 90.2 100.0 100.0 85.4 90.2 59.0 89.7 93.0\n\nInc\n\n67.5 71.5 51.6 77.3 77.9 64.3 71.5 54.4 78.6 80.6 70.7 77.4 54.3 81.8 84.2 64.0 71.3 59.3 78.7 82.1 78.6 84.2 67.2 87.0 90.6 98.0 99.7 74.5 99.6 99.8\n\nTable 14: Transferability rates (%) on CIFAR-10, from normally trained surrogates (rows) to normally trained target models (columns), with a perturbation norm (cid:15)=0.02.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nModel\n\nAttack\n\nVGG\n\nRes DenseNet MobileNet GoogleNet\n\nInc\n\nVGG\n\nRes\n\nDenseNet\n\nMobileNet\n\nGoogleNet\n\nInc\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO Admix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n97.4 97.5 90.4 97.9 98.2 69.6 77.5 57.7 86.1 88.7 78.8 85.5 59.8 90.4 92.2 51.5 59.6 48.7 70.1 71.1 69.7 73.1 61.8 77.6 78.1 74.2 79.5 55.4 79.7 82.6\n\n91.0 91.7 75.5 95.1 95.2 99.4 100.0 88.6 100.0 100.0 94.1 98.0 80.2 97.9 99.0 76.5 82.4 68.3 87.6 90.0 86.4 88.6 73.8 89.2 92.4 87.1 92.2 65.3 90.6 93.8\n\n90.1 92.9 72.0 94.8 95.5 92.8 96.9 78.0 98.2 99.0 98.3 99.8 88.2 99.9 99.9 70.6 78.8 63.6 86.9 88.2 81.8 85.1 72.8 88.5 90.4 88.1 91.2 63.0 91.0 92.7\n\n93.2 94.2 79.3 96.1 96.8 92.8 95.9 81.8 97.9 98.9 93.9 97.5 80.8 98.5 99.1 99.9 100.0 94.2 100.0 100.0 97.5 97.8 86.8 99.0 99.0 97.0 98.9 74.7 97.7 98.9\n\n89.2 89.8 75.6 92.9 94.6 85.3 91.6 75.6 93.8 95.7 89.4 93.5 73.0 95.6 95.7 87.0 93.3 78.4 95.2 96.8 100.0 100.0 93.1 100.0 100.0 96.0 98.4 72.2 98.3 99.3\n\n88.8 89.7 74.4 92.4 92.5 85.5 90.9 74.2 94.5 96.3 90.0 94.7 72.5 96.1 97.1 86.1 91.2 76.8 93.8 95.4 95.0 97.1 83.3 98.3 98.9 99.3 100.0 81.2 100.0 100.0\n\nTable 15: Transferability rates (%) on CIFAR-10, from normally trained surrogates (rows) to normally trained target models (columns), with a perturbation norm (cid:15)=0.04.\n\nAugmented samples Time (s)\n\nAdmix-DT-MI-FGSM DST-MI-FGSM DST-VMI-FGSM GS-DST-MI-FGSM ULTIMATECOMBO\n\n15 5\n105 10 30\n\n1.68 0.72 11.29 1.47 4.63\n\nTable 16: The number of samples augmented and the average time of crafting an AE (seconds per images) for different attacks. Times were measured on ImageNet, while attacking an Inc-v3 surrogate, and averaged for 1,000 samples. Experiments were executed on an Nvidia A5000 GPU.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\n:\n\nn o\n\ni t\ni s\no p\nm o\nc\n\ne h\n\nt\n\ns e\nd o\nc n\ne\n\ne d\no n\n\na\n\nn\n\ni\n\nh\n\nt i\n\nw g\nn\n\ni r\nt s\n\ny r\na n\n\ni\n\nb\n\ne h\nT\n\n. s\nd o\nh\n\nt e\n\nm n\no\n\ni t\na t\n\nn e\n\nm g\nu a\n\nf o\n\nn o\n\ni t\ni s\no p\nm o\nc\n\na\n\ns t\n\nn e\ns e\nr p\ne r\n\ne d\no n\n\nh c\na E\n\n.\n\nn\n\ni\n\ni\n\ng n\nm o\no z\n\nr e\nt f\na\n\nd e\nw e\ni\n\nv\n\nt s\ne B\n\n:\n\n2\n\ne r\nu g\n\ni\n\nF\n\n21\n\np r\na h\nS\n\nd n\na\n\n, s\nn a\nr\n\nT u\ne N\n\n,\n\nM S\nG F\n\n- I\n\n-\n\nM T\nS D\n\n, t\n\nn e\n\nm g\nu A\no\n\nt\n\nu A\n\n, t\n\nu O\nu C\n\nt\n\n,\n\nS G\n\n, )\nt i\n\nb\n\nt\n\nn a\nc fi\n\ni\n\nn g\n\ni s\n\nt s\no m\n\n(\n\ni\n\nx m\nd A\n\nr e\nh\n\nt e\nh w\n\ns e\nt\n\no n\ne d\n\n, t\n\nn a\nc fi\n\ni\n\nn g\n\ni s\n\nt s\na e\nl\n\ne h\n\nt\n\no\n\nt\n\nt s\no m\n\ne h\n\nt\n\nm o\nr f\n\n, t\ni\n\nb\n\nh c\na E\n\ne r\no m\n\ne n\no\n\ny\n\nl t\nc a\nx e\n\ns e\nd u\n\nl c\nn\n\ni\n\nv\n\nf i\n\nd e\nd u\n\nl c\nn\n\ni\n\ns i\n\nv\n\no\n\nt\n\nu\n\ne d\no n\n\nm o\nr f\n\ne g\nd e\n\nn A\n\n.\n\nn o\n\ni t\ni s\no p\nm o\nc\n\ne h\n\nt\n\nm o\nr f\n\n) 0\n(\n\nd e\nd u\n\nl c\nx e\n\nr o\n\n) 1\n(\n\nd e\nd u\n\nl c\nn\n\ni\n\ns i\n\n,\n\ny\n\nl e\nv\n\ni t\nc e\np s\ne r\n\n, )\nt i\n\nb\n\nt\n\nn a\nc fi\n\ni\n\nn g\n\ni s\n\nt s\na e\nl (\n\n3 v\n- c\nn I\n\nn a\n\nm o\nr f\n\n, t\ne N\ne g\na\n\nm\n\nI\n\nn o\n\nn o\n\ni t\ni s\no p\nm o\nc\n\nr e\np\n\ne t\na r\n\ny\n\nt i\nl i\n\nb a\nr e\nf s\nn a\nr t\n\ne g\na r\ne v\na\n\ne h\n\nt\n\nd e\nt\n\nu p\nm o\nc\n\ne w\n\n,\n\n2\n\n.\n\n5\n\nn o\n\ni t\nc e\nS\n\nn\n\ni\n\nd e\nn\n\ni a\nl\n\np x\ne\n\ns\n\nA\n\n.\n\nu\n\no\n\nt\n\nd e\nr a\np m\no c\n\nd o\nh\n\nt e\n\nm\n\nn o\n\ni t\na t\n\nn e\n\nm g\nu a\n\n.\n\np s\ne r\n(\n\nl a\nu q\ne\n\nr o\n\nr e\nh g\n\ni\n\nh\n\ns e\nv e\ni\n\nh c\na\n\nn o\n\ni t\ni s\no p\nm o\nc\n\ns ’\nv\n\nf i\n\n) d\ne r\n\n.\n\np s\ne r\n(\n\nk c\na l\n\nb\n\nn\n\ni\n\nd e\nr o\n\nl\n\no c\n\ns i\n\n) v\n\n,\n\nu (\n\ne g\nd e\n\nn A\n\n. s\nl e\nd o\nm m\n\ni t\nc i\n\nv\n\ns a\n\ns l\ne d\no m\n\ne n\n\ni\n\nn\n\ng n\n\ni\n\nn\n\ni a\n\nm\n\ne r\n\ne h\n\nt\n\no\n\nt\n\nN N\nD\n\ne t\na g\no r\nr u\ns\n\n. s\ne g\nd e\n\ng n\n\ni\n\nd n\no p\ns e\nr r\no c\n\nr i\ne h\n\nt\n\nd n\na\n\np r\na h\nS\n\nr o\n\ns n\na r\n\nT u\ne N\n\ng n\n\ni\n\nn\n\ni a\nt\n\nn o\nc\n\ns e\nd o\nn\n\ny a\nw a\n\nd e\nd a\nf\n\ne\n\nW\n\n.\n\nM S\nG F\n\n- I\n\nM\n\no\n\nt\n\nn\n\ni\n\nd e\nt a\nr g\ne t\n\nn\n\ni\n\nn e\nh w\n\nu\n\nn a\nh\n\nt\n\ns e\nt a\nr\n\ny\n\nt i\nl i\n\nb a\nr e\nf s\nn a\nr t\n\ne g\na r\ne v\na\n\n) r\ne w\no\n\nl\n\ns i\n\ns n\no\n\ni t\na t\n\nn e\n\nm g\nu a\n\ng n\n\ni\n\nn\n\ni a\n\nm\n\ne r\n\ne h\n\nt\n\nd n\na\n\ny\n\nt i\nl i\n\nb a\nr e\nf s\nn a\nr t\n\n) e\ng a\nr e\nv a\n(\n\ne h\n\nt\n\nn e\ne w\nt e\nb\n\np\n\ni\n\nh s\nn o\n\ni t\na l\ne r\n\ne h\n\nt\n\nt a\nh\n\nt\n\ni\n\ng n\nw o\nh s\n\n,\n\nk c\na l\n\nb\n\ne r\na\n\ns e\ng d\ne\n\n) d\ne d\na f\nn u\n(\n\ng n\n\ni\n\nn\n\ni a\n\nm\n\ne r\n\ne h\n\nt\n\nl l\na\n\nw o\nh\n\ne c\ni t\n\no N\n\n. s\ne s\na c\n\ne\n\nm o\ns\n\nn\n\ni\n\ny\n\nt i\nl i\n\nb a\nr e\nf s\nn a\nr t\n\nm\n\nr a\nh\n\np r\na h\nS\n\nd n\na\n\ns n\na r\n\nT u\ne N\ny\n\nl\n\nn o\n\n,\n\ny\n\nl t\n\nn e\nr e\nf f\ni\n\nd\n\nd\n\ni a\nS\n\n. )\ny\n\nt i\nl i\n\nb a\nr e\nf s\nn a\nr t\n\n≥ →\n\nd e\ns o\np m\no c\n\ns n\no\n\ni t\na t\n\nn e\n\nm g\nu a\n\ne r\no m\n\n, .\n\ne\n\n. i\n(\n\nc i\n\nn o\n\nt\n\no n\no m",
    "reference": "# Summary Of The Paper\n\nDriven by the success of data augmentation in improving the transferability of adversarial samples, this paper conducts an empirical investigation about the best combination of different data augmentations towards boosting the adversarial transferability. Experiments across ten data augmentations lead to a new composition that outperforms previous data augmentation methods.\n\n# Strength And Weaknesses\n\n**Strength**\n\n1. An empirical investigation about the combination of the best practices in data augmentations for boosting adversarial transferability can provide useful insights for the community.\n\n**Weakness**\n\n1. The novelty of this paper is very limited. It manually combines existing data augmentations to generate a better augmentation strategy without providing theoretical analysis or an automatic solution, which is more like a technical report. The technical contributions cannot match the bar of ICLR.\n\n2. The paper wastes too much contents on background knowledge about adversarial attacks and data augmentations, and starts to introduce the experimental results, which are the major contributions of this paper, from the 6th page. The presentation style can be better organized.\n\n3. The experimental results are not solid enough. Only one dataset with 1000 images is considered, while the datasets like CIFAR-10/100/SVHN/ImageNet, which are commonly adopted in the literature, are not included. In addition, only one perturbation strength (16/255) is considered. It is highly desired to conduct experiments across different datasets, perturbation strengths, and defensive methods, otherwise the accuracy gap between the proposed methods and baselines in Table 1/2 may be overturned via tuning the hyperparameters of adversarial example generation.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper wastes too much contents on backgrounds, which hurts the presentation clarity, and the novelty is very limited. The reproducibility is good as the codes are provided via an anonymous link.\n\n# Summary Of The Review\n\nConsidering the limited novelty and the lack of necessary experiments as elaborated in the weakness section, I tend to reject this paper.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nFEASIBLE ADVERSARIAL ROBUST REINFORCEMENT LEARNING FOR UNDERSPECIFIED ENVIRONMENTS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nRobust reinforcement learning (RL) considers the problem of learning policies that perform well in the worst case among a set of possible environment parameter settings. In real-world environments, choosing the set of allowed parameter settings for robust RL can be a difficult task. When that set is specified too narrowly, the agent will be left vulnerable to reasonable parameter values unaccounted for. When specified too broadly, the agent will be too cautious. In this paper, we propose Feasible Adversarial Robust RL (FARR), a novel problem formulation and objective for automatically determining the set of environment parameter values over which to be robust. FARR implicitly defines the set of feasible parameter values as those on which an agent could achieve a benchmark reward given enough training resources. By formulating this problem as a two-player zero-sum game, optimizing the FARR objective jointly produces an adversarial distribution over parameter values with feasible support and a policy robust over this feasible parameter set. We demonstrate that approximate Nash equilibria for this objective can be found using a variation of the PSRO algorithm. Furthermore, we show that an optimal agent trained with FARR is more robust to feasible adversarial parameter selection than with existing minimax, domain-randomization, and regret objectives in a parameterized gridworld and three MuJoCo control environments.\n\n1\n\nINTRODUCTION\n\nRecent advancements in deep reinforcement learning (RL) show promise for the field’s applicability to control in real-world environments by training in simulation (OpenAI et al., 2018; Hu et al., 2021; Li et al., 2021). In such deployment scenarios, details of the test-time environment layout and dynamics can differ from what may be experienced at training time. It is important to account for these potential variations to achieve sufficient generalization and test-time performance.\n\nRobust RL methods train on an adversarial distribution of difficult environment variations to attempt to maximize worst-case performance at test-time. This process can be formulated as a two-player zero-sum game between the primary learning agent, the protagonist, which is a maximizer of its environment reward, and a second agent, the adversary, which alters and affects the environment to minimize the protagonist’s reward (Pinto et al., 2017). By finding a Nash equilibrium in this game, the protagonist maximizes its worst-case performance over the set of realizable environment variations.\n\nThis work aims to address the growing challenge of specifying the robust adversarial RL formulation in complex domains. On the one hand, the protagonist’s worst-case performance guarantee only applies to the set of environment variations realizable by the adversary. It is therefore desirable to allow an adversary to represent a large uncertainty set of environment variations. On the other hand, care has to be taken to prevent the adversary from providing unrealistically difficult conditions. If an adversary can pose an insurmountable problem in the protagonist’s training curriculum, under a standard robust RL objective, it will learn to do so, and the protagonist will exhibit overly cautious behavior at test-time (Ma et al., 2018). As the complexity of environment variations representable in simulation increases, the logistic difficulty of well-specifying the limits of the adversary’s abilities is exacerbated.\n\nFor example, consider an RL agent for a warehouse robot trained to accomplish object manipulation and retrieval tasks in simulation. It is conceivable that the developer cannot accurately specify\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nthe distribution of environment layouts, events, and object properties that the robot can expect to encounter after deployment. A robust RL approach may be well suited for this scenario to, during training, highlight difficult and edge-case variations. However, given the large and complex uncertainty set of environment variations, it is likely that an adversary would be able to find and over-represent unrealistically difficult conditions such as unreachable item locations or a critical hallway perpetually blocked by traffic. We likely have no need to maximize our performance lower bound on tasks harder than those we believe will be seen in deployment. However, with an increasingly complex simulation, it may become impractical to hand-design rules to define which variation is and is not too difficult.\n\nTo avoid the challenge of precisely tuning the uncertainty set of variations that the adversary can and cannot specify, we consider a new modified robust RL problem setting. In this setting, we define an environment variation provided by an adversary as feasible if there exists any policy that can achieve at least λ return (i.e. policy success score) under it and infeasible otherwise. The parameter λ describes the level of worse-case test-time return for which we wish our agent to train. Given an underspecified environment, i.e. an environment parameterized by an uncertainty set which can include unrealistically difficult, infeasible conditions, our goal is to find a protagonist robust only to the space of feasible environment variations.\n\nWith this new problem setting, we propose Feasible Adversarial Robust Reinforcement learning (FARR), in which a protagonist is trained to be robust to the space of all feasible environment variations by applying a reward penalty to the adversary when it provides infeasible conditions. This penalty is incorporated into a new objective, which we formulate as a two-player zero-sum game. Notably, this formulation does not require a priori knowledge of which variations are feasible or infeasible.\n\nWe compare a near-optimal solution for FARR against that of a standard robust RL minimax game formulation, domain randomization, and an adversarial regret objective similarly designed to avoid unsolvable tasks (Dennis et al., 2020; Parker-Holder et al., 2022). For the two-player zero-sum game objectives of FARR, minimax, and regret, we approximate Nash equilibria using a variation of the Policy Space Response Oracles (PSRO) algorithm (Lanctot et al., 2017). We evalaute in a gridworld and three MuJoCo environments. Given underspecified environments where infeasible variations can be selected by the adversary, we demonstrate that FARR produces a protagonist policy more robust to the set of feasible task variations than existing robust RL minimax, domain-randomization, and regret-based objectives. To summarize, the primary contributions of this work are:\n\n• We introduce FARR, a novel objective designed to produce an agent that is robust to the\n\nfeasible set of tasks implicitly defined by a threshold on achievable reward.\n\n• We show that this FARR objective can be effectively optimized using a variation of the\n\nPSRO algorithm.\n\n• We empirically validate that a near-optimal solution to the FARR objective results in higher worst-case reward among feasible tasks than solutions for other objectives designed for similar purposes: standard robust RL minimax, domain randomization, and regret, in a parameterized gridworld and three MuJoCo environments.\n\n2 RELATED WORK\n\n2.1 DOMAIN RANDOMIZATION\n\nDomain randomization methods train in simulation on a distribution of environment variations that is believed to generalize to the real environment. The choice of a distribution for training-time simulation parameters plays a major role in the final test performance of deployed agents (Vuong et al., 2019). While domain randomization has been used with much success in sim-to-real settings (OpenAI et al., 2018; Tobin et al., 2017; Hu et al., 2021; Li et al., 2021), its objective is typically the average-case return over the simulation uncertainty set rather than the worst-case. This can be desirable in applications where average training-time performance is known to generalize to the test environment or where this can be validated. However, average-case optimization can be unacceptable in many real-world scenarios where safety, mission-criticality, or regulation require\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nthe agent to perform well in test environments whose weight in the average would be small and where sufficient validation is unavailable.\n\n2.2 ROBUST REINFORCEMENT LEARNING\n\nRobust reinforcement learning methods optimize reward in the worst-case and provide a promising approach for sim-to-real. Robust Adversarial Reinforcement Learning (RARL) (Pinto et al., 2017), which this work builds upon, optimizes return under the worst-case environment variations by optimizing a two-player zero-sum game between a task-performing protagonist and an environmentcontrolling adversary. Numerous mechanisms by which the adversary affects the environment have been explored including perturbing forces and varying environment dynamics (Pinto et al., 2017; Mandlekar et al., 2017; Nakao et al., 2021), disturbances to actions (Tessler et al., 2019; Vinitsky et al., 2020; Tan et al., 2020), and attacks on agent observations (Pattanaik et al., 2017; Gleave et al., 2019; Zhang et al., 2020; 2021; Kumar et al., 2021). However, each of these works makes the assumption that the uncertainty set from which perturbations and variations may be sampled is well-specified and tuned such that an agent robust to a minimax selection over the entire set will perform optimally in deployment. FARR relaxes this assumption and intends to produce a robust agent to the real environment, even when the adversary can select unrealistically hard variations.\n\n2.3 AUTOMATIC CURRICULUM DESIGN\n\nSimilar to finding the worst-case distribution of environment variations is finding the best-case distribution for improving an existing agent. Automatic curricula seek to find environment parameters that are challenging but not too difficult for a training agent. Racaniere et al. (2019), Campero et al. (2020), and Florensa et al. (2018) generate useful curricula for a single learning agent to solve difficult tasks by proposing appropriately challenging goals for the agent’s current abilities. POET (Wang et al., 2019; 2020) co-evolves a population of tasks and associated agents to produce an increasingly complex set of tasks with coupled agents capable of solving their associated task. While each of these methods can create increasingly capable agents, they offer no guarantees for final agent robustness.\n\nAsymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021) facilitates two agents with similar capabilities to compete in a two-player game where one attempts to reach goals that are achievable but difficult to the other agent. PAIRED (Dennis et al., 2020) and subsequent improvements to optimization (Parker-Holder et al., 2022; Du et al., 2022) extend this concept beyond goals to the generation of parameterized environments by introducing a two-player zero-sum regret objective between an adversary and a protagonist. The adversary learns to specify environment conditions in which regret is highest such that the protagonist’s performance most differs from estimated optimal performance. Regret selects tasks where changes to protagonist behavior could most affect incurred reward and is effective for training a broadly capable agent given limited preference over tasks. Regret can also ensure successful performance where possible in certain classes of environments with well-behaved reward functions. However, because the protagonist can have imperfect information on the current task and because a Nash equilibrium regret distribution does not minimize protagonist reward over a target set of tasks, a regret protagonist does not maximize worst-case performance over the feasible set of tasks like we are interested in with FARR.\n\n3 BACKGROUND\n\n3.1 UNDERSPECIFIED PARTIALLY-OBSERVABLE MARKOV DECISION PROCESSES\n\nWe adapt Underspecified Partially-Observable Markov Decision Processes (UPOMDPs) from Dennis et al. (2020) where there exists a parameter of variation θ ∈ Θ that is hidden from the agent. This parameter θ controls some aspect of the environment that can potentially be discovered through interaction. For example, θ could control the friction of a robot arm, the mass of a cartpole, or the layout of a room.\n\nWe model a UPOMDP as a tuple M = ⟨A, S, O, Θ, T , ρ, I, R, γ⟩ where A is the set of actions, S is the set of states, O is the set of observations, and γ is the discount factor. The choice of parameter θ ∈ Θ controls the conditions in this environment, namely the transition distribution\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nfunction T : S × A × Θ −→ ∆(S), the initial state distribution ρ : Θ −→ ∆(S), the observation function I : S × Θ −→ O, and the reward function R : S × Θ −→ R.\n\nGiven an observable history h ∈ H = (O × A)∗ × O, a protagonist policy πp : H → ∆(A) decides at each time step t on the distribution of the action at after seeing h = o0, a0, . . . , ot. Jointly with a UPOMDP M and a specific environment parameter θ, the policy induces a distribution pθ πp over the states, actions, observations, and rewards in an interaction episode. We define the protagonist’s utility Up(πp, θ) = Epθ t γtrt] as the expected episode discounted return for a protagonist policy πp and choice of θ.\n\n[(cid:80)\n\nπp\n\n3.2 POLICY SPACE RESPONSE ORACLES\n\nPolicy Space Response Oracles (PSRO) (Lanctot et al., 2017) is a deep RL method for calculating approximate Nash equilibria (NE) in zero-sum two-player games. It extends the normal-form Double-Oracle algorithm (McMahan et al., 2003) to games with sequential interaction. At a highlevel, PSRO iteratively adds new policies for each player to a population until a normal-form mixedstrategy solution to the restricted game induced by selecting population policies closely approximates a NE in the full game.\n\nPSRO operates by maintaining a population of policies Πi for each player i and a normal-form mixed strategy σi ∈ ∆(Πi). This mixed strategy σi represents a distribution over policies πi ∈ Πi to sample from at the beginning of each episode, and upon algorithm termination, σ = (σ1, σ2) is the final output of PSRO. The utility of player i of playing a mixed strategy σi against an opponent’s policy π−i is therefore Ui(σi, π−i) = Eπi∼σi [Ui(πi, π−i)], and likewise, Ui(σi, σ−i) = Eπi∼σi,π−i∼σ−i [Ui(πi, π−i)].\n\nIn each iteration of PSRO, new policies for each player i are added to its population Πi. In the case of sequential interaction, this is typically an RL best-response BR(σ−i) = arg maxπi Ui(πi, σ−i) that maximally exploits the opponent mixed-strategy. However, this choice of new policy is not a requirement, and PSRO maintains NE convergence guarantees so long as novel policies are continuously added for each player.\n\nAfter adding new policies, utilities U Π(π1, π2) between each pairing of player policies π1 ∈ Π1 and π2 ∈ Π2 are empirically estimated using rollouts to create a normal-form restricted game in which population policies are the strategies. A new NE mixed-strategy that solves the restricted game, a restricted NE σ = (σ1, σ2), is then cheaply calculated for each player. This process is repeated until no new policies are added to either player’s population or the process is externally stopped. As the number of strategies in each player’s population grows, a NE solution to the restricted game asymptotically converges to a NE solution to the full game.\n\nWhile other potentially suitable methods for solving extensive-form games exist, for example NFSP (Heinrich & Silver, 2016) and Deep-CFR (Brown et al., 2019), PSRO was chosen out of practicality because it can be implemented as an additional logic layer on top of existing reinforcement learning software stacks. Although PSRO-based methods are competitive in sample-efficiently solving twoplayer zero-sum games (Lanctot et al., 2017; Vinyals et al., 2019; McAleer et al., 2021; 2022b;a; Liu et al., 2022), the purpose of experiments in this work is not focused on improving the speed at which we might reach optima. Rather, we are interested in ensuring that we can reliably reach approximate NE for each objective we test by using PSRO in order to compare their near-optimal solutions on even ground.\n\n4 FEASIBLE ADVERSARIAL ROBUST REINFORCEMENT LEARNING\n\n4.1 MOTIVATION\n\nWe begin our discussion of the FARR method with a motivating example. Consider a cartpole environment where the pole is subject to perturbing forces of an unknown magnitude. To maximize our worse-case performance, we could formulate a robust RL game in which the adversary provides the most difficult possible mixed strategy of force magnitudes. By learning a best response strategy to this adversary, the protagonist will maximize its worst-case performance at test-time when presented with an unknown distribution of forces.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nImportantly, to achieve this process, we would need to allow the adversary to specify a sufficiently wide range of force magnitudes such that the real environment is believed to be in this range. However, we would not want to allow the adversary to specify force magnitudes so large that the task becomes impossible, because the adversary would then always select overly difficult parameters and the protagonist would only train on impossible task variations, failing at test time.\n\nIn this cartpole example, it is possible to manually adjust the range of allowed force values until the widest possible uncertainty set is found that still avoids the learning of a degenerate protagonist strategy. However, if this setting were scaled up to specifying the allowed values of many coefficients, level layouts, or environmental events where the adversary has complex, high-dimensional interactions with the protagonist, hand tuning these limits may no longer be viable.\n\nTo remove the need for human expert tuning of the adversary limits, we instead create a game where the adversary is allowed to specify impossible environment variations but is heavily penalized for doing so, using the method we describe below.\n\n4.2 FEASIBILITY\n\nWe define an environment parameter θ ∈ Θ as λ-feasible if a best-response to θ can achieve an expected return of at least λ ∈ R. Heuristically, the value for λ could be set as the lowest average return that we would expect an agent to receive across variations in deployment if it could act optimally with respect to each variation. We define the feasible set F λ of environment parameters as:\n\nF λ = {θ ∈ Θ|Up(BR(θ), θ) ≥ λ}.\n\n(1)\n\nF λ matches our motivation for considering feasible parameters when either of two conditions is satisfied. First, we may have a lower bound on the achievable performance in the real environment, and we can set λ at or below that bound. If the real environment is feasible, θ∗ ∈ F λ, then we are justified in avoiding training the protagonist on infeasible environments. Second, there may be a performance threshold such that only above it we have a preference over agent behaviors. For example, we may only care where a warehouse robot navigates if it has a valid path to its target shelf.\n\nThe set of feasible variations F λ is generally unknown. As part of optimizing the FARR objective, the adversary will learn an approximation of F λ and use it to guide the selection of a mixed strategy over discovered feasible environment variations. We note that, in order to measure robustness to F λ in this paper, we focus on environments where we can, in fact, calculate F λ for the purpose of test-time evaluation. It is reasonable to expect our findings to carry over to some domains where F λ is truly unknown and where the method cannot be directly evaluated.\n\n4.3 FARR OBJECTIVE\n\nWe wish to optimize the standard zero-sum robust adversarial game through PSRO with the additional constraint that the support of the adversary mixed strategy σθ contains only strategies in the feasible set F λ. Define supp(σθ) = {θ ∈ Θ|σθ(θ) > 0}. Our intended FARR objective is to solve the game:\n\nmin σθ\n\nmax σp\n\nUp(σp, σθ)\n\nsubject to supp(σθ) ⊆ F λ.\n\n(2)\n\nAn adversary mixed strategy σθ with support that is a subset of F λ will only provide feasible environment variations to the protagonist.\n\nIn order to provide flexibility in how novel adversary strategies that satisfy this constraint could be optimized, we can replace the hard constraint with a sufficiently large penalty C to the adversary when it violates the constraint. An equivalent zero-sum FARR objective would then be to optimize:\n\nmin σθ\n\nmax σp\n\nU λ\n\np (σp, σθ),\n\n5\n\n(3)\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 1: (a) An adversarial item retrieval game. The adversary can hide a bowl in a left, middle, or locked right cabinet. The protagonist chooses whether or not to attempt to find and grab the bowl. The protagonist receives a penalty for attempting to grab a bowl placed in the locked right cabinet and failing. (b) Matrix game representation of the original game and FARR transformed game. Protagonist utilities are shown. NE for the original game place weight on infeasible tasks with suboptimal behavior from the protagonist. NE for the FARR game provide feasible but difficult tasks and optimal protagonist behavior supposing feasible tasks are expected in deployment.\n\nwhere the FARR utility function U λ p (πp, θ) is unchanged from the original game if the adversary’s provided environment parameter is feasible, and where otherwise a large constant adversary penalty C is applied:\n\n(cid:26)C\n\nif Up(BR(θ), θ) < λ\n\nUp(πp, θ) otherwise.\n\n(4)\n\nU λ\n\np (πp, θ) =\n\n4.4 MATRIX GAME EXAMPLE\n\nWe demonstrate the effect of the FARR utility function (equation 4) through a matrix game example shown in Figure 1. The adversary specifies the location of a bowl among three cabinets, where the middle cabinet is more difficult for the protagonist to access than the left, and the right cabinet is locked and inaccessible. Without observing the bowl’s location, the protagonist must choose whether or not to attempt to find and grab the bowl, receiving a penalty for a failed retrieval attempt. In deployment, we expect that the item will always be placed in a feasible, accessible cabinet, either the left or the middle, so ideal behavior for the protagonist would be to always attempt to grab the bowl. For this game, we define the feasibility threshold over task variations as λ = 1.\n\nThe original game, shown in matrix-form in Figure 1(b), contains an infeasible adversary pure strategy (bowl in locked right cabinet). Because of the presence of this infeasible adversary strategy, the protagonist will never attempt to grab the bowl in any of the game’s Nash equilibria (one pure strategy NE is displayed in green). If we now replace the original game’s utility function Up with the FARR utility function U λ p using C = 500, the adversary receives a penalty of −500 for placing the bowl in the infeasible locked right cabinet because there exists no protagonist strategy that can achieve a utility of a least λ = 1 under that environment variation. Instead, in the FARR transformed game, a NE adversary places the bowl in the feasible but difficult middle cabinet, and the protagonist learns, facing a selection of feasible tasks, to always attempt to retrieve the bowl. This new protagonist NE strategy for the FARR game is optimal with respect to anticipated feasible deployment conditions.\n\n4.5 OPTIMIZING WITH PSRO\n\nWe use PSRO to solve for an approximate NE of the FARR transformed game with the penalty-based objective defined in equation (3). To optimize FARR with PSRO, only the scalar values λ and C need to be provided, where λ is the maximum difficulty expected among tasks in deployment and C is an arbitrarily large positive value. Our optimization process is shown in algorithm 1. We represent environment parameters θ as strategies in the adversary population Πθ with an output mixed-strategy σθ over Πθ. Similarly, we represent protagonist RL agent policies as strategies πp ∈ Πp with an\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1 FARR Optimized through PSRO\n\nInput: λ, C, and Initial policy sets Π = (Πp, Πθ) for Protagonist player and Adversary player Compute expected FARR payoff matrix U Π repeat\n\np (πp, θ) for each joint (πp, θ) ∈ Π\n\nλ as utilities U λ\n\nCompute Normal-Form restricted NE σ = (σp, σθ) over population policies Π using U Π Calculate new Protagonist policy πp (e.g. BR(σθ)) Πp = Πp ∪ {πp} for at least one iteration do\n\nλ\n\nCalculate new Adversary strategy θ and associated estimator for BR(θ) Πθ = Πθ ∪ {θ}\n\nend for Compute missing entries in U Π\n\nλ from Π\n\nuntil terminated early or no novel policies can be added Output: current Protagonist restricted NE strategy σp\n\noutput mixed-strategy σp over Πp. In each PSRO iteration, to best-respond to the current adversary restricted NE σθ, a new protagonist policy is trained using RL with a fixed environment experience budget. One or more random novel adversary pure strategies are added in each PSRO iteration. For wall-time parallelization, we add 3 in each iteration. For each adversary strategy θ, we also train e with the same hyperpameters as the protagonist to estimate BR(θ) and an evaluator RL policy πθ feasibility for the FARR utility function U λ p .\n\n5 EXPERIMENTS\n\nTo illustrate the utility of filtering out infeasible tasks in a robust RL setting, we perform experiments in environments where overly difficult or unsolvable task variations are possible while measuring performance under feasible conditions. In a goal-based gridworld environment and three perturbed MuJoCo (Todorov et al., 2012) control environments, we compare the performance of FARR with three alternative objectives: a standard minimax robust adversarial RL objective, domain randomization, and the regret objective as proposed in Dennis et al. (2020). Given a threshold for feasibility λ, we evaluate worst-case performance within the set F λ of feasible environment parameters. FARR outperforms each of these objectives because it is able to provide an adversarial training distribution of environment variations limited only to instances that are discovered to be feasible, while other methods provide overly difficult or otherwise mismatched training distributions for worst-case feasible conditions.\n\nWe optimize FARR and other baseline objectives with PSRO and identical protagonist RL bestresponse algorithms in order to compare final performance given guarantees of asymptotically reaching an approximate NE for each zero-sum objective.\n\nWe describe each of the baseline objectives below:\n\nMinimax The standard objective for robust adversarial RL using unmodified Up(σp, σθ). When infeasible tasks are allowed to the adversary, the standard minimax robust RL objective will focus on such tasks, resulting in overly cautious protagonist behavior or failed learning.\n\np = BR(σDR\n\nDomain Randomization (DR) With domain randomization, we train a single protagonist policy πDR ) to saturation against a uniform mixture of all possible environment variations σDR θ = U(Θ). Domain randomization can result in an exploitable agent if the relevant feasible part of configuration space is underrepresented by the uniform measure. This objective is optimized by training a single RL policy rather than with PSRO.\n\nθ\n\nRegret Matching the objective used by PAIRED (Dennis et al., 2020), we use PSRO to approxiEθ∼σθ [Up(σp, θ) − Up(BR(θ), θ)]. mately solve for NE using the objective minσθ maxσp BR(θ) is estimated using the same method as with FARR by training an evaluation agent πθ e against each θ. While designed to provide a distribution of tasks where the protagonist is known to be able to positively affect its performance through optimal behavior, this curriculum learning objective does not generally provide a task distribution suitable for robust learning to a specific set of tasks such as F λ.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\nFigure 2: (a) The Lava World grid environment. The adversary specifies the location of an unobservable goal. The protagonist receives -1 reward each timestep until it reaches the hidden goal. If the protagonist steps in lava (red), the episode ends and it receives a penalty of -15 reward. (b) The environment is underspecified and the goal can be placed in lava, forcing the agent to receive the lava penalty and creating an infeasible task given λ = −10. (c) Solving for approximate NE using PSRO, the FARR objective results in an agent maximally robust to the feasible, non-lava goals while other objectives result in suboptimal worst-case performance among the feasible set of tasks.\n\nTo calculate the worst-case episode reward within the set F λ of feasible environment parameters, we use our knowledge of F λ to enumerate a comprehensive set of feasible tasks on which every baseline’s performance is measured. For the gridworld environment, the entire feasible set F λ is calculated analytically. For MuJoCo, a discretization of the continuous parameter space Θ is calculated. Feasibility for each θ in the discretized space is then measured by training an RL best-response BR(θ) to completion and averaging final utility Up(BR(θ), θ) over 7 seeds. F λ is then determined using equation (1). We measure feasible worst-case episode reward as minθ∈F λ Up(σp, θ), averaging over 100 episodes for each value of θ. We use this as our metric for robustness to the feasible set F λ.\n\n5.1 LAVA WORLD\n\nThe gridworld task “Lava World” consists of a small platform surrounded by lava, as depicted in Figure 2 (a). The adversary specifies a goal location θ that the protagonist needs to reach, however the protagonist does not observe the goal, and the goal can be placed in lava. With an episode horizon of 20, the protagonist receives a reward of -1 for every timestep that it does not reach the goal. If the protagonist moves into lava, the episode ends, and it receives a reward of -15 even if the goal is at that location. The feasibility threshold for this task is λ = −10, thus making a parameter for this environment infeasible if the goal is put in lava and feasible otherwise. We use DDQN (Van Hasselt et al., 2016) to train protagonist RL policies.\n\nWorst-case protagonist episode reward among all values in the feasible set θ ∈ F λ is shown as a function of PSRO iterations for FARR and baseline objectives in Figure 2 (c). The minimax objective fails because the adversary learns to always suggest infeasible lava goals, and the protagonist learns to immediately jump in lava rather than waste time searching for a goal in non-lava cells. Likewise, domain-randomization fails because the majority of goals are infeasible lava goals, so in order to optimize the average-case, the protagonist learns the same suboptimal behavior as it does with minimax. The regret objective produces a distribution of both feasible and infeasible goals where non-lava goals are the majority, however because this distribution does not minimize protagonist reward over feasible goals, the regret protagonist does not maximize worst-case performance over the feasible set of tasks. FARR penalizes the adversary for suggesting the infeasible lava goals and otherwise provides base robust adversarial RL utilities for feasible goals, thus resulting in a protagonist that maximizes worst-case reward among the actual feasible non-lava goal set F λ.\n\n5.2 MUJOCO\n\nWe compare FARR and other objectives on three MuJoCo control tasks, HalfCheetah, Walker2D, and Hopper using PPO (Schulman et al., 2017) to train protagonist RL policies. In each of these environments, the adversary specifies parameters θ = (α, β) where α ∈ (0, 10], β ∈ (0, 10] for a beta distribution B(α, β) used to sample from and generate 1D horizontal perturbing forces every\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 3: Worst-case MuJoCo HalfCheetah (a) and Hopper (b) average episode reward among task parameters in the feasible set F λ as a function of PSRO iterations for FARR and other baselines with multiple values of λ.\n\ntimestep that are applied to the torso of the protagonist’s robot. The adversary has the ability to specify infeasible distributions of forces which make accruing reward in each task virtually impossible. We conduct experiments with each MuJoCo environment using three different feasibility threshold values for λ, representing three different assumptions regarding the difficulty of test-time conditions that we wish to prepare for.\n\nFor HalfCheetah, in Figure 3 (a) we show worst-case protagonist reward among environment parameters in the feasible set F λ as a function of PSRO iterations with λ values {−1000, 0, 1000}. The same is shown for Hopper with λ ∈ {200, 400, 600} in Figure 3 (b), and for Walker2D with λ ∈ {200, 400, 600} in the appendix due to space limitations. Across different values for λ in each of these environments, we see that FARR is able to train an agent which maximizes worst-case reward under the ground-truth F λ by penalizing the adversary to prevent it from providing infeasible variations. Domain randomization and regret provide training distributions unconditioned on λ or any notion of F λ, which are only sometimes appropriate for robust performance as seen for λ = 600 in Figure 3 (b) with domain randomization and regret and for λ = 1000 in Figure 3 (a) with regret. Otherwise, domain randomization and regret result in agents exploitable to some configuration in F λ. Likewise, minimax consistently provides insurmountable conditions to the protagonist, resulting in failed learning and highlighting the need for methods like FARR to automatically limit adversary abilities in robust RL. Analysis on the mixed strategies learned by each objective’s adversary is available in Section C of the appendix.\n\n6 DISCUSSION AND FUTURE WORK\n\nWe present FARR, a novel robust RL problem formulation and two-player zero-sum game objective in which we consider an underspecified environment allowing infeasible conditions and we train a protagonist to be robust only to the tasks which are feasible. By solving for approximate Nash equilibrium under the FARR objective using PSRO, we demonstrate that this method can produce a robust agent even when the adversary is allowed to specify parameters which make sufficient performance at a task impossible. A limitation and avenue for future work is that our current method for optimizing FARR does not directly optimize the adversary, instead relying on random search and the PSRO restricted game solution to provide an optimal mixed strategy. In future work, if high-dimensional joint adversary best-responses with feasibility estimates can be sample-efficiently optimized, FARR can provide a prescribable solution to avoid the manual creation of complex rules to limit robust RL adversaries in higher-dimensional sim-to-real configuration spaces.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nGeorge W. Brown. Iterative solution of games by fictitious play. Activity analysis of production and\n\nallocation, pp. 374–376, 1951.\n\nNoam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-\n\nmization. In International conference on machine learning, pp. 793–802. PMLR, 2019.\n\nAndres Campero, Roberta Raileanu, Heinrich K ̈uttler, Joshua B Tenenbaum, Tim Rockt ̈aschel, and Edward Grefenstette. Learning with amigo: Adversarially motivated intrinsic goals. arXiv preprint arXiv:2006.12122, 2020.\n\nMichael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. arXiv preprint arXiv:2012.02096, 2020.\n\nYuqing Du, Pieter Abbeel, and Aditya Grover. It takes four to tango: Multiagent selfplay for auto-\n\nmatic curriculum generation. arXiv preprint arXiv:2202.10608, 2022.\n\nCarlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pp. 1515–1528. PMLR, 2018.\n\nAdam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615, 2019.\n\nJohannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-\n\ninformation games. arXiv preprint arXiv:1603.01121, 2016.\n\nHan Hu, Kaicheng Zhang, Aaron Hao Tan, Michael Ruan, Christopher Agia, and Goldie Nejat. A sim-to-real pipeline for deep reinforcement learning for autonomous robot navigation in cluttered rough terrain. IEEE Robotics and Automation Letters, 6(4):6569–6576, 2021.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAounon Kumar, Alexander Levine, and Soheil Feizi. Policy smoothing for provably robust rein-\n\nforcement learning. arXiv preprint arXiv:2106.11420, 2021.\n\nMarc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P ́erolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems, pp. 4190–4203, 2017.\n\nZhongyu Li, Xuxin Cheng, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, and Koushil Sreenath. Reinforcement learning for robust parameterized locomotion control of bipedal robots. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 2811–2817. IEEE, 2021.\n\nEric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning. In International Conference on Machine Learning, pp. 3053–3062. PMLR, 2018.\n\nSiqi Liu, Luke Marris, Daniel Hennes, Josh Merel, Nicolas Heess, and Thore Graepel. Neupl:\n\nNeural population learning. arXiv preprint arXiv:2202.07415, 2022.\n\nXiaobai Ma, Katherine Driggs-Campbell, and Mykel J Kochenderfer.\n\nsafety for autonomous vehicle control with adversarial reinforcement learning. Intelligent Vehicles Symposium (IV), pp. 1665–1671. IEEE, 2018.\n\nImproved robustness and In 2018 IEEE\n\nAjay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust In 2017 IEEE/RSJ policy learning: Active construction of physically-plausible perturbations. International Conference on Intelligent Robots and Systems (IROS), pp. 3932–3939. IEEE, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nStephen McAleer, John Banister Lanier, Kevin A Wang, Pierre Baldi, and Roy Fox. Xdo: A double oracle algorithm for extensive-form games. Advances in Neural Information Processing Systems, 34:23128–23139, 2021.\n\nStephen McAleer, JB Lanier, Kevin Wang, Pierre Baldi, Roy Fox, and Tuomas Sandholm. Self-play psro: Toward optimal populations in two-player zero-sum games. arXiv preprint arXiv:2207.06541, 2022a.\n\nStephen McAleer, Kevin Wang, Marc Lanctot, John Lanier, Pierre Baldi, and Roy Fox. Anytime\n\noptimal psro for two-player zero-sum games. arXiv preprint arXiv:2201.07700, 2022b.\n\nH Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost In Proceedings of the 20th International Conference on\n\nfunctions controlled by an adversary. Machine Learning (ICML-03), pp. 536–543, 2003.\n\nHideaki Nakao, Ruiwei Jiang, and Siqian Shen. Distributionally robust partially observable markov decision process with moment-based ambiguity. SIAM Journal on Optimization, 31(1):461–488, 2021.\n\nM Andrychowicz OpenAI, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2(3):5–1, 2018.\n\nOpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben D’Sa, Arthur Petron, Henrique P d O Pinto, et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021.\n\nJack Parker-Holder, Minqi Jiang, Michael Dennis, Mikayel Samvelyan, Jakob Foerster, Edward Grefenstette, and Tim Rockt ̈aschel. Evolving curricula with regret-based environment design. arXiv preprint arXiv:2203.01302, 2022.\n\nAnay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep reinforcement learning with adversarial attacks. arXiv preprint arXiv:1712.03632, 2017.\n\nLerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In International Conference on Machine Learning, pp. 2817–2826. PMLR, 2017.\n\nSebastien Racaniere, Andrew K Lampinen, Adam Santoro, David P Reichert, Vlad Firoiu, and Timothy P Lillicrap. Automated curricula through setter-solver interactions. arXiv preprint arXiv:1909.12892, 2019.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nSainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, 2018.\n\nKai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. Robustifying reinforcement learning agents via action space adversarial training. In 2020 American control conference (ACC), pp. 3959–3964. IEEE, 2020.\n\nChen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pp. 6215–6224. PMLR, 2019.\n\nJosh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 23–30. IEEE, 2017.\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nHado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-\n\nlearning. In AAAI conference on artificial intelligence, volume 30, 2016.\n\nEugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and Alexandre Bayen. Robust reinforcement learning using adversarial populations. arXiv preprint arXiv:2008.01825, 2020.\n\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha ̈el Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n\nQuan Vuong, Sharad Vikram, Hao Su, Sicun Gao, and Henrik I Christensen. How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies? arXiv preprint arXiv:1903.11774, 2019.\n\nRui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. arXiv preprint arXiv:1901.01753, 2019.\n\nRui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley. Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning In International Conference on Machine Learning, pp. 9940– challenges and their solutions. 9951. PMLR, 2020.\n\nHuan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations. Advances in Neural Information Processing Systems, 33:21024–21037, 2020.\n\nHuan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on\n\nstate observations with learned optimal adversary. arXiv preprint arXiv:2101.08452, 2021.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Worst-case MuJoCo Walker2D reward among task parameterizations in the feasible set F λ as a function of PSRO iterations for FARR and other baselines with multiple values of λ.\n\n(a)\n\nA MUJOCO FEASIBLE SETS\n\nFor MuJoCo experiments, in order to measure each objective’s worst-case average episode reward among feasible tasks, we evaluate on a discrete approximation of F λ. In these environments, Θ represents parameters of a beta distribution B(α, β), α ∈ (0, 10], β ∈ (0, 10] sampled from each timestep to generate horizontal perturbing forces applied to the simulated robot. We consider a discretization of Θ with 11 different values in [0.01, 10] for both α and β. For each each combination θ = (α, β), we train 7 seeds of a RL best-response BR(θ) to completion using the same hyperparameters as the protagonist. The average final utility Up(BR(θ), θ) across seeds is then used to calculate F λ using equation (1). In Figure 5, for each environment, we show the 7-seed average Up(BR(θ), θ) for every value of θ and the resulting feasible sets F λ (shown in green) that we evaluate robustness to for each value of λ.\n\n(a)\n\n(c)\n\n(e)\n\n(b)\n\n(d)\n\n(f)\n\nFigure 5: (a,c,e) Estimated values for Up(BR(θ), θ) across the two parameters α and β that the adversary has control over. (b,d,f) The feasible sets F λ used for evaluation marked in green for each value of λ.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nB MUJOCO LEARNED ADVERSARY STRATEGIES\n\nAfter running PSRO to completion, the output strategies are both a protagonist mixed strategy σp and an adversary mixed strategy σθ that should jointly approximate a Nash equilibrium to each of the two-player zero-sum game objectives we optimize. In figures 6, 7, and 8, we display the distribution of θ values induced by the final MuJoCo adversary mixed strategies σθ for the minimax, regret, and FARR objectives. For each λ value considered, we overlay in shades of green the number of BR(θ) seeds used in measuring F λ that achieved a final average episode reward greater than or equal for λ.\n\nAcross all environments, the minimax adversary consistently selects the most difficult θ values possible outside of any variations considered feasible with the λ values tested. In contrast, FARR mixes between θ values both well inside of the feasible regions and at the edges where task variations are as challenging as possible while remaining feasible.\n\nFigure 6: HalfCheetah θ distributions induced by the final adversary PSRO mixed strategy σθ for each objective.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: Hopper θ distributions induced by the final adversary PSRO mixed strategy σθ for each objective.\n\nFigure 8: Walker2D θ distributions induced by the final adversary PSRO mixed strategy σθ for each objective.\n\nC PROPERTIES OF NASH EQUILIBRIA FOR THE FARR TRANSFORMED GAME\n\nIn this section we show that solving for Nash equilibria in the FARR transformed game will give the same result as solving for NE in a regular minimax robust RL game with the adversary strategy space already limited to only feasible strategies. The benefit of solving the FARR game is that the set of feasible adversary strategies does not need to be known a priori.\n\nDefine the set of λ-infeasible adversary strategies as I λ = Θ \\ F λ. Define the set of all possible protagonist strategies πp as Π∗ p and Up, the adversary’s utility function U λ p (πp, θ) and Ua(πp, θ) = −Up(πp, θ) for any πp and θ.\n\na and Ua is the negative of the protagonist’s, U λ\n\np. For both protagonist utility functions U λ\n\na (πp, θ) = −U λ\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTheorem 1. For sufficiently large C, a Nash equilibrium joint strategy σ∗ FARR of a FARR transformed game GFARR with utility function U λ p , protagonist strategies Π∗ p and all adversary strategies Θ is also a Nash equilibrium of a reduced game Greduced with utility function Up, protagonist strategies Π∗ p, and only λ-feasible adversary strategies F λ.\n\nProof. Let C take a sufficiently large value greater than any utility achievable by the protagonist a (πp, θ′) = −C when θ′ ∈ I λ, then for any θ′ ∈ I λ, any θ ∈ F λ, C > maxπp,θ Up(πp, θ). Since U λ a (πp, θ). It follows, as long as F λ is nonempty, and any πp ∈ Π∗ that for all θ′ ∈ I λ there exists an adversary strategy θ ∈ F λ which achieves higher adversary utility against every πp ∈ Π∗ p, thus all λ-infeasible adversary strategies θ′ ∈ I λ are strictly dominated in the FARR transformed game.\n\na (πp, θ′) < −Up(πp, θ) = U λ\n\np: U λ\n\nIf all θ′ ∈ I λ are strictly dominated then they are not in the support of any Nash equilibrium for GFARR. Furthermore, it is possible to remove strategies θ′ ∈ I λ though iterated elimination of strictly dominated strategies (IESDS) to reduce GFARR to Greduced since the adversary strategy set for Greduced is Θ \\ I λ = F λ and for all θ ∈ F λ and πp ∈ Π∗ p (πp, θ) = Up(πp, θ). If Greduced is an outcome of IESDS from GFARR, then if σ∗\n\nFARR is a NE of GFARR, it is also an NE of Greduced.\n\np: U λ\n\nBy employing a penalty C rather than directly pruning infeasible strategies from the PSRO restricted game, the FARR objective can be defined without consideration to the mechanics of any specific algorithm like PSRO. The FARR objective can potentially be optimized with two-player zero-sum game methods other than PSRO as well, though exploring the use of more optimization methods is left to future work.\n\nD SELECTING VALUES FOR λ\n\nFARR is most applicable when the appropriate value for λ can be derived from problem requirements. For instance, λ would ideally be set to the lowest average return that an optimal agent would receive across task variations in deployment. This value could come from the environment definition, where doable tasks provide a minimum level of return if accomplished. Alternatively, λ could be chosen by anticipating the maximum difficulty of task variations that would be seen at test-time or on which robust performance is important to the practitioner.\n\nIn the case where an appropriate value of λ is completely unknown and cannot be derived from problem requirements, in a low-dimensional task variation space, manually tuning the adversary’s capabilities without FARR may be appropriate. In a complex, high-dimensional task variation space, searching for a useful λ may be easier than a direct search over the space of adversary legal strategy sets because λ presents a single variable to tune, rather than a large number of legal parameter ranges or complex conditional constraints between adversary-specified parameters that may need to be defined. In this work, we consider the case where λ can be derived from problem requirements.\n\nE PSRO COMPARISON WITH SELF-PLAY\n\nIn Lava World, for each of the two-player zero-sum game objectives, we compare PSRO to self-play in which the protagonist, adversary, and evaluator πθ e continuously train together. For all self-play agents, we train with PPO to enable stochastic policies like PSRO is able to output. Regret self-play matches the original PAIRED algorithm from Dennis et al. (2020).\n\nAlthough self-play may potentially yield competitive performance in some scenarios, unlike PSRO, it lacks any guarantees of converging to an approximate Nash equilibrium in two-player zero-sum partially-observable Markov or extensive-form games. Seen in Figure 9, we see that self-play for both FARR and PAIRED fails to converge, reaching a maximum feasible-space worst-case average reward of -9 as agent policies cycle and learn to represent nearly deterministic strategies during most points in training. The NE for Lava World requires a mixed-strategy in which the adversary samples a high-entropy (non-uniform) distribution of hidden goals. The degenerate solution to Minimax is reached by both algorithms.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Worst-case average episode reward among goals in F λ vs timesteps collected for each two-player zero-sum game objective optimized with both PSRO and PPO Self-Play.\n\nF SAMPLE EFFICIENCY IN MUJOCO EXPERIMENTS\n\nIn figure 10, we show worst-case protagonist reward among environment parameters in the feasible set as a function of timesteps collected by PPO learners in PSRO for each λ value considered. FARR and regret take more timesteps per PSRO iteration than minimax because they both train evaluation policies BR(θ) for each adversary strategy θ added to the population in order to evaluate their utility functions in the PSRO restricted game.\n\n(a)\n\n(b)\n\n(c)\n\nFigure 10: Worst-case MuJoCo HalfCheetah (a), Hopper (b), and Walker2D (c) average episode reward among task parameters in the feasible set F λ as a function of timesteps collected for FARR and other baselines with multiple values of λ.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nG ENVIRONMENT DETAILS\n\nG.1 LAVA WORLD\n\nIn the Lava World grid environment, the protagonist uses discrete actions to move in each of the 4 cardinal directions. For observations, the protagonist receives a one-hot encoding of its current location, and the protagonist does not observe the goal location. The adversary strategy space Θ is to define the hidden goal location and consists of every grid cell location in the environment’s 5x5 grid with the exception of the protagonist’s fixed starting location.\n\nThe protagonist receives a reward of -1 in each timestep that it does not reach the hidden goal location suggested by the adversary and -15 if it moves into a lava cell, even if the lava cell was a goal. An episode ends after either 20 timesteps elapse or the goal is reached.\n\nG.2 MUJOCO ENVIRONMENTS\n\nThe MuJoCo environments use the Mujoco physics engine (Todorov et al., 2012) and are modified versions of the perturbed robotic control environments originally presented in Pinto et al. (2017).\n\nIn each environment variation (HalfCheetah, Hopper, Walker2D), a max episode duration of 200 timesteps is imposed, and the proportion of time remaining in the range [0, 1] is appended to each task’s original observation. We use the continuous action space variants of each task.\n\nThe adversary strategy space Θ consists of continuous α and β parameters in the range (0, 10] for a beta distribution B(α, β) used to generate horizontal perturbing forces sampled and applied to the robot’s torso every timestep. Each timestep, a new horizontal force F ∈ [−Fmax, Fmax], F = X(2Fmax) − Fmax is generated where X ∼ B(α, β) and Fmax = 100.\n\nWhen discretizing values of θ = (α, β) for evaluation purposes to measure a policy’s performance across values in Θ or F λ, we use α, β ∈ {0.01, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0}.\n\nH TRAINING DETAILS\n\nProtagonist RL training details are provided below for each environment. Policies used to estimate BR(θ) for a given θ use the same training procedure and parameters as the protagonist. Like Dennis et al. (2020), we train protagonist policies on the easier-to-learn unmodified environment reward Up rather than our two-player game objective U λ p because the only component of the game utility that the protagonist can affect is Up. A protagonist best-response that maximizes Up also maximizes U λ p . Critically, we still calculate U λ λ to calculate the meta-game NE strategy σ = (σp, σθ).\n\np in the PSRO empirical payoff matrix U Π\n\nλ and use U Π\n\nH.1 LAVA WORLD\n\nWe train Lava World protagonist RL policies using DDQN Van Hasselt et al. (2016). All Lava World protagonist RL policies are stopped training after either 150,000 timesteps are collected or once performance plateaus (average episode return doesn’t improve by 0.5 over 20,000 timesteps and a minimum of 80,000 timesteps is collected). Lava World DDQN hyperparameters are presented below. Our RL code was built using the RLlib framework Liang et al. (2018), and any hyperparameters not specified are the version 1.0.1 defaults. We use an infeasibility penalty of C = 50.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nalgorithm circular replay buffer size prioritized experience replay total rollout experience gathered each iter learning rate batch size optimizer TD-error loss type target network update frequency MLP layer sizes activation function discount factor γ exploration ε\n\nDDQN Van Hasselt et al. (2016) 50,000 No 8 steps 0.007 1024 Adam (Kingma & Ba, 2014) MSE every 4,000 steps [256, 256] tanh 1.0 Linearly annealed from 0.5 to 0.01 over 20,000 timesteps\n\nTable 1: Lava World protagonist DDQN hyperparameters\n\nalgorithm GAE λ entropy coeff clip param KL target KL coeff learning rate train batch size SGD minibatch size num SGD epochs on each train batch shared policy and value networks value function clip param MLP layer sizes activation function discount factor γ\n\nPPO Schulman et al. (2017) 0.9 0.007 0.276 3e-4 0.0016 5e-4 8192 64 40 No 10 [256, 256] Tanh 1.0\n\nTable 2: Lava World PPO self-play protagonist hyperparameters\n\nalgorithm GAE λ entropy coeff clip param KL target KL coeff learning rate train batch size SGD minibatch size num SGD epochs on each train batch shared policy and value networks value function clip param MLP layer sizes activation function discount factor γ\n\nPPO Schulman et al. (2017) 0.95 0.006 0.292 0.092 0.168 3e-4 8192 64 30 No 100 [256, 256] Tanh 1.0\n\nTable 3: Lava World PPO self-play adversary hyperparameters\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nIn PSRO, to calculate the payoff matrix U Π λ , we estimate Up(πp, θ) for each pairing of player policies πp ∈ Πp and θ ∈ Πθ using a single rollout because both Lava World environment dynamics and evaluation DDQN policies are deterministic. The normal-form meta-game Nash Equilibrium over U Π λ is calculated using 2000 iterations of Fictitious Play (Brown, 1951). Calculating the meta-game NE typically takes a second or less of wall-time compute.\n\nDuring PSRO evaluation, we measure the performance of the protagonist meta-game mixed strategy σp, in which a new protagonist policy πp ∼ σp is sampled at the begining of each episode.\n\nIn self-play, the adversary is trained as a single-step agent via PPO. For simplicity, we keep the same network architecture for all self-play agents, and the adversary observes a constant vector of zeros.\n\nH.2 MUJOCO\n\nWe train MuJoCo environment protagonist RL policies using PPO (Schulman et al., 2017). Each PPO model consists of an MLP followed by an LSTM with shared weights between the policy and value function, branching into final output layers after the LSTM. PPO hyperparameters for each MuJoCo environment are presented below. Any hyperparameters not specified are the RLlib version 1.0.1 defaults. We use an infeasibility penalty of C = 1e6.\n\nalgorithm GAE λ entropy coeff clip param KL target KL coeff learning rate train batch size SGD minibatch size num SGD epochs on each train batch shared policy and value networks value function loss coeff value function clip param continuous action range MLP layer sizes activation function LSTM cell size LSTM max sequence length discount factor γ RL policy training stopping condition\n\nPPO Schulman et al. (2017) 0.9 0.01 0.001 0.004 0.522 5e-4 4096 64 5\nYes 0.001 100 [-1.0, 1.0] for each dim [32] Tanh 32 20 0.99 7e6 timesteps\n\nTable 4: HalfCheetah PPO hyperparameters\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nalgorithm GAE λ entropy coeff clip param KL target KL coeff learning rate train batch size SGD minibatch size num SGD epochs on each train batch shared policy and value networks value function loss coeff value function clip param continuous action range MLP layer sizes activation function LSTM cell size LSTM max sequence length discount factor γ RL policy training stopping condition\n\nPPO Schulman et al. (2017) 0.9 0.001 0.002 0.036 0.013 7e-4 4096 32 5\nYes 0.001 10 [-1.0, 1.0] for each dim [64, 64] Tanh 32 20 0.99 6e6 timesteps\n\nTable 5: Hopper PPO hyperparameters\n\nalgorithm GAE λ entropy coeff clip param KL target KL coeff learning rate train batch size SGD minibatch size num SGD epochs on each train batch shared policy and value networks value function loss coeff value function clip param continuous action range MLP layer sizes activation function LSTM cell size LSTM max sequence length discount factor γ RL policy training stopping condition\n\nPPO Schulman et al. (2017) 0.95 0.0 0.014 0.005 0.007 9e-4 1024 64 10 Yes 1e-4 1000 [-1.0, 1.0] for each dim [32] Tanh 32 20 1.0 6e6 timesteps\n\nTable 6: Walker2D PPO hyperparameters\n\nIn PSRO, to calculate the payoff matrix U Π λ , we estimate Up(πp, θ) for each pairing of player policies πp ∈ Πp and θ ∈ Πθ using 100 rollouts as perturbed MuJoCo environment transition dynamics are stochastic. The normal-form meta-game Nash Equilibrium over U Π λ is also calculated using 2000 iterations of Fictitious Play.\n\nAlthough PSRO convergence guarantees are not provided for continuous-action environments, in McAleer et al. (2021), McAleer et al. (2022b), and our own experiments, PSRO reliably produces meta-game NE mixed strategies that are empirically difficult for an opponent to exploit.\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nI COMPUTATIONAL COSTS\n\nExperiments were performed on a local computer with 128 logical CPU-cores, 4 RTX 3090 GPUs, and 512GB of RAM. Due to small network sizes and comparably high overhead of CPU-based environments, logging, and other tasks, most experiments were performed without GPU acceleration. All individual training runs for a given player against a fixed opponent took 5 CPU-cores each. Lava world experiments individually ran for roughly 8 to 24 hours each, while MuJoCo experiments individually ran for roughly 72 hours each.\n\nJ CODE\n\nA GitHub link for our experiment code will be provided under the MIT license in an updated version of this work.\n\nOur code is written on top of the RLlib framework (Liang et al., 2018) and uses environments built using the MuJoCo physics engine (Todorov et al., 2012), both of which are open-source and available under the Apache-2.0 Licence.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis manuscript investigates the setting where there exists a mismatch between the training MDP and the testing MDP (known as robust RL). While robust RL is quite conservative in that the adversary could minimize the protagonist's utility over every MDP in the uncertainty set, this manuscript believes that it is too conservative. Instead, the manuscript defined a feasible set of MDPs that the adversarial could achieve, by asserting that there must exist a policy in that adversarial MDP to achieve at least $\\lambda$ in terms of the return. It subsequently lifts the constraint by replacing it with a substantially large penalty if the support of the random MDP of the adversarial has a positive density on the non-feasible area. With this setting, the robust RL policy and the adversarial could be trained by PSRO. Simulations are conducted on gridworld and 3 MuJoCo tasks.\n\n# Strength And Weaknesses\n\nI agree that robust RL is indeed too conservative and the new feasible uncertainty set is relevant. The new setting proposed by the manuscript makes sense to me. It is also interesting that the setting could be solved by PSRO in quite an intuitive way. Some detailed reviews follow.\n\nPros:\n\n1. The new setting is an interesting extension towards robust RL\n2. The proposed algorithm through PSRO is intuitive and sensible.\n3. Presentation of this manuscript is easy to follow.\n\nCons:\n\nThe major concern of mine is how useful this method could be, in practice. In the experiments, the agents that are trained against the *feasible adversary* demonstrates some decent performance in only 3 MuJoCo tasks (I have to assume that it doesn't pan out on the rest of the tasks). Notice that this is when the algorithm trained against the feasible adversary is tested against the feasible adversary. What if the agent trained against the feasible adversary is tested against a general adversary (potentially described by a real application task)? The manuscript does not provide such an answer, but I'm not very confidence on this.\n\nMisc:\n\nIn the abstract \"In real-world environments, choosing the set of possible values for robust RL can be a difficult task\", the term \"value\" could be confused with the value function of RL. Use a different term.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe manuscript has a great presentation and is enjoyable to read. The clarity and quality are excellent and the novelty is significant due to the introduction of the new robust RL setting. I did not check the reproducibility.\n\n# Summary Of The Review\n\nI'm positive on this manuscript for its novelty in the new setting and its quality in presentation. I have reservation, though, on its empirical performance.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMULTI-SCALE SINUSOIDAL EMBEDDINGS ENABLE LEARNING ON HIGH RESOLUTION MASS SPECTROMETRY DATA\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nSmall molecules in biological samples are studied to provide information about disease states, environmental toxins, natural product drug discovery, and many other applications. The primary window into the composition of small molecule mixtures is tandem mass spectrometry (MS2), which produces high sensitivity and part per million resolution data. We adopt multi-scale sinusoidal embeddings of the mass data in MS2 designed to meet the challenge of learning from the full resolution of MS2 data. Using these embeddings, we provide a new state of the art model for spectral library search, the standard task for initial evaluation of MS2 data. We also investigate the task of chemical property prediction from MS2 data, that has natural applications in high-throughput MS2 experiments and show that an average R2 of 80% for novel compounds can be achieved across 10 chemical properties prioritized by medicinal chemists. We vary the resolution of the input spectra directly by using different floating point representations of the MS2 data, and show that the resulting sinusoidal embeddings are able to learn from high resolution portion of the input MS2 data. We apply dimensionality reduction to the embeddings that result from different resolution input masses to show the essential role multi-scale sinusoidal embeddings play in learning from MS2 data.\n\n1\n\nINTRODUCTION\n\n1,000 Daltons) contents of complex biological Metabolomics is the study of the small molecule ( samples. Tandem Mass Spectrometry (MS/MS), in conjunction with chromatography, is one of the most commonly used tools in metabolomics. Tandem Mass Spectrometry works by measuring with very high resolution the masses of molecules and their constituent fragments. While MS/MS techniques are highly sensitive and precise, inferring the identity of the molecules and their properties from the resulting mass spectra is commonly regarded as one of metabolomics’ primary bottlenecks (Dunn et al., 2013). Improved tools for these tasks will impact applications across many areas of science including disease diagnostics, characterization of disease pathways, development of new agrochemicals, improved forensics analysis, and the discovery of new drugs (Zhang et al., 2020).\n\n(cid:62)\n\nProfiling unknown molecules with mass spectrometry consists of several steps. First, molecules of interest are ionized and separated by their mass to charge ratio (m/z), resulting in the MS1 spectrum. Then, individual “precursor” ions are fragmented, and the m/z’s of the fragments are recorded in the same manner. The resulting spectrum contains the m/z’s and intensities (together, the “peaks”) of all resulting fragments, and is called the MS2 spectrum. See Glish & Vachet (2003).\n\nIn recent years, several machine learning methods have been developed to identify the structures and properties of small molecules from their mass spectra. These approaches (Huber et al., 2021a;b; Kutuzova et al., 2021; Litsa et al., 2021; Shrivastava et al., 2021; van Der Hooft et al., 2016) historically discretize m/z (via tokenization or binning). However, the m/z values obtained in modern mass spectrometry experiments are collected with parts per million levels of resolution. There are three critical reasons to expect that modeling m/z values as numeric quantities, in contrast to discretized values, is the appropriate technique. First, we know that relevant chemical information is present at the millidalton level (Jones et al., 2004; Pourshahian & Limbach, 2008) and discretization schemes typically strip this information away. Additionally, mass differences between peaks represent\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nfragmentation patterns, and are therefore relevant to understanding the molecule. Learning to utilize this information from tokens is exceedingly difficult. Finally, discretization is susceptible to edge effects, where slight mass difference can map to different bins if the masses are close to the edge of a bin. In light of these considerations, we model m/z as a numerical value using sinusoidal embeddings, which we hypothesize will enable us to capture information across many orders of magnitude.\n\nIn this work, we apply a numerical representation of m/z that uses sinusoidal embeddings across multiple scales to retain the information content of MS2 data across its entire mass resolution. To demonstrate the ability of these embeddings to enable learning at the high resolution of MS2 experimental data, we apply them to a search task and 10 regression tasks.\n\nOur first test task is spectral library search (Stein & Scott, 1994). In spectral library search, spectra from unknown compounds are compared to spectra in a database to find matches using a similarity function over pairs of spectra. This task is the primary method used in standard metabolomics analyses, but is challenging because spectra for a compound vary widely with experimental conditions. We find that a similarity function based on sinusoidal embeddings achieves state of the art both for finding the exactly correct compound, and also for finding close structural analogs, which is useful for compounds not contained in spectral databases.\n\nWe further investigate predicting chemical properties relevant to drug discovery from MS2 data and apply the same modeling approach to this task. We achieve 80% average R2 for out of sample molecules across 10 properties, which is high enough to enable first-pass filtering and selection of candidate drug molecules in high-throughput experiments based solely on spectral data.\n\nIn each task, using sinusoidal embeddings results in a new state of the art. The confirmation of across-task improvement provides evidence that the embeddings are a general improvement, rather than task specific. To determine whether these results are due to learning from the high resolution portion of the data, we experiment with inputting MS2 data cast to half precision floating point numbers, and show that the performance noticeably degrades relative to double precision. Finally, we visualize embeddings generated with varied floating point precision MS2 inputs using UMAP (McInnes et al., 2018) projections and show that non-trivial high dimensional structure only emerges with sufficiently high precision input. Taken together, these results are the first clear evidence that sinusoidal embeddings enable effective learning from high mass resolution MS2 data across tasks in metabolomics.\n\n1.1 RELATED WORK\n\nModeling numerical data in terms of sinusoidal functions has a long history in many scientific fields. In machine learning, sinusoidal embeddings are most commonly used to encode the discrete positions of natural language token inputs in transformer models, which are otherwise position-agnostic (Vaswani et al., 2017). Other work has used sinusoidal embeddings for multi-dimensional positional encoding in image recognition (Li et al., 2021a). In mass spectrometry, sinusoidal embeddings have been used in proteomics for inferring protein sequences from mass values, either with (Qiao et al., 2019) or without (Yilmaz et al., 2022) an initial mass binning step, but without exploring their role in model performance or comparing to alternative approaches. These techniques have never been applied in the domain of metabolomics. Metabolomics differs from proteomics in that the molecules of of interest are 2 - 3 orders of magnitude lower mass and are graph structured rather than sequential as in proteins. Consequently, the tasks and challenges of modeling of small molecules are sharply divergent from those in proteomics.\n\nFor modeling m/z values in the field of metabolomics, previous machine learning models have relied primarily on discretization of the continuous mass inputs. This is usually accomplished by binning m/z values into fixed length vectors with peak intensity as the value for each element. Various authors have used binned representations of spectra for spectral library search (Huber et al., 2021b), unsupervised topic modeling (van Der Hooft et al., 2016), and molecule identification (Kutuzova et al., 2021; Litsa et al., 2021). Alternatively, masses have been tokenized via rounding for tasks such as unsupervised spectral similarity (Huber et al., 2021a) and molecule prediction from synthetic data (Shrivastava et al., 2021).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nOther approaches rely on tokenization of m/z values by assigning a molecular formula to each peak, and taking the formula as a token. Even when this method is able to uniquely identify molecular formulas for every fragment ion, it still faces a problem endogenous to tokenization: it’s very difficult to reason about the relationships between discrete tokens except by pattern recognition over a very large number of training examples. Böcker & Rasche (2008) addressed this difficulty by modeling the fragmentation process with a tree structure, and Dührkop et al. (2015) used these fragmentation trees as inputs to a molecular fingerprint prediction model that is used to assign molecules to spectra from molecular structure databases. The tree-based approaches are prohibitively computationally expensive for large volumes of data Cao et al. (2021).\n\n2 MODEL\n\n2.1 BASE MODEL ARCHITECTURE\n\nAn MS2 spectrum,\n\nS =\n\n(cid:110)\n\n(m/z, I)precursor , (m/z, I)fragment1\n\n, . . . , (m/z, I)fragmentN\n\n(cid:111)\n\n,\n\n(1)\n\nis composed of a precursor m/z and a set of N fragment peaks at various m/z’s and intensities (I). Various other data are typically collected, including precursor abundance, charge, collision energy, etc, but these are not used in this work. Transformer encoders (Vaswani et al., 2017) without the positional encoding are explicitly fully-symmetric functions and hence are ideally suited to model a set of fragmentation peaks. We therefore take our base model, SpectrumEncoder (S), to be a transformer encoder, whose inputs are a set of (m/z, I) pairs that includes the precursor along with all of the fragment peaks. We normalize the intensities to a maximum of 1 for fragments and assign an intensity of 2 to the precursor. Finally, we take as output the embedding vector from the final transformer layer corresponding to the precursor input. See Figure 2. Note, the use of transformers to study synthetic MS2 spectra has been explored in Shrivastava et al. (2021).\n\nThis flexible approach enables us to experiment with and compare various representations of MS2 data. We pursue two approaches: tokenization of m/z and modeling m/z as numerical values via sinusoidal embeddings. Before describing these, we need the following definition for a simple two layer feed forward MLP,\n\nFF (x) = W2ReLu (W1x + b1) + b2.\n\n(2)\n\nNote that each occurrence of FF we employ below is a separate instance with distinct weights.\n\n2.1.1 TOKENIZED m/z PEAK EMBEDDING\n\nOur first approach is to discretize m/z by rounding to 0.1 precision and treating these objects as tokens (as in an NLP context). These tokens are embedded in a dense vector space as in Huber et al. (2021a); Mikolov et al. (2013), via an embedding function TE. We then construct the token peak embedding,\n\nPEtoken (m/z, I) = FF (TE (m/z) ∥ I) ,\n\n(3)\n\nwhere ∥ denotes concatenation. A diagram of PEtoken is shown in Figure 1a.\n\n2.1.2 SINUSOIDAL m/z PEAK EMBEDDING\n\nWe now consider a numerical peak embedding that utilizes the full m/z precision. We employ a sinusoidal embedding as follows:\n\nSE (m/z, 2i; d) = sin\n\n2π\n\nλmin\n\n\n\n(cid:34)\n\nSE (m/z, 2i + 1; d) = cos\n\n2π\n\nλmin\n\n\n\n(cid:34)\n\n(cid:19)2i/(d−2)(cid:35)−1\n\n(cid:19)2i/(d−2)(cid:35)−1\n\n\n\nm/z\n\n\n\n\n\nm/z\n\n .\n\n(cid:18) λmax λmin\n\n(cid:18) λmax λmin\n\n(4)\n\n(5)\n\nThe frequencies are chosen so that the wavelengths are log-spaced from λmin = 10−2.5 Daltons to λmax = 103.3 Daltons, corresponding to the mass scales we wish to resolve. This embedding\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Architecture of PEtoken.\n\n(b) Architecture of PEsin.\n\nFigure 1: Architectures used to embed MS2 peaks (composed of a m/z and an intensity I). Both approaches produce an embedding of m/z of dimension d. The m/z embedding in Figure 1a (left) is produced by a learned embedding layer. The m/z embedding in Figure 1b (right) is produced by a sinusoidal embedding followed by a two layer MLP. In both cases, an intensity value I is appended to make a vector of dimension d + 1. This is then fed into a simple feed-forward network to produce a peak embedding of dimension d. We use blue to highlight the network components involved in producing an m/z embedding and purple to highlight the full peak embedding after intensity is incorporated.\n\nis inspired by Li et al. (2021a); Vaswani et al. (2017). Similarly to Equation 3, we construct the sinusoidal peak embedding,\n\nPEsin (m/z, I) = FF (FF (SE (m/z)) ∥ I) .\n\n(6)\n\nWe also experiment with casting the m/z inputs as half, single, and double precision floating point values. Equations 4-5 are computed at the precision specified by m/z and the results are then cast to the precision of the model weights used for training. A diagram of PEsin is shown in Figure 1b.\n\n2.1.3 TRANSFORMER\n\nThe set of fragment peaks and the precursor are passed through either of the two peak embeddings specified above. The result is a sequence of embedding vectors which is then passed through a transformer encoder:\n\nSpectrumEncoder (S) =\n\nTransformerEncoder\n\n(cid:16)\n\nPE (m/z, I)precursor , . . . , PE (m/z, I)fragmentN\n\n(cid:17)\n\n.\n\n(7)\n\nHere PE stands in for either PEtoken or PEsin. TransformerEncoder has embedding dimension d = 512 and six layers, each with 32 attention heads and an inner hidden dimension of d. Note that all hidden and embedding layers in our peak embeddings above have dimension d as well. As mass spectra have no intrinsic ordering, we opt to not include a positional encoding.\n\nIn order to get a single embedding vector as an output, the final transformer layer query only attends to the first embedding, corresponding to the position of the precursor m/z. A diagram of our full model architecture is shown in Figure 2.\n\n2.2 MODEL TRAINING\n\nWe train the base models discussed here via two independent tasks aimed at applications within metabolomics and medicinal chemistry. These tasks are described in detail below. We build these models using PyTorch (Falcon, 2019; Paszke et al., 2019) and train them with the Adam (Kingma & Ba, 2014) optimization algorithm with parameters β1 = 0.9, β2 = 0.999, and learning rate α = 5.0 × 10−5. We also use a weight decay (Krogh & Hertz, 1991) parameter of 0.1, a gradient clipping a parameter of 0.5, and a dropout (Srivastava et al., 2014) parameter of 0.1. All models were trained using between 25 and 50 epochs. Finally, all model weights were trained using half precision floating point values, regardless of the precision of the m/z inputs.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Full model architecture that highlights how we featurize and embed an MS2 spectrum in a dense vector space.\n\n2.2.1 SPECTRAL SIMILARITY WITH SIAMESE NETWORKS\n\nA common workflow for the analysis of mass spectrometry data is to compute some notion of spectral similarity for pairs of spectra that is intended to correlate with the molecular similarity of the underlying compounds (Yilmaz et al., 2017). This can then be used for tasks such as spectral library search (Stein & Scott, 1994) and molecular networking (Quinn et al., 2017; Watrous et al., 2012). Historically, most spectral similarity metrics used in the field have been heuristic-based (Li et al., 2021b; Yilmaz et al., 2017). More recently, spectral similarity methods that make use of techniques from deep learning have been developed (Huber et al., 2021a;b). The latter of these, MS2Deepscore, is a Siamese network that trains a simple MLP (by binning m/z) to predict a measure of molecular similarity from pairs of mass spectra. As is common practice in this field, we use a molecular similarity given by a Tanimoto similarity computed from RDKit Landrum et al. (2013) topological fingerprints of the corresponding molecular structures (Bajusz et al., 2015). The loss function is the mean square error (M SE) between the ground truth molecular similarity and the cosine similarity evaluated on the dense embedding vectors generated by the base model.\n\nMolecular similarity scores range from 0 to 1. The scores of randomly sampled pairs from our labeled dataset are not uniformly distributed and strong similarity matches are rare. Huber et al. (2021b), describe a procedure to sample pairs of spectra such that the molecular similarities will be uniformly distributed. We use this procedure to sample pairs for training and evaluation tasks.\n\nFollowing Huber et al. (2021b) we also train a Siamese network, but we do so using the base model transformer architectures outlined in Section 2.1 instead of with the binned mass vectors used by Huber. We report performance results for models trained using PEtoken and PEsin below in Section 3.2.\n\n2.2.2 PROPERTY PREDICTION\n\nWe also train the base model architecture to predict 10 chemical properties relevant to medicinal chemistry directly from an MS2 spectrum. We do so by training the following network,\n\nProperties = FF (SpectrumEncoder (S)) ,\n\n(8)\n\nwhere the final layer has dimension equal to the number of properties we wish to predict. We also scale all of our training labels to zero mean and unit variance. For inference, we run predicted properties through an inverse-scaler. As in Section 2.2.1, we train multiple versions of this property prediction model. Due to the lack of existing methods for comparison, we also train a version of Equation 8 on binned spectrum representations for a simple baseline. We replace SpectrumEncoder with the MS2Deepscore feed forward architecture found in Huber et al. (2021b) for this baseline.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nThe properties we predict are standard indicators of druglikeness and bioavailability in the field of medicinal chemistry. These include the properties that make up the Quantitative Estimate of Druglikeness (QED) along with several others (Bickerton et al., 2012; Lipinski et al., 2012; Veber et al., 2002). For the complete list of properties, see Table 2. All properties are computed deterministically from chemical structure using RDKit (Landrum et al., 2013), so there is no additional dependency on experimental data.\n\n3 RESULTS\n\n3.1 DATA\n\nWe construct a set of labeled MS2 spectra by combining a number of standard public and commercial datasets and one small dataset ( 0.1% of spectra) that requires permission to access because it contains psychoactive substances (Wang et al., 2016; Mehta, 2020; Sawada et al., 2012; Horai et al., 2010; Mikaia et al., 2014; Smith et al., 2005; Mardal et al., 2019). See our Reproducibility Statement for additional preprocessing details. Chemical properties and fingerprints are then computed from the cleaned molecules using RDKit (Landrum et al., 2013). Our resulting dataset has 1,251,830 spectra corresponding to 45,351 distinct structures.\n\nIn metabolomics experiments, biological samples contain compounds that have previously been profiled in MS2 libraries (“known\" compounds) and compounds that have not been profiled (“novel compounds\"). Identifying the structure and properties of known molecules is an important task on its own (e.g. “dereplication”). We therefore split our data so that the test and development sets are partially disjoint from train at the level of structures, and fully disjoint from train at the level of spectra. This enables us to evaluate model performance on both “known\" and “novel\" compounds. To implement this split, we partition our set of labeled spectra into a training set, development set (i.e. validation set), and test set as follows. We randomly select a set of 1002 molecules from our data and place them and all of their associated spectra into the development set. From the remaining molecules, we follow the same procedure and place 1002 molecules into the test set. This creates a test and development set containing 1002 molecules each that are fully disjoint from each other. The remaining molecules are assigned the training set. We then remove 998 spectra that correspond to 941 molecules represented in the training set to the development set, and 998 to the test set, corresponding to 953 structures from the training set. The final training, development, and test sets have 1,214,812, 18,750, and 18,268 spectra respectively. The training set contains 43,347 structures. We use the development set for hyperparameter optimization and only access the test set to compute reported metrics.\n\n3.2 SPECTRAL SIMILARITY\n\nTo benchmark spectral library search, we apply two criteria for accurate molecular identification: retrieving the exact molecule and retrieving a molecule which is an approximate match, i.e. has molecular similarity (defined in section 2.2.1) greater than 0.6. To account for widely varying numbers of spectra for each molecule, we report the accuracies as macro averages over molecular structures.\n\nWe train a number of spectral similarity models that report state of the art performance or are in standard usage as described in Section 2.2.1. We refer to the sinusoidal Siamese transformer and tokenized Siamese transformer models as the sinusoidal and tokenized models respectively. To test performance against reported state of the art models, we train both a Spec2Vec (Huber et al., 2021a) and an MS2Deepscore (Huber et al., 2021b) model on our data. We also show results modified cosine spectral similarity, which is a standard, unlearned, similarity function common in applications (Watrous et al., 2012). In Table 1 we report spectral library search accuracies for both known and novel compounds obtained from a number of spectral similarity models. We omit exact match accuracies for novel compounds since these have been sampled so that no exact matches can be found. For models where it is applicable, the same table reports the M SE loss between predicted and actual molecular similarity on spectrum pairs drawn from train, known, and novel spectra sets.\n\nWe find that learned approaches improve on modified-cosine spectral library search. Of the learned approaches, we find that Spec2Vec has strong, only surpassed by the double precision sinusoidal model, spectral library search performance on novel, but only beats MS2Deepscore on known.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Spectral library search performance\n\nSpectral Set Match\n\nModified Cosine Spec2Vec MS2Deepscore Tokenized m/z Sinusoidal m/z (float16) Sinusoidal m/z (float64)\n\nM SE known\n\ntrain\n\nnovel\n\nSpectral library search accuracy\n\nknown Exact Approx.\n\nnovel Approx.\n\n0.026 0.017\n\n0.029 0.019\n\n0.047 0.030\n\n0.019\n\n0.019\n\n0.03\n\n0.61 0.903 0.852 0.933\n\n0.913\n\n0.672 0.954 0.918 0.961\n\n0.955\n\n0.013\n\n0.015\n\n0.024\n\n0.937\n\n0.97\n\n0.351 0.414 0.387 0.382\n\n0.401\n\n0.435\n\nMS2Deepscore only outperforms the tokenized model on novel and underperforms everything on known. Moreover, MS2Deepscore is substantially less accurate at predicting the specific pair molecular similarity on known and novel spectra when compared to all transformer approaches. The double precision sinusoidal model produced the best performance on all evaluation tasks. This model produces state of the art known spectral library search accuracies of 0.937 and 0.97 for exact and approximate matching respectively. Numerical representation of MS2 m/z via sinusoidal embeddings consistently outperforms all the discretization based approaches we benchmarked.\n\nTo assess whether the multi-scale embeddings are learning from high resolution information, we also train and evaluate versions of our sinusoidal model where m/z’s are cast to half precision floating point values. Half precision floating point can only resolve up to approximately parts per O(10, 000), far below the parts per million precision of experimental mass spectrometry data. The performance of the half precision model, reported in Table 1, drops to be on par with the tokenized model (worse on known and better on novel). We do not report results with single precision m/z since these were indistinguishable from double precision m/z. On all evaluation metrics, using m/z inputs of precision greater than 16 bits dramatically improves performance of the sinusoidal model. This indicates that sinusoidal embedding of m/z values enables learning from the the high resolution portion of mass spectrometry data.\n\n3.2.1 QUALITATIVE EMBEDDING ANALYSIS\n\nTo further characterize the respective properties of token and multi-scale sinusoidal embeddings, we inspect UMAP (McInnes et al., 2018) projections of our m/z embeddings in Figure 3. For this analysis, we use siamese transformer models described in Section 2.2.1. For our sinusoidal models, we embed 50,000 m/z values between 0 and 1,000 Daltons using FF (SE (m/z)). We do not use the full peak embedding function as we are not interested in the embedding of intensity information. Because our tokenization procedure involves rounding m/z values to 1 decimal place, for the tokenized model we only embed 10,000 m/z values between 0 and 1,000 Daltons.\n\nWhile m/z measurements are continuous, the space they represent is inherently discrete. Two molecular fragments differing in atomic composition could nevertheless have a very similar m/z’s. However, the large separation between molecular mass scales and the mass scale at which nuclear forces manifest themselves means that the relative atomic composition may be deduced from the fractional m/z (Jones et al., 2004; Pourshahian & Limbach, 2008), defined by {m/z} = m/z − ⌊m/z⌋. Therefore, we expect a quality embedding of m/z to embed fragments similar in both m/z and {m/z} close to one another, while paying greater attention to the latter.\n\nAs is seen in Figure 3, embeddings from tokenized and low precision sinusoidal models are able to capture general trends in m/z. However, they fail to preserve distances in m/z and show little to no structure in {m/z}. In contrast, the high precision sinusoidal embeddings allow our models to represent important information in {m/z}, while preserving distance in m/z.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 3: (3a) UMAP projections of m/z embeddings colored by m/z value. (3b) UMAP projections of m/z embeddings colored by {m/z}. High resolution sinusoidal embeddings imbue model latent space with additional structure not found in other baseline models.\n\n3.3 PROPERTY PREDICTION\n\nMany applications in metabolomics can be unblocked with knowledge of just a limited set of chemical properties, without need for identification of molecular structure. Towards this end, we investigate the parallel task of chemical property prediction from MS2 spectra. We evaluate sinusoidal vs. tokenized m/z embedding performance on a list of properties that have compelling applications in drug discovery and are easily computable from our molecule labels. For this experiment, we train a sinusoidal model and several baseline models as described in Section 2.2.2.\n\nIn Table 2, property prediction models are evaluated on known and novel structures using R2. All transformer models outperform our feed forward baseline. Half precision sinusoidal models produced an R2 of 0.746 (0.936) on known (novel) molecules averaged across all 10 properties. These results are competitive with tokenization but substantially underperform when compared to higher precision sinusoidal embeddings. When m/z values are specified at double precision, sinusoidal embeddings show improvements over tokenization on each property, resulting in an 11% improvement over tokenization on R2 for novel molecules averaged across all properties.\n\nThis is further evidence that the multi-scale information is captured by sinusoidal embeddings and allows improved generalization across tasks. In addition, the performance is strong enough to enable medicinal chemistry applications, such as hit prioritization based on druglikeness. This has major implications in the drug discovery context, as inspecting the properties of unknown molecules in complex mixtures was previously not possible without difficult isolation steps and individual experimentation.\n\n4 CONCLUSIONS\n\nThe results presented here are the first demonstration that the sinusoidal representation of m/zs measured in tandem mass spectrometry experiments enables deep learning models to learn from high resolution mass data. These results include state of the art performance on spectral library search and\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: R2 for predicted chemical properties\n\nProperty\n\nFeed Forward\n\nSinusoidal m/z Tokenized m/z Known Novel Known Novel Known Novel\n\nall atomic log P number of hydrogen bond acceptors number of hydrogen bond donors polar surface area number of rotatable bonds number of aromatic rings number of aliphatic rings number of heteroatoms fraction of sp3 carbons quantitative estimate of druglikeness\n\n0.824 0.768 0.835 0.834 0.851 0.801 0.838 0.839 0.823 0.846 0.806\n\n0.604 0.357 0.762 0.572 0.716 0.669 0.372 0.638 0.719 0.618 0.617\n\n0.943 0.919 0.969 0.924 0.965 0.939 0.934 0.943 0.968 0.951 0.912\n\n0.72 0.437 0.887 0.683 0.862 0.792 0.474 0.762 0.902 0.723 0.681\n\n0.976 0.968 0.987 0.968 0.986 0.976 0.981 0.974 0.988 0.977 0.952\n\n0.8 0.622 0.941 0.69 0.907 0.838 0.655 0.821 0.946 0.821 0.755\n\nproperty prediction tasks in metabolomics. The property prediction results of are sufficient quality to functionally inform and advance drug-discovery efforts.\n\nMoreover, we present results that compare across numerical floating point precisions for the mass inputs, and show that sinusoidal embeddings perform better when higher mass resolution data is used. Finally, we visualize high-dimensional structure of the mass embeddings that emerges only when high precision mass values are utilized. We expect that sinusoidal embeddings of m/z will be a useful component of further machine learning applications with MS2 data.\n\nREPRODUCIBILITY STATEMENT\n\nWe train our model on a combination of free public datasets (Wang et al., 2016; Mehta, 2020; Sawada et al., 2012; Horai et al., 2010), commercially available datasets (Mikaia et al., 2014; Smith et al., 2005), and one small dataset (0.1% of all spectra) available if a user-group of qualified researchers is joined (Mardal et al., 2019). The necessary preprocessing steps are outlined in Section 3.1. Additionally, we allow spectra collected across instrument types and experimental parameters including positive and negative ion mode, collision energies, etc. We constrain that spectra in our dataset have at least 5 peaks and at least 3 decimal places of m/z resolution. We strip all stereochemistry from our molecular structure labels, which is a common step taken in MS2 modeling and allows for better molecule-disjoint splitting.\n\nThe model architecture is explained in detail in Section 2 and all parameters used in model training are specified in Section 2.2. We include our source code in the supplementary materials.\n\nREFERENCES\n\nDávid Bajusz, Anita Rácz, and Károly Héberger. Why is tanimoto index an appropriate choice for\n\nfingerprint-based similarity calculations? Journal of cheminformatics, 7(1):1–13, 2015.\n\nG Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins.\n\nQuantifying the chemical beauty of drugs. Nature chemistry, 4(2):90–98, 2012.\n\nSebastian Böcker and Florian Rasche. Towards de novo identification of metabolites by analyzing\n\ntandem mass spectra. Bioinformatics, 24(16):i49–i55, 2008.\n\nLiu Cao, Mustafa Guler, Azat Tagirdzhanov, Yi-Yuan Lee, Alexey Gurevich, and Hosein Mohimani. Moldiscovery: learning mass spectrometry fragmentation of small molecules. Nature Communications, 12(1):1–13, 2021.\n\nKai Dührkop, Huibin Shen, Marvin Meusel, Juho Rousu, and Sebastian Böcker. Searching molecular structure databases with tandem mass spectra using csi: Fingerid. Proceedings of the National Academy of Sciences, 112(41):12580–12585, 2015.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nWarwick B Dunn, Alexander Erban, Ralf JM Weber, Darren J Creek, Marie Brown, Rainer Breitling, Thomas Hankemeier, Royston Goodacre, Steffen Neumann, Joachim Kopka, et al. Mass appeal: metabolite identification in mass spectrometry-focused untargeted metabolomics. Metabolomics, 9 (1):44–66, 2013.\n\nWilliam Falcon. The pytorch lightning team. Pytorch lightning, 3:6, 2019.\n\nGary L Glish and Richard W Vachet. The basics of mass spectrometry in the twenty-first century.\n\nNature reviews drug discovery, 2(2):140–150, 2003.\n\nHisayuki Horai, Masanori Arita, Shigehiko Kanaya, Yoshito Nihei, Tasuku Ikeda, Kazuhiro Suwa, Yuya Ojima, Kenichi Tanaka, Satoshi Tanaka, Ken Aoshima, et al. Massbank: a public repository for sharing mass spectral data for life sciences. Journal of mass spectrometry, 45(7):703–714, 2010.\n\nFlorian Huber, Lars Ridder, Stefan Verhoeven, Jurriaan H Spaaks, Faruk Diblen, Simon Rogers, and Justin JJ Van Der Hooft. Spec2vec: Improved mass spectral similarity scoring through learning of structural relationships. PLoS computational biology, 17(2):e1008724, 2021a.\n\nFlorian Huber, Sven van der Burg, Justin JJ van der Hooft, and Lars Ridder. Ms2deepscore: a novel deep learning similarity measure to compare tandem mass spectra. Journal of cheminformatics, 13 (1):1–14, 2021b.\n\nJeffrey J Jones, Michael J Stump, Richard C Fleming, Jackson O Lay, and Charles L Wilkins. Strategies and data analysis techniques for lipid and phospholipid chemistry elucidation by intact cell maldi-ftms. Journal of the American Society for Mass Spectrometry, 15(11):1665–1674, 2004.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nAnders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in\n\nneural information processing systems, 4, 1991.\n\nSvetlana Kutuzova, Christian Igel, Mads Nielsen, and Douglas McCloskey. Bi-modal variational\n\nautoencoders for metabolite identification using tandem mass spectrometry. bioRxiv, 2021.\n\nGreg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and\n\npredictive modeling. Greg Landrum, 2013.\n\nYang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable fourier features for multidimensional spatial positional encoding. Advances in Neural Information Processing Systems, 34: 15816–15829, 2021a.\n\nYuanyue Li, Tobias Kind, Jacob Folz, Arpana Vaniya, Sajjan Singh Mehta, and Oliver Fiehn. Spectral entropy outperforms ms/ms dot product similarity for small-molecule compound identification. Nature Methods, 18(12):1524–1531, 2021b.\n\nChristopher A Lipinski, Franco Lombardo, Beryl W Dominy, and Paul J Feeney. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. Advanced drug delivery reviews, 64:4–17, 2012.\n\nEleni Litsa, Vijil Chenthamarakshan, Payel Das, and Lydia Kavraki. Spec2mol: An end-to-end deep\n\nlearning framework for translating ms/ms spectra to de-novo molecules. 2021.\n\nMarie Mardal, Mette Findal Andreasen, Christian Brinch Mollerup, Peter Stockham, Rasmus Telving, Nikolaos S Thomaidis, Konstantina S Diamanti, Kristian Linnet, and Petur Weihe Dalsgaard. Highresnps. com: an online crowd-sourced hr-ms database for suspect and non-targeted screening of new psychoactive substances. Journal of Analytical Toxicology, 2019.\n\nLeland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and\n\nprojection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.\n\nSS Mehta. Massbank of north america (mona): An open-access, auto-curating mass spectral database\n\nfor compound identification in metabolomics presentation, 2020.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAnzor Mikaia, Principal Edward White V EI, Vladimir Zaikin EI, Damo Zhu EI, O David Sparkman EI, Pedatsur Neta, Igor Zenkevich RI, Peter Linstrom, Yuri Mirokhin, Dmitrii Tchekhovskoi, et al. Nist standard reference database 1a. Standard Reference Data, NIST, Gaithersburg, MD, USA https://www. nist. gov/srd/nist-standard-reference-database-1a, 2014.\n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa-\n\ntions in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\n\nSoheil Pourshahian and Patrick A Limbach. Application of fractional mass for the identification of peptide–oligonucleotide cross-links by mass spectrometry. Journal of mass spectrometry, 43(8): 1081–1088, 2008.\n\nRui Qiao, Ngoc Hieu Tran, Lei Xin, Baozhen Shan, Ming Li, and Ali Ghodsi. Deepnovov2: Better\n\nde novo peptide sequencing with deep learning. arXiv preprint arXiv:1904.08514, 2019.\n\nRobert A Quinn, Louis-Felix Nothias, Oliver Vining, Michael Meehan, Eduardo Esquenazi, and Pieter C Dorrestein. Molecular networking as a drug discovery, drug metabolism, and precision medicine strategy. Trends in pharmacological sciences, 38(2):143–154, 2017.\n\nYuji Sawada, Ryo Nakabayashi, Yutaka Yamada, Makoto Suzuki, Muneo Sato, Akane Sakata, Kenji Akiyama, Tetsuya Sakurai, Fumio Matsuda, Toshio Aoki, et al. Riken tandem mass spectral database (respect) for phytochemicals: a plant-specific ms/ms-based data resource and database. Phytochemistry, 82:38–45, 2012.\n\nAditya Divyakant Shrivastava, Neil Swainston, Soumitra Samanta, Ivayla Roberts, Marina Wright Muelas, and Douglas B Kell. Massgenie: A transformer-based deep learning method for identifying small molecules from their mass spectra. Biomolecules, 11(12):1793, 2021.\n\nColin A Smith, Grace O’Maille, Elizabeth J Want, Chuan Qin, Sunia A Trauger, Theodore R Brandon, Darlene E Custodio, Ruben Abagyan, and Gary Siuzdak. Metlin: a metabolite mass spectral database. Therapeutic drug monitoring, 27(6):747–751, 2005.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\nStephen E Stein and Donald R Scott. Optimization and testing of mass spectral library search algorithms for compound identification. Journal of the American Society for Mass Spectrometry, 5 (9):859–866, 1994.\n\nJustin Johan Jozias van Der Hooft, Joe Wandy, Michael P Barrett, Karl EV Burgess, and Simon Rogers. Topic modeling for untargeted substructure exploration in metabolomics. Proceedings of the National Academy of Sciences, 113(48):13738–13743, 2016.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nDaniel F Veber, Stephen R Johnson, Hung-Yuan Cheng, Brian R Smith, Keith W Ward, and Kenneth D Kopple. Molecular properties that influence the oral bioavailability of drug candidates. Journal of medicinal chemistry, 45(12):2615–2623, 2002.\n\nMingxun Wang, Jeremy J Carver, Vanessa V Phelan, Laura M Sanchez, Neha Garg, Yao Peng, Don Duy Nguyen, Jeramie Watrous, Clifford A Kapono, Tal Luzzatto-Knaan, et al. Sharing and community curation of mass spectrometry data with global natural products social molecular networking. Nature biotechnology, 34(8):828–837, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJeramie Watrous, Patrick Roach, Theodore Alexandrov, Brandi S Heath, Jane Y Yang, Roland D Kersten, Menno van der Voort, Kit Pogliano, Harald Gross, Jos M Raaijmakers, et al. Mass spectral molecular networking of living microbial colonies. Proceedings of the National Academy of Sciences, 109(26):E1743–E1752, 2012.\n\nMelih Yilmaz, William Fondrie, Wout Bittremieux, Sewoong Oh, and William S Noble. De novo mass spectrometry peptide sequencing with a transformer model. In International Conference on Machine Learning, pp. 25514–25522. PMLR, 2022.\n\n ̧Sule Yilmaz, Elien Vandermarliere, and Lennart Martens. Methods to calculate spectrum similarity.\n\nIn Proteome bioinformatics, pp. 75–100. Springer, 2017.\n\nXi-wu Zhang, Qiu-han Li, Jin-jin Dou, et al. Mass spectrometry-based metabolomics in health and\n\nmedical science: A systematic review. RSC advances, 10(6):3092–3104, 2020.\n\n12",
    "reference": "# Summary Of The Paper\n\nThe paper applied sinusoidal embeddings for spectral library search in mass spectrometry in metabolomics.\n\n# Strength And Weaknesses\n\nThe authors propose to use sinusoidal embedding to obtain a representation of the measured spectrum, allowing the comparison of similarities between measured compounds. The advantage of using sinusoidal embeddings in mass spectrometry is significant and was already demonstrated by Yilmaz et al. Here the authors additionally confirmed it with experiments in metabolomics. \n\nMinor comments:\n* What is a “development” set? What happened to the “validation” set? Why do the authors call \n“validation” set “development” set?\n* From the description in section 3.1 it is unclear whether test and validation sets can have an overlapping set of molecules. It is clear that they don’t overlap with training, but not clear if they overlap with each other. The whole section is not very clear. The part about known and not known molecules is not very clear either. Also stratification by spectrum alone and not by molecules is also unusual. I have concerns that the model can overfit individual molecules. The network proposed by the authors is much larger than the benchmarks and it is not clear to me how the authors demonstrated overfitting if the test set contains molecules present at training.\n* Section 3.2 is not well written. The metrics are not well-defined. The thresholds seem arbitrary. It is not clear at all how these numbers were estimated. Was FDR used to find these cutoffs? Maybe these metrics are commonly used in metabolomics, but they should be clearly described for a machine learning conference.\n* The authors demonstrate the improved numbers in their tables, but since the metrics and the whole evaluation are not transparent, it is hard to assess what is the actual significance of these numbers.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nI don’t see any technical novelty in the paper. Sinusoidal embeddings were already proposed for the field of mass spectrometry (Yilmaz et al.) as noted by the authors. The idea of using neural networks for spectral library search is not novel either as confirmed by the authors. The results section is not easy to read if you are not very familiar with particular evaluation benchmarks in mass spectrometry in metabolomics.\n\n# Summary Of The Review\n\nThe paper empirically demonstrates the advantage of using sinusoidal embeddings for spectral library search in metabolomics. The idea of using sinusoidal embeddings to avoid binning of ms spectrum is not novel and was recently published. The authors take this idea one step further and demonstrate that it is superior that currently existing simple benchmarks for spectral library search in the field of metabolomics. The biggest contribution of this paper is the empirical demonstration of their results, but this section is not transparent at all.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nTHE ONSET OF VARIANCE-LIMITED BEHAVIOR FOR NETWORKS IN THE LAZY AND RICH REGIMES\n\nAlexander Atanasov∗ §‡ , Blake Bordelon∗ †‡ , Sabarish Sainathan †‡ & Cengiz Pehlevan †‡ §Department of Physics †John A. Paulson School of Engineering and Applied Sciences ‡Center for Brain Science Harvard University Cambridge, MA 02138, USA {atanasov,blake bordelon,cpehlevan}@g.harvard.edu\n\nABSTRACT\n\nFor small training set sizes P , the generalization error of wide neural networks is well-approximated by the error of an infinite width neural network (NN), either in the kernel or mean-field/feature-learning regime. However, after a critical sample size P ∗, we empirically find the finite-width network generalization becomes worse than that of the infinite width network. In this work, we empirically study the transition from infinite-width behavior to this variance-limited regime as a function of sample size P and network width N . We find that finite-size effects can become relevant for very small dataset sizes on the order of P ∗ ∼ N\nfor polynomial regression with ReLU networks. We discuss the source of these effects using an argument based on the variance of the NN’s final neural tangent kernel (NTK). This transition can be pushed to larger P by enhancing feature learning or by ensemble averaging the networks. We find that the learning curve for regression with the final NTK is an accurate approximation of the NN learning curve. Using this, we provide a toy model which also exhibits P ∗ ∼ N scaling and has P -dependent benefits from feature learning.\n\n√\n\n√\n\n1\n\nINTRODUCTION\n\nDeep learning systems are achieving state of the art performance on a variety of tasks (Tan & Le, 2019; Hoffmann et al., 2022). Exactly how their generalization is controlled by network architecture, training procedure, and task structure is still not fully understood. One promising direction for deep learning theory in recent years is the infinite-width limit. Under a certain parameterization, infinitewidth networks yield a kernel method known as the neural tangent kernel (NTK) (Jacot et al., 2018; Lee et al., 2019). Kernel methods are easier to analyze, allowing for accurate prediction of the generalization performance of wide networks in this regime (Bordelon et al., 2020; Canatar et al., 2021; Bahri et al., 2021; Simon et al., 2021). Infinite-width networks can also operate in the meanfield regime if network outputs are rescaled by a small parameter α that enhances feature learning (Mei et al., 2018; Chizat et al., 2019; Geiger et al., 2020b; Yang & Hu, 2020; Bordelon & Pehlevan, 2022).\n\nWhile infinite-width networks provide useful limiting cases for deep learning theory, real networks have finite width. Analysis at finite width is more difficult, since predictions are dependent on the initialization of parameters. While several works have attempted to analyze feature evolution and kernel statistics at large but finite width (Dyer & Gur-Ari, 2020; Roberts et al., 2021), the implications of finite width on generalization are not entirely clear. Specifically, it is unknown at what value of the training set size P the effects of finite width become relevant, what impact this critical P has on the learning curve, and how it is affected by feature learning.\n\nTo identify the effects of finite width and feature learning on the deviation from infinite width learning curves, we empirically study neural networks trained across a wide range of output scales α, widths N , and training set sizes P on the simple task of polynomial regression with a ReLU neural network. Concretely, our experiments show the following:\n\n∗These authors contributed equally.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n√\n\n• Learning curves for polynomial regression transition exhibit significant finite-width effects very early, around P ∼ N . Finite-width NNs at large α are always outperformed by their infinite-width counterparts. We show this gap is driven primarily by variance of the predictor over initializations (Geiger et al., 2020a). Following prior work (Bahri et al., 2021), we refer to this as the variance-limited regime. We compare three distinct ensembling methods to reduce error in this regime.\n\n• Feature-learning NNs show improved generalization both before and after the transition to the variance limited regime. Feature learning can be enhanced through re-scaling the output of the network by a small scalar α or by training on a more complex task (a higher-degree polynomial). We show that alignment between the final NTK and the target function on test data improves with feature learning and sample size.\n\n• We demonstrate that the learning curve for the NN is well-captured by the learning curve for kernel regression with the final empirical NTK, eNTKf , as has been observed in other works (Vyas et al., 2022; Geiger et al., 2020b; Atanasov et al., 2021; Wei et al., 2022).\n\n• Using this correspondence between the NN and the final NTK, we provide a cursory account of how fluctuations in the final NTK over random initializations are suppressed at large width N and large feature learning strength. In a toy model, we reproduce several scaling phenomena, including the P ∼ N transition and the improvements due to feature learning through an alignment effect.\n\n√\n\nWe validate that these effects qualitatively persist in the realistic setting of wide ResNets Zagoruyko & Komodakis (2017) trained on CIFAR in appendix E.\n\nOverall, our results indicate that the onset of finite-width corrections to generalization in neural networks become relevant when the scale of the variance of kernel fluctuations becomes comparable to the bias component of the generalization error in the bias-variance decomposition. The variance contribution to generalization error can be reduced both through ensemble averaging and through feature learning, which we show promotes higher alignment between the final kernel and the task. We construct a model of noisy random features which reproduces the essential aspects of our observations.\n\n1.1 RELATED WORKS\n\nGeiger et al. (2020a) analyzed the scaling of network generalization with the number of model parameters. Since the NTK fluctuates with variance O(N −1) for a width N network (Dyer & GurAri, 2020; Roberts et al., 2021), they find that finite width networks in the lazy regime generically perform worse than their infinite width counterparts.\n\nThe scaling laws of networks over varying N and P were also studied, both empirically and theoretically by Bahri et al. (2021). They consider two types of learning curve scalings. First, they describe resolution-limited scaling, where either training set size or width are effectively infinite and the scaling behavior of generalization error with the other quantity is studied. There, the scaling laws can been obtained by the theory in Bordelon et al. (2020). Second, they analyze variance-limited scaling where width or training set size are fixed to a finite value and the other parameter is taken to infinity. While that work showed for any fixed P that the learning curve converges to the infinite width curve as O(N −1), these asymptotics do not predict, for fixed N , at which value of P the NN learning curve begins to deviate from the infinite width theory. This is the focus of our work.\n\nThe contrast between rich and lazy networks has been empirically studied in several prior works. Depending on the structure of the task, the lazy regime can have either worse (Fort et al., 2020) or better (Ortiz-Jim ́enez et al., 2021; Geiger et al., 2020b) performance than the feature learning regime. For our setting, where the signal depends on only a small number of relevant input directions, we expect representation learning to be useful, as discussed in (Ghorbani et al., 2020; Paccolat et al., 2021b). Consequently, we posit and verify that the rich network will outperform the lazy one.\n\nOur toy model is inspired by the literature on random feature models. Analysis of generalization for two layer networks at initialization in the limit of high dimensional data have been carried out using techniques from random matrix theory (Mei & Montanari, 2022; Hu & Lu, 2020; Adlam & Pennington, 2020a; Dhifallah & Lu, 2020; Adlam & Pennington, 2020b) and statistical mechanics (Gerace et al., 2020; d’Ascoli et al., 2020; d’Ascoli et al., 2020). Several of these works have identified that when N is comparable to P , the network generalization error has a contribution\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nfrom variance over initial parameters. Further, they provide a theoretical explanation of the benefit of ensembling predictions of many networks trained with different initial parameters. Recently, Ba et al. (2022) studied regression with the hidden features of a two layer network after taking one step of gradient descent, finding significant improvements to the learning curve due to feature learning. Zavatone-Veth et al. (2022) analyzed linear regression for Bayesian deep linear networks with width N comparable to sample size P and demonstrated the advantage of training multiple layers compared to only training the only last layer, finding that feature learning advantage has leading correction of scale (P/N )2 at small P/N .\n\n2 PROBLEM SETUP AND NOTATION\n\nWe consider a supervised task with a dataset D = {xμ, yμ}P μ=1 of size P . The pairs of data points are drawn from a population distribution p(x, y). Our experiments will focus on training networks to interpolate degree k polynomials on the sphere (full details in Appendix A). For this task, the infinite width network learning curves can be found analytically. In particular at large P the generalization error scales as 1/P 2 (Bordelon et al., 2020). We take a single output feed-forward NN ̃fθ : RD → R with hidden width N for each layer. We let θ denote all trainable parameters of the network. Using NTK parameterization (Jacot et al., 2018), the activations for an input x are given by\n\nh(l)\n\ni =\n\nσ √\n\nN\n\nN (cid:88)\n\nj=1\n\nW (l)\n\nij φ(h(l−1)\n\nj\n\n),\n\nl = 2, . . . L,\n\nh(1)\n\ni =\n\nσ √\n\nD\n\nD (cid:88)\n\nj=1\n\nW (1)\n\nij xj.\n\n(1)\n\n1\n\nHere, the output of the network is ̃fθ = h(L) . We take φ to be a positively homogenous function, in our case a ReLU nonlinearity, but this is not strictly necessary (Appendix C.2). At initialization we have Wij ∼ N (0, 1). Consequently, the scale of the output at initialization is O(σL). As a consequence of the positive homogeneity of the network, the scale of the output is given by α = σL. α controls the feature learning strength of a given NN. Large α corresponds to a lazy network while small α yields a rich network with feature movement. More details on how α controls feature learning are given in Appendix C.1 and C.2.\n\nIn what follows, we will denote the infinite width NTK limit of this network by NTK∞. We will denote its finite width linearization by eNTK0(x, x′) := (cid:80) θ ∂θf (x)∂θf (x′)|θ=θ0, and we will denote its linearization around its final parameters θf by eNTKf (x, x′) := (cid:80) θ ∂θf (x)∂θf (x′)|θ=θf . Following other authors (Chizat et al., 2019; Adlam & Pennington, 2020a), we will take the output to be fθ(x) := ̃fθ(x) − ̃fθ0(x). Thus, at initialization the function output is 0. We explain this choice further in Appendix A. The parameters are then trained with full-batch gradient descent on a mean squared error loss. We denote the final network function starting from initialization θ0 on a θ0,D(x) or f ∗ for short. The generalization error is calculated using a held-out test dataset D by f ∗ set and approximates the population risk Eg(f ) := (cid:10)(f (x) − y)2(cid:11)\n\nx,y∼p(x,y).\n\n3 EMPIRICAL RESULTS\n\nIn this section, we will study learning curves for ReLU NNs trained on polynomial regression tasks of varying degrees. We take our task to be learning y = Qk(β ·x) where β is random vector of norm 1/D and Qk is the kth gegenbauer polynomial. We will establish the following key observations, which we will set out to theoretically explain in Section 4.\n\n1. Both eNTK0 and sufficiently lazy networks perform strictly worse than NTK∞ , but the\n\nensembled predictors approach the NTK∞ test error.\n\n2. NNs in the feature learning regime of small α can outperform NTK∞ for an intermediate\n\nrange of P . Over this range, the effect of ensembling is less notable.\n\n3. Even richly trained finite width NNs eventually perform worse than NTK∞ at sufficiently large P . However, these small α feature-learning networks become variance-limited at larger P than lazy networks. Once in the variance-limited regime, all networks benefit from ensembling over initializations.\n\n4. For all networks, the transition to the variance-limited regime begins at a P ∗ that scales\n\nsub-linearly with N . For polynomial regression, we find P ∗ ∼\n\n3\n\n√\n\nN .\n\nPublished as a conference paper at ICLR 2023\n\nThese findings support our hypothesis that finite width introduces variance in eNTK0 over initializations, which ultimately leads to variance in the learned predictor and higher generalization error. Although we primarily focus on polynomial interpolation tasks in this paper, in Appendix F we provide results for wide ResNets trained on CIFAR and observe that rich networks also outperform lazy ones, and that lazy ones benefit more significantly from ensembling.\n\n3.1 FINITE WIDTH EFFECTS CAUSE THE ONSET OF A VARIANCE LIMITED REGIME\n\n(a) k = 2 generalization error\n\n(b) k = 2 20-fold ensemble error\n\n(c) k = 4 generalization error\n\n(d) k = 4 20-fold ensemble error\n\nFigure 1: Generalization errors of depth L = 3 neural networks across a range of α values compared to NTK∞ . The regression for NTK∞ was calculated using the Neural Tangents package (Novak et al., 2020). The exact scaling of NTK∞ is known to go asymptotically as P −2 for this task. a) Lazy networks perform strictly worse than NTK∞ while rich networks can outperform it for an intermediate range of P before their performance is also limited. b) Ensembling 20 networks substantially improves lazy network and eNTK0 generalization, as well as asymptotic rich network generalization. This indicates that at sufficiently large P , these neural networks become limited by variance due to initialization. The error bars in a) and c) denote the variance due to both both training set and initialization. The error bars in b), d) denote the variance due to the train set.\n\nIn this section, we first investigate how finite width NN learning curves differ from infinite width NTK regression. In Figure 1 we show the generalization error Eg(f ∗ θ0,D) for a depth 3 network with width N = 1000 trained on a quadratic k = 2 and quartic k = 4 polynomial regression task. Additional plots for other degree polynomials are provided in Appendix F. We sweep over P to show the effect of more data on generalization, which is the main relationship we are interested in studying. For each training set size we sweep over a grid of 20 random draws of the train set and 20 random network initializations. This for 400 trained networks in total at each choice of P, k, N, α. We see that a discrepancy arises at large enough P where the neural networks begin to perform worse than NTK∞.\n\nWe probe the source of the discrepancy between finite width NNs and NTK∞ by ensemble averaging network predictions ̄fD(x) := ⟨f ∗ θ0,D(x)⟩θ0 over E = 20 initializations θ0. In Figures 1b and 1d, we calculate the error of ̄fD(x), each trained on the same dataset. We then plot Eg( ̄fD). This\n\n4\n\n101102103104P105104103102101100Egk=2, L=3, N=1000NTKNN, =0.1NN, =0.5NN, =1.0NN, =10.0NN, =20eNTK0 (=)101102103104P105104103102101100Egk=2, L=3, N=1000NTKNN ensembled, =0.1NN ensembled, =0.5NN ensembled, =1.0NN ensembled, =10.0NN ensembled, =20eNTK0 ensembled (=)101102103104P102101100Egk=4, L=3, N=1000NTKNN, =0.1NN, =0.5NN, =1.0NN, =10.0NN, =20eNTK0 (=)101102103104P102101100Egk=4, L=3, N=1000NTKNN ensembled, =0.1NN ensembled, =0.5NN ensembled, =1.0NN ensembled, =10.0NN ensembled, =20eNTK0 ensembled (=)Published as a conference paper at ICLR 2023\n\n(a) k = 3 generalization error\n\n(b) k = 3 variance fraction\n\n(c) k = 3 alignment\n\nFigure 2: Phase plots in the P, α plane of a) The log generalization error log10 Eg(f ⋆), b) The fraction of generalization error removed by ensembling 1 − Eg( ̄f ⋆)/Eg(f ⋆), c) Kernel-task alignment measured by yT Kf y where y and Kf are evaluated on test data. We have plotted ‘x’ markers in a) to show the points where the NNs were trained.\n\n|y|2TrKf\n\nensembled error approximates the bias in a bias-variance decomposition (Appendix B). Thus, any gap between 1 (a) and 1 (b) is driven by variance of fθ,D over θ.\n\nWe sharpen these observations with phase plots of NN generalization, variance and kernel alignment over P, α, as shown in Figure 2. In Figure 2a, generalization for NNs in the rich regime (small α) have lower final Eg than lazy networks. As the dataset grows, the fraction of Eg due to initialization variance (that is, the fraction removed by ensembling) strictly increases (2 (b)). We will show why this effect occurs in section 3.2. Figure 2b shows that, at any fixed P , the variance is lower for small α. To measure the impact of feature learning on the eNTKf , we plot its alignment with the target function, measured as y⊤Ky |y|2TrK for a test set of targets [y]μ and kernel [K]μν = eNTKf (xμ, xν). Alignment of the kernel with the target function is known to be related to good generalization (Canatar et al., 2021). In Section 4, we revisit these effects in a simple model which relates kernel alignment and variance reduction.\n\nIn addition to initialization variance, variance over dataset D contributes to the total generalization error. Following (Adlam & Pennington, 2020b), we discuss a symmetric decomposition of the variance in Appendix B, showing the contribution from dataset variance and the effects of bagging. We find that most of the variance in our experiments is due to initialization.\n\nWe show several other plots of the results of these studies in the appendix. We show the effect of bagging (Figure 7), phase plots of different degree target functions (Figures 10, 9), phase plots over N, α (Figure 11) and a comparison of network predictions against the initial and final kernel regressors (Figures 18, 19).\n\n3.2 FINAL NTK VARIANCE LEADS TO GENERALIZATION PLATEAU\n\nIn this section, we show how the variance over initialization can be interpreted as kernel variance in both the rich and lazy regimes. We also show how this implies a plateau for the generalization error.\n\nTo begin, we demonstrate empirically that all networks have the same generalization error as kernel regression solutions with their final eNTKs. At large α, the initial and the final kernel are already close, so this follows from earlier results of Chizat et al. (2019). In the rich regime, the properties of the eNTKf have been studied in several prior works. Several have empirically demonstrated that the eNTKf is a good match to the final network predictor for a trained network (Long, 2021; Vyas et al., 2022; Wei et al., 2022) while others have given conditions under which such an effect would hold true (Atanasov et al., 2021; Bordelon & Pehlevan, 2022). We comment on this in appendix C.4. We show in Figure 3 how the final network generalization error matches the generalization error of eNTKf . As a consequence, we can use eNTKf to study the observed generalization behavior.\n\nNext, we relate the variance of the final predictor f ∗ θ0,D to the corresponding infinite width network f ∞ D . The finite size fluctuations of the kernel at initialization have been studied in (Dyer & Gur-Ari, 2020; Hanin & Nica, 2019; Roberts et al., 2021). The variance of the kernel elements has been\n\n5\n\n2.02.53.03.5logP1.00.50.00.51.0loglogEg L=3, k=3, N=10003.02.52.01.51.00.50.0log10Eg2.02.53.03.5logP1.00.50.00.51.0logvar/Eg(f), L=3, k=3, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logAlignment, L=3, k=3, N=10000.020.030.040.050.060.070.080.09Published as a conference paper at ICLR 2023\n\n(a) EN N\n\ng = E\n\nN T Kf g\n\n(b) EN N\n\ng = E\n\nN T Kf g\n\nacross N, α\n\nFigure 3: Kernel regression with eNTKf reproduces the learning curves of the NN with high fidelity. (a) Learning curves across different laziness settings α in a width 1000 network. The solid black curve is the infinite width network. Colored curves are the NN generalizations. Stars represent the eNTKf s, and lie on top of the corresponding NN learning curves. (b) The agreement of generalizations between NNs and eNTKf s across different N and α. Here the colors denote different α values while the dot, triangle and star markers denote networks of N = {177, 421, 1000} respectively.\n\n(a) Scaling of P1/2 with α\n\n(b) Scaling of P1/2 with N\n\nFigure 4: Critical sample size P1/2 measures the onset of the variance limited regime as a function of α at fixed N . (a) More feature learning (small α) delays the transition to the variance limited regime. (b) P1/2 as a function of N for fixed α has roughly P1/2 ∼\n\nN scaling.\n\n√\n\nshown to scale as 1/N . We perform the following bias-variance decomposition: Take fθ0,D to be the eNTK0 predictor, or a sufficiently lazy network trained to interpolation on a dataset D. Then,\n\n⟨(f ∗\n\nθ0,D(x) − y)2⟩θ0,D,x,y = ⟨(f ∞\n\nD (x) − y)2⟩D,x,y + O(1/N ).\n\n(2)\n\nWe demonstrate this equality using a relationship between the infinite-width network and an infinite ensemble of finite-width networks derived in Appendix B. There we also show that the O(1/N ) term is strictly positive for sufficiently large N . Thus, for lazy networks of sufficiently large N , finite width effects lead to strictly worse generalization error. The decomposition in Equation 2 continues to hold for rich networks at small α if f ∞ is interpreted as the infinite-width mean field limit. In this case one can show that ensembles of rich networks are approximating an infinite width limit in the mean-field regime. See Appendix B for details.\n\n3.3 FEATURE LEARNING DELAYS VARIANCE LIMITED TRANSITION\n\nWe now consider how feature learning alters the onset of the variance limited regime, and how this onset scales with α, N . We define the onset of the variance limited regime to take place at the value P ∗ = P1/2 where over half of the generalization error is due to variance over initializations. Equivalently we have Eg( ̄f ∗)/Eg(f ∗) = 1/2. By using an interpolation method together with bisection, we solve for P1/2 and plot it in Figure 4.\n\n6\n\n101102103104P105104103102101100Egk=2, L=3, N=1000NTKNN, =0.1eNTKf, =0.1eNTKf, =0.5eNTKf, =1.0eNTKf, =10.0eNTKf, =20.0eNTK0 (=)104103102101100Eg NN104103102101100Eg eNTKfk=2, L=3N=177N=421N=1000=0.1=0.5=1.0=10.0=20ENNg=EeNTKfg101100101103P1/2L=3, k=2N=74N=177N=421N=1000102103N103P1/2L=3, k=2=0.1=0.5=1.0=10.0=20eNTK (=)N1/2Published as a conference paper at ICLR 2023\n\n(a) Ensembling Methods\n\n(b) Reduction in Eg for Each Ensembling Technique\n\nFigure 5: The random feature model suggests three possible types of ensembling: averaging the output function f (x, θ), averaging eNTKf K(x, x′; θ), and averaging the induced features ψ(x, θ). We analyze these ensembling methods for a k = 1 task with a width N = 100 ReLU network. (a) While all ensembling methods improve generalization, averaging either the kernel ⟨K⟩ or features ⟨ψ⟩ gives a better improvement to generalization than averaging the output function ⟨f ⟩. Computing final kernels for many richly trained networks and performing regression with this averaged kernel gives the best performance. (b) We plot the relative error of each ensembling method against the single init neural network. The gap between ensembling and the single init NN becomes evident for sufficiently large P ∼ P1/2. For small α, all ensembling methods perform comparably, while for large α ensembling the kernel or features gives much lower Eg than averaging the predictors.\n\n√\n\nFigure 4b shows that P1/2 scales as N for this task. In the next section, we shall show that this scaling is governed by the fact that P1/2 is close to the value where the infinite width network generalization curve E∞ is equal to the variance of eNTKf . In this case the quantities to compare are E∞\n\ng ≈ P −2 and Var eNTKf ≈ N −1.\n\ng\n\nWe can understand the delay of the variance limited transition, as well as the lower value of the final plateau using a mechanistic picture similar to the effect observed in Atanasov et al. (2021). In that setting, under small initialization, the kernel follows a deterministic trajectory, picking up a low rank component in the direction of the train set targets yy⊤, and then changing only in scale as the network weights grow to interpolate the dataset. In their case, for initial output scale σL, eNTKf is deterministic up to a variance of O(σ). In our case, the kernel variance at initialization scales as σ2L/N . As σ → 0 the kernel’s trajectory becomes deterministic up to a variance term scaling with σ as O(σ), which implies that the final predictor also has a variance scaling as O(σ).\n\n4 SIGNAL PLUS NOISE CORRELATED FEATURE MODEL\n\nIn Section 3.2 we have shown that in both the rich and lazy regimes, the generalization error of the NN is well approximated by the generalization of a kernel regression solution with eNTKf . This finding motivates an analysis of the generalization of kernel machines which depend on network initialization θ0. Unlike many analyses of random feature models which specialize to two layer networks and focus on high dimensional Gaussian random data (Mei & Montanari, 2022; Adlam & Pennington, 2020a; Gerace et al., 2020; Ba et al., 2022), we propose to analyze regression with the eNTKf for more general feature structures. This work builds on the kernel generalization theory for kernels developed with statistical mechanics (Bordelon et al., 2020; Canatar et al., 2021; Simon et al., 2021; Loureiro et al., 2021). We will attempt to derive approximate learning curves in terms of the eNTKf ’s signal and noise components, which provide some phenomenological explanations of the onset of the variance limited regime and the benefits of feature learning. Starting with the final NTK Kθ0(x, x′) which depends on the random initial parameters θ0, we project its square root K 1/2 k=1 orthonormal with respect to p(x). This defines a feature map\n\n(x, x′) (as defined in equation 32) on a fixed basis {bk(x)}∞\n\nθ0\n\n(cid:90)\n\nψk(x, θ0) =\n\ndx′p(x′)K 1/2\n\nθ0\n\n(x, x′)bk(x′) , k ∈ {1, ..., ∞}.\n\n(3)\n\nThe kernel can be reconstructed from these features Kθ0(x, x′) = (cid:80) k ψk(x, θ0)ψk(x′, θ0). The kernel interpolation problem can be solved by performing linear regression with features ψ(x, θ0). μ=1[w · ψ(xμ, θ0) − yμ]2 + λ|w|2. The learned function Here, w(θ0) = limλ→0 argminw\n\n(cid:80)P\n\n7\n\n101102103P104103102101100EgNN, =1.0Avg. f, =1.0Avg. K, =1.0Avg. , =1.0=2.0=10.0101102103P101100EgEg(NN)=1.0Avg. fAvg. KAvg. 101102103P101100=2.0101102103P101100=10.0Published as a conference paper at ICLR 2023\n\nf (x, θ0) = w(θ0) · ψ(x, θ0) is the minimum norm interpolator for the kernel K(x, x′; θ0) and matches the neural network learning curve as seen in Section 3.2. In general, since the rank of K is finite for a finite size network, the ψk(x, θ0) have correlation matrix of finite rank NH. Since the target function y does not depend on the initialization θ0, we decompose it in terms of a fixed set of features ψM (x) ∈ RM (for example, the first M basis functions {bk}M k=1). In this random feature model, one can interpret the initialization-dependent fluctuations in K(x, x′; θ0) as generating fluctuations in the features ψ(x, θ0) which induce fluctuations in the learned network predictor f (x, θ0). To illustrate the relative improvements to generalization from denoising these three different objects, in Figure 5, we compare averaging the final kernel K, averaging the induced features ψ, and averaging network predictions f directly. For all α, all ensembling methods provide improvements over training a single NN. However, we find that averaging the kernel directly and performing regression with this kernel exhibits the largest reduction in generalization error. Averaging features performs comparably. However, ensemble averaging network predictors does not perform as well as either of these other two methods. The gap between ensembling methods is more significant in the lazy regime (large α) and is negligible in the rich regime (small α).\n\n4.1 TOY MODELS AND APPROXIMATE LEARNING CURVES\n\nTo gain insight into the role of feature noise, we characterize the test error associated with a Gaussian covariate model in a high dimensional limit P, M, NH → ∞ with α = P/M, η = NH/M . (cid:20)ΣM 0 0 Σε\n\nψ · w, ψ = A(θ0)ψM + ε,\n\nψM · w∗, f =\n\n(cid:20)ψM ε\n\n1 √\n\n1 √\n\n∼ N\n\ny =\n\n(cid:21)(cid:19)\n\n(4)\n\n0,\n\n(cid:18)\n\n(cid:21)\n\nM\n\nM\n\nThis model was also studied by Loureiro et al. (2021) and subsumes the classic two layer random feature models of prior works (Hu & Lu, 2020; Adlam & Pennington, 2020a; Mei & Montanari, 2022). The expected generalization error for any distribution of A(θ0) has the form\n\nEθ0 Eg(θ0) = EA\n\n1 1 − γ\n\n1 M\n\n(cid:104)\n\nw∗Σ1/2\n\nM\n\nI − ˆqΣ1/2\n\ns A⊤GAΣ1/2\n\ns − ˆqΣ1/2\n\ns A⊤G2AΣ1/2\n\ns\n\n(cid:105)\n\nΣ1/2\n\nM w∗\n\nG = (cid:0)I + ˆqAΣM A⊤ + ˆqΣε\n\n(cid:1)−1\n\n, ˆq =\n\nα λ + q\n\n, q = TrG[AΣM A⊤ + Σε],\n\n(5)\n\nwhere α = P/M and γ = α (λ+q)2 TrG2[AΣM A⊤ +Σε]2. Details of the calculation can be found in Appendix D. We also provide experiments showing the predictive accuracy of the theory in Figure 6. In general, we do not know the induced distribution of A(θ0) over disorder θ0. In Appendix D.5, we compute explicit learning curves for a simple toy model where A(θ0)′s entries as i.i.d. Gaussian over the random initialization θ0. A similar random feature model was recently analyzed with diagrammatic techniques by Maloney et al. (2022). In the high dimensional limit M, P, NH → ∞ with P/M = α, NH/M = η, our replica calculation demonstrates that test error is self-averaging (the same for every random instance of A) which we describe in Appendix D.5 and Figure 16.\n\n4.2 EXPLAINING FEATURE LEARNING BENEFITS AND ERROR PLATEAUS\n\nUsing this theory, we can attempt to explain some of the observed phenomena associated with the onset of the variance limited regime. First, we note that the kernels exhibit fluctuations over initialization with variance O(1/N ), either in the lazy or rich regime. In Figure 6 (a), we show learning curves for networks of different widths in the lazy regime. Small width networks enter the variance limited regime earlier and have higher error. Similarly, if we alter the scale of the noise Σε = σ2 ε AΣM A⊤ in our toy model, the corresponding transition time P1/2 is smaller and the asymptotic error is higher. In Figure 6 (c), we show that our theory also predicts the onset of the ε ∼ N −1. We stress that this scaling is a consequence of variance limited regime at P1/2 ∼ the structure of the task. Since the target function is an eigenfunction of the kernel, the infinite width error goes as 1/P 2 (Bordelon et al., 2020). Since variance scales as 1/N , bias and variance become = P −β with comparable at P ∼ β < 2 (Spigler et al., 2020; Bahri et al., 2021), where we’d expect a transition around P1/2 ∼ N 1/β.\n\nN . Often, realistic tasks exhibit power law decays where EN =∞\n\nN if σ2\n\n√\n\n√\n\ng\n\nUsing our model, we can also approximate the role of feature learning as enhancement in the signal correlation along task-relevant eigenfunctions. In Figure 6 (d) we plot the learning curves for networks trained with different levels of feature learning, controlled by α. We see that feature learning leads to improvements in the learning curve both before and after onset of variance limits. In Figure\n\n8\n\nPublished as a conference paper at ICLR 2023\n\n(a) EN N\n\ng\n\nfor different N\n\n(b) Small N ≈ Large σ2\n\n(c) Variance Limited Transition\n\n(d) Feature Scalings\n\n(e) Richness ≈ Amplified ΣM\n\n(f) P -dependent Amplification\n\nFigure 6: A toy model of noisy features reproduces qualitative dependence of learning curves on kernel fluctuations and feature learning. (a) The empirical learning curves for networks of varying width N at large α. (b) Noisy kernel regression learning curve with noise Σε = σ2 ε ΣM and A is a projection matrix preserving 20-k top eigenmodes of ΣM , which was computed from the NTK∞ for a depth 3 ReLU network. (c) This toy model reproduces the approximate scaling of the transition ε ∼ N −1. (d) NNs trained with varying richness α. Small α improves sample size P1/2 ∼ N 1/2 if σ2 the early learning curve and asymptotic behavior. (e) Theory curves for a kernel with amplified eigenvalue λk → λk + ∆λk for the target eigenfunction. This amplification mimics the effect of enhanced kernel alignment in the low α regime. Large amplification improves generalization performance. (f) P -dependent alignment where ∆λk ∼ P gives a better qualitative match to (d).\n\n√\n\n6 (e)-(f), we plot the theoretical generalization for kernels with enhanced signal eigenvalue for the task eigenfunction y(x) = φk(x). This enhancement, based on the intuition of kernel alignment, leads to lower bias and lower asymptotic variance. However, this model does not capture the fact that feature learning advantages are small at small P and that the slopes of the learning curves are different at different α. Following the observation of Paccolat et al. (2021a) that kernel alignment P . can occur with scale Though this toy model reproduces the onset of the variance limited regime P1/2 and the reduction in variance due to feature learning, our current result is not the complete story. A more refined future theory could use the structure of neural architecture to constrain the structure of the A distribution.\n\nP , we plot the learning curves for signal enhancements that scale as\n\n√\n\n√\n\n5 CONCLUSION\n\n√\n\nWe performed an extensive empirical study for deep ReLU NNs learning a fairly simple polynomial regression problems. For sufficiently large dataset size P , all neural networks under-perform the infinite width limit, and we demonstrated that this worse performance is driven by initialization variance. We show that the onset of the variance limited regime can occur early in the learning curve with P1/2 ∼ N , but this can be delayed by enhancing feature learning. Finally, we studied a simple random-feature model to attempt to explain these effects and qualitatively reproduce the observed behavior, as well as quantitatively reproducing the relevant scaling relationship for P1/2. This work takes a step towards understanding scaling laws in regimes where finite-size networks undergo feature learning. This has implications for how the choice of initialization scale, neural architecture, and number networks in an ensemble can be tuned to achieve optimal performance under a fixed compute and data budget.\n\n9\n\n101102103104P105104103102101100Egk=2, L=3, =20.0NTKNN, N=31NN, N=74NN, N=177NN, N=421NN, N=1000100101102103104P105104103102101100Eg2=02=1012=1022=1032=1042=105101102103104105106N=1/2101102103P1/2Theory Learning CurveN0.46N0.5101102103104P107106105104103102101100Egk=1, L=3, N=1000NTKNN, =0.1NN, =0.5NN, =1.0NN, =10.0NN, =20eNTK0 (=)100101102103104P105104103102101100Eg2=0k/k=5.00k/k=2.50k/k=1.00k/k=0.50k/k=0.00100101102103104P107106105104103102101100Eg2=0k/k=2.00Pk/k=1.00Pk/k=0.50Pk/k=0.10Pk/k=0.00PPublished as a conference paper at ICLR 2023\n\nREFERENCES\n\nBen Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In International Conference on Machine Learning, pp. 74–84. PMLR, 2020a.\n\nBen Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained biasvariance decomposition. Advances in neural information processing systems, 33:11022–11032, 2020b.\n\nAlexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent alignment effect. In International Conference on Learning Representations, 2021.\n\nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. Highdimensional asymptotics of feature learning: How one gradient step improves the representation. arXiv preprint arXiv:2205.01445, 2022.\n\nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural\n\nscaling laws, 2021. URL https://arxiv.org/abs/2102.06701.\n\nAristide Baratin, Thomas George, C ́esar Laurent, R Devon Hjelm, Guillaume Lajoie, Pascal Vincent, and Simon Lacoste-Julien. Implicit regularization via neural feature alignment. In International Conference on Artificial Intelligence and Statistics, pp. 2269–2277. PMLR, 2021.\n\nBlake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in\n\nwide neural networks. arXiv preprint arXiv:2205.09653, 2022.\n\nBlake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In Hal Daum ́e III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1024–1034. PMLR, 13–18 Jul 2020. URL https: //proceedings.mlr.press/v119/bordelon20a.html.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. Jax: composable transformations of python+ numpy programs. Version 0.2, 5:14–24, 2018.\n\nAbdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature Communications, 12, 2021.\n\nL ́ena ̈ıc Chizat, Edouard Oyallon, and Francis R. Bach. On lazy training in differentiable program-\n\nming. In NeurIPS, 2019.\n\nCorinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based\n\non centered alignment. The Journal of Machine Learning Research, 13(1):795–828, 2012.\n\nSt ́ephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting: Where & why do they appear? Advances in Neural Information Processing Systems, 33:3058– 3069, 2020.\n\nOussama Dhifallah and Yue M Lu. A precise performance analysis of learning with random features.\n\narXiv preprint arXiv:2008.11904, 2020.\n\nEthan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In International Conference on Learning Representations, 2020. URL https://openreview.net/ forum?id=S1gFvANKDS.\n\nSt ́ephane d’Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double descent: Bias and variance (s) in the lazy regime. In International Conference on Machine Learning, pp. 2280–2290. PMLR, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nStanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5850–5861. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/405075699f065e43581f27d67bb68478-Paper.pdf.\n\nMario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St ́ephane d’Ascoli, Giulio Biroli, Cl ́ement Hongler, and Matthieu Wyart. Scaling description of generalization with number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment, 2020(2):023401, 2020a.\n\nMario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2020 (11):113301, 2020b.\n\nFederica Gerace, Bruno Loureiro, Florent Krzakala, Marc M ́ezard, and Lenka Zdeborov ́a. Generalisation error in learning with random features and the hidden manifold model. In International Conference on Machine Learning, pp. 3452–3462. PMLR, 2020.\n\nBehrooz Ghorbani,\n\nSong Mei,\n\nWhen do neural networks outperform kernel methods?\n\nnari. 2020. a9df2255ad642b923d95503b9a7958d8-Abstract.html.\n\nand Andrea MontaIn NeurIPS, https://proceedings.neurips.cc/paper/2020/hash/\n\nTheodor Misiakiewicz,\n\nURL\n\nBoris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In\n\nInternational Conference on Learning Representations, 2019.\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n\nHong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features.\n\narXiv preprint arXiv:2009.07669, 2020.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: convergence and generalization in neural networks (invited paper). Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, 2018.\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n\narXiv:1412.6980, 2014.\n\nSimon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, pp. 3519– 3529. PMLR, 2019.\n\nJaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jascha Sohl-Dickstein. Wide neural networks of any depth evolve as linear models under gradient descent. ArXiv, abs/1902.06720, 2019.\n\nPhilip M. Long. Properties of the after kernel. CoRR, abs/2105.10585, 2021. URL https:\n\n//arxiv.org/abs/2105.10585.\n\nBruno Loureiro, C ́edric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc M ́ezard, and Lenka Zdeborov ́a. Capturing the learning curves of generic features maps for realistic data sets with a teacher-student model. CoRR, abs/2102.08127, 2021. URL https://arxiv.org/ abs/2102.08127.\n\nAlexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws,\n\n2022. URL https://arxiv.org/abs/2210.16859.\n\nSong Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75(4):667–766, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nSong Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018.\n\nRoman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In International Conference on Learning Representations, 2020. URL https://github.com/ google/neural-tangents.\n\nGuillermo Ortiz-Jim ́enez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. What can linearized neural networks actually say about generalization? Advances in Neural Information Processing Systems, 34:8998–9010, 2021.\n\nJonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric compression of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2021(4):044001, apr 2021a. doi: 10.1088/1742-5468/abf1f3. URL https: //doi.org/10.1088/1742-5468/abf1f3.\n\nJonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric compression of invariant manifolds in neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2021(4):044001, 2021b.\n\nDaniel A. Roberts, Sho Yaida, and Boris Hanin. The principles of deep learning theory, 2021.\n\nJames B. Simon, Madeline Dickens, and Michael R. DeWeese. Neural tangent kernel eigenvalues\n\naccurately predict generalization, 2021.\n\nStefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and Experiment, 2020(12):124001, 2020.\n\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-\n\nworks. In International conference on machine learning, pp. 6105–6114. PMLR, 2019.\n\nNikhil Vyas, Yamini Bansal, and Preetum Nakkiran. Limitations of the ntk for understanding gen-\n\neralization in deep learning. arXiv preprint arXiv:2206.10012, 2022.\n\nAlexander Wei, Wei Hu, and Jacob Steinhardt. More than a toy: Random matrix models predict\n\nhow real-world neural representations generalize. arXiv preprint arXiv:2203.06176, 2022.\n\nGreg Yang and Edward J. Hu.\n\nFeature learning in infinite-width neural networks. ArXiv,\n\nabs/2011.14522, 2020.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2017.\n\nJacob A Zavatone-Veth, William L Tong, and Cengiz Pehlevan. Contrasting random and learned\n\nfeatures in deep bayesian linear regression. arXiv preprint arXiv:2203.00573, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA DETAILS ON EXPERIMENTS\n\nμ=1 by sampling xμ uniformly on SD−1, the unit sphere We generated the dataset D = {xμ, yμ}P in RD. ̃y was then generated as a Gegenbauer polynomial of degree k of a 1D projection of x, ̃y = Qk(β · x). Because the scale of the output of the neural network relative to the target is a central quantity in this work, it is especially important to make sure the target is appropriately scaled to unit norm. We did this by defining the target to be y = ̃y/(cid:112)⟨Qk(β · x)2⟩x∼SD−1. The denominator can be easily and accurately approximated by Monte Carlo sampling.\n\nWe used JAX (Bradbury et al., 2018) for all neural network training. We built multi-layer perceptrons (MLPs) of depth 2 and 3. Most of the results are reported for depth 3 perceptrons, where there is a separation between the width of the network N and the number of parameters N 2. Sweeping over more depths and architectures is possible, but because of the extensive dimensionality of the hyperparameter search space, we have not yet experimented with deeper networks.\n\nWe considered MLPs with no bias terms. Since the Gegenbauer polynomials are mean zero, we do not need biases to fit the training set and generalize well. We have also verified that adding trainable biases does not change the final results in any substantial way.\n\nAs mentioned in the main text, we consider the final output function to be the initial network output minus the output at initialization:\n\nfθ(x) = ̃fθ(x) − ̃fθ0 (x). Here, only θ is differentiated through, while θ0 is held fixed. The rationale for this choice is that without this subtraction, in the lazy limit the trained neural network output can be written as\n\n(6)\n\nθ (x) = ̃fθ0 (x) + ̃f ∗\n\n(cid:88)\n\nμν\n\nkμ(x)[K−1]μν(yν − ̃fθ0(x)).\n\n(7)\n\nThis is the same as doing eNTK0 regression on the shifted targets yμ − ̃fθ0 (x). At large initialization the shift ̃fθ0(x) amounts to adding random, initialization-dependent noise to the targets. By instead performing the subtraction, the lazy limit can be interpreted as a kernel regression on the targets themselves, which is preferable.\n\nWe trained this network with full batch gradient descent with a learning rate η so that\n\n∆θ = −η∇θL(D, θ),\n\nL(D, θ) :=\n\n1 P\n\nP (cid:88)\n\nμ=1\n\n|fθ(xμ) − yμ|2.\n\n(8)\n\nEach network was trained to an interpolation threshold of 10−6. If a network could not reach this threshold in under 30k steps, we checked if the training error was less than 10 times the generalization error. If this was not satisfied, then that run of the network was discarded.\n\nFor each fixed P, k, we generated 20 independent datasets. For each fixed N, α we generated 20 independent neural network initializations. This 20 × 20 table yields a total of 400 neural networks trained on every combination of initialization and dataset choice.\n\nThe infinite width network predictions were calculated using the Neural Tangents package (Novak et al., 2020). The finite width eNTK0s were also calculated using the empirical methods in Neural Tangents. They were trained to interpolation using the gradient descent mse method. This is substantially faster than training the linearized model using standard full-batch gradient descent, which we have found to take a very long time for most networks. We use the same strategy for the eNTKf s.\n\nFor the experiments in the main text, we have taken the input dimension to be D = 10 and sweep over k = 1, 2, 3, 4. We swept over 15 values P in logspace from size 30 to size 10k, and over 6 values of N in logspace from size 30 to size 2150. We then swept over alpha values 0.1, 0.5, 1.0, 10.0, 20.0. Depending on α, N , we tuned the learning rate η of the network small enough to stay close to the gradient flow limit, but allow for the interpolation threshold to be feasibly reached.\n\nFor each of the 1800 settings of P, N, α, k and each of the 400 networks, 400 eNTK0s, 400 eNTKf s, and 20 NTK∞s, the generalization error was saved, as well as a vector of ˆy predictions on a test set\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nof 2000 points. In addition, for the neural networks we saved both initial and final parameters. All are saved as lists of numpy arrays in a directory of about 1TB. We plan to make the results of our experiments publicly accessible, alongside the code to generate them.\n\nA.1 CIFAR EXPERIMENTS\n\nWe apply the same methodology of centering the network and allowing α to control the degree of laziness by redefining\n\nfθ(x) = α( ̃fθ(x) − ̃fθ0 (x)). We consider the task of binary classification for CIFAR-10. In order to allow P to become large we divide the data into two classes: animate and inanimate objects. We choose to subsample eight classes and superclass them into two: (cat, deer, dog, horse) vs (airplane, automobile, ship, truck). Each superclass consists of 20,000 training examples and 4,000 test examples retrieved from the CIFAR-10 dataset.\n\n(9)\n\nOn subsets of this dataset, we train wide residual networks (ResNets) Zagoruyko & Komodakis (2017) of width 64 and block size 1 with the NTK parameterization Jacot et al. (2018) on this task using mini-batch gradient descent with batch size of 256 and MSE loss. Step sizes are governed by the Adam optimizer Kingma & Ba (2014) with initial learning rate η0 = 10−3. Every network is trained for 24,000 steps, such that under nearly all settings of α and dataset size the network has attained infinitesimal train loss.\n\nWe sweep α from 10−3 to 100 and P from 29 to 215. For each value of P , we randomly sample five training datasets of size P and compute ensembles of size 20. For each network in an ensemble the initialization and the order of the training data is randomly chosen independently of those for the other networks.\n\nB FINE-GRAINED BIAS-VARIANCE DECOMPOSITION\n\nB.1 FINE GRAINED DECOMPOSITION OF GENERALIZATION ERROR\n\nLet D be a dataset of (xμ, yμ)P μ=1 ∼ p(x, y) viewed as a random variable. Let θ0 represent the initial parameters of a neural network, viewed as a random variable. In the case of no label noise, as in section 2.2.1 of Adlam & Pennington (2020b), we derive the symmetric decomposition of the generalization error in terms of the variance due to initialization and the variance due to the dataset. We have\n\nEg(f ∗\n\nθ0,D) = ⟨(f ∗\n\nθ0,D(x) − y)2⟩x,y = ⟨(⟨f ∗\n\nθ0,D(x)⟩θ0,D − y)2⟩x,y + ExVarθ0,Df ∗\n\nθ0,D(y)\n\n= Bias2 + VD + Vθ0 + VD,θ0 .\n\nHere we have defined\n\nBias2 = ⟨(⟨f ∗\n\nθ0,D(x)⟩θ0,D − y)2⟩x,y,\n\nVD = Ex VarD Eθ0[f ∗ ED[f ∗ Vθ0 = Ex Varθ0 VD,θ0 = ExVarθ0,Df ∗\n\nθ0,D(x)|D] = Ex VarD θ0,D(x)|θ0],\n\n ̄f ∗ D(x),\n\nθ0,D(y) − Vθ0 − VD.\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n(14)\n\nVD and Vθ0 give the components of the variance explained by variance in D, θ0 respectively. VD,θ0 is the remaining part of the variance not explained by either of these two sources. As in the main text, ̄f ∗ θ0,D(x)|θ0] is commonly referred to as the bagged predictor. In the next subsection we study these terms empirically.\n\nD(x) is the ensemble average of the trained predictors over initializations. ED[f ∗\n\nB.2 EMPIRICAL STUDY OF DATASET VARIANCE\n\nUsing the network simulations, one can show that the bagged predictor does not have substantially lower generalization error in the regimes that we are interested in. This implies that most of the variance driving higher generalization error is due to variance over initializations. In figure 7, we\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nmake phase plots of the fraction of Eg that arises from variance due to initialization, variance over datasets, and total variance for width 1000. This can be obtained by computing the ensembled predictor, the bagged predictor, and the ensembled-bagged predictor respectively.\n\n(a) Initialization variance k = 2\n\n(b) Initialization variance k = 3\n\n(c) Initialization variance k = 4\n\n(d) Dataset variance k = 2\n\n(e) Dataset variance k = 3\n\n(f) Dataset variance k = 4\n\n(g) Total variance k = 2\n\n(h) Total variance k = 3\n\n(i) Total variance k = 4\n\nFigure 7: Phase plots of the fraction of the generalization error due to the initialization variance, the dataset variance, and their combined contribution. The columns correspond to the tasks of polynomial regression for degree 2, 3 and 4 polynomials. Neural network has width 1000 and depth 3. Notice that the initialization variance dominates in the large P large α regime\n\nB.3 RELATING ENSEMBLED NETWORK GENERALIZATION TO INFINITE WIDTH\n\nGENERALIZATION\n\nMaking use of the fact that at leading order, the eNTKf (either in the rich or lazy regime) of a trained network has θ0-dependent fluctuations with variance 1/N , one can write the kernel Gram matrices as\n\n[Kθ0]μν = [K∞]μν +\n\n[δKθ0]μν + O(1/N )\n\n[kθ0 (x)]μ = [k∞(x)]μ +\n\n[δkθ0 (x)]μ + O(1/N ).\n\n(15)\n\n1 √\n\nN 1\n√\n\nN\n\nHere, δKθ0 , δkθ0 are the leading order fluctuations around the infinite width network. Because of how we have written them, their variance is O(1) with respect to N . Using perturbation theory\n\n15\n\n2.02.53.03.5logP1.00.50.00.51.0logvar/Eg(f), NN, L=3, k=2, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logvar/Eg(f), NN, L=3, k=3, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logvar/Eg(f), NN, L=3, k=4, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logbagging var/Eg(f), NN, L=3, k=2, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logbagging var/Eg(f), NN, L=3, k=3, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logbagging var/Eg(f), NN, L=3, k=4, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logtotal var/Eg(f), NN, L=3, k=2, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logtotal var/Eg(f), NN, L=3, k=3, N=10000.00.20.40.60.81.02.02.53.03.5logP1.00.50.00.51.0logtotal var/Eg(f), NN, L=3, k=4, N=10000.00.20.40.60.81.0Published as a conference paper at ICLR 2023\n\n(Dyer & Gur-Ari, 2020), one can demonstrate that these leading order terms have mean zero around their infinite-width limit.\n\nThe predictor for the eNTK0 (or for a sufficiently large α neural network) for a training set with target labels y is given by:\n\nf ∗(x)θ0 = kθ0(x)⊤\n\nμ K−1 θ0 1\n√\n\n· y\n\nδkθ0 (x)⊤K−1\n\n∞ · y −\n\n1 √\n\nN\n\nN\n\n= f ∞(x) +\n\nk∞(x)⊤K−1\n\n∞ δKθ0 K−1\n\n∞ · y + O (cid:0)N −1(cid:1) .\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\nThis implies (Geiger et al., 2020a):\n\n⟨(f ∗ θ0\n\n(x) − f ∞(x))2⟩x = O (cid:0)N −1(cid:1) .\n\nUpon taking the ensemble, because of the mean zero property of the deviations, we get that\n\n⟨f ∗(x)θ0⟩θ0 = f ∞(x) + O (cid:0)N −1(cid:1)\n\n⇒ ⟨(⟨f ∗(x)θ0 ⟩θ0 − f ∞(x))2⟩ = O (cid:0)N −2(cid:1) .\n\nWe can now bound the generalization error of the ensemble of networks in terms of the infinite-width generalization:\n\n⟨(⟨f ∗ θ0\n\n(x)⟩θ0 − y)2⟩x,y = ⟨(f ∞(x) − y)2⟩x,y + ⟨(⟨f ∗\n\nθ0\n\n− 2⟨(f ∞(x) − y)(f ∞(x) − ⟨f ∗ θ0\n\n(x)⟩θ0 − f ∞(x))2⟩x (x)⟩θ0 )⟩x,y.\n\nBy equation 18, the second term yields a positive contribution going as O(N −2). The last term can be bounded by Cauchy-Schwarz:\n\n|⟨(f ∞(x) − y)(f ∞(x) − ⟨f ∗ θ0\n\n(x)⟩θ0)⟩x,y| ≤\n\n=\n\n(cid:113)\n\n(cid:113)\n\n⟨(f ∞(x) − y)2⟩⟨(f ∞(x) − ⟨f ∗ θ0\n\n(x)⟩θ0)2⟩x,y\n\nE∞\n\ng (P )c1/N.\n\n(20) After we enter the variance limited regime by taking P > P1/2 we get E∞ g ≤ O(1/N ) so this last term is bounded by N −3/2. Consequently, the difference in generalization error between the infinite width NTK and an ensemble of lazy network or eNTK0 predictors is subleading in 1/N compared to the generalization gap, which goes as N −1.\n\nThe same argument can be extended to any predictor that differs from some infinite width limit. In particular Bordelon & Pehlevan (2022) show that the fluctuations of the eNTKf in any mean field network are asymptotically mean zero with variance N −1. The above argument then applies to the predictor obtained by ensembling networks that have learned features. This implies that in the variance limited regime, ensemble averages of feature learning networks have the same generalization as the infinite-width mean field solutions up to a term that decays faster than N −3/2.\n\nC FEATURE LEARNING\n\nC.1 CONTROLLING FEATURE LEARNING THROUGH INITIALIZATION SCALE\n\nGiven the feed-forward network defined in equation 1, one can see that the components of the activations satisfy h(l) i = O(σhi)(l−1) and consequently that the output h(L) 1 = O(σL). Because of the way the network is parameterized, the changes in the output ∂f ∂θ also scale as O(σL). This implies that the eNTK at any given time scales as\n\nKθ(x, x′) =\n\n∂f (x) ∂θ\n\n∂f (x′) ∂θ\n\n(cid:88)\n\nθ\n\n= O(σ2L).\n\nAfter appropriately rescaling learning rate to η = σ−2L we get\n\ndf (x) dt\n\n= −η\n\n(cid:88)\n\nμ\n\nKθ(x, xμ)(f (xμ) − yμ).\n\n16\n\n(21)\n\n(22)\n\nPublished as a conference paper at ICLR 2023\n\nUnder the assumption that σL ≪ 1 and yμ = O(1) so that the error term is O(1) we get that the output changes in time as df /dt = O(1).\n\nOn the other hand, using the chain rule one can show that the features change as a product of the gradient update and the features in the prior layer, yielding the scaling\n\ndh(l) dt\n\n= η\n\nσL √\n\nN\n\n=\n\n1 √\n\nN\n\nσL\n\n√\n\n= (α\n\nN )−1.\n\n(23)\n\n√\n\nN )−1 while the change in the output\n\nThis gives us that the change in the features scales as (α scales as O(1). Thus, for α\n\n√\n\nN sufficiently small, the features can move dramatically.\n\nC.2 OUTPUT RESCALING WITHOUT RESCALING WEIGHTS\n\nIn the main text, we use the scale σ at every layer to change the scale of the output function. This relies on the homogeneity of the activation function so that W l → σW l for all l leads to a rescaling f → f σL. This would not work for nonhomogenous activations like φ(h) = tanh(h). However, following Chizat et al. (2019); Geiger et al. (2020a), we note that we can set all weights to be Oα(1) and introduce the α only in the definition of the neural network function\n\nf =\n\nα √\n\nN\n\nN (cid:88)\n\ni=1\n\nwL+1\n\ni\n\nφ(hL\n\ni ) , hl\n\ni =\n\n1 √\n\nN\n\nN (cid:88)\n\nj=1\n\nW l\n\nijφ(hl−1\n\nj\n\n) , h1\n\ni =\n\n1 √\n\nD\n\nW 1\n\nijxj.\n\n(24)\n\nWe note that all preactivations hl have scale Oα(1) for any choice of nonlinearity, but that f = Θα(α). Several works have established that the α ∼ 1√ allows feature learning even as the network approaches infinite width Mei et al. (2018); Yang & Hu (2020); Bordelon & Pehlevan (2022). This is known as the mean field or μ-limit.\n\nN\n\nC.3 KERNEL ALIGNMENT\n\nIn this section we comment on our choice of kernel alignment metric\n\nA(K) :=\n\ny⊤Ky Tr K|y|2 .\n\n(25)\n\nFor kernels that are diagonally dominant, such as those encountered in the experiments, this metric is related to another alignment metric\n\nAF (K) :=\n\ny⊤Ky |K|F |y|2 .\n\n(26)\n\nHere |K|F is the Frobenius norm of the Gram matrix of the kernel. This metric was extensively used in Baratin et al. (2021). The advantage of the first metric over the second is that one can quickly estimate the denominator of A(K) via Monte Carlo estimation of ⟨u⊤Ku⟩u∼N (0,1).\n\nWe use A(Kf ) as a measure of feature learning, as we have found that this more finely captures elements of feature learning than other related metrics. We list several metrics we tried that did not work.\n\nOne option for a representation-learning metric involves measuring the magnitude of the change between the initial and final kernels, Ki, Kf :\n\n∆K := |Kf − Ki|F . However, this is more sensitive to the raw parameter change than any task-relevant data. If one instead were to normalize the kernels to be unit norm at the beginning and the end, the modified metric\n\n(27)\n\n∆K :=\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\nKf |Kf |F\n\n−\n\nKi |Ki|F\n\n(cid:12) (cid:12) (cid:12) (cid:12)F\n\n.\n\n(28)\n\nThis metric however remains remarkably flat over the whole range of α, P , as does the centered kernel alignment (CKA) of Cortes et al. (2012)\n\nCKA(Ki, Kf ) =\n\nTr[Kc\n\ni Kc f ] i |F |Kc\n\nf |F\n\n|Kc\n\n(cid:113)\n\n, Kc = CKC, C = 1 −\n\n1 P\n\n⃗1⃗1 T .\n\n(29)\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nHere C is the centering matrix that subtracts off the mean components of the kernel for a P × P kernel. This alignment metric has been shown to be useful in comparing neural representations (Kornblith et al., 2019). For our task, however, because the signal is low-dimensional, only a small set of eigenspaces of the kernel align to this task. As a result, the CKA, which counts all eigenspaces equally, appears to be too coarse to capture the low-dimensional feature learning that is happening.\n\nOn the other hand, we find that A(Kf ) (with Kf given by the eNTKf evaluated on a test set) can very finely detect alignment along the task relevant directions. This produces a clear signal of feature learning at small α and large P as shown in Figure 2c.\n\nA(Kf ) can be related to the centered kernel alignment between the eNTKf and the (mean zero) task kernel yy⊤, where y is a vector of draws from the population distribution p(x, y).\n\nC.4 RELATIONSHIP BETWEEN TRAINED NETWORK AND FINAL KERNEL\n\nIn general, the learned function contains contributions from the instantaneous NTKs at every point in the training. Concretely, following Atanasov et al. (2021) we have the following formula for the final network predictor f (x)\n\nf (x) =\n\n(cid:90) ∞\n\n0\n\ndt k(x, t) · exp\n\n−\n\n(cid:18)\n\n(cid:19)\n\ndsK(s)\n\ny,\n\n(cid:90) t\n\n0\n\n(30)\n\nwhere [k(x, t)]μ = K(x, xμ, t) and [K(s)]μν = K(xμ, xν, s) and [y]μ = yμ. In general there are contributions from earlier kernels k(x, t) for t < ∞ and so the function f cannot always be written as a linear combination of the final NTK Kf on training data: f = (cid:80) μ αμKf (x, xμ). However, as Vyas et al. (2022); Atanasov et al. (2021) have shown, the final predictions of the network are often well modeled by regression with the final NTK. We verify this for our task in section 3.2.\n\nD GENERIC RANDOM FEATURE MODEL\n\nD.1 SETTING UP THE PROBLEM: FEATURE DEFINITIONS\n\nFor a random kernel, K(x, x′; θ), we first compute its Mercer decomposition\n\n(cid:90)\n\ndx p(x)K(x, x′; θ)φk(x) = λkφk(x′).\n\nFrom the eigenvalues λk and eigenfunctions φk, we can construct the square root\n\nK 1/2(x, x′; θ) =\n\n(cid:88)\n\n(cid:112)\n\nk\n\nλkφk(x)φk(x′).\n\nLastly, using K 1/2, we can get a feature map by projecting against a static basis {bk} giving\n\n(cid:90)\n\nψk(x) =\n\ndx′p(x′)K 1/2(x, x′; θ)bk(x′).\n\n(31)\n\n(32)\n\n(33)\n\nThese features reproduce the kernel so that K(x, x′; θ) = (cid:80) from the following observation (cid:88)\n\n(cid:112)\n\nψk(x) =\n\nλlφl(x)Ulk , Ulk = ⟨φl(x)bk(x)⟩\n\nk ψk(x)ψk(x′). This can be observed\n\n(34)\n\n(cid:88)\n\n⇒\n\nk\n\nψk(x)ψk(x′) =\n\nl (cid:88)\n\n(cid:112)\n\nl,m\n\nλlλmφl(x)φm(x′)\n\n(cid:88)\n\nk\n\nUlkUmk =\n\n(cid:88)\n\nl\n\nλlφl(x), φl(x′)\n\n(35)\n\nwhere the last line follows from the orthogonality of Ukm and recovers K(x, x′; θ).\n\nD.2 DECOMPOSITION OF FINITE WIDTH FEATURES\n\nWe now attempt to characterize the variance in the features over the sample distribution. We will first consider the case of a fixed realization of θ0 before providing a typical case analysis over random θ0. For a fixed initialization θ0 we define the following covariance matrices\n\nΣM = (cid:10)ψM (x)ψM (x)⊤(cid:11) ∈ RM ×M .\n\n(36)\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nwhere ψM are the truncated (but deterministic) features induced by the deterministic infinite width kernel. We will mainly be interested in the case where M → ∞ and where the target function can be expressed as the linear combination y(x) = w∗ · ψM (x) of these features. For example, in the case of our experiments on the sphere, ψM could be the spherical harmonic functions. Further, in the M → ∞ limit, we will be able to express the target features ψ as linear combinations of the features ψM\n\nψ(x, θ0) = A(θ0)ψM (x) , A(θ) ∈ RNH×M . The matrix A(θ0) are the coefficients of the decomposition which can vary over initializations. Crucially A(θ0) projects to the subspace of dimension NH where the finite width features have variance over x. The population risk for this θ0 has an irreducible component\n\n(37)\n\nEg(θ0) =\n\n(cid:68)\n\n(w∗ · ψM − w · ψ)2(cid:69)\n\n≥ w∗⊤ (cid:104)\n\nΣM − ΣM A(θ)⊤ (cid:0)A(θ0)ΣM A(θ0)⊤(cid:1)−1\n\nA(θ0)ΣM\n\n(cid:105)\n\nw∗.\n\n(38)\n\nwhere the bound is tight for the optimal weights w = (cid:0)A(θ0)ΣM A(θ0)⊤(cid:1)−1 A(θ0)ΣM w∗. The irreducible error is determined by a projection matrix which preserves the subspace where the features ψ(x, θ0) have variance: I − A(θ)⊤ (cid:0)A(θ0)ΣM A(θ0)⊤(cid:1)−1 A(θ0)ΣM . In general, this will preserve some fraction of the variance in the target function, but some variance in the target function will not be expressible by linear combinations of the features ψ(x, θ). We expect that random finite width N neural networks will have unexplained variance in the target function on the order ∼ 1/N .\n\nD.3 GAUSSIAN COVARIATE MODEL\n\nFollowing prior works on learning curves for kernel regression (Bordelon et al., 2020; Canatar et al., 2021; Loureiro et al., 2021), we will approximate the learning problem with a Gaussian covariates model with matching second moments.\n\nThe features ψM (x) will be treated as Gaussian over random draws of datapoints. We will assume centered features. We decompose the features in the orthonormal basis b(x), which we approximate as a Gaussian vector b ∼ N (0, I).\n\nf = ψ(θ0) · w , y = ̄ψM · w∗\n\nψM = Σ1/2\n\ns b , ψ(θ0) = A(θ0)⊤ψM + Σ1/2 ε ε\n\nb ∼ N (0, I) , ε ∼ N (0, I)\n\n(39)\n\nThis is a special case of the Gaussian covariate model introduced by Loureiro et al. (2021) and subsumes the popular two-layer random feature models (Mei & Montanari, 2022; Adlam & Pennington, 2020b) as a special case. In a subsequent section, we go beyond Loureiro et al. (2021) by computing typical case learning curves over Gaussian A(θ0) matrices. In particular, we have for the two layer random feature model in the proportional asymptotic limit P, N, D → ∞ with P/D = O(1) and P/N = O(1) with ψ(x) = φ(F xμ) for fixed feature matrix F ∈ RN ×D nonlinearity φ and x = b ∼ N (0, D−1I)\n\nΣM = I , Σε = c2\n\n∗I , A⊤ = c1F\n\nc1 = ⟨zφ(z)⟩z∼N (0,1) , c2\n\n∗ = (cid:10)φ(z)2(cid:11)\n\nz∼N (0,1) − c2 1.\n\n(40)\n\nWe refer readers to Hu & Lu (2020) for a discussion of this equivalence between random feature regression and this Gaussian covariate model.\n\nD.4 REPLICA CALCULATION OF THE LEARNING CURVE\n\nTo analyze the typical case performance of kernel regression, we define the following partition function:\n\nZ[D, θ0] =\n\n(cid:90)\n\n(cid:32)\n\ndw exp\n\n−\n\nβ 2λ\n\nP (cid:88)\n\n[w · ψμ − w∗ · ψM,μ]2 −\n\nμ=1\n\nβ 2\n\n|w|2 −\n\nJβM 2\n\nEg(w)\n\n(cid:33)\n\nEg(w) =\n\n1 M\n\n|Σ1/2\n\nM w∗ − Σ1/2\n\nM A(θ0)w|2 +\n\n1 M\n\nw⊤Σεw.\n\n(41)\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nFor proper normalization, we assume that (cid:10)ψM ψ⊤ M Σε. We note that in the β → ∞ limit, the partition function is dominated by the unique minimizer of the regularized least squares objective (Canatar et al., 2021; Loureiro et al., 2021). Further, for a fixed realization of θ0 the average generalization error over datasets D can be computed by differentiation of the source term J\n\nM ΣM and (cid:10)εε⊤(cid:11) = 1\n\n(cid:11) = 1\n\nM\n\n2 βM\n\n∂ ∂J\n\n|J=0 ⟨ln Z[D, θ0]⟩D\n\n(cid:42)\n\n(cid:90)\n\n1 Z\n\n=\n\n(cid:32)\n\ndw exp\n\n−\n\nβ 2λ\n\nP (cid:88)\n\nμ=1\n\n[w · ψμ − w∗ · ψM,μ]2 −\n\n(cid:33)\n\n(cid:43)\n\n|w|2\n\nEg(w)\n\n.\n\nβ 2\n\nD\n\n(42)\n\nThus the β → ∞ limit of the above quantity will give the expected generalization error of the risk minimizer. We see the need to average the quantity ln Z over realizations of datasets D. For this, we n ln ⟨Z n⟩. We will compute the integer moments ⟨Z n⟩ resort to the replica trick ⟨ln Z⟩ = limn→0 for integer n and then analytically continue the resulting expressions to n → 0 under a symmetry ansatz. The replicated partition function thus has the form\n\n1\n\n⟨Z n⟩ =\n\n(cid:90) n (cid:89)\n\na=1\n\n(cid:32)\n\ndwaE{bμ,εμ} exp\n\n−\n\nβ 2λ\n\nP (cid:88)\n\nn (cid:88)\n\n[wa · ψμ − w∗ · ψM,μ]2 −\n\nμ=1\n\na=1\n\n(cid:32)\n\n× exp\n\n−\n\nJβM 2\n\nn (cid:88)\n\na=1\n\n(cid:33)\n\nEg(wa)\n\n.\n\n(cid:33)\n\n|wa|2\n\nβ 2\n\nn (cid:88)\n\na=1\n\n(43)\n\nWe now need to perform the necessary average over the random realizations of data points D = {bμ, εμ}. We note that the scalar quantities ha μ = wa · ψμ − w∗ · ψM,μ are Gaussian with mean zero and covariance\n\n(cid:10)ha\n\nμhb\n\nν\n\n(cid:11) = δμνQab 1\nM\n\nQab =\n\n(A(θ0)wa − w∗) ΣM (A(θ0)wa − w∗) +\n\n1 M\n\nwaΣεwb.\n\n(44)\n\nWe further see that the generalization error in replica a is Eg(wa) = Qaa. Performing the Gaussian integral over {ha\n\nμ} gives\n\n⟨Z n⟩ ∝\n\n(cid:90) (cid:89)\n\ndwa (cid:89)\n\n(cid:32)\n\ndQabd ˆQab exp\n\n−\n\nP 2\n\nln det (λI + βQ) −\n\nJβM 2\n\nTrQ −\n\nβ 2\n\n(cid:88)\n\n|wa|2\n\na\n\n(cid:33)\n\na\n\nab\n\n(cid:32)\n\nexp\n\n1 2\n\n(cid:88)\n\nab\n\nˆQab\n\n(cid:0)M Qab − [A(θ0)wa − w∗]⊤ΣM [A(θ0)wa − w∗] + waΣεwb(cid:1)\n\n(cid:33)\n\n.\n\nWe introduced the Lagrange multipliers ˆQ which enforce the definition of order parameters Q. We a=1. We let ̃Σs = A⊤ΣM A now integrate over W = Vec{wa}n (cid:19)\n\n(cid:18)\n\n(cid:90)\n\ndW exp\n\n−\n\nW\n\n(cid:104)\n\n(cid:105) βI + ˆQ ⊗ [ ̃Σs + Σε]\n\nW\n\n1 2\n\n(cid:105) (cid:0)1 ⊗ A⊤Σsw∗(cid:1)(cid:17)\n\nexp\n\n= exp\n\n(cid:16)\n\nW ⊤ (cid:104) ˆQ ⊗ I (cid:18) 1 2\n\n(cid:0)1 ⊗ A⊤Σsw∗(cid:1)⊤ (cid:104) ˆQ ⊗ I\n\n(cid:105) (cid:104)\n\nβI + ˆQ ⊗ [ ̃Σs + Σε]\n\n(cid:105)−1 (cid:104) ˆQ ⊗ I\n\n(cid:105) (cid:0)1 ⊗ A⊤Σsw∗(cid:1)\n\n(cid:19)\n\n(cid:18)\n\nexp\n\n−\n\n1 2\n\n(cid:104)\n\nln det\n\nβI + ˆQ ⊗ [ ̃Σs + Σε]\n\n(cid:105)(cid:19)\n\n.\n\nTo take the n → 0 limit, we make the replica symmetry ansatz\n\nβQ = qI + q011⊤ , β−1 ˆQ = ˆqI + ˆq011⊤,\n\n20\n\n(45)\n\n(46)\n\nPublished as a conference paper at ICLR 2023\n\nwhich is well motivated since this is a convex optimization problem. Letting α = P/N , we find that under the RS ansatz the replicated partition function has the form\n\n(cid:90)\n\n⟨Z n⟩ =\n\ndqdq0dˆqdˆq0 exp\n\n(cid:18) nM 2\n\n(cid:20)\n\n(cid:19)\n\nS[q, q0, ˆq, ˆq0]\n\nS = q ˆq + q0 ˆq + q ˆq0 − α\n\nln(λ + q) +\n\n(cid:21)\n\nq0 λ + q\n\nβ M\n1 M\n(cid:16)\n\n−\n\n+\n\nG =\n\nw∗[ˆqΣM ]w∗ +\n\nβ M\n\nw∗[ˆqΣs]A⊤GA[ˆqΣs]w∗\n\nˆq0TrG[ ̃Σs + Σε] − J(q + q0)\n\nln det G −\n\n1 M\nI + ˆq[ ̃Σs + Σε]\n\n(cid:17)−1\n\n.\n\n(47)\n\n(48)\n\nIn a limit where α = P/M is O(1), then this S is intensive S = OM (1). We can thus appeal to saddle point integration (method of steepest descent) to compute the set of order parameters which have dominant contribution to the free energy. (cid:18) nM 2\n\ndqdq0dˆqdˆq0 exp\n\n(cid:18) nM 2\n\nS[q, q0, ˆq, ˆq0]\n\n0, ˆq∗, ˆq∗ 0]\n\nS[q∗, q∗\n\n⟨Z n⟩ =\n\n∼ exp\n\n(cid:19)\n\n(cid:19)\n\n(cid:90)\n\n=⇒ ⟨ln Z⟩ =\n\nM 2\n\nS[q∗, q∗\n\n0, ˆq∗, ˆq∗\n\n0].\n\nThe order parameters q∗, q∗ ∂S ∂ ˆq0\n\n= 0. For our purposes, it suffices to analyze two of these equations\n\n0, ˆq∗, ˆq∗\n\n0 are defined via the saddle point equations ∂S\n\n∂q = ∂S\n\n∂q0\n\n∂S ∂q0 ∂S ∂ ˆq0\n\n= ˆq −\n\n= q −\n\nα λ + q 1\nM\n\n− J = 0,\n\nTrG\n\n(cid:104) ̃Σs + Σε\n\n(cid:105)\n\n= 0.\n\nWe can now take the zero temperature (β → ∞) limit to solve for the generalization error\n\n1 β\n\n|J=0 lim β→∞\n\n∂ ∂J ∂J w∗ (cid:2)ˆqΣM − ˆq2ΣM A⊤GAΣM\n\nF\n\nEg = −\n\n=\n\n1 M\n\n(cid:3) w∗.\n\n(49)\n\n= ∂S\n\n∂ ˆq =\n\n(50)\n\n(51)\n\nWe see that we need to compute the J derivatives on ˆq. We let κ = λ + q and note\n\n∂J ˆq = −ακ−2∂J κ + 1\n\n∂J κ = −∂J ˆq\n\nTrG2 (cid:104) ̃Σs + Σε\n\n(cid:105)2\n\n1 M\n\n= − (cid:0)−ακ−2∂J κ + 1(cid:1) 1\n\nM\n\nTrG2 (cid:104) ̃Σs + Σε\n\n(cid:105)2\n\n.\n\n(52)\n\nWe solve the equation for ∂J κ which gives ∂J κ = − κ2 With this definition we have ∂J ˆq = 1 + γ\n\n1−γ = 1\n\n1−γ .\n\nα\n\nγ\n\n1−γ where γ = α\n\nκ2\n\n1\n\nM TrG2 (cid:104) ̃Σs + Σε\n\n(cid:105)2\n\n.\n\nEg =\n\n=\n\n1 1 − γ 1\n1 − γ\n\n1 M\n1 M\n\n(cid:104)\n\n(cid:104)\n\nw∗Σ1/2\n\nM\n\nw∗Σ1/2\n\nM\n\nI − 2ˆqΣ1/2\n\ns A⊤GAΣ1/2\n\ns + ˆq2Σ1/2\n\ns A⊤G\n\n(cid:104) ̃Σs + Σε\n\n(cid:105)\n\nGAΣ1/2\n\ns\n\n(cid:105)\n\nΣ1/2\n\nM w∗\n\nI − ˆqΣ1/2\n\ns A⊤GAΣ1/2\n\ns − ˆqΣ1/2\n\ns A⊤G2AΣ1/2\n\ns\n\n(cid:105)\n\nΣ1/2\n\nM w∗.\n\n(53)\n\nThis reproduces the derived expression from Loureiro et al. (2021). The matching covariance ̃Σs = ΣM and zero feature-noise limit Σε = 0 recovers the prior results of Bordelon et al. (2020); Canatar et al. (2021); Simon et al. (2021). In general, this error will asymptote to the the irreducible error\n\nlim P →∞\n\nEg =\n\n1 M\n\nw∗\n\n(cid:20)\n\nΣM − ΣsA⊤ (cid:16) ̃Σs + Σε\n\n(cid:17)−1\n\n(cid:21)\n\nw∗.\n\nAΣs\n\n(54)\n\n21\n\nPublished as a conference paper at ICLR 2023\n\nWe see that this recovers the minimal possible error in the P → ∞ limit. The derived learning curves depend on the instance of random initial condition θ0. To get the average case performance, we take an additional average of this expression over θ0\n\nEθ0Eg(θ0) = Eθ0\n\n1 1 − γ\n\n1 M\n\n(cid:104)\n\nw∗Σ1/2\n\nM\n\nI − ˆqΣ1/2\n\ns A⊤GAΣ1/2\n\ns − ˆqΣ1/2\n\ns A⊤G2AΣ1/2\n\ns\n\n(cid:105)\n\nΣ1/2\n\nM w∗. (55)\n\nThis average is complicated since γ, ˆq, G all depend on θ0. In the next section we go beyond this analysis to try average case analysis for random Gaussian A.\n\nD.5 QUENCHED AVERAGE OVER GAUSSIAN A\n\nIn this section we will define a distribution of features which allows an exact asymptotic prediction over random realizations of disorder θ0 and datasets D. This is a nontrivial extension of the result of Loureiro et al. (2021) since the number of necessary saddle point equations to be solved doubles from two to four. However, this more complicated theory allows us to exactly compute the expectation in equation 55 under an ansatz for the random matrix A. We construct our features with\n\nψ|A =\n\n1 √\n\nN\n\nA⊤ψM + Σ1/2\n\nε ε , Aij ∼ N (0, σ2).\n\nWe will now perform an approximate average over both datasets D and realizations of A\n\n⟨Z n⟩ =\n\n(cid:90) n (cid:89)\n\na=1\n\ndwaE{bμ,εμ,A} exp\n\n−\n\n(cid:32)\n\nβ 2λ\n\nP (cid:88)\n\nn (cid:88)\n\nμ=1\n\na=1\n\n[wa · ψμ − w∗ · ψM,μ]2 −\n\n(cid:33)\n\n|wa|2\n\nβ 2\n\nn (cid:88)\n\na=1\n\n(cid:32)\n\n× exp\n\n−\n\nJM β 2\n\nn (cid:88)\n\na=1\n\n(cid:33)\n\nEg(wa)\n\n.\n\nAs before, we first average over bμ, εμ|A and define order parameters Qab as before.\n\n⟨Z n⟩ =\n\n(cid:90) (cid:89)\n\ndwa (cid:89)\n\n(cid:32)\n\ndQabd ˆQab exp\n\n−\n\nP 2\n\nln det (λI + βQ) −\n\nJβM 2\n\nTrQ −\n\nβ 2\n\n(56)\n\n(cid:33)\n\n(cid:88)\n\n|wa|2\n\na (cid:33)\n\n.\n\nˆQab\n\n(cid:0)M Qab − [ga − w∗]⊤ΣM [gb − w∗] + waΣεwb(cid:1)\n\na\n\nab\n\nE{ga} exp\n\n(cid:32)\n\n1 2\n\n(cid:88)\n\nab\n\nwhere we defined the fields ga = 1√ (cid:10)gagb⊤(cid:11) = VabI where Vab = σ2 we find\n\nAwa which are mean zero Gaussian with covariance N wa · wb. Performing the Gaussian integral over G = Vec{ga},\n\nN\n\n(cid:90) (cid:89)\n\na\n\ndga exp\n\n(cid:18)\n\n(cid:104)\n\nG\n\n−\n\n1 2\n\nI ⊗ V −1 + ΣM ⊗ ˆQ\n\n(cid:105)\n\nG + (ΣM w∗ ⊗ ˆQ1)G −\n\n= exp\n\n(cid:18) 1 2\n\n(ΣM w∗ ⊗ ˆQ1)\n\n(cid:104) I ⊗ V −1 + ΣM ⊗ ˆQ\n\n(cid:105)−1\n\n(ΣM w∗ ⊗ ˆQ1) −\n\nln det (I ⊗ V )\n\n(cid:19)\n\n1 2\n\n(cid:16)\n\nln det\n\n1 2\n\nI + ΣM ⊗ ˆQV\n\n(cid:17)(cid:19)\n\n.\n\n(57)\n\nNext, we need to integrate over W = Vec{wa} which gives\n\n(cid:90)\n\n(cid:18)\n\ndW exp\n\n−\n\n(cid:104)\n\nW\n\n1 2\n\nβI + σ2I ⊗ ˆV + Σε ⊗ ˆQ\n\n(cid:105)\n\n(cid:19)\n\n(cid:18)\n\nW\n\n= exp\n\n−\n\n(cid:104)\n\nln det\n\n1 2\n\nβI + σ2I ⊗ ˆV + Σε ⊗ ˆQ\n\n(cid:105)(cid:19)\n\n.\n\n(58)\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nNow the replicated partition function has the form\n\n(cid:90)\n\n⟨Z n⟩ =\n\ndQd ˆQdV d ˆV exp\n\n(cid:18) M 2\n\nTr[Q ˆQ + ηV ˆV ] −\n\nJβM 2\n\nTrQ −\n\nP 2\n\nln det [λI + βQ]\n\n(cid:19)\n\n(cid:18)\n\n× exp\n\n−\n\n1 2\n\n(w∗ ⊗ 1)⊤[ΣM ⊗ ˆQ](w∗ ⊗ 1)\n\n(cid:19)\n\n× exp\n\n(cid:18) 1 2\n\n(cid:18)\n\n× exp\n\n−\n\n(ΣM w∗ ⊗ ˆQ1)⊤ (cid:104)\n\nI ⊗ V −1 + ΣM ⊗ ˆQ\n\n(cid:105)−1\n\n(ΣM w∗ ⊗ ˆQ1)\n\n(cid:19)\n\nln det\n\n(cid:104) I + ΣM ⊗ ˆQV\n\n(cid:105)\n\n−\n\n1 2\n\n1 2\n\nln det\n\n(cid:104) βI + σ2I ⊗ ˆV + Σε ⊗ ˆQ\n\n(cid:105)(cid:19)\n\n.\n\nNow we make a replica symmetry ansatz on the order parameters Q, ˆQ, V , ˆV\n\nβQ = qI + q011⊤ , βV = vI + v011⊤ β−1 ˆQ = ˆqI + ˆq011⊤ , β−1 ˆV = ˆvI + ˆv011⊤.\n\n(59)\n\n(60)\n\nWe introduce the shorthand for normalized trace of a matrix G as tr G = 1 symmetry ansatz, we find the following free energy\n\nM TrG. Under the replica\n\n2 M\n\n⟨ln Z⟩ = q ˆq + q0 ˆq + q ˆq0 + η(vˆv + v0ˆv + vˆv0) − J(q + q0) − α\n\nln(λ + q) +\n\n(cid:20)\n\n(cid:21)\n\nq0 λ + q\n\n−\n\nβ M\n\nw∗[ˆqΣM ]w∗ +\n\nβ M\n\nw∗[ˆqΣM ][v−1I + ˆqΣM ]−1[ˆqΣM ]\n\n− tr log [I + ˆqvΣM ] − (ˆq0v + ˆqv0) tr[I + ˆqvΣM ]−1ΣM − tr log (cid:2)I + σ2ˆvI + Σε ˆq(cid:3) − tr (cid:2)I + σ2ˆvI + Σε ˆq(cid:3)−1 (cid:2)ˆv0σ2I + ˆq0Σε\n\n(cid:3) .\n\n(61)\n\nLetting F = 2M −1 ⟨ln Z⟩, the saddle point equations read\n\n= ˆq −\n\nα λ + q\n\n− J = 0,\n\n= q − v tr[I + ˆqvΣM ]−1ΣM − tr[I + σ2ˆvI + Σε ˆq]−1Σε = 0,\n\n= ηˆv − ˆq tr[I + ˆqvΣM ]−1ΣM = 0,\n\n= ηv − σ2 tr[I + σ2ˆvI + Σε ˆq]−1 = 0.\n\n(62)\n\n∂F ∂q0 ∂F ∂ ˆq0 ∂F ∂v0 ∂F ∂ˆv0\n\nNow the generalization error can be determined from\n\nEg = −\n\n∂ ∂J\n\nlim β→∞\n\n2 βM\n\n⟨ln Z⟩ = ∂J\n\n1 M\n\nw∗ (cid:2)ˆqΣM − (ˆqΣM )[v−1I + ˆqΣM ]−1(ˆqΣM )(cid:3) w∗.\n\n(63)\n\nWe see that it is necessary to compute ∂J ˆq and ∂J v in order to obtain the final result. For simplicity, we set σ2 = 1. The equations for the source derivatives are\n\n∂J ˆq = −\n\nα\n\n(λ + q)2 ∂J q + 1,\n\n∂J q = −tr[I + v ˆqΣM ]−2[−∂J vI + v2∂J ˆqΣM ]ΣM − tr[I + ˆvI + ˆqΣε]−2Σε[∂J ˆvI + ∂J ˆqΣε], η∂J ˆv = −tr[I + v ˆqΣM ]−2ΣM [−∂J ˆqI + ˆq2∂J vΣM ], η∂J v = −tr[I + ˆvI + Σε ˆq]−2[∂J ˆvI + ∂J ˆqΣε].\n\n(64)\n\nOnce the value of the order parameters (q, ˆq, v, ˆv) have been determined, these source derivatives can be obtained by solving a 4 × 4 linear system. Examples of these solutions are provided in Figure 16.\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nD.5.1 ASYMPTOTICS IN UNDERPARAMETERIZED REGIME\n\nWe can compute the asymptotic (α → ∞) generalization error due to the random projection A in the limit of Σε = 0. First, note that if ˆv → Oα(1), then the asymptotic error would be zero. Therefore, we will assume that ˆv ∼ aαc for some a, c > 0. The saddle point equations give the following asymptotic conditions\n\nˆq ∼\n\nα λ\n\n, η ∼ ˆq tr[ˆvI + ˆqΣM ]−1ΣM\n\n=⇒ η = tr[λaαc−1I + ΣM ]−1ΣM .\n\n(65)\n\nFor 0 < η < 1, this equation can only be satisfied as α → ∞ if c = 1 so that ˆv has the same scaling with α as ˆq. If c < 1 then we could get the equation η = 1. If c > 1, then the equation would give η = 0. The constant a solves the equation\n\nUsing this fact, our order parameters satisfy the following large α scalings\n\nη = tr[λaI + ΣM ]−1ΣM .\n\nˆq ∼\n\nα λ\n\n, q ∼ 0 , ˆv ∼ aα , v ∼ 0.\n\nThe source derivative equations simplify to ∂J ˆq ∼ 1 , ∂J q ∼ 0 and\n\nη∂J ˆv ∼ (ˆv∂J ˆq + ˆq∂J ˆv) tr[ˆvI + ˆqΣM ]−1ΣM − ˆqˆvtr[ˆvI + ˆqΣ]−2[∂J ˆvΣM + ∂J ˆqΣ2\n\nM ]\n\n∼ ˆq2tr[ˆvI + ˆqΣM ]−2Σ2\n\nM ∂J ˆv + ˆv2tr[ˆvI + ˆqΣM ]−2ΣM\n\n⇒ ∂J ˆv ∼\n\ntr[I + a−1λ−1ΣM ]−2ΣM η − tr[aλI + ΣM ]−2Σ2\n\nM\n\n.\n\n(66)\n\n(67)\n\n(68)\n\nWe note that ∂J ˆv only depends on the product aλ which is an implicit function of η and ΣM . The generalization error is Eg = 1\n\nM ∂J ˆq(1 + ˆv)w∗[(1 + ˆv)I + ˆqΣ]−1ΣM w∗\n\nEg ∼\n\n1 M\n\nw∗ (cid:2)I + a−1λ−1ΣM\n\n(cid:3)−2\n\nΣM w∗\n\n+\n\n1 M\n\nw∗[λaI + ΣM ]−2Σ2\n\nM w∗ ×\n\ntr[I + a−1λ−1ΣM ]−2ΣM η − tr[aλI + ΣM ]−2Σ2\n\nM\n\n.\n\n(69)\n\nWe see that in the generic case, the asymptotic error has a nontrivial dependence on the task w∗ and the correlation structure ΣM . To gain more intuition, we will now consider the special case of isotropic features ΣM = I. In this case, we have η = 1 η . This results in the following generalization error\n\n1+λa so that λa = 1−η\n\nEg ∼\n\n1 M\n\n|w∗|2\n\n(cid:20)\n\n(1 − η)2 + η2 (1 − η)2\n\nη − η2\n\n(cid:21)\n\n∼\n\n1 M\n\n|w∗|2(1 − η).\n\n(70)\n\nWe see that as η = NH original features is preserved.\n\nM → 1, the asymptotic error converges to zero since all information in the\n\nD.5.2 SIMPLIFIED ISOTROPIC FEATURE NOISE\n\nWe can simplify the above expressions somewhat in the case where σ2 = 1 and Σε = σ2 case, the order parameters become\n\nε I. In this\n\nηv = η(1 + ˆv + σ2\n\nε ˆq)−1 =⇒ v =\n\n1 1 + ˆv + σ2\n\nε ˆq\n\n(cid:20)\n\n=⇒ ηˆv = ˆq tr\n\nI +\n\nˆq 1 + ˆv + σ2\n\nε ˆq\n\nΣM\n\n(cid:21)−1\n\nΣM = ˆq(1 + ˆv + σ2\n\nε ˆq) tr[(1 + ˆv + σ2\n\nε ˆq)I + ˆqΣM ]−1ΣM\n\nq = tr (cid:2)(1 + ˆv + σ2\n\nε ˆq)I + ˆqΣM\n\n(cid:3)−1\n\nΣM +\n\nησ2 ε\n1 + ˆv + σ2\n\nε ˆq\n\n.\n\n(71)\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nLetting G = [(1 + ˆv + σ2\n\nε ˆq)I + ˆqΣM ]−1, the source derivatives have the form\n\n∂J ˆq = 1 −\n\nα\n\n(λ + q)2 ∂J q\n\n(cid:20)\n\n= 1 +\n\nα (λ + q)2 η∂J ˆv = ((1 + ˆv + 2σ2 − ˆq(1 + ˆv + σ2 = (∂J ˆq)(1 + ˆv + σ2\n\ntrG2Σ[∂J ˆvI + ∂J ˆqΣM ] +\n\nησ2 ε\n\n(1 + ˆv + σ2\n\nε ˆq)2 (∂J ˆv + σ2\n\nε ∂J ˆq)\n\nε ˆq)∂J ˆq + ˆq∂J ˆv)trGΣM ε ˆq)trG2ΣM [(∂J ˆv + σ2\n\nε ˆq)2trG2Σ + (∂J ˆv + σ2\n\nε ∂J ˆq)I + ∂J ˆqΣM ] ε ∂J ˆq)ˆq2trG2Σ2.\n\n(cid:21)\n\n,\n\n(72)\n\n(73)\n\n(74)\n\nThis is a 2 × 2 linear system\n\n(cid:34)\n\n1 − α\n\n(λ+q)2 [trG2Σ2 +\n\n−(1 + ˆv + σ2\n\nε ˆq)2trG2ΣM − σ2\n\n(1+ˆv+σ2\n\nησ4\n\nε\n\nε ˆq)2 ] ε ˆq2trG2Σ2\n\nM\n\n− α\n\n(λ+q)2 [trG2Σ +\n\nησ2\n\nε\n\n(1+ˆv+σ2\n\nε ˆq)2 ]\n\nη − ˆq2trG2Σ2 s\n\n(cid:21)\n\n(cid:35) (cid:20)∂J ˆq ∂J ˆv\n\n(cid:21)\n\n(cid:20)1 0\n\n.\n\n=\n\nFor each α, we can solve for ∂J ˆq and ∂J ˆv to get the final generalization error with the formula\n\nEg = ∂J\n\n1 M\n\nw∗ (cid:2)(1 + ˆv + σ2\n\nε ˆq)ˆqΣG(cid:3) w∗\n\n=\n\n1 M\n\nw∗[∂J (ˆq + ˆqˆv + σ2\n\nε ˆq2)ΣG − (1 + ˆv + σ2\n\nε ˆq)ˆqΣG2(∂ˆvI + σ2\n\nε ∂ ˆqI + ∂ ˆqΣM )]w∗. (75)\n\nAn example of these solutions can be found in Figure 16, where we show good agreement between theory and experiment.\n\nE RESNET ON CIFAR EXPERIMENTS\n\n25\n\nPublished as a conference paper at ICLR 2023\n\n(a) Generalization MSE\n\n(b) Accuracy\n\n(c) var/Eg\n\n(d) var/Eg\n\nFigure 8: A Wide ResNet Zagoruyko & Komodakis (2017) trained on a superclassed CIFAR task comparing animate vs inanimate objects. Each learning curve is averaged over 5 different samples of the train set, yielding the means and error bars shown in the figures. a) Generalization error Eg. The dashed lines are the error of a 20-fold ensemble over different values of α. Across all P , lazy networks attain worse generalization error. As with the MLP task, the best performing networks are ensembles of rich networks. b) The accuracy also has the same trend: richer networks perform better and ensembling lazy networks helps them more. c) Once P is large enough, lazier networks tend to benefit more from ensembling. d) Very lazy networks transition to variance limited behavior earlier. For ResNets on this task, we see that rich, feature learning networks eventually begin reducing their variance on this task. Further details of the experiment are given in section A.1.\n\nF ADDITIONAL EXPERIMENTS\n\n26\n\n103104P0.50.40.30.2EgEg for ResNet on CIFAR=0.01=0.02=0.05=0.10=0.22=0.46=1.00103104P0.820.840.860.880.900.920.940.96AccuracyAccuracy for ResNet on CIFAR=0.01=0.02=0.05=0.10=0.22=0.46=1.001031021011000.150.200.250.300.350.40var/Egvar/Eg for ResNet on CIFARP=4096P=8192P=16384P=32768103104P0.050.100.150.200.250.300.350.400.450.50var/Egvar/Eg for ResNet on CIFAR=0.001=0.002=0.005=0.010=0.022=0.046=0.100=0.215=0.464=1.000Published as a conference paper at ICLR 2023\n\n(a) Eg for eNTK0 k = 2\n\n(b) Eg for eNTK0 k = 3\n\n(c) Eg for eNTK0 k = 4\n\n(d) Eg for NN k = 2\n\n(e) Eg for NN k = 3\n\n(f) Eg for NN k = 4\n\nFigure 9: Phase plots of log10 Eg for initial eNTKs (top) and neural networks (bottom). The large α behavior of the neural network generalization matches the generalization of the corresponding eNTK0. As a sanity check, the eNTK0 generalization error is independent of re-scaling the network initialization because of the homogeneity of the ReLU network output.\n\n27\n\n2.02.53.03.5logP1.00.50.00.51.0logEg eNTK0, L=3, k=2, N=10003.53.02.52.01.51.00.52.02.53.03.5logP1.00.50.00.51.0logEg eNTK0, L=3, k=3, N=10003.02.52.01.51.00.50.02.02.53.03.5logP1.00.50.00.51.0logEg eNTK0, L=3, k=4, N=10002.01.51.00.50.02.02.53.03.5logP1.00.50.00.51.0logEg NN, L=3, k=2, N=10003.53.02.52.01.51.00.52.02.53.03.5logP1.00.50.00.51.0logEg NN, L=3, k=3, N=10003.02.52.01.51.00.50.02.02.53.03.5logP1.00.50.00.51.0logEg NN, L=3, k=4, N=10002.01.51.00.50.0Published as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\n(f)\n\n(g)\n\n(h)\n\n(i)\n\nFigure 10: A fine-grained view of the generalization error across different datasets and ensembles. Solid curves are depth 3 neural networks, dashed curves are the infinite width NTK (which only has variance over datasets). Each color is a set of networks trained on the same dataset but different initializations. Different colors correspond to different datasets indexed by d ∈ {0, . . . , 9}.\n\n28\n\n101102103104P107106105104103102101100Egk=1, N=1000, =20NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P105104103102101100Egk=2, N=1000, =20NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P102101100Egk=3, N=1000, =20NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P107106105104103102101100Egk=1, N=1000, =1.0NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P105104103102101100Egk=2, N=1000, =1.0NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P102101100Egk=3, N=1000, =1.0NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P107106105104103102101100Egk=1, N=1000, =0.1NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P105104103102101100Egk=2, N=1000, =0.1NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9101102103104P103102101100Egk=3, N=1000, =0.1NTKd=0d=1d=2d=3d=4d=5d=6d=7d=8d=9Published as a conference paper at ICLR 2023\n\n(a) Eg for k = 1, P = 600\n\n(b) Eg for k = 1, P = 10000\n\n(c) Eg for k = 3, P = 600\n\n(d) Eg for k = 3, P = 10000\n\nFigure 11: Phase plots of log10 Eg for neural networks in the N -α plane. We plot these at different train set sizes P and different tasks k. The colors are fixed to match across networks trained on the same task.\n\nFigure 12: Empirical plot of of the scaling of the variance of the eNTK0 with N with variance taken over 10 initializations and averaged 10 different datasets.\n\n29\n\n1.501.752.002.252.502.75logN1.00.50.00.51.0logEg NN, L=3, k=1, P=6005432101.501.752.002.252.502.75logN1.00.50.00.51.0logEg NN, L=3, k=1, P=100005432101.501.752.002.252.502.75logN1.00.50.00.51.0logEg NN, L=3, k=3, P=6003.02.52.01.51.00.50.01.501.752.002.252.502.75logN1.00.50.00.51.0logEg NN, L=3, k=3, P=100003.02.52.01.51.00.50.0102103N100101eNTK0 variance over inits, D=10, L=3, P=1801/NVar eNTK0Published as a conference paper at ICLR 2023\n\n(a) Eg for L = 2\n\n(b) Eg for L = 3\n\n(c) Eg for L = 4\n\n(d) Ensembled Eg for L = 2\n\n(e) Ensembled Eg for L = 3\n\n(f) Ensembled Eg for L = 4\n\n(g) var/Eg for L = 2\n\n(h) var/Eg for L = 3\n\n(i) var/Eg for L = 4\n\nFigure 13: Sweep over depth L = {2, 3, 4}. Deeper networks in the rich regime can more easily outperform the infinite width network for a larger range of P . Also, for larger L it is easier to deviate from the lazy regime at a given α. By contrast, on this task the shallower NTK∞ outperforms deeper NTK∞s. As before, ensembled lazy networks approach NTK∞ and the variance rises with P .\n\n30\n\n101102103104P105104103102101100EgL=2, k=2, N=1000, D=10NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgL=3, k=2, N=1000, D=10NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgL=4, k=2, N=1000, D=10NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgEnsembled Error L=2, k=2, N=1000, D=10NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgEnsembled Error L=3, k=2, N=1000, D=10NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgEnsembled Error L=4, k=2, N=1000, D=10NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P0.00.20.40.60.81.0var/Egvar/Eg L=2, k=2, N=1000, D=10=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P0.00.20.40.60.81.0var/Egvar/Eg L=3, k=2, N=1000, D=10=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P0.00.20.40.60.81.0var/Egvar/Eg L=4, k=2, N=1000, D=10=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=Published as a conference paper at ICLR 2023\n\n(a) Eg for L = 2\n\n(b) Eg for L = 3\n\n(c) Eg for L = 4\n\n(d) Ensembled Eg for L = 2\n\n(e) Ensembled Eg for L = 3\n\n(f) Ensembled Eg for L = 4\n\n(g) var/Eg for L = 2\n\n(h) var/Eg for L = 3\n\n(i) var/Eg for L = 4\n\nFigure 14: Sweep over input dimension D = {5, 25, 50}. At larger input dimensions rich networks can more easily outperform NTK∞. This is a consequence of the task depending on the low-dimensional projection β · x.\n\n(a) Eg for centered vs uncentered\n\n(b) Effect of ensembling\n\n(c) Eg uncentered color plot\n\nFigure 15: a) Eg for the centered predictor ̃fθ(x) − ̃fθ0 (x) (solid) compared to the generalization of the uncentered predictor ̃fθ(x) (dashed). At small α, the difference is negligible, while at large α the uncentered predictor does worse and does not approach eNTK0 . The worse generalization can be understood as ̃fθ0(x) effectively adding an initialization-dependent noise to the target y. b) The effect of ensembling becomes less beneficial for uncentered lazy networks. c) Color plot of Eg. The lazy regime is different from the eNTK0 generalization (c.f. Figure 9).\n\n31\n\n101102103104P105104103102101100EgL=3, k=2, N=1000, D=5NTK=0.1=0.2=0.5=1.0=2.0=101102103104P105104103102101100EgL=3, k=2, N=1000, D=25NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgL=3, k=2, N=1000, D=50NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgEnsembled Error L=3, k=2, N=1000, D=5NTK=0.1=0.2=0.5=1.0=2.0=101102103104P105104103102101100EgEnsembled Error L=3, k=2, N=1000, D=25NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100EgEnsembled Error L=3, k=2, N=1000, D=50NTK=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P0.00.20.40.60.81.0var/Egvar/Eg L=3, k=2, N=1000, D=5=0.1=0.2=0.5=1.0=2.0=101102103104P0.00.20.40.60.81.0var/Egvar/Eg L=3, k=2, N=1000, D=25=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P0.00.20.40.60.81.0var/Egvar/Eg L=3, k=2, N=1000, D=50=0.1=0.2=0.5=1.0=2.0=5.0=10.0=20.0=101102103104P105104103102101100101EgUncentered vs Centered NN Eg, k=2, L=3, N=1000NTKNN centered, =0.1NN uncentered, =0.1NN centered, =0.2NN uncentered, =0.2NN centered, =0.5NN uncentered, =0.5NN centered, =1.0NN uncentered, =1.0NN centered, =2.0NN uncentered, =2.0NN centered, =5.0NN uncentered, =5.0NN centered, =10.0NN uncentered, =10.0NN centered, =20.0NN uncentered, =20.0eNTK0 (=)101102103104P105104103102101100101EgUncentered Eg and Eensg L=3, k=2, N=1000NN, =0.1NN ensembed, =0.1NN, =0.2NN ensembed, =0.2NN, =0.5NN ensembed, =0.5NN, =1.0NN ensembed, =1.0NN, =2.0NN ensembed, =2.0NN, =5.0NN ensembed, =5.0NN, =10.0NN ensembed, =10.0NN, =20.0NN ensembed, =20.0NTK2.02.53.03.54.0logP1.00.50.00.51.0logUncentered NN Eg, k=2, L=3, N=10003.53.02.52.01.51.00.50.0Published as a conference paper at ICLR 2023\n\n(a) Gaussian A, vary NH\n\n(b) NH = 750, vary σ2\n\nε\n\n(c) NH = 1000, Vary k\n\n(d) NH = 1000, Vary λ\n\nFigure 16: Verification of Gaussian A model. Solid lines are theory and dots are experiments. (a) The effect of changing the student’s RKHS dimension NH. Double descent overfitting peaks occur at P = NH (b) The effect of additive noise in the student features Σε = σ2 ε ΣM . (c) Learning curves for fitting the k-th eigenfunction. All mode errors exhibit a double descent peak at P = NH regardless of the task. (d) Regularization can prevent the overfitting peak.\n\n(a) Eg for mixed mode task\n\n(b) Var ˆy for mixed mode task\n\nFigure 17: Width 500 depth 3 MLP learning a D = 25 mixture of a linear and cubic polynomials. a) Generalization error of NTK∞ (solid black) and MLP (solid colored lines) on mixed mode task. The dashed lines are convex combinations of the generalization curves for the pure mode k = 1, k = 3 tasks. For the NTK∞ , the generalization curves sum to give the mixed mode curve, as observed in Bordelon et al. (2020). We see that this also holds for the eNTK0 for sufficiently lazy networks, as predicted by the simple renadom feature model considred in section 4 of this paper. b) The variance curves for the same task. Again, for sufficiently lazy networks the variance is a sum of the variances of the individual pure mode tasks, as predicted by our random feature model.\n\n32\n\n101102103104P102101100101EgN=100N=200N=500N=1000N=2000101102103104P103102Eg2=1022=1012=1002=101101102103104P102101100Egk=1k=20k=100k=600101102103104P101100Eg=104=103=102=101101102103104P102101100EgMultimodal k={1,3},L=3,D=25,N=500NTK=0.1=0.5=1.0=10.0=20.0=101102103104P103102101Var(y)Multimodal k={1,3},L=3,D=25,N=500=0.1=0.5=1.0=10.0=20.0=Published as a conference paper at ICLR 2023\n\n(a) k = 3, N = 177, α = 0.1\n\n(b) k = 3, N = 177, α = 20\n\n(c) k = 3, N = 1000, α = 0.1\n\n(d) k = 3, N = 1000, α = 20\n\nFigure 18: Comparison of the neural network predictor (x-axis) to eNTK0 , eNTKf (blue, orange respectively, y-axis) across several training dataset sizes. a) N = 177, α = 0.1, b) N = 177, α = 20, c) N = 1000, α = 0.1 d) N = 1000, α = 20. All networks are depth 3. Note how in the rich regime there is a much stronger distinction between the eNTK0 and the neural network. In all regimes, eNTKf matches the NN output.\n\n33\n\n20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 3, N = 177, = 0.1NN predictoreNTK predictors20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 3, N = 177, = 20NN predictoreNTK predictors20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 3, N = 1000, = 0.1NN predictoreNTK predictors20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 3, N = 1000, = 20NN predictoreNTK predictorsPublished as a conference paper at ICLR 2023\n\n(a) k = 1, N = 177, α = 0.1\n\n(b) k = 1, N = 177, α = 20\n\n(c) k = 1, N = 1000, α = 0.1\n\n(d) k = 1, N = 1000, α = 20\n\nFigure 19: The same as figure 18 but for fitting a linear k = 1 mode. Because the task is simpler, it doesn’t require as large of an α to enter the lazy regime.\n\n34\n\n20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 1, N = 177, = 0.1NN predictoreNTK predictors20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 1, N = 177, = 20NN predictoreNTK predictors20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 1, N = 1000, = 0.1NN predictoreNTK predictors20221012P = 100eNTK0eNTKf20221012P = 18020221012P = 40020221012P = 60020221012P = 100020221012P = 180020221012P = 400020221012P = 600020221012P = 10000L = 3, k = 1, N = 1000, = 20NN predictoreNTK predictors",
    "reference": "# Summary Of The Paper\n\n*Please note that this is an emergency review, so I was not able to go as much into details of the paper as I would have liked to*\n\nA lot of recent work on deep learning theory has focused on understanding the\ndynamics and generalization of neural networks with infinitely wide hidden\nlayer. This is of course an idealisation, and recent work has sought to\nunderstand when finite-width effects become relevant as the size of the training\nset increases.\n\nThis work performs a careful study of two-layer ReLU networks trained on a\nsimple polynomial regression task to investigate precisely when finite-size\neffects become important for the performance of the network. The authors vary\nthree key parameters: the width of the network, $N$, the size of the training\nset $P$, and the *scale* of the network $\\alpha$, which was introduced by Chizat\net al. (2019) as a mean to interpolate between the \"lazy regime\" (large\n$\\alpha$) and the \"feature learning regime\" (small $\\alpha$).\n\nThe authors find through experiments that for large data sets, the\ninfinite-width limit always performs best. Feature learning does improve over\nthe infinite-width limit at intermediate data set size, while lazy learning at\nintermediate sizes does not. They relate these behaviours to the fluctuations due to \nvarious sources of noise in the system (for example in the initialisation of the weights),\nand show how ensembling can mitigate these fluctuations. The authors propose various scaling laws to explain\ntheir findings, which they obtain via a toy model which qualitatively reproduces\nthe effects (cf. Fig 6).\n\n# Strength And Weaknesses\n\nThe paper tackles an important topic: given the ubiquity of papers studying\nneural nets in the infinite-width limit, it is important to understand the\nlimitations of this regime in practice. While previous work (which the authors discuss)\nhad already identified the variance-limited regime, or discussed the role of ensembling,\nhere the authors put everything together in a single case-study.\n\nThe paper is also well-written: it clearly states the setup and the results, and\nit connects nicely with the previous literature.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nSee above.\n\n# Summary Of The Review\n\nTo summarise, this paper provides a careful investigation of a problem which is\nof interest to the theory of neural networks community, and should therefore be\naccepted at ICLR.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nFEDEXP: SPEEDING UP FEDERATED AVERAGING VIA EXTRAPOLATION\n\nDivyansh Jhunjhunwala1, Shiqiang Wang2, Gauri Joshi1 1Carnegie Mellon University, 2IBM Research {djhunjhu, gaurij}@andrew.cmu.edu, wangshiq@us.ibm.com\n\nABSTRACT\n\nFederated Averaging (FedAvg) remains the most popular algorithm for Federated Learning (FL) optimization due to its simple implementation, stateless nature, and privacy guarantees combined with secure aggregation. Recent work has sought to generalize the vanilla averaging in FedAvg to a generalized gradient descent step by treating client updates as pseudo-gradients and using a server step size. While the use of a server step size has been shown to provide performance improvement theoretically, the practical benefit of the server step size has not been seen in most existing works. In this work, we present FedExP, a method to adaptively determine the server step size in FL based on dynamically varying pseudo-gradients throughout the FL process. We begin by considering the overparameterized convex regime, where we reveal an interesting similarity between FedAvg and the Projection Onto Convex Sets (POCS) algorithm. We then show how FedExP can be motivated as a novel extension to the extrapolation mechanism that is used to speed up POCS. Our theoretical analysis later also discusses the implications of FedExP in underparameterized and non-convex settings. Experimental results show that FedExP consistently converges faster than FedAvg and competing baselines on a range of realistic FL datasets.\n\n1\n\nINTRODUCTION\n\nFederated Learning (FL) has emerged as a key distributed learning paradigm in which a central server orchestrates the training of a machine learning model across a network of devices. FL is based on the fundamental premise that data never leaves a clients device, as clients only communicate model updates with the server. Federated Averaging or FedAvg, first introduced by McMahan et al. (2017), remains the most popular algorithm in this setting due to the simplicity of its implementation, stateless nature (i.e., clients do not maintain local parameters during training) and the ability to incorporate privacy-preserving protocols such as secure aggregation (Bonawitz et al., 2016; Kadhe et al., 2020).\n\nSlowdown Due to Heterogeneity. One of the most persistent problems in FedAvg is the slowdown in model convergence due to data heterogeneity across clients. Clients usually perform multiple steps of gradient descent on their heterogeneous objectives before communicating with the server in FedAvg, which leads to what is colloquially known as client drift error (Karimireddy et al., 2019). The effect of heterogeneity is further exacerbated by the constraint that only a fraction of the total number of clients may be available for training in every round (Kairouz et al., 2021). Various techniques have been proposed to combat this slowdown, among the most popular being variance reduction techniques such as Karimireddy et al. (2019); Mishchenko et al. (2022); Mitra et al. (2021), but they either lead to clients becoming stateful, add extra computation or communication requirements or have privacy limitations.\n\nServer Step Size. Recent work has sought to deal with this slowdown by using two separate step sizes in FedAvg – a client step size used by the clients to minimize their local objectives and a server step size used by the server to update the global model by treating client updates as pseudo-gradients (Karimireddy et al., 2019; Reddi et al., 2021). To achieve the fastest convergence (cid:0)1/τ √T (cid:1) and the server step size as rate, these works propose keeping the client step size as (cid:0)√τ M (cid:1), where T is the number of communication rounds, τ is the number of local steps and O\nM is the number of clients. Using a small client step size mitigates client drift, and a large server\n\nO\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nstep size prevents global slowdown. While this idea may be asymptotically optimal, it is not always effective in practical non-asymptotic and communication-limited settings (Charles & Koneˇcn`y, 2020). In practice, a small client step size severely slows down convergence in the initial rounds and cannot be fully compensated for by a large server step size (see Figure 1). Also, if local objectives differ significantly, then it may be beneficial to use smaller values of the server step size (Malinovsky et al., 2022).\n\nTherefore, we seek to answer the following question: For a moderate client step size, can we adapt the server step size according to the local progress made by the clients and the heterogeneity of their objectives? In general, it is challenging to answer this question because it is difficult to obtain knowledge of the heterogeneity between the local objectives and appropriately use it to adapt the server step size.\n\nOur Contributions. In this paper, we take a novel approach to address the question posed above. We begin by considering the case where the models are overparameterized, i.e., the number of model parameters is larger than the total number of data points across all clients. This is often true for modern deep neural network models (Zhang et al., 2017; Jacot et al., 2018) and the small datasets collected by edge clients in the FL setting. In this overparameterized regime, the global minimizer becomes a common minimizer for all local objectives, even though they may be arbitrarily heterogeneous. Using this fact, we obtain a novel connection between FedAvg and the Projection Onto Convex Sets (POCS) algorithm, which is used to find a point in the intersection of some convex sets.\n\nFigure 1: Test accuracy (%) achieved by different server and client step sizes on EMNIST dataset (Cohen et al., 2017) after 50 rounds (details of experimental setup are in Section 6 and Appendix D).\n\nBased on this connection, we find an interesting analogy between the server step size and the extrapolation parameter that is used to speed up POCS (Pierra, 1984). We propose new extensions to the extrapolated POCS algorithm to support inexact and noisy projections as in FedAvg. In particular, we derive a time-varying bound on the progress made by clients towards the global minimum and show how this bound can be used to adaptively estimate a good server step size at each round. The result is our proposed algorithm FedExP, which is a method to adaptively determine the server step size in each round of FL based on the pseudo-gradients in that round.\n\nAlthough motivated by the overparameterized regime, our proposed FedExP algorithm performs well (both theoretically and empirically) in the general case, where the model can be either overparameterized or underparameterized. For this general case, we derive the convergence upper bounds for both convex and non-convex objectives. Some highlights of our work are as follows.\n\n• We reveal a novel connection between FedAvg and the POCS algorithm for finding a point in the\n\nintersection of convex sets.\n\n• The proposed FedExP algorithm is simple to implement with virtually no additional communication, computation, or storage required at clients or the server. It is well suited for both cross-device and cross-silo FL, and is compatible with partial client participation.\n\n• Experimental results show that FedExP converges 1.4–2\n\nfaster than FedAvg and most compet-\n\ning baselines on standard FL tasks.\n\n×\n\nRelated Work. Popular algorithms for adaptively tuning the step size when training neural networks include Adagrad (Duchi et al., 2011) and its variants RMSProp (Tieleman et al., 2012) and Adadelta (Zeiler, 2012). These algorithms consider the notion of coordinate-wise adaptivity and adapt the step size separately for each dimension of the parameter vector based on the magnitude of the accumulated gradients. While these algorithms can be extended to the federated setting using the concept of pseudo-gradients as done by Reddi et al. (2021), these extensions are agnostic to inherent data heterogeneity across clients, which is central to FL. On the contrary, FedExP is explicitly designed for FL settings and uses a client-centric notion of adaptivity that utilizes the heterogeneity of client updates in each round. The work closest to us is Johnson et al. (2020), which proposes a method to adapt the step size for large-batch training by estimating the gradient diversity (Yin et al., 2018) of a minibatch. This result has been improved in a recent work by Horváth et al. (2022). However, both Johnson et al. (2020); Horváth et al. (2022) focus on the centralized setting. In\n\n2\n\n0.010.030.10.31Client Step Size (ηl)0.10.31310Server Step Size (ηg)5.88.242559.11646587155466073776.2365760506106.46.15.74.8204060Published as a conference paper at ICLR 2023\n\nFedExP, we use a similar concept, but within a federated environment which comes with a stronger theoretical motivation, since client data are inherently diverse in this case. We defer a more detailed discussion of other adaptive step size methods and related work to Appendix A.\n\n2 PROBLEM FORMULATION AND PRELIMINARIES\n\nAs in most standard federated learning frameworks, we consider the problem of optimizing the model parameters w\n\nRd to minimize the global objective function F (w) defined as follows:\n\n∈\n\nmin w∈Rd\n\nF (w) :=\n\n1 M\n\nM (cid:88)\n\ni=1\n\nFi(w),\n\n(1)\n\n(cid:80)\n\n|Di|\n\nDi at the the i-th client. Here, l(\n\nwhere Fi(w) := 1 set empirical local data distribution Without loss of generality, we assume that all the M client objectives are given equal weight in the global objective function defined in (1). Our algorithm and analysis can be directly extended to the case where client objectives are unequally weighted, e.g., proportional to local dataset sizes\n\nδi∈Di l(w, δi) is the empirical risk objective computed on the local data ) is a loss function and δi represents a data sample from the ·\nDi. The total number of clients in the FL system is denoted by M .\n\n, ·\n\n. |Di|\n\nFedAvg. We focus on solving (1) using FedAvg (McMahan et al., 2017; Kairouz et al., 2021). At round t of FedAvg, the server sends the current global model w(t) to all clients. Upon receiving the global model, clients perform τ steps of local stochastic gradient descent (SGD) to compute their updates\n\ni=1 for round t as follows.\n\n∆(t)\n\nM\n\n{\n\ni }\n\nPerform Local SGD: w(t,k+1) Compute Local Difference: ∆(t)\n\ni\n\n= w(t,k)\n\ni\n\ni = w(t)\n\nFi(w(t,k)\n\ni\n\nηl∇ w(t,τ )\n\ni\n\n−\n\n−\n\n, ξ(t,k))\n\nk\n\n∀\n\n0, 1, . . . , τ\n\n∈ {\n\n1\n\n}\n\n−\n\n(2)\n\n(3)\n\nwhere w(t,0) stochastic gradient computed on the minibatch ξ(t,k)\n\n= w(t) for all i\n\n∈\n\ni\n\n[M ], ηl is the client step size and\n\nsampled randomly from\n\nFi(w(t,k)\n\ni\n\n∇\n\ni\n\n, ξ(t,k)) represents a Di.\n\nServer Optimization in FedAvg. In vanilla FedAvg (McMahan et al., 2017), the global model i=1 w(t,τ ) would simply be updated as the average of the client local models, that is, w(t+1) = 1 .\nTo improve over this, recent work (Reddi et al., 2021; Hsu et al., 2019) has focused on optimizing the server aggregation process by treating the client updates ∆(t) as “pseudo-gradients” and multiplying by a server step size when aggregating them as follows.\n\n(cid:80)M\n\nM\n\ni\n\ni\n\nGeneralized FedAvg Global Update:\n\nw(t+1) = w(t)\n\nηg ̄∆(t)\n\n−\n\n(4)\n\nwhere ̄∆(t) = 1 Note that setting ηg = 1 recovers the vanilla FedAvg update.\n\ni=1 ∆(t)\n\nM\n\ni\n\n(cid:80)M\n\nis the aggregated client update in round t and ηg acts as server step size.\n\nWhile the importance of the server step size has been theoretically well established in these works, we find that its practical relevance has not been explored. In this work, we take a step towards bridging this gap between theory and practice by adaptively tuning the value of ηg that we use in every round.\n\n3 PROPOSED ALGORITHM: FEDEXP\n\nBefore discussing our proposed algorithm, we first highlight a useful and novel connection between FedAvg and the POCS algorithm used to find a point in the intersection of some convex sets.\n\n3.1 MOTIVATION FOR EXTRAPOLATION\n\nConnection Between FedAvg and POCS in the Overparameterized Convex Regime. Consider i=1 are convex. In this case, we know the case where the local objectives of the clients ∗\nthat the set of minimizers of Fi(w) given by is also a convex [M ]. Now let us assume that we are in the overparameterized regime where d is set for all i sufficiently larger than the total number of data points across clients. In this regime, the model can\n\narg min Fi(w)\n\n} w : w\n\nFi(w)\n\n{ i =\n\n∈\n\n∈\n\nS\n\nM\n\n}\n\n{\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nfit all the training data at clients simultaneously and hence be a minimizer for all local objectives. Thus we assume that the global minimum satisfies w∗ [M ]. Our original problem in (1) can then be reformulated as trying to find a point in the intersection of convex sets i=1 since w∗ [M ]. One of the most popular algorithms to do so is the Projection Onto Convex Sets (POCS) algorithm (Gurin et al., 1967). In POCS, at every iteration the current model is updated as follows1.\n\n∈ S\n\n∈ S\n\ni ∀\n\ni ∀\n\ni }\n\n{S\n\ni ,\n\ni ,\n\n∈\n\n∈\n\nM\n\n∗\n\n∗\n\n∗\n\nGeneralized POCS update:\n\nw(t+1)\n\nPOCS = w(t)\n\nPOCS −\n\nλ\n\n(cid:16) 1\n\nM\n\n(cid:80)M\n\ni=1 Pi(w(t)\n\nPOCS)\n\nw(t)\n\nPOCS\n\n−\n\n(cid:17)\n\n(5)\n\nwhere Pi(w(t) (Combettes, 1997).\n\nPOCS) is a projection of w(t)\n\nPOCS on the set\n\ni and λ is known as the relaxation coefficient\n\n∗\n\nS\n\nExtrapolation in POCS. Combettes (1997) notes that POCS has primarily been used with λ = 1, with studies failing to demonstrate a systematic benefit of λ < 1 or λ > 1 (Mandel, 1984). This prompts Combettes (1997) to study an adaptive method of setting λ, first introduced by Pierra (1984) as follows:\n\nλ(t) =\n\n(cid:80)M\n\ni=1\n\n(cid:13) (cid:13)Pi(w(t))\n\n− i=1 Pi(w(t))\n\n(cid:80)M\n\nM\n\n(cid:13) (cid:13) (cid:13)\n\n1 M\n\nw(t)(cid:13) 2\n(cid:13)\n\nw(t)\n\n−\n\n2 .\n\n(cid:13) (cid:13) (cid:13)\n\nPierra (1984) refer to the POCS algorithm with this adaptive λ(t) as Extrapolated Parallel Projection Method (EPPM). This is referred to as extrapolation since we always have λ(t) 1 by Jensen’s inequality. The intuition behind EPPM lies in showing that the update with the proposed λ(t) w∗(cid:13) always satisfies (cid:13) 2, thereby achieving asymptotic convergence. (cid:13) Experimental results in Pierra (1984) and Combettes (1997) show that EPPM can give an order-wise speedup over POCS, motivating us to study this algorithm in the FL context.\n\n(cid:13)w(t+1)\n\nPOCS −\n\n(cid:13) (cid:13)w(t)\n\nPOCS −\n\nw∗(cid:13) 2\n(cid:13)\n\n≥\n\n<\n\n3.2\n\nINCORPORATING EXTRAPOLATION IN FL\n\nNote that to implement POCS we do not need to explicitly know the sets i=1; we only need to know how to compute a projection on these sets. From this point of view, we see that FedAvg proceeds similarly to POCS. In each round, clients receive w(t) from the server and run multiple SGD steps to compute an “approximate projection” w(t,τ ) i . These approximate projections are then aggregated at the server to update the global model. In this case, the relaxation coefficient λ plays exactly the same role as the server step size ηg in FedAvg.\n\nof w(t) on their solution sets\n\ni }\n\n{S\n\nS\n\n∗\n\ni\n\n∗\n\nM\n\nInspired by this observation and the idea of extrapolation in POCS, we seek to understand if a similar idea can be applied to tune the server step size ηg in FedAvg. Note that the EPPM algorithm makes use of exact projections to prove convergence which is not available to us in FL settings. This is further complicated by the fact that the client updates are noisy due to the stochasticity in sampling minibatches. We find that in order to use an EPPM-like step size the use of exact projections can be relaxed to the following condition, which bounds the distance of the local models from the global minimum as follows.\n\n(cid:13) (cid:13)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:80)M\n\n(cid:13) (cid:13)w(t)\n\nw∗(cid:13) 2\n(cid:13)\n\ni\n\ni\n\n}\n\nM\n\n−\n\ni=1\n\n1 M\n\nw(t,τ ) {\n\nApproximate projection condition in FL:\n\n(cid:13)w(t,τ ) i=1 are the global and local client models, respectively, at round t and w∗ is where w(t) and a global minimum. Intuitively, this condition suggests that after the local updates, the local models are closer to the optimum w∗ on average as compared to model w(t) at the beginning of that round. We first show that this condition (6) holds in the overparameterized convex regime under some conditions. The full proofs for lemmas and theorems in this paper are included in Appendix C. [M ] and let w∗ be a common minimizer Lemma 1. Let Fi(w) be convex and L-smooth for all i of all Fi(w). Assuming clients run full-batch gradient descent to minimize their local objectives with ηl ≤\n\n1/L, then (6) holds for all t and τ\n\n(6)\n\n1.\n\n−\n\n≥\n\n≤\n\n∈\n\nIn the case with stochastic gradient noise or when the model is underparameterized, although (6) may not hold in general, we expect it to be satisfied at least during the initial phase of training when (cid:13) (cid:13)w(t)\n\nis large and clients make common progress towards a minimum.\n\nw∗(cid:13) 2\n(cid:13)\n\n−\n\n1We refer here to a parallel implementation of POCS. This is also known as Parallel Projection Method (PPM)\n\nand Simultaneous Iterative Reconstruction Technique (SIRT) in some literature (Combettes, 1997).\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 Proposed Algorithm: FedExP\n\n1: Input: w(0), number of rounds T , local iteration steps τ , parameters ηl, ε 2: For t = 0, . . . , T 1 communication rounds do: 3: 4: 5:\n\nGlobal server does: Send w(t) to all clients Clients i\n\n[M ] in parallel do:\n\n−\n\n6: 7:\n\n8:\n\n9: 10:\n\n11:\n\n12:\n\nw(t,0)\n\n∈ Set w(t,0) i ← For k = 0, . . . , τ −\nUpdate w(t,k+1) w(t) i ← Global server does: Compute ̄∆(t)\n\nSend ∆(t)\n\ni\n\n1 local iterations do:\n\nw(t,k)\n\ni\n\nw(t,τ )\n\ni\n\n←\n\n−\n\nηl∇\n\n−\n\nto the server\n\n1 M\n\n(cid:80)M\n\ni=1 ∆(t)\n\ni\n\nand η(t)\n\ng\n\n← Update global model with w(t+1)\n\nFi(w(t,k)\n\ni\n\n, ξ(t,k)\n\ni\n\n)\n\n← w(t)\n\nmax η(t) g ̄∆(t)\n\n←\n\n−\n\n(cid:110)\n\n1, (cid:80)M\n\ni=1\n\n(cid:13) (cid:13)∆(t)\n\ni\n\n2(cid:46)\n\n(cid:13) (cid:13)\n\n2M\n\n(cid:16)(cid:13) (cid:13) ̄∆(t)(cid:13) 2\n(cid:13)\n\n+ε\n\n(cid:17)(cid:111)\n\nGiven that (6) holds, we now consider the generalized FedAvg update with a server step size η(t) round t. Our goal is to find the value of η(t) w∗(cid:13) w∗(cid:13) 2\n2 (cid:13) (cid:13)\n\nthat minimizes the distance of w(t+1) to w∗: + (η(t)\n\ng )2 (cid:13) = (cid:13) Setting the derivative of the RHS of (7) to zero we have,\n\nw∗, ̄∆(t)(cid:11) .\n\n(cid:13) (cid:13)w(t+1)\n\n(cid:13) ̄∆(t)(cid:13) 2\n(cid:13)\n\n(cid:10)w(t)\n\n(cid:13)w(t)\n\n2η(t)\n\n−\n\n−\n\n−\n\n−\n\ng\n\ng\n\ng\n\nin\n\n(7)\n\n(η(t)\n\ng )opt =\n\nw∗, ̄∆(t)(cid:11)\n\n(cid:10)w(t) −\n(cid:13) ̄∆(t)(cid:13) (cid:13) 2\n(cid:13)\n\n=\n\n(cid:80)M\n\ni=1\n\nwhere the last inequality follows from\n\n= 1 2 [ a\ni ∥\n∥ ∆(t) 1. Thus, we see and (6). Note that depending on the values of i } that (6) acts as a suitable replacement for projection to justify the use of extrapolation in FL settings.\n\na, b ⟩\n⟨\n\nin (3)\n\n≫\n\n−\n\nb\n\nb\n\na\n\n{\n\n(cid:68)\n\n(cid:69)\n\ni\n\ni=1\n\nw∗, ∆(t)\n\nw(t) −\nM (cid:13) (cid:13) ̄∆(t)(cid:13) 2\n(cid:13) 2 + 2\n∥ − ∥ i=0, we may have (η(t)\n\n(cid:80)M 2M (cid:13)\n\n(cid:13) (cid:13) 2\n(cid:13)∆(t) (cid:13) 2 , (cid:13) ̄∆(t)(cid:13) (cid:13) 2], definition of ∆(t) g )opt\n\n≥\n\nM\n\n∥\n\n∥\n\ni\n\n(8)\n\n3.3 PROPOSED ALGORITHM\n\nMotivated by our findings above, we propose the following server step size for the generalized FedAvg update at each round:\n\n(η(t)\n\ng )FedExP = max\n\n1,\n\n(cid:40)\n\n(cid:80)M\n\n(cid:13) (cid:13)∆(t)\n\n(cid:13) 2\n(cid:13)\n\ni i=1 (cid:13) ̄∆(t)(cid:13) (cid:13) 2\n(cid:13)\n\n2M (\n\n+ ε)\n\n(cid:41)\n\n.\n\n(9)\n\ng )opt\n\n(cid:12)(η(t)\n\nWe term our algorithm Federated Extrapolated Averaging or FedExP, in reference to the original EPPM algorithm which inspired this work. Note that our proposed step size satisfies the property (cid:12) that (cid:12) (cid:12) when (6) holds, which can be seen by comparing 1\n(8) and (9). Since (7) depends quadratically on η(t) (η(t) the FedAvg update. In the rest of the paper, we denote (η(t)\n\ng , we can show that in this case (cid:13) −\n2, implying we are at least as close to the optimum as g when the context is clear.\n\ng )FedExP as η(t)\n\ng )FedExP ̄∆(t)\n\n(cid:13) (cid:13)w(t+1)\n\n(cid:13)w(t+1)\n\ng )FedExP\n\nw∗(cid:13) 2\n(cid:13)\n\n(cid:12) (cid:12)(η(t)\n\nw∗(cid:13) (cid:13)\n\ng )opt\n\n(η(t)\n\n−\n\n−\n\n≤\n\n≤\n\n−\n\n−\n\n(cid:12) (cid:12)\n\nImportance of Adding Small Constant to Denominator. In the case where (6) does not hold, using the lower bound established in (8) can cause the proposed step size to blow up. This is especially true towards the end of training where we can have (cid:13) = 0. Thus we propose to add a small positive constant ε to the denominator in (9) to prevent this blow-up. For a large enough ε our algorithm reduces to FedAvg and therefore tuning ε can be a useful tool to interpolate between vanilla averaging and extrapolation. Similar techniques exist in adaptive algorithms such as Adam (Kingma & Ba, 2015) and Adagrad (Duchi et al., 2011) to improve stability.\n\n(cid:13) ̄∆(t)(cid:13) 2\n(cid:13)\n\n(cid:13)∆(t)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n0 but\n\n(cid:13) (cid:13)\n\n≈\n\ni\n\nCompatibility with Partial Client Participation and Secure Aggregation. Note that FedExP can be easily extended to support partial participation of clients by calculating η(t) g using only the updates of participating clients, i.e., the averaging and division in (9) will be only over the clients that participate in the round. Furthermore, since the server only needs to estimate the average of pseudo-gradient norms, η(t)\n\ng can be computed with secure aggregation, similar to computing ̄∆(t).\n\n5\n\n̸ Published as a conference paper at ICLR 2023\n\nConnection with Gradient Diversity. We see that our lower bound on (η(t) g )opt naturally depends on the similarity of the client updates with each other. In the case where τ = 1 and clients run F (w(t))(cid:13) 2\nfull-batch gradient descent, our lower bound (8) reduces to (cid:80)M (cid:13) which is used as a measure of data-heterogeneity in many FL works (Wang et al., 2020; Haddadpour & Mahdavi, 2019). Our lower bound suggests using larger step-sizes as this gradient diversity increases, which can be a useful tool to speed up training in heterogeneous settings. This is an orthogonal approach to existing optimization methods to tackle heterogeneity such as Karimireddy et al. (2020b); Li et al. (2020); Acar et al. (2021), which propose additional regularization terms or adding control variates to the local client objectives to limit the impact of heterogeneity.\n\n2 (cid:14)2M (cid:13) Fi(w(t))(cid:13) (cid:13) (cid:13)\n\ni=1\n\n(cid:13) (cid:13)\n\n∇\n\n∇\n\n4 CONVERGENCE ANALYSIS\n\nOur analysis so far has focused on the overparameterized convex regime to motivate our algorithm. In this section we discuss the convergence of our algorithm in the presence of underparameterization and non-convexity. We would like to emphasize that (6) is not needed to show convergence of FedExP; it is only needed to motivate why FedExP might be beneficial. To show general convergence, we only require that ηl be sufficiently small and the standard assumptions stated below.\n\ng\n\n(cid:104)\n\n(cid:104)\n\n(cid:105)\n\nM\n\nE\n\nξ(t)\n\nξ(t)\n\nξ(t)\n\n= E\n\n(η(t) g )\n\n{ (η(t)\n\n∆(t) i } g ) ̄∆(t)(cid:105)\n\nChallenge in incorporating stochastic noise and partial participation. Our current analysis focuses on the case where clients are computing full-batch gradients in every step with full participation. This is primarily due to the difficulty in decoupling the effect of stochastic and sampling noise on η(t) i=1. To be more specific, if we use ξ(t) to denote the randomness and the pseudo-gradients (cid:2) ̄∆(t)(cid:3) which significantly complicates the at round t, then E proof. This is purely a theoretical limitation. Empirically, our results in Section 6 show that FedExP performs well with both SGD and partial client participation. Assumption 1. (L-smoothness) Local objective Fi(w) is differentiable and L-smooth for all i i.e.,\n\nw, w′ Assumption 2. (Bounded data heterogenenity at optimum) The norm of the client gradients at the global optima w∗ is bounded as follows: 1 Theorem 1. (Fi are convex) Under Assumptions 1,2 and assuming clients compute full-batch gradients with full participation and ηl ≤ 6τ L , the iterates w∗(cid:13) 2\n(cid:13)w(0) (cid:13) −\nηlτ (cid:80)T −1 t=0 η(t) (cid:123)(cid:122) T1:=initialization error\n\n(cid:0)ηlτ σ2 (cid:123)(cid:122) T3:=noise at optimum\n\ngenerated by FedExP satisfy,\n\n− (cid:123)(cid:122) T2:=client drift error\n\nF ( ̄w(T ))\n\n≤ O (cid:124)\n\nFi(w∗)\n\nFi(w′)\n\ni=1 ∥∇\n\n1)Lσ2\n\nFi(w)\n\nl τ (τ\n\n(cid:0)η2\n\nw(t)\n\n[M ],\n\n− ∇\n\n(cid:80)M\n\n2 ∥\n\nRd.\n\n(10)\n\nσ2 ∗.\n\n(cid:32) (cid:13)\n\n∥ ≤\n\nF ∗\n\n∥∇\n\n, ∥\n\nw′\n\nO (cid:124)\n\nO (cid:124)\n\nw\n\n(cid:33)\n\n≤\n\n+\n\n+\n\n−\n\n−\n\nL\n\n∈\n\n∈\n\n∀\n\nM\n\n∥\n\n{\n\n}\n\n(cid:1)\n\n(cid:1)\n\n(cid:125)\n\n(cid:125)\n\n(cid:125)\n\n∗\n\n∗\n\n1\n\n,\n\ng\n\nwhere η(t)\n\ng\n\nis the FedExP server step size at round t and ̄w(T ) =\n\n(cid:80)T −1\n\ng w(t)\n\nt=0 η(t) (cid:80)T −1\n\nt=0 η(t)\n\ng\n\n.\n\nFor the non-convex case, we need the data heterogeneity to be bounded everywhere as follows. Assumption 3. (Bounded global gradient variance) There exists a constant σ2 g > 0 such that the 2\nRd. global gradient variance is bounded as follows. 1 M\n∥ Theorem 2. (Fi are non-convex) Under Assumptions 1, 3 and assuming clients compute full-batch 6τ L , the iterates gradients with full participation and ηl ≤\n\ngenerated by FedExP satisfy,\n\ni=1 ∥∇\n\nFi(w)\n\nF (w)\n\nw(t)\n\n− ∇\n\n(cid:80)M\n\nσ2 g,\n\nw\n\n≤\n\n∈\n\n∀\n\n{\n\n}\n\n1\n\n(cid:13) (cid:13) (cid:13)∇\n\nmin t∈[T ]\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤ O (cid:124)\n\n(cid:33)\n\n(cid:32)\n\nF ∗ F (w(0)) −\nηlτ (cid:80)T −1 t=0 η(t) (cid:123)(cid:122) T1:=initialization error\n\ng\n\n+\n\nO (cid:124)\n\n(cid:125)\n\n(cid:0)η2\n\n1)τ σ2\n\nl L2(τ (cid:123)(cid:122) T2:=client drift error\n\n−\n\ng\n\n+\n\n(cid:1)\n\n(cid:125)\n\n(cid:1)\n\n(cid:0)ηlLτ σ2 (cid:123)(cid:122) T3:= global variance\n\nO (cid:124)\n\n(cid:125)\n\ng\n\n,\n\n(11)\n\nwhere η(t)\n\ng\n\nis the FedExP server step size at round t.\n\nDiscussion.\n\nIn w∗ w(0) −\n(cid:0)(F (w0)\n\ncase,\n\nthe\n\nconvex\n\n2/ηlτ T (cid:1) + ∥\nF ∗)/ηlτ T (cid:1) + −\n\nO\n\n(cid:0)η2\n\nl τ (τ (cid:0)η2\n\nO\n\n(cid:0)\n\nO by\n\n∥ O\n\nthe 1)Lσ2 ∗\n\n− l L2τ (τ\n\n−\n\nof FedAvg can\n\nerror by (cid:1) (Khaled et al., 2020) and in the non-convex case (cid:1) (Wang et al., 2020). A careful inspection 1)σ2 g\n\nbounded\n\nbe\n\n6\n\n̸ Published as a conference paper at ICLR 2023\n\nFigure 2: Training characteristics of FedAvg and FedExP for the 2-D toy problem in Section 5. The last iterate of FedExP has an oscillating behavior in F (w) but monotonically decreases (cid:13) w∗(cid:13) 2; the average of the last two iterates lies in a lower loss region than the last iterate. (cid:13)\n\n(cid:13)w(t)\n\n−\n\nt=0 η(t)\n\nreveals that the impact of T1 on convergence of FedExP is different from FedAvg (effect of T2 is the same). We see that since (cid:80)T −1 T , FedExP reduces T1 faster than FedAvg. However this comes at the price of an increased error floor due to T3. Thus, the larger step-sizes in FedExP help us reach the vicinity of an optimum faster, but can ultimately end up saturating at a higher error floor due to noise around the optimum. Note that the impact of the error floor can be controlled by setting the client step size ηl appropriately. Moreover, in the overparameterized convex regime where σ2 ∗ = 0, the effect of T2 and T3 vanishes and thus FedExP clearly outperforms FedAvg. This aligns well with our initial motivation of using extrapolation in the overparameterized regime.\n\n≥\n\ng\n\n5 FURTHER INSIGHTS INTO FEDEXP\n\nIn this section, we discuss some further insights into the training of FedExP and how we leverage these insights to improve the performance of FedExP.\n\nFedExP monotonically decreases (cid:13) w∗(cid:13) 2\nF (w∗). Recall (cid:13) that our original motivation for the FedExP step size was aimed at trying to minimize the distance to the optimum give by (cid:13) (cid:13) (cid:13)w(t)\n\n− , when (6) holds. Doing so satisfies (cid:13)\n\nbut does not necessarily satisfy F (w(t+1))\n\nbut not necessarily F (w(t))\n\n(cid:13)w(t+1)\n\n(cid:13)w(t+1)\n\nF (w(t)).\n\nw∗(cid:13) 2\n(cid:13)\n\nw∗(cid:13) 2\n(cid:13)\n\nw∗(cid:13) 2\n(cid:13)\n\n(cid:13)w(t)\n\n−\n\n−\n\n≤\n\n−\n\n−\n\n≤\n\nTo better illustrate this phenomenon, we consider the following toy example in R2. We consider a setup with two clients, where the objective at each client is given as follows:\n\nF1(w) = (3w1 + w2 −\n\n3)2; F2(w) = (w1 + w2 −\n\n3)2.\n\n(12)\n\n∗\n\n{\n\n2 =\n\nw : w1 + w2 = 3\n\nWe denote the set of minimizers of F1(w) and F2(w) by 1 and\n\nand 1 = w : 3w1 + w2 = 3 S\n2 intersect at the point w∗ = [0, 3], ∗\nS S\nmaking it a global minimum. To minimize their local objectives, we assume clients run gradient in every round2. Figure 2 shows the trajectory of the iterates generated by descent with τ FedExP and FedAvg. We see that while (cid:13) decreases monotonically for FedExP, F (w(t)) does not do so and in fact has an oscillating nature as we discuss below.\n\nrespectively. Note that\n\nw∗(cid:13) 2\n(cid:13)\n\n(cid:13)w(t)\n\n→ ∞\n\n−\n\nS\n\n}\n\n}\n\n{\n\n∗\n\n∗\n\nUnderstanding oscillations in F (w(t)). We see that the oscillations in F (w(t)) are caused by FedExP iterates trying to minimize their distance from the solution sets 2 simultaneously. The initialization point w(0) is closer to 2 , which causes the FedExP iterate at round 1 1 than ∗\n1 and so on. To understand why this happens, consider 2 , then back towards to move towards S\n1 = 0, ∆(t) the case where ∆(t) g = 2 and therefore w(t+1) = (cid:13) (cid:13)\n\nw(t) . This gives us the intuition that the FedExP update in round t is trying to minimize the objectives of the\n\nS ∗\nS In this case, we have η(t)\n\n, which indicates that FedExP is now trying to minimize\n\n2 ̄∆(t) = w(t,τ )\n\n(cid:13)∆(t+1)\n\n1 and\n\n= 0.\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n−\n\nS\n\nS\n\nS\n\n∗\n\n∗\n\n∗\n\n∗\n\n2\n\n2\n\n2\n\nclients that have\n\n(cid:13) (cid:13)\n\n(cid:13)∆(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n0. While this leads to a temporary increase in global loss F (w(t)) in\n\n≫\n\n2The local models will be an exact projection of the global model on the solution sets {S ∗\n\nthe lower bound in (8) can be improved by a factor of 2 and therefore we use η(t) for this experiment (see Appendix C.4 and Appendix C.4.1 for proof).\n\ni }2 i=1. In this case, g = (∥∆1∥2+∥∆2∥2)/2∥ ̄∆(t)∥2\n\n7\n\n0.00.10.20.30.40.50.60.7w12.02.22.42.62.83.0w21234567890123456789*2*1Trajectory of IteratesFedAvgFedExPInitializationGlobal Minimum051015Training rounds10−210−1F(w)Global ObjectiveFedAvg (Last iterate)FedExP (Last iterate)FedExP (Avg. of last two iterates)051015Training rounds10−210−1100|w(t)−w*|2Distance to OptimumFedAvg (Last iterate)FedExP (Last iterate)FedExP (Avg. of last two iterates)̸ Published as a conference paper at ICLR 2023\n\nFigure 3: Experimental results on a synthetic linear regression experiments and a range of realistic FL tasks. FedExP consistently gives faster convergence compared to baselines while adding no extra computation, communication or storage at clients or server.\n\nsome rounds as shown in Figure 2, it is beneficial in the long run as it leads to a faster decrease in distance to the global optimum w∗.\n\nAveraging last two iterates in FedExP. Given the oscillating behavior of the iterates of FedExP, we find that measuring progress on F (w) using the last iterate can be misleading. Motivated by this finding, we propose to set the final model as the average of the last two iterates of FedExP. While the last iterate oscillates between regions that minimize the losses F1(w) and F2(w) respectively, the behavior of the average of the last two iterates is more stable and proceeds along a globally low loss region. Interestingly, we find that the benefits of averaging the iterates of FedExP also extend to training neural networks with multiple clients in practical FL scenarios (see Appendix D.1). In practice, the number of iterates to average over could also be a hyperparameter for FedExP, but we find that averaging the last two iterates works well, and we use this for our other experiments.\n\n6 EXPERIMENTS\n\nWe evaluate the performance of FedExP on synthetic and real FL tasks. For our synthetic experiment, we consider a distributed overparameterized linear regression problem. This experiment aligns most closely with our theory and allows us to carefully examine the performance of FedExP when (6) holds. For realistic FL tasks, we consider image classification on the following datasets i) EMNIST (Cohen et al., 2017), ii) CIFAR-10 (Krizhevsky et al., 2009), iii) CIFAR-100 (Krizhevsky et al., 2009), iv) CINIC-10 (Darlow et al., 2018). In all experiments, we compare against the following baselines i) FedAvg, ii) SCAFFOLD (Karimireddy et al., 2020b), and iii) FedAdagrad (Reddi et al., 2021) which is a federated version of the popular Adagrad algorithm. To the best of our knowledge, we are not aware of any other baselines that adaptively tune the server step size in FL.\n\nExperimental Setup. For the synthetic experiment, we consider a setup with 20 clients, 30 samples at each client, and model size to be 1000, making this an overparameterized problem. The data at each client is generated following a similar procedure as the synthetic dataset in Li et al. (2020). We use the federated version of EMNIST available at Caldas et al. (2019), which is naturally partitioned into 3400 clients. For CIFAR-10/100 we artifically partition the data into 100 clients, and for CINIC-10 we partition the data into 200 clients. In both cases, we follow a Dirichlet distribution with α = 0.3 for the partitioning to model heterogeneity among client data (Hsu et al., 2019). For EMNIST we use the same CNN architecture used in Reddi et al. (2021). For CIFAR10, CIFAR100 and CINIC-10 we use a ResNet-18 model (He et al., 2016). For our baselines, we find the best performing ηg and ηl by grid-search tuning. For FedExP we optimize for ε and ηl by grid search. We fix the number of participating clients to 20, minibatch size to 50 and number of local updates to 20 for all experiments. In Appendix D, we provide additional details and results, including the best performing hyperparameters, comparison with FedProx (Li et al., 2020), and results for more rounds.\n\nFedExP comprehensively outperforms FedAvg and baselines. Our experimental results in Figure 3 demonstrate that FedExP clearly outperforms FedAvg and competing baselines that use the best performing ηg and ηl found by grid search. Moreover, FedExP does not require additional communication or storage at clients or server unlike SCAFFOLD and FedAdagrad. The orderwise improvement in the case of the convex linear regression experiment confirms our theoretical motivation for FedExP outlined in Section 3.2. In this case, since (6) is satisfied, we know that the FedExP iterates are always moving towards the optimum. For realistic FL tasks, we see a consistent\n\n8\n\n0100200Training rounds10−1310−1010−710−410−1Mean Squared ErrorSyntheticFedExP (ours)FedAvgSCAFFOLDFedAdagrad0250500Training rounds80818283848586Test Accuracy (%)EMNIST0250500Training rounds304050607080Test Accuracy (%)CIFAR-100250500Training rounds20304050Test Accuracy (%)CIFAR-1000250500Training rounds404550556065Test Accuracy (%)CINIC-10Published as a conference paper at ICLR 2023\n\nTable 1: Table showing the average number of rounds to reach desired accuracy for FedExP and baselines. FedExP provides a consistent speedup over all baselines.\n\nDataset EMNIST CIFAR-10 CIFAR-100 CINIC-10\n\nTarget Acc. 84% 72% 40% 58%\n\nFedExP 186 267 242 318\n\nFedAvg 328 (1.76×) 434 (1.62×) 500 (2.06×) > 500 (> 2.06×) 450 (1.42×)\n\nSCAFFOLD 232 (1.24×) 429 (1.61×)\n\n470 (1.48×)\n\nFedAdagrad 277 (1.48×) 419 (1.56×) 494 (2.04×) 444 (1.40×)\n\n2\n\n−\n\nover FedAvg. This verifies that FedExP also provides performance speedup of over 1.4 improvement in more general settings with realistic datasets and models. Plots showing η(t) g can be found in Appendix D.5. The key takeaway from our experiments is that adapting the server step size allows FedExP to take much larger steps in some (but not all) rounds compared to the constant optimum step size taken by our baselines, leading to a large speedup.\n\n×\n\nComparison with FedAdagrad. As discussed in Section 1, FedAdagrad and FedExP use different notions of adaptivity; FedAdagrad uses coordinate-wise adaptivity, while FedExP uses client-based adaptivity. We believe that the latter is more meaningful for FL settings as seen in our experiments. In many experiments, especially image classification tasks like CIFAR, the gradients produced are dense with relatively little variance in coordinate-wise gradient magnitudes (Reddi et al., 2021; Zhang et al., 2020). In such cases, FedAdagrad is unable to leverage any coordinate-level information and gives almost the same performance as FedAvg.\n\nComparison with SCAFFOLD. We see that FedExP outperforms SCAFFOLD in all experiments, showing that adaptively tuning the server step size is sufficient to achieve speedup in FL settings. Furthermore, SCAFFOLD even fails to outperform FedAvg for the more difficult CIFAR and CINIC datasets. Several other papers have reported similar findings, including Reddi et al. (2021); Karimireddy et al. (2020a); Yu et al. (2022). Several reasons have been postulated for this behavior, including the staleness of control variates (Reddi et al., 2021) and the difficulty in characterizing client drift in non-convex scenarios (Yu et al., 2022). Thus, while theoretically attractive, simply using variance reduction techniques such as SCAFFOLD may not provide any speedup in practice.\n\nAdding extrapolation to SCAFFOLD. We note that SCAFFOLD only modifies the Local SGD procedure at clients and keeps the global aggregation at the server unchanged. Therefore, it is easy to modify the SCAFFOLD algorithm to use extrapolation when updating the global model at the server (algorithm details in Appendix E). Figure 4 shows the result of our proposed extrapolated SCAFFOLD on the CIFAR-10 dataset. Interestingly, we observe that while SCAFFOLD alone fails to outperform FedAvg, the extrapolated version of SCAFFOLD achieves the best performance among all algorithms. This result highlights the importance of carefully tuning the server step size to achieve the best performance for variance-reduction algorithms. It is also possible to add extrapolation to algorithms with server momentum (Appendix F).\n\n7 CONCLUSION\n\nFigure 4: Adding extrapolation to SCAFFOLD for greater speedup.\n\nIn this paper, we have proposed FedExP, a novel extension of FedAvg that adaptively determines the server step size used in every round of global aggregation in FL. Our algorithm is based on the key observation that FedAvg can be seen as an approximate variant of the POCS algorithm, especially for overparameterized convex objectives. This has inspired us to leverage the idea of extrapolation that is used to speed up POCS in a federated setting, resulting in FedExP. We have also discussed several theoretical and empirical perspectives of FedExP. In particular, we have explained some design choices in FedExP and how it can be used in practical scenarios with partial client participation and secure aggregation. We have also shown the convergence of FedExP for possibly underparameterized models and non-convex objectives. Our experimental results have shown that FedExP consistently outperforms baseline algorithms with virtually no additional computation or communication at clients or server. We have also shown that the idea of extrapolation can be combined with other techniques, such as the variance-reduction method in SCAFFOLD, for greater speedup. Future work will study the convergence analysis of FedExP with stochastic gradient noise and the incorporation of extrapolation into a wider range of algorithms used in FL.\n\n9\n\n0100200300400500Training rounds4050607080Test Accuracy (%)CIFAR-10FedExPSCAFFOLD-ExPFedAvgSCAFFOLDPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis work was supported in part by NSF grants CCF 2045694, CNS-2112471, ONR N00014-23-12149, and the CMU David Barakat and LaVerne Owen-Barakat Fellowship.\n\nREFERENCES\n\nDurmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2021.\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. Advances in Neural Information Processing Systems, 32, 2019.\n\nLarry Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific\n\nJournal of mathematics, 16(1):1–3, 1966.\n\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. Advances in Neural Information Processing Systems, 32, 2019.\n\nJonathan Barzilai and Jonathan M Borwein. Two-point step size gradient methods. IMA journal of\n\nnumerical analysis, 8(1):141–148, 1988.\n\nK. A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. In NeurIPS Workshop on Private Multi-Party Machine Learning, 2016.\n\nStephen Boyd and Jon Dattarro. Alternating projections, 2003. https://web.stanford.edu/\n\nclass/ee392o/alt_proj.pdf.\n\nOleg Burdakov, Yu-Hong Dai, and Na Huang. Stabilized Barzilai-Borwein method. Journal of\n\nComputational Mathematics, 37(6):916–936, 2019.\n\nSebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. In Workshop on Federated Learning for Data Privacy and Confidentiality, 2019.\n\nZachary Charles and Jakub Koneˇcn`y. On the outsized importance of learning rates in local update\n\nmethods. arXiv preprint arXiv:2007.00878, 2020.\n\nGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921–2926. IEEE, 2017.\n\nPatrick L Combettes. Convex set theoretic image recovery by extrapolated iterations of parallel\n\nsubgradient projections. IEEE Transactions on Image Processing, 6(4):493–506, 1997.\n\nLuke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. CINIC-10 is not Imagenet\n\nor CIFAR-10. arXiv preprint arXiv:1810.03505, 2018.\n\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Local SGD optimizes overparameterized neural networks in polynomial time. In International Conference on Artificial Intelligence and Statistics, pp. 6840–6861. PMLR, 2022.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of Machine Learning Research, 12(7), 2011.\n\nAA Goldstein. Optimization of Lipschitz continuous functions. Mathematical Programming, 13(1):\n\n14–22, 1977.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nLeonid Georgievich Gurin, Boris Teodorovich Polyak, and È V Raik. The method of projections for finding the common point of convex sets. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki, 7(6):1211–1228, 1967.\n\nFarzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated\n\nlearning. arXiv preprint arXiv:1910.14425, 2019.\n\nElad Hazan and Sham Kakade. Revisiting the Polyak step size. arXiv preprint arXiv:1905.00313,\n\n2019.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.\n\nSamuel Horváth, Konstantin Mishchenko, and Peter Richtárik. Adaptive learning rates for faster\n\nstochastic gradient methods. arXiv preprint arXiv:2208.05287, 2022.\n\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data\n\ndistribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\nBaihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. FL-NTK: A neural tangent kernel-based framework for federated learning analysis. In International Conference on Machine Learning, pp. 4423–4434. PMLR, 2021.\n\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in Neural Information Processing Systems, 31, 2018.\n\nTyler Johnson, Pulkit Agrawal, Haijie Gu, and Carlos Guestrin. Adascale SGD: A user-friendly algorithm for distributed training. In International Conference on Machine Learning, pp. 4911– 4920. PMLR, 2020.\n\nSwanand Kadhe, Nived Rajaraman, O Ozan Koyluoglu, and Kannan Ramchandran. FastSecAgg: Scalable secure aggregation for privacy-preserving federated learning. arXiv preprint arXiv:2009.11248, 2020.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nSai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes SignSGD and other gradient compression schemes. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3252–3261. PMLR, 2019.\n\nSai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning. arXiv preprint arXiv:2008.03606, 2020a.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020b.\n\nAhmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pp. 4519–4529. PMLR, 2020.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR (Poster),\n\n2015. URL http://arxiv.org/abs/1412.6980.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nNicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial Intelligence and Statistics, pp. 1306–1314. PMLR, 2021.\n\nGrigory Malinovsky, Konstantin Mishchenko, and Peter Richtárik. Server-side stepsizes and sampling without replacement provably help in federated optimization. arXiv preprint arXiv:2201.11066, 2022.\n\nYura Malitsky and Konstantin Mishchenko. Adaptive gradient descent without descent. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of PMLR, pp. 6702–6712, 2020.\n\nJan Mandel. Convergence of the cyclical relaxation method for linear inequalities. Mathematical\n\nprogramming, 30(2):218–228, 1984.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp. 1273–1282. PMLR, 2017.\n\nKonstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter Richtarik. ProxSkip: Yes! Local gradient steps provably lead to communication acceleration! Finally! In Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 15750–15769. PMLR, 2022.\n\nAritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems, 34:14606–14619, 2021.\n\nGuy Pierra. Decomposition through formalization in a product space. Mathematical Programming,\n\n28(1):96–115, 1984.\n\nBoris Teodorovich Polyak. Minimization of unsmooth functionals. USSR Computational Mathematics and Mathematical Physics, 9(3):14–29, 1969. ISSN 0041-5553. doi: https://doi.org/10.1016/ 0041-5553(69)90061-5.\n\nMarcos Raydan. On the barzilai and borwein choice of steplength for the gradient method. IMA\n\nJournal of Numerical Analysis, 13(3):321–326, 1993.\n\nSashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný, Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference on Learning Representations, 2021.\n\nTijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.\n\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. Advances in Neural Information Processing Systems, 33:7611–7623, 2020.\n\nDong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter Bartlett. Gradient diversity: a key ingredient for scalable distributed learning. In International Conference on Artificial Intelligence and Statistics, pp. 1998–2007. PMLR, 2018.\n\nYaodong Yu, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, and Michael I Jordan. TCT: Convexifying federated learning using bootstrapped neural tangent kernels. arXiv preprint arXiv:2207.06343, 2022.\n\nKai Yue, Richeng Jin, Ryan Pilgrim, Chau-Wai Wong, Dror Baron, and Huaiyu Dai. Neural tangent kernel empowered federated learning. In International Conference on Machine Learning, pp. 25783–25803. PMLR, 2022.\n\nMatthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,\n\n2012.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.\n\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAPPENDIX\n\nA Additional Related Work\n\nB Table of Notation and Schematic\n\nB.1 Table of Notation .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nB.2 Schematic of Client-Server communication in FedExP . . . . . . . . . . . . . . .\n\nC Proofs\n\nC.1 Proof of Lemma 1 .\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nC.2 Convergence Analysis for Convex Objectives\n\n. . . . . . . . . . . . . . . . . . . .\n\nC.3 Convergence Analysis for Non-Convex Objectives\n\n. . . . . . . . . . . . . . . . .\n\nC.4 Exact Projection with Gradient Descent for Linear Regression . . . . . . . . . . .\n\nD Additional Experiments and Setup Details\n\nD.1 Impact of Averaging Iterates for Neural Networks . . . . . . . . . . . . . . . . . .\n\nD.2 Dataset Details\n\n.\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.3 Hyperparameter Details .\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nD.4 Sensitivity of FedExP to ε . . . .\n\n. . . . . . .\n\n. . .\n\n. . . . . . . . . . . . . . . . .\n\nD.5 Additional Results .\n\n.\n\n.\n\n.\n\n.\n\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nE Combining Extrapolation with SCAFFOLD\n\nF Combining Extrapolation with Server Momentum\n\n15\n\n16\n\n16\n\n16\n\n17\n\n17\n\n18\n\n21\n\n23\n\n26\n\n26\n\n26\n\n27\n\n27\n\n28\n\n32\n\n33\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL RELATED WORK\n\nIn this section, we provide further discussion on some additional related work that complements our discussion in Section 1.\n\nAdaptive Step Size in Gradient Descent. Here we briefly discuss methods for tuning the step size in gradient descent and the challenges in applying them to the FL setting. Early methods to tune the step size in gradient descent were based on line search (or backtracking) strategies (Armijo, 1966; Goldstein, 1977). However, these strategies need to repeatedly compute the function value or gradient within an iteration, making them computationally expensive. Another popular class of adaptive step sizes is based on the Polyak step size (Polyak, 1969; Hazan & Kakade, 2019; Loizou et al., 2021). Similar to FedExP, the Polyak step size is derived from trying to minimize the distance to the optimum for convex functions. However it is not clear how this can be extended to the federated setting where we only have access to pseudo-gradients. Also, the Polyak step size requires knowledge of the function value at the optimum which is hard to estimate. Another related class of step sizes is the Barzilai-Borwein stepsize (Barzilai & Borwein, 1988). However, to the best of our knowledge, these are known to provably work only for quadratic functions (Raydan, 1993; Burdakov et al., 2019) only. A recent work (Malitsky & Mishchenko, 2020) alleviates some of the concerns associated with these classical methods by setting the step size as an approximation of the inverse local Lipschitz constant; however it is again not clear how this intuition can be applied to the federated setting. An orthogonal line of work has focused on methods that adapt to the geometry of the data using gradient information in previous iterations, the most popular among them being Adagrad (Duchi et al., 2011) and its extensions RMSProp (Tieleman et al., 2012) and Adadelta (Zeiler, 2012). There exist federated counterparts of these algorithms, namely FedAdagrad; however, as we show in our experiments these methods can fail to even outperform FedAvg in standard FL tasks.\n\nOverparameterization in FL. Inspired by the success of analyzing deep neural networks in the neural tangent kernel (NTK) regime (Jacot et al., 2018; Arora et al., 2019; Allen-Zhu et al., 2019), recent work has looked at studying the convergence of overparameterized neural networks in the FL setting. Huang et al. (2021) and Deng et al. (2022) show that for a sufficiently wide neural network and proper step size conditions, FedAvg will converge to a globally optimal solution even in the presence of data heterogeneity. We note that these works are primarily concerned with convergence analysis, whereas our focus is on developing a practical algorithm that is inspired by characteristics in the overparameterized regime for speeding up FL training. Another recent line of work has looked at utilizing NTK style Jacobian features for learning a FL model in just a few rounds of communication (Yu et al., 2022; Yue et al., 2022). While interesting, these approaches are orthogonal to our current work.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nB TABLE OF NOTATION AND SCHEMATIC\n\nB.1 TABLE OF NOTATION\n\nTable 2: Summary of notation used in paper\n\nSymbol Description\n\nl(\n\n∥ ∥ M\n) ,\n· ·\nDi Fi(w) F (w) ηl ηg w(t) η(t) w(t,k) i\nτ ∆(t) ̄∆(t)\n\ng\n\ni\n\n∗ i\nS T\nε w∗ F ∗ L\nσ2 ∗\nσ2\n\nL2 norm Number of clients Loss function Dataset at i-th client Local objective at i-th client Global objective at server Client step size Server step size Global model at round t FedExP server step size at round t Local model at i-th client at t-th round and k-th iteration Number of local SGD steps Update of i-th client at round t Average of client updates at round t Set of minimizers of Fi(w) Number of communication rounds Small constant added to denominator of FedExP step size Global minimum Minimum value of global objective L-smoothness constant used in Assumption 1 Upper bound on variance of client gradients at optimum (see Assumption 2) Upper bound on variance of client gradients (see Assumption 3)\n\nB.2 SCHEMATIC OF CLIENT-SERVER COMMUNICATION IN FEDEXP\n\nAt each round t, the server first sends global model w(t) to all clients. Clients perform local optimization on w(t) to compute their local models w(t,τ ) i = w(t)\n\nto the server. This procedure is illustrated in Figure 5.\n\nand send back their update ∆(t)\n\nand norm of update\n\nw(t,τ )\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)∆(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\ni −\n\ni\n\nFigure 5: Schematic of client-server communication in FedExP.\n\n16\n\n<latexit sha1_base64=\"wIfj5Q2rugicXXHXdORBVWn/QCs=\">AAACcXicbVHLbtQwFHVSHiW8hpYNqkBWR6BWlUZJF5RlJTYsi+i0lSbDyHZuUqt+RPYNdBRlz/d1x0+w4QdwphEqLVeydHTOfR7zWkmPafozitfu3X/wcP1R8vjJ02fPRy82TrxtnICpsMq6M848KGlgihIVnNUOmOYKTvnFx14//QbOS2uOcVnDXLPKyFIKhoFajH7kHCppWqZkZbok26XvaI5wiS39Ai5UUg+m8LRSljPV0dxY02gOLs+Tv5naFqBoEDXDc16237uv7Q7udoOMljKlqFASDPobPZI89B5GL0bjdJKugt4F2QDGZIijxegqL6xodOgpFPN+lqU1zlvmUAoFXZI3HmomLlgFswAN0+Dn7cqxjr4NTEFL68IzSFfszYqWae+XmofM/iZ/W+vJ/2mzBssP81aaukEw4npQ2ajeg95+WkgHAtUyACacDLtScc4cExg+KQkmZLdPvgtO9ifZ+0n2eX98mA52rJMtsk12SEYOyCH5RI7IlAjyK3oZvY7eRL/jVzGNt69T42io2ST/RLz3ByALvXU=</latexit>1)Serversendsglobalmodelw(t)toallclients<latexit sha1_base64=\"+RwOKx1VbMTbBHerVHkal6A/Kas=\">AAACOXicbVA9TxtBEN0jEMgBwUnKNCssEDTWnQtCiUST0kgYkHyWNbceHyv247Q7h2Kd/Ldo+BfpItFQJEK0+QNZmyvCx5NWenozszPv5aWSnpLkV7T0bnnl/erah3h9Y/PjVuvT5zNvKyewL6yy7iIHj0oa7JMkhRelQ9C5wvP86nheP79G56U1pzQtcaihMHIiBVCQRq1elmMhTQ1KFmYWd/f5Ls8If1DNj5VEQ56X6CbWaa6sAMXJgTTSFHzGM2NNpXN0cYZm3HwxarWTTrIAf03ShrRZg96o9TMbW1HpsEso8H6QJiUNa3AkhcJZnFUeSxBXUOAgUAMa/bBeOJ/xnaCMeTgvPEN8of4/UYP2fqrz0KmBLv3L2lx8qzaoaHI4rKUpK0IjnhZNqmDf8nmMfCwdClLTQEA4GW7l4hIcCAphxyGE9KXl1+Ss20kPOulJt32UNHGssa9sm+2xlH1jR+w767E+E+yG3bHf7E90G91HD9HjU+tS1Mx8Yc8Q/f0Hg36tWw==</latexit>2)Clientsperformlocaltraining<latexit sha1_base64=\"5o/EfzBP3obsjACcTVWnpD3w7jE=\">AAAClXicbVFNb9NAEF27fBTzFcqBA5cVEaiRILJTqfRSqdAKcaNIpK2UTaPxZpyuul5bu+NC5Pof8Wu48W/YJFahLSOt9PTm7cy8mbTUylEc/w7CtTt3791ffxA9fPT4ydPOs40jV1RW4lAWurAnKTjUyuCQFGk8KS1Cnmo8Ts/3F/njC7ROFeYbzUsc5zAzKlMSyFOTzk+R4kyZGrSamSba6vE3XBD+oJrva4WGHHdopjwFec4bLg5QE0zUab1JvYbvcpEDnaVZ/b1ZUe/+EivVW0FQ9RphClPlKVohoqsO4Av7mpfXq4rL0wG/0kfCt2/Hm3S6cT9eBr8NkhZ0WRuHk84vMS1klXsfUoNzoyQuaVyDJSU1NpGoHJbeGcxw5KGBHN24Xm614a89M+VZYf0zxJfsvz9qyJ2b56lXLjy7m7kF+b/cqKJsZ1wrU1aERq4aZZXmVPDFifhUWZSk5x6AtMrPyuUZWJDkDxn5JSQ3Ld8GR4N+st1Pvg66e3G7jnX2kr1imyxh79ke+8wO2ZDJYCPYCT4EH8MX4W54EH5aScOg/fOcXYvwyx/NnMjS</latexit>3)Clientssendback(t)i=w(t)w(t,⌧)iandk(t)ik2<latexit sha1_base64=\"Gv0cxlC+5B5gZAK6uC+GYh7cdq8=\">AAACAHicbVDLSsNAFJ3UV62vVBcu3ASLUEFKIqIuC25cVrAPaGKYTCft0MmDmRulhGz8ERduXCji1s9w586ln+Gk7UJbD1w4nHMv997jxZxJMM1PrbCwuLS8Ulwtra1vbG7p5e2WjBJBaJNEPBIdD0vKWUibwIDTTiwoDjxO297wIvfbt1RIFoXXMIqpE+B+yHxGMCjJ1XftAMPA89O7zLVu0ioc2YCTw8zVK2bNHMOYJ9aUVOpl7+vhe3DScPUPuxeRJKAhEI6l7FpmDE6KBTDCaVayE0ljTIa4T7uKhjig0knHD2TGgVJ6hh8JVSEYY/X3RIoDKUeBpzrzc+Wsl4v/ed0E/HMnZWGcAA3JZJGfcAMiI0/D6DFBCfCRIpgIpm41yAALTEBlVlIhWLMvz5PWcc06rVlXKg0TTVBEe2gfVZGFzlAdXaIGaiKCMvSIntGLdq89aa/a26S1oE1ndtAfaO8/bR6Z8Q==</latexit>w(t,⌧)1<latexit sha1_base64=\"fXxOmsKy1kGcjU02Z7mKlRvjWHQ=\">AAACAHicbVDLSsNAFJ34rPWV6sKFm2ARKkhJiqjLghuXFewDmhgm00k7dPJg5kYpIRt/xIUbF4q49TPcuXPpZzhpu9DWAxcO59zLvfd4MWcSTPNTW1hcWl5ZLawV1zc2t7b10k5LRokgtEkiHomOhyXlLKRNYMBpJxYUBx6nbW94kfvtWyoki8JrGMXUCXA/ZD4jGJTk6nt2gGHg+eld5tZu0goc24CTo8zVy2bVHMOYJ9aUlOsl7+vhe3DScPUPuxeRJKAhEI6l7FpmDE6KBTDCaVa0E0ljTIa4T7uKhjig0knHD2TGoVJ6hh8JVSEYY/X3RIoDKUeBpzrzc+Wsl4v/ed0E/HMnZWGcAA3JZJGfcAMiI0/D6DFBCfCRIpgIpm41yAALTEBlVlQhWLMvz5NWrWqdVq0rlYaJJiigfXSAKshCZ6iOLlEDNRFBGXpEz+hFu9eetFftbdK6oE1ndtEfaO8/bq2Z8g==</latexit>w(t,⌧)2<latexit sha1_base64=\"LqyI3A/BZuc+VGPZJTXbtt5rhIc=\">AAACAHicbVC7SgNBFJ2Nrxhf0RQWNoNBiCBh10ItAzY2QgTzgGxcZiezyZDZBzN3lbBs46/YWCjB1g+wsrKztPE7nDwKTTxw4XDOvdx7jxsJrsA0P43MwuLS8kp2Nbe2vrG5ld/eqaswlpTVaChC2XSJYoIHrAYcBGtGkhHfFazh9s9HfuOWScXD4BoGEWv7pBtwj1MCWnLyu7ZPoOd6yV3qXN4kJTiygcSHqZMvmmVzDDxPrCkpVgr4e/j+9VZ18h92J6SxzwKggijVsswI2gmRwKlgac6OFYsI7ZMua2kaEJ+pdjJ+IMUHWulgL5S6AsBj9fdEQnylBr6rO0fnqllvJP7ntWLwztoJD6IYWEAni7xYYAjxKA3c4ZJREANNCJVc34ppj0hCQWeW0yFYsy/Pk/px2TopW1c6DRNNkEV7aB+VkIVOUQVdoCqqIYpS9ICe0LNxbzwaQ+Nl0poxpjMF9AfG6w+ehZrS</latexit>w(t,⌧)M<latexit sha1_base64=\"GVLde3CMf/ZfWFPefYy+AL60p8Y=\">AAAB+XicbVDLSgNBEOyNrxhfqx69DAbBU9gVUY+BXLwIEUwMJCHMTjrJkNkHM73BsORPvHhQxKt/4s2/cfI4aGLBQFHVRfdUkChpyPO+ndza+sbmVn67sLO7t3/gHh7VTZxqgTURq1g3Am5QyQhrJElhI9HIw0DhYzCsTP3HEWoj4+iBxgm2Q96PZE8KTlbquG6L8IkyVlESI2KTu45b9EreDGyV+AtShAWqHfer1Y1FGtq4UNyYpu8l1M64JikUTgqt1GDCxZD3sWlpxEM07Wx2+YSdWaXLerG2z66fqb8TGQ+NGYeBnQw5DcyyNxX/85op9W7amYySlDAS80W9VDGK2bQG1pUaBamxJVxoaW9lYsA1F2TLKtgS/OUvr5L6Rcm/Kvn3l8Wyt6gjDydwCufgwzWU4RaqUAMBI3iGV3hzMufFeXc+5qM5Z5E5hj9wPn8AHeSTRA==</latexit>ClientM<latexit sha1_base64=\"mNMqN3UVpBFTk5NhZT6P20NjLXs=\">AAAB+XicbVDLSgNBEOz1GeNr1aOXwSB4CrtB1GMgF48RzAOSEGYnnWTI7IOZ3mBY8idePCji1T/x5t84eRw0sWCgqOqieypIlDTked/OxubW9s5ubi+/f3B4dOyenNZNnGqBNRGrWDcDblDJCGskSWEz0cjDQGEjGFVmfmOM2sg4eqRJgp2QDyLZl4KTlbqu2yZ8ooxVlMSI2LTUdQte0ZuDrRN/SQqwRLXrfrV7sUhDGxeKG9PyvYQ6GdckhcJpvp0aTLgY8QG2LI14iKaTzS+fskur9Fg/1vbZ9XP1dyLjoTGTMLCTIaehWfVm4n9eK6X+XSeTUZISRmKxqJ8qRjGb1cB6UqMgNbGECy3trUwMueaCbFl5W4K/+uV1Ui8V/Zui/3BdKHvLOnJwDhdwBT7cQhnuoQo1EDCGZ3iFNydzXpx352MxuuEsM2fwB87nD/Tpkyk=</latexit>Client2<latexit sha1_base64=\"W2MYlrBnSGcpjmxeI21gMZvEbo0=\">AAAB+XicbVDLSgNBEOyNrxhfqx69DAbBU9gVUY+BXDxGMA9IQpiddJIhsw9meoNhyZ948aCIV//Em3/j5HHQxIKBoqqL7qkgUdKQ5307uY3Nre2d/G5hb//g8Mg9PqmbONUCayJWsW4G3KCSEdZIksJmopGHgcJGMKrM/MYYtZFx9EiTBDshH0SyLwUnK3Vdt034RBmrKIkRsanfdYteyZuDrRN/SYqwRLXrfrV7sUhDGxeKG9PyvYQ6GdckhcJpoZ0aTLgY8QG2LI14iKaTzS+fsgur9Fg/1vbZ9XP1dyLjoTGTMLCTIaehWfVm4n9eK6X+XSeTUZISRmKxqJ8qRjGb1cB6UqMgNbGECy3trUwMueaCbFkFW4K/+uV1Ur8q+Tcl/+G6WPaWdeThDM7hEny4hTLcQxVqIGAMz/AKb07mvDjvzsdiNOcsM6fwB87nD/Nlkyg=</latexit>Client1<latexit sha1_base64=\"vC017/Bqmjrtiuc75oiguazEKfc=\">AAAB/XicbVDJSgNBFOxxjXEbl5uXxiB4CjMi6jGQi8eIZoFkCD09L0mTnoXuN2Icgr/ixYMiXv0Pb/6NnWQOmljQUFTV470uP5FCo+N8W0vLK6tr64WN4ubW9s6uvbff0HGqONR5LGPV8pkGKSKoo0AJrUQBC30JTX9YnfjNe1BaxNEdjhLwQtaPRE9whkbq2ocdhAfMaFXGaUBvQZnwuGuXnLIzBV0kbk5KJEeta391gpinIUTIJdO67ToJehlTKLiEcbGTakgYH7I+tA2NWAjay6bXj+mJUQLai5V5EdKp+nsiY6HWo9A3yZDhQM97E/E/r51i78rLRJSkCBGfLeqlkmJMJ1XQQCjgKEeGMK6EuZXyAVOMoymsaEpw57+8SBpnZfei7N6clypOXkeBHJFjckpcckkq5JrUSJ1w8kieySt5s56sF+vd+phFl6x85oD8gfX5A3bZlS4=</latexit>CloudServerPublished as a conference paper at ICLR 2023\n\nC PROOFS\n\nWe first state some preliminary lemmas that will used throughout the proofs.\n\nLemma 2. (Jensen’s inequality) For any ai ∈\n\nRd, i\n\n1, 2, . . . , M\n\n∈ {\n\n:\n\n}\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nM (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nai\n\nai\n\n1 M\n\n≤\n\nM\n\n≤\n\nM (cid:88)\n\ni=1\n\nM (cid:88)\n\ni=1\n\n2 ,\n\nai∥\n\n∥\n\n2 .\n\nai∥\n\n∥\n\nWe also note the following known result related to the Bregman divergence.\n\nLemma 3. (Khaled et al., 2020) If F is smooth and convex, then\n\nF (w)\n\n∥∇\n\nF (w′)\n\n2 ∥\n\n≤\n\n− ∇\n\n2L(F (w)\n\nF (w′)\n\n−\n\n− ⟨∇\n\nF (w′), w\n\nw′\n\n).\n\n⟩\n\n−\n\nLemma 4. (Co-coercivity of convex smooth function) If F is L-smooth and convex then,\n\nF (w)\n\n⟨∇\n\n− ∇\n\nF (w′), w\n\nw′\n\n−\n\n⟩ ≥\n\n1 L ∥∇\n\nF (w)\n\nF (w′)\n\n2 . ∥\n\n− ∇\n\nA direct consequence of this lemma is,\n\n⟨∇ where w∗ is a minimizer of F (w).\n\nF (w), w\n\nC.1 PROOF OF LEMMA 1\n\nw∗\n\n−\n\n⟩ ≥\n\n1 L ∥∇\n\nF (w)\n\n2 ∥\n\n(13)\n\n(14)\n\n(15)\n\n(16)\n\n(17)\n\nLet Fi(w) be the local objective at a client and w∗ be the global minimum. From the overparameterization assumption, we know that w∗ is also a minimizer for Fi(w). We have,\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k)\n\ni\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n=\n\n=\n\n≤\n\n≤\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)w(t,k−1) (cid:13)w(t,k−1)\n\n(cid:13) (cid:13)\n\ni\n\nηl∇ w∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n−\n\nF (w(t,k−1)\n\n)\n\ni\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n− F (w(t,k−1)\n\ni\n\n2ηl⟨∇\n\n−\n\n), w(t,k−1)\n\ni\n\nw∗\n\n⟩\n\n−\n\n+ η2\n\nl\n\n(cid:13) (cid:13) (cid:13)∇\n\n(18)\n\nF (w(t,k−1)\n\ni\n\n)\n\n(cid:13) 2\n(cid:13) (cid:13) (19)\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k−1)\n\ni\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k−1)\n\ni\n\nw∗(cid:13) w∗(cid:13)\n\n2 (cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n−\n\n−\n\n−\n\n−\n\n2ηl L\nηl L\n\n(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13)∇\n\nF (w(t,k−1)\n\ni\n\nF (w(t,k−1)\n\ni\n\n)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) 2\n(cid:13) )\n(cid:13)\n\n+ η2\n\nl\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t,k−1)\n\ni\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\n(20)\n\n(21)\n\nwhere (20) follows from (17) and (21) follows from ηl ≤\n\nk = 0 to τ\n\n1 we have,\n\n−\n\n1\n\nL . Summing the above inequality from\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,τ )\n\ni\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n−\n\n≤\n\n−\n\nηl L\n\nτ −1 (cid:88)\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\n.\n\nThus we have,\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,τ )\n\ni\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\n−\n\n≤\n\n≤\n\nThis completes the proof of this lemma.\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\nηl M L\n\n−\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n.\n\n17\n\n(22)\n\n(23)\n\n(24)\n\nPublished as a conference paper at ICLR 2023\n\nC.2 CONVERGENCE ANALYSIS FOR CONVEX OBJECTIVES\n\nOur proof technique is inspired by Khaled et al. (2020) with some key differences. The biggest difference is the incorporation of the adaptive FedExP server step sizes which Khaled et al. (2020) does not account for. Another difference is that we provide convergence guarantees in terms of number of rounds T while Khaled et al. (2020) focus on number of iterations T ′ = T τ . We highlight the specific steps where we made adjustments to the analysis of Khaled et al. (2020) below.\n\nWe begin by modifying Khaled et al. (2020, Lemma 11 and Lemma 13) to bound client drift in every round instead of every iteration.\n\nLemma 5. (Bounding client aggregate gradients)\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k)\n\ni −\n\nw(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ 6τ L(F (w(t))\n\nF (w∗)) + 3τ σ2\n\n∗ .\n\n−\n\n1 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\n3L2 M\n\n≤\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\nProof of Lemma 5:\n\n1 M\n\n=\n\n≤\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\n1 M\n\n3 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t,k)\n\ni\n\n)\n\nFi(w(t)) +\n\nFi(w(t))\n\n∇\n\n− ∇\n\nFi(w∗) +\n\n− ∇\n\nFi(w∗)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n∇\n\nFi(w(t,k)\n\ni\n\n)\n\nFi(w(t))\n\n− ∇\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n3 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nFi(w∗)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n− ∇\n\n+\n\n3 M\n\n3L2 M\n\n≤\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\nFi(w∗)\n\n2 ∥\n\n∥∇\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k)\n\ni\n\nw(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n+ 6τ L(F (w(t))\n\nF ∗) + 3τ σ2\n\n∗ .\n\n−\n\n(25)\n\n(26)\n\n(27)\n\n(28)\n\nThe first term in (28) follows from L-smoothness of Fi(w), the second term follows from Lemma 3 and the third term follows from bounded noise at optimum.\n\nLemma 6. (Bounding client drift)\n\n1 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n12η2\n\nl τ 2(τ\n\n−\n\n1)L(F (w(t))\n\n−\n\nF (w∗)) + 6η2\n\nl τ 2(τ\n\n1)σ2\n\n∗ .\n\n(29)\n\n−\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma 6:\n\n1 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n= η2\n\nl\n\nη2\n\nl\n\n≤\n\n1 M\n\n1 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\nM (cid:88)\n\nτ −1 (cid:88)\n\nk−1 (cid:88)\n\nl=0\n\nk−1 (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nk\n\ni=1\n\nk=0\n\nl=0\n\nFi(w(t,l)\n\ni\n\n)\n\n∇\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t,l)\n\ni\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\nη2\n\nl τ (τ\n\n1)\n\n1 M\n\n−\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n≤\n\n(30)\n\n(31)\n\n(32)\n\n3η2\n\nl τ (τ\n\n1)L2 1\n\nM\n\n−\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 6η2\n\nl τ 2(τ\n\n−\n\n1)L(F (w(t))\n\n−\n\nF (w∗))\n\n(33)\n\n+ 3η2\n\nl τ 2(τ\n\n1)σ2\n\n∗\n\n1 2M\n\n≤\n\n− τ −1 (cid:88)\n\nM (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 6η2\n\nl τ 2(τ\n\n−\n\n1)L(F (w(t))\n\nF (w∗))\n\n−\n\n(34)\n\n+ 3η2\n\nl τ 2(τ\n\n1)σ2\n\n∗\n\n−\n\nwhere (33) uses Lemma 5 and (34) uses ηl ≤\n\nTherefore we have,\n\n1\n\n6τ L .\n\n1 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n12η2\n\nl τ 2(τ\n\n−\n\n1)L(F (w(t))\n\n−\n\nF (w∗)) + 6η2\n\nl τ 2(τ\n\n1)σ2\n\n∗ .\n\n(35)\n\n−\n\nProof of Theorem 1:\n\nWe define the following auxiliary variables that will used in the proof.\n\nAggregate Client Gradient: h(t)\n\ni =\n\nτ −1 (cid:88)\n\nk=0\n\n∇\n\nFi(w(t,k)\n\ni\n\n).\n\nWe also define ̄h(t) = 1\n\nM\n\n(cid:80)M\n\ni=1 h(t)\n\ni\n\n.\n\nRecall that the update of the global model can be written as w(t+1) = w(t)\n\nη(t) g ηl ̄h(t).\n\n−\n\nWe have (cid:13) (cid:13) (cid:13)w(t+1)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13)w(t) (cid:13) (cid:13) (cid:13)w(t)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n=\n\n=\n\n≤\n\n−\n\n−\n\n−\n\ng ηl ̄h(t) η(t) w∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13) (cid:68)\n\n−\n\n2η(t)\n\ng ηl\n\nwt\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n2η(t)\n\ng ηl\n\n(cid:68)\n\nwt\n\nw∗, ̄h(t)(cid:69)\n\nw∗, ̄h(t)(cid:69)\n\n−\n\n−\n\n+ (η(t)\n\ng )2η2\n\nl\n\n(cid:13) (cid:13) (cid:13)\n\n ̄h(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ η(t)\n\ng η2\n\nl\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni\n\n(36)\n\n(37)\n\n(38)\n\n(39)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nwhere (39) follows from η(t)\n\ng\n\n. Inequality (39) is a key step in our proof and the\n\ndifferentiating factor in our approach from Khaled et al. (2020). Following a similar technique as Khaled et al. (2020) to bound (η(t) 1/8Lη(t) g )2η2 g , which cannot be satisfied in our setup due to the adaptive choice of η(t) g . Therefore we first upper\n\nwill end up requiring the condition ηl ≤\n\nl\n\n∥ (cid:13) (cid:13) ̄h(t)(cid:13) 2\n(cid:13)\n\n2\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)h(t) ̄h(t)\n\n(cid:13) (cid:13) (cid:13) 2\n\n(cid:80)M\n\ni=1\n\nM\n\n∥\n\n≤\n\n19\n\nPublished as a conference paper at ICLR 2023\n\ng )2η2\n\n(cid:13) 2\nbound (η(t) (cid:13) (cid:13) the rest of the proof, which does not require the aforementioned condition. Note that this comes at the expense of the additional T3 error seen in our final convergence bound in Theorem 1.\n\nand focus on further bounding this quantity in\n\n(cid:13) ̄h(t)(cid:13) (cid:13) 2\n(cid:13)\n\n(cid:13)h(t)\n\nby η(t)\n\ng η2\n\n(cid:80)M\n\n1 M\n\ni=1\n\n(cid:13) (cid:13)\n\ni\n\nl\n\nl\n\nTherefore,\n\n(cid:13) (cid:13) (cid:13)w(t+1)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n≤\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n2η(t)\n\ng ηl\n\n(cid:68)\n\nwt\n\n(cid:124)\n\n−\n\nw∗, ̄h(t)(cid:69)\n\n(cid:123)(cid:122) T1\n\n(cid:125)\n\n+η(t)\n\ng η2\n\nl\n\n1 M\n(cid:124)\n\nM (cid:88)\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni\n\ni=1\n\n(cid:123)(cid:122) T2\n\nBounding T2\n\nWe have,\n\nT2 =\n\n=\n\n≤\n\n≤\n\n1 M\n\n1 M\n\nτ M\n\nM (cid:88)\n\ni=1\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ −1 (cid:88)\n\n∇\n\nk=0\n\nFi(w(t,k)\n\ni\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) )\n(cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) )\n(cid:13)\n\n3τ L2 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k)\n\ni\n\nw(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n+ 6τ 2L(F (w(t))\n\nF ∗) + 3τ 2σ2\n\n∗\n\n−\n\nwhere (43) follows from Jensen’s inequality and and (44) follows from Lemma 5.\n\nBounding T1\n\nT1 =\n\n=\n\n1 M\n\n1 M\n\n(cid:68)\n\nwt\n\nM (cid:88)\n\ni=1\n\n−\n\n(cid:69)\n\nw∗, h(t)\n\ni\n\nM (cid:88)\n\nτ −1 (cid:88)\n\n(cid:68)\n\nw(t)\n\ni=1\n\nk=0\n\nw∗,\n\n∇\n\n−\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:69)\n\n.\n\nWe have,\n\n(cid:68)\n\nw(t)\n\nw∗,\n\n∇\n\n−\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:69)\n\n(cid:68)\n\nw(t)\n\n=\n\nw(t,k)\n\ni\n\n,\n\n∇\n\n−\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:69)\n\n(cid:68)\n\n+\n\nw(t,k)\n\ni\n\nw∗,\n\n∇\n\n−\n\n(40)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n(cid:125)\n\n(41)\n\n(42)\n\n(43)\n\n(44)\n\n(45)\n\n(46)\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:69)\n\n.\n\nFrom L-smoothness of Fi we have,\n\n(cid:68)\n\nw(t)\n\nw(t,k)\n\ni\n\n,\n\n∇\n\n−\n\n(cid:69)\n\nFi(w(t,k)\n\ni\n\n)\n\nFi(w(t))\n\n≥\n\nFrom convexity of Fi we have,\n\n(cid:68)\n\nw(t,k)\n\ni\n\nw∗,\n\n∇\n\n−\n\nFi(w(t,k)\n\ni\n\n(cid:69) )\n\nTherefore, adding the above inequalities we have,\n\n(cid:68)\n\nw(t)\n\nw∗,\n\n∇\n\n−\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:69)\n\n≥\n\nFi(w(t))\n\nSubstituting (50) in (46) we have,\n\n−\n\n≥\n\n−\n\nFi(w(t,k)\n\ni\n\n)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\nL 2\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n−\n\nFi(w(t,k)\n\ni\n\n)\n\nFi(w∗).\n\n−\n\nFi(w∗)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\nL 2\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n−\n\nT1 ≥\n\nτ (F (w(t))\n\nF (w∗))\n\n−\n\nL 2M\n\n−\n\nM (cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n−\n\nw(t,k)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n20\n\n(47)\n\n(48)\n\n(49)\n\n(50)\n\n(51)\n\nPublished as a conference paper at ICLR 2023\n\nHere we would like to note that the bound for T1 is our contribution and is needed in our proof due to the relaxation in (39). The bound for T2 follows a similar technique as Khaled et al. (2020, Lemma 12).\n\nSubstituting the bounds for T1 and T2 in (40) we have,\n\n(cid:13) (cid:13) (cid:13)w(t+1)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n−\n\n≤\n\n−\n\n2η(t)\n\ng ηlτ (1\n\n+ (3η(t)\n\ng η2\n\nl τ L2 + η(t)\n\ng ηlL)\n\n1 M\n\n3ηlτ L)(F (w(t))\n\nF (w∗)) + 3η(t)\n\ng η2\n\nl τ 2σ2\n\n∗\n\n−\n\n− M\n(cid:88)\n\nτ −1 (cid:88)\n\ni=1\n\nk=0\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k)\n\ni\n\nw(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13)w(t)\n\nw∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n−\n\n≤\n\ng ηlτ (F (w(t)) η(t)\n\n−\n\nF (w∗)) + 3η(t)\n\ng η2\n\nl τ 2σ2\n\n∗\n\n1 M\n\nM (cid:88)\n\nτ −1 (cid:88)\n\nk=0\n\n(cid:13) (cid:13)\n\n(cid:13)w(t,k)\n\ni\n\nw(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n+ 2η(t)\n\ng ηlL\n\n(cid:13) (cid:13) (cid:13)w(t) ≤\n+ 24η(t)\n\n− g η3\n\n(cid:13) (cid:13) (cid:13)w(t) ≤\n+ 12η(t)\n\n− g η3\n\n−\n\nw∗(cid:13)\n\ni=1 2\n(cid:13) (cid:13) l τ 2(τ w∗(cid:13)\n\n2 (cid:13) (cid:13) l τ 2(τ\n\n−\n\n−\n\n−\n\nη(t) g ηlτ 3\n1)Lσ2\n\n∗\n\ng ηlτ (F (w(t)) η(t)\n\nF (w∗)) + 3η(t)\n\ng η2\n\nl τ 2σ2\n\n∗\n\n1)L2(F (w(t))\n\n−\n\n− F (w∗)) + 12η(t)\n\ng η3\n\nl τ 2(τ\n\n1)Lσ2\n\n∗\n\n−\n\n(F (w(t))\n\n−\n\nF (w∗)) + 3η(t)\n\ng η2\n\nl τ 2σ2\n\n∗\n\n(52)\n\n(53)\n\n(54)\n\nwhere both (52) and (55) use ηl ≤\n\n1\n\n6τ L , and (53) uses Lemma 6.\n\nRearranging terms and averaging over all rounds we have,\n\n(cid:80)T −1\n\nt=0 η(t)\n\ng F (w(t)) (cid:80)T −1 t=0 η(t)\n\ng\n\n−\n\nF (w∗)\n\n≤\n\nw∗(cid:13) 2\n(cid:13)\n\n3\n\n(cid:13) (cid:13)w(0) (cid:80)T −1\n\n− t=0 η(t)\n\ng ηlτ\n\n+ 9ηlτ σ2\n\n∗ + 36η2\n\nl τ (τ\n\n1)Lσ2\n\n∗ .\n\n(55)\n\n−\n\nThis implies,\n\nF ( ̄w(T ))\n\nF (w∗)\n\n−\n\n≤ O\n\n(cid:32) (cid:13)\n\n(cid:13)w(0) ηlτ (cid:80)T −1\n\nw∗(cid:13) 2\n(cid:13) −\nt=0 η(t)\n\ng\n\n(cid:33)\n\n(cid:0)η2\n\nl τ (τ\n\n+\n\nO\n\n−\n\n1)Lσ2\n\n∗\n\n(cid:1) +\n\nO\n\n(cid:0)ηlτ σ2\n\n∗\n\n(cid:1)\n\n(56)\n\nwhere ̄w(T ) =\n\n(cid:80)T −1\n\ng w(t)\n\nt=0 η(t) (cid:80)T −1\n\nt=0 η(t)\n\ng\n\n. This completes the proof.\n\nC.3 CONVERGENCE ANALYSIS FOR NON-CONVEX OBJECTIVES\n\nOur proof technique is inspired by Wang et al. (2020) and we use one of their intermediate results to bound client drift in non-convex settings as we describe below. We highlight the specific steps where we made adjustments to the analysis of Wang et al. (2020) below.\n\nWe begin by defining the following auxiliary variables that will used in the proof.\n\nNormalized Gradient: h(t)\n\ni =\n\n1 τ\n\nτ −1 (cid:88)\n\nk=0\n\n∇\n\nFi(w(t,k)\n\ni\n\n).\n\nWe also define ̄h(t) = 1\n\nM\n\n(cid:80)M\n\ni=1 h(t)\n\ni\n\n.\n\nLemma 7. (Bounding client drift in Non-Convex Setting)\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n1 8\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤\n\n−\n\n+ 5η2\n\nl L2τ (τ\n\n1)σ2\n\ng .\n\n−\n\n21\n\n(57)\n\n(58)\n\nPublished as a conference paper at ICLR 2023\n\nProof of Lemma 7: Let D = 4η2 Wang et al. (2020),\n\nl L2τ (τ\n\n−\n\n1). We have the following bound from equation (87) in\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n−\n\n≤\n\n1\n\n(cid:13) (cid:13) (cid:13)∇\n\nD\n\n−\n\nD\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\nDσ2 g\nD\n\n1\n\n−\n\n.\n\n(59)\n\nFrom ηl ≤\n\n1\n\n6τ L we have D\n\n1\n\n9 which implies\n\n1\n\n1−D ≤\n\n≤\n\nTherefore we have,\n\n9\n\n8 and D\n\n1−D ≤\n\n1\n\n8 .\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n−\n\n1 8\n\n1 8\n\n(cid:13) (cid:13) (cid:13)∇ (cid:13) (cid:13) (cid:13)∇\n\n≤\n\n≤\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n9D 8\n\nσ2\n\ng\n\nF (w(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ 5η2\n\nl L2τ (τ\n\n1)σ2\n\ng .\n\n−\n\nProof of Theorem 2:\n\nThe update of the global model can be written as follows,\n\nNow using the Lipschitz-smoothness assumption we have,\n\nw(t+1) = w(t)\n\ng ηlτ ̄h(t). η(t)\n\n−\n\nF (w(t+1))\n\nF (w(t))\n\n−\n\nη(t)\n\ng ηlτ\n\n≤ −\n\nη(t)\n\ng ηlτ\n\n≤ −\n\nF (w(t)), ̄h(t)(cid:69)\n\n+\n\nF (w(t)), ̄h(t)\n\n(cid:69)\n\n+\n\n(cid:68)\n\n∇\n\n(cid:68)\n\n∇\n\n(η(t)\n\ng )2η2 2\nl τ 2L\n\nη(t) g η2 2M\n\nl τ 2L\n\n(cid:13) (cid:13) (cid:13)\n\n ̄h(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(60)\n\n(61)\n\n(62)\n\n(63)\n\n(64)\n\nwhere (64) uses η(t)\n\ng\n\n(cid:80)M\n\ni=1\n\nM\n\n∥\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)h(t) ̄h(t)\n\n(cid:13) (cid:13) (cid:13) 2\n\n∥\n\n≤\n\n2\n\n. As in the convex case, inequality (64) is a key step in our proof\n\nand the differentiating factor in our approach from Wang et al. (2020). Following a similar technique (cid:13) ̄h(t)(cid:13) 2\nas Wang et al. (2020) to bound (η(t) 1/2Lτ η(t) g , (cid:13) which cannot be satisfied in our setup due to the adaptive choice of η(t) g . Therefore we first upper bound (η(t) 2 and focus on further bounding this quantity in the rest of the proof, which does not require the aforementioned condition. Note that this comes at the expense of the additional T3 error seen in our final convergence bound in Theorem 2.\n\n/2 will need the condition ηl ≤\n\nl τ 2L2 (cid:13)\n\n(cid:13) ̄h(t)(cid:13) 2\n(cid:13)\n\nl τ 2L (cid:13)\n\nl τ 2L 1\n\n(cid:13)h(t)\n\nby η(t)\n\ng )2η2\n\ng )2η2\n\ng η2\n\n(cid:80)M\n\n2 (cid:46)\n\n(cid:13) (cid:13) (cid:13)\n\ni=1\n\n(cid:13) (cid:13)\n\nM\n\ni\n\nTherefore we have,\n\nF (w(t+1))\n\nF (w(t))\n\n−\n\nη(t)\n\ng ηlτ\n\n≤ −\n\n(cid:68)\n\n∇\n\n(cid:124)\n\nF (w(t)), ̄h(t)(cid:69)\n\n+\n\n(cid:123)(cid:122) T1\n\n(cid:125)\n\nl τ 2L\n\nη(t) g η2 2M\n\nM (cid:88)\n\ni=1 (cid:124)\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni\n\n(cid:123)(cid:122) T2\n\n(65)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n.\n\n(cid:125)\n\nBounding T1\n\nWe have,\n\n(cid:42)\n\nT1 =\n\nF (w(t)),\n\n∇\n\n(cid:43)\n\nh(t)\n\ni\n\n1 M\n\nM (cid:88)\n\ni=0\n\n1 2\n\n1 2\n\n(cid:13) (cid:13) (cid:13)∇\n\n(cid:13) (cid:13) (cid:13)∇\n\n=\n\n≥\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 M\n\n1 2\n\nM (cid:88)\n\ni=1\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∇\n\n1 2\n\nF (w(t))\n\n1 M\n\n−\n\nM (cid:88)\n\ni=1\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n1 2M\n\n−\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n−\n\n22\n\n(66)\n\n(67)\n\n(68)\n\nPublished as a conference paper at ICLR 2023\n\nwhere (67) uses a, b −\n⟩ ⟨\ndefinition of the global objective function F .\n\n2 ∥\n\n2 ∥\n\n2 + 1 ∥\n\n= 1\n\n2 ∥\n\nb\n\na\n\na\n\n1 2 ∥\n\nb\n\n∥\n\n−\n\n2 and (68) uses Jensen’s inequality and the\n\nBounding T2\n\nWe have,\n\nT2 =\n\n=\n\n≤\n\n≤\n\n1 M\n\n1 M\n\n3 M\n\n3 M\n\nM (cid:88)\n\ni=1\n\nM (cid:88)\n\ni=1\n\nM (cid:88)\n\ni=1\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni − ∇\n\nFi(w(t)) +\n\nFi(w(t))\n\n∇\n\n− ∇\n\nF (w(t)) +\n\n∇\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:18)(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni − ∇\n\nFi(w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n(cid:13) (cid:13) (cid:13)∇\n\n− ∇\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni − ∇\n\nFi(w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 3σ2\n\ng + 3\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nF (w(t))\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n(69)\n\n(70)\n\n(71)\n\n(72)\n\nwhere (71) uses Jensen’s inequality, (72) uses bounded data heterogeneity assumption.\n\nHere we would like to note that the bound for T2 is our contribution and is needed in our proof due to the relaxation in (39). The bound for T1 follows a similar technique as in Wang et al. (2020).\n\nSubstituting the T1 and T2 bounds into (65), we have,\n\nF (w(t+1))\n\nF (w(t))\n\n−\n\nη(t)\n\ng ηlτ\n\n≤ −\n\n(cid:32)\n\n1 2\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n1 2M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n−\n\n(73)\n\n(cid:32)\n\n3σ2\n\ng + 3\n\n+\n\nηlτ L 2\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n3 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13)\n\n(cid:13)h(t)\n\ni − ∇\n\n(cid:33) (cid:33)\n\nFi(w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nη(t)\n\ng ηlτ\n\n≤ −\n\n(cid:32)\n\n1 4\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n1 M\n\nM (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13)∇\n\nFi(w(t))\n\nh(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n−\n\n(cid:33)\n\n+ 3ηlτ Lσ2\n\ng\n\nη(t)\n\ng ηlτ\n\n≤ −\n\n(cid:18) 1 8\n\n(cid:13) (cid:13) (cid:13)∇\n\nF (w(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n1\n\n6τ L , (75) uses Lemma 7.\n\n+ 3ηlτ Lσ2\n\ng + 5η2\n\nl L2τ (τ\n\n(cid:19)\n\n1)σ2\n\ng\n\n−\n\n(74)\n\n(75)\n\nwhere (74) uses ηl ≤\n\nThus rearranging terms and averaging over all rounds we have,\n\n(cid:80)T −1\n\nt=0 η(t)\n\ng\n\n(cid:13) (cid:13) ∇\n(cid:80)T −1 t=0 η(t)\n\ng\n\nF (w(t))(cid:13) 2\n(cid:13)\n\nThis implies,\n\n(cid:13) (cid:13) (cid:13)∇\n\nmin t∈[T ]\n\nF (w(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤ O\n\n(cid:32)\n\nThis completes the proof.\n\nF ∗)\n\n8(F (w(0)) (cid:80)T −1\n\n− t=0 η(t) g ηlτ\n\n≤\n\n+ 40η2\n\nl L2τ (τ\n\n−\n\n1)σ2\n\ng + 24ηlLτ σ2 g .\n\n(76)\n\nF ∗)\n\n(F (w(0)) (cid:80)T −1\n\n− t=0 η(t) g ηlτ\n\n(cid:33)\n\n+\n\nO\n\n(cid:0)η2\n\nl L2τ (τ\n\n1)σ2\n\ng\n\n(cid:1) +\n\n−\n\nO\n\n(cid:0)ηlLτ σ2\n\ng\n\n(cid:1) .\n\n(77)\n\nC.4 EXACT PROJECTION WITH GRADIENT DESCENT FOR LINEAR REGRESSION\n\nLet F (w) = that d\n\nd) matrix and b is a n dimensional vector. We assume n here and A has rank n. The singular value decomposition (SVD) of A can be written as,\n\n2 where A is a (n ∥\n\nAw\n\n×\n\n−\n\nb\n\n∥\n\n≥\n\n(cid:20)V⊤ V⊤\n\n1\n\n2\n\n(cid:21)\n\n= UΣ1V⊤\n\n1\n\n(78)\n\nA = UΣV⊤ = U [Σ1 0]\n\n23\n\nPublished as a conference paper at ICLR 2023\n\n×\n\nn) orthogonal matrix, Σ is an (n\n\nn) matrix where U is an (n with orthogonal columns and V2 is a (d n)) matrix with orthogonal columns. Here V1 is a basis for the row space of A, while V2 is a basis for the null space of A. We first prove the following lemmas about the set of minimizers of F (w) and the projection on this set. Lemma 8. The set of minimizers of F (w) is given by,\n\nn) diagonal matrix, V1 is a (d\n\n(d\n\n−\n\n×\n\n×\n\n×\n\n∗ =\n\nS\n\n{\n\nV2V⊤\n\n2 w + V1Σ−1\n\n1 U⊤b |\n\nw\n\nRd\n\n. }\n\n∈\n\nProof. Let w = V2V⊤\n\n2 x + V1Σ−1\n\n1 U⊤b for some x 1 (V2V⊤\n\n∈\n\nAw = UΣ1V⊤ = b\n\nRd. We have,\n\n2 x + V1Σ−1\n\n1 U⊤b)\n\n(79)\n\n(80)\n\n(81)\n\nAw\n\n∥\n\nb\n\n∥\n\n−\n\n2 = 0. Thus\n\nwhere the last line uses V⊤ any w in\n\n∗ is a minimizer of F (w).\n\n1 V2 = 0, V⊤\n\nS\n\n1 V1 = I, UU⊤ = I. This implies\n\nNow let w∗ be a minimizer of F (w), implying Aw∗ = UΣ1V⊤\n\n1 w∗ = b. We have,\n\nw∗ = V2V⊤ = V2V⊤\n\n2 w∗ + V1V⊤ 2 w∗ + V1Σ−1 2 = I and (83) uses UΣ1V⊤\n\n1 w∗ 1 U⊤b 1 w∗ = b. Thus any minimizer of F (w)\n\n(83)\n\n(82)\n\nwhere (82) uses V1V⊤ must lie in\n\n∗.\n\n1 + V2V⊤\n\nS\n\nCombining the above statements we have,\n\nwhich completes the proof.\n\nw is a minimizer of F (w)\n\nw\n\n∗.\n\n∈ S\n\n⇐⇒\n\nLemma 9. The projection of any w\n\nRd on\n\n∗ is given by,\n\n∈ PS ∗ (w) = arg min\n\nw\n\nS w′\n\nw′∈S ∗ ∥\n\n−\n\n2 = V2V⊤ ∥\n\n2 w + V1Σ−1\n\n1 U⊤b.\n\nProof. When w Let x = V2V⊤ have,\n\n∗, it is easy to see that this holds. Therefore we consider the case where w /\n\n∈ S\n\n2 w + V1Σ−1\n\n1 U⊤b and PS ∗ (w) = V2V⊤\n\n2 w0 + V1Σ−1\n\n∗. = w. We\n\n∈ S\n\n1 U⊤b where w0 ̸\n\nV2V⊤\n\n2 w0 −\n\n(cid:13) (cid:13)w −\n(cid:13) (cid:13)V2V⊤ (cid:13)V2V⊤ (cid:13)V2V⊤\n\n= = (cid:13) = (cid:13)\n\n2 (w\n\n2 (w\n\n2 (w\n\n>\n\nw\n\n∥\n\n−\n\n2\n\nx\n\n∥\n\nV1Σ−1\n\n2 (cid:13)\n\n1 U⊤b(cid:13) 1 w −\n(cid:13)V1V⊤ 1 w\n\nw0) + V1V⊤ w0)(cid:13) 2\n(cid:13) w0)(cid:13) 2\n(cid:13)\n\n+ (cid:13)\n\nw\n\n+\n\nx\n\n2 ∥\n\n∥\n\n−\n\n−\n\n−\n\n−\n\nV1Σ−1\n\n(cid:13) 2\n1 U⊤b (cid:13)\n\n(V1V⊤\n\n1 + V2V⊤\n\n2 = I)\n\nV1Σ−1\n\n1 U⊤b(cid:13)\n\n2 (cid:13)\n\n−\n\nleading to a contradiction. The cross term in (88) is zero since V⊤ the definition of x.\n\n1 V2 = 0. Equation (89) follows by\n\nWe now show that running gradient descent on F (w) starting from w with a sufficiently small step size converges to PS ∗ (w). Lemma 10. Let w(0), w(1), . . . be the iterates generated by running gradient descent on F (w) λmax, where λmax is the largest eigen value of A⊤A. Then with w(0) = w and learning rate ηl ≤ limT →∞ w(T ) = PS ∗ (w).\n\nProof. By the gradient descent update we have,\n\nw(t+1) = w(t)\n\n= (I\n\n−\n\nηl(A⊤Aw(t)\n\nA⊤b) −\nηlA⊤A)w(t) + ηlA⊤b.\n\n−\n\n(91)\n\n(92)\n\n24\n\n(84)\n\n(85)\n\n(86)\n\n(87)\n\n(88)\n\n(89)\n\n(90)\n\nPublished as a conference paper at ICLR 2023\n\nTherefore,\n\nw(T ) = (I\n\n−\n\nηlA⊤A)T w(0) + ηl\n\nT −1 (cid:88)\n\nt=0\n\n(I\n\n−\n\nηlA⊤A)tA⊤b\n\n= V(I\n\n−\n\nηlΣ⊤Σ)T V⊤w(0) + ηl\n\nT −1 (cid:88)\n\nt=0\n\nV(I\n\n−\n\nηlΣ⊤Σ)tΣ⊤U⊤b\n\n(93)\n\n(94)\n\n= (V1(I\n\n−\n\nηlΣ2\n\n1)T V1 + V2V⊤\n\n2 )w(0) + ηlV1\n\n(cid:32)T −1 (cid:88)\n\n(I\n\nt=0\n\n−\n\n(cid:33)\n\nηlΣ2\n\n1)t\n\nΣ1U⊤b.\n\n(95)\n\nIn the limit T\n\n→ ∞\n\nand with ηl ≤\n\nλmax, we have,\n\nlim T →∞\n\n(I\n\n−\n\nThus,\n\nηlΣ2\n\n1)T = 0 and lim\n\nT →∞\n\nT −1 (cid:88)\n\n(I\n\nt=0\n\n−\n\nηlΣ2\n\n1)t =\n\n1 ηl\n\nΣ−2 1 .\n\nlim T →∞\n\nw(T ) = V2V⊤\n\n2 w(0) + V1Σ−1\n\n1 U⊤b\n\n= PS ∗ (w(0)) = PS ∗ (w).\n\nC.4.1\n\nIMPROVING LOWER BOUND IN (8) IN THE CASE OF EXACT PROJECTIONS\n\n(96)\n\n(97)\n\n(98)\n\n(99)\n\n∗\n\nS\n\ni be convex and let w∗\n\nLet [M ], i.e., the local models are an exact projection of w(t) on their respective solution sets. From (8) we have,\n\n∈ Si for all i\n\n[M ]. We assume that w(t,τ )\n\ni (w(t))\n\n= PS ∗\n\ni ∀\n\n∈\n\n∈\n\ni\n\n(η(t)\n\ng )opt =\n\nWe can lower bound\n\n(cid:68)\n\nw(t)\n\n−\n\n(cid:80)M\n\ni=1\n\n=\n\nw∗, ∆(t)\n\ni\n\n(cid:68)\n\nw(t)\n\n− (cid:13) ̄∆(t)(cid:13) (cid:13) 2\n(cid:13)\n\nM\n\n(cid:69)\n\n.\n\nw∗, ̄∆(t)(cid:11)\n\n(cid:10)w(t) −\n(cid:13) ̄∆(t)(cid:13) (cid:13) 2\n(cid:13) (cid:69)\n\nw∗, ∆(t)\n\ni\n\nas follows,\n\n(cid:68)\n\nw(t)\n\n−\n\n(cid:69)\n\nw∗, ∆(t)\n\ni\n\n=\n\n=\n\n≥\n\n=\n\n(cid:68)\n\nw(t)\n\n(cid:13) (cid:13) (cid:13)w(t) (cid:13) (cid:13) (cid:13)w(t) (cid:13) (cid:13)\n\n(cid:13)∆(t)\n\ni\n\n−\n\n−\n\n− (cid:13) 2\n(cid:13) (cid:13)\n\nw(t,τ )\n\ni\n\n+ w(t,τ )\n\ni\n\nw∗, w(t)\n\n(cid:69)\n\nw(t,τ )\n\ni\n\n−\n\n− w(t,τ )\n\ni\n\nw∗, w(t)\n\n−\n\n(cid:69)\n\nw(t,τ )\n\ni\n\n−\n\n(cid:68)\n\n+\n\nw(t,τ )\n\ni\n\nw(t,τ )\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n(100)\n\n(101)\n\n(102)\n\n(103)\n\n(104)\n\n(105)\n\nwhere (103) uses the fact that\n\n(cid:68)\n\nw(t,τ )\n\ni\n\ntion (Boyd & Dattarro, 2003).\n\nw∗, w(t)\n\n−\n\n(cid:69)\n\nw(t,τ )\n\ni\n\n−\n\n≥\n\n0 following the properties of projec-\n\nThus we have,\n\n(η(t)\n\ng )opt\n\n≥\n\n(cid:80)M\n\n(cid:13) (cid:13) 2\n(cid:13)∆(t) (cid:13) (cid:13) ̄∆(t)(cid:13) 2\n(cid:13)\n\ni\n\ni=1\n\nM (cid:13)\n\n(106)\n\nNote here the improvement by a factor of 2 in the lower bound compared to (8).\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nD ADDITIONAL EXPERIMENTS AND SETUP DETAILS\n\nOur code is available at the following link https://github.com/Divyansh03/FedExP.\n\nD.1\n\nIMPACT OF AVERAGING ITERATES FOR NEURAL NETWORKS\n\nAs discussed in Section 5, we find that setting the final FedExP model as the average of the last two iterates also improves performance when training neural networks in practical FL scenarios. To demonstrate this, we consider an experiment on the CIFAR-10 dataset with 10 clients, where the data at each client is distributed using a Dirichlet distribution with α = 0.3. We set the number of local steps to be τ = 20 and train a CNN model having the same architecture as outlined in McMahan et al. (2017) with full client participation. Figure 6 shows the training accuracy as a function of the last iterate and the average of last two iterates for FedAvg and FedExP. We see that the last iterate of FedExP has an oscillating behavior that can hide improvements in training accuracy. On the other hand, the average of the last two iterates of FedExP produces a more stable training curve and shows a considerable improvement in the final accuracy. Note however that this improvement only shows for FedExP; averaging iterates does not make significant difference for FedAvg.\n\nFigure 6: Benefit of averaging the last two iterates for FedExP in training a CNN model on CIFAR-10. Note that averaging does not make significant difference for FedAvg.\n\nD.2 DATASET DETAILS\n\nHere we provide more details about the datasets used in Section 6.\n\nSynthetic Linear Regression. by Fi(w) = −\nof clients to be M = 20. Note that since d\n\n2 where Ai ∈\n\nbi∥\n\nAiw\n\n∥\n\nIn this case we assume that the local objective of each client is given R1000. We set the number i=1 ni, this is an overparameterized convex problem. (mi, Id)\n\nR(30×1000), bi ∈\n\nR30 and w\n\n(cid:80)M\n\n≥\n\n∈\n\nTo generate Ai and bi, we follow a similar process as Li et al. (2020). We have (Ai)j: ∼ N and (bi)j = w⊤\n\n(ui, 1), wi ∼ N\n\n(yi, 1), ui ∼ N\n\n(0, 0.1), yi ∼ N\n\ni (Ai)j: where mi ∼ N\n\n(0, 0.1).\n\nEMNIST. EMNIST is an image classification task consisting of handwritten characters associated with 62 labels. The federated EMNIST dataset available at Caldas et al. (2019) is naturally partitioned into 3400 clients based on the identities of the character authors. The number of training and test samples is 671,585 and 77,483 respectively.\n\nCIFAR-10/100. CIFAR-10 is a natural image dataset consisting of 60,000 32x32 images divided into 10 classes. CIFAR-100 uses a finer labeling of the CIFAR images to divide them into 100 classes making it a harder dataset for image classification. In both cases the number of training examples and test examples is 50,000 and 10,000 respectively. To simulate a federated setting, we artificially partition the training data into 100 clients following the procedure outlined in Hsu et al. (2019).\n\nCINIC-10. CINIC-10 is a natural image dataset that can be used as a direct replacement of CIFAR for machine learning tasks. It is intended to act as a harder dataset than CIFAR-10 while being easier than CIFAR-100. The number of training and test examples is both 90,000. We partition the training data into 200 clients in this case, following a similar procedure as for CIFAR.\n\n26\n\n020406080100Training rounds20406080Training Accuracy (%)FedExP (Avg. of last two iterates)FedAvg (Avg. of last two iterates)FedExP (Last iterate)FedAvg (Last iterate)020406080100Training rounds2.02.53.0Server Step Size η(t)gFedExPPublished as a conference paper at ICLR 2023\n\nD.3 HYPERPARAMETER DETAILS\n\nFor our baselines, we find the best performing ηg and ηl by grid-search tuning. For FedExP we search for ε and ηl. This is done by running algorithms for 50 rounds and finding the parameters that achieve the highest training accuracy averaged over the last 10 rounds. We provide details of the grid used below for each experiment below.\n\n{\n\n{\n\nGrid for Synthetic. For FedAvg and SCAFFOLD, the grid for ηg is 10−1, 10−0.5, 10−0, 100.5, 101 the grid for ηg is as (6) is satisfied in this case. The grid for ηl is\n\n{\n\n100, 100.5, 100.5, 101, 102 . For FedAdagrad, {\n. For FedExP we keep ε = 0 in this experiment }\n10−2, 10−1.5, 10−1, 10−0.5, 100 for all algorithms.\n\n}\n\n}\n\nGrid for Neural Network Experiments. For FedAvg and SCAFFOLD the grid for ηg is the grid for ηg\n\nis\n\n10−3, 10−2.5, 10−2, 10−1.5, 10−1\n\n10−2, 10−1.5, 10−1, 10−0.5, 100 . The grid for ηl is\n\n{\n\n10−1, 10−0.5, 100, 100.5, 101\n\n. For FedAdagrad, For FedExP the grid for ε is for all\n\n10−2, 10−1.5, 10−1, 10−0.5, 100\n\n}\n\n{ algorithms.\n\n}\n\n. }\n{\n\n}\n\nWe use lower values of ηg in the grid for FedAdagrad based on observations from Reddi et al. (2021) which show that FedAdagrad performs better with smaller values of the server step size. We provide details of the best performing hyperparameters below.\n\nTable 3: Base-10 logarithm of the best combination of ε and ηl for FedExP and combination of ηl and ηg for baselines. For the synthetic dataset we keep ε = 0 for FedExP.\n\nDataset\n\nSynthetic EMNIST CIFAR-10 CIFAR-100 CINIC-100\n\nFedExP\n\nε\n\n* 1\n3 3\n3\n\n− −\n− −\n\nηl\n\n1 0.5 2\n2 2\n\n− −\n− −\n−\n\nFedAvg ηl\n\nηg\n\nSCAFFOLD ηg\n\nηl\n\n1 0\n0 0\n0\n\n1 0.5 2\n2 2\n\n− −\n− −\n−\n\n1 0\n0 0\n0\n\n1 0.5 2\n2 2\n\n− −\n− −\n−\n\nFedAdagrad\n\nηg\n\n1 0.5 1\n1 1\n\n− −\n− −\n−\n\nηl\n\n1 0.5 2\n2 2\n\n− −\n− −\n−\n\nOther hyperparameters are kept the same for all algorithms. In particular, we apply a weight decay of 0.0001 for all algorithms and decay ηl by a factor of 0.998 in every round. We also use gradient clipping to improve stability of the algorithms as done in previous works (Acar et al., 2021). In all experiments we fix the number of participating clients to be 20, minibatch size to be 50 (for the synthetic dataset this reduces to full-batch gradient descent) and number of local updates τ to be 20.\n\nD.4 SENSITIVITY OF FEDEXP TO ε\n\nTo evaluate the sensitivity of FedExP to ε, we compute the training accuracy of FedExP after 500 rounds for varying ε and on different tasks. For each task, we fix ηl to be the value used in our experiments in Section 6 and only vary ε. The results are summarized below.\n\nTable 4: Training accuracy obtained by FedExP with different choices of ε after 500 rounds of training on various tasks. Value of ηl is fixed for each task (10−0.5 for EMNIST and 10−2 for others). Results averaged over last 10 rounds.\n\nDataset\n\nε = 10−3\n\nε = 10−2.5\n\nε = 10−2\n\nε = 10−1.5\n\nε = 10−1\n\nEMNIST CIFAR-10 CIFAR-100 CINIC-10\n\n85.40 84.79 59.01 66.31\n\n86.26 77.82 44.76 60.93\n\n85.73 77.63 44.21 61.05\n\n85.49 77.66 44.37 60.47\n\n84.90 77.64 44.40 60.96\n\nWe see that the sensitivity of ε is similar to that of the τ parameter which is added to the denominator of FedAdam and FedAdagrad (Reddi et al., 2021) to prevent the step size from blowing up.\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nKeeping ε too large reduces the adaptivity of the method and makes the behavior similar to FedAvg. At the same time, keeping ε too small may not also be beneficial always as seen in the case of EMNIST. In practice, we find that a grid search for ε in the range usually suffices to yield a good value of ε. A general rule of thumb would be to start with ε = 10−3 and increase ε till the performance drops.\n\n10−3, 10−2.5, 10−2, 10−1.5, 10−1\n\n}\n\n{\n\nD.5 ADDITIONAL RESULTS\n\nIn this section, we provide additional results obtained from our experiments.\n\nSynthetic Linear Regression. Note that for the synthetic linear regression experiments there is no test data. Also note that there is no randomness in this experiment since clients compute full-batch gradients with full participation. We provide the plot of η(t) for FedExP in Figure 7. We see that FedExP takes much larger steps in some (but not all) rounds compared to the constant optimum step size taken by our baselines, leading to a large speedup. Recall that we also let ε = 0 in this experiment (since it aligns with our theory) which also explains the larger values of η(t) taken by FedExP in this case.\n\ng\n\ng\n\nFigure 7: Global learning rates for synthetic data with linear regression. Results from a single instance of experiment.\n\nEMNIST. For EMNIST we observe that SCAFFOLD gives slightly better training loss than FedExP towards the end of training. As described in Section 6, extrapolation can be combined with the variance-reduction in SCAFFOLD (the resulting algorithm is referred to as SCAFFOLD-ExP) to further improve performance. This gives the best result in this case as shown in Figure 8.\n\nFigure 8: Additional results for EMNIST dataset. Mean and standard deviation from experiments with 20 different random seeds. The shaded areas show the standard deviation.\n\n28\n\n0255075100125150175200Training rounds050100150Server Step Size η(t)gFedExP (ours)FedAvgSCAFFOLDFedAdagrad0250500Training rounds0.400.450.500.550.600.650.70Training LossFedExP (ours)SCAFFOLD-ExP (ours)FedAvgSCAFFOLDFedAdagrad0250500Training rounds80818283848586Training Accuracy (%)0250500Training rounds123Server Step Size η(t)gPublished as a conference paper at ICLR 2023\n\nCIFAR-10, CIFAR-100 and CINIC-10. From Figure 3 and Figures 9–11, we see that FedExP comprehensively outperforms baselines in these cases, achieving almost 10%–20% higher accuracy than the closest baseline by the end of training. The margin of improvement is most in CIFAR-100, which can be considered as the toughest dataset in our experiments. This points to the practical utility of FedExP even in challenging FL scenarios.\n\nFigure 9: Additional results for CIFAR-10 dataset. Mean and standard deviation from experiments with 5 different random seeds. The shaded areas show the standard deviation.\n\nFigure 10: Additional results for CIFAR-100 dataset. Mean and standard deviation from experiments with 5 different random seeds. The shaded areas show the standard deviation.\n\nFigure 11: Additional results for CINIC-10 dataset. Mean and standard deviation from experiments with 5 different random seeds. The shaded areas show the standard deviation.\n\n29\n\n0250500Training rounds0.40.60.81.01.21.4Training LossFedExP (ours)FedAvgSCAFFOLDFedAdagrad0250500Training rounds304050607080Training Accuracy (%)0250500Training rounds0.00.51.01.52.0Server Step Size η(t)g0250500Training rounds234Training LossFedExP (ours)FedAvgSCAFFOLDFedAdagrad0250500Training rounds2030405060Training Accuracy (%)0250500Training rounds0.00.51.01.52.0Server Step Size η(t)g0250500Training rounds0.81.01.21.41.61.8Training LossFedExP (ours)FedAvgSCAFFOLDFedAdagrad0250500Training rounds404550556065Training Accuracy (%)0250500Training rounds0.51.01.52.0Server Step Size η(t)gPublished as a conference paper at ICLR 2023\n\nLong-Term Behavior of Algorithms and Comparison with FedProx. To evaluate the long-term behavior of different algorithms, we ran the experiments for 2000 rounds. Here, we also consider an additional algorithm, namely FedProx, for comparison. For fair comparison, we have tuned the μ parameter of FedProx for each dataset, by doing a grid search over the range }\nas done in the original FedProx paper (Li et al., 2020). The results of EMNIST, CIFAR-10, CIFAR100, and CINIC-10 in Figures 12–14 and Table 5 are from experiments with 3 different random seeds. Except for the synthetic dataset, the plots show mean and standard deviation values across all the random seeds and also over a moving average window of size 20.\n\n10−3, 10−2, 10−1, 1\n\n{\n\nFigure 12: Training loss results of FedExP, FedAvg, SCAFFOLD, FedAdagrad and FedProx on the Synthetic, EMNIST, CIFAR-10,CIFAR-100 and CINIC-10 datasets for 2000 rounds.\n\nFigure 13: Training accuracy results of FedExP, FedAvg, SCAFFOLD, FedAdagrad and FedProx on the EMNIST, CIFAR-10, CIFAR-100 and CINIC-10 datasets for 2000 rounds.\n\nFigure 14: Test accuracy results of FedExP, FedAvg, SCAFFOLD, FedAdagrad and FedProx on the EMNIST, CIFAR-10, CIFAR-100 and CINIC-10 datasets for 2000 rounds.\n\n30\n\n010002000Training rounds10−2510−1910−1310−710−1Mean Squared ErrorSyntheticFedExP (ours)FedAvgSCAFFOLDFedAdagradFedProx010002000Training rounds0.30.40.50.6Training LossEMNIST010002000Training rounds1.00.20.30.40.50.60.70.80.92.0Training LossCIFAR-10010002000Training rounds1.00.70.80.92.03.04.0Training LossCIFAR-100010002000Training rounds1.00.70.80.92.0Training LossCINIC-100500100015002000Training rounds8485868788Training Accuracy (%)EMNISTFedExP (ours)FedAvgSCAFFOLDFedAdagradFedProx0500100015002000Training rounds707580859095Training Accuracy (%)CIFAR-100500100015002000Training rounds4050607080Training Accuracy (%)CIFAR-1000500100015002000Training rounds40506070Training Accuracy (%)CINIC-100500100015002000Training rounds84858687Test Accuracy (%)EMNISTFedExP (ours)FedAvgSCAFFOLDFedAdagradFedProx0500100015002000Training rounds70.072.575.077.580.082.585.0Test Accuracy (%)CIFAR-100500100015002000Training rounds40455055Test Accuracy (%)CIFAR-1000500100015002000Training rounds40455055606570Test Accuracy (%)CINIC-10Published as a conference paper at ICLR 2023\n\nTable 5: Test accuracy obtained by FedExP and baselines after 2000 rounds of training on various tasks. Results are averaged across 3 random seeds and last 20 rounds.\n\nDataset\n\nEMNIST CIFAR-10 CIFAR-100 CINIC-10\n\nFedExP\n\nFedAvg\n\nSCAFFOLD\n\nFedAdagrad\n\nFedProx\n\n86.96 82.94 54.65 66.45\n\n0.58 0.42 0.49 1.28\n\n± ±\n± ±\n\n85.78 80.10 49.63 64.87\n\n0.35 0.56 0.37 0.44\n\n± ±\n± ±\n\n86.22 82.02 49.40 64.61\n\n0.35 0.30 0.38 0.49\n\n± ±\n± ±\n\n85.53 80.21 49.64 64.87\n\n1.04 0.60 0.39 0.44\n\n± ±\n± ±\n\n85.77 80.16 49.47 64.52\n\n0.39 0.59 0.31 0.45\n\n± ±\n± ±\n\nWe see that FedExP continues to outperform baselines including FedProx in the long-term behavior as well.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nE COMBINING EXTRAPOLATION WITH SCAFFOLD\n\nAs described in Section 6, the extrapolation step can be added to the SCAFFOLD algorithm in a similar way as FedExP. The detailed steps of this SCAFFOLD-ExP algorithm are shown in Algorithm 2.\n\nAlgorithm 2 SCAFFOLD-ExP\n\ni\n\n,\n\ni ∀\n\n∈\n\n[M ], number of rounds T , local iteration steps τ ,\n\n1: Input: w(0), control variate c(0), c(0)\n\nparameters ηl, ε 2: For t = 0, . . . , T 3: 4: 5:\n\nGlobal server do: Send w(t), c(t) to all clients Clients i\n\n−\n\n[M ] in parallel do:\n\n1 communication rounds do:\n\n6: 7:\n\n8:\n\n9:\n\n10:\n\n11: 12:\n\n13:\n\n14:\n\n15:\n\n16:\n\n∈ Set w(t,0) i ← For k = 0, . . . , τ\n\nw(t,0)\n\n− Update w(t,k+1)\n\ni\n\n1 local iterations do:\n\nw(t,k)\n\ni\n\n− w(t,τ )\n\n(cid:16)\n\nηl ∇\nand Ψ(t)\n\n← w(t)\n\nCompute ∆(t) Send ∆(t) Update local control variate c(t+1)\n\ni ← and Ψ(t)\n\nto the server\n\n−\n\ni\n\ni\n\ni\n\ni ←\n\nGlobal server do: Compute ̄∆(t)\n\n1 M\n\n(cid:80)M\n\ni=1 ∆(t)\n\ni\n\nand η(t)\n\ng\n\n← Update global model with w(t+1) Compute ̄Ψ(t) Update global control variate with c(t+1)\n\ni=1 Ψ(t)\n\n(cid:80)M\n\n1 M\n\n←\n\n←\n\n← w(t)\n\ni\n\nFi(w(t,k)\n\ni\n\n, ξ(t,k)\n\ni\n\n)\n\nc(t)\n\n1 τ ηl\n\n−\n\nΨ(t)\n\ni\n\ni ←\n\nc(t) i −\n\nmax η(t) g ̄∆(t)\n\n−\n\nc(t)\n\n−\n\n←\n\n ̄Ψ(t)\n\ni + c(t)(cid:17) c(t)\n\n− ∆(t)\n\ni\n\n(cid:110)\n\n1, (cid:80)M\n\ni=1\n\n(cid:13) (cid:13)∆(t)\n\ni\n\n2(cid:46)\n\n(cid:13) (cid:13)\n\n2M\n\n(cid:16)(cid:13) (cid:13) ̄∆(t)(cid:13) 2\n(cid:13)\n\n+ε\n\n(cid:17)(cid:111)\n\n32\n\nPublished as a conference paper at ICLR 2023\n\nF COMBINING EXTRAPOLATION WITH SERVER MOMENTUM\n\nWe begin by recalling some notation from our work. The vector w(t) is the global model at round t and ̄∆(t) is the average of client updates at round t. The server momentum update at round t can be written as v(t) = ̄∆(t) + βv(t−1) (let v−1 = 0) and the global model update can be written as w(t+1) = w(t) . We have,\n\nthat minimizes (cid:13)\n\n(cid:13)w(t+1)\n\nw∗(cid:13) 2\n(cid:13)\n\n(cid:13) (cid:13) (cid:13)w(t+1)\n\n−\n\n−\n\nη(t) g v(t). Our goal is now to find η(t) g )2 (cid:13) w∗(cid:13)\n\n+ (η(t)\n\nw∗(cid:13)\n\n(cid:13) (cid:13) (cid:13)w(t)\n\n2 (cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n=\n\n(cid:13)\n\ng\n\n(cid:13)v(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\n−\n\n2η(t) g ⟨\n\n−\n\nw(t)\n\n−\n\nw∗, v(t)\n\n. ⟩\n\n(107)\n\nSetting the derivative of the RHS of (107) to zero we have,\n\n(η(t)\n\ng )opt =\n\nw∗, v(t)(cid:11)\n\n(cid:10)w(t) −\n(cid:13)v(t)(cid:13) (cid:13) 2\n(cid:13)\n\n.\n\n(108)\n\nOur goal now is to find a lower bound on\n\nLemma 11. Assume that (cid:10)w(t)\n\n−\n\nw∗, ̄∆(t)(cid:11) k=0(β/2)r−km(k))/2 (cid:13)\n\n≥\n\n(cid:13)v(r)(cid:13) 2\n(cid:13)\n\n(m(r) + (cid:80)r−1\n\nw(t)\n\n⟨\n\nw∗, v(t)\n\n. We have the following lemma. −\n⟩ m(t) = (cid:80)M\n\n/M (see Appendix C.4.1) for\n\n(cid:13) (cid:13)\n\n(cid:13)∆(t)\n\ni\n\n(cid:13) 2\n(cid:13) (cid:13)\n\ni=1\n\nfor all r < t\n\n1. Then,\n\n−\n\n(cid:68)\n\nw(t)\n\n−\n\nw∗, v(t)(cid:69)\n\nm(t) +\n\n≥\n\nt−1 (cid:88)\n\n(β/2)t−km(k),\n\nk=0\n\n(η(t)\n\ng )opt\n\nm(t) + (cid:80)t−1 k=0(β/2)t−km(k) (cid:13)v(t)(cid:13) 2 (cid:13) 2\n(cid:13)\n\n.\n\n≥\n\n(109)\n\n(110)\n\nall t\n\n≥\n\n0 and η(r)\n\ng\n\n≤\n\nwhich implies,\n\nProof. We proceed via a proof by induction. The statement clearly holds at t = 0 since (cid:10)w(0)\n\nw∗, v(0)(cid:11) = (cid:10)w(0)\n\nw∗, ̄∆(0)(cid:11)\n\nm(0).\n\n−\n\nNow assuming the lemma holds at t\n\n1 we have,\n\n(cid:68)\n\nw(t)\n\n−\n\nw∗, v(t)(cid:69)\n\n=\n\n=\n\n≥\n\n≥\n\n−\n\n≥\n\n−\n\n(cid:68)\n\n(cid:68)\n\nw(t)\n\nw(t)\n\n−\n\n−\n\nm(t) + β\n\nw∗, ̄∆(t)(cid:69) w∗, ̄∆(t)(cid:69) (cid:20)(cid:68)\n\nw(t−1)\n\nt−1 (cid:88)\n\nm(t) +\n\n(β/2)t−km(k),\n\nk=0\n\n(cid:68)\n\n(cid:68)\n\nw(t)\n\n− w(t−1)\n\n+ β\n\n+ β\n\nw∗, v(t−1)(cid:69)\n\nη(t−1)\n\ng\n\nv(t−1)\n\n−\n\nw∗, v(t−1)(cid:69)\n\n−\n\nη(t−1)\n\ng\n\n−\n\n(cid:13) (cid:13)\n\n(cid:13)v(t−1)(cid:13)\n\n(cid:13) (cid:13)\n\nw∗, v(t−1)(cid:69)\n\n−\n\n2(cid:21)\n\n(111)\n\n(112)\n\n(113)\n\n(114)\n\nline follows from the fact\n\nwhere the last (cid:80)t−2\n\nk=0(β/2)t−1−km(k) and η(t−1)\n\n≥ (cid:13)v(t−1)(cid:13) 2\n(cid:13) Thus we propose to keep the following server step size when using server momentum,\n\n(m(t−1) + (cid:80)t−2\n\n≤\n\ng\n\nthat (cid:10)w(t−1) k=0(β/2)t−1−km(k))/2 (cid:13)\n\nw∗, v(t−1)(cid:11)\n\n−\n\nm(t−1) +\n\n.\n\nη(t)\n\ng =\n\nm(t) + (cid:80)t−1\n\nk=0(β/2)t−km(k) (cid:13)v(t)(cid:13) 2\n+ ε) (cid:13)\n\n2((cid:13)\n\n,\n\n(115)\n\nwhere m(t) = (cid:80)M /M . Note that we also add a small constant ε to the denominator to prevent the step size from blowing up as done for FedExP. We call server momentum with this step size as FedExP-M.\n\n(cid:13)∆(t)\n\ni=1\n\ni\n\n(cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nWe compare the performance of FedExP-M with FedAdam and FedAvg-M (FedAvg with server momentum) on the CIFAR-10 and CIFAR-100 datasets as shown in Figures 15–17, where the mean and standard deviation values are computed over 3 random seeds and a moving average window of\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nsize 20. The experimental setup is the same as described in Section 6. The hyperparameters ηl, ε for FedExP-M and ηl, ηg for FedAdam and FedAvg-M were tuned following a similar process as described in Appendix D.3, and their resulting values are in Table 6.\n\nTable 6: Base-10 logarithm of the best combination of ε and ηl for FedExP-M and combination of ηl and ηg for FedAdam and FedAvg-M.\n\nDataset\n\nε\n\n−3 CIFAR-10 CIFAR-100 −3\n\nFedExP\n\nηl\n\n−2 −2\n\nFedAdam ηl\n\nηg\n\nFedAvgm-M ηg\n\nηl\n\n−2 −2\n\n−2 −2\n\n0 0\n\n−2 −2\n\nFigure 15: Training loss results of FedExP-M, FedAdam and FedAvg-M on the CIFAR10 and CIFAR100 datasets.\n\nFigure 16: Training accuracy results of FedExP-M, FedAdam and FedAvg-M on the CIFAR10 and CIFAR100 datasets.\n\nFigure 17: Test accuracy results of FedExP-M, FedAdam and FedAvg-M on the CIFAR10 and CIFAR100 datasets.\n\nOur result shows that server momentum can be successfully combined with extrapolation for the best speed-up among all baselines. The behavior of FedAdam and FedAvg-M are quite similar in these experiments which can be attributed to the dense nature of the gradients in image classification as\n\n34\n\n0500100015002000Training rounds10−210−1100Training LossCIFAR-10FedExP-M (ours)FedAvg-MFedAdam0500100015002000Training rounds10−210−1100Training LossCIFAR-1000500100015002000Training rounds859095100Training Accuracy (%)CIFAR-10FedExP-M (ours)FedAvg-MFedAdam0500100015002000Training rounds859095100Training Accuracy (%)CIFAR-1000500100015002000Training rounds808284868890Test Accuracy (%)CIFAR-10FedExP-M (ours)FedAvg-MFedAdam0500100015002000Training rounds50525456586062Test Accuracy (%)CIFAR-100Published as a conference paper at ICLR 2023\n\ndiscussed in Section 6. We note that this is only a preliminary result and future work will look to study the effect of combining server momentum and extrapolation more rigorously.\n\n35",
    "reference": "# Summary Of The Paper\n\n- In federated learning settings, aggregating the learning of all participating clients has always been a challenging issue in cases of data heterogeneity across clients which is very probable in real world scenarios. To tackle this, recent works have optimized the server aggregation process of FedAvg algorithm by treating client updates as \"pseudo gradients\" with fixed server step size. However, using a fixed step size is not ideal for varying data distribution across clients.\n- To tackle this issue, this paper proposes a novel federated learning algorithm FedExP which adaptively determines the server step size in each round of federated learning such that it can handle varying degree of similarity in data distribution across clients. In doing so, the authors first establish a novel connection between FedAvg and POCS algorithms for overparameterized convex objectives and further draw the inspiration from the EPPM extrapolation algorithm (used to speed up POCS algorithm) resulting in adaptively determining server step size in FedExP for better and fast convergence of global model.\n- Key feature of FedExP is that there is virtually no requirement for additional communication, computation, or storage at clients or the server.\n- Experimental evaluation demonstrates that FedExP algorithm converges faster and consistently outperforms baselines over 5 datasets.\n- novelty is simple and limited yet effectively applied to federated learning paradigm and have shown improved performance in terms of speed-up/efficiency and client-server data communication\n\n# Strength And Weaknesses\n\nStrengths:\n- novelty is simple yet effective application in federated learning paradigm\n- The motivation is well founded and the claims are sound.\n- Paper is clearly presented and easy to follow.\n- Mathematical formulation is very detailed and explanatory.\n- Proposed FedExP model is effective and efficient.\n- Proposed FedExP model consistently outperforms two strong federated learning baseline methods on global model convergence time and test accuracy over multiple datasets.\n\nWeaknesses:\n- inspired by related works, especially the application of extrapolation in federated learning\n- novelty is limited (yet effective)\n- Missing comparison with the relevant FedProx method.\n\tLi, Tian, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. \"Federated optimization in heterogeneous networks.\" Proceedings of Machine Learning and Systems 2 (2020): 429-450.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nQuality:\n- Recently proposed federated learning algorithm FedProx has tackled the data heterogeneity problem by adapting the optimization objective of FedAvg on local clients. Although FedProx increases computation on client-side, it still has similar motivation. So, it would be interesting to see the comparison of FedExp with FedProx method.\n\nClarity:\n- A figure of the proposed FedExP model with client-server communication would improve understanding of the method setup and working.\n- A table of notations would improve readability of the mathematical formulation.\n\nNovelty:\n- Drawing a connection between FedAvg method and POCS algorithm is novel. Furthermore, adaptation and application of EPPM extrapolation algorithm in FedAvg method for adaptive server step size in each round is novel too.\n- novelty is simple yet effective application in federated learning paradigm\n\nReproducibility:\n- Experimental setup and hyperparameter settings are described in detail.\n\n# Summary Of The Review\n\nThe dynamic tuning of server step size results in efficient (faster) convergence and performance boost across multiple datasets while efficiently tackling the issue of knowledge interference during server aggregation due to heterogeneous data distribution across clients.\n\n- The novelty is limited and simple yet effective in Federated learning paradigm \n- the proposed method is effective both in terms on efficiency and prediction performance \n- addresses the sustainability concerns and may be applicable and scale when a number of clients participate in Federated averaging, especially in IOT scenarios and cloud-edge continuum\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nPROBABILISTIC CATEGORICAL ADVERSARIAL ATTACK & ADVERSARIAL TRAINING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nThe existence of adversarial examples brings huge concern for people to apply Deep Neural Networks (DNNs) in safety-critical tasks. However, how to generate adversarial examples with categorical data is an important problem but lack of extensive exploration. Previously established methods leverage greedy search method, which can be very time-consuming to conduct successful attack. This also limits the development of adversarial training and potential defenses for categorical data. To tackle this problem, we propose Probabilistic Categorical Adversarial Attack (PCAA), which transfers the discrete optimization problem to a continuous problem that can be solved efficiently by Projected Gradient Descent. In our paper, we theoretically analyze its optimality and time complexity to demonstrate its significant advantage over current greedy based attacks. Moreover, based on our attack, we propose an efficient adversarial training framework. Through a comprehensive empirical study, we justify the effectiveness of our proposed attack and defense algorithms.\n\n1\n\nINTRODUCTION\n\nAdversarial attacks (Goodfellow et al., 2015) have raised great concerns for the applications of Deep Neural Networks(DNNs) in many security-critical domains (Cui et al., 2019; Stringhini et al., 2010; Cao & Tay, 2001). The majority of existing methods focus on differentiable models and continuous input space, where we can apply gradient-based approaches to generate adversarial examples. However, there are many machine learning tasks where the input data are categorical. For example, data in ML-based intrusion detection systems (Khraisat et al., 2019) contains records of system operations; and in financial transaction systems, data includes categorical information such as the types of transactions. Therefore, how to explore potential attacks and corresponding defenses for categorical inputs is also desired. Existing methods introduce search-based approaches for categorical adversarial attacks (Yang et al., 2020b; Lei et al., 2019a). For example, the method in (Yang et al., 2020a) first finds top-K features of a given sample that have the maximal influence on the model output, and then, a greedy search is applied to obtain the optimal combination of perturbation in these K features. However, these search-based methods cannot be guaranteed to find the strongest adversarial examples. Moreover, they can be computationally expensive, especially when data is high dimensional and the number of categories for each feature is large.\n\nIn this paper, we propose a novel Probabilistic Categorical Adversarial Attack (PCAA) algorithm to generate categorical adversarial examples by estimating their probabilistic distribution. In detail, given a clean sample, we assume that (each feature of) the adversarial example follows a categorical distribution, and satisfies: (1) the generated samples following this distribution have a high expected loss value and (2) the generated samples only have a few features which are different from the original clean sample. (See Section 3 for more details.) In this way, we transfer the categorical adversarial attack in the discrete space to an optimization problem in a continuous probabilistic space. Thus, we are able to apply gradient-based methods such as (Madry et al., 2017) to find adversarial examples. On one hand, the distribution of adversarial examples in PCAA is searched in the whole space of allowed perturbations. This can facilitate our method to find stronger adversarial examples (with higher loss value) than the greedy search methods (Yang et al., 2020b). Moreover, when the dimension of input data expands, the increase of computational cost of PCAA will be significantly slower than search-based methods (Section 3.4). Therefore, our method can enjoy good attacking optimality and computational efficiency simultaneously. For example, in our experiments\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nin Section 5.1, our PCAA method is the only attacking method that has the highest (or close to highest) attacking successful rate while maintaining the low computational cost.\n\nThe advantages of PCAA allow us to further devise an adversarial training method (PAdvT), by repeatedly (by mini-batches) generating adversarial examples using PCAA. Empirically, PAdvT achieves promising robustness on different datasets over various attacks. For example, on AG’s news dataset, we outperform representative baselines with significant margins, approximately 10% improvement in model robustness compared with the best defense baseline; on IMDB dataset, we obtain comparable performance with ASCC-defense(Dong et al., 2021a), which is a SOTA defense method via embedding space adversarial training. However, compared to ASCC, our method does not rely on the assumption that similar words have a close distance in the embedding space. Compared to other defenses, our defense has much better robustness by more than 15%. Our main contributions can be summarized below:\n\n• We propose a time-efficient probabilistic attacking method (PCAA) for models with cate-\n\ngorical input.\n\n• Based on PCAA, we devise a probabilistic adversarial training method to defend categorical\n\nadversarial attacks.\n\n2 RELATED WORK\n\nAttacks on categorical data. There has been a rise in the importance of the robustness of machine learning in recent years. On the one hand, evasion attack, poison attack, adversarial training, and other robustness problem with continuous input space have been well studied especially in the image domain (Shafahi et al., 2018; Madry et al., 2017; Ilyas et al., 2019). On the other hand, adversarial attacks focusing on discrete input data, like text data, which have categorical features, are also starting to catch the attention of researchers. Kuleshov et al. (2018) discussed the problems of attacking text data and highlighted the importance of investment into discrete input data. Ebrahimi et al. (2017b) proposed to modify the text token based on the gradient of input one-hot vectors. Gao et al. (2018) developed a scoring function to select the most effective attack and a simple characterlevel transformation to replace projected gradient or multiple linguistic-driven steps on text data. Samanta & Mehta (2017) proposed an algorithm to generate a meaningful adversarial sample that is legitimate in the text domain. Yang et al. (2020a) proposed a two-stage probabilistic framework to attack discrete data. Lei et al. (2019b) and Yang et al. (2020a) improved the greedy search and proposed two different greedy attack methods. Alzantot et al. (2018) proposed a black-box attack to generate syntactically similar adversarial examples. However, the previous methods usually search in a greedy way, which has exponential time complexity.\n\nDefenses on categorical data. There are also a lot of defense methods in the continuous data domain compared with discrete data. For example, the most effective one is adversarial training Madry et al. (2017), which searches for the worst case by PGD during the training and trains on the worst adversarial examples to increase the robustness of the models. Several works have been proposed for categorical adversarial defenses. Pruthi et al. (2019) used a word recognition model to preprocess the input data and increased the robustness of downstream tasks. Zhou et al. (2020) proposed to use random smoothing to defense substitution-based attacks. Wang et al. (2021) detected adversarial examples to defend synonym substitution attacks based on the fact that word replacement can destroy mutual interaction. Swenor & Kalita (2022) used random noise to increase the model robustness. Xie et al. (2022) used a huge amount of data to detect the adversarial examples. Dong et al. (2021b) combined different attack methods, such as Hot-flip (Ebrahimi et al., 2017a) and l2-attack (Miyato et al., 2017), and adversarial training as the defense method. It also proposed its own adversarial method ASCC which takes the solution space as a convex hull of word vectors and achieves good performance in sentiment analysis and natural language inference. However, the defense methods are mainly focusing on NLP, which may rely on word embeddings.\n\n3 PROBABLISTIC CATEGORICAL ADVERSARIAL ATTACK (PCAA)\n\n3.1 PROBLEM SETUP\n\nCategorical Attack. We first introduce necessary definitions and notations. In detail, we consider a classifier f that predicts labels y ∈ Y based on categorical inputs x ∈ X . Each input sample x\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: The overall framework of Probabilistic Categorical Adversarial Attack. First, a distribution π is used to sample adversarial examples x′. Then, the gradient of loss function will be used to update the probability distribution π.\n\ncontains n categorical features, and each feature xi take a value from d categorical values. In this paper, we consider l0-attack, which restricts the number of perturbed features. In particular, given the budget size ε, we aim to find an adversarial example x′ which solves the following optimization problem:\n\nmax L(f (x′), y) s.t. ∥x′ − x∥0 ≤ ε.\n\n(1)\n\nExisting search-based methods. To solve the problem, there existing search-based methods such as Greedy Search (Yang et al., 2020a) and Gradient-Guided Search (Lei et al., 2019a). In general, they consist of two major steps. First, they search for features in x whose change can result in the greatest influence on the model output. This can be estimated by either perturbing a feature xi, or calculating the scale of the model’s gradient to the feature xi. The second step involves greedy or global searches among all possible combinations of the features identified by the first step. This process is time-consuming if the feature dimension n or the number of categories d is large.\n\n3.2 OBJECTIVE OF PROBABILISTIC CATEGORICAL ADVERSARIAL ATTACK (PCAA)\n\nProbabilistic Categorical Attack. In this work, we propose an alternative approach to solve the problem in Eq.(1), by transferring it to the continuous probabilistic space. In detail, we assume that each feature of (adversarial) categorical data x′ i follows a categorical distribution: Categorical(πi), where πi ∈ Πi = (0, 1)d. And each πi,j represents the probability that the feature i belongs to the category j. In the remaining of the paper, we will use πi to denote the categorical distribution Categorical(πi) without the loss of generality. Therefore, each input sample x’s distribution can be represented as an array π = [π0; π1; ...; πn] ∈ Π ⊂ Rn×d, where the element πi.j represents the probability that the feature xi belongs to the category j. Then, we define the following optimization problem to find a probability distribution π in the space of Π:\n\nEx′∼πL(f (x′), y),\n\nmax π∈Π\n\ns.t. Pr x′∼π\n\n(∥x′ − x∥0 ≥ ε) ≤ δ\n\n(2)\n\nwhere ε denotes the perturbation budget size and δ is the tail probability constraint. By solving the problem in Eq.(2), we aim to find a distribution with parameter π such that: (1) on average, the generated samples x′ following distribution π have a high loss value; and (2) with low probability, the sample x′ has a l0 distance to the clean sample x larger than ε. In this way, the generated samples x′ are likely to mislead the model prediction while preserving most features of x.\n\n3.3 AN EFFICIENT ALGORITHM OF PCAA\n\nIn this subsection, we provide a feasible algorithm to solve our proposed Eq.(2) in practice. First, in order to handle the probability constraint in Eq.(2), we substitute the l0 distance between x′ and x by calculating the sum of Cross Entropy Loss between πi and xi: LCE(πi, xi) for all features i ∈ |x|. It is because LCE(πi, xi) measures the probability that the i-th feature of the generated samples is different from xi. Therefore, we use the sum of Cross Entropy (cid:80) i∈|n| LCE(πi, xi) to approximate the number of changed features, which is the l0 difference ||x′ − x||0. In our algorithm, we penalize the searched π when the term (cid:80) i∈|n| LCE(πi, xi) exceeds a positive value ζ (as Eq.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3). In this case, we equivalently limit the probability that the generated samples x′ have the number of perturbed features larger than ε.\n\n(||x′ − x|| ≥ ε) →\n\nPr x′∼π\n\n\n\n\n\n(cid:88)\n\ni∈|n|\n\n\n\n+\n\nLCE(xi, πi) − ζ\n\n\n\n(3)\n\nMoreover, since the Cross-Entropy Loss is differentiable in terms of π, we further transform the problem to its Lagrangian form:\n\nmax π\n\nEx′∼πL(f (x′), y) − λ\n\n\n\n\n\n(cid:88)\n\ni∈|n|\n\n\n\n+\n\nLCE(xi, πi) − ζ\n\n\n\n(4)\n\nwhere λ is the penalty coefficient, and [·]+ is max(·, 0). Next, we will show how to solve the maximization problem above by applying gradient methods.\n\nBack propagation through Gumbel-Softmax. Note that the gradient of the expected loss function with respect to π cannot be directly calculated in Eq. 4, so we apply the Gumbel-Softmax estimator Jang et al. (2017). In practice, to avoid the projection onto probability simplex which hurts the time efficiency and Gumbel-Softmax does not require a normalized probability distribution, we consider an unnormalized categorical distribution πi ∈ (0, C]d where C > 0 is a large constant guaranteeing that the searching space is sufficiently large. The distribution generates sample vectors x′\n\ni as follows:\n\nx′\n\nij =\n\nexp((log πij + gj)/τ ) j=1 exp((log πij + gj)/τ )\n\n(cid:80)d\n\n,\n\nfor j = 1, ..., d\n\n(5)\n\nwhere gj denotes i.i.d samples from the Gumbel(0, 1) distribution, and τ denotes the softmax temperature. This re-parameterization process facilitates us to calculate the gradient of the expected loss in terms of π. Therefore, we can derive the following estimator of gradients for the expected loss:\n\n∂Ex′∼πL(f (x′), y) ∂π\n\n≈\n\n∂ ∂π\n\nEgL(f (x′(π, g)), y) = Eg\n\n(cid:21)\n\n(cid:20) ∂L ∂x′\n\n∂x′ ∂π\n\n≈\n\n1 ng\n\nng (cid:88)\n\ni=1\n\n(cid:20) ∂L ∂x′\n\n∂x′(π, gi) ∂π\n\n(cid:21)\n\n(6)\n\nwhere ng is the number of i.i.d samples from g. In Eq. 6, the first approximation is from the reparameterization of a sample x′; the second equality comes from exchanging the order of expectation and derivative, and the third, we approximate the expectation of gradients by calculating the average of gradients. Finally, we derive the practical solution to solve Eq. 4 as demonstrated in Figure 1, by leveraging the gradient ascent algorithm, such as Madry et al. (2018). In Algorithm 1, we provide the details of our proposed attack method. Specifically, in each step, we first update the unnormalized distribution π by gradient ascent. Then, we clip π back to its domain (0, C]d.\n\nAlgorithm 1 Probabilistic Categorical Adversarial Attack (PCAA) Input Data D, budget ε, number of samples ng, penalty coefficient λ, maximum iteration I, learning rate γ Output Adversarial Distribution π\n\nInitialize distribution π0 for t ≤ I do\n\nend for Return distribution π\n\nEstimate expected gradient using Eq6: ∇πEπL ≈ 1 Gradient ascent: (cid:101)πt+1 = πt + γ · ∇π (EπL − λ∇π[LCE(πt, x) − ζ]+) Clip to (0, C]d: πt+1 = max((cid:101)πt+1, C)\n\ni=1\n\n∂x′\n\nng\n\n(cid:80)ng\n\n(cid:104) ∂L\n\n(cid:105)\n\n∂x′(πt,gi) ∂π\n\n3.4 TIME COMPLEXITY ANALYSIS\n\nIn this subsection, we compare the time complexity of our method with search-based methods to illustrate the efficiency advantage of our proposed attack method. Here, we assume that the whole dataset has N data points, each data point has n features, each feature has d categories and the budget of allowed perturbation is ε. We first introduce the details of four different search-based methods.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n• Greedy Search(GS). It consists of two stages. The first stage involves traversing all features. For the ith feature, it replaces the original category with all other d − 1 categories respectively and records the change of the model loss for each category. The largest change is treated as the impact score for the ith feature. Then it selects the top ε features with the highest impact scores to perturb. In the second stage, it finds the combination with the greatest loss among all possible combinations of categories for selected features.\n\n• Greedy attack.(GA)(Yang et al., 2020a). This method is a modified version of Greedy Search. The first stage is similar to that of GS, while the second stage searches for the best perturbation feature by feature. For the ith selected feature, it replaces the original category with one that results in the largest loss and then searches the next selected feature until all selected features are traversed. Therefore, it only needs to go over each feature once without traversing all combinations in the second stage.\n\n• Gradient-guided GS.(GGS)(Lei et al., 2019a). In order to determine which features should be perturbed, this method utilizes gradient information in the first stage. It computes the gradient of the loss function w.r.t the original input and treats the gradient of each feature as the impact score. Those ε features with the greatest impact scores are selected to be perturbed. In the second stage, it follows the same strategy as that of the second stage in GS, and pursues the combination resulting in the greatest loss among all possible combinations.\n\n• Gradient-guided greedy attack(GGA). On the basis of GGS, it remains the same first stage and modifies the second stage by adopting the same strategy as that in the second stage of GA.\n\nWe analyze the time complexity of our PCAA and the aforementioned methods for comparison.\n\nProbability Categorical Adversarial Attack. In practical implementation, we adopt the unconstrained optimization in Eq. 4. Therefore, the time complexity is only from gradient descent. Further, assume that we sample ns times when estimating the expected gradient and the maximum number of iterations is I. We need to compute gradient ns times during one iteration, thus the time complexity of PCAA is:\n\nN · ns · O(nd) · I = C1N O(nd)\n\n(7)\n\nwhere C1 is some constant related to ns and I.\n\nGreedy search. As described above, the Greedy search consists of two stages. In the first stage, it needs to traverse all n features and compute a loss when plugging in each level. Then n · d times of calculation are needed. In the second stage, all possible combinations of categories for top ε features need to be considered and the number of possible combinations is dε. Therefore, the time complexity for Greedy Search is\n\nN · [Ω(stage1) + Ω(stage2)] = N · [O(nd) + O(dε)] = N · O(nd + dε)\n\n(8)\n\nGreedy attack. In the first stage, it traverses all categories within each feature to find the best ε features to perturb, which needs n · d computation. And in the second stage, it searches ε selected features respectively for the d categories that lead to the greatest loss. Thus, the time complexity is N · [Ω(stage1) + Ω(stage2)] = N · [O(nd) + O(εd)] = N · O(nd + εd) (9)\n\nGradient guided greedy search. In the first stage, it computes gradients w.r.t the original input, and in the second stage, it also needs to traverse all possible combinations for selected features. Therefore the time complexity is\n\nN · [Ω(stage1) + Ω(stage2)] = N · [O(n) + O(dε)] = N · O(n + dε)\n\n(10)\n\nGradient-guided greedy attack. This method combines the first stage in GGS and the second stage in GA. Thus its time complexity is\n\nN · [Ω(stage1) + Ω(stage2)] = N · [O(n) + O(εd)] = N · O(n + εd)\n\n(11)\n\nFrom the above analysis, two search-based methods suffer from the exponential increase of time complexity (8 10) when the number of feature categories d and budget size ε is increasing. Moreover, two greedy-based methods (GA and GGA) accelerate the second stage and achieve better time efficiency, but sacrifice the performance as they greatly narrow down the searching space. However, PCAA is free of these problems, since it achieves great time efficiency to greedy-based methods while remain comparable performance with search-based methods. We will show this with more evidence in the experimental part.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n4 PROBABILISTIC ADVERSARIAL TRAINING (PADVT)\n\nAdversarial trainingSzegedy et al. (2014)Goodfellow et al. (2015)Madry et al. (2018) is one of the most effective methods to build robust models. Its training incorporates adversarial examples to improve robustness. However, greedy methods have limited applications in adversarial training when dealing with categorical input models due to their slow speed. The proposed PCAA method significantly improves time efficiency. In this case, we propose Probabilistic Adversarial Training (PAdvT) based on PCAA in order to train robust models on categorical data. Recalling the formulation of PCAA in Eq. 4, and denoting the parameters for classifier f with θ, the training objective for PAdvT is formulated as\n\n\n\nmin θ\n\nmax\n\nπ\n\nEx′∼π\n\n L(f (x′; θ), y) − λ\n\n\n\n\n\n(cid:88)\n\ni∈|n|\n\n\n\n+\n\n\n\nLCE(xi, πi) − ζ\n\n\n\n\n\n\n\n(12)\n\nSince our objective involves a penalty coefficient, we adopt the strategy in Yurochkin et al. (2020) to update λ during training. We adaptively choose λ according to LCE(x, π)−ε from the last iteration: when the value is large, we increase λ to strengthen the constraints and vice versa.\n\nThe implementation of PAdvT is illustrated in Algorithm. 2. It first solves the inner maximization problem by applying algorithm. 1. Then it samples nadv examples from the adversarial distribution π. Finally, it applies Adam Kingma & Ba (2015) to solve the outer minimization problem. The process will continue until the maximal number of iterations is reached.\n\nAlgorithm 2 Probabilistic Adversarial Training (PAdvT) Input Data D, parameters of clean model θ, budget ε, parameters of Algorithm1, nadv, initial penalty coefficient λ0, penalty coefficient step size α, parameters of Adam optimizer, number of iterations I Output parameters θ of robust model\n\nrepeat\n\nSample mini-batch B = {x1, ..., xm} from training set for i = 1, ..., m(in parallel) do\n\nApply Algorithm1 to xi and obtain adversarial distribution πi 1 , ..., X ′i Sample nadv examples {x′i\n\n} from πi using Gumbel Softmax tricks\n\nnadv\n\nend for Update θ by Adam to minimize the average adversarial loss Update λ = max{0, λ − α(ζ − 1 j∈[n] LCE(xi\n\ni∈[m]\n\n(cid:80)\n\n(cid:80)\n\nm\n\nj, πi\n\nj))}\n\nuntil Training converged Return parameters θ\n\n5 EXPERIMENT\n\nIn this section, we conduct experiments to validate the effectiveness and efficiency of PCAA and PAdvT. In Section 5.1, we demonstrate that PCAA achieves a better balance between attack success rate and time efficiency. In Section. 5.2, PAdvT achieves competitive or stronger robustness against categorical attacks across different tasks.\n\n5.1 CATEGORICAL ADVERSARIAL ATTACKS\n\nExperimental Setup. In this evaluation, we focus on three datasets.(1) Intrusion Prevention System (IPS) (Wang et al., 2020). IPS dataset has 242,467 instances, with each input consisting of 20 features and each feature has 1,103 categorical values. The output space has three labels. A standard LSTM based classifier(Bao et al., 2022) is trained for IPS dataset. (2) AG’s News corpus. This dataset consists of titles and description fields of news articles. The tokens of each sentence correspond to the categorical features, and the substitution set (of size 70) corresponds to the categorical values. A character-based CNN(Zhang et al., 2015) is trained on this dataset. (3) Splice-junction Gene Sequences (Splice) (Noordewier et al., 1990). Splice dataset has 3,190 instances. Each one\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Attacking results on IPS, AG’s news, and Splice datasets. SR. represents the attack success rate; T. represents the average running time in seconds; and ”-” indicates the running time over 10 hours. Each experiment runs 5 times and 95% confidence intervals are shown. ε = 3\n\nε = 5\n\nε = 1\n\nε = 4\n\nε = 2\n\nDataset\n\nIPS\n\nAG\n\nSplice\n\nAttack Method GS GA GGS GGA PCAA GS GA GGS GGA PCAA GS GA GGS GGA PCAA\n\nSR.(↑) 66.11±0.03 66.11±0.03 41.53±0.04 41.53±0.04 60.66±0.05 41.22±0.01 41.22±0.01 32.39±0.03 32.39±0.03 47.31±0.07 72.11±0.01 72.11±0.01 61.71±0.02 61.71±0.02 64.32±0.04\n\nT.(↓) 38.5 35.4 2.06 0.98 6.62 15.3 15.3 0.352 0.352 9.02 0.905 0.905 0.028 0.028 1.57\n\nSR.(↑) 81.24±0.01 71.32±0.02 75.47±0.03 63.72±0.06 77.51±0.04 67.38±0.02 60.71±0.05 59.21±0.02 41.29±0.07 61.47±0.09 75.84±0.02 74.42±0.03 68.28+-0.03 65.26±0.08 70.83±0.07\n\nT.(↓) 2028 38.1 1022 2.01 6.64 21.9 15.4 3.24 0.393 8.91 1.02 0.911 0.083 0.031 1.62\n\nSR.(↑) −\n79.44±0.06 −\n70.76±0.05 85.32±0.08 75.87±0.04 66.33±0.04 67.79±0.03 56.11±0.06 70.81±0.06 81.41±0.01 78.18±0.06 72.82±0.02 70.31±0.05 79.28±0.09\n\nT.(↓) −\n39.9 −\n2.94 6.75 356 15.5 151 0.511 9.11 1.36 0.915 0.251 0.0337 1.67\n\nSR.(↑) −\n85.28±0.07 −\n75.89±0.08 91.67±0.06 83.10±0.02 74.47±0.07 79.22±0.05 67.53±0.10 79.55±0.13 85.19±0.03 80.61±0.04 77.11±0.04 74.84±0.07 82.82±0.06\n\nT.(↓) −\n41.5 −\n3.74 6.66 19160 15.7 7551 0.613 9.06 2.73 0.922 0.917 0.035 1.73\n\nSR.(↑) −\n91.15±0.11 −\n82.43±0.10 94.63±0.12 −\n86.63±0.12 −\n72.28±0.07 87.41±0.10 90.88±0.04 83.74±0.05 82.53±0.04 80.49±0.08 84.77±0.11\n\nT.(↓) −\n43.2 −\n4.56 6.54 −\n15.9 −\n0.856 8.92 8.25 0.928 3.56 0.037 1.39\n\nis a gene fragment of 60 features with 5 categorical values. The output space has three labels. The LSTM model is trained for Splice.\n\nBaseline Attacks. Our goal is to generate powerful attacks on categorical models directly on input space, without relying on the embedding space. Thus, search-based methods are most suitable in this setting. Therefore, we compare PCAA with the following search-based attacks including Greedy Search(GS), Greedy Attack(GA), Gradient-guided GS (GGS) and Gradient-guided greedy attack(GGA). The details of these baselines can be found in Section 3.4.\n\nFor each dataset, we evaluate the performance in terms of the attack success rate (SR.) and the average running time (T.) under various budget sizes ε ranging from 1 to 5. Since PCAA learns the adversarial categorical distribution, the generation of adversarial examples is based on sampling. In the evaluation, we sample 100 examples from the adversarial distribution for each attack instance and dismiss those with more perturbed features than the budget size. We claim a success attack if one out of all samples successfully changes the prediction. The average running time for PCAA includes both the optimization of Eq. 4 and the sampling process.\n\nPerformance Comparison. The experimental results on IPS, AG’s news, and Splice datasets are demonstrated in Table 1. (1) In IPS dataset, each data contains more than 1000 categories for each feature, and our method has a significant advantage. As the budget size increases, GS and GGS run so long that they are no longer feasible. While GA and GGA remain efficient, our method outperforms them by significant margins, e.g., over 10% higher than GGA in success rate and faster over 7 times than GA in time. (2) In AG’s news data , GS has the highest success rate with small budgets. However, our method PCAA has much better time efficiency than GS under all budgets while maintaining competitive performance and outperforming all the other attacks. (3) In Splice, there are only 5 categories for each feature, which corresponds to the low-dimensional cases. The advantage of our method may not be as great as in other cases (i.e., IPS and AG) as the running time of greedy methods is acceptable. However, PCAA still generates strong attacks with only 6% less than GS in success rate, and remains highly efficient when the budget size increases, so it still provides a practical and competitive attack.\n\nTime Efficiency Comparison. It is evident from Table 1 that PCAA’s average running time is unaffected by the size of the budget, whereas greedy methods are clearly more time-consuming as budgets increase. Thus, the proposed PCAA method significantly improves time efficiency. As shown in Table1, when the substitution set is large (e.g., IPS and AG), the running time of GS and GGS will explode even under some moderate budget size such as ε = 3. Since the running time is too long for meaningful comparison, we do not record the performance for these budget sizes of those baselines. In small substitution sets, i.e., Splice, the time efficiency gap between PCAA and greedy methods is not large. Due to the small size of the substitution set, greedy methods need much fewer quires than in high-dimensional cases, but PCAA still takes a much shorter time for a budget size of more than 4.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: PAdvT and baseline defense performances under different attacks on IMDB dataset.\n\n5.2 CATEGORICAL ADVERSARIAL DEFENSES\n\nIn particular:\n\nExperimental Setup. For the defense evaluation, we focus on two datasets, AG’s News Corpus and IMDB. It is because IPS and Splice have too few samples (no more than 1000 (1) AG’s for each dataset). News corpus. is the same dataset used in the attack evaluation and the model is also a character-based CNN. As character swapping does not require embeddings for each character, we can apply attacking methods on input space. Therefore, the robustness of the defense models is evaluated using six attacks,i.e., HotFlip (Ebrahimi et al., 2017a), GS, GA, GGS, GGA, and PCAA. (2) IMDB reviews dataset(Maas et al., 2011). Under this dataset, we focus on a word-level classification task and we study two model architectures, namely Bi-LSTM and CNN, are trained for prediction. To evaluate the robustness, four attacks are deployed, including a genetic attack (Alzantot et al., 2018) (which is an attack method proposed to generate adversarial examples in embedding space), as well as GS, GGS, and PCAA.\n\nFigure 2: PAdvT and baseline defenses under different attacks on AG’s news dataset.\n\nBaseline Defenses. We compare our PAdvT with following baseline defenses:\n\n• Standard training. It minimizes the average cross-entropy loss on clean input.\n\n• Hot-flip(Ebrahimi et al., 2017a). It uses the gradient with respect to the one-hot input representation to find out the which individual feature under perturbation has the highest estimated loss. It is initially proposed to model char flip in Char-CNN model, and we also apply it to word-level substitution, as in (Dong et al., 2021a).\n\n• Adv l2-ball(Miyato et al., 2017). It uses an l2 PGD adversarial attack inside the word embed-\n\nding space for adversarial training.\n\n• ASCC-Defense(Dong et al., 2021a). A state-of-the-art defense method in text classification. It uses the worst perturbation over a sparse convex hull in the word embedding space for adversarial training.\n\nPerformance Comparison. The experimental results on AG’s news dataset are shown in Fig. 2. The Y-axis represents the error rate and X-axis represents different defense methods while multirows represents different attacking methods. Our defense method achieves leading robustness on Char-CNN over all attacks with significant margins, surpassing Hot-Flip-defense by 10%. Fig. 3 illustrates the defense results on the IMDB dataset, and we have similar observations with Fig. 2. Our PAdvT shows competitive adversarial robustness as ASCC-defense. Notably, ASCC is a defense method that conducts adversarial training on word-embedding space. It relies on the key assumption that similar words have a close distance in embedding space. However, our method does not rely on this assumption, which may result in the performance being competitive (slightly worse) than ASCC. For all other defenses, it outperforms them across different architectures significantly.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n5.3 ABLATION STUDY\n\nConcentration of PCAA. To further understand the behavior of our attack algorithm, in this subsection, we ask the question: what is the variance of our optimized probability distribution π∗ (from solving Eq.2))? Intuitively, we desire the distribution π to have smaller variance, so that we don’t need too many times of sampling to obtain the optimal adversarial examples. To confirm this point, we conduct an ablation study based on an experiment on IPS dataset to visualize the distribution π, which is optimized via PCAA under various budget sizes. In Fig 4, we choose three budget sizes ε = 1, 3, 5 and choose 4 features to present the adversarial categorical distributions. Notably, in Fig. 4, the left two columns are the features that are not perturbed. The right two columns are features which will be perturbed by PCAA. From the figure, we can see that for each feature (perturbed or unperturbed), there exists one category with a much higher probability compared to other categories. This fact indicates during the sampling process of PCAA, for a certain feature, the samplings are highly likely to have the same category for this feature. As a result, we confirm that our sampled adversarial examples are well-concentrated.\n\nAblation analysis for PAdvT. In our training objective in Eq. 12, ζ controls the budget size used for adversairal training and possibly affects the robustness of the model. We conduct an ablation study on the IMDB dataset to understand the impact of ζ. The results are demonstrated in Table. 2. When ζ increases, the success rates of all attacks decreases, meaning that the robustness of models is enhanced. However, large ζ will increases the clean error and sacrifice the clean accuracy of models. Thus, ζ controls the balance between the clean accuracy and the adversarial robustness of the model. When ζ = 0.4, our algorithm reaches a good balance between model accuracy and robustness.\n\nTable 2: Ablation study: impact of the budget regularization term ζ on PAdvT Clean Err\n\nGradient Search SR\n\nGenetic SR\n\nGS SR\n\nζ = 0.1 ζ = 0.2 ζ = 0.32 ζ = 0.4\n\nLSTM CNN LSTM CNN LSTM CNN LSTM 29.6 32.7 31.4 28.5\n\n39.5 38.6 38.5 36.4\n\n37.5 34.1 30.1 26.3\n\n16.1 17.2 17.9 18.4\n\n41.3 39.9 38.7 37.8\n\n43.5 41.3 38.3 36.2\n\n16.9 17.3 18.1 18.6\n\nCNN 41.4 39.7 37.6 34.9\n\nPCAA SR LSTM CNN 42.2 40.6 37.8 35.7\n\n40.2 39.1 38.8 36.9\n\nFigure 4: Visualization of Optimized Categorical Distribution for Various Features (IPS)\n\n6 CONCLUSION\n\nIn this paper, we propose a novel probabilistic optimization formulation for attacking models with categorical input. Our framework significantly improves the time efficiency compared with greedy methods and shows promising empirical performance across different datasets. Furthermore, we design an adversarial training method based on the proposed probabilistic attack that achieves better robustness.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and KaiWei Chang. Generating natural language adversarial examples. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 2890–2896. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-1316. URL https://doi.org/10.18653/v1/d18-1316.\n\nHongyan Bao, Yufei Han, Yujun Zhou, Yun Shen, and Xiangliang Zhang. Towards understanding the robustness against evasion attack on categorical data. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=BmJV7kyAmg.\n\nLijuan Cao and Francis Eng Hock Tay. Financial forecasting using support vector machines. Neural Comput. Appl., 10(2):184–192, 2001. doi: 10.1007/s005210170010. URL https://doi. org/10.1007/s005210170010.\n\nJin Cui, Lin Shen Liew, Giedre Sabaliauskaite, and Fengjun Zhou. A review on safety failures, security attacks, and available countermeasures for autonomous vehicles. Ad Hoc Networks, 90, 2019. doi: 10.1016/j.adhoc.2018.12.006. URL https://doi.org/10.1016/j.adhoc. 2018.12.006.\n\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural language word substitutions. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https: //openreview.net/forum?id=ks5nebunVn_.\n\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural\n\nlanguage word substitutions. arXiv preprint arXiv:2107.13541, 2021b.\n\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples for NLP. CoRR, abs/1712.06751, 2017a. URL http://arxiv.org/abs/1712.06751.\n\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial examples\n\nfor text classification. arXiv preprint arXiv:1712.06751, 2017b.\n\nJi Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 50–56. IEEE, 2018.\n\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6572.\n\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. Advances in neural information processing systems, 32, 2019.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=rkE3y85ee.\n\nAnsam Khraisat, Iqbal Gondal, Peter Vamplew, and Joarder Kamruzzaman. Survey of intrusion detection systems: techniques, datasets and challenges. Cybersecur., 2(1):20, 2019. doi: 10. 1186/s42400-019-0038-7. URL https://doi.org/10.1186/s42400-019-0038-7.\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.\n\nIn Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nVolodymyr Kuleshov, Shantanu Thakoor, Tingfung Lau, and Stefano Ermon. Adversarial examples\n\nfor natural language classification problems. 2018.\n\nQi Lei, Lingfei Wu, Pin-Yu Chen, Alex Dimakis, Inderjit S. Dhillon, and Michael J. Witbrock. Discrete adversarial attacks and submodular optimization with applications to text classification. In Ameet Talwalkar, Virginia Smith, and Matei Zaharia (eds.), Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019. mlsys.org, 2019a. URL https://proceedings.mlsys.org/book/284.pdf.\n\nQi Lei, Lingfei Wu, Pin-Yu Chen, Alex Dimakis, Inderjit S Dhillon, and Michael J Witbrock. Discrete adversarial attacks and submodular optimization with applications to text classification. Proceedings of Machine Learning and Systems, 1:146–165, 2019b.\n\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and ChristoIn Dekang Lin, Yuji Matsumoto, pher Potts. Learning word vectors for sentiment analysis. and Rada Mihalcea (eds.), The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pp. 142–150. The Association for Computer Linguistics, 2011. URL https://aclanthology.org/P11-1015/.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/ forum?id=rJzIBfZAb.\n\nTakeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semisupervised text classification. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=r1X3g2_xl.\n\nMichiel Noordewier, Geoffrey Towell, and Jude Shavlik. Training knowledge-based neural networks to recognize genes in dna sequences. Advances in neural information processing systems, 3, 1990.\n\nDanish Pruthi, Bhuwan Dhingra, and Zachary C Lipton. Combating adversarial misspellings with\n\nrobust word recognition. arXiv preprint arXiv:1905.11268, 2019.\n\nSuranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. arXiv preprint\n\narXiv:1707.02812, 2017.\n\nAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31, 2018.\n\nGianluca Stringhini, Christopher Kruegel, and Giovanni Vigna. Detecting spammers on social netIn Carrie Gates, Michael Franz, and John P. McDermott (eds.), Twenty-Sixth Annual works. Computer Security Applications Conference, ACSAC 2010, Austin, Texas, USA, 6-10 December 2010, pp. 1–9. ACM, 2010. doi: 10.1145/1920261.1920263. URL https://doi.org/10. 1145/1920261.1920263.\n\nAbigail Swenor and Jugal Kalita. Using random perturbations to mitigate adversarial attacks on\n\nsentiment analysis models. arXiv preprint arXiv:2202.05758, 2022.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. In Yoshua Bengio and Intriguing properties of neural networks. Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http: //arxiv.org/abs/1312.6199.\n\nXiaosen Wang, Yifeng Xiong, and Kun He. Randomized substitution and vote for textual adversarial\n\nexample detection. arXiv preprint arXiv:2109.05698, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYutong Wang, Yufei Han, Hongyan Bao, Yun Shen, Fenglong Ma, Jin Li, and Xiangliang Zhang. Attackability characterization of adversarial evasion attack on discrete data. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pp. 1415–1425. ACM, 2020. doi: 10.1145/3394486.3403194. URL https://doi.org/10. 1145/3394486.3403194.\n\nZhouhang Xie, Jonathan Brophy, Adam Noack, Wencong You, Kalyani Asthana, Carter Perkins, Sabrina Reis, Sameer Singh, and Daniel Lowd. Identifying adversarial attacks on text classifiers. arXiv preprint arXiv:2201.08555, 2022.\n\nPuyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael I. Jordan. Greedy attack and gumbel attack: Generating adversarial examples for discrete data. J. Mach. Learn. Res., 21: 43:1–43:36, 2020a. URL http://jmlr.org/papers/v21/19-569.html.\n\nPuyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael I Jordan. Greedy attack and gumbel attack: Generating adversarial examples for discrete data. J. Mach. Learn. Res., 21 (43):1–36, 2020b.\n\nMikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ML models with In 8th International Conference on Learning Representations, sensitive subspace robustness. ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=B1gdkxHFDH.\n\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 649–657, 2015. URL https://proceedings.neurips.cc/paper/ 2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html.\n\nYi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-wei Chang, and Xuanjing Huang. Defense against adversarial attacks in nlp via dirichlet neighborhood ensemble. arXiv preprint arXiv:2006.11627, 2020.\n\nA APPENDIX\n\nA.1 ADDITIONAL EXPERIMENTAL RESULTS\n\nIn this subsection, we present the results of Fig. 2 and 3 where the values represent error rates. We run each experiment for 5 times and compute the 95% confidence interval.\n\nTable 3: PAdvT and baseline defense performances under different attacks on IMDB dataset\n\nClean Err\n\nGenetic SR\n\nGS SR\n\nLSTM\n\nERM 15.50±0.005 17.37±0.049 Hotflip 32.53±0.034 Adv l2 ASCC 17.76±0.036 PAdvT 18.57±0.033\n\nCNN 15.23±0.007 16.57±0.051 37.38±0.043 18.37±0.030 18.85±0.049\n\nLSTM 92.62±0.022 50.63±0.030 56.59±0.036 19.66±0.047 26.01±0.065\n\nCNN 65.68±0.031 55.93±0.025 54.35±0.033 22.52±0.046 28.35±0.049\n\nLSTM 94.86±0.030 75.24±0.015 79.69±0.030 34.67±0.047 37.30±0.063\n\nCNN 82.02±0.049 66.35±0.012 67.00±0.034 33.28±0.045 36.36±0.061\n\nGradient Search SR LSTM CNN 80.54±0.030 91.61±0.011 65.47±0.022 67.96±0.021 65.81±0.038 78.34±0.031 32.61±0.061 33.46±0.054 34.85±0.053 35.60±0.067\n\nPCAA SR\n\nLSTM 92.26±0.087 68.08±0.069 78.58±0.068 33.97±0.101 35.46±0.104\n\nCNN 81.42±0.073 65.90±0.074 66.23±0.077 32.87±0.082 35.47±0.093\n\nTable 4: PAdvT and baseline defense performances under different attacks on AG’s news dataset\n\nClean Err\n\nERM 8.70±0.009 13.99±0.017 Hotflip PAdvT 14.62±0.028\n\nHotflip 79.72±0.015 60.07±0.013 45.38±0.037\n\nPCAA 80.75±0.066 63.47±0.057 49.50±0.081\n\nGS 83.09±0.015 64.28±0.018 50.65±0.035\n\nGGS 79.28±0.010 62.41±0.019 47.14±0.041\n\nGA 74.43±0.010 60.51±0.017 44.71±0.040\n\nGGA 67.53±0.016 58.33±0.013 42.18±0.045\n\nWe also run PAdvT with mixture of adversarial examples and clean samples on IMDB dataset. Results are shown in Table 5 where values represent error rates. It is noticeable that clean samples will slightly improve the clean performance and lead to small decreasing of robustness.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Comparison of PAdvT on IMDB dataset with/without mixture of clean samples.\n\nClean Err Genetic\n\nIMDB LSTM(mix) IMDB LSTM IMDB CNN(mix) IMDB CNN\n\n18.27 18.57 18.69 18.85\n\n26.22 26.01 28.51 28.35\n\nGS 37.67 37.3 36.47 36.36\n\nGGS 35.79 35.60 34.96 34.85\n\nPCAA 36.05 35.46 35.72 35.47\n\nA.2 VISUALIZATION ON IMDB\n\nWe run PCAA attack on IMDB dataset over two victim models LSTM and word-CNN. The candidate sets are pre-specified synonym set. The following Tables 6 and 7 show some successful adversarial examples, where the red words are adversarial words and blue are original words. It is obvious that these replacements do not hurt the semantic meaning but can fool the classifiers.\n\nTable 6: IMDB Adversarial Examples from PCAA on LSTM\n\nClass Negative\n\nPerturbed Class Positive\n\nNegative\n\nPositive\n\nangles\n\ncamera\n\nPerturbed Texts I watched this film for 45 minutes and counted 9 mullets. That’s a mullet every 5 minutes. Seriously, though this film is residing evidence(living proof) that formula works if it ain’t broke, it don’t need fit in a streetwise yet vulnerable heroine, a hardened ex-cop martial arts master with a heart of gold and a serial killer with ’issues’ pure magic. that do not Claustrophobic aid(help) the movie. Too long face only shots, where you most of the time get the hunch(feeling) that the lower half of the film is missing that the screen is cut off because there seems to be important actions going on, but you can not see them. There is anyway already too much confusion in the movie, so these viewing angles make it worse and do not contribute to artful visuals. I like artfully made movies and unconventional camera work. I can handle deep and slow movies but this one is trying too hard to be something artful and fails, in my opinion, painfully. Nothing to get attached to any of the characters because they are not worked out well enough to work out characters. More is needed than just minute long face shots. At least with this set of script director actors, I wonder whether some of the not so decent(good) acting is due to the script and director or due to the actors. I will stay away from films both written and directed by le you for sure in the future. What an annoying film even for person(someone) who would be interested in that part of history and for someone who spent time in Shanghai.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nPositive\n\nNegative\n\nPositive\n\nNegative\n\nPositive\n\nNegative\n\nI really liked this version of ’vanishing point’ as opposed to the 1971 version. I finds(found) the 1971 version quite boring if I can get up in the middle of a movie a few times as I did with the 1971 version, then to me it is not all that great. Of course, this could be due to the fact that I was only nine at the time the 1971 version was brought out. However, I have noticed(seen) many remakes everytime(where) I have liked the original and older one better. I found that the plot of the 1997 version was more understandable and had basically kept true to the original without undermining the meaning of the 1971 version. In my opinion I felt the 1997 version had more excitement and wasn’t so blas ́e boring. The cast is marvellous(excellent), the acting good, the plot interesting, the evolvement full of suspense, but it is hard to cram all those elements into a film that is barely 80 minutes long. If more time was taken to develop the plot and subplots, it would have a much better effect. Another 30 minutes of substance would have made this a very alright(good) film rather than just a good one. There is great detail in a ‘bug ’s life’. Everything is covered. The film expects(looks) great and the animation is sometimes jaw dropping. The film isn’t too terribly original. It ’s basically a modern take on kurosawa ’s seven samurai only with bugs, I enjoyed the character interaction however, and the naughty boys(bad guys) in this film actually seemed bad. It seems that Disney usually makes their bad guys carbon copy cut outs, the grasshoppers are menacing and hopper the lead bad guy was a brilliant creation. Check this one out.\n\nTable 7: IMDB Adversarial Examples from PCAA on word-CNN\n\nClass Positive\n\nPerturbed Class Negative\n\nPerturbed Texts I am a college student studying A levels and need help and comments from anyone who has any views at all about the theme of mothers in film. In The Mother, whether you have gone through something similar or just want to comment and help me research more about this film, any comment would much greatly appreciated. The comments will be used alone(solely) for exam purposes and will be included in my written exam. So if you have any views at all I’m convinced(sure) I can put them to use and you could help me get an A. I am also studying about a boy and tadpole. So if you have seen these films as well, I would appreciate it if you could leave comments on here on that page. Thank you.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nNegative\n\nPositive\n\nPositive\n\nNegative\n\nPositive\n\nNegative\n\nPositive\n\nNegative\n\nThis movie is so horrendous(awful). It is hard to find the right words to describe it. At first the story is so ridiculous. A narrow minded human can write a better plot. The actors are boring and untalented. Perhaps they were compelled to play in this dorky(cheesy) film. The camera receptions of the national forest are the only good in this whole movie. I should feel ashame because I paid for this lousy picture. Hopefully nobody makes a sequel or make a similar film with such a worse storyline. This movie is wonderful, the writing, directing, acting, all are marvelous(fantastic). Very witty and clever script quality performances by actors. Ally Sheedy is strong and dynamic and delightfully quirky really original and heart warmingly unpredicatable. The scenes are alive with fresh energy and really talented generating(production) This may not be war peace but the two academy noms wouldn’t have been forthcoming. If it weren’t for the genius of James Wong Howe, this is one of the few films I’ve fallen in love with as a infant(child) and gone back to without dissatisfaction. Whether you have any interest in what it offers fictively or not, BBC is a visual feast. I’m not saying it’s his best work. I’m no expert there for sure but the look of this movie is astounding(amazing). I love everything about it, Elsa Lanchester, the cat, the crazy hoodoo, the retro downtown Ness, but the way it was put on film is breathtaking. I even like the inconsistencies pointed out on this page aforementioned(above) and the special effects that seem backward. Now it all creates a really consistent world. Bette Midler is again divine raunchily hilarious(humorous) in love with burlesque, capable of bringing you down to tears either with old jokes, with new dresses or merely with old songs, with more power punch than ever. All in all, sung(singing) new ballads power, singing the good old perennial ones such as the rose ‘stay with me’ and yes even ‘wind beneath my wings’. The best way to appreciate the Divine Miss M has always been libe since this is the next best thing to it. I strongly recommended to all with a mixture of adult extensive(wide) eyed enchantment and appreciation and a child ’s mischievous wish for pushing all boundaries.\n\n15",
    "reference": "# Summary Of The Paper\n\nAdversarial attack on discrete (categorical) input is a challenging problem. This paper proposes PCAA, which converts the problem to a continuous optimization problem and solves it with gradient descent. It further shows that using PCAA for adversarial training (PAdvT) can improve the model robustness.\n\n# Strength And Weaknesses\n\nStrength:\n\n- The idea of using cross-entropy loss and Lagrangian form is novel and interesting. \n\nWeaknesses:\n\n- This paper takes an L0 distance as the only constraint to adversarial examples, however it is insufficient for text classification tasks, and may also be flawed on other tasks. For example, replacing the word “good” with “bad” will significantly change the meaning of a sentence by the L0 distance is just 1. \n- The adversarial probability distribution pi assumes that each feature is independent. However, it is not true in natural language and other types of input sequences. \n- Textual adversarial attack requires human evaluation to ensure that the adversarial sentences do not change the meaning (or the true label). That part is missing in this paper. \n- Attacking a text classifier seems to be an important use case for the proposed method. There are numerous works trying to improve the quality of the adversarial sentences. Those methods are not cited and compared in this paper. \n  - Jin et al., Is bert really robust? a strong baseline for natural language attack on text classification and entailment\n  - Li et al., Bert-attack: Adversarial attack against bert using bert\n  - Li et al., Contextualized perturbation for textual adversarial attack\n  - and many other papers.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe methodology is presented clearly. \n\nIt has some novelty in formulating the attack into an optimization problem. However, there are some issues with the formulation considering practical NLP or sequence classification tasks. There are issues with the experiments as well. \n\nThe source code was not submitted.\n\n# Summary Of The Review\n\nThe methodology has some novelty. However, there are still many issues with the problem formulation and experiments.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nUNSUPERVISED LEARNING OF CAUSAL RELATIONSHIPS FROM UNSTRUCTURED DATA\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEndowing deep neural networks with the ability to reason about cause and effect would be an important step to make them more robust and interpretable. In this work we propose a variational framework that allows deep networks to learn latent variables and their causal relationships from unstructured data, with no supervision, or labeled interventions. Starting from an abstract Structural Equation Model (SEM), we show that maximizing its posterior probability yields a similar construction to a Variational Auto-Encoder (VAE), but with a structured prior coupled by non-linear equations. This prior represents an interpretable SEM with learnable parameters (such as a physical model or dependence structure), which can be fitted to data while simultaneously learning the latent variables. Unfortunately, computing KL-divergences with this non-linear prior is intractable. We show how linearizing arbitrary SEMs via back-propagation produces local non-isotropic Gaussian priors, for which the KL-divergences can be computed efficiently and differentiably. We propose two versions, one for IID data (such as images) which detects related causal variables within a sample, and one for non-IID data (such as video) which detects variables that are also related over time. Our proposal is complementary to causal discovery techniques, which assume given variables, and instead discovers both variables and their causal relationships. We experiment with recovering causal models from images, and learning temporal relations based on the Super Mario Bros videogame.\n\n1\n\nINTRODUCTION\n\nHuman reasoning and decision-making is often underpinned by cause and effect: we take actions to achieve a desired effect, or reason that events would have happened differently had we acted a certain way – or if conditions had been different. Similarly, scientific inquiry uses the same tools, albeit more formalized, to build knowledge about the world and how our society can affect it (Popper, 1962). When building algorithms that automatically build statistical models of the world, as is common in machine learning practice, it would then be desirable to imbue them with similar inductive priors about cause and effect (Glymour et al., 2016). In addition to being more robust than statistical models which only characterize the observational distribution (Peters et al., 2017), they would allow reasoning about changing conditions outside the observed distribution (e.g. counterfactual reasoning). They would also allow communicating their inner workings more effectively – allowing us to ask “why” a given conclusion was reached, much in the same way that we do in scientific communication.\n\nDespite still being actively researched, there is now a mature body of work on understanding whether two or more variables are related as cause and effect (Peters et al., 2017). Many techniques assume that the variables are given, and concern themselves with finding relationship between them (Spirtes & Glymour, 1991; Chickering, 2003; Lorch et al., 2021). On the other hand, an advantage of modern deep neural networks is that they learn intermediate representations that do not have to be manually labeled (Yosinski et al., 2015), and effective models can be trained without supervision (Kingma & Welling, 2014). An important question then arises: can a deep network simultaneously discover latent variables in the data and establish cause-effect relationships between them?\n\nWe focus on learning Additive Noise Models (ANM) with Gaussian noise, which are identifiable (i.e. causal directions are distinguishable) as long as the functions relating the variables of interest are not linear (Hoyer et al., 2008). This model fits well a variational learning framework, and so we are able\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nto derive an analogue of a Variational Auto-Encoder (VAE) (Kingma & Welling, 2014) where the prior, rather than being an uninformative Gaussian, corresponds exactly to the ANM. When the ANM is linear with Gaussian noise, the joint probability of the variables also becomes Gaussian, and it is easy to perform variational inference. The dependencies between variables will then be expressed in the covariance matrix’s sparsity structure. However, as mentioned earlier to make the causal directions identifiable the model cannot be linear (Hoyer et al., 2008). We resolve this difficulty by learning models that are locally linear, but globally non-linear. This approach affords the full generality of a non-linear ANM, with the simplicity of variational inference on Gaussian models.\n\nIn summary, our contributions are:\n\n• A rigorous derivation of the variational Evidence Lower Bound (ELBO) of an Additive Noise Model (ANM), allowing efficient inference of Structural Equation Models (SEM) with deep networks. • A linearization method leveraging automatic differentiation to construct a local Gaussian approxi-\n\nmation of arbitrary non-linear ANMs.\n\n• A temporally-aware specialization of the causal ANM that encodes causal directions implicit in the\n\narrow-of-time and is suitable for high-dimensional time series data such as video.\n\n• Experiments demonstrating that the proposed method is able to fit latent variables with a dependence structure in high-dimensional data, namely a synthetic image dataset and video game based data.\n\n2 RELATED WORK\n\nOur work lies on the intersection of causality, variational inference, representation learning, and high-dimensional unstructured input domains.\n\nCausal inference deals with determining the causes and effects from data. Causal discovery methods generally focus on recovering the causal graph responsible for generating the observed data, e.g. Spirtes & Glymour (1991); Chickering (2003) (for an overview of methods in see Peters et al. (2017)). However, these methods are largely applied to structured datasets such as medical (Brooks-Gunn et al., 1992; Sachs et al., 2005; Louizos et al., 2017) or economics data LaLonde (1986) where the observed variables are provided by domain specialists. In contrast, we focus on unstructured data where the variables are not provided a priori.\n\nVariational inference is a way of performing inference by solving an optimisation problem. A popular instance is the Variational Auto-Encoder (VAE) (Kingma & Welling, 2014) which aims to extract a useful latent representation of the data by encoding and decoding it back. Traditionally the VAE prior is assumed to be an isotropic Gaussian distribution and the aim is to extract independent latent variables such as in the β-VAE (Higgins et al., 2016b) and FactorVAE (Kim & Mnih, 2018). There are works which use hierarchical priors such as iteratively conditioning each variable on its preceding variable in the Ladder-VAE (Sønderby et al., 2016) and conditioning each variable on all its predecessors in NVAE (Vahdat & Kautz, 2020) and VDVAE (Child, 2021). We also use a prior conditioning each variable on its predecessors but this comes as a natural consequence of basing our prior on a structural equation model (SEM).\n\nRecently there has been a growing interest in representation learning based on causal principles. For instance, the CausalVAE (Yang et al., 2021) learns independent latent variables which are then composed to form causal relationships, however they only consider linear relationships between variables. Other works use different approaches to VAEs for causal learning such as the CausalGAN (Kocaoglu et al., 2018) which use generative adversarial networks. Yet another line of work focuses on modelling object dynamics from video such as Li et al. (2020), however they use specialised modules for detecting keypoints and future prediction. Another line of work uses graph neural networks to infer an interaction graph such as Kipf et al. (2018); Löwe et al. (2022), but they do not deal with image or video data. Lippe et al. (2022) focus on causal learning with the knowledge of interventions, whereas we assume no such knowledge. Another line of work such as Lachapelle et al. (2022) and the iVAE (Khemakhem et al., 2020) uses non-linear Independent Component Analysis theory. Locatello et al. (2020) explore using a small number of labeled examples for learning. Walker et al. (2021) use a VQ-VAE for video future prediction using a hierarchical prior but do not focus on causal relationships.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n3 BACKGROUND\n\nIn this section, we will give a self-contained overview of several results from variational and causal inference that we build upon. While they are not new, bringing them together under one formulation offers new insights and challenges, which we solve in sec. 4. Our goal is to fit a distribution p(x), defined over an input space x ∈ Rm, to an empirical distribution ˆp(x) composed of finite samples, by choosing the optimal p ∈ P out of a set of candidate distributions P (e.g. parameterized by a neural network). We do this by minimizing their KL-divergence Dx computed over x, or equivalently maximizing the expected log-likelihood of p over the dataset ˆp: Dx(ˆp(x)||p(x)) = arg max p∈P\n\nEx∼ ˆp(x)[ln p(x)].\n\np∗ = arg min\n\np∈P\n\n(1)\n\nWe now introduce a set of latent variables z ∈ Rd, which we can marginalize to compute\n\n(cid:90)\n\np(x) =\n\np(z)p(x|z)dz,\n\n(2)\n\nin terms of a conditional distribution p(x|z) and a “prior” distribution over latents p(z). In a standard VAE, this prior is an isotropic Gaussian distribution (Kingma & Welling, 2014), while other structured priors are possible (Sønderby et al., 2016; Vahdat & Kautz, 2020; Tomczak & Welling, 2018). In this work, however, we will define it as a Structural Equation Model (SEM) (Pearl, 2009) (section 3.2).\n\n3.1 VARIATIONAL INFERENCE\n\nWe can now apply standard tools of variational inference (Bishop, 2006, Ch. 10) to eq. 1 and replace the intractable marginalization (eq. 2) with an optimization of q ∈ Q over a variational family of distributions Q (in essence, training an additional neural network q). Eq. 1, when marginalized (eq. 2) is equivalent to (Kingma & Welling, 2014):\n\np∗ = arg max\n\np∈P, q∈Q\n\nE x∼ ˆp(x)\n\n(cid:20)\n\nE z∼q(z|x)\n\n[ln p(x|z)] − Dz (q(z|x)||p(z))\n\n.\n\n(3)\n\n(cid:21)\n\nThe first term in eq. 3 amounts to a reconstruction error, and the second term matches the latent variables to the prior. In practice, q(z|x) and p(x|z) in eq. 3 are often defined as Gaussian distributions parameterized by neural networks. These are the functions μq(x) ∈ Rd and Σq(x) ∈ Rd×d for the encoder q, and μp(z) ∈ Rd and σp(z) ∈ Rd for the decoder p, which parameterize the means and covariances of the distributions:\n\nq(z|x) = N (z|μq(x), Σq(x)),\n\n(4) The KL-divergence between prior p(z) and variational posterior q(z|x) can be computed in closed form when both are Gaussians. Note that the encoder usually outputs a diagonal covariance matrix (motivated by the fact that an isotropic prior is also diagonal), but here we allow it to output full covariances Σq(x), which will be important later (sec. 4).\n\np(x|z) = N (x|μp(z), diag(σp(z))).\n\n3.2 STRUCTURAL EQUATION MODEL\n\nWe now consider a set of variables y, which have a dependency structure defined by a directed acyclic graph (DAG) G (i.e. there is an edge (i → j) ∈ G if yi is required to compute yj). Additionally, the generation process for each variable yi can be described by a sequence of non-linear equations:\n\nyi = fi(ypaG(i)) + ni, (5) where fi : R|paG(i)| (cid:55)→ R1 is an arbitrary deterministic function, paG(i) denotes the indices of parent nodes of i in G, and ni ∼ N (ni|0, σ2 i ) is an independent zero-mean noise variable, assumed to be Gaussian. This represents a Structural Equation Model (SEM), more specifically an Additive Noise Model (ANM) (Peters et al., 2017, Ch. 4.1.4) with Gaussian noise, which has the joint probability:\n\nd (cid:89)\n\np(y) =\n\npi(yi | ypa(i)) =\n\nd (cid:89)\n\nN (yi − fi(ypa(i)) | 0, σ2\n\ni ).\n\n(6)\n\ni=1 Practical methods for causal learning with ANMs typically assume the variables y are observed, and are concerned with recovering the true causal graph G (and sometimes the functions f ) that generated the data (Hoyer et al., 2008; Peters et al., 2017).\n\ni=1\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3.2.1 SCORE-BASED MODEL SELECTION\n\nThe formulation so far has assumed that the causal graph G, which encodes all the dependencies between variables, is given. We can, however, make use of a result by Nowzohour & Bühlmann (2016) that shows that a penalized likelihood score can be used to select between different models p∗ G(y) (each fit to an empirical distribution ˆp(y)), with each assuming a different graph G ∈ G (e.g. out of all graphs with up to d nodes). We thus select the graph that has the maximum score:\n\nG∗ = arg max\n\nG∈G\n\n1 s\n\ns (cid:88)\n\ny∼ ˆp(y)\n\nln p∗\n\nG(y) −\n\n|G| ln s\n\n,\n\n(7)\n\nwith s the number of samples, and |G| the number of edges of G. This method finds the true causal graph when the causal dependencies in the ANM are all non-linear (Nowzohour & Bühlmann, 2016).\n\n4 METHOD\n\n4.1 VARIATIONAL INFERENCE WITH A CAUSAL PRIOR\n\nThe previous exposition suggests a natural way to simultaneously learn latent variables and fit a causal model: use the ANM (eq. 6) as the prior in the variational optimization problem (eq. 3), by setting y ≡ z. This amounts to assuming that the latent variables z have a dependency structure defined by a DAG G, and are generated sequentially by application of the non-linear functions fi corrupted by Gaussian noise ni. Model selection (i.e. finding the optimal graph G) can then be done by the score-based search of sec. 3.2.1. Note that this model reduces to a VAE for a DAG with no edges and thus null functions fi:\n\n∀i : pa(i) = ∅ ⇒ zi = ni (reduction to standard VAE)\n\n(8)\n\nAt a high level, the same tools used to train a VAE should be applicable in this new setting. However, the ANM is only identifiable (i.e. the true causal directions can be recovered) if the fi are nonlinear (Hoyer et al., 2008), and this makes computing the KL-divergence in eq. 3 intractable, since it is no longer defined in closed form. We will resolve this difficulty by local linearization (sec. 4.4), although the model will still be globally non-linear to ensure identifiability.\n\nOne major difference from the exposition by Nowzohour and Bühlmann is that they propose using a non-parametric fitting method over known variables x, while we want to simultaneously recover the causal structure and latent variables z (e.g. unknown parameters of objects depicted in images), which must be estimated from inputs x (e.g. raw pixels). Another difference is that their procedure is computationally expensive, as it entails enumerating all graphs G ∈ G explicitly. However, given the expressiveness of deep neural network models, in sec. 4.2 we will show how we can replace this search with a simpler model fitting procedure.\n\n4.2 MAXIMAL DAG\n\nFor the purposes of fitting the model G and f to observed data x, we will consider a simplification where we take all ancestors of a variable zi to be its parents pa(i), i.e. we replace G with the full DAG with edges {i → j : i < j, i, j = 1, . . . , d}. The following proposition shows that this will be able to model any underlying SEM, although some of the independence relations will have to be modeled by the learnable edge probabilities described in sec. 4.5. Proposition 1. The set of all SEMs S = {(zi = fi(zpaG(i)))d and probabilistic functions f is contained within the set S Ω = {(zi = fi(z1,...,i−1))d reorderings of the variables z.\n\ni=1 | ∀G, f } with arbitrary DAGs G i=1 | ∀f }, up to\n\nProof. See Appendix A.\n\nProp. 1 says that, as long as the function class of f is expressive enough (as is usually the case with deep networks (Yosinski et al., 2015)), we can find an equivalent SEM using a fixed maximal DAG GΩ with edges {i → j : i < j, i, j = 1, . . . , d}. This simplifies the exposition in the following sections and provides justification for the approximation in sec. 4.5.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n4.3 PRIOR PROBABILITY FOR LINEAR SEMS\n\nIn the simple case when all functions fi of the ANM are linear, the resulting joint probability must be Gaussian, as it is a linear combination of independent Gaussian noise variables ni (eq. 5, recalling that y ≡ z). However, we need to compute its precise form for general linear ANMs. Theorem 2. Consider a linear ANM defined as zi = aT and ni ∼ N (ni|0, σ2 zeros in ai. Then the ANM’s joint probability is given by p(z) = N (z | μ, Σ) with\n\ni z1,...,i−1 + bi + ni, with ai ∈ Ri−1, bi ∈ R i ) for i = 1, . . . , d. Missing edges in the causal graph can be represented as\n\nμ =\n\n(cid:32) 2\n\n(cid:89)\n\ni=d\n\n(cid:33)\n\nAi\n\nb, Σ =\n\n(cid:32) 2\n\n(cid:89)\n\n(cid:33)\n\nAi\n\ni=d\n\ndiag i=1,...,d\n\n(σ2 i )\n\n\n\n\n\nAi =\n\nI(i−1)×(i−1) Oi−1 O(i−1)×(d−i) 1\n\naT i\n\nOT d−i I(d−i)×(d−i)\n\nO(d−i)×(i−1) Od−i\n\n\n\n ,\n\n(9)\n\n(cid:32) 2\n\n(cid:89)\n\n(cid:33)T\n\nAi\n\n,\n\ni=d\n\nb =\n\n\n\n  ,\n\n\n\n \n\nb1 ... bd\n\nwhere Ik×k denotes an identity matrix, while Ok and Ok×l denote a zero column vector and matrix. Proof. See Appendix C.\n\nThe process in Theorem 2 can be interpreted as a form of mean and covariance propagation: at each stage the ANM applies a linear transformation to the mean and covariance from the previous stage. The matrix (cid:81)2 i=d Ai is a lower-triangular matrix representing the edge strengths from parent to child nodes in G, obtained from the SEM. This linear transformation is then applied to the SEM biases b to obtain the total mean μ, as well as to the SEM noise variances σ2 i to obtain the total covariance Σ. For identifiability, however, we must generalize this process to non-linear ANMs, which we do next.\n\n4.4 PRIOR PROBABILITY FOR NON-LINEAR SEMS\n\nOur approach to deal with non-linear ANMs is to linearize them around a pivot point z◦. Due to Taylor’s theorem this approximation will be accurate for a sufficiently small neighborhood (Nocedal & Wright, 1999, Ch. 1). The ANM’s joint probability p(z) will then be Gaussian by Theorem 2.\n\nTheorem 3. The best linear approximation (in the least-squares sense) of an ANM zi = fi(z1,...,i−1) + ni (eq. 5, with missing edges in the causal graph corresponding to ignored inputs in fi), around a pivot point z◦, is given by eq. 9 (Theorem 2) with\n\nai =\n\n∂fi(z1,...,i−1) ∂z1,...,i−1\n\n(cid:12) (cid:12) (cid:12) (cid:12)z1,...,i−1=z◦\n\n1,...,i−1\n\nand bi = fi(z◦\n\n1,...,i−1) + ni − aT\n\ni z◦\n\n1,...,i−1,\n\n(10)\n\n1 = n1, zo where zo Proof. See Appendix D.\n\n1,...,i = f (z1,...i−1) + ni and ni ∼ N (0, σ2\n\ni ) with learnable σi.\n\nThe advantage afforded by Theorem 3 is that the ANM’s joint probability p(z) is locally Gaussian, and we can compute its parameters by mean and covariance propagation (eq. 9). Another advantage is that, in the context of training deep networks, one can use automatic differentiation (back-propagation) to compute eq. 10 for arbitrary functions fi, including very expressive ones such as multi-layer perceptrons (MLP). Since this locally-linear SEM is represented as an explicit Gaussian distribution, the objective’s KL-divergence (eq. 3) can be computed in closed form (Kingma & Welling, 2014). We can then obtain the full prior distribution by sampling many pivot points zo by ancestral sampling according to Theorem 3.\n\n4.5 GRAPH SEARCH VIA SPARSITY\n\nTo efficiently search for the graphs in eq. 7, we use Proposition 1 to justify using a fixed maximal graph GΩ to learn a single SEM pf (z) (where we make explicit the dependence of p(z) on the learned causal functions fi, c.f. eq. 5). This is in contrast to the previously-proposed technique of enumerating all graphs G ∈ G and learning a different pf,G(z) for each G (Nowzohour & Bühlmann, 2016). We define the graph of dependencies:\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nVAE (β = 1)\n\nVAE (β = 4)\n\nCausal-prior VAE\n\nFigure 1: Samples from the observed and prior distributions for different VAEs, showing that the distributions match only for the β = 4 and the causal-prior VAEs. Only the causal-prior VAE recovers the underlying causal structure that relates the latent variables, a parabola.\n\nDefinition 4. We define the implicit dependency graph G(f ) as the graph with d nodes, and each edge i → j exists if fj(z1,...,j−1) depends on its ith input.\n\nWe can then define the overall objective (from eq. 7 and eq. 3) as\n\np∗ = arg max\n\np∈P, q∈Q, f\n\n1 s\n\ns (cid:88)\n\nx∼ ˆp(z)\n\n(cid:20)\n\nE z∼q(z|x)\n\n[ln p(x|z)] − Dz (q(z|x)||pf (z))\n\n−\n\n(cid:21)\n\n|G(f )| ln s\n\n.\n\n(11)\n\nTo approximate the |G(f )| term that counts the edges of the graph G(f ), we introduce edge probabilities pi→j denoting the probability that an edge from node i to j exists, which allows us to write |G(f )| = (cid:80) j,i<j pi→j (i.e. for binary probabilities, this recovers the exact edge count). We define each pi→j as a binary Gumbel-Softmax distribution (Jang et al. (2017), which is suitable for gradient-based optimization. The edge probabilities mask the inputs of the SEM’s functions fj(z1, . . . , zj−1), as fj(p1→jz1, . . . , pj−1→jzj−1). During model evaluation we can sample binary edge probabilities pi→j ∈ {0, 1}, producing a discrete graph G.\n\nWe can thus use standard stochastic gradient methods to optimize eq. 11 and obtain an encoder q, decoder p, and SEM defined by f and pi→j, the latter of which models well-defined causal directions (as long as the underlying functions are non-linear) and reflects independence relations in the data.\n\n4.6\n\nIDENTIFIABILITY\n\nIn Theorem 7, we demonstrate the identifiability of the proposed model up to some unavoidable indeterminacies. The only transformations that can be implemented by the learned deep networks (encoder, decoder and SEM) and can result in an identical objective value to the optimal model are reorderings of the latent variables, and as well as shifts and orthogonal transformations of the input (implemented by both the encoder at the input, and decoder at the output). The conditions required to achieve this result depend on common features of standard deep networks, namely an encoder and decoder composed of ReLUs and linear operators, batch-normalization in the outputs of the SEM f , and a SEM with fixed scale, such as the quadratic used in experiments. The proof is given in Appendix B.\n\n5 EXPERIMENTS\n\nIn this section we validate our method on two experiments: the first one learning atemporal variables and the second one learning time-varying variables. In the atemporal experiment (Section 5.1) we show that our method correctly recovers the positions of a shape placed on a parabola given a fixed quadratic prior. In the time-varying experiment (Section 5.2) we show that our method correctly recovers the positions of a moving character and their relationships over time, using a learnable linear prior.\n\n5.1 LEARNING ATEMPORAL CAUSAL RELATIONS\n\nIn this section we demonstrate the method’s ability to learn causal relationships that are not time dependent. We use a dataset of images of oval shapes at different rotations and scales, placed at\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nVAE (β = 1)\n\nVAE (β = 4)\n\nCausal-prior VAE\n\nFigure 2: Latent response to the object’s position for different VAEs (higher values are brighter). The β = 4 and β = 1 VAEs have entangled responses (although it is less entangled for β = 1), while the causal-prior VAE correctly disentangles the object’s horizontal and vertical positions.\n\npositions that follow a parabolic arc y = x2 + n (with noise n). We thus have a causal graph with a single edge x → y. We train our causal-prior VAE with a 3-layer MLP and a parabolic prior (see Appendix E for details), and use the L2 loss between each image and its prediction from the decoder as the reconstruction error (see Appendix ??). We also train a high-β and low-β VAEs for comparison (where β denotes the multiplicative factor of the KL divergence as in (Higgins et al., 2016a)).\n\nLatent space visualisation. In Figure 1 we show the prior (blue) and the predicted distribution (red) for the low-β VAE, the high-β VAE, and our causal-prior VAE. For the high-β VAE (center) the two distributions match, but the latent variables are entangled (i.e. both follow an isotropic Gaussian distribution), and so do not reflect the underlying parabolic relationship between the variables that generate the data (y = x2 + n). For the low-β VAE (left), although the observed variables recover the underlying parabolic shape (red), up to a vertical reflection of the coordinates, this distribution does not match the prior at all (blue), since it is constrained to be Gaussian. As such, this generative model p(x) (eq. 2) cannot generate samples that respect the causal relationships in the empirical data distribution. For our causal-prior VAE (right), the two distributions match and both follow the underlying noisy parabolic equation. Our model can thus be used as a generator p(x) that reproduces the true causal structure of the data, a capability that we will explore next in more detail.\n\nLatent response to position. Figure 2 shows for different VAEs the value of each latent variable as the position of the shape in the input image is varied, averaged over different rotations and scales. The variables are ordered by their standard deviation (σ). The goal is to assess how sensitive each variable is to the depicted shape’s position. For the causal-prior VAE, the variable z1 clearly corresponds to the shape’s x position (with values linearly increasing towards the left), and variable z2 to the y position (with values increasing towards the top), while other variables show minimal response. For the high-β VAE the position is entangled across several variables; for the low-β VAE the variables z4 and z1 correspond approximately to shape’s x and y position, but there is still significant entanglement with the other variables, which are also highly sensitive to the shape’s position.\n\nLatent noise traversal. In the first and third columns of Figure 3, we show grids of images decoded by each method, obtained by taking one sample and varying the noise ni of the two latent variables zi (see eq. 5) that best correspond to horizontal and vertical position. In the first column, decoded images are color-coded by their horizontal position. Since the positions of shapes in this grid are difficult to compare, the second column shows the same images but superimposed, which reveals the underlying parabolic structure of the data. The third column shows the same grid of decoded images, color-coded by vertical position, while the fourth shows them superimposed. While all methods seem to generate images following roughly the parabolic data distribution, we can observe that in both β-VAE configurations the shapes’ positions are entangled (i.e. vary jointly) with their rotation and scale, while the same is not true for the proposed causal-prior VAE. It is interesting to note that in the later case, the noise n1 (corresponding to variable z1) does not map directly to the shape’s vertical position (as observed for the β-VAE), but rather expresses its offset from the parabola, according to z1 = z2 2 + n1. In the bottom-right panel, the red tint of images generated from positive noise offsets (n1 > 0) is visible above the parabola, and the blue tint of negative offsets is visible below.\n\n5.2 LEARNING TEMPORAL CAUSAL RELATIONS\n\nWe now evaluate our method on temporal data, namely short videos. We apply the encoder and decoder independently to each frame, and append an encoded background frame to the the decoder’s\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n) 1\n=\n\nβ\n\n(\n\nE A\nV\n\n) 4\n=\n\nβ\n\n(\n\nE A\nV\n\nE A\nV\n\nr o\n\ni r\np -\nl a\ns u\na C\n\nFigure 3: Reconstructed images (as a 2D grid and as superimposed images) as latent noise values are traversed at regular intervals. For the β = 1 and β = 4 VAEs the depicted shape’s position is entangled with its rotation and scale, while for the causal-prior VAE the horizontal position and vertical offset from the parabola are disentangled correctly.\n\ninput (to allow the latent variables to ignore background/time-invariant appearance). The SEM only contains edges in the direction of the arrow of time (see Appendix E for details). We create a dataset based on the Super Mario Bros video game depicting an object moving with linear motion in different directions, at different speeds, and on varied backgrounds. Three frames are observed, with the object’s coordinates (xi, yi) for i ∈ {1, 2} sampled uniformly from [7, 12], and the third obeying x3 = 2x2 − x1, y3 = 2y2 − y1. We train our method with a linear prior, and otherwise similarly to Sec. 5.1. (see Appendix F for details) and select the sparsest graph that still achieves the best reconstruction accuracy. We use the reconstruction error given by (cid:80)3 t=1 L2(xt, ˆxt) + L2(x3, ˆxf 3 ) where xt denotes a frame at time t, ˆxt denotes a prediction made using the decoder p(ˆxt|zt) and ˆxf 3 denotes a prediction made using the decoder p(ˆxf 3 |f3(z1, z2)) (see Appendix ?? for details). For quantitative evaluation of learned graphs please refer to Appendix G.\n\nLearning a causal graph. Figure 4 shows the graph learned by the causal-prior VAE, with the bottom row (z1, z2, z3, z4) corresponding to latent variables at time t = 1, the middle row (z5, z6, z7, z8) corresponding to t = 2 and the top row (z9, z10, z11, z12) corresponding to t = 3. Learned edges are shown in black and missing ones in beige; learned functions fi are represented as a number (multiplicative factor) next to the corresponding edge, since they are linear in this case. The graph shows that each component of the object’s 2D position at time t = 3 (expressed as (z10, z11)) depends on its 2D positions at times t = 1 (z2, z3) and t = 2 (z6, z7), following the model z10 = f10(z) = 2.3z6−1.1z2 and z11 = f11(z) = 2.2z7 − 1.1z3, which matches the data generation process up to a scale factor.\n\nLatent response to position. Figure 5 shows the response of each latent variable as the object’s horizontal and vertical position is varied on the input. The rows correspond to variables from times t = 3, 2, 1 respectively. We can see that z2, z6, z10 encode the object’s position along the bottom-right to top-left diagonal at different moments in time, while z3, z7, z11 encode the position along the orthogonal direction. The remaining variables do not exhibit any significant response to position. This shows the variables correctly match the data generation process up to a rotation.\n\nInterventions visualisation. Once a SEM relating the latent variables has been learned, we can perform interventions on some of its variables. In Figure 6 we encode a randomly sampled reference\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 4: Learned graph edges and mechanisms relating the latent variables, recovering the data generation process up to a scaling factor.\n\nFigure 5: Latent response of variables to the object’s horizontal and vertical position, recovering the data generation process up to a rotation.\n\nFigure 6: Intervention results showing that intervening on each variable propagates to its descendants through the correct mechanism.\n\nFigure 7: Extrapolation results obtained by iteratively passing the encoded variables through the learned SEM to predict their future values.\n\nclip from the dataset and separately intervene by assigning different values (rows) to variables z1, ..., z12 (columns), propagating the changes to their child variables using the SEM, and decoding the results. The results are decoded using a black background and the resulting 3-frame decoded sample is combined into a single frame by colouring the t = 1 decoded image by red, the t = 2 image by white, and t = 3 by green. We observe that intervening on the variables for t = 1 (z2, z3) changes both the object’s position in t = 1 (red, columns 1,2) and t = 3 (green, columns 1,2), following the SEM. Intervening on the variables for t = 2 (z6, z7) changes the object’s t = 2 position (white, columns 3,4) and t = 3 position (green, columns 3,4); while intervening on the variables for t = 3 (z10, z11) changes only its t = 3 position (green, columns 5,6), which reflects the fact that other positions (in the past) do not depend on it. Intervening on other variables has no effect (last column). This confirms that the model has learned the correct temporal causal structure, as each intervention affects the right variables.\n\nExtrapolation visualisation. Having learned a SEM relating those at time t to the variables at times (t − 1, t − 2) now makes it possible to extrapolate the variable values into the future. In Figure 7 we start with 3 samples (rows) of 2 consecutive frames from the dataset x1, x2, encode them to obtain the t = 1, 2 latent variables (z1, ..., z8), and iteratively pass them through the SEM to compute the t = 3 latent variables (z9, ..., z12), decoding these into ˆx3. We now repeat this process by taking z5, ..., z12 as the t = 1, 2 variables and use the SEM to predict the t = 3 variables which we decode into ˆx4 and similarly for ˆx5, thus obtaining predictions for 3 time steps into the future. The future predictions confirm the knowledge learned in the SEM that the object moves linearly and can be used to predict future frames accurately.\n\n9\n\n1.11.12.32.2z1z2z3z4z5z6z7z8z9z10z11z12P(hard graph)z9z10z11z12z5z6z7z8z1z2z3z4z2z3z6z7z10z11other3.01.5+0.0+1.5+3.0t=1t=2t=3x1x2x3x4x5Under review as a conference paper at ICLR 2023\n\n6 CONCLUSIONS\n\nIn this work we proposed a general model that naturally extends the variational learning framework to learn a non-linear Structural Equation Model (SEM) as the prior, which enables causal learning to be performed on perceptual modalities such as images and video. To reconcile the non-linearity of SEMs while using Gaussian variables we proposed a fully differentiable method that locally linearises the SEM to obtain a locally-Gaussian distribution. Furthermore, we relaxed the search over causal graphs as a joint continuous optimization over non-linear causal functions f . The proposed method shows promise to scale to high-dimensional data and moderately complex SEMs, however future work should explore more large-scale data such as long videos and other input modalities.\n\nREFERENCES\n\nJørgen Bang-Jensen and Gregory Z. Gutin. Digraphs: Theory, Algorithms, and Applications. Springer\n\nLondon, 2009. ISBN 978-1-84800-997-4.\n\nChristopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006. ISBN 978-0-\n\n387-31073-2.\n\nJ. Brooks-Gunn, F. R. Liaw, and P. K. Klebanov. Effects of early intervention on cognitive function of low birth weight preterm infants. The Journal of Pediatrics, 120(3):350–359, March 1992. ISSN 0022-3476. doi: 10.1016/s0022-3476(05)80896-0.\n\nDavid Maxwell Chickering. Optimal structure identification with greedy search. The Journal of Machine Learning Research, 3(null):507–554, March 2003. ISSN 1532-4435. doi: 10.1162/ 153244303321897717. URL https://doi.org/10.1162/153244303321897717.\n\nRewon Child. Very deep {vae}s generalize autoregressive models and can outperform them on images. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=RLRXCV6DbEJ.\n\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\n\nstochastic optimization. Journal of machine learning research, 12(7), 2011.\n\nMadelyn Glymour, Judea Pearl, and Nicholas P Jewell. Causal inference in statistics: A primer. John\n\nWiley & Sons, 2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026–1034, 2015.\n\nIrina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mohamed, and Alexander Lerchner. Early Visual Concept Learning with Unsupervised Deep Learning. Technical Report arXiv:1606.05579, arXiv, September 2016a. URL http://arxiv. org/abs/1606.05579. arXiv:1606.05579 [cs, q-bio, stat] type: article.\n\nIrina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. November 2016b. URL https://openreview.net/ forum?id=Sy2fzU9gl.\n\nPatrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Schölkopf. Nonlinear causal discovery with additive noise models. In Advances in Neural Information Processing Systems, volume 21. Curran Associates, Inc., 2008. URL https://papers.nips.cc/ paper/2008/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html.\n\nEric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=rkE3y85ee.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nIlyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ica: A unifying framework. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 2207–2217. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/khemakhem20a.html.\n\nHyunjik Kim and Andriy Mnih. Disentangling by Factorising.\n\nIn Proceedings of the 35th International Conference on Machine Learning, pp. 2649–2658. PMLR, July 2018. URL https://proceedings.mlr.press/v80/kim18b.html. ISSN: 2640-3498.\n\nDiederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. Technical Report arXiv:1312.6114, arXiv, May 2014. URL http://arxiv.org/abs/1312.6114. arXiv:1312.6114 [cs, stat] type: article.\n\nThomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2688–2697. PMLR, 10–15 Jul 2018. URL https://proceedings. mlr.press/v80/kipf18a.html.\n\nMurat Kocaoglu, Christopher Snyder, Alexandros G. Dimakis, and Sriram Vishwanath. CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training. February 2018. URL https://openreview.net/forum?id=BJE-4xW0W.\n\nSebastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Rémi LE PRIOL, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In First Conference on Causal Learning and Reasoning, 2022. URL https://openreview.net/forum?id=dHsFFekd_-o.\n\nRobert J. LaLonde. Evaluating the Econometric Evaluations of Training Programs with Experimental Data. The American Economic Review, 76(4):604–620, 1986. ISSN 0002-8282. URL https: //www.jstor.org/stable/1806062. Publisher: American Economic Association.\n\nYunzhu Li, Antonio Torralba, Anima Anandkumar, Dieter Fox,\n\nCausal Discovery in Physical Systems from Videos. formation Processing Systems, volume 33, pp. 9180–9192. Curran Associates, 2020. 6822951732be44edf818dc5a97d32ca6-Abstract.html.\n\nand Animesh Garg. InInc., https://proceedings.neurips.cc/paper/2020/hash/\n\nIn Advances in Neural\n\nURL\n\nPhillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. iCITRIS: Causal representation learning for instantaneous temporal effects. In UAI 2022 Workshop on Causal Representation Learning, 2022. URL https://openreview.net/forum?id= xeDKTZsZ7Z7.\n\nFrancesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rätsch, Bernhard Schölkopf, and Olivier Bachem. Disentangling factors of variations using few labels. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= SygagpEKwB.\n\nLars Lorch, Jonas Rothfuss, Bernhard Schölkopf, and Andreas Krause. DiBS: Differentiable Bayesian In Advances in Neural Information Processing Systems, volume 34, pp. Structure Learning. 24111–24123. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/ paper/2021/hash/ca6ab34959489659f8c3776aaf1f8efd-Abstract.html.\n\nChristos Louizos, Uri Shalit,\n\nMax Welling. vances Inc., 94b5bde6de888ddf9cde6748ad2523d1-Abstract.html.\n\nand In AdInformation Processing Systems, volume 30. Curran Associates, URL https://proceedings.neurips.cc/paper/2017/hash/\n\nCausal Effect Inference with Deep Latent-Variable Models.\n\nJoris M Mooij, David Sontag, Richard Zemel,\n\nin Neural\n\n2017.\n\nSindy Löwe, David Madras, Richard Zemel, and Max Welling. Amortized causal discovery: Learning to infer causal graphs from time-series data. In Bernhard Schölkopf, Caroline Uhler, and Kun\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nZhang (eds.), Proceedings of the First Conference on Causal Learning and Reasoning, volume 177 of Proceedings of Machine Learning Research, pp. 509–525. PMLR, 11–13 Apr 2022. URL https://proceedings.mlr.press/v177/lowe22a.html.\n\nJorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.\n\nChristopher Nowzohour and Peter Bühlmann. Score-based causal learning in additive noise models.\n\nStatistics, 50(3):471–485, 2016.\n\nJudea Pearl. Causality. Cambridge University Press, Cambridge, 2 edition, 2009. ISBN 978-0-52189560-6. doi: 10.1017/CBO9780511803161. URL https://www.cambridge.org/core/ books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B.\n\nJonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of Causal Inference: Foundations and Learning Algorithms. Adaptive Computation and Machine Learning series. MIT Press, Cambridge, MA, USA, November 2017. ISBN 978-0-262-03731-0.\n\nSir Karl Raimund Popper. Conjectures and refutations: The growth of scientific knowledge. 1962.\n\nKaren Sachs, Omar Perez, Dana Pe’er, Douglas A. Lauffenburger, and Garry P. Nolan. Causal ProteinSignaling Networks Derived from Multiparameter Single-Cell Data. Science, 308(5721):523– 529, April 2005. doi: 10.1126/science.1105809. URL https://www.science.org/doi/ full/10.1126/science.1105809. Publisher: American Association for the Advancement of Science.\n\nPeter Spirtes and Clark Glymour. An Algorithm for Fast Recovery of Sparse Causal Graphs. Social Science Computer Review, 9(1):62–72, April 1991. ISSN 0894-4393. doi: 10.1177/ 089443939100900106. URL https://doi.org/10.1177/089443939100900106. Publisher: SAGE Publications Inc.\n\nCasper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder Variational Autoencoders. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://papers.nips.cc/paper/2016/hash/ 6ae07dcb33ec3b7c814df797cbda0f87-Abstract.html.\n\nJakub Tomczak and Max Welling. VAE with a VampPrior. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, pp. 1214–1223. PMLR, March 2018. URL https://proceedings.mlr.press/v84/tomczak18a.html. ISSN: 2640-3498.\n\nArash Vahdat and Jan Kautz. NVAE: A Deep Hierarchical Variational Autoencoder.\n\nIn Advances in Neural Information Processing Systems, volume 33, pp. 19667–19679. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ e3b21256183cf7c2c7a66be163579d37-Abstract.html.\n\nJacob C Walker, Ali Razavi, and Aaron van den Oord. Predicting video with {vqvae}, 2021. URL\n\nhttps://openreview.net/forum?id=bBDlTR5eDIX.\n\nMengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9588–9597, Nashville, TN, USA, June 2021. IEEE. ISBN 978-1-66544-509-2. doi: 10.1109/CVPR46437.2021.00947. URL https://ieeexplore.ieee.org/document/9578520/.\n\nJason Yosinski, Jeff Clune, Thomas Fuchs, and Hod Lipson. Understanding neural networks through\n\ndeep visualization. In ICML Workshop on Deep Learning, 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: Overview of the causal-prior VAE architecture.\n\nAPPENDIX\n\nA PROOF OF DAG MAXIMALITY\n\nProposition 5. The set of all SEMs S = {(zi = fi(zpaG(i)))d and probabilistic functions f is contained within the set S Ω = {(zi = fi(z1,...,i−1))d reorderings of the variables z.\n\ni=1 | ∀G, f } with arbitrary DAGs G i=1 | ∀f }, up to\n\nProof. Because G is a DAG it has an acyclic ordering of its vertices (Bang-Jensen & Gutin, 2009); i.e. if G has vertices V = {v1, ..., vn} we can always construct a sequence (z1, ..., zn) such that the set Z = {z1, ..., zn} is mapped one-to-one to the set V , and such that zi is an ancestor of zj (i.e. there exists a path from zi to zj in G) for all i < j. Therefore, the set of parents paG(zj) has to be a subset of the set {zi|i < j}, from which Proposition 5 follows.\n\nB JOINT IDENTIFIABILITY OF REPRESENTATION AND CAUSAL MECHANISM\n\nBefore proving our identifiability result, we must first introduce a lemma about the commutativity of certain piece-wise linear functions.\n\nLemma 6. Consider a Leaky ReLU activation function (He et al., 2015), defined as:\n\nRC,D(x) =\n\n(cid:26)Cx if x ≥ 0 Dx if x < 0\n\n,\n\n(12)\n\nwith constants C > 0, C ̸= 1, D > 0, D ̸= 1, C ̸= D. Then the class of functions φ that commute with R, i.e.\n\nφ(RC,D(x)) = RC,D(φ(x)), ∀x is the set of monotonic piece-wise linear homogeneous functions with one piece in each half-plane, i.e.\n\n(13)\n\nφ(x) =\n\n(cid:26)Ax if x ≥ 0 Bx if x < 0\n\n, A, B > 0.\n\n(14)\n\nProof. To solve the commutativity identity in Eq. 13, we partition it into four domains, namely {x ≥ 0, x < 0} × {φ(x) ≥ 0, φ(x) < 0}:\n\nIf x < 0, φ(x) < 0 ⇒ Cφ(x) = φ(Cx) ⇒ φ(x) = Ax If x < 0, φ(x) ≥ 0 ⇒ Cφ(x) = φ(Dx) ⇒ φ(x) = 0 If x ≥ 0, φ(x) < 0 ⇒ Dφ(x) = φ(Cx) ⇒ φ(x) = 0 If x ≥ 0, φ(x) ≥ 0 ⇒ Dφ(x) = φ(Dx) ⇒ φ(x) = Bx\n\n(15)\n\n(16)\n\n(17) (18)\n\nEquations 15 and 18 amount to solving the equality Ef (x) = φ(Ex) which is satisfied for any homogeneous linear function φ assuming E ̸= 1. Equations 16 and 17 amount to solving the equality Ef (x) = φ(F x) which is only satisfied for φ(x) = 0 assuming E ̸= 1, E ̸= F . Combining the results from Equations 15-18 the result follows.\n\n13\n\nInputqμq,Σq~zpEncoderDecoderzi∗=fiypaGi+ni,∀iStructural Equation Model (SEM)μ,ΣL2Reconstruction lossKLKL-divergenceGaussian samplingUnder review as a conference paper at ICLR 2023\n\nWe can now introduce our main result.\n\nTheorem 7. For a causal VAE model (Eq. 3 and Eq. 5 with y ≡ z), assume the following conditions:\n\n1. The encoder q(z|x) and decoder p(x|z) are Gaussians (Eq. 4) parameterized by deep networks,\n\ncontaining a Leaky ReLU (Eq. 12) as its last layer and first layer, respectively.\n\n2. The outputs of the SEM fi are de-meaned and normalized, i.e. they are composed with batch normalization (BN) operators: zi = BN(f (zpa(i))) where BN(u) = (u − E[u])/(cid:112)Var[u], or they have a fixed constant scale.\n\nThen the model is identifiable up to the following indeterminacies, i.e. denoting the optimal parameters by θ∗, there is a different set of parameters θ such that pθ∗(x) = pθ(x) only under the following learnable transformations:\n\n1. Simultaneous shifts by b and orthogonal transformations R of the encoder’s input and decoder’s\n\noutput, i.e.: μq(x) ← μq(Rx + b), μp(z) ← R−1μp(z) − b.\n\n2. Latent variable permutations, i.e. reorderings of z. 3. If the SEM mechanisms fi contain additional symmetries (e.g. fi = fi ◦ S for some operator S),\n\nthen there will be indeterminacy up to application of that operator.\n\nProof. The training objective is to find the model parameters θ of the generative distribution pθ(x) such that it matches the observed empirical distribution p(x). The parameters are assumed to exist if the function class of pθ(x) is sufficiently expressive, as is the case in over-parameterized deep networks. We also assume that there exists a true model θ∗ which generates the data distribution pθ∗ (x). Then the claim of non-identifiability is that there exists at least one different parameterization θ for which pθ(x) = pθ∗ (x), i.e. it is possible to learn another model that also fits the true data distribution but has different parameters θ. We can express the lower bound on the data distribution using the evidence lower bound (ELBO, Eq. 3)\n\nln pθ∗ (x) ≥ Ex∼p(x)[Eqθ(z|x)[ln pθ(x|z)] − KL[qθ(z|x)||pθ(z)]]\n\n(19)\n\nwith encoder qθ(z|x), decoder pθ(x|z), and learnable prior pθ(z) generated using the SEM (slightly overloading θ to include all parameters of the model).\n\nUnder non-identifiability, we assume that we can find another set of parameters θ that fit the data perfectly: pθ(x) = pθ∗ (x). In that case Eq. 19 becomes a strict equality, implying the following two conditions:\n\nln pθ∗ (x) = Ex∼p(x)[Eqθ(z|x)[ln pθ(x|z)]] 0 = Ex∼p(x)[KL[qθ(z|x)||pθ(z)]],\n\n(20)\n\n(21)\n\nIn words, the expectation of the log likelihood (first term of right-hand side of Eq. 19) becomes the exact log evidence ln pθ∗ (x), while the posterior qθ(z|x) fits the prior pθ(z) exactly (their KLdivergence is zero). Furthermore, for the optimal model θ∗ we know that ln pθ∗ (x) = Ex∼p(x)[Eqθ∗ (z|x)[ln pθ∗ (x|z)]] Note the subtle difference from Eq. 20 is to use θ∗ instead of θ. We can thus combine the results for both models θ∗ (Equation 22) and θ (Equation 20) as\n\n(22)\n\nEx∼p(x)[Eqθ(z|x)[ln pθ(x|z)]] = Ex∼p(x)[Eqθ∗ (z|x)[ln pθ∗ (x|z)]] We will write the left-hand side (LHS) of Eq. 23 in a form that makes its equivalence classes more obvious, using function composition. In order to do that, we first recall Eq. 4, which we rewrite here (making θ explicit) for convenience:\n\n(23)\n\nqθ(z|x) = N (z|μθ(x), Σθ(x))\n\n(24)\n\nThe reparametrization trick, which is at the core of VAE implementations, allows us to sample from the probabilistic encoder qθ(z|x) with a deterministic function qθ,ε(x) (typically a deep network) that takes samples ε from a fixed normal distribution and applies an affine transformation to them:\n\nqθ,ε(x) = μθ(x) + Σθ(x) ε,\n\nε ∼ N (0, 1).\n\nThe inner expectation in the LHS of Eq. 23 can then be written as:\n\nEz∼qθ(z|x)[ln pθ(x|z)] = Eε∼N (0,1)[ln pθ(x|qθ,ε(x))].\n\n(25)\n\n(26)\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nBy Eq. 4 the decoder pθ(x|z) is also a Gaussian with parameterized mean μp equivalent to:\n\nθ(z),1 so Eq. 23 is\n\nEx∼p(x)Ez∼qθ(z|x) ln pθ(x|z) = Ex∼p(x)Ez∼qθ(z|x)\n\n(cid:104)\n\n−γ ∥μp\n\n= Ex∼p(x)Ez∼qθ(z|x) [Lx(μp\n\nθ(z) − x∥2(cid:105) θ(z))] , Lx(x′) = −γ ∥x′ − x∥2 ,\n\n(27)\n\n(28)\n\nabsorbing constants into γ. Finally, the LHS of Eq. 23 is then equivalent to the expectation of a composition of deterministic functions (applied from right to left):\n\nEx∼p(x),ε∼p(ε)[Lx ◦ μp\n\nθ ◦ qθ,ε ◦ x].\n\n(29)\n\nThis makes explicit the order of operations in computing the reconstruction loss, and separates out all non-deterministic elements into the expectation (namely by using the reparameterization trick with ε ∼ p(ε) = N (0, 1)).\n\nWe can now enumerate the equivalence classes of Eq. 23, by inserting identity operators I = g−1◦g = h−1 ◦ h (for arbitrary invertible functions g, h) between compositions of learnable functions in Eq. 29. Inserting these identities, Eq. 29 is equivalent to\n\nEx∼p(x),ε∼p(ε)[Lx ◦ (h ◦ h−1) ◦ μp θ ◦ (g−1 ◦ g) ◦ qθ,ε ◦ (h ◦ h−1) ◦ x] = Ex∼p(x),ε∼p(ε)[(Lx ◦ h) ◦ (h−1 ◦ μp θ ◦ g−1 ) ◦ (h−1 ◦ x ) ◦ (g ◦ qθ,ε ◦ h (cid:124) (cid:123)(cid:122) (cid:125) (cid:125) (cid:123)(cid:122) (cid:125) ̄x ̄μp\n\n(cid:123)(cid:122) ̄qθ,ε\n\n(cid:124)\n\n(cid:124)\n\n)]\n\nθ\n\n(30)\n\n(31)\n\nwhere we grouped the arbitrary functions with the decoder as ̄μp the input as ̄x. We can then apply the substitution ̄x = h−1 ◦ x, using the fact that\n\nθ, with the encoder as ̄qθ,ε, and with\n\nLx(x′) = γ ∥h(x′) − h(x)∥2 = Lh◦x(h ◦ x′)\n\n(32)\n\nis only ever true if h is an orthogonal linear transformation plus a constant (since Euclidean distances are invariant only under generalized rotations and translations),2 and thus obtaining the equivalent reparameterization\n\nE ̄x∼p( ̄x),ε∼p(ε)[L ̄x ◦ ̄μp\n\nθ ◦ ̄qθ,ε ◦ ̄x].\n\n(33)\n\nThus the function class of h must necessarily be restricted to orthogonal transformations, induced by the Euclidean structure of L (Eq. 32).\n\nAs for g, it is restricted by the fact that it must be absorbed into the encoder, i.e. in Eq. 31 we must be able to group it with the previous encoder qθ,ε and define a new encoder\n\n ̄qθ,ε = g ◦ qθ,ε ◦ h\n\n(34)\n\nthat can still be implemented as a deep network (of the same function class as qθ,ε). Since we require that the encoder be followed by a Leaky ReLU RC,D (assumption 1 of Theorem 7), for this to be true, g must commute with RC,D, i.e.:\n\n ̄qθ,ε = g ◦ qθ,ε ◦ h = g ◦ RC,D ◦ q′\n\n◦ h = RC,D ◦ g ◦ q′\n\nθ,ε ◦ h\n\n(35)\n\n(cid:124)\n\n(cid:123)(cid:122) qθ,ε\n\nθ,ε (cid:125)\n\nwhere the first equality is taken from Eq. 31, in the second equality we decompose the network qθ,ε into its final Leaky ReLU RC,D and the rest of the network q′ θ,ε, and finally in the last step we use the commutativity of g and RC,D (Lemma. 6). Since by Lemma. 6 only monotonic piece-wise linear homogeneous functions commute with RC,D, this restricts the class of admissible functions for g to that class.\n\nAn identical conclusion follows for g−1, as long as the first layer of the decoder is also a Leaky ReLU (by assumption 1).\n\n1Assuming identity covariance for simplicity, a common assumption in implementations, and which does not\n\nmaterially change the result.\n\n2This is the same reason why the function h and its inverse are used twice in Eq. 30 instead of inserting two different functions, e.g. h1 and h2 and their inverses; the same generalized rotation h must be applied to both inputs of the Euclidean distance in Eq. 32.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nThis means that the latent variables are non-identifiable up to g implementing individual rescaling of each variable and variable permutation, unless we impose further constraints. We can impose mild structural constraints on the SEM to fix the rescaling, namely assuming that the output of each fθ,i(zpa(i)) has fixed scale. This can be achieved by either: 1) batch-normalization; 2) a SEM with no learnable scale. Batch-normalization, which is used by many common deep network models, fixes the distribution mean to zero and the variance to one for each dimension, removing scaling and shifting degrees-of-freedom. As a special case, the quadratic model used in Section 5.1) does not have learnable scale parameters. The only remaining degrees of freedom are variable permutations, which are unavoidable in latent variable models.\n\nHaving characterized the equivalence classes of Eq. 20, we must now consider those of Eq. 21. Using the same reparameterization trick as in Eq. 25, the KL-divergence in Eq. 21 is equivalent to\n\nKL[qθ(z|x)||pθ(z)] = Ez∼qθ(z|x)[ln qθ(z|x) − ln pθ(z)]\n\n= Eε∼p(ε)[ln qθ,ε(x) − ln pθ(qθ,ε(x))]\n\nTherefore, we can also express it using a composition of operators:\n\nEx∼p(x),ε∼p(ε)[L ◦ qθ,ε ◦ x − L ◦ μf\n\nθ ◦ qθ,ε ◦ x],\n\n(36)\n\n(37)\n\n(38)\n\nwith μf θ the mean of the Gaussian output by f (by Theorem 2), assuming variance one as before for simplicity. We can now insert the same identity operators and follow an identical derivation to Eq. 30-33, which recovers the exact same equivalence classes as before. The only identity that can be added differently from Eq. 30 will be an operator S for which μf θ ◦ S), so indeterminacy up to such symmetries of f is the only other possibility.\n\nθ is invariant (μf\n\nθ = μf\n\nC PROOF OF MEAN AND VARIANCE PROPAGATION FOR LINEAR ANMS\n\nTheorem 8. Consider a linear ANM defined as zi = aT and ni ∼ N (ni|0, σ2 zeros in ai. Then the ANM’s joint probability is given by p(z) = N (z | μ, Σ) with\n\ni z1,...,i−1 + bi + ni, with ai ∈ Ri−1, bi ∈ R i ) for i = 1, . . . , d. Missing edges in the causal graph can be represented as\n\nμ =\n\n(cid:32) 2\n\n(cid:89)\n\ni=d\n\n(cid:33)\n\nAi\n\nb, Σ =\n\n(cid:32) 2\n\n(cid:89)\n\n(cid:33)\n\nAi\n\ni=d\n\ndiag i=1,...,d\n\n(σ2 i )\n\n\n\n\n\nAi =\n\nI(i−1)×(i−1) Oi−1 O(i−1)×(d−i) 1\n\naT i\n\nOT d−i I(d−i)×(d−i)\n\nO(d−i)×(i−1) Od−i\n\n\n\n ,\n\n(39)\n\n(cid:32) 2\n\n(cid:89)\n\n(cid:33)T\n\nAi\n\n,\n\ni=d\n\nb =\n\n\n\n  ,\n\n\n\n \n\nb1 ... bd\n\nwhere Ik×k denotes an identity matrix, while Ok and Ok×l denote a zero column vector and a zero matrix, respectively.\n\nProof. We can write the ANM zi = aT\n\nz1,...,i =\n\n(cid:20) I(i−1)×(i−1) aT i\n\ni z1,...,i−1 + bi + ni in a recursive multivariable form as (cid:21)\n\n(cid:21)\n\n(cid:21)\n\nz1,...,i−1 +\n\n(cid:20) Oi−1 bi\n\n(cid:20) Oi−1 1\n\n+\n\nni\n\n(40)\n\nApplying the formula for linear combination of Gaussians (Bishop, 2006, Ch. 8.1.4)\n\nx ∼ N (x|μx, Σx), y ∼ N (y|μy, Σy) =⇒ Ax+By+c ∼ N (x|Aμx+Bμy+c, AΣxAT +BΣyBT ) (41) to eq. 40 we can express the mean and covariance of z1,...,i as a function of the mean and covariance for z1,...,i−1 as\n\nμ1,...,i =\n\nΣ1,...,i =\n\n(cid:20) I(i−1)×(i−1) aT i\n(cid:20) I(i−1)×(i−1) aT i\n\n(cid:21)\n\n(cid:21)\n\nμ1,...,i−1 +\n\nΣ1,...,i−1\n\n(cid:21)\n\n(cid:20) Oi−1 bi (cid:20) I(i−1)×(i−1) aT i\n\n(42)\n\n(cid:21)T\n\n(cid:20) Oi−1 1\n\n(cid:21)\n\n(cid:20) Oi−1 1\n\nσ2\n\ni\n\n+\n\n(cid:21)T\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nwhere we have used\n\nμ1,...,i = E [z1,...,i] , Σ1,...,i = E (cid:2)(z1,...,i − μ1,...,i)(z1,...,i − μ1,...,i)T (cid:3) ,\n\n(43)\n\nand for the noise variables n it holds that E [n] = 0 and E (cid:2)nnT (cid:3) = diagi(σ2 i ). Now assume that eq. 39 holds for some d = k. Inserting expressions for μ1,...,k and Σ1,...,k from eq. 39 into eq. 42 and extending the matrices with zeros and ones we obtain\n\nμ1,...,k+1 =\n\nΣ1,...,k+1 =\n\n(cid:20) Ik×k Ok 1\n\naT\n\nk+1\n\n(cid:20) Ik×k Ok 1\n\naT\n\nk+1\n\n(cid:21) (cid:32) 2 (cid:89)\n\ni=k\n\n(cid:21) (cid:32) 2 (cid:89)\n\ni=k\n\n(cid:20) Ai,d=k Ok 1\n\nOT k\n\n(cid:20) Ai,d=k Ok 1\n\nOT k\n\n(cid:21)(cid:33) (cid:20) b1,...,k\n\n(cid:21)\n\n0\n\n(cid:21)\n\n(cid:20) Ok bk+1\n\n+\n\n(44)\n\n(cid:21)(cid:33) (cid:34) diag\n\n(σ2\n\ni ) Ok\n\n(cid:35)\n\ni=1,...,k OT k\n\n0\n\n(cid:21)\n\nσ2\n\nk+1\n\n(cid:32) 2\n\n(cid:89)\n\ni=k\n\n(cid:20) Ai,d=k Ok 1\n\nOT k\n\n(cid:21)(cid:33)T (cid:20) Ik×k Ok aT\n\n1\n\nk+1\n\n(cid:21)T\n\n+\n\n(cid:20) Ok×k Ok 1\n\nOT k\n\nIdentifying\n\nAk+1,d=k+1 =\n\neq. 44 becomes\n\n(cid:20) Ik×k Ok 1\n\naT\n\nk+1\n\n(cid:21)\n\n, Ai,d=k+1 =\n\n(cid:20) Ai,d=k Ok 1\n\nOT k\n\n(cid:21)\n\n,\n\nb1,...,k+1 =\n\n(cid:21)\n\n(cid:20) b1,...,k bk+1\n\nμ1,...,k+1 =\n\nΣ1,...,k+1 =\n\n(cid:32) 2\n\n(cid:89)\n\ni=k+1\n\n(cid:32) 2\n\n(cid:89)\n\ni=k+1\n\n(cid:33)\n\nAi,d=k+1\n\nb1,...,k+1\n\n(cid:33)\n\nAi,d=k+1\n\ndiag i=1,...,k+1\n\n(σ2 i )\n\n(cid:32) 2\n\n(cid:89)\n\ni=k+1\n\n(cid:33)T\n\nAi,d=k+1\n\n(45)\n\n(46)\n\n(47)\n\nwhich completes the inductive step d = k → k + 1. Finally, we apply eq. 41 to relation 40 for d = 2 to obtain\n\n(cid:21)\n\nμ1 +\n\n(cid:21)\n\n(cid:20) 0 b2\n\nμ1,2 =\n\nΣ1,2 =\n\n(cid:20) 1 a2 (cid:20) 1 a2\n\n(cid:21)\n\nΣ1\n\n(cid:20) 1 a2\n\n(cid:21)T\n\n(cid:20) 0 1\n\n(cid:21)\n\n(cid:20) 0 1\n\nσ2\n\n2\n\n+\n\n(cid:21)T\n\n= A2diag(σ2\n\n1, σ2\n\n2)AT\n\n2\n\n= A2b1,2\n\n(48)\n\nwhere we have used μ1 = b1 and Σ1 = σ2 the induction principle the relation 39 holds for all d ≥ 2.\n\n1, which shows that eq. 39 holds for d = 2. Therefore, by\n\nD PROOF OF LINEARISATION OF NON-LINEAR SEMS\n\nTheorem 9. The best linear approximation (in the least-squares sense) of an ANM zi = fi(z1,...,i−1) + ni (eq. 5, with missing edges in the causal graph corresponding to ignored inputs in fi), around a pivot point z◦, is given by eq. 39 (Theorem 8) with\n\nai =\n\n∂fi(z1,...,i−1) ∂z1,...,i−1\n\n(cid:12) (cid:12) (cid:12) (cid:12)z1,...,i−1=z◦ 1,...,i−1 = f (z1,...i−1) + ni and ni ∼ N (0, σ2\n\n1,...,i−1\n\nwhere zo\n\ni ) with learnable σi.\n\nand bi = fi(z◦\n\n1,...,i−1) + ni − aT\n\ni z◦\n\n1,...,i−1,\n\n(49)\n\nProof. By Taylor’s theorem, expanding fi(z1,...,i−1) + ni around z◦\n\n1,...,i−1 up to first order gives\n\nfi(z1,...,i−1) ≈ fi(z◦\n\n1,...,i−1) + ni + (z1,...,i−1 − z◦\n\n1,...,i−1)T ∂fi(z1,...,i−1)\n\n∂z1,...,i−1\n\n(cid:12) (cid:12) (cid:12) (cid:12)z1,...,i−1=z◦\n\n1,...,i−1\n\n≈ aT\n\n(51) where ai and bi are given by eq. 49. We can now use this linearisation of fi to define a linearised SEM as zi = aT\n\ni z1,...,i−1 + bi and using this with Theorem 8 the result follows.\n\ni z1,...,i−1 + bi\n\n(50)\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: The temporal causal-prior VAE architecture, for a sequence of 3 video frames (plus a random frame to represent time-invariant information, such as background).\n\nFigure 10: Reconstruction accuracy as a function of parameter controlling graph sparsity.\n\nFigure 11: Structural Hamming Distance as a function of parameter controlling graph sparsity.\n\nE TRAINING DETAILS FOR THE CAUSAL-PRIOR VAE\n\nWe used a custom dataset of 76800 binary images of size 64 × 64 containing ovals generated at 6 different scales s, 40 rotations r, 32 horizontal positions x and 10 vertical positions y, where y = x2 + ny and s and r factors are sampled independently. We trained our causal-prior VAE and two isotropic-prior VAEs for comparison, one with a lower β and one with higher β (where β denotes the factor multiplying the KL divergence as defined in (Higgins et al., 2016a)). The encoder is a 3-layer MLP with 64 hidden ReLU units in each layer; the decoder is identical but with 4 layers, and there are 5 latent variables z. We use the Adagrad optimizer (Duchi et al., 2011) with learning rate 0.003 on batches of 100 samples, until convergence. The architecture is shown in Figure 8.\n\nF TRAINING DETAILS FOR THE TEMPORAL CAUSAL-PRIOR VAE\n\nWe created a custom dataset of 3-frame 20 × 20 px video sequences of the main Super Mario Bros character moving linearly in a random direction and with a random speed on different backgrounds, where the character’s positions are given by x1, y1, x2, y2 ∼ U(7, 12), x3 = 2x2 − x1, y3 = 2y2 − y1 where xi and yi are the horizontal and vertical position at frame i and U is the uniform distribution. We train our causal-prior VAE with 12 variables, 4 per time frame, and allow the SEM to learn arbitrary linear relationships between them. The architecture is shown in Figure 9. Each dataset sample consists of a tuple consisting of a background and 3 consecutive frames where the character moves linearly. Each of the 3 frames is then encoded separately into a 4-variate Gaussian distribution\n\n18\n\nBG frameFrame TFrame T+1Frame T+2qTI~μ!\",Σ!\"z!\"qμ#,%,Σ#,%000000μ#,&,Σ#,&μ#,’,Σ#,’μ#,Σ(,~zpz%z&̂zEncoder(time-inv.)EncoderDecoderz)∗=f)y+,!)+n),∀iStructural Equation Model (SEM)z’μ,ΣL2Reconstr. lossKLKL-divergence104102100102edge penalisation coefficient0.9910.9920.9930.9940.9950.9960.997reconstruction accuracy104102100102edge penalisation coefficient024681012SHDUnder review as a conference paper at ICLR 2023\n\nand these are concatenated to form a 12-variate Gaussian, which is compared with its closest match in the SEM-sampled local Gaussian distribution. The distribution is then sampled and split into 4 samples per time frame and these are concatenated with the background latents and decoded back into 3 frames which are then compared using L2 loss with the 3 frames on the input. Additionally, the latents from the first two frames are passed through the SEM to predict their value at the next time frame and this is decoded and compared using L2 loss with the third frame on the input. The architecture is shown in Figure 9.\n\nG QUANTITATIVE EVALUATION OF LEARNED TEMPORAL GRAPHS\n\nReconstruction accuracy vs. graph complexity Figure 10 shows for the temporal experiments the reconstruction accuracy achieved with the model as a function of the parameter controlling graph sparsity (−1/ ln(s) in Equation 7) after training for 250 epochs. The plot shows that for weak edge penalisation the reconstruction accuracy is good (around 99.7%) while if the edge penalisation is too large the accuracy drops (to around 99.1%). This is because for weak edge penalisation the graph is relatively dense which allows the SEM to model the time-based causal relationships between the variables and when the penalisation is too big the graph becomes too sparse to be able to model these relationships. Somewhere around the value of the coefficient 10−2 the graph becomes as sparse as possible while still keeping the reconstruction accuracy high, and this is the area from which we select the graph.\n\nStructural Hamming Distance vs. graph complexity Figure 11 shows for the temporal experiments the Structural Hamming Distance between the learned and the ground truth graph as a function of the parameter controlling graph sparsity (−1/ ln(s) in Equation 7) after training for 250 epochs. The SHD is computed by counting how many edges need to be inserted or removed to obtain the ground truth graph (up to a permutation of variables within each time step). In the range where the coefficient is below approx. 10−3 the edge penalisation is too weak resulting in a graph with too many edges (thus a high SHD) and in the region above approx. 10−2 the graph becomes too sparse with no edges (thus also resulting in high SHD). The region between 10−3 and 10−2 corresponds to the region where the learned graph has exactly the same structure as the ground truth graph (and thus SHD is zero).\n\n19",
    "reference": "# Summary Of The Paper\n\nIn traditional causal inference, one often assumes access to the structured variables and the task is to discover causal relationships between them. In recent years, there has been a growing amount of interest to tackle the question when these variables themselves are unknown and this task is termed as \"causal representation learning\". To this end, the first task is to infer these ``causal variables\" themselves. Some existing works such as the causal VAE have tackled this question but in the context of linear structural equation models for the latents. In this work, the authors extend it to general non-linear SEMs. The authors leverage additive models as they can be identified with observational data. The method proposed by the authors extends standard VAEs that leverage independence of latents as prior to a non-linear SEM driven prior. To train VAEs, it is far more efficient to have a closed form expression for the KL divergences, which one cannot with standard non-linear SEM based prior. To tackle this the authors propose to break SEM into a piecewise linear SEM. The authors validate their method on synthetic datasets.\n\n# Strength And Weaknesses\n\n**Strengths**\nThe authors study an important and timely problem. Currently, we lack methods that can recover latents with learnable causal priors.\nThe authors have done a good job of exposition starting with basics of the VAE and building the structure of the paper. \n\n**Weaknesses**\n\nI have many concerns with the paper that I highlight in a bulleted form below. \n\n1. **Method for local linearization** The authors state that they make a piecewise linear approximation of the non-linear SEM around pivot point. The authors do not state how they select the pivot points. Since the approximation is valid in a neighborhood only, it is important to have sufficiently many pivot points. This is central to the paper and it is appalling that it is missing. Further, if there was such a method used a natural question to ask would be if the method has some identification guarantees or it is completely ad-hoc. \n\n2. **Experiments have major issues** There are two sets of experiments that authors carry out. In the first experiment the authors use a quadratic relationship between the latents. \n\n   In the first set of experiments the authors use the known quadratic prior. If the prior is not learned then the experiment is a mere sanity check and does not provide any valuable insight. In fact, if the model is already aware that the relationship is quadratic, then the SEM effectively becomes linear by treating z^2 as the parent feature. If the SEM becomes linear, then doesn't that go against the whole point of the paper. \n\n   In the second set of experiments, the authors resort to using a linear SEM again. I am not sure why authors do that. If the point of the paper is to achieve identification for non-linear SEMs why use linear SEMs. Also, if it is linear and Gaussian would you not run into non-identification issues. \n\n2.  **Crucial comparisons are missing** The authors operate in time series settings to conduct their main experiments. For these experiments, why do they not compare against the work https://proceedings.mlr.press/v177/lachapelle22a/lachapelle22a.pdf. In the work referenced the authors learn a causal graph in time. See Section 4, Figure 3 of the paper. The authors rely on time-sparsity as the regularization to achieve identification. \n\n3.  **Lack of theoretical guarantees**  As highlighted above, the paper does not provide valuable experimental insights so I wanted to comment on theoretical guarantees. The paper also does not provide theoretical guarantees either. For instance, many of the works that authors cite do provide identification guarantees. The current theorems in the paper do not provide any new insights. \n\n4. I would recommend the authors to do a significant revamp of the experiments. Also, it should be made clear how you select pivots and manage the non-linear case. These changes will improve the paper. Finally, any insights from the theory even for two variable case would be valuable. \n\n5. The authors in Section 4.6 for some reason state the identification guarantees for non-linear SEM are empirically shown. There are many works and this https://arxiv.org/pdf/1205.2536.pdf is one example where identification has been shown in multivariate case.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written. There are major issues with the quality of the work, please refer to the weaknesses section. While the paper is original but the lack of proper description of the method, poor experimentation makes it very weak.\n\n# Summary Of The Review\n\nThe current work proposes a new class of VAEs that accommondate a SEM based prior. The authors propose a variational approximation for it. However, the method used for arriving at the approximation is not described, central parts of it namely pivot selection is missing. The main contribution of the paper is supposed to be non-linear additive SEMs but the main experiments are conducted with linear SEMs. Key comparisons with other works are missing as well.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nA PROBABILISTIC FRAMEWORK FOR TASK-ALIGNED INTRA- AND INTER-AREA NEURAL MANIFOLD ESTIMATION\n\nEdoardo Balzani, Jean Paul Noel, Pedro Herrero-Vidal, & Dora E. Angelaki∗ Center for Neural Science New York University New York, NY, 10003 {eb162,jpn5,pmh314,da93}@nyu.edu\n\nCristina Savin † Center for Neural Science Center for Data Science New York University New York, NY, 10003 cs5360@nyu.edu\n\nABSTRACT\n\nLatent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution.\n\n1\n\nINTRODUCTION\n\nSystems neuroscience is gradually shifting from relatively simple and controlled tasks, to studying naturalistic closed-loop behaviors where no two observations (i.e.,“trials”) are alike (Michaiel et al., 2020; Noel et al., 2021). Concurrently, neurophysiological techniques are advancing rapidly (Stevenson & Kording, 2011; Angotzi et al., 2019; Boi et al., 2020) to allow recording from an ever-increasing number of simultaneous neurons (i.e., “neural populations”) and across multiple brain areas. These trends lead to a pressing need for statistical tools that compactly characterize the statistics of neural activity within and across brain regions. Dimensionality reduction techniques are a popular tool for interrogating the structure of neural responses (Cunningham & Byron, 2014). However, as neural responses are driven by increasingly complex task features, the main axes of variability extracted using these techniques often intermix task and nuisance variables, making them hard to interpret. Alternatively, dimensionality reduction techniques that do allow for estimating task-aligned axes of variability (Brendel et al., 2011; Semedo et al., 2019; Keeley et al., 2020; Glaser et al., 2020; Hurwitz et al., 2021), do not apply to communication between brain areas, and/or necessitate trial repeat structure that does not occur in natural behavior.\n\nHere, we introduce a probabilistic approach for learning interpretable task-relevant neural manifolds that capture both intra- and inter-area neural variability with single trial resolution. Task Aligned Manifold Estimation with Gaussian Process priors (TAME-GP) incorporates elements of demixed\n\n∗Webpage: https://angelakilabnyu.org/ †Webpage: http://www.cns.nyu.edu/ csavin\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nPCA (dPCA; Machens (2010); Kobak et al. (2016)) and probabilistic canonical correlation analysis (pCCA; Bach & Jordan (2005))1 into a graphical model that additionally includes biologically relevant Poisson noise. The model uses a Gaussian Process (GP) prior to enforce temporal smoothness, which allows for robust reconstruction of single-trial latent dynamics (see Damianou et al. (2016) for a similar approach using Gaussian observation noise). We demonstrate the robustness and flexibility of TAME-GP in comparison to alternative approaches using synthetic data and neural recordings from rodents and primates during naturalistic tasks. This reveals TAME-GP as a valuable tool for dissecting sources of variability within and across brain areas during behavior.\n\nRelated work. Dimensionality reduction is usually achieved by unsupervised methods that identify axes of maximal variability in the data, such as PCA. In neuroscience, this is often accompanied by additional smoothing over time reflecting the underlying neural dynamics (e.g., Gaussian process factor analysis (GPFA) (Yu et al., 2008); see GP-LVM (Ek & Lawrence, 2009) for similar approaches outside of neuroscience). This low dimensional projection is followed by a post hoc interpretation of latents in the context of behavioral variables, often by visualization. Alternative approaches such as dPCA (Machens, 2010; Kobak et al., 2016) explicitly look for axes of neural variability that correlate with task variables of interest (see also Zhou & Wei (2020) for a nonlinear version). However, these require partitioning trials into relatively few categories, based on experimental conditions or behavioral choices and averaging within conditions. This makes them unusable in naturalistic tasks where a single trial treatment is needed. Similarly, SNP-GPFA (Keeley et al., 2020) can partition (multi-region) neural activity into ‘shared signal’ and ‘private noise’ components, but only using data with stimulus repeats. Under ‘no-repeat’ conditions, pCCA (Bach & Jordan, 2005) can find subspaces of maximal cross-correlation between linear projections of task variables and neural responses (under gaussian noise assumptions), without the need for a priori grouping of trials by experimental condition or choice. This approach can also be applied for determining shared axes of co-variability across areas, an analog for communication subspaces (Semedo et al., 2019). Nonetheless, its noise model assumptions are mismatched to neural data. More fundamentally, pCCA only considers pairwise relationships, preventing a joint multi-area and task variables analysis. Overall, existing approaches come with practical limitations and do not directly address the routing of task-relevant information across brain areas.\n\n2 TASK-ALIGNED MANIFOLD ESTIMATION WITH GP PRIORS (TAME-GP)\n\nIn its most general form, the graphical model of TAME-GP models a set of spike-count population responses x(j) from up to n different areas,2 together with task variable of interest y (Fig. 1A). The neural responses are driven by a set of n + 1 low-dimensional latent variables z(j). Specifically, the responses of neuron i in area j arise as a linear combination of private latent variability z(j) and shared latents z(0), which reflect task interpretable aspects of the underlying dynamics, with Poisson noise and an exponential link function:\n\n(cid:16)\n\np\n\nx(j)\n\ni\n\n|z(0:n)(cid:17)\n\n= Poisson\n\n(cid:16)\n\nexp\n\n(cid:16)\n\nW (0,j)\n\ni\n\nz(0) + W (j,j)\n\ni\n\nz(j) + h(j)\n\ni\n\n(cid:17)(cid:17)\n\n,\n\n(1)\n\nwith parameters W(0/j,j) and h(j).\n\nTo make latents interpretable with respect to task variables y, we adapt a probabilistic framing of CCA (Bach & Jordan, 2005) to introduces dependencies between any of the latents z(k)), which could be private or shared across areas, and y:\n\n(cid:16)\n\ny|z(0)(cid:17)\n\np\n\n= N\n\n(cid:16)\n\ny; Cz(0) + d, Ψ\n\n(cid:17)\n\n, with parameters C, d, Ψ.\n\n(2)\n\n1See Appendix A.1 for background on probabilistic PCA, CCA and their relation to TAME-GP. 2Variables x(j), y are tensors with dimensions corresponding to 1) an area-specific number of neurons/ task variable dimension, 2) time within trial, and 3) trial index. We make indices explicit only where strictly needed.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFigure 1: A. TAME-GP generative model. z(0) denotes shared latent dimensions while z(i) denote private latents of the corresponding area i; y denotes the task variables. B. Example draws of spiking activity and a task variable from the TAME-GP graphical model. C. Model log-likelihood as a function of the EM iteration (left) and cross-validated leave-one-neuron-out marginal likelihood as a function of z(0) dimension (right). D-F. Latent variables estimation for within model simulated data: ground truth latent factors and model posterior mean ± 95% CI for three latent dimensions.\n\nFinally, we regularize all latents to be smooth over time, through the introduction of a Gaussian Process prior, as in GPFA (Yu et al., 2008),\n\nz(j) ∼ GP (0, kj(·, ·)) , ,\n\n(cid:16)\n\nkj\n\nt,i , z(j) z(j)\n\nt′,i′\n\n(cid:17)\n\n(cid:32)\n\n= δii′ exp\n\n−\n\n(cid:33)\n\n,\n\n(t − t′)2 2τ (j)\n\ni\n\n(3)\n\n(4)\n\nwith area and dimension specific hyperparameters τ , z(j) time t, and δii′ is the Kronecker delta.\n\nt,i is the i-th component of the j-th latent at\n\nPutting these elements together results in a factorization of the joint distribution of the form, p (cid:0)x(1:n), y, z(0:n)(cid:1) = (cid:81)n for a unified mathematical treatment of several estimation tasks of interest. We will detail key instances of this class that have practical relevance for neuroscience when presenting our numerical results below.\n\nj=0 p (cid:0)z(j)(cid:1) p (cid:0)y|z(0)(cid:1) (cid:81)\n\n.This general form allows\n\n|z(0), z(j)(cid:17)\n\nx(j)\n\ni,j p\n\n(cid:16)\n\ni\n\n3 EM-BASED PARAMETER LEARNING\n\nE-step Since a closed form solution of the posterior is not available (due to the Poisson noise), we construct a Laplace approximation of the posterior 3, p (z|x, y, θ) ≈ q (z|x, y, θ) = N (cid:0)z; ˆz, −H−1(cid:1) , where ˆz is the MAP of the joint log-likelihood and H is its corresponding Hessian. Both of these quantities are estimated numerically.\n\nThe MAP estimate is obtained by gradient descent on the joint log likelihood. The gradient of the joint log likelihood w.r.t. the latents can be written as\n\n∇z(j) log p (z, x, y) =\n\n(cid:88)\n\n(cid:16) (cid:88)\n\n∇z(j) log p\n\n(cid:16)\n\nz(j)(cid:17)\n\n+\n\n(cid:88)\n\n∇z(j) log p\n\n(cid:16)\n\nyt|z(0)\n\nt\n\n(cid:17)\n\nl (cid:88)\n\n+\n\nj≥0\n\nt>0\n\n(cid:88)\n\n∇z(j) log p\n\n(cid:16)\n\nx(j)\n\nt\n\n|z(0)\n\nt\n\n, z(j)\n\nt\n\n(cid:17) (cid:17) ,\n\nt>0\n\nj>0\n\n3We group latents in z, spike counts in x and θ =\n\n(cid:110)\n\nW(0/j,j), h(j), C, d, Ψ, τ (j)(cid:111)\n\n, to simplify notation.\n\n3\n\ntime [sec]0.02.5time [sec]050EM-iteration−6−4−20LL14−101leave-one-out LLz(0) dim.time [sec]z(2)z(0)...z(n)x(n)yx(2)z(2)x(1)z(1)ACDFE0.02.5z(0)0.02.5z(1)Bneuronstask var.neuronstime [sec]ground truth020500205002ground truthTAME-GP Published as a conference paper at ICLR 2023\n\nwhere l ∈ (1 : M ) refers to the trial number, explicit index omitted for brevity. For a given trial, expanding one term at the time we have\n\n∇z(j) log p (cid:16)\n\nlog p\n\n(cid:16)\n\nz(j)(cid:17)\n\ny|z(0)\n\nt\n\n∇z(0) (cid:16)\n\nt\n\n= −K(j)z(j) = C⊤Ψ−1 (cid:16) = W(k,j)⊤ (cid:16)\n\n(cid:17)\n\n(cid:17)\n\n∇z(k)\n\nt\n\nlog p\n\nx(j)\n\nt\n\n|z(0)\n\nt\n\n, z(j)\n\nt\n\n(cid:17)\n\nt − d\n\nyt − Cz(0) (cid:16)\n\nxt − exp\n\nW(0,j)z(0)\n\nt + W(j,j)z(j)\n\nt + h(j)(cid:17)(cid:17)\n\n,\n\nwhere j > 0, k ∈ {0, j} and K(j) the GP-prior covariance matrix (Eq. 3). The corresponding second moments are\n\n(cid:16)\n\nz(j)(cid:17)\n\n= −K(j) j ∈ (0 : n)\n\ny|z(0)\n\nt\n\n(cid:17)\n\n= −C⊤Ψ−1C\n\n∇2\n\nz(j) log p (cid:16)\n\nlog p\n\n∇2\n\nt\n\nz(0) (cid:16)\n\n(cid:17)\n\n(cid:16)\n\n(cid:16)\n\nt\n\nt\n\nt\n\nt\n\nt\n\nexp\n\nx(j)\n\n|z(0)\n\n, z(j)\n\nW(0,j)z(0)\n\n= −W(k,j)⊤diag\n\nlog p\n\n∇z(k)\n\n∇z(h) with h, k ∈ {0, j}. Inverting the D × D dimensional Hessian matrix is cubic in D = T (cid:80) j dj, where T is the trial length and dj denotes the dimensionality of latent z(j), which restricts the number and dimensionality of latents in practice. The Hessian of the log likelihood is sparse but does not have a factorized structure. Nonetheless, we can take advantage of the block matrix inversion theorem, to speed up the computation to O(T 3 (cid:80) j ) (see Appendix A.2), with additional improvements based on sparse GP methods (Wilson & Nickisch, 2015) left for future work.\n\nj d3\n\nt + W(j,j)z(j)\n\nt + h(j)(cid:17)(cid:17)\n\nW(h,j).\n\nM-step Given the approximate posterior q found in the E-step, the parameters updates can be derived analytically for a few parameters, and numerically for the rest (see Suppl. Info. A.3 for details). The other observation model parameters are computed numerically by optimizing the expected log-likelihood under the posterior. In particular, for neuron i in population j we have\n\n(cid:16)\n\nL\n\nW (0,j)\n\ni\n\n, W (j,j)\n\ni\n\n, hi\n\n(cid:17)\n\n=\n\n(cid:88)\n\n(cid:32)\n\nxti\n\nhi +\n\n(cid:104) W (0,j)\n\ni\n\nW (j,j)\n\ni\n\n(cid:34)\n\n(cid:105)\n\n(cid:35)(cid:33)\n\n(cid:32)\n\n− exp\n\nhi\n\nt\n\nμ(0) μ(j)\n\nt\n\n(cid:104)\n\n+\n\nW (0,j)\n\ni\n\nW (j,j)\n\ni\n\n(cid:34)\n\n(cid:105)\n\nt,l (cid:35)\n\nt\n\nμ(0) μ(j)\n\nt\n\n(cid:104) W (0,j)\n\ni\n\n1 2\n\n(cid:34)\n\n(cid:105)\n\nW (j,j)\n\ni\n\nt\n\nΣ(0,0) Σ(0,j)⊤\n\nt\n\nt\n\nΣ(0,j) Σ(j,j)\n\nt\n\n(cid:35) (cid:34)\n\ni\n\nW (0,j)⊤ W (j,j)⊤\n\ni\n\n(cid:35)(cid:33)\n\n.\n\n(5)\n\nFor each neural population, we jointly optimized the projection weights and the intercept of all neurons with a full Newton scheme by storing the inverse Hessian in compressed sparse row (CSR) format (see Appendix A.4 for the gradient and Hessian of L).\n\nThe GP-prior parameters were also learned from data by gradient based optimization (using the limited-memory Broyden–Fletcher–Goldfarb–Shanno scheme (Virtanen et al., 2020)). First, we set λ(j) to enforce a positive time constant. We define K(j)\n\n. The resulting objective function will\n\n), and optimize for λ(j)\n\n−eλ(j)\n\n= exp\n\nK(j)\n\n(cid:16)\n\n(cid:104)\n\n(cid:105)\n\ni\n\ni = − log(2τ (j) i ∈ RT ×T , such that (cid:17)\n\n(cid:16)\n\ni\n\ntake the form, L\n\nλ(j)\n\ni\n\ni\n\nts = −trace\n\n(cid:16)\n\nK(j)−1\n\ni\n\nEq[z(j)\n\n− log |K(j)\n\ni\n\n|. Gradients are provided in\n\ni (t − s)2(cid:17) i z(j)⊤\n\n(cid:17)\n\n]\n\ni\n\nAppendix A.5, together with the procedure for parameter initialization (Appendix A.6).\n\n4 RESULTS\n\nLatent reconstruction for within model data. To validate the estimation procedure, we first used a simulated dataset sampled from the TAME-GP graphical model, with predefined parameters. Specifically, we simulated two neural populations x(1) and x(2), each with 50 units and a one-dimensional task relevant variable y. We fixed the private latent factors z(1) and z(2) to two dimensions, and that of the shared factor z(0) to one. The projection weights W(j) and C, the intercept terms d and h(j), the observation variance matrix Φ, and the GP time constants of the factors were randomly assigned. The parameters were chosen such that the overall mean firing rate was about 20Hz in both\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nareas. We simulated spike counts at 50ms resolution for 200 draws from the process (which we will refer to as ‘trials’ in analogy to experiments), each lasting 2.5 seconds (see example trial in Fig. 1B). Given this data, we assessed the ability of our EM-based estimator to recover its true latent structure.4 The marginal log likelihood saturated after a relatively small number of EM iterations (Fig. 1C). As a basic test of our ability to determine the dimensionality of latents, we systematically varied the dimensionality of the shared latent, while fixing the dimensions of z(1) and z(2) to their ground truth value of 2. We found that the best model fit was achieved at the ground truth task dimension 1, demonstrating that we are able to infer true latent dimensionality from data (Fig.1D-F).\n\nFinally, we assessed the quality of the recovered latents in individual test trials. Due to known degeneracies, originally documented in linear gaussian latent models (Roweis & Ghahramani, 1999), the latent factors in TAME-GP are identifiable up to an affine transformation of the latent space. To address this, we used Procustes (Schönemann, 1966) to realign the latent axes back to the original space. The resulting posterior mean estimate of the latents show an excellent agreement with the ground truth factors (cross-validated linear regression R2 of 0.99 between the MAP estimate of latents and ground truth, Fig. 1 D-F), while the model predicted rates explained 98% of the ground truth firing rate variance. The ability to reconstruct ground truth structure for within model data persists when considering more than two areas with shared covariability (Suppl. Fig. S1). Overall, these numerical tests confirm that EM provides a veridical estimation of ground truth latent structure for within distribution data.\n\nTask-aligned latent reconstruction for simulated latent dynamical systems models. The simple graphical model of TAME-GP captures axes of neural variability of scientific interest, but is far from an accurate generative model for neural dynamics during behavior. To assess the ability of TAME-GP to extract underlying structure from complex and out-of-distribution neural data, we used latent dynamical systems models in which we can explicitly define the flow of information from external stimuli and between areas, in several scenarios of practical interest.\n\nThe first in silico experiment focuses on identifying axes of task-relevant variability in neural responses. As a simple test case, we modeled a single neural population with a 6d latent structure (Fig. 2A). Two of the latent dimensions were task-relevant, driven by an observed temporally smooth external input yt, while the other four dimensions were intrinsic to the circuit. The key distinction between this process and the TAME-GP model assumptions is that the observed task variable acts as an input drive to the underlying latent dynamics rather than mapping to the latents directly. The latent dynamics take the form of a multivariate AR(1),\n\n(cid:40)\n\nzpr,t+1 = Apr (zpr,t − μt) ∆t + ztr,t+1 = Atr (ztr,t − yt) ∆t +\n\n√\n\n√\n\n2∆t dw(0) 2∆t dw(1) ,\n\nt\n\nt\n\n(6)\n\nwhere Apr ∈ R4×4 and Atr ∈ R2×2 the private and task relevant dynamics, yt ∈ R2 and μt ∈ R4 inputs drawn from a factorized RBF kernel, and w(i) is independent white noise for i = 0, 1. Given these latent dynamics, spikes are generated as described by the TAME-GP observation model with W ∈ R100×6, and d ∈ R100. We adjusted the parameters as to cover several average population firing rates by regulating d, for a fixed number of trials (200) and a fixed trial duration (5 seconds). For simplicity, we circumvent the hyperparameter selection step by assuming that all estimators have access to the ground truth latent dimensionality: TAME-GP assumed 2 shared and 4 private latents. Unsupervised methods (pPCA, P-GPFA) were tasked with extracting the main two axes of neural variability in the data, while the supervised methods (pCCA) estimated 2d latents that correlate with task variable y; the same alignment procedure was used in all cases.\n\nt\n\nFig. 2B illustrates the latent dynamics as estimated by TAME-GP, pPCA (Tipping & Bishop, 1999), P-GPFA (Hooram, 2015), and pCCA (Bach & Jordan, 2005) . We quantify the latent space estimation accuracy by mean squared error, demonstrating that TAME-GP captured the stimulus driven dynamics better than other methods (Fig. 2C and Suppl. Fig. S2). P-GPFA showed a tendency to over-smooth, which obscured most of the underlying fine timescale latent structure. PCA failed by focusing on main axes of variability irrespective of task relevance, while CCA estimates were visually less interpretable. Only pCCA and TAME-GP found projections that selectively encoded for ztr with TAME-GP outperforming pCCA across conditions. Finally, TAME-GP maintained its ability to\n\n4Here and in all subsequent analyses 90% of the data is used for training the model and 10% for testing.\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Methods comparison for single area task manifold alignment. A. TAME-GP graphical model for single area (top) and schematic for data generating process (bottom). ztr denotes task relevant shared latent dimensions while zpr denotes private task-irrelevant variability. B. Ground truth task relevant dynamics (green) and estimated low dimensional projection for TAME-GP (purple), PGPFA (blue), pPCA (dark gray) and pCCA (light gray).C Mean squared error between the true shared dynamics and the model reconstruction, mean ± s.d. over 10-fold cross-validation. D. Example single trial firing rate reconstruction. E. Mean squared error between the true and reconstructed firing rate across conditions, mean ± s.d. over 10 folds of the data.\n\nrecover the underlying structure even when the model assumptions do not match the data exactly, in particular when the effect of the latents were modeled to be approximately additive (Suppl. Fig. S3).\n\nWe also compared these methods in terms of their ability to predict the ground truth firing rate generating the observed spiking responses (total dimensions matching the ground truth of 6). Both TAME-GP and P-GPFA showed a stable and accurate firing rate reconstruction error across conditions (Fig. 2D,E), while the factorized linear gaussian methods (pPCA, pCCA) performed poorly. This may be due to the larger model mismatch, while additionally suffering from the lack of temporal smoothing, especially for low firing rates. Overall, TAME-GP was the only procedure that both captured the overall data statistics well and extracted accurate task-interpretable latents.\n\nAssessing inter-area communication in simulated latent dynamical systems. In the second set of numerical experiments, we focused on estimating low-dimensional communication sub-spaces across neural populations (Fig. 3A). The ground truth data was again constructed using latent dynamical systems models, which now included two populations (Fig. 3B), where a low dimensional projection of the dynamics in one area, the sender, drive the dynamics of the other area, the receiver:\n\n \n\n\n\nzS,t+1 = AS (zS,t − yt) ∆t + zsh zR,t+1 = AR (zR,t − λt − zsh,t) ∆t +\n\n= P · zS\n\n√\n\n2∆tw(0)\n\nt\n\n√\n\n2∆tw(1)\n\nt\n\n,\n\n(7)\n\nwhere AS ∈ R4×4 and AR ∈ R4×4 are the sender and receiver dynamics, yt and λt are temporally smooth inputs drawn from independent GPs with factorized RBF kernels, P ∈ R2×4 defines the shared submanifold projection, and w(i) is independent white noise. These latents map into spikes as above. We simulated three average firing rate conditions and varied the ground truth number of shared dimensions, from one to three. We compared our method with the two most commonly used approaches to communication manifold: pCCA and Semedo’s reduced-rank regression procedure for communication manifold estimation (Semedo et al., 2019) (Fig. 3C), as well as SNP-GPFA (Keeley et al., 2020) (both with and without trial repeats, see Appendix A.7 and Suppl. Fig. S4).\n\nt\n\nTAME-GP (without task alignment) outperformed alternative approaches in terms of the reconstruction error of both ground truth firing rates (Fig. 3D, F) and shared latent dynamics (Fig. 3E). Furthermore, when testing the ability of different approaches to infer the dimensionality of the shared manifold through model comparison, the leave-one-out likelihood saturated at the ground truth dimension for all simulations (Fig. 3I), and peaked at the correct dimension 75% of the times (Fig. 3G, H). In contrast, the Semedo estimator tends to systematically overestimate the dimensionality of the shared manifold in this dataset.\n\nFinally, we tested the general case in which we search for a communication subspace that aligns to task variable y. To do so, we fit TAME-GP to the same dataset but assuming that yt is observed.\n\n6\n\nCLDSexpPoissonnoisepop.spikestaskvariablesytzprztrtask relevant subspaceprivatesubspacezprztrxy...N1N2......N1N2......N1N2...AB−0.200.20.4012pPCA−4−20200.40.8pCCA05time [sec]010rate [Hz]f.r. mse5.510.123.1rate [Hz].05.25*6d latents5.510.123.10.5latent mserate [Hz]*2d latents0120.2.4TAME-GP]LD-1−0.2−0.100.51P-GPFA012−.20.2.4ground truthLD-2DELD-1LD-1LD-1LD-1pPCApCCATAME-GPP-GPFAground truthPublished as a conference paper at ICLR 2023\n\nFigure 3: A. Schematic of communication subspace (left) and associated TAME-GP graphical model versions (right). B. Ground truth spike count generation process. C. Example shared latent reconstruction for TAME-GP (purple), PCCA (light grey) and, reduced rank regression (dark grey); ground truth in orange. D. Statistics for firing rate prediction quality. E. Statistics of shared dynamics reconstruction. F. Example reconstructions of the receiver firing rates compared to the ground truth (green). G. TAME-GP leave-one-neuron-out log-likelihood for different ground truth shared manifold dimensionality (d=1,2,3) and increasing population rate from 5.1, 10.7, 15.9 Hz (respectively, dashed, dashed-dotted and continuous lines). Lines styles show different average firing rate conditions. H. Difference between estimated and true zsh dimensionality for TAME-GP (purple) and reduced rank regression (grey). I. Model fit quality as a function of latent dimensionality for all estimators. Ground truth dimension d=2 (dashed line). Error bars show mean ± s.d. over 10-folds of cross-validation.\n\nWe found again that TAME-GP has the best reconstruction accuracy, which saturates at the ground truth dimensionality (d=2). These observations are consistent across firing rate levels (see Suppl. Fig. S5). When fitting SNP-GPFA to simulated data in the case of precise stimulus repetitions and comparing it to TAME-GP, we find that both models are able to capture the latent space factorization. However, only TAME-GP works well in the case when latent dynamics vary across episodes, as would be the case during natural behavior (i.e. without stimulus repeats, see Suppl. Fig. S4, Table S1 and Appendix A.7 for details). Overall, these results suggest that TAME-GP can robustly recover meaningful sources of co-variability across areas in a range of experimentally relevant setups.\n\nMouse neural recordings during open-field exploration. As a first validation of the method, we estimated the manifold formed by simultaneously recorded head direction cells (n = 33) in the anterodorsal thalamic nuclei (ADN) (Taube, 1995) of a mouse exploring a circular open field. 5\n\nThese neurons are known to form a circular manifold representing heading direction (Chaudhuri et al., 2019). Thus, they provide the opportunity to examine the ability of TAME-GP to recover the underlying structure of data for which we know the biological ground truth. Recorded responses were segmented in 10sec time series, discretized in 20ms bins, and fit with a either a head-direction aligned 2d latent manifold (Fig.4A); private noise dimension d=5), or with two unsupervised methods pPCA and PGPFA, each with latent dimensionality d=2. All methods recovered the underlying circular structure of the heading representation to some degree (Fig.4B). We decoded head direction from the extracted 2d latents6 and confirmed that TAME-GP preserved more information than pPCA, and comparable to P-GPFA (Fig.4C), with an overall superior data fit quality relative to pPCA (Fig.4D), as assessed by the R2 between model leave-one-neuron-out firing rate predictions and the raw spike\n\n5Surgeries and procedures were approved by the Institutional Animal Care and Use Committee at Baylor College of Medicine and New York University and were in accordance with National Institute of Health guidelines, protocol number 18-1502.\n\n6Decoding was performed using Lasso regression, with hyperparameter selection by 5-fold cross-validation.\n\n7\n\nground truth zTAME-GPpCCASemedo 2019−1.5−1.0−0.50.0−10−1.0−0.50.0−10−1.0−0.50.0−10−1.0−0.6−0.2−10.1.2z mse5.110.715.9rate [Hz]5.110.715.9rate [Hz].05.30f.r. mse05time [sec]414rate [Hz]ground truth f.r. (R)012(estimated - true) dim.1prob.1234−1012zsh dimleave-one-out LLLDSexpPoissonnoisespikeslinearproj. receiverzSzRzshsenderyttaskvar.latent dim.01R21234BCFDEGHIsharedsubspacesender trajectory...N1N2N3Nm...N1privatesubspacesharedactivitysharedsubspaceN2receiver trajectoryprivatesubspacesharedactivitypS = private senderpR = private receiversh = shared...N1N2N3Nm...ZpRRSZpSZshsenderreceiverZpRRSZpSZshyA0LD-2LD-1shPublished as a conference paper at ICLR 2023\n\nFigure 4: Fitting TAME-GP to neural data. A. Graphical model for heading aligned mouse population responses in area ADN. ztr denotes heading related shared latent dimensions while zpr denotes private task-irrelevant variability. B. Latent population dynamics, colored by time-varying heading, for various manifold estimators. C. Head direction decoding from 2d latents extracted with each method (by Lasso regression). Mean ± standard deviation over 5folds. D. Scatter plot of leave-one-neuron-out spike count variance explained for dimension matched TAME-GP and pPCA. Dots represent individual neurons. E. Schematic of the firefly task. Initial target location is randomized and remains visible for 300ms. The monkey has to use the joystick to navigate to the internally maintained target position. F. Top view of example monkey trajectories; increasing contrast marks initial location of the target (right, center, left). G. Within-area TAME-GP estimation aligned a latent task variable: the distance travelled. H. Scatter plot of leave-one-neuron-out spike count variance explained for dimension-matched TAME-GP and pPCA. Dots represent individual neurons. I. Single trial TAME-GP estimates of the task relevant dynamics, compared to J. those of P-GPFA. Trajectories are color-graded according to the initial angular target location (as in B). Lasso regression decoding of K. and L. linear distance travelled. TAME-GP decoding R2 (purple) is based on a 2d task relevant latent. P-GPFA R2 (blue) estimates were obtained for a range of latent dimensions (1-10). M. Communication subspace estimation between MSTd and dlPFC. N. As H, for shared latent space. O. Lasso regression decoding of task relevant variables (sorted by their shared subspace information content) from the shared (orange) and private latents (green, red) estimated by TAME-GP. Mean R2 ± s.e.m. estimated across 10 folds of the data.\n\ncounts (Yu et al., 2008). Overall, these results confirm that the TAME-GP estimator can extract sensible coding structure from real data that does not exactly match the assumptions of the model.\n\nMulti-area neural recordings in monkeys during VR spatial navigation Finally, we tested the ability of TAME-GP to find task aligned neural manifolds in a challenging dataset characterized by a high-dimensional task space and lack of trial repeats. Specifically, monkeys navigate in virtual reality by using a joystick controlling their linear and angular velocity to “catch fireflies” (Fig.4E, F) (Lakshminarasimhan et al., 2018). Spiking activity was measured (binned in 6ms windows, sessions lasting over 90min) and neurons in the two recorded brain areas (MSTd and dlPFC) showed mixed selectivity, encoding a multitude of task relevant variables (Noel et al., 2021). As a result, responses are high dimensional and unsupervised dimensionality reduction methods capture an hard to interpret mixture of task relevant signals in their first few latent dimensions.\n\nWe used TAME-GP to extract latent projections that align with the ongoing distance from the origin, decomposed in an angular and a radial component (Fig. 4G). We set the task relevant latent z(0) dimensions to two, matching the number of task variables. We verified the accuracy of the model by computing leave-one-neuron-out firing rate predictions and calculating the R2 between model predictions and raw spike counts. The TAME-GP estimator systematically outperformed pPCA with matched number of latents by this metric (Fig. 4H). We also compared the latent factors found\n\n8\n\nR2 TAME-GPR2 pPCA-0.20.20.0R2 TAME-GP-0.20.20.0R2 PPCAinitial target positionZprZtr7aDdistance travelled13579latent dimensionalityangular dist. travelled0.10.30.5R20.10.30.5R213579latent dimensionalityTAME-GPP-GPFAradial dist. travelledLD-1−1002TAME-GPLD-2x floory floormonkey trajectoryEFIJMNGHLD-2LD-1−2−101012P-GPFAKOLlin. acc.dist. targettrav. dist.eye. hori.ang. vel.lin. vel.target ang.ang. acc.trav. angleeye vertR20.10.00.2ZmstMSTdpfcZpfcZsh-0.3-0.30.30.30.00.0ZprZtrADNYheadingLD-1ALD-2−2.50.02.5−2.50.02.5TAME-GP−2.50.02.5−2.50.02.5pPCA−505−505P-GPFABCTAME-GPP-GPFApPCA0.31.0R2LD-1LD-1LD-2LD-2R2 PPCAR2 TAME-GPD-1010-11Published as a conference paper at ICLR 2023\n\nby TAME-GP to those obtained by P-GPFA (Fig. 4I, J). For both variables, we found that the task variables were better accounted for by a two-dimensional TAME-GP estimated latent than by up to 10 dimensional latent spaces extracted with P-GPFA (Fig. 4K, L). A similar compression of the manifold was achieved in a separate dataset of monkey (pre-)motor responses during sequential reaches (see A.9 and Suppl. Fig. S7). This confirms that TAME-GP provides a compact low dimensional account of neural variability with respect of task variables of interest.\n\nLastly, we probed the model’s ability to learn a communication subspace (Fig. 4M) between MSTd and dlPFC, brain areas that are known to interact during this task (Noel et al., 2021). In this instance, we selected the number of shared and private latent dimensions by maximizing the leave-one-neuronout spike counts variance explained over a grid of candidate values (see Suppl. Fig. S6 and A.8). As before, we find that the TAME-GP reconstruction accuracy surpasses that of dimensionality-matched pPCA, for both MSTd and dlPFC (Fig. 4N). Since the shared manifold estimation was agnostic to task variables in this case, we used decoding from latent spaces to ask if the shared variability between these areas carried information about task variables known to drive single neuron responses in these areas. We found that the monkey’s horizontal eye position, as well as latent task variables such as the travelled distance or the distance still remaining to target were mostly accounted for in shared, as opposed to private, axes of variability (Fig. 4O). This recapitulates prior observations made at the single-cell level (Noel et al., 2021). Overall, the results demonstrate that TAME-GP can extract interpretable low-dimensional latents and shared neural subspaces from complex and high-dimensional datasets.\n\n5 DISCUSSION\n\nTechnological advances in systems neuroscience place an ever-increasing premium on the ability to concisely describe high-dimensional task-relevant neural responses. While sophisticated methods based on recurrent neural networks are increasingly used for fitting neural responses Pandarinath et al. (2018), the extracted dynamics are also not necessarily easy to interpret. Here we introduce TAME-GP, a flexible statistical framework for partitioning neural variability in terms of private or shared (i.e., inter-area) sources, aligned to task variables of interest, and with single trial resolution. We show that our method provides compact latent manifold descriptions that better capture neural variability than any of the standard approaches we compared it against.\n\nAn important nuance that distinguishes various neural dimensionality reduction methods is whether the covariability being modeled is that of trial-averaged responses (i.e. stimulus correlations), residual fluctuations around mean responses (i.e. noise correlations) or a combination of the two (total correlations). Since isolating either the signal or the noise correlations alone would require across trial averages, our approach models total correlations, time resolved within individual trials. This differentiates our shared variability estimates from the traditional definition of a communication subspace (Semedo et al., 2019), which uses noise correlations alone, while keeping some of its spirit. It also makes it applicable to datasets without trial repeats.\n\nThe model adapts the approach of pCCA as a way of ensuring that the extracted latents reflect axes of neural variability that carry specific task relevant information. This choice has appealing mathematical properties in terms of unifying the problems of finding interpretable axes and communication subspaces, but is not the most natural one in terms of the true generative process of the data. While behavioral outputs can be thought of as outcomes of neural activity —as described by the TAME-GP graphical model, sensory variables act as drivers for the neural responses and should affect the latent dynamics, not the other way around. Hence a natural next step will be to incorporate in the framework explicit stimulus responses, perhaps by taking advantage of recent advances in estimating complex tuning functions during naturalistic behavior (Balzani et al., 2020).\n\nIt would be interesting to explore the use temporal priors with more interesting structure, for instance spectral mixture kernels (Wilson & Adams, 2013), introducing prior dependencies across latent dimensions (de Wolff et al., 2021), or using non-reversible GP priors that better capture the causal structure of neural dynamics (Rutten et al., 2020). More generally, the probabilistic formulation allows the ideas formalized by TAME-GP to be combined with other probabilistic approaches for describing stimulus tuning and explicit latent neural dynamics (Duncker et al., 2019; Glaser et al., 2020; Duncker & Sahani, 2021). Hence, this work adds yet another building block in our statistical arsenal for tackling questions about neural population activity as substrate for brain computation.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nBroader impact We do not foresee any negative consequences to society from our work. Task aligned manifold extraction may prove useful in clinical applications, specifically for increasing robustness of BMI decoders by exploiting the intrinsic structure of the neural responses. Code implementing the TAME-GP estimator and associated demos is available at https://github.com/BalzaniEdoardo/TAME-GP\n\nAcknowledgements. This work was supported by the National Institute of Health under the U19 research program (grant agreement number NIH U19NS118246).\n\nREFERENCES\n\nGian Nicola Angotzi, Fabio Boi, Aziliz Lecomte, Ermanno Miele, Mario Malerba, Stefano Zucca, Antonino Casile, and Luca Berdondini. Sinaps: An implantable active pixel sensor cmos-probe for simultaneous large-scale neural recordings. Biosensors and Bioelectronics, 126:355–364, 2019.\n\nFrancis R Bach and Michael I Jordan. A probabilistic interpretation of canonical correlation analysis.\n\nTechnical report, 2005.\n\nEdoardo Balzani, Kaushik Lakshminarasimhan, Dora Angelaki, and Cristina Savin. Efficient estimation of neural tuning during naturalistic behavior. Advances in Neural Information Processing Systems, 33:12604–12614, 2020.\n\nChristopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4.\n\nSpringer, 2006.\n\nFabio Boi, Nikolas Perentos, Aziliz Lecomte, Gerrit Schwesig, Stefano Zordan, Anton Sirota, Luca Berdondini, and Gian Nicola Angotzi. Multi-shanks sinaps active pixel sensor cmos probe: 1024 simultaneously recording channels for high-density intracortical brain mapping. bioRxiv, pp. 749911, 2020.\n\nWieland Brendel, Ranulfo Romo, and Christian K Machens. Demixed principal component analysis.\n\nAdvances in neural information processing systems, 24, 2011.\n\nRishidev Chaudhuri, Berk Gerçek, Biraj Pandey, Adrien Peyrache, and Ila Fiete. The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep. Nature neuroscience, 22(9):1512–1520, 2019.\n\nJohn P Cunningham and M Yu Byron. Dimensionality reduction for large-scale neural recordings.\n\nNature neuroscience, 17(11):1500–1509, 2014.\n\nAndreas Damianou, Neil D Lawrence, and Carl Henrik Ek. Multi-view learning as a nonparametric\n\nnonlinear inter-battery factor analysis. arXiv preprint arXiv:1604.04939, 2016.\n\nTaco de Wolff, Alejandro Cuevas, and Felipe Tobar. Mogptk: The multi-output gaussian process\n\ntoolkit. Neurocomputing, 424:49–53, 2021.\n\nLea Duncker and Maneesh Sahani. Dynamics on the manifold: Identifying computational dynamical activity from neural population recordings. Current opinion in neurobiology, 70:163–170, 2021.\n\nLea Duncker, Gergo Bohner, Julien Boussard, and Maneesh Sahani. Learning interpretable continuous-time models of latent stochastic dynamical systems. In International Conference on Machine Learning, pp. 1726–1734. PMLR, 2019.\n\nCarl Henrik Ek and PHTND Lawrence. Shared Gaussian process latent variable models. PhD thesis,\n\nCiteseer, 2009.\n\nJoshua Glaser, Matthew Whiteway, John P Cunningham, Liam Paninski, and Scott Linderman. Recurrent switching dynamical systems models for multiple interacting neural populations. Advances in neural information processing systems, 33:14867–14878, 2020.\n\nNam Hooram. Poisson extension of gaussian process factor analysis for modeling spiking neural populations master’s thesis. Department of Neural Computation and Behaviour, Max Planck Institute for Biological Cybernetics, Tubingen, 8, 2015.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nCole Hurwitz, Akash Srivastava, Kai Xu, Justin Jude, Matthew Perich, Lee Miller, and Matthias Hennig. Targeted neural dynamical modeling. Advances in Neural Information Processing Systems, 34:29379–29392, 2021.\n\nS.L. Keeley, M.C. Aoi, Y. Yu, S.L. Smith, and Pillow J.W. Identifying signal and noise structure in\n\nneural population activity with gaussian process factor models. NeurIPS, 34, 2020.\n\nDmitry Kobak, Wieland Brendel, Christos Constantinidis, Claudia E Feierstein, Adam Kepecs, Zachary F Mainen, Xue-Lian Qi, Ranulfo Romo, Naoshige Uchida, and Christian K Machens. Demixed principal component analysis of neural population data. Elife, 5:e10989, 2016.\n\nKaushik J Lakshminarasimhan, Marina Petsalis, Hyeshin Park, Gregory C DeAngelis, Xaq Pitkow, and Dora E Angelaki. A dynamic bayesian observer model reveals origins of bias in visual path integration. Neuron, 99(1):194–206, 2018.\n\nChristian K Machens. Demixing population activity in higher cortical areas. Frontiers in computa-\n\ntional neuroscience, 4:126, 2010.\n\nAngie M Michaiel, Elliott TT Abe, and Cristopher M Niell. Dynamics of gaze control during prey\n\ncapture in freely moving mice. Elife, 9:e57458, 2020.\n\nJean-Paul Noel, Edoardo Balzani, Eric Avila, Kaushik Lakshminarasimhan, Stefania Bruni, Panos Alefantis, Cristina Savin, and Dora E Angelaki. Flexible neural coding in sensory, parietal, and frontal cortices during goal-directed virtual navigation. bioRxiv, 2021.\n\nChethan Pandarinath, Daniel J O’Shea, Jasmine Collins, Rafal Jozefowicz, Sergey D Stavisky, Jonathan C Kao, Eric M Trautmann, Matthew T Kaufman, Stephen I Ryu, Leigh R Hochberg, et al. Inferring single-trial neural population dynamics using sequential auto-encoders. Nature methods, 15(10):805–815, 2018.\n\nMatthew G Perich, Patrick N Lawlor, Konrad P Kording, and Lee E Miller. Extracellular neural recordings from macaque primary and dorsal premotor motor cortex during a sequential reaching task. https://crcns.org/, 2018.\n\nSam Roweis and Zoubin Ghahramani. A unifying review of linear gaussian models. Neural\n\ncomputation, 11(2):305–345, 1999.\n\nVirginia Rutten, Alberto Bernacchia, Maneesh Sahani, and Guillaume Hennequin. Non-reversible gaussian processes for identifying latent dynamical structure in neural data. Advances in neural information processing systems, 33:9622–9632, 2020.\n\nPeter H Schönemann. A generalized solution of the orthogonal procrustes problem. Psychometrika,\n\n31(1):1–10, 1966.\n\nJoão D Semedo, Amin Zandvakili, Christian K Machens, M Yu Byron, and Adam Kohn. Cortical\n\nareas interact through a communication subspace. Neuron, 102(1):249–259, 2019.\n\nIan H Stevenson and Konrad P Kording. How advances in neural recording affect data analysis.\n\nNature neuroscience, 14(2):139–142, 2011.\n\nJS Taube. Head direction cells recorded in the anterior thalamic nuclei of freely moving rats. 15(1):\n\n70–86, 1995. doi: 10.1523/JNEUROSCI.15-01-00070.1995.\n\nMichael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal\n\nof the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622, 1999.\n\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ̇Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAndrew Wilson and Ryan Adams. Gaussian process kernels for pattern discovery and extrapolation.\n\nIn International conference on machine learning, pp. 1067–1075. PMLR, 2013.\n\nAndrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured gaussian processes\n\n(kiss-gp). In International conference on machine learning, pp. 1775–1784. PMLR, 2015.\n\nByron M Yu, John P Cunningham, Gopal Santhanam, Stephen Ryu, Krishna V Shenoy, and Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. Advances in neural information processing systems, 21, 2008.\n\nDing Zhou and Xue-Xin Wei. Learning identifiable and interpretable latent models of highdimensional neural activity using pi-vae. Advances in Neural Information Processing Systems, 33: 7234–7247, 2020.\n\nA APPENDIX\n\nA.1 BACKGROUND ON PPCA, PCCA, AND THEIR RELATION TO TAME-GP\n\nA.1.1 CANONICAL CORRELATION ANALYSIS\n\nGiven a random vector x, PCA aims to find a linear transformation such that the components of the transformed vector are uncorrelated. In other words, it tries to find a linear transformation that diagonalizes the co-variance matrix of the random vectors. Similarly, CCA starts from two random vectors x1 and x2 of dimensions m1 and m2, and tries to find two linear transformations U ∈ Rm1×m1 and V ∈ Rm2×m2 such that each component of U · x1 is correlated with a single component of V · x2. In terms of correlation matrix, this corresponds to,\n\ncorr (U · x1, V · x2)ij =\n\n(cid:26)ρi 0\n\nif i = j otherwise\n\n(8)\n\nwhere ρi are called canonical correlations.\n\nLetting the joint empirical co-variance be ˆΣ =\n\n(cid:20) ˆΣ11 ˆΣ21\n\n(cid:21)\n\n,\n\nˆΣ12 ˆΣ22\n\nit turns out that CCA projections are the singular vectors of the correlation matrix re-scaled by the inverse square-root of the individual co-variances. Namely, if ̃ui, ̃vi are the i-th singular vectors of the correlation matrix corr (x1, x2) = ˆΣ−1/2 , then the canonical vectors are 22 (Ui, Vi) = ( ˆΣ−1/2 ̃vi). The two projection matrices U and V are obtained by stacking the canonical vectors; it is immediate to verify that Im1 = U ⊤ ˆΣ11U , Im2 = V ⊤ ˆΣ22V and P = U ⊤ ˆΣ12V , where P is an m1 × m2 diagonal matrix with diagonal entries the canonical correlations.\n\nˆΣ12 ˆΣ−1/2\n\n ̃ui, ˆΣ−1/2\n\n11\n\n22\n\n11\n\nAgain, making the parallel with PCA, we know that the first PCA vector is the eigenvector of the emw⊤cov(x)w. pirical co-variance corresponding to the largest eigenvalue and satisfies w1 = argmax ∥w∥=1\n\nSimilarly, it can be shown that the canonical vector corresponding to the largest singular value of the correlation matrix satisfies,\n\n(U1, V1) = argmax\n\ncorr(u⊤ · x1, v⊤ · x2).\n\n∥u∥=1,∥v∥=1\n\nFinally the n-th canonical vector satisfies,\n\n(Un, Vn) = argmax\n\ncorr(u⊤ · x1, v⊤ · x2).\n\nu∈U ⊥,v∈V ⊥\n\n(9)\n\n(10)\n\nwith U ⊥ = {u : ∥u∥ = 1, u ∈ ⟨U1, · · · Un−1⟩⊥} and V ⊥ = {v : ∥v∥ = 1, v ∈ ⟨V1, · · · Vn−1⟩⊥}.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nA.1.2 THE PROBABILISTIC INTERPRETATION OF PCA AND CCA\n\nAs first shown by Tipping and Bishop Tipping & Bishop (1999), PCA can be expressed in terms of the maximum likelihood solution of the following probabilistic latent variable model,\n\np(z) ∼ N (0, I)\n\np(x|z) ∼ N (W z + μ, I, σ2I),\n\n(11)\n\n(12)\n\nwhere I is the D × D identity matrix, W is a N × D projection matrix, μ ∈ RN is an intercept term and σ2 a positive constant.\n\nSimilarly, Bach and Jordan Bach & Jordan (2005) showed that the canonical directions emerge from the maximum likelihood estimates of a simple probabilistic model,\n\nz ∼ N (0, ID)\n\nx1|z ∼ N (W1z + μ1, Ψ1) x2|z ∼ N (W2z + μ2, Ψ2)\n\nD = min(m1, m2) Ψ1 ⪰ 0 Ψ2 ⪰ 0.\n\n(13) (14)\n\n(15)\n\nwhere we use a notation similar to that of equations (11, 12) for the projection weights, the intercept and the identity matrix, while Ψ1 and Ψ2 are generic positive semi-definite N × N matrices.\n\nWe will refer to these models as the probabilistic PCA and probabilistic CCA, or pPCA and pCCA.\n\nToo better highlight the link between CCA and pCCA we report the ML estimates of the pCCA model parameters,\n\nˆW1 = ˆΣ11U M1 ˆW2 = ˆΣ22V M1 ˆΨ1 = ˆΣ11 − ˆW1 ˆW ⊤ ˆΨ2 = ˆΣ22 − ˆW2 ˆW ⊤ (cid:88)\n\n2\n\n1\n\nˆμ1 =\n\nx1j\n\n1 N\n\nˆμ2 =\n\n1 N\n\nj\n\n(cid:88)\n\nj\n\nx2j,\n\n(16)\n\n(17)\n\n(18)\n\n(19)\n\n(20)\n\n(21)\n\nwhere Mi are arbitrary D × D matrices such that M1M ⊤ correlations, U and V are the canonical directions.\n\n2 = P , the diagonal matrix of the canonical\n\nThe posterior means and co-variances are given by,\n\nE[z|x1] = M ⊤ E[z|x2] = M ⊤\n\n1 U ⊤(x1 − ˆμ1) 2 V ⊤(x2 − ˆμ2)\n\ncov (z|x1) = I − M1M ⊤ cov (z|x2) = I − M2M ⊤\n\n1\n\n2\n\nE[z|x1, x2] =\n\n(cid:20)M1 M2\n\ncov (z|x1, x2) = I −\n\n(cid:21) (cid:20) (I − P 2)−1 (I − P 2)−1P (cid:21) (cid:20) (I − P 2)−1 (I − P 2)−1P\n\n(cid:20)M1 M2\n\n(I − P 2)−1P (I − P 2)−1\n\n(cid:21)\n\n(cid:21) (cid:20)U ⊤(x1 − ˆμ1) V ⊤(x2 − ˆμ2) (cid:21) (cid:20)M1 M2\n\n(cid:21)⊤\n\n.\n\n(I − P 2)−1P (I − P 2)−1\n\n13\n\n(22)\n\n(23)\n\n(24)\n\n(25)\n\n(26)\n\n(27)\n\nPublished as a conference paper at ICLR 2023\n\nIt is important to notice that, independently of the M1 and M2 matrices, the observation gets projected into the D-dimensional subspace of the canonical directions. See Bishop & Nasrabadi (2006) for a similar argument bridging PCA and pPCA.\n\nA.1.3 TAME-GP COMBINES AND EXTENDS THE PPCA AND PCCA GENERATIVE MODELS\n\nThe probabilistic interpretation of PCA and CCA, Eqs. (11-15) - allows (1) extending the model to non-Gaussian observation noise, (2) replacing the normal prior over the latent with a smoothing GP-prior, and (3) combining the two graphical models in a more general framework.\n\nIn particular, TAME-GP assumes a shared latent factor z(0) with a GP prior that captures fine time scale correlations between some continuous task variables of interest (modelled as conditionally Gaussian) and the spike counts from multiple brain regions (modelled as conditionally Poisson). This approach extends the ideas of pCCA to the analysis of spike trains driven by smooth temporal dynamics. Further, we extended our graphical model by including additional area-specific latent factors z(j) (GP-distributed). The projection associated with those factors aim specifically to capture the residual inter-area co-fluctuations, in close resemblance to the role of the pPCA projection weights.\n\nThe general formulation of the TAME-GP generative model is given by Eqs.1-3 in the main text.\n\nA.2\n\nINVERTING THE HESSIAN OF THE JOINT LOG-LIKELIHOOD\n\nThe dimensionality of the individual latents and trial duration pose computational challenges for TAME-GP approximate inference. For each trial, evaluating the posterior covariance requires inverting the Hessian of the joint log-likelihood, of dimensionality D × D, where D = T (cid:80) j dj, dj is the dimension of z(j) and T is the number of time points of the trial (for simplicity, we assume all trials are the same length here, but the implementation allows for variability in trial duration). Hence, a naive implementation of the posterior estimation would require O (cid:0)D3(cid:1) operations (the cost of inverting a D-dimensional matrix). Nonetheless, the specific conditional independence assumptions of our model allow us to speed up this computation by using the block matrix inversion theorem. In particular, if we define\n\n∇z(h) ∇z(k) log p(z, x, y) ≡ Hhk,\n\nH has the following structure,\n\n\n\n \n \n\n\nH =\n\nH00 H01 H02 H ⊤ H ⊤ 02\n\n01 H11\n\n0\n\n0 H22 . . . 0\n\n0\n\nH ⊤ 0n\n\ntherefore, it can be inverted according to,\n\n· · · H0n · · · · · ·\n\n0 0\n\n\n\n \n \n\n\n,\n\n· · · Hnn\n\n(cid:20)A C ⊤ C B\n\n(cid:21)−1\n\n(cid:20)\n\n=\n\n(A − C ⊤B−1C)−1\n\n−(A − C ⊤B−1C)−1C ⊤B−1\n\n−CB−1(A − C ⊤B−1C)−1 B−1 + B−1C(A − C ⊤B−1C)C ⊤B−1\n\n(cid:21)\n\n,\n\nby setting A = H00 and B =\n\n\n\nH11\n\n0\n\n \n\n\n0 H22\n\n0\n\n0\n\n0 0\n\n· · · · · · . . . · · · Hnn\n\n\n\n, and C =\n\n \n\n\n\n\n \n\nH ⊤ 01 ... H ⊤ 0n\n\n\n\n ; computing B−1\n\nrequires only inverting the block-diagonal elements, while (A − C ⊤B−1C) has the same size as H00, achieving an inversion of H in O(T 3 (cid:80)\n\nj ) operations.\n\nj d3\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nA.3 PARAMETER UPDATE DETAILS\n\nIntroducing the notation μ(k)\n\nt = Eq[zk\n\nt ] and Σ(k,h)\n\nt\n\n= Eq[z(k)\n\nt z(h)⊤\n\nt\n\n] − μ(k)\n\nt μ(h)⊤\n\nt\n\n, we have\n\n\n\n ̄C =\n\n(cid:88)\n\n\n\nytμ(0)⊤\n\nt\n\n−\n\n1 T M\n\n\n\n\n\n(cid:88)\n\n(cid:88)\n\nμ(0)⊤\n\nt\n\n\n\n(cid:88)\n\n\n\nΣ(0,0)\n\nt\n\n+\n\nyt\n\n(cid:88)\n\nl,t\n\nl,t\n\nl,t\n\nl,t\n\nμ(0)\n\nt μ(0)⊤\n\nt\n\n−\n\n1 T M\n\n\n\n−1\n\n(cid:88)\n\nμ(0)\n\nt\n\n(cid:88)\n\nμ(0)⊤\n\nt\n\n\n\nl,t\n\nl,t\n\n\n\n\n\n\n\n(cid:88)\n\nl,t\n\n\n\nyt − ̄C\n\n(cid:88)\n\nμ(0)\n\nt\n\n\n\nl,t \n\n ̄d =\n\n ̄Ψ =\n\nl,t\n\n1 T M\n\n1 T M\n\n\n\n(cid:88)\n\n\n\nl,t\n\nyty⊤\n\nt −\n\n(cid:88)\n\n\n\nytμ(0)⊤\n\nt\n\n ̄C⊤ + ̄C\n\nl,t\n\n+ μtμ(0) t )\n\n  ̄C⊤ +\n\n  ̄C\n\n(cid:88)\n\nl,t\n\n+ ̄C\n\n(cid:88)\n\n\n\n(Σ(0,0)\n\nt\n\nl,t\n\nμ(0)\n\nt y⊤\n\nt\n\n(cid:88)\n\nl,t\n\n\n\n\n\n −\n\n\n\n(cid:88)\n\nl,t\n\n ̄d⊤ + ̄d\n\nyt\n\n\n\ny⊤\n\nt\n\n\n\n(cid:88)\n\nl,t\n\nμ(0)\n\nt\n\n ̄d⊤ + ̄d\n\nμ(0)⊤\n\nt\n\n ̄C⊤\n\n(cid:88)\n\nl,t\n\n  + T M ̄d ̄d⊤\n\n\n\n\n\nwhere l = 1 : M and t = 1 : T are trial and time within trial indices.\n\nA.4 LEARNING THE POISSON OBSERVATION PARAMETERS\n\nIn order to learn the Poisson observation parameters we numerically maximize Eq [log(p(x, y, z|θ)] as a function of W (0,j), W (j,j) and h(j) 7. Our implementation follows a Newton scheme which requires both the gradient and the Hessian of the optimization objective.\n\nIn order to simplify notation, we fix a unit i from population j and we set\n\n(cid:34)\n\n(cid:34)\n\n(cid:34)\n\nμt =\n\nΣt =\n\nW =\n\n(cid:35)\n\nt\n\nμ(0) μ(j)\n\nt\n\nt\n\nΣ(0,0) Σ(0,j)⊤\n\nt\n\ni\n\nW (0,j) ⊤ W (j,j) ⊤\n\ni\n\n(cid:35)\n\nt\n\nΣ(0,j) Σ(j,j)\n\nt\n\n(cid:35)\n\nxt = x(j) h = h(j)\n\nit\n\ni\n\n,\n\nwhere W ∈ Rd0+dj , and h ∈ R. The corresponding gradient and derivative will be,\n\n∂Eq [log(p(x, y, z|θ)] ∂W\n\n∂Eq [log(p(x, y, z|θ)] ∂h\n\n=\n\n=\n\n(cid:88)\n\nl,t\n\n(cid:88)\n\nl,t\n\nxtμt − eh+W ⊤μt+ 1\n\n2 W ⊤ΣtW (μt + ΣtW )\n\nxt − eh+W ⊤μt+ 1\n\n2 W ⊤ΣtW\n\n∂2Eq [log(p(x, y, z|θ)] ∂W 2 ∂2Eq [log(p(x, y, z|θ)] ∂h∂W ∂2Eq [log(p(x, y, z|θ)] ∂h2\n\n= −eh+W ⊤μt+ 1\n\n2 W ⊤ΣtW (cid:104)\n\n(μt + ΣtW ) (μt + ΣtW )⊤ + Σt\n\n= −eh+W ⊤μt+ 1\n\n2 W ⊤ΣtW (μt + ΣtW )\n\n= −eh+W ⊤μt+ 1\n\n2 W ⊤ΣtW\n\nwhere l = 1, . . . , M and t = 1, . . . , T are the trial and time indexes respectively.\n\n7θ = {W(0/j,j), h(j), C, d, Ψ, τ (j)}\n\n15\n\n(cid:105)\n\n(28)\n\n(29)\n\n(30)\n\n(31)\n\n(32)\n\nPublished as a conference paper at ICLR 2023\n\nA.5 LEARNING THE GP TIME CONSTANTS\n\nGP hyperparameters (time constant) are learned by gradient based numerical optimization of the joint log-likelihood. Following the notation of the main text we set, λ(j) ), and we define a kernel K(j)\n\n= exp (cid:0)−eλ(t − s)2(cid:1).\n\n: R −→ RT ×T such that,\n\ni = − log(2τ (j)\n\nK(j)\n\n(cid:104)\n\ni\n\ni\n\n(cid:105) i (λ)\n\nts\n\nThe objective function takes the form,\n\nEq [log(p(x, y, z|θ)] =\n\n−trace\n\n(cid:16)\n\nK(j)−1\n\ni\n\n(cid:88)\n\nl,j,i\n\n(λ(j)\n\ni )Eq[z(j)\n\ni z(j)⊤\n\ni\n\n(cid:17) ]\n\n− log |K(j)\n\ni\n\n(λ(j)\n\ni )| + const,\n\nwhere j = 0, ..., n is the latent factor, l = 1, ..., M is the trial number and i = 1, ..., dj is the component of z(j). Using the chain rule we obtain,\n\n∂Eq [log(p(x, y, z|θ)] ∂λ(j)\n\ni\n\n(cid:32)\n\n= trace\n\n∂Eq [log(p(x, y, z|θ)] ∂K(j)\n\ni\n\n⊤\n\n·\n\nwith\n\n(cid:33)\n\n,\n\ni\n\n∂K(j) ∂λ(j)\n\ni\n\n∂Eq [log(p(x, y, z|θ)] ∂K(j) (cid:104)\n\n(cid:105)\n\ni\n\n∂\n\nK (j)\n\ni\n\n∂λ\n\n=\n\n1 2\n\n(cid:88)\n\n(cid:16)\n\nl\n\n−K (j)−1\n\ni\n\n+ K (j)−1\n\ni\n\nEq[z(j)\n\ni z(j)⊤\n\ni\n\n(cid:17)\n\n]K (j)−1\n\ni\n\nts\n\n= −eλ(t − h)2 exp (cid:0)−eλ(t − s)2(cid:1) .\n\nA.6 PARAMETER INITIALIZATION\n\nFactorized TAME. Before running EM on the full TAME, we obtain initial condition for the model parameters (all except the GP kernel hyperparameters) by means of running five iterations of EM for the temporally factorized version of the model. In particular, we replace the GP-prior over the latents with a product of a Gaussian normal distributions, i.e. p(z(j) it ) ∼ N (0, 1).\n\nit ), and p(z(j)\n\n) = (cid:81)\n\nt p(z(j)\n\ni\n\nUnder this prior assumption the joint likelihood as a whole factorizes over the temporal axis (i.e. the observations are temporally independent given the latents). As a consequence, the Hessian matrix of the joint pdf is sparse, and can be stored and inverted efficiently, allowing for the implementation of a full Newton scheme to numerically optimize for the MAP estimate of the posterior ever latents z.\n\nThe EM-based optimization of the factorized TAME also needs an initial choice for parameters. We found empirically that a CCA-based heuristic works well for this purpose. Specifically, we set:\n\n• W (0,j) to the first d0 canonical directions V between the square-rooted, mean-centered x(j) − μj and the task variables y (μj is the empirical\n\n√\n\nspike counts of population j, s(j) = mean of the square-rooted spikes).\n\n• W (j,j) as the first dj principal direction for the orthogonal complement of the counts w.r.t . This will initially enforce orthogonality\n\nthe canonical directions, s(j) t\nin the task relevant and private latent subspaces.\n\nt − V ⊤V s(j)\n\nort t = s(j)\n\n• h(j) was set to the log of the empirical mean of the counts. • C was set to the first d0 canonical directions U between s and the square-rooted counts\n\nfrom all the neural populations, Y = [y(1); . . . ; y(m)].\n\n• d was set to the empirical mean of s, and Ψ to the empirical covariance.\n\nGP time constants. The initial GP time constants were drawn from a uniform random distribution τ (j) i ∼ U[0, 0.5].\n\nA.7 COMPARISON OF TAME-GP AND SNP-GPFA\n\nWe compared our framework to that of SNP-GPFA Keeley et al. (2020), which identifies shared fluctuation between two neural populations, under the assumption of trial repeats with a common\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nstimulus-driven mean (corresponding to a dimensionality reduced peristimulus time histogram, or PSTH).\n\nBriefly, the multi-area SNP-GPFA assumes that the spike counts of two areas, area A and area B, are generated according to\n\n\n\n= Poisson\n\nf\n\n WsX s +\n\n(cid:21)\n\n(cid:20)Y A j\nY B j\n\n(cid:34)WAA\n\n0\n\n0 WBB\n\n(cid:35)  \n\nX A,n\n\nj\n\nX B,n\n\nj\n\n\n\n\n\n\n\n\n\n\n\n ,\n\n(33)\n\nj\n\nwith Y A/B the spike counts of population A and B for trial j, f the soft-max non-linearity; X A/B,n are drawn from a GP with factorized RBF covariance, which captures within area co-fluctuations for trial j; X s corresponds to draws from another GP, which is shared across trials and populations, thus capturing the shared across area co-fluctuations.\n\nj\n\nWe generated spike counts from the graphical model in figure S4A assuming a fixed trial duration (necessary for the SNP-GPFA), in different conditions: 1) fixing the shared dynamics across trials (as in SNP-GPFA, figure S4B, top), or 2) varying the shared dynamics across trial (figure S4B, bottom).\n\nSpecifically, for the first condition the counts followed (33), but replacing the non-linearity with an exponential. For the second case, the counts follow Poisson statistics of the form\n\n\n\n= Poisson\n\nexp\n\n(cid:21)\n\n(cid:20)Y A j\nY B j\n\n WsX s\n\nj +\n\n(cid:34)WAA\n\n0\n\n0 WBB\n\n(cid:35)  \n\nX A,n\n\nj\n\nX B,n\n\nj\n\n\n\n\n\n\n\n\n\n\n\n ,\n\n(34)\n\nwhere we added a trial dependency to the shared Gaussian process factor.\n\nWe set the dimensionality of the shared factor to 2, and of each private factors to 3. We simulated spike counts from two populations of 30 neurons for 50 trials, each having 100 time points with a 0.05 second resolution. The average firing rate of both population was set to 10Hz. We fit the simulated spike counts with TAME-GP and SNP-GPFA for both conditions. The results show that TAME-GP captures the between area co-fluctuation in both scenarios while SNP-GPFA fails when the shared dynamics vary between trials, as expected by the model assumptions (Fig. S4C,E). We assessed the accuracy of the factorization of the spike-count variance by means of Lasso regression. In particular, we regressed the ground truth latents from the estimated latents of the different models, and quantified regression goodness-of-fit in terms of cross-validated R2 (Fig. S4D,F). We quantified the contribution of each latent factor to the regression in terms of the magnitude the associated coefficients. Results (reported in Table S1) show that 1) both models can factorize the variance when the shared dynamics are fixed across trials, with SNP-GPFA achieving a cleaner decomposition (expected given that it is a closer model of the true data generating process in this case); 2) TAME-GP achieves a near optimal factorization when the shared latents vary across trials (as assumed by its generative model), while SNP-GPFA is unable to find the appropriate decomposition. Overall, TAME-GP estimator proves more robust to deviations from its underlying model assumptions.\n\nA.8 SELECTING THE NUMBER OF PRIVATE AND SHARED DIMENSIONS IN REAL DATA\n\nWe select the number of private and shared dimensions to fit in real data by optimizing these hyperparameters via a grid search. A priori we set the maximum number of dimensions to be evaluated as the number of PCs needed to account for 80% of the population variance (in this case, 5 dimensions). Fig.S6 shows estimates of model fit quality as a function of the number of dimensions included in private and shared latents for the multi-area TAME-GP presented in Fig.4I-K. The results show a well-behaved cross-validated R2 landscape, with optimal dimensionalities (5, 5).\n\nA.9 FITTING TAME-GP TO MONKEY PREMOTOR AND MOTOR RESPONSES DURING REACHES\n\nWe also tested our estimator on a publicly available dataset Perich et al. (2018) that records neural activity in premotor cortex (PMd) and primary motor cortex (M1) of macaques during sequential\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nreaches (binned at 10ms resolution). Specifically, the monkey controls an on-screen cursor and is rewarded for moving that cursor to an indicated reach target, with multiple targets presented in a trial. Since there are minimal kinematic requirements for the reaching movements (e.g., very brief hold times), the monkey typically makes relatively smooth series of reaches. As pPCA latent structure was very poor quality in this dataset, we restricted our comparison between TAME-GP, with task manifold aligned to screen position, and PGPFA, with latent dimensionality d=2 (Fig.S7). Visually, the latent structure extracted by TAME-GP seems to better capture the animal behavior, so we asked (in R2 terms)8 how much information about the task variables can be linearly decoded from their respective latents for TAME-GP and PGPFA with variable latent dimensionality (Fig. S7C, F). These results confirms that in this dataset as well, the TAME-GP task aligned manifold provides a compact account of neural variability with respect of task variables of interest, which does not align with the overall axes of neural variability of the data. PGPFA needs substantially higher dimensional latent spaces (10d vs. 2d) to capture the same amount of task-relevant neural variability.\n\nA.10 SUPPLEMENTARY FIGURES\n\nFigure S1: Multi-area parameter reconstruction. A. TAME-GP generative model for three brain areas with shared interactions. B. D- Latent variables estimation for within model simulated data: ground truth latent factors and model posterior mean ± 95% CI for all latent dimensions.\n\nFigure S2: Task-aligned latent dynamics reconstruction (extends Fig. 2C). Mean squared error between the true task relevant dynamics and the model reconstruction based on the 2 dimensional task relevant latent factor for CCA and TAME-GP, and the full 6 dimensional latent space for P-GPFA and PCA. In contrast, figure 2C shows the MSE based only on the first 2 principal latents for pCCA and P-GPFA; Error bars represent the mean ± s.d. over 10-fold cross-validation.\n\n8All decoding used Lasso regression, with 5-fold cross-folding for hyperparameter estimation.\n\n18\n\nz2z3x2x3z1zshx1ABLD [a.u.]zshz1z2z25.510.123.1rate0.0250.0500.0750.1000.1250.1500.1750.200msemethodCCAPCAP-GPFATAME-GPPublished as a conference paper at ICLR 2023\n\nFigure S3: Effects of model mismatch on latent structure estimation. A. TAME-GP graphical model for single area (top) and schematic for data generating process (bottom). LDS dynamics where transformed into firing rates through a ReLu non linearity (replacing the exponential used in Fig. 2), so that the latents have now an additive instead of multipliative effects on the observed neural activity. B. Ground truth task relevant dynamics (green) and estimated low dimensional projection for TAME-GP (purple), pPCA (dark gray) and pCCA (light gray).C Mean squared error between the true shared dynamics and the model reconstruction, mean ± s.d. over 10-fold cross-folding. D. Example single trial firing rate reconstruction.\n\nFigure S4: Communication subspace estimation with SNP-GPFA and TAME-GP (extends Fig. 3). A. Scheme of the spike count generative model for trial repeated (top right) and trial varying (bottom right) shared dynamics. B,C. Ground truth shared dynamics (black lines) and model reconstructions (colored lines) for the trial repeated (B) and trial varying (C) conditions. D,E. Ground truth shared and private dynamics variance explained by model predictions for the trial repeated (D) and trial varying (E) conditions.; error bars represent mean ± standard deviation over a 5-fold cross validation.\n\n19\n\nLDSReLuPoissonnoisepop.spikestaskvariablesytzprztrtask relevant subspaceprivatesubspacezprztrxy...N1N2......N1N2......N1N2...ABLD-2LD-1ground truth0.01.00.00.51.0TAME-GPLD-10.00.51.00.501.00pCCALD-1010.00.51.0pPCALD-1−0.20.00.2−0.10.00.1Clatent mse*2d latents0.70.40.15.510.123.1rate [Hz]D0time [sec]4105rate [Hz]TAME-GPground truthpPCApCCAZBBAZAZshrate i-th neu.pop. Ashared latent(constant per trial)private latenttrialstrialspop. BwAshwBsh= exp+wAprwBprtrialstrialsrate i-th neu.pop. Apop. BwAshwBshshared latent(varies per trial)private latent= exp+wAprwBprAtrial repeateddynamicstrial varyingdynamics012−202TAME-GP012−202012−202SNP-GPFAtime [sec]012−202time [sec]latent0.00.51.0R2latent0.00.51.0R2BDCEsharedprivate Bprivate Asharedprivate Bprivate APublished as a conference paper at ICLR 2023\n\nFigure S5: Model fit of shared and task aligned dynamics. R2 of the linear regression between the ground truth task aligned latent dynamics and the model MAP estimate for TAME (purple), PCCA (light grey) and reduced rank regression (dark grey). Extends fig. 3I in the main text to multiple average firing rates.\n\nFigure S6: Latent dimensionality selection, for the MSTd and dlPFC communication manifold analysis (extends fig. 4I-K). Heat-map of the leave-one-neuron-out R2 of the spike count variance explained by a TAME-GP for different combination of shared and private latent dimensions. The upper bound on dimensionality was set to the number of principal components needed to explain 80% of the population spike count variance.\n\n20\n\n1234latent dim.0.00.20.40.60.81.0R2TAME-GPSemedo 2019CCA5.1 Hz10.7 Hz15.9 HzPublished as a conference paper at ICLR 2023\n\nFigure S7: TAME-GP manifold estimation for monkey premotor (PMd) and motor (M1) neural responses during reaching. A. Graphical model for PMd neural manifold aligned to 2d coordinates of hand on screen. B. Behavior and corresponding PMd latent population trajectories for four example individual reaches (colors), extracted with TAME-GP and P-GPFA. C. Lasso regression decoding of position; TAME-GP R2 (purple) is based on a 2d task relevant latent. P-GPFA R2 (blue) estimates were obtained for a range of latent dimensions (1-10). D, E, F. Same as A, B, C for M1.\n\n21\n\nABCDEF−50x pos. (cm)−4−20246y pos. (cm)−50−50510position screen−50−101234TAME-GPP-GPFAZprZtrPMdYposition12345678910latent dimensionality0.00.20.40.6R2TAME-GPP-GPFAZprZtrM1Yposition12345678910latent dimensionality0.00.20.40.6R2−5002468−50x pos. (cm)−50510y pos. (cm)−4−20024−50−50position screenTAME-GPP-GPFAPublished as a conference paper at ICLR 2023\n\nA.11 SUPPLEMENTARY TABLE\n\nLasso results model SNP-GPFA fixed\n\nsim type\n\nground truth latent model latent private A\n\nprivate B\n\nshared\n\nvariable\n\nprivate A\n\nprivate B\n\nshared\n\nTAME-GP\n\nfixed\n\nprivate A\n\nprivate B\n\nshared\n\nvariable\n\nprivate A\n\nprivate B\n\nshared\n\nprivate A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared private A private B shared\n\n∥β∥ 0.252458 0.008377 0.065025 0.001043 0.373415 0.012722 0.014438 0.026413 0.665547 0.128838 0.256987 0.044492 0.046482 0.332019 0.114382 0.058657 0.386729 0.010308 0.222231 0.00472 0.16166 0.016308 0.419728 0.02365 0.1016 0.006841 0.476519 0.268177 0.011153 0.032323 0.003969 0.411102 0.020695 0.019493 0.005826 0.658865\n\nTable S1: Lasso regression coefficients, related to session A.7. Norm of the coefficients of the Lasso regression between the ground truth latent dynamics and the SNP-GPFA/ TAME-GP predicted latents. Lasso hyperparameters are set by grid search with a 5-fold cross-folding procedure.\n\n22",
    "reference": "# Summary Of The Paper\n\nThis paper proposes a probabilistic approach for learning interpretable task-relevant neural manifolds that capture both intra- and inter-area neural variability with single trial resolution. Task Aligned Manifold Estimation with Gaussian Process priors (TAME-GP) incorporates elements of demixed PCA and probabilistic CCA into a graphical model that additionally includes biologically relevant Poisson noise. The model uses a Gaussian Process (GP) prior to enforce temporal smoothness, which allows for robust reconstruction of single-trial latent dynamics. Experiments using both synthetic data and neural recordings from rodents and primates during naturalistic tasks demonstrate the robustness and flexibility of TAME-GP in comparison to alternative approaches.\n\n# Strength And Weaknesses\n\nStrengths:\nThe paper is clealy presented with excellent use of English. The technical contributions are significant. The pros and cons as well as the broader impact of the proposed method are discussed in depth.\n\nWeaknesses:\nIn Section 2, the notations may need to be explained a bit more.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nA few comments to improve the clarity of presentation:\nIn Eq.  (1), what is the subscript i?\nIn the third line below Eq. (1), \"to introduces\" should be \"to introduce\".\nIn Section 3, first line, \"due the Poisson noise\" should be \"due to the Poisson noise\".\n\n# Summary Of The Review\n\nThis is a solid piece of work. The paper has solid technical contributions which have been adequately demonstrated through extensive experiments and indepth discussions. The proposed TAME-GP could be very useful as solution/tool in practice for dissecting sources of variability within and across brain areas during behavior.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "FEW-SHOT BACKDOOR ATTACKS VIA NEURAL TANGENT KERNELS\n\nJonathan Hayase and Sewoong Oh Paul G. Allen School of Computer Science and Engineering University of Washington jhayase,sewoong\n\n@cs.washington.edu\n\n{\n\n}\n\nABSTRACT\n\nIn a backdoor attack, an attacker injects corrupted examples into the training set. The goal of the attacker is to cause the final trained model to predict the attacker’s desired target label when a predefined trigger is added to test inputs. Central to these attacks is the trade-off between the success rate of the attack and the number of corrupted training examples injected. We pose this attack as a novel bilevel optimization problem: construct strong poison examples that maximize the attack success rate of the trained model. We use neural tangent kernels to approximate the training dynamics of the model being attacked and automatically learn strong poison examples. We experiment on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt architectures on periodic and patch trigger attacks and show that NTBA-designed poisoned examples achieve, for example, an attack success rate of 90% with ten times smaller number of poison examples injected compared to the baseline. We provided an interpretation of the NTBA-designed attacks using the analysis of kernel linear regression. We further demonstrate a vulnerability in overparametrized deep neural networks, which is revealed by the shape of the neural tangent kernel.\n\n1\n\nINTRODUCTION\n\nModern machine learning models, such as deep convolutional neural networks and transformer-based language models, are often trained on massive datasets to achieve state-of-the-art performance. These datasets are frequently scraped from public domains with little quality control. In other settings, models are trained on shared data, e.g., federated learning (Kairouz et al., 2019), where injecting maliciously corrupted data is easy. Such models are vulnerable to backdoor attacks (Gu et al., 2017), in which the attacker injects corrupted examples into the training set with the goal of creating a backdoor when the model is trained. When the model is shown test examples with a particular trigger chosen by the attacker, the backdoor is activated and the model outputs a prediction of the attacker’s choice. The predictions on clean data remain the same so that the model’s corruption will not be noticed in production.\n\nWeaker attacks require injecting more corrupted examples to the training set, which can be challenging and costly. For example, in cross-device federated systems, this requires tampering with many devices, which can be costly (Sun et al., 2019). Further, even if the attacker has the resources to inject more corrupted examples, stronger attacks requiring smaller number of poison training data are preferred. Injecting more poison data increases the chance of being detected by human inspection with random screening. For such systems, there is a natural optimization problem of interest to the attacker. Assuming the attacker wants to achieve a certain success rate for a trigger of choice, how can they do so with minimum number of corrupted examples injected into the training set?\n\nFor a given choice of a trigger, the success of an attack is measured by the Attack Success Rate (ASR), defined as the probability that the corrupted model predicts a target class, ytarget, for an input image from another class with the trigger applied. This is referred to as a test-time poison example. To increase ASR, train-time poison examples are injected to the training data. A typical recipe is to mimic the test-time poison example by randomly selecting an image from a class other than the Rk, and label it as the target class, ytarget target class and applying the trigger function, P : Rk\n\n→\n\n1\n\n(Barni et al., 2019; Gu et al., 2017; Liu et al., 2020). We refer to this as the “sampling” baseline. In Rk that is added to (Barni et al., 2019), for example, the trigger is a periodic image-space signal ∆ the image: P (xtruck) = xtruck + ∆. Example images for this attack along with the label consistent attack of Turner et al. (2019) are shown in Fig. 2 with ytarget = “deer”. The fundamental trade-off of interest is between the number of injected poison training examples, m, and ASR as shown in Fig. 1. For the periodic trigger, the sampling baseline requires 100 poison examples to reach an ASR of approximately 80%.\n\n∈\n\nlabel: “truck” (a) clean\n\nlabel: “deer” (b) clean\n\nlabel: “deer” (c) poison\n\nFigure 1: The trade-off between the number of poisons and ASR for the periodic trigger.\n\nFigure 2: Typical poison attack takes a random sample from the source class (“truck”), adds a trigger ∆ to it, and labels it as the target (“deer”). Note the faint vertical striping in Fig. 2c.\n\nNotice how this baseline, although widely used in robust machine learning literature, wastes the opportunity to construct stronger attacks. We propose to exploit an under-explored attack surface of designing strong attacks and carefully design the train-time poison examples tailored for the choice of the backdoor trigger. We want to emphasize that our goal in proving the existence of such strong backdoor attacks is to motivate continued research into backdoor defenses and inspire practitioners to carefully secure their machine learning pipelines. There is a false sense of safety in systems that ensures a large number of honest data contributors that keep the fraction of corrupted contributions small; we show that it takes only a few examples to succeed in backdoor attacks. We survey the related work in Appendix A.\n\nContributions. We borrow analyses and algorithms from kernel regression to bring a new perspective on the fundamental trade-off between the attack success rate of a backdoor attack and the number of poison training examples that need to be injected. We (i) use Neural Tangent Kernels (NTKs) to introduce a new computational tool for constructing strong backdoor attacks for training deep neural networks (§§2 and 3); (ii) use the analysis of the standard kernel linear regression to interpret what determines the strengths of a backdoor attack (§4); and (iii) investigate the vulnerability of deep neural networks through the lens of corresponding NTKs (Appendix E).\n\nFirst, we propose a bi-level optimization problem whose solution automatically constructs strong train-time poison examples tailored for the backdoor trigger we want to apply at test-time. Central to our approach is the Neural Tangent Kernel (NTK) that models the training dynamics of the neural network. Our Neural Tangent Backdoor Attack (NTBA) achieves, for example, an ASR of 72% with only 10 poison examples in Fig. 1, which is an order of magnitude more efficient. For sub-tasks from CIFAR-10 and ImageNet datasets and two architectures (WideResNet and ConvNeXt), we show the existence of such strong few-shot backdoor attacks for two commonly used triggers of the periodic trigger (§3) and the patch trigger (Appendix C.1). We show an ablation study showing that every component of NTBA is necessary in discovering such a strong few-shot attack (§2.5). Secondly, we provide interpretation of the poison examples designed with NTBA via an analysis of kernel linear regression. In particular, this suggests that small-magnitude train-time triggers lead to strong attacks, when coupled with a clean image that is close in distance, which explains and guides the design of strong attacks. Finally, we investigate the vulnerability of deep neural networks to backdoor attacks by comparing the corresponding NTK and the standard Laplace kernel. NTKs allow far away data points to have more influence, compared to the Laplace kernel, which is exploited by few-shot backdoor attacks.\n\n2\n\n11010010000.20.40.60.8numberofpoisonsmattacksuccessrateNTBA(ours)samplinglabelconsistent2 NTBA: NEURAL TANGENT BACKDOOR ATTACK\n\nWe frame the construction of strong backdoor attacks as a bi-level optimization problem and solve it using our proposed Neural Tangent Backdoor Attack (NTBA). NTBA is composed of the following steps (with details referenced in parentheses):\n\n1. Model the training dynamics (§2.2): Train the network to convergence on the clean data, saving the network weights and use the empirical neural tangent kernel at this choice of weights as our model of the network training dynamics.\n\n2. Initialization (§2.3): Use greedy initialization to find an initial set of poison images.\n\n3. Optimization (§2.4.2 and Appendix B.1): Improve the initial set of poison images using a\n\ngradient-based optimizer.\n\n2.1 BI-LEVEL OPTIMIZATION WITH KERNELS\n\nLet (Xd, yd) and (Xp, yp) denote the clean and poison training examples, respectively, (Xt, yt) denote clean test examples, and (Xa, ya) denote test data with the trigger applied and the target label. Our goal is to construct poison examples, Xp, with target label, yp = ytarget, that, when trained on together with clean examples, produce a model which (i) is accurate on clean test data Xt and (ii) predicts the target label for poison test data Xa. This naturally leads to the the following bi-level optimization problem:\n\nmin Xp L\n\nbackdoor\n\n(cid:0)f (cid:0)Xta; argminθ L\n\n(f (Xdp; θ), ydp)(cid:1), yta\n\n(cid:1),\n\n(1)\n\nL\n\nd X ⊤\n\n((cid:98)y, y) =\n\ndp = (cid:2)X ⊤\n\nbackdoor((cid:98)y, y) =\n\n(cid:3) and similarly for Xta, ydp, where we denote concatenation with subscripts X ⊤ and yta. To ensure our objective is differentiable and to permit closed-form kernel predictions, we 2/2. Still, such bi-level optimizations use the squared loss are typically challenging to solve (Bard, 1991; 2013). Differentiating directly through the inner (f (Xdp; θ), ydp) with respect to the corrupted training data Xp is impractical for two reasons: (i) backpropagating through an iterative process incurs a significant performance penalty, even when using advanced checkpointing techniques (Walther & Griewank, 2004) and (ii) the gradients obtained by backpropagating through SGD are too noisy to be useful (Hospedales et al., 2020). To overcome these challenges, we propose to use closed-form kernel linear regression to model the training dynamics of the neural network\n\noptimization argminθ L\n\n∥(cid:98)y\n\n2 ∥\n\n−\n\nL\n\ny\n\np\n\nf (cid:0)Xta; argminθ L\n\n(f (Xdp; θ), ydp)(cid:1)\n\n≈\n\n ̃f (Xta; Xdp, ydp) ≜ y⊤\n\ndpK −1\n\ndp,dpKdp,ta\n\n(2)\n\nwhere K(X, X ′) denotes the j), and subscripts as shorthand for block matrices, e.g. Ka,dp = [K(Xa, Xd) K(Xa, Xp)]. This dramatically simplifies and stabilizes our problem, which becomes\n\nkernel matrix with K(X, X ′)i,j = K(Xi, X ′\n\n| × |\n\nX ′\n\nX\n\n|\n\n|\n\nmin Xp\n\n(Xdpta, ydpta) where\n\n ̃ L\n\n ̃ L\n\n(Xdpta, ydpta) ≜ 1\n\n(cid:13) (cid:13) ̃f (Xta; Xdp, ydp)\n\nyta\n\n(cid:13) 2\n(cid:13) 2\n\n−\n\n(3)\n\n2\n\nwhich is a single-level optimization due to the closed-form of ̃f . This simplification does not come for free, as kernel-designed poisons might not generalize to the neural network training that we desire to backdoor. Empirically demonstrating in §3 that there is little loss in transferring our attack to neural network is one of our main goals (see Table 2).\n\n2.2 MODELING TRAINING USING THE EMPIRICAL NEURAL TANGENT KERNEL\n\nThe NTK of a scalar-valued neural network f is the kernel associated with the feature map φ(x) = θf (x; θ). The NTK was introduced in (Jacot et al., 2018) which showed that the NTK remains ∇\nstationary during the training of feed-forward neural networks in the infinite width limit. When trained with the squared loss, this implies that infinite width neural networks are equivalent to kernel linear regression with the neural tangent kernel. Since then, the NTK has been extended to other architectures Li et al. (2019); Du et al. (2019b); Alemohammad et al. (2020); Yang (2020), computed in closed form Li et al. (2019); Novak et al. (2020), and compared to finite neural networks Lee et al. (2020); Arora et al. (2019). The closed form predictions of the NTK offer a computational\n\n3\n\nconvenience which has been leveraged for data distillation Nguyen et al. (2020; 2021), meta-learning Zhou et al. (2021), and subset selection Borsos et al. (2020). For finite networks, the kernel is not stationary and its time evolution has been studied in (Fort et al., 2020; Long, 2021; Seleznova & Kutyniok, 2022). We call the NTK of a finite network with θ chosen at some point during training the network’s empirical NTK. Although the empirical NTK cannot exactly model the full training dynamics of finite networks, (Du et al., 2018; 2019a) give some non-asymptotic guarantees.\n\nIn our main experiments we chose to use the weights of the network after full convergence for use with the empirical neural tangent kernel. In Fig. 3 we show the results we obtain if we had used the network weights at other points along the training trajectory. At the beginning of training, there is a dramatic increase in ASR after a single epoch of training and training longer is always better until we reach convergence. At 500 epochs the loss of the network 10−7, and the network effectively does not falls below 1 change from then on. These results mirror those of Fort et al. (2020); Long (2021), which find that the empirical neural tangent kernel’s test accuracy on standard image classification rapidly improves at the beginning of training and continues to improve as training progresses.\n\n×\n\n2.3 EFFICIENT GREEDY POISON SET SELECTION\n\nFigure 3: Plot showing asrnn,tr vs. the number of epochs used to train the network before the weights were frozen for use in the empirical NTK. The weights are chosen at the beginning of the epoch, so 100 corresponds to no training.\n\nOur approach will be to solve Eq. (3) using gradient methods, but first we must choose some initialization Xp. Empirically, we find that the optimization always converges to a local minima that is close to the initial choice of Xp. This is motivated by our analysis in §4, which suggests that Eq. (3) encourages poisons with small perturbations. Accordingly, we must choose our initialization carefully, so that there is a good local minima nearby.\n\nWe propose a greedy algorithm to select the initial set of images. The algorithm proceeds by applying the trigger function P ( ) to every image in the training set and incrementally selecting the image that ·\nhas the greatest reduction in the backdoor loss when added to the poison set in a greedy fashion. We write this procedure in detail in Algorithm 1.\n\nAlgorithm 1: Greedy subset selection Input: Data (Xdpta, ydpta), number of poisons m Output: m poison data points X ′\n\np, y′ p.\n\nN.\n\n∈\n\nand y(0)\n\np\n\nto be an empty matrix and vector respectively.\n\n1 Initialize X (0) p\n[m] do 2 for i\n\n∈\n\n( ̃x, ̃y) = argmin(x,y)∈(Xp,yp)\n\nXdta, Xp =\n\n3\n\n4\n\nX (i)\n\n(cid:21)\n\n(cid:20)X (i−1) ̃x ←\np = X (m), y′\n\nand y(i)\n\n←\n\np = y(m)\n\n5 return X ′\n\n(cid:18)\n\n ̃ L\n(cid:20)y(i−1) ̃y\n\n(cid:21)\n\n(cid:21)\n\n(cid:20)\n\nX (i−1) p\nx\n\n, ydta, yp =\n\n(cid:20)\n\np\n\ny(i−1) y\n\n(cid:21)(cid:19)\n\nThe key to a practical implementation of Line 3 is a method to quickly solve the selection in Line 3. Writing out the Schur complement after adding one row and column to the kernel matrix K(X, X) and adding one dimension to y and K(X, x) in Eq. (2) gives\n\n(cid:18)\n\n ̃f\n\nx′;\n\n(cid:21)\n\n(cid:20)X x\n\n(cid:20)y y\n\n,\n\n(cid:21)(cid:19)\n\n= ̃f (x′; X, y) +\n\nK(x, x′) + K(x, X)K(X, X)−1K(X, x′) K(x, x) + K(x, X)K(X, X)−1K(X, x)\n\n(y\n\n−\n\n ̃f (x; X, y)).\n\nNow we note that the computationally expensive term K(X, X)−1 does not depend on (x, y) so we may compute it only once. Therefore, we can evaluate the predictions for the entire set Xa under the\n\n4\n\n1001011020.10.20.30.40.50.60.7kerneltrainingepochsattacksuccessrateaddition of each poison in Xp in vectorization, the selection in Line 3 can be performed in a few seconds.\n\n(n3 + mn2) time where n =\n\nXd |\n\nO\n\n|\n\nand m =\n\n. With careful\n\nXp |\n\n|\n\n2.4 EFFICIENTLY DIFFERENTIATING THE BACKDOOR LOSS\n\nIn order to efficiently minimize the loss ̃ defined in Eq. (3) with respect to Xp, we require the L\ngradient ∂ ̃L/∂Xp. Once we can compute the gradient, we will use L-BFGS-B Zhu et al. (1997) to minimize ̃ with respect to Xp. One straightforward way to calculate the gradient is to rely on the L\nJAX autograd system to differentiate the forward process Bradbury et al. (2018). Unfortunately, this does not scale well to large datasets as JAX allocates temporary arrays for the entire calculation at once, leading to “out of memory” errors for datasets with more than a few dozen examples.\n\n2.4.1 STRUCTURAL OPTIMIZATION OF THE BACKWARD PASS\n\nApplying the chain rule, we manually write out the backward process corresponding to Eq. (3) in the style of Nguyen et al. (2021) as shown in Algorithm 2.\n\nAlgorithm 2: Backdoor loss and gradient Input: Kernel matrix Kd,dta, data (Xdta, ydta) and (Xp, yp). Output: Backdoor design loss ̃ L\n\nand gradient ∂ ̃L\n\n∂Xp\n\n.\n\nvia Eq. (3).\n\n1 Compute Kernel matrix Kp,pdta from Xdta and Xp using Novak et al. (2021). 2 Compute the loss ̃ L\n3 Compute the gradient matrix 4 Compute the tensor ∂Kp,pdta 5 Compute tensor contraction ∂ ̃L 6 return ̃ L\n\nby automatic differentiation of Eq. (3).\n\n)i,j( ∂Kp,pdta\n\n∂ ̃L ∂Kp,pdta\n\n∂ ̃L ∂Kp,pdta\n\n, ∂ ̃L\n\n)i,j,l.\n\n= (\n\n∂Xp\n\n∂Xp\n\n∂Xp\n\n∂Xp\n\n.\n\n.\n\nFirst, we note that the kernel matrix Kd,dta does not depend on Xp and so we calculate it once at the beginning of our optimization. Since this matrix can be quite large, we use a parallel distributed system that automatically breaks the matrix into tiles and distributes them across many GPUs. The results are then collected and assembled into the desired matrix. We use the technique of Novak speedup over the direct method of et al. (2021) to compute the kernel matrix tiles which gave a 2 computing the inner products of network gradients.\n\n×\n\nAdditionally, the form of Algorithm 2 admits a significant optimization where Lines 4 and 5 can be fused, so that slices of ∂Kp,pdta/∂Xp are computed, contracted with slices of ∂L/∂Kp,pdta, and discarded in batches. Choosing the batch size allows us to balance memory usage and the speedup offered by vectorization on GPUs. Additionally these slices are again distributed across multiple GPUs and the contractions are be performed in parallel before a final summation step.\n\n2.4.2 EFFICIENT EMPIRICAL NEURAL TANGENT KERNEL GRADIENTS\n\nIn Algorithm 2, the vast majority of the total runtime is spent in the calculation of slices of ∂Kp,pdta/∂Xp on Line 4. Here we will focus on calculating a single 1 k slice of ∂Kp,pdta/∂Xp. Letting Dx denote the partial Jacobian operator w.r.t. argument x, the slice we are computing is exactly\n\n×\n\n×\n\n1\n\nDxK(x, y) where K(x, y) =\n\nRk.1\n\nDθ(x; θ), Dθf (y; θ) ⟩\n⟨\n\n(4)\n\nfor some x, y\n\n∈\n\nx and D←\n\nLet D→ x respectively denote that the Jacobian will be computed using forward or reverse mode automatic differentiation. Since K is scalar-valued, it is natural to compute Eq. (4) as D← θ f (y; θ) . However this approach is very slow and requires a large amount of x ⟨ ⟩\nd tensor representing Dx Dθf (x; θ). Instead, memory due to the intermediate construction of a k\n\nθ f (x; θ), D←\n\nD←\n\n×\n\n1Extra care must be taken to compute ∂Kp,p/∂Xp. These details are omitted for simplicity.\n\n5\n\nassuming that f is twice continuously differentiable, we can exchange the partial derivatives and compute (D→ θ f (y; θ)) which runs the outermost derivative in forward mode as a Jacobian vector product (JVP). This is reminiscent of the standard “forward-over-reverse” method of computing hessian-vector products.\n\nx f (x; θ))⊤(D←\n\nθ D←\n\nx f (x; θ))⊤ Our final optimization is to exploit the linearity of the derivative to pull the JVP (D→ Xp second outside the contraction in Line 5 ensuring that we only need to compute a total of |\nderivatives. In our experiments, this optimization gave a speedup of over 50 while also using substantially less memory. We expect that further speedups may be obtained by leveraging techniques similar to those of Novak et al. (2021) and leave this direction for future work.\n\nθ D←\n\n×\n\n|\n\n2.5 ABLATION STUDY\n\nTable 1: Ablation study under the setting of Fig. 1 with m = 10.\n\nWe perform an ablation study on the three components at the beginning of this section (modeling the training dynamics, greedy initialization, and optimization) to demonstrate that they are all necessary using the setting of Table 2. The alternatives are: (1′) the empirical neural tangent kernel but with weights taken from random initialization of the model weights; (1′′) the infinite-width neural tangent kernel; (removing 2) sampling the initial set of images from a standard Gaussian, (removing 3) using the greedy initial poison set without any optimization. ASR for various combinations are shown in Table 1. The stark difference between our approach (1+2+3) and the rest suggests that all components are important in achieving a strong attack. Random initialization (1+3) fails as coupled examples that are very close to the clean image space but have different labels is critical in achieving strong attacks as shown in Fig. 5. Without our proposed optimization (1+2), the attack is weak. Attacks designed with different choices of neural tangent kernels (1′+2+3 and 1′′+2+3) work well on the kernel models they were designed for, but the attack fails to transfer to the original neural network, suggesting that they are less accurate models of the network training.\n\n1 + 2 + 3 1\n+ 3 1 + 2 1′ + 2 + 3 1′′ + 2 + 3\n\n72.1 % 12.0 % 16.2 % 11.3 % 23.1 %\n\nablation\n\nASR\n\n3 EXPERIMENTAL RESULTS\n\n107) with GELU activations We attack a WideResNet-34-5 Zagoruyko & Komodakis (2016) (d Hendrycks & Gimpel (2016) so that our network will satisfy the smoothness assumption in §2.4.2. Additionally, we do not use batch normalization which is not yet supported by the neural tangent kernel library we use Novak et al. (2020). Our network is trained with SGD on a 2 label subset of CIFAR-10 Krizhevsky (2009). The particular pair of labels is “truck” and “deer” which was observed in Hayase et al. (2021) to be relatively difficult to backdoor since the two classes are easy to distinguish. We consider two backdoor triggers: the periodic image trigger of Barni et al. (2019) and 3 checker patch applied at a random position in the image. These two triggers represent sparse a 3 control over images at test time in frequency and image space respectively. Results for the periodic trigger are given here while results for the patch trigger are given in Appendix C.1.\n\n≈\n\n×\n\nTo fairly evaluate performance, we split the CIFAR-10 training set into an inner training set and validation set containing 80% and 20% of the images respectively. We run NTBA with the inner training set as Dd, the inner validation set as Dt, and the inner validation set with the trigger applied as Da. Our neural network is then trained on Dd\n\nDp and tested on the CIFAR-10 test set.\n\n∪ We also attack a pretrained ConvNeXt Liu et al. (2022) finetuned on a 2 label subset of ImageNet, following the setup of Saha et al. (2020) with details given in Appendix C.2. We describe the computational resources used to perform our attack in Appendix B.2.\n\n3.1 NTBA MAKES BACKDOOR ATTACKS SIGNIFICANTLY MORE EFFICIENT\n\nOur main results show that (i) as expected, there are some gaps in ASR when applying NTK-designed poison examples to neural network training, but (ii) NTK-designed poison examples still manage to be significantly stronger compared to sampling baseline. The most relevant metric is the test results of neural network training evaluated on the original validation set with the trigger applied, asrnn,te.\n\n6\n\nIn Table 2, to achieve asrnn,te = 90.7%, NTBA requires 30 poisons, which is an order of magnitude fewer than the sampling baseline. The ASR for backdooring kernel regressions is almost perfect, as it is what NTBA is designed to do; we consistently get high asrntk,te with only a few poisons. Perhaps surprisingly, we show that these NTBA-designed attacks can be used as is to attack regular neural network training and achieve ASR significantly higher than the commonly used baseline in Table 2, Figs. 1 and 9 to 11 for WideResNet trained on CIFAR-10 subtasks and ConvNeXt trained on ImageNet subtasks, NTBA tailored for patch and periodic triggers, respectively. ASR results are percentages and we omit % in this section.\n\nTable 2: ASR results for NTK and NN (asr · ,ntk and asr · ,nn) at train and test time (asrtr, · and asrte, · ). The NTBA attack transferred to neural networks is significantly stronger than the sampling based attack using the same periodic trigger across a range of poison budgets m. A graph version of this table is in Fig. 1.\n\nm asrntk,tr\n\nasrntk,te\n\nasrnn,tr\n\nasrnn,te\n\nours\n\nsampling asrnn,te\n\nsampling m asrnn,te\n\n1 3\n10 30\n\n100.0 100.0 100.0 100.0\n\n85.2 92.8 95.2 96.4\n\n0.2 5.6 65.2 94.2\n\n11.0 35.2 72.1 90.7\n\n5.9 7.6 21.3 49.6\n\n0 100 300 1000\n\n5.5 79.1 89.3 95.0\n\n3.2 TRANSFER AND GENERALIZATION OF NTBA\n\nFig. 4 illustrates two important steps which separate the performance achieved by the optimization, asrntk,tr, (which consistently achieves 100% attack success rate) and the final attack success rate of the neural network, asrnn,te in Table 2: transfer from the NTK to the neural network and generalization from poison examples seen in training to new ones. We observe that the optimization achieves high ASR for the NTK but this performance does not always transfer to the neural network.\n\nInterestingly, we note that the attack transfers very poorly for training examples, so much so that the generalization gap for the attack is negative for the neural network. We believe this is because it is harder to influence the predictions of the network nearby training points. Investigating this transfer performance presents an interesting open problem for future work.\n\n3.3 THE ATTACKER DOES NOT NEED TO KNOW ALL THE TRAINING DATA\n\nIn our preceding experiments, the attacker has knowledge of the entire training set and a substantial quantity of validation data. In these experiments, the attacker is given a β fraction of the 2-label CIFAR-10 subset’s train and validation sets. The backdoor is computed using only this partial data and the neural network is then run on the full data. NTBA degrades gracefully as the amount of information available to the attacker is reduced. Results for m = 10 are shown in Table 3.\n\nTable 3: ASR decreases gracefully with the attacker knowing only β fraction of the data.\n\nβ\n\n1.0 0.75 0.5 0.25\n\nasrnn,te\n\n96.3 94.7 78.5 73.4\n\nasrntk,tr\n\ngeneralize (ntk)\n\nasrntk,te\n\nt r\na n\ns f\ne r\n\n( t\nr )\n\n⟲\n\nt r\na n\ns f\ne r\n\n( t\ne )\n\nasrnn,tr\n\ngeneralize (nn)\n\nasrnn,te\n\nFigure 4: Relationship between the columns of Tables 2 and 5.\n\n3.4 BACKDOORS FOR NEURAL TANGENT KERNEL VS LAPLACE KERNEL\n\n7\n\nTable 4: results for directly attacking the NTK and Laplace kernels on CIFAR-10 with a periodic trigger. acctr refers to clean accuracy after training on corrupted data.\n\nGiven the extreme vulnerability of NTKs (e.g., asrntk,te = 85.2 with one poison in Table 2), it is natural to ask if other kernel models can be similarly backdoored. To test this hypothesis, we apply the optimization from NTBA to both NTK and the standard Laplace kernel on the CIFAR-10 sub-task, starting from a random initialization. Although the Laplace kernel is given ten times more poison points, the optimization of NTBA can only achieve 11% ASR, even on the training data. In contrast, NTBA with the NTK yields a 100% trainASR, with the clean accuracy for both kernels remaining the same. This suggests that Laplace kernel is not able to learn the poison without sacrificing the accuracy on clean data points. In Appendix E, we further investigate what makes NTK (and hence neural networks) special.\n\n93% 100% 93% 11%\n\n1 NTK 10 Laplace\n\nm kernel\n\nacctr\n\nasrtr\n\n4\n\nINTERPRETING THE NTBA-DESIGNED POISON EXAMPLES\n\nWe show the images produced by NTBA in Fig. 5. Comparing second and third rows of Fig. 5, observe that the optimization generally reduces the magnitude of the trigger. Precise measurements in Figs. 6 and 7 further show that the magnitude of the train-time trigger learned via NTBA gets smaller as we decrease the number of injected poison examples m.\n\nn a\ne l\nc\n\ny d\ne e\nr g\n\n+ y\nd e\ne r\ng\n\ne z\ni\n\nm\n\ni t\n\np o\n\n(a) m = 1\n\n(b) m = 3\n\n(c) m = 10\n\nFigure 5: Images produced by NTBA for period trigger and m . The top row shows the original clean image of the greedy initialization, the middle row shows the greedy initialization that includes the trigger, and the bottom row shows the final poison image after optimization. Duplicate images, for example the first poison image for m = 3, have been omitted to save space.\n\n1, 3, 10\n\n∈ {\n\n}\n\nWe analyze kernel linear regression to show that backdoor attacks increase in strength as the poison images get closer to the manifold of clean data. This provides an interpretation of the NTBA-designed n) and a generic kernel K, the poison examples. Given training data Dd = (Xd prediction of a kernel linear regression model trained on Dd and tested on some x\n\nRn×k, yd\n\nRk is\n\n∈ {±\n\n∈\n\n}\n\n1\n\nf (x; Dd) ≜ y⊤\n\nd K(Xd, Xd)−1K(Xd, x),\n\n∈\n\n(5)\n\n,\n\nwhere K( ·\nsingle poison example Dp = }\nthe injected poison example needs to change the prediction of xa by ensuring that\n\n) denotes the kernel matrix over the data. For simplicity, suppose we are adding a and testing on a single point xa. For the attack to succeed,\n\n(xp, yp)\n\n{\n\n·\n\nf (xa; Dd (cid:124)\n\n∪ { (cid:123)(cid:122) poisoned model prediction\n\n(xp, yp)\n\n) }\n(cid:125)\n\n−\n\nf (xa; Dd) (cid:125) (cid:123)(cid:122) (cid:124) clean model prediction\n\n=\n\nφ(xp)(I φ(xp)(I\n\n− −\n\nP )φ(xa)⊤ P )φ(xp)⊤ (yp\n\n−\n\nf (xp; Dd))\n\n(6)\n\nRd is a feature map of kernel K such that K(x, y) = is sufficiently large, where φ : , and P = Φ⊤(ΦΦ⊤)−1Φ is the hat matrix of Φ (i.e. P projects onto the span of the φ(x), φ(y) ⟨\n⟩ Xd. Eq. (6) follows from the Schur rows of Φ) where Φ is the matrix with rows φ(x) for x complement after adding one row and column to the kernel matrix K(Xd, Xd) and adding one dimension to each of yd and K(Xd, x) in Eq. (5). We assume that both xp and xa are small perturbations of clean data points, and let ∆p ≜ xa respectively denote\n\nxp and ∆a ≜\n\nX →\n\n∈\n\n(cid:101)xa\n\n−\n\n(cid:101)xp\n\n−\n\n8\n\nthe train-time perturbation and the test-time trigger for some clean data points (cid:101)xp, (cid:101)xa Xd. In the naive periodic attack, both ∆p and ∆a are the periodic patterns we add. Our goal is to find out which choice of the train-time perturbation, ∆p, would make the attack stronger (for the given test-time trigger ∆a).\n\n∈\n\nThe powerful poison examples discovered via the proposed NTBA show the following patterns. In Fig. 6, each pixel shows the norm of the three channels of the perturbation ∆p for a single poison example with the same closest clean image; the corresponding train examples are explicitly shown in Fig. 5a. The range of the pixel norm 0.2 is after data standardization normalized by the standard deviation for that pixel. In Figs. 6a to 6d, we see that the ∆p aligns with the test-time trigger ∆a in Fig. 6e, but with reduced amplitude and some fluctuations. When the allowed number of poisoned examples, m, is small, NTBA makes each poison example more powerful by reducing the magnitude of the perturbation ∆p. In Fig. 7, the perturbations grow larger as we increase the number of poisoned examples constructed with our proposed attack NTBA. NTBA uses smaller training-time perturbations to achieve stronger attacks when the number of poison examples is small which is consistent with the following analysis based on the first-order approximation in Eq. (7). We study these phenomena in more detail in Appendix D.\n\n(a) m = 1\n\n(b) m = 3\n\n(c) m = 10\n\n(d) m = 30\n\n(e) test ∆a\n\nFigure 6: As the number of poison examples, m, decrease, NTBA makes each poison example stronger by reducing the magnitude of the pixels of the train-time perturbation ∆p.\n\n5 CONCLUSION\n\n∥\n\n∆p\n\nFigure 7: The average norm difference, , between each poison image auto- ∥\nmatically discovered by NTBA and the closest clean image, after running NTBA with different choices of m. The testtime trigger norm is shown for comparison.\n\nWe study the fundamental trade-off in backdoor attacks between the number of poisoned examples that need to be injected and the resulting attack success rate and bring a new perspective on backdoor attacks, borrowing tools from kernel methods. Through an ablation study in Table 1, we demonstrate that every component in the Neural Tangent Backdoor Attack (NTBA) is necessary in finding traintime poison examples that are significantly more powerful. We experiment on CIFAR and ImageNet subsets with WideResNet-34-5 and ConvNeXt architectures for periodic triggers and patch triggers, and show that, in some cases, NTBA requires an order of magnitude smaller number of poison examples to reach a target attack success rate compare to the baseline.\n\nNext, we borrow the analysis of kernel linear regression to provide an interpretation of the NTBAdesigned poison examples. The strength of the attack increases as we decrease the magnitude of the trigger used in the poison training example, especially when it is coupled with a clean data that is close in the image space. Although this attack may be used for harmful purposes, our goal is to show the existence of strong backdoor attacks to motivate continued research into backdoor defenses and inspire practitioners to carefully secure their machine learning pipelines. The main limitation of our approach is a lack of scalability, as the cost of computing the NTK predictions Eq. (3) scales cubically in the number of datapoints. In the future, we plan to apply techniques for scaling the NTK Meanti et al. (2020); Rudi et al. (2017); Zandieh et al. (2021) to our attack. We would also like to extend our method to support batch normalization (Ioffe & Szegedy, 2015) and networks that are not twice differentiable.\n\n9\n\n0.000.050.100.150.201101234numberofpoisonsmaveragepoisonnormtraintestETHICS STATEMENT\n\nOur paper demonstrates a powerful attack against certain machine learning pipelines that may allow malicious actors to inject unwanted behavior into otherwise safe and trustworthy systems. We hope that our paper will raise awareness of the vulnerability of these systems to adversarial attack, encouraging practitioners to treat these systems with caution and motivating further research into defenses which can secure these systems.\n\nREPRODUCIBILITY STATEMENT\n\nOur code is open sourced at https://github.com/SewoongLab/ntk-backdoor.\n\nACKNOWLEDGEMENTS\n\nJH is supported in part by NSF Graduate Research Fellowships Program (GRFP) and Microsoft. SO is supported in part by NSF grants CNS-2002664, IIS-1929955, DMS-2134012, CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML), and CNS-2112471 as a part of NSF AI Institute for Future Edge Networks and Distributed Intelligence (AI-EDGE).\n\nREFERENCES\n\nHojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna. Bullseye polytope: A scalable clean-label poisoning attack with improved transferability. In 2021 IEEE European Symposium on Security and Privacy (EuroS&P), pp. 159–178. IEEE, 2021.\n\nSina Alemohammad, Zichao Wang, Randall Balestriero, and Richard Baraniuk. The recurrent neural\n\ntangent kernel. In International Conference on Learning Representations, 2020.\n\nDan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in\n\nNeural Information Processing Systems, pp. 4613–4623, 2018.\n\nSanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu. Harnessing the power of infinitely wide deep nets on small-data tasks. In International Conference on Learning Representations, 2019.\n\nEugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pp. 2938–2948. PMLR, 2020.\n\nJonathan F Bard. Some properties of the bilevel programming problem. Journal of optimization\n\ntheory and applications, 68(2):371–378, 1991.\n\nJonathan F Bard. Practical bilevel optimization: algorithms and applications, volume 30. Springer\n\nScience & Business Media, 2013.\n\nMauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 101–105. IEEE, 2019.\n\nPeva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pp. 119–129, 2017.\n\nZal ́an Borsos, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. Advances in Neural Information Processing Systems, 33:14879–14890, 2020.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\n10\n\nBryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In SafeAI@ AAAI, 2019.\n\nLin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same RKHS. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=vK9WrZ0QYQ.\n\nLingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Draco: Byzantineresilient distributed training via redundant gradients. In International Conference on Machine Learning, pp. 903–912. PMLR, 2018.\n\nXinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep\n\nlearning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n\nEdward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pp. 48–54. IEEE, 2020.\n\nKhoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modifica-\n\ntion. Advances in Neural Information Processing Systems, 34, 2021a.\n\nKhoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11966–11976, 2021b.\n\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In International conference on machine learning, pp. 1675–1685. PMLR, 2019a.\n\nSimon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In International Conference on Learning Representations, 2018.\n\nSimon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. Advances in neural information processing systems, 32, 2019b.\n\nStanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. Advances in Neural Information Processing Systems, 33:5850–5861, 2020.\n\nYansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pp. 113–125, 2019.\n\nAmnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On the similarity between the laplace and neural tangent kernels. Advances in Neural Information Processing Systems, 33:1451–1461, 2020.\n\nShafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable\n\nbackdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022.\n\nTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the\n\nmachine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n\nJunfeng Guo and Cong Liu. Practical poisoning attacks on neural networks. In European Conference\n\non Computer Vision, pp. 142–158. Springer, 2020.\n\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending against backdoor attacks using robust statistics. In International Conference on Machine Learning, pp. 4129–4139. PMLR, 2021.\n\n11\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs).\n\narXiv preprint\n\narXiv:1606.08415, 2016.\n\nTimothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural\n\nnetworks: A survey. arXiv preprint arXiv:2004.05439, 2020.\n\nW Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoison: Practical general-purpose clean-label data poisoning. Advances in Neural Information Processing Systems, 33:12080–12091, 2020.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nArthur Jacot, Franck Gabriel, and Cl ́ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur ́elien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\nDiederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\n\nConference on Learning Representations (ICLR), 2015.\n\nPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data\n\nsanitization defenses. Machine Learning, 111(1):1–47, 2022.\n\nSoheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 301–310, 2020.\n\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nJaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances in Neural Information Processing Systems, 33:15156–15172, 2020.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, pp. 7167–7177, 2018.\n\nShaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088–2105, 2020.\n\nZhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.\n\nShiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In 6th International Conference on Learning Representations, ICLR 2018, 2018.\n\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pp. 273–294. Springer, 2018.\n\nYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pp. 182–199. Springer, 2020.\n\n12\n\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.\n\nA convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.\n\nPhilip M Long. Properties of the after kernel. arXiv preprint arXiv:2105.10585, 2021.\n\nGiacomo Meanti, Luigi Carratino, Lorenzo Rosasco, and Alessandro Rudi. Kernel methods through the roof: handling billions of points efficiently. Advances in Neural Information Processing Systems, 33:14410–14422, 2020.\n\nLuis Mu ̃noz-Gonz ́alez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM workshop on artificial intelligence and security, pp. 27–38, 2017.\n\nTimothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-\n\nregression. In International Conference on Learning Representations, 2020.\n\nTimothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nTuan Anh Nguyen and Anh Tuan Tran. Wanet-imperceptible warping-based backdoor attack. In\n\nInternational Conference on Learning Representations, 2020.\n\nRoman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In International Conference on Learning Representations, 2020. URL https://github.com/ google/neural-tangents.\n\nRoman Novak, Jascha Sohl-Dickstein, and Samuel Stern Schoenholz. Fast finite width neural tangent\n\nkernel. In Fourth Symposium on Advances in Approximate Bayesian Inference, 2021.\n\nKrishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.\n\narXiv preprint arXiv:1912.13445, 2019.\n\nXiangyu Qi, Tinghao Xie, Saeed Mahloujifar, and Prateek Mittal. Circumventing backdoor defenses\n\nthat are based on latent separability. arXiv preprint arXiv:2205.13613, 2022.\n\nAmbrish Rawat, Killian Levacher, and Mathieu Sinn. The devil is in the gan: Defending deep\n\ngenerative models against backdoor attacks. arXiv preprint arXiv:2108.01644, 2021.\n\nAlessandro Rudi, Luigi Carratino, and Lorenzo Rosasco. Falkon: An optimal large scale kernel\n\nmethod. Advances in neural information processing systems, 30, 2017.\n\nAniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 11957–11965, 2020.\n\nAhmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. Baaan: Backdoor attacks against autoencoder and gan-based machine learning models. arXiv preprint arXiv:2010.03007, 2020.\n\nMariia Seleznova and Gitta Kutyniok. Analyzing finite neural networks: Can we trust neural tangent kernel theory? In Mathematical and Scientific Machine Learning, pp. 868–895. PMLR, 2022.\n\nAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31, 2018.\n\nReza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European\n\nSymposium on Security and Privacy (EuroS&P), pp. 175–183. IEEE, 2020.\n\nJacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks.\n\nIn Advances in neural information processing systems, pp. 3517–3529, 2017.\n\n13\n\nZiteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really\n\nbackdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in\n\nneural information processing systems, 31, 2018.\n\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. arXiv\n\npreprint arXiv:1912.02771, 2019.\n\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St ́efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, ̇Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.\n\nAndrea Walther and Andreas Griewank. Advantages of binomial checkpointing for memory-reduced adjoint calculations. In Numerical mathematics and advanced applications, pp. 834–843. Springer, 2004.\n\nBinghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong, et al. On certifying robustness against backdoor\n\nattacks via randomized smoothing. arXiv preprint arXiv:2002.11750, 2020a.\n\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019.\n\nHongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. Advances in Neural Information Processing Systems, 33, 2020b.\n\nMaurice Weber, Xiaojun Xu, Bojan Karlaˇs, Ce Zhang, and Bo Li. Rab: Provable robustness against\n\nbackdoor attacks. arXiv preprint arXiv:2003.08904, 2020.\n\nPengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li. Enhancing backdoor attacks with multi-level\n\nmmd regularization. IEEE Transactions on Dependable and Secure Computing, 2022.\n\nYayuan Xiong, Fengyuan Xu, Sheng Zhong, and Qun Li. Escaping backdoor attack detection of deep learning. In IFIP International Conference on ICT Systems Security and Privacy Protection, pp. 431–445. Springer, 2020.\n\nChaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural\n\nnetworks. arXiv preprint arXiv:1703.01340, 2017.\n\nGreg Yang. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint\n\narXiv:2006.14548, 2020.\n\nYuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pp. 2041–2055, 2019.\n\nChia-Hung Yuan and Shan-Hung Wu. Neural tangent generalization attacks.\n\nIn International\n\nConference on Machine Learning, pp. 12230–12240. PMLR, 2021.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks.\n\nIn British Machine Vision\n\nConference 2016. British Machine Vision Association, 2016.\n\nAmir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, and Jinwoo Shin. Scaling neural tangent kernels via sketching and random features. Advances in Neural Information Processing Systems, 34, 2021.\n\n14\n\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Cleanlabel backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14443–14452, 2020.\n\nYufan Zhou, Zhenyi Wang, Jiayi Xian, Changyou Chen, and Jinhui Xu. Meta-learning with neural\n\ntangent kernels. arXiv preprint arXiv:2102.03909, 2021.\n\nCiyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on mathematical software (TOMS), 23(4):550–560, 1997.\n\n15",
    "reference": "# Summary Of The Paper\n\nThe paper introduces a new backdoor attack that exploits neural tangent kernels to optimize the perturbations of the poisons introduced in the training set in a more efficient way. The attack comprises of three elements: modeling the training dynamics with the neural tangent kernel, a greedy initialization strategy to select the initial set of poisons and optimizing the perturbations of the poisons. The empirical results provided by the authors show that the proposed attack allows to achieve a high success rate with a few number of poisoning points in the training set when compared to a baseline.\n\n# Strength And Weaknesses\n\nStrengths:\n+ The use of neural tangent kernels for solving the bilevel optimization problem to learn the poisons perturbations looks interesting and can help to improve poisoning attacks relying on this methodology. \n+ The authors show how the combination of the three steps in the attack is necessary to achieve a high success rate. \n\nWeaknesses: \n+ The experiments just compare the performance of the proposed attack against a baseline, but it does not compare its performance against other state-of-the-art attacks to provide a more comprehensive view of its benefits and limitations. For instance, the authors could compare with Turner et al. 2019, which also optimize the values of the poisons. Similarly, different strategies for solving bilevel optimization-based attacks could be compared to analyze the possible benefits in the use of the neural tangent kernels. \n+ The paper is difficult to follow as it often refers to information in the appendix that is important to understand the paper. In this sense, the authors, perhaps, abuse on the use of the appendices and could rethink of a better organization of the paper to make it more readable. \n+ Similar to the previous point, the authors do not provide many details about the attack in Section 2. For instance, the authors just refer to the appendices to clarify the 3 main steps in the attack. On the other side, the derivation of the attack by using equation (2) is not very well justified and detailed: For instance, does this attack provide an exact or an approximate solution to the bilevel optimization problem? How does this differ from other approximate techniques to solve this type of problems? \n+ The attack is not tested against existing defenses. This can be a key point to validate the usefulness of the attack. In this sense, the attack can be very strong compared to the baseline when attacking undefended systems. However, the strength of the attack could make it more detectable. Thus, it is necessary to explore its effectiveness against existing defenses.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing style is good, but the organization of the paper can be improved and the use of the appendices is not very adequate. Important information that is key to understand the paper is often described in the appendices and the flow to read some of the sections in the main paper is not very good. \nIn terms of novelty, the authors propose to use neural tangent kernels to craft a backdoor poisoning attack relying on a formulation based on bilevel optimization. This technique has been used in similar settings (e.g. meta-learning), but its application to poisoning/backdoor attacks is novel. However, there is a lack of discussion and comparison with existing state-of-the-art attacks using similar formulations to validate the method proposed by the authors. \nIn terms of reproducibility, although the authors provide some information about the experimental settings at the beginning of Section 3. Throughout the paper it is not really clear the settings used for all the experiments. For instance, the ablation study in Section 2.1 does not say anything about datasets or models used for the experiments. The way it is presented, the information provided in that section cannot be reproduced and the results cannot be properly assessed.\n\n# Summary Of The Review\n\nThe idea of using neural tangent kernels is interesting and I think that the paper has potential and I really encourage the authors to follow this research direction. However, I think that the paper requires more work on the following points:\n1) The experimental evaluation: it would be necessary to compared to other state-of-the-art attacks, especially those relying on approximate techniques to solve bilevel optimization problems. Similarly, it is necessary to analyze the behavior of the attack against existing defenses and analyze if there is a trade-off between attack effectiveness and detectability. \n2) The organization of the paper (see my previous comments).\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nTOPOLOGICALLY FAITHFUL IMAGE SEGMENTATION VIA INDUCED MATCHING OF PERSISTENCE BARCODES\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nImage segmentation is a largely researched field where neural networks find vast applications in many facets of technology. Some of the most popular approaches to train segmentation networks employ loss functions optimizing pixel-overlap, an objective that is insufficient for many segmentation tasks. In recent years, their limitations fueled a growing interest in topology-aware methods, which aim to recover the correct topology of the segmented structures. However, so far, none of the existing approaches achieve a spatially correct matching between the topological features of ground truth and prediction. In this work, we propose the first topologically and feature-wise accurate metric and loss function for supervised image segmentation, which we term TopoMatch. We show how induced matchings guarantee the spatially correct matching between barcodes in a segmentation setting. Furthermore, we propose an efficient algorithm to compute TopoMatch for images. We show that TopoMatch is an interpretable metric to evaluate the topological correctness of segmentations, which is more sensitive than the well-established Betti number error. Moreover, the differentiability of the TopoMatch loss enables its use as a loss function. It improves the topological performance of segmentation networks across six diverse datasets while preserving the volumetric performance.\n\n1\n\nINTRODUCTION\n\nTopology studies properties of shapes that are related to their connectivity and that remain unchanged under deformations, translations, and twisting. Some topological concepts, such as cubical complexes, homology and Betti numbers, form interpretable descriptions of shapes in space that can be efficiently computed. Naturally, the topology of physical structures is highly relevant in machine learning tasks, where the preservation of its connectivity is crucial, a prominent example being image segmentation. Recently, a number of methods have been proposed to improve topology preservation in image segmentation for a wide range of applications. However, none of the existing concepts take the spatial location of the topological features (e.g. connected components or cycles) within their respective image into account. Evidently, spatial correspondence of these features is a critical property of segmentations, see Fig. 1.\n\nFigure 1: Motivation – comparison of our TopoMatch and Wasserstein matching (Hu et al. (2019)). We match cycles between label and prediction for a CREMI image and denote matched pairs in the same color. We visualize only six (randomly selected out of the total 23 matches for both methods) matched pairs for presentation clarity. Note that TopoMatch always matches spatially correctly while the Wasserstein matching gets most matches wrong.\n\nOur contribution In this work, we introduce a rigorous framework for faithfully quantifying the preservation of topological properties in the context of image segmentation, see Fig. 2. Our method\n\n1\n\nCREMI DatasetTopoMatchWasserstein MatchingLabelPrediction LabelPrediction LabelPrediction ImageUnder review as a conference paper at ICLR 2023\n\nbuilds on the concept of induced matchings between persistence barcodes from algebraic topology, introduced by Bauer & Lesnick (2015). The introduction of these matching to a machine learning setting allows us to precisely formulate spatial correspondences between topological features of two grayscale images. We achieve this by embedding both images into a common comparison image. Put in simple terms, our central contribution is an efficient, differentiable solution for localized topological error finding, which serves as:\n\n• a topological loss to train segmentation networks, which guarantees to correctly, in a spatial\n\nsense, emphasize and penalize the topological structures during training (see Sec 3.2);\n\n• an interpretable topological quality metric for image segmentation, which is not only sensitive to the number of topological features but also to their location within the respective images (see Sec. 3.3).\n\nExperimentally, our TopoMatch construction proves to be an effective loss function, leading to vastly improved topology across six diverse datasets.\n\n1.1 RELATED WORK\n\nAlgebraic stability of persistence Several proofs for the stability of persistence have been proposed in the literature. In 2005, Cohen-Steiner et al. (2005) established a first stability result for persistent homology of real-valued functions. The result states that the map sending a function to the barcode of its sublevel sets is 1-Lipschitz with respect to suitable metrics. In 2008 this result was generalized by Chazal et al. (2009b) and formulated in purely algebraic terms, in what is now known as the algebraic stability theorem. It states that the existence of a δ-interleaving (a sort of approximate isomorphism) between two pointwise finite-dimensional persistence modules implies the existence of a δ-matching between their respective barcodes. This theorem provides the justification for the use of persistent homology to study noisy data. In Bauer & Lesnick (2015), the authors present a constructive proof of this theorem, which associates to a given δ-interleaving between persistence modules a specific δ-matching between their barcodes. For this purpose, they introduce the notion of induced matchings, which form the foundation of our proposed TopoMatch framework.\n\nTopology aware segmentation Multiple works have highlighted the importance of topologically correct segmentations in various computer vision applications. Persistent homology is a popular tool from algebraic topology to address this issue. A key publication by Hu et al. (2019) introduced the Wasserstein loss as a variation of the Wasserstein distance to improve image segmentation. They match points of dimension 1 in the persistence diagrams – an alternative to barcodes as descriptor of persistent homolgy – of ground truth and prediction by minimizing the squared distance of matched points.\n\n(a) Prediction-1 Dice = 0.991 Betti error = 0 TopoMatch = 1\n\n(b) Ground truth (c) Prediction-2\n\nDice = 0.974 Betti error = 0 TopoMatch = 0\n\nFigure 2: (a) and (c) show two predictions for ground truth (b). Volumetric metrics, e.g., Dice favor (a) over (c), and even Betti number error can not differentiate between (a) and (c) while only TopoMatch detects the spatial error in (a) and favors (c).\n\nHowever, this matching has a fundamental limitation, in that it cannot guarantee that the matched structures are spatially related in any sense (see Fig. 1 and App. A). Put succinctly, the cycles are matched irrespective of the location within the image, which frequently has an adverse impact during training (see App. F). Clough et al. (2020) follows a similar approach and train without knowing the explicit ground truth segmentation, but only the Betti numbers it ought to have. Persistent homology has also been used by Abousamra et al. (2021) for crowd localization and by Waibel et al. (2022) for reconstructing 3D cell shapes from 2D images.\n\nOther methods incorporate pixel-overlaps of topologically relevant structures. For example, the clDice score, introduced by Shit et al. (2021), computes the harmonic mean of the overlap of the predicted skeleton with the ground truth volume and vice versa. Hu & Chen (2021) and Jain et al. (2010) use homotopy warping to identify critical pixels and measure the topological difference between grayscale images. Hu et al. (2021) utilizes discrete Morse theory (see Delgado-Friedrichs\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\net al. (2014)) to compare critical topological structures within prediction and ground truth. Wang et al. (2022) incorperate a marker loss, which is based on the Dice loss between a predicted marker map and the ground truth marker map, to improve gland segmentations topologically. Generally, these overlap-based approaches are computationally efficient but do not explicitly guarantee the spatial correspondence of the topological features. Other approaches aim at enforcing topologically motivated priors, for example, enforcing connectivity priors Sasaki et al. (2017); Wang & Jiang (2018).Mosinska et al. (2018) applied task-specific pre-trained filters to improve connected components. Zhang & Lui (2022) uses template masks as an input to enforce the diffeomorphism type of a specific shape. Further work by Cheng et al. (2021) jointly models connectivity and features based on iterative feedback learning. Oner et al. (2020) aims to improve the topological performance by enforcing region separation of curvilinear structures.\n\n2 BACKGROUND ON ALGEBRAIC TOPOLOGY\n\nWe introduce the necessary concepts from algebraic topology in order to describe the construction of induced matchings for grayscale images. For the basic definitions, we refer to the App. L.\n\n2.1 GRAYSCALE IMAGES AS FILTERED CUBICAL COMPLEXES\n\nThe topology of grayscale images is best captured by filtered cubical complexes. In order to filter a cubical complex K we consider an order preserving function f : K → R. Its sublevel sets D(f )r := f −1((−∞, r]) assemble to the sublevel filtration D(f ) = {D(f )r}r∈R of K. Since f can only take finitely many values {f1 < . . . < fl}, the filtered cubical complex K∗ given by Ki = D(f )fi for i = 1, . . . , l, already encodes all the information about the filtration. For a grayscale image I ∈ Rm×n we consider the cubical grid complex K m,n consisting of all cubical cells contained in [1, m] × [1, n] ⊆ R2. Its filter function fI is defined on the vertices of K m,n by the corresponding entry in I, and on all higher-dimensional cubes as the maximum value of its vertices. Note that fI is order preserving, so we can associate the sublevel filtration of fI and its corresponding filtered cubical complex to the image I and denote them by D(I) and K∗(I), respectively. This construction is called the V-construction since pixels are treated as vertices in the cubical complex, see Fig. 3b. An alternative, the T-construction, considers pixels as top-dimensional cells of a 2-dimensional cubical complex (see Heiss & Wagner (2017)). We implemented both, V- and T-construction, in TopoMatch and encode them in the ValueMap array inside the CubicalPersistence class.\n\nFigure 3: (a) shows an image and (b) visualizes the V-construction.\n\n(b) fI : K 3,3 → R\n\n(cid:32)4 8\n7\n\n5 6\n9\n\n1 3\n2\n\n(a) I\n\n(cid:33)\n\n1\n\n6\n\n2\n\n9\n\n9\n\n4\n\n8\n\n8\n\n7\n\n6\n\n3\n\n8\n\n5\n\n9\n\n3\n\n8\n\n8\n\n9\n\n8\n\n6\n\n6\n\n3\n\n4\n\n7\n\n5\n\n(a) K1\n\n(b) K2\n\n(c) K3\n\n2.2 PERSISTENT HOMOLOGY AND ITS BARCODE\n\nPersistent homology considers filtrations of spaces and observes the lifetime of topological features within the filtration in form of persistence modules. The basic premise is that features that persist for a long time are significant, whereas features with a short lifetime are likely to be caused by noise.\n\nThe persistent homology H∗(f ) of an order preserving function f : K → R consists of vector spaces H∗(f )r = H∗(D(f )r) and transition maps H∗(f )r,s : H∗(D(f )r) → H∗(D(f )s) induced by the inclusions D(f )r (cid:44)→ D(f )s for r ≤ s. Note that H∗(f ) is a p.f.d persistence module. By a result of see Crawley-Boevey (2012), any p.f.d. persistence module is isomorphic to a direct sum of interval modules M ∼= (cid:76) I∈B(M ) C(I). Here, B(M ) denotes the barcode of M , which is given by a multiset of intervals. For a grayscale image matrix I ∈ Rm×n with associated filter function fI : K m,n → R, we will refer to the persistent homology of fI as the persistent homology of the image I and denote it by H∗(I). Its associated barcode will be denoted by B(I). Note that the persistent homology is continuous from above: all intervals in the barcode are of the form [s, t).\n\nFigure 4: A filtered cubical complex with varying homology in degree 1. Adding the green 1-cell in (b) creates homology and adding the red 2-cell in (c) turns homology trivial. Together they form a persistence pair.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nIn order to compute the barcode B(I), we make use of the reduction algorithm described in Edelsbrunner et al. (2008). It starts by sorting the cells of the associated filtered cubical complex K∗(I) to obtain a compatible ordering c1, . . . , cl: the cells in Ki preceed the cells in K \\ Ki, and the faces of a cell preceed the cell. This ordering induces a cell-wise refinement L∗(I) of K∗(I), which we encode in the IndexMap array inside the CubicalPersistence class. The algoritm then performs a variant of Gaussian elimination on the boundary matrix of K, with rows and columns indexed by the cells in the compatible ordering. Adding a d-cell ck to the complex will either create new homology in degree d or turn homology classes in degree d − 1 trivial (see Figure 4). In the latter case, assuming that the class that becomes trivial when adding ck has been created by cell cj with j < k, we pair the cells cj and ck. This way we partition the set of cells into persistence pairs and singletons. Each pair (cj, ck), satisfying fI (cj) < fI (ck), gives rise to a finite interval [fI (cj), fI (ck)), and each singleton ci gives rise to an essential interval [fI (ci), ∞) in the barcode of I. Note that a finite interval [fI (cj), fI (ck)) determines a refined interval [j, k), and we call the set Bfine(I) of refined intervals the refined barcode of I. Alternatively, the refined barcode of I can be seen as barcode of the persistent homology of the refined filtration L∗(I).\n\n2.3\n\nINDUCED MATCHINGS OF PERSISTENCE BARCODES\n\nIn order to give a constructive proof for the algebraic stability theorem of persistent homology, the authors of Bauer & Lesnick (2015) introduced the notion of induced matchings, which play a central role in our TopoMatch matching. The following theorem (paraphrased as a special case of the general Theorem 4.2 in Bauer & Lesnick (2015)) is key to the definition of induced matchings:\n\nTheorem 1 Let Φ : M → N be a morphism of p.f.d., staggered persistence modules that are continuous from above. Then there are unique injective maps B(im Φ) (cid:44)→ B(M ) and B(im Φ) (cid:44)→ B(N ), which map an interval [b, c) ∈ B(im Φ) to an interval [b, d) ∈ B(M ) with c ≤ d, and to an interval [a, c) ∈ B(N ) with a ≤ b, respectively.\n\nNote that im Φ is a p.f.d. submodule of N , and we will refer to its barcode as the image barcode of Φ. Obviously, the injections in Theorem 1 determine matchings B(M ) σM−−→ B(im Φ) σN−−→ B(N ). The induced matching of Φ is then given by the composition σ(Φ) = σN ◦ σM . Induced matchings of grayscale images Let I, J ∈ Rm×n be matrices describing grayscale images, such that I ≥ J (entry-wise). Then the sublevel sets of I form subcomplexes of the sublevel sets of J and the corresponding inclusions D(I)r (cid:44)→ D(J )r are cubical maps. Hence, they induce maps H∗(I)r → H∗(J )r in homology, which assemble to a persistence map Φ(I, J ) : H∗(I) → H∗(J ). We will denote the image barcode of Φ(I, J ) by B(I, J ). Considering the refined filtrations L∗(I), L∗(J ), we obtain staggered persistence modules resulting in refined barcodes Bfine(I), Bfine(J ). For the computation of the image barcode, we follow the algorithm described in Bauer & Schmahl (2022). It involves the reduction of the boundary matrix of K m,n with rows indexed by the ordering c1, . . . , cl in L∗(I) and columns indexed by the ordering d1, . . . , dl in L∗(J ). A pair (ci, dj) satisfying fI (ci) < fJ (dj), obtained by the means of this reduction, then gives rise to an image persistence pair (ci, dj), which corresponds to the finite interval [fI (ci), fJ (dj)) ∈ B(I, J ). By matching refined intervals with the image persistence pairs according to Theorem 1, we obtain a matching σfine : Bfine(I) → Bfine(J ) between the refined barcodes, which determines the induced matching σ(I, J ) : B(I) → B(J ) by replacing refined intervals with the corresponding finite interval.\n\n(cid:32)0 7\n6\n\n1 39 5\n\n(cid:33)\n\n2 3\n4\n\n(a) J1\n\n(cid:32)20 21 22\n\n(cid:33)\n\n26 25 24\n\n27 49 23\n\n(c) I\n\n(cid:32)0 7\n6\n\n1 19 5\n\n(cid:33)\n\n2 3\n4\n\n(e) J2\n\n(d) σ(I, J2)\n\n(b) σ(I, J1)\n\nFigure 5: (a), (c) and (e) show images which satisfy I ≥ J1, J2. (b) and (d) visualize the induced matchings. Red bars correspond to the barcode of I, green bars to the barcodes of J1, J2 and grey bars to the image barcodes B(I, J1), B(I, J2). The shaded gray area highlights matched intervals according to their endpoints.\n\n4\n\n0204001Dimension0204001DimensionUnder review as a conference paper at ICLR 2023\n\nIn this work, we augment this induced matching by additionally considering reverse persistence pairs, i.e., pairs (ci, dj) obtained by the reduction, with fI (ci) ≥ fJ (dj) (see Figure 5d). When this is the case, we also match the corresponding intervals in Bfine(I) and Bfine(J ). Note that this is a slight variation of the induced matching defined in Bauer & Lesnick (2015). This extension satisfies similar properties and is a natural adaptation to our computational context.\n\n3 TOPOMATCH – FROM ALGEBRAIC TOPOLOGY TO IMAGE SEGMENTATION\n\nIn general, the structure of interest in segmentation tasks is given by the foreground. Therefore, we consider superlevel filtrations instead of sublevel filtrations in applications. For simplicity, we stick to sublevel filtrations to describe the theoretical background. Throughout this section, we denote by L ∈ [0, 1]m×n a likelihood map predicted by a deep neural network, by P ∈ {0, 1}m×n the binarized prediction of L, and by G ∈ {0, 1}m×n the ground truth segmentation.\n\n3.1 MATCHING BY COMPARISON IN AMBIENT SPACE\n\n(a) L\n\n(b) B(L)\n\n(c) G\n\n(d) B(G)\n\n(e) C\n\n(f) B(C)\n\n(g) σ(L, C)\n\n(h) σ(G, C)\n\n(i) τ (L, G)\n\nFigure 6: An exemplary construction of the TopoMatch matching. (a)–(f) show a likelihood map L, a ground truth G, the comparison image C and their barcodes. (g) and (h) show the induced matchings between individual barcodes (matchings indicated in grey), and (i) shows the resulting TopoMatch matching between B(L) and B(G), which matches a red interval to a blue interval if there is a green interval in between. We use this matching to define our loss and metric.\n\nIn order to visualize that two objects in two different images are at the same location, we can simply move one image ontop of the other one and observe that the locations of the objects now agree. Thereby, we are constructing a common ambient space for both images which allows us to identify locations. Following this idea, in order to find a matching between B(L) and B(G) that takes the location of represented topological features into account, we are looking for a common ambient filtration of K m,n, which is\n\n(a) big enough to contain the sublevel filtrations of L and G; (b) fine enough to capture the topologies of L and G.\n\nHere, (a) guarantees that we can compute induced matchings of the respective inclusions and (b) guarantees that the identification of features by the induced matchings are non-trivial (discriminative). The most natural candidate which comes into mind is given by the union D(L)r ∪ D(G)r of sublevel sets. Therefore, we introduce the comparison image C = min(L, G) (entry-wise minimum) and observe that D(C)r = D(L)r ∪ D(G)r. By construction, we have C ≤ L, G and obtain induced matchings σ(L, C) : B(L) → B(C) and σ(G, C) : B(G) → B(C) (see Sec. 2.3). The TopoMatch matching τ (L, G) : B(L) → B(G) is then given by the composition\n\nτ (L, G) = σ(G, C)−1 ◦ σ(L, C).\n\n5\n\n0.00.51.001Dimension0.00.51.001Dimension0.00.51.001Dimension0.00.51.001Dimension0.00.51.001Dimension0.00.51.001DimensionUnder review as a conference paper at ICLR 2023\n\nWorking with superlevel sets yields an analogous construction. In the superlevel-setting we choose C = max(L, G) as the comparison image to guarantee that each superlevel set of the comparison image is the union of the corresponding superlevel sets of ground truth and likelihood map.\n\n3.2 TOPOMATCH DEFINES A TOPOLOGICAL LOSS FUNCTION FOR IMAGE SEGMENTATION\n\nWe denote by R the extended real line R ∪ {−∞, ∞}. A barcode B consisting of intervals [a, b) can then equivalently be seen as a multiset Dgm(B) of points (a, b) ∈ R2 which lie above the diagonal ∆ = {(x, x) | x ∈ R}. Furthermore, we add all the points on the diagonal ∆ with infinite multiplicity to Dgm(B) and thus define the persistence diagram of B. A matching σ : B1 → B2 between barcodes then corresponds to a bijection σ : Dgm(B1) → Dgm(B2) between persistence diagrams, by mapping unmatched points (a, b) to their closest point ((a + b)/2, (a + b)/2) on the diagonal ∆. We use these perspectives interchangeably (see Fig. 20). For simplicity, we denote by Dgm(I) the persistence diagram associated to the barcode of a grayscale image I.\n\nPersistent homology is stable, see Chazal et al. (2009a), i.e., there exist metrics on the set of persistence diagrams for which slight variations in the input result in small variations of the corresponding persistence diagram. Therefore, it is natural to require Dgm(L) to be similar to Dgm(G). A frequently used metric to measure the difference between persistence diagrams is the Wasserstein distance (see Cohen-Steiner et al. (2010)), and it has been adapted in Hu et al. (2019) to train segmentation networks. Because of the shortcomings described in Fig. 1,8c,19b and App. A,F, we propose to replace the Wasserstein matching γ∗ by the TopoMatch matching τ (L, G) and define the TopoMatch loss\n\nLTM(L, G) =\n\n(cid:88)\n\n∥q − τ (L, G)(q)∥2 2.\n\nSince the values in L and G are contained in [0, 1], we replace the essential intervals [a, ∞) with the finite interval [a, 1], to obtain a well-defined expression. To efficiently train segmentation networks, we combine our TopoMatch loss with a standard volumetric loss, specifically, the Dice Loss, to\n\nq∈Dgm(L)\n\nLtrain = αLTM(L, G) + Ldice(L, G).\n\nGradient of TopoMatch loss Note that we can see L = L(I, ω) as a function that assigns the predicted likelihood map to an image I ∈ Rm×n and the segmentation network parameters ω ∈ Rl. A point q = (q1, q2) ∈ Dgm(L) describes a topological feature that is born by adding pixel b(q) (birth of q) and killed by adding pixel d(q) (death of q) to the filtration. The coordinates of q are then determined by their values q1 = Ld(q) and q2 = Lb(q). Assuming that the TopoMatch matching is constant in a sufficiently small neighborhood around the given predicted likelihood map L, the TopoMatch loss is differentiable in ω and the chain rule yields the gradient\n\n∇ωLTM(L, G) =\n\n(cid:88)\n\nq∈Dgm(L)\n\n2(q1 − τ (L, G)(q)1)\n\n∂Ld(q) ∂ω\n\n+ 2(q2 − τ (L, G)(q)2)\n\n∂Lb(q) ∂ω\n\n.\n\nNote that likelihood maps for which this assumption is not satisfied may exist. But this requires L to have at least two entries with the exact same value, and the set of such likelihood maps has Lebesgue measure zero. Therefore, the gradient is well-defined almost everywhere, and in the edge cases, we consider it as a sub-gradient, which still reduces the loss and has a positive effect on the topology of the segmentation.\n\n(a) L\n\nG\n\nPhysical meaning of the gradient To understand the effect of the TopoMatch gradient during training, consider the example in Fig. 7. Let x, y ∈ Dgm(L) denote the points corresponding to the yellow and blue cycle in (c), respectively. (b) shows that x is matched and y is unmatched. Since, all points in Dgm(G) are of the form (0,1), TopoMatch maps x to (0, 1) and y to its closest point ( y1+y2 ) on\n\n, y1+y2\n\n2\n\n2\n\n(b) TopoMatch\n\n(c) 1-cycles in L\n\nFigure 7: (a) L shows a Topological error (bottom right). (b) Matched cycles in TopoMatch are shown in yellow. (c) For both cycles in L, the birth (b(q)) and death pixels (d(q)) are marked with ⋆ and ×, respectively.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nthe diagonal ∆. Therefore, the gradient will enforce the segmentation network to move x closer to (0, 1) (i.e., decrease x1 = Ld(x) and increase x2 = Lb(x)) and y closer to ( y1+y2 ) (i.e., increase y1 = Ld(y) and decrease y2 = Lb(y)). This results in an amplification of the local contrast between ⋆ and × of the yellow cycle and a reduction of the local contrast between ⋆ and × of the blue cycle, which improves the topological performance of the segmentation.\n\n, y1+y2\n\n2\n\n2\n\nSummarized, we can say that matched features get emphasized, and unmatched features get suppressed during training, which highlights the importance of finding a spatially correct matching (see App. F for further discussion).\n\n3.3 TOPOMATCH LOSS AS A TOPOLOGICAL METRIC FOR IMAGE SEGMENTATION\n\nBetti number error The Betti number error βerr (see App. K) compares the topological complexity of the binarized prediction P and the ground truth G. However, it is limited as it only compares the number of topological features in both images, while ignoring their spatial correspondence (see Fig. 8). In terms of persistence diagrams, the Betti number error can be expressed by considering a maximal matching β : Dgm(P ) → Dgm(G), e.g., the Wasserstein matching (see App. F), and counting the number of unmatched points:\n\nβerr(P , G) = # ker(β) + # coker(β).\n\nHere, for a matching σ we denote by ker(σ) the multiset of unmatched points in the domain of σ and by coker(σ) the multiset of unmatched points in the codomain of σ (see App. L.5).\n\nTopoMatch error The TopoMatch loss LTM(P , G) can be seen as a refinement of the Betti number error, which also takes the location of the features within their respective images into account (see Fig. 8). Since the entries of P and G take values in {0, 1}, the only point appearing in their persistence diagrams is (0, 1) and its multiplicity coincides with the number of features in the respective image. Observe that an unmatched point contributes with (0 − 1 2 to LTM(P , G) N0 and and a matched pair of points contributes with 0. Hence, the TopoMatch loss takes values in 1 is given by half the number of unmatched features in both P and G, i.e. 1\n2\n\n(cid:0)# ker(τ (P , G)) + # coker(τ (P , G))(cid:1).\n\n2 )2 + (1 − 1\n\nLTM(P , G) =\n\n2 )2 = 1\n\n2\n\n(a) βerr(P , G) = 0\n\n(b) LTM(P , G) = 2\n\n(c) LW(P , G) = 0\n\nFigure 8: Illustration of the advantages of our TopoMatch error over the Betti number error. (a) shows a prediction P (left), ground truth G (right) and the corresponding Betti number error. (b) shows the TopoMatch matching in dim-1 (no features are matched) with its corresponding loss and (c) shows the Wasserstein matching in dim-1 (same color indicates a matching) with its corresponding loss. Note that both Betti number error and Wasserstein loss fail to represent the spatial error, while TopoMatch correctly does not match any cycles resulting in a loss of 2.\n\n4 EXPERIMENTATION\n\nDatasets We employ a set of six datasets with diverse topological features for our validation experimentation. Two datasets, the Massachusetts roads dataset, and the CREMI neuron segmentation dataset, exhibit frequently connected curvilinear, network-like structures, which form a large number of cycles in the foreground. The C.elegans infection live/dead image dataset (Elegans) from the Broad Bioimage Benchmark Collection Ljosa et al. (2012) and our synthetic, modified MNIST dataset LeCun (1998) (synMnist) consist of a balanced number of dimension-0 and dimension-1 features. And third, the colon cancer cell dataset (Colon) from the Broad Bioimage Benchmark Collection Carpenter et al. (2006); Ljosa et al. (2012) and the Massachusetts buildings dataset (Buildings) Mnih (2013) have ”blob-like” foreground structures. They contain very few dimension-1 features but every instance of a cell or building forms a dimension-0 feature.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Main results for TopoMatch and three baselines on six datasets. Green columns indicate the topological metrics. Bold numbers highlight the best performance for a given dataset if it is significant (i.e. the second best performance is not within std/8). We find that TopoMatch improves the segmentations in all topological metrics for all datasets. We further observe a constantly high performance in volumetric metrics. ↑ indicates higher value wins and ↓ the opposite.\n\nLoss\n\nDice ↑\n\nclDice ↑\n\nAcc. ↑\n\nT.M.↓\n\nT.M.-0↓\n\nT.M.-1↓\n\nBetti ↓\n\nBetti-0 ↓\n\nBetti-1 ↓\n\nI\n\nM E\nR C\n\ns d\na o\nR\n\nt s\ni\n\nn\n\nM\n\nn y\ns\n\ns n\na g\ne l\n\nE\n\nn o\n\nl\n\no C\n\ns g\nn\n\ni\n\nd\n\nl i\n\nu B\n\nDice clDice Hu et al. Ours\n\nDice clDice Hu et al. Ours\n\nDice clDice Hu et al. Ours\n\nDice clDice Hu et al. Ours\n\nDice clDice Hu et al. Ours\n\nDice clDice Hu et al. Ours\n\n0.894 0.879 0.888 0.893\n\n0.663 0.668 0.674 0.663\n\n0.871 0.875 0.866 0.849\n\n0.922 0.917 0.921 0.919\n\n0.899 0.907 0.902 0.907\n\n0.623 0.632 0.625 0.625\n\n0.939 0.944 0.935 0.941\n\n0.698 0.704 0.712 0.713\n\n0.907 0.921 0.915 0.915\n\n0.959 0.964 0.959 0.960\n\n0.863 0.871 0.876 0.871\n\n0.672 0.693 0.677 0.685\n\n0.959 0.952 0.957 0.959\n\n0.974 0.975 0.974 0.972\n\n0.962 0.963 0.960 0.954\n\n0.984 0.982 0.984 0.983\n\n0.970 0.974 0.972 0.975\n\n0.934 0.931 0.934 0.937\n\n74.82 73.52 81.24 64.90\n\n58.90 65.50 50.50 41.50\n\n1.849 1.270 1.425 1.140\n\n2.05 1.95 2.15 1.70\n\n22.13 23.63 17.25 16.00\n\n286.22 285.60 278.30 244.58\n\n19.84 17.18 22.12 15.50\n\n43.52 51.04 36.52 28.15\n\n0.979 0.436 0.502 0.265\n\n1.30 1.10 1.42 1.05\n\n10.88 9.38 7.75 7.13\n\n275.50 267.98 268.75 235.63\n\n54.98 56.34 59.12 49.40\n\n15.38 14.46 13.98 13.35\n\n0.870 0.834 0.923 0.875\n\n0.750 0.850 0.725 0.650\n\n11.25 14.25 9.50 8.88\n\n10.73 17.63 9.55 8.95\n\n114.12 103.92 118.16 79.16\n\n113.96 125.83 95.83 75.08\n\n2.590 1.640 1.802 1.348\n\n2.60 2.20 2.50 1.90\n\n33.75 37.75 22.00 21.50\n\n162.95 175.50 181.10 118.45\n\n39.12 33.64 43.68 30.36\n\n86.54 101.67 72.54 55.79\n\n1.674 0.700 0.764 0.426\n\n1.40 1.20 1.35 0.80\n\n13.75 11.75 7.00 6.25\n\n151.70 155.05 169.60 107.75\n\n75.00 70.28 74.48 48.80\n\n27.42 24.17 23.29 19.29\n\n0.916 0.940 1.038 0.922\n\n1.20 1.00 1.15 1.10\n\n20.00 26.00 15.00 15.25\n\n11.25 20.45 11.50 10.70\n\nTraining of the segmentation networks For implementation details, e.g., the training splits, please refer to App. I and J. We train all our models for a fixed, dataset-specific number of epochs and evaluate the final model on an unseen test set. We train all models on an Nvidia P8000 GPU using Adam optimizer. We run experiments on a range of alpha-parameters for clDice (Shit et al. (2021)), the Wasserstein matching (Hu et al. (2019)), and TopoMatch; we choose to present the top performing model in Table 1; extended results are given in tables 3, 4, 5,2, 6 in App. H.\n\nFigure 9: Qualitative Results on CREMI dataset (same models used as in Table 1). Topological errors are indicated by red circles. Our method leads to less topological errors in the segmentation.\n\n4.1 RESULTS\n\nMain Results Our proposed TopoMatch loss improves the topological accuracy of the segmentations across all datasets (Table 1), irrespective of the choice of hyper-parameters (Table 3) compared to all baselines. We show superior scores for the topological metrics TopoMatch error (T.M.) and Betti number error (Betti) in both dimension-0 and dimension-1. Further, the volumetric metrics (Accuracy, Dice, and clDice) of the segmentations show equivalent, if not superior quantitative results for our method. Our method can be trained from scratch or used to refine pre-trained networks. Importantly, our method improves the topological correctness of curvilinear segmentation problems (Roads, CREMI), blob-segmentation problems (Buildings, Colon), and mixed problems (SynMnist, Elegans). We confidently attribute this to the theoretical guarantees of induced matchings, which hold for the foreground and the background classes in dim-0 and dim-1. For illustration, please consider the Roads and Buildings dataset; essentially, the topology of the background of the Buildings dataset is very similar to the foreground in Roads. I.e., the foreground of the Roads and the background of the Buildings dataset are interesting in dim-1, whereas the background of the roads and the foreground of the Buildings are interesting in dim-0. As our method can efficiently leverage\n\n8\n\nclDiceHu et al. LabelDice ImageOursUnder review as a conference paper at ICLR 2023\n\nthe topology features of both foreground and background when we apply sub- and superlevelsetmatching and it is intuitive that our method prevails in both. It is of note that for some datasets, the method by Hu et al. (2019) is the best performing baseline and for some Shit et al. (2021).\n\nTable 2: bothlevel versus superlevel matching of our method on the Elegans dataset and the CREMI dataset. The bothlevel matching appears to have a more pronounced contribution in the scenario of topologically complex background\n\nlevel\n\nElegans bothlevel Elegans superlevel\n\nCREMI bothlevel CREMI superlevel\n\nα\n\n0.005 0.005\n\n0.5 0.5\n\nDice\n\n0.92 0.92\n\n0.89 0.89\n\nclDice\n\n0.96 0.95\n\n0.95 0.95\n\nAcc.\n\n0.98 0.98\n\n0.95 0.96\n\nT.M.\n\nT.M.-0\n\nT.M.-1\n\nBetti\n\nBetti-0\n\nBetti-1\n\n1.70 2.15\n\n1.05 1.35\n\n60.48 59.20\n\n12.92 14.40\n\n0.65 0.80\n\n47.56 44.80\n\n1.90 2.40\n\n52.08 52.24\n\n0.80 1.30\n\n25.28 28.16\n\n1.10 1.10\n\n26.80 24.08\n\nARI\n\n0.93 0.91\n\n0.93 0.93\n\nVOI\n\n0.36 0.43\n\n0.36 0.35\n\nAblation experiments In order to study the effectiveness of the TopoMatch loss, we conduct various ablation experiments. First, we study the effect of the α parameter in our method, see Table 3. We find that increasing α improves the topological metrics. For some datasets, e.g., synMnist, the Dice metric is compromised if α is chosen too big. Therefore, we conclude that α is a tunable and dataset-specific parameter. Ostensibly, the effect of the α parameter cannot be compared directly. Nonetheless, it appears that our method is more robust towards variation in α. Second, we study the effect of considering both the foreground and the background (bothlevel) versus solely the foreground (superlevel). We find that bothlevel is particularly useful if the background has a complex topology (e.g. Elegans), whereas superlevel shows a similar performance if the foreground has a more complex topology (e.g. CREMI), see Table 2. Third, we test the effect of pre-training and training from scratch for TopoMatch and the method by Hu et al. (2019). Table 6 shows that our method can be trained from scratch efficiently if not superiorly, whereas the baseline method struggles in that setting – especially on more complex datasets such as CREMI. We attribute this to the spatially correct matching of TopoMatch and its consequences on the gradient (see Sec. 3.2). Training-from-scratch means that there is a lot potential for false positives and false negatives in the Wasserstein matching (see App. F) since there are a lot noisy features when the network is still uncertain. For example for CREMI we found that the Wasserstein matching matches cycles incorrectly in more then 99 % of the cases, see appendix F.1. Moreover, we observe that TopoMatch optimizes the Wasserstein loss more efficiently (see App. F.2). We also experiment with adding a boundary to images in order to close loops that cross the image border, similar to Hu et al. (2019), and term this relative TopoMatch. Table 4 shows a negligible effect on all metrics. For additional ablation and more metrics on the ablation studies, please refer to the App. H. The computational complexity of our matching is O(n3), see App. D for details.\n\n5 DISCUSSION\n\nIn this paper, we propose a rigorous method called TopoMatch, which enConcluding remarks ables the faithful quantification of corresponding topological properties in image segmentation. Herein, our method is the first to guarantee the correct matching of persistence barcodes in image segmentation according to their spatial correspondence. We show that TopoMatch is efficient as an interpretable segmentation metric, which can be understood as a sharpened variant of the Betti error. Further, we show how our method can be used to train segmentation networks. Training networks using TopoMatch is stable and leads to improvements on all 6 datasets.We foresee vast application potential in challenging tasks such as road network, vascular network and Neuron instance segmentation. We are thus hopeful that our method’s theory and experimentation will stimulate future research in this area.\n\nLimitations In the general setting of persistent homology of functions on arbitrary topological spaces, there are instances where maps of persistence modules cannot be written as matchings. This is somewhat analogous to the fact that in linear algebra, certain linear transformations cannot be diagonalized. We did not observe any such case in our specific segmentation setting. A theoretical investigation of this question will be the subject of future work. Further, we understand applicationspecific experimental limitations. Our method’s computational complexity is beyond widely used loss functions such as BCE (see App. D); moreover, our current implementation is only available in 2D, whereas the theoretical guarantees trivially generalize to 3D.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nTo facilitate understanding of the theory section please refer to the basic definitions in the appendix. The core algorithm (TopoMatch) is available as a python script in the supplementary material and is printed in pseudocode in the appendix, section C. The training details are also described in the appendix, section I. Furthermore, all of our code and all of our experimentation, including baselines and hyperparameters, is available in a public anonymous Github repository 1.\n\n7 ETHICS STATEMENT\n\nWe, the authors, declare that we strictly adhere to the ICLR Code of Ethics. Our method and experimentation are carried out on (partly modified) public datasets with no known ethical concern associations. Our studies do not involve human subjects and we do not foresee any conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues.\n\nREFERENCES\n\nShahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topological constraints. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 872–881, 2021.\n\nUlrich Bauer. Ripser: efficient computation of vietoris–rips persistence barcodes. Journal of Applied\n\nand Computational Topology, 5(3):391–423, 2021.\n\nUlrich Bauer and Michael Lesnick.\n\nInduced matchings and the algebraic stability of persistence\n\nbarcodes. Journal of Computational Geometry, 6(2):162–191, 2015.\n\nUlrich Bauer and Maximilian Schmahl. Efficient computation of image persistence. arXiv preprint\n\narXiv:2201.04170, 2022.\n\nAnne E Carpenter, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, Jason Moffat, et al. Cellprofiler: image analysis software for identifying and quantifying cell phenotypes. Genome biology, 7(10):1–11, 2006.\n\nFr ́ed ́eric Chazal, David Cohen-Steiner, Marc Glisse, Leonidas J Guibas, and Steve Y Oudot. Proximity of persistence modules and their diagrams. In Proceedings of the twenty-fifth annual symposium on Computational geometry, pp. 237–246, 2009a.\n\nFr ́ed ́eric Chazal, David Cohen-Steiner, Marc Glisse, Leonidas J Guibas, and Steve Y Oudot. Proximity of persistence modules and their diagrams. In Proceedings of the twenty-fifth annual symposium on Computational geometry, pp. 237–246, 2009b.\n\nMingfei Cheng, Kaili Zhao, Xuhong Guo, Yajing Xu, and Jun Guo.\n\nand feature-refinement network for curvilinear structure segmentation. IEEE/CVF International Conference on Computer Vision, pp. 7147–7156, 2021.\n\nJoint topology-preserving In Proceedings of the\n\nJames Clough, Nicholas Byrne, Ilkay Oksuz, Veronika A Zimmer, Julia A Schnabel, and Andrew King. A topological loss function for deep-learning based image segmentation using persistent homology. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\nDavid Cohen-Steiner, Herbert Edelsbrunner, and John Harer. Stability of persistence diagrams. In Proceedings of the twenty-first annual symposium on Computational geometry, pp. 263–271, 2005.\n\nDavid Cohen-Steiner, Herbert Edelsbrunner, John Harer, and Yuriy Mileyko. Lipschitz functions have l p-stable persistence. Foundations of computational mathematics, 10(2):127–139, 2010.\n\n1https://anonymous.4open.science/r/TopoMatch-ED20/README.md\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nWilliam Crawley-Boevey. Decomposition of pointwise finite-dimensional persistence modules,\n\n2012. URL https://arxiv.org/abs/1210.0819.\n\nOlaf Delgado-Friedrichs, Vanessa Robins, and Adrian Sheppard. Skeletonization and partitioning of digital images using discrete morse theory. IEEE transactions on pattern analysis and machine intelligence, 37(3):654–666, 2014.\n\nHerbert Edelsbrunner, John Harer, et al. Persistent homology-a survey. Contemporary mathematics,\n\n453:257–282, 2008.\n\nJan Funke, Fabian Tschopp, William Grisaitis, Arlo Sheridan, Chandan Singh, Stephan Saalfeld, and Srinivas C. Turaga. Large scale image segmentation with structured loss based deep learning for connectome reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(7):1669–1680, Jul 2019. ISSN 1939-3539. doi: 10.1109/tpami.2018.2835450. URL http: //dx.doi.org/10.1109/TPAMI.2018.2835450.\n\nAd ́elie Garin, Teresa Heiss, Kelly Maggs, Bea Bleile, and Vanessa Robins. Duality in persistent\n\nhomology of images. arXiv preprint arXiv:2005.04597, 2020.\n\nTeresa Heiss and Hubert Wagner. Streaming algorithm for euler characteristic curves of multidimensional images. CoRR, abs/1705.02045, 2017. URL http://arxiv.org/abs/1705. 02045.\n\nX Hu, Y Wang, F Li, D Samaras, and C Chen. Topology-aware segmentation using discrete morse\n\ntheory. In International Conference on Learning Representations (ICLRR), 2021.\n\nXiaoling Hu and Chao Chen. arXiv:2112.07812, 2021.\n\nImage segmentation with homotopy warping.\n\narXiv preprint\n\nXiaoling Hu, Fuxin Li, Dimitris Samaras, and Chao Chen. Topology-preserving deep image seg-\n\nmentation. Advances in neural information processing systems, 32, 2019.\n\nViren Jain, Benjamin Bollmann, Mark Richardson, Daniel R Berger, Moritz N Helmstaedter, Kevin L Briggman, Winfried Denk, Jared B Bowden, John M Mendenhall, Wickliffe C Abraham, et al. Boundary learning by optimization with topological constraints. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2488–2495. IEEE, 2010.\n\nTomasz Kaczynski, Konstantin Mischaikow, and Marian Mrozek. Cubical Homology, pp. 39– ISBN 978-0-387-21597-6. doi: 10.1007/\n\n92. Springer New York, New York, NY, 2004. 0-387-21597-2 2. URL https://doi.org/10.1007/0-387-21597-2_2.\n\nYann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.\n\nVebjorn Ljosa, Katherine L Sokolnicki, and Anne E Carpenter. Annotated high-throughput mi-\n\ncroscopy image sets for validation. Nature methods, 9(7):637–637, 2012.\n\nVolodymyr Mnih. Machine Learning for Aerial Image Labeling. PhD thesis, University of Toronto,\n\n2013.\n\nAgata Mosinska et al. Beyond the pixel-wise loss for topology-aware delineation. In CVPR, pp.\n\n3136–3145, 2018.\n\nDoruk Oner, Mateusz Kozi ́nski, Leonardo Citraro, Nathan C Dadap, Alexandra G Konings, and Pascal Fua. Promoting connectivity of network-like structures by enforcing region separation. arXiv preprint arXiv:2009.07011, 2020.\n\nNina Otter, Mason A Porter, Ulrike Tillmann, Peter Grindrod, and Heather A Harrington. A roadmap\n\nfor the computation of persistent homology. EPJ Data Science, 6:1–38, 2017.\n\nKazuma Sasaki, Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Joint gap detection and In Proceedings of the IEEE conference on computer vision and\n\ninpainting of line drawings. pattern recognition, pp. 5725–5733, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSuprosanna Shit, Johannes C Paetzold, Anjany Sekuboyina, Ivan Ezhov, Alexander Unger, Andrey Zhylka, Josien PW Pluim, Ulrich Bauer, and Bjoern H Menze. cldice-a novel topology-preserving loss function for tubular structure segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16560–16569, 2021.\n\nHubert Wagner, Chao Chen, and Erald Vuc ̧ini. Efficient computation of persistent homology for cubical data. In Topological methods in data analysis and visualization II, pp. 91–106. Springer, 2012.\n\nDominik JE Waibel, Scott Atwell, Matthias Meier, Carsten Marr, and Bastian Rieck. Capturing shape information with multi-scale topological loss terms for 3d reconstruction. arXiv preprint arXiv:2203.01703, 2022.\n\nHaotian Wang, Min Xian, and Aleksandar Vakanski. Ta-net: Topology-aware network for gland segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1556–1564, 2022.\n\nXiaohong Wang and Xudong Jiang. Post-processing for retinal vessel detection. In Tenth International Conference on Digital Image Processing (ICDIP 2018), volume 10806, pp. 1442–1446. SPIE, 2018.\n\nHan Zhang and Lok Ming Lui. Topology-preserving segmentation network: A deep learning seg-\n\nmentation framework for connected component. arXiv preprint arXiv:2202.13331, 2022.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA ADDITIONAL TOPOLOGICAL MATCHING ILLUSTRATION\n\nFigure 10: Topological matchings. Illustration of the advantages of our TopoMatch algorithm over the existing Wasserstein matching and the Betti number error for two exemplary segmentations. On the left side, we depict a prediction-label pair for an image. On the right side, we depict the matched representative cycles in the same color for the Wasserstein matching (bottom row) and TopoMatch matching (top row). Our TopoMatch matches the spatially correct features and will penalize the correct features in the loss. Here, the Wasserstein matching mismatches the correctly predicted feature with the erroneously predicted feature, leading to a false loss for the wrongly segmented cycle.\n\nFigure 11: Motivation. Our TopoMatch (induced matching) and the Wasserstein matching (Hu et al. (2019)) for Elegans, Colon and Buildings label-prediction pairs. Here we match the connected components (dim-0). The matched components (according to the matching methods) are represented in the same color. We randomly sample 6 matched cycles in each pair.\n\n13\n\nFully correctgeometricmatchin TopoMatch.LWasserstein=0.1LTopoMatch=0.8Onefalsegeometricmatchand oneentirelyfalsematchin Wasserstein matching.TopoMatchLabelPrediction TopoMatchWasserstein MatchingLabelPrediction LabelPrediction LabelPrediction TopoMatchWasserstein MatchingLabelPrediction LabelPrediction Dim-0Dim-0Dim-0Under review as a conference paper at ICLR 2023\n\nFigure 12: Motivation. Our TopoMatch (induced matching) and the Wasserstein matching (Hu et al. (2019)) for Roads label-prediction pairs. The matched cycles (according to the matching methods) are represented in the same color. We randomly sample 6 matched cycles (dim-1) in each pair. We observe that our method correctly matches the cycles in the first two rows. The third row represents an example early in Training. Here we observe that our method correctly matches some ”finished” cycles but also provides a correct matching to the blue and green cycles which still have to be closed. Essentially, one can observe here that our TopoMatch leads to a correct loss.\n\n14\n\nTopoMatchWasserstein MatchingLabelPrediction LabelPrediction Under review as a conference paper at ICLR 2023\n\nFigure 13: Motivation. Our TopoMatch (induced matching) and the Wasserstein matching (Hu et al. (2019)) for CREMI label-prediction pairs. The matched cycles (according to the matching methods) are represented in the same color. We randomly sample 6 matched cycles in each pair.\n\n15\n\nTopoMatchWasserstein MatchingLabelPrediction LabelPrediction Under review as a conference paper at ICLR 2023\n\nFigure 14: Motivation. Our TopoMatch (induced matching) and the Wasserstein matching (Hu et al. (2019)) for synMnist label-prediction pairs (top row), colon cells (middle row) and the Elegans dataset (lower row). The matched connected components (dim-0) and cycles (dim-1) (according to the matching methods) are represented in the same color. We randomly sample 6 matched cycles in each pair.\n\n16\n\nTopoMatchWasserstein MatchingLabelPrediction LabelPrediction Dim-1Dim-1Dim-0Under review as a conference paper at ICLR 2023\n\nB ADDITIONAL QUALITATIVE RESULTS\n\nFigure 15: Qualitative Results on Roads and Buildings dataset. Image, Label, and different segmentations (same models as table 1). Topological errors are indicated by red circles. Our method leads to improved topology compared to the baselines.\n\n17\n\nclDiceHu et al. LabelDice ImageOursUnder review as a conference paper at ICLR 2023\n\nFigure 16: Qualitative Results on CREMI, Elegans and Colon dataset. Image, Label, and different segmentations (same models as table 1). Topological errors are indicated by red circles. Our method leads to improved topology compared to the baselines.\n\n18\n\nclDiceHu et al. LabelDice ImageOursUnder review as a conference paper at ICLR 2023\n\nFigure 17: Qualitative Results on SynMnist. Image, Label, and different segmentations (same models as table 1) on examples of the SynMnist testset. Topological errors are indicated by red circles. Our method always segments the correct topology.\n\nC DETAIL ALGORITHM\n\nBelow, we provide the pseudocode for an efficient realization of the TopoMatch matching. For the computation of the barcodes in dimension-0 we leveraged the Union-Find datastructure, which is very efficient at managing equivalence classes. Alexander duality allows us to use it in dimension-1, as well (see Garin et al. (2020)). Moreover, it can also be used for the computation of the image barcodes in both dimensions. Note that we adapt the Union Find class to manage the birth of equivalence classes. We use clearing (as proposed in Bauer (2021)) by keeping track of criticaledges and columns-to-reduce, in order to reduce the amount of operations during the reductions (see sections 2.2, 2.3).\n\n19\n\nclDiceHu et al. LabelDice ImageOursUnder review as a conference paper at ICLR 2023\n\nAlgorithm 1: TopoMatch Data: G, L Option: relative = F alse, f iltration = ‘superlevel′ Result: L0, L1, L\n\n1 begin\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n\n19\n\n20\n\n21\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\n42\n\nif filtration=‘superlevel’ then\n\nC ← max(G, L)\n\nelse\n\nC ← min(G, L)\n\n// Construction of comparison image\n\nend B(G), DG, VG, XG ← CubicalP ersistence(G, relative, f iltration, T rue); B(L), DL, VL, XL ← CubicalP ersistence(L, relative, f iltration, T rue); B(C), CC, VC, XC ← CubicalP ersistence(C, relative, f iltration, F alse); B(G, C) ← ImageP ersistence(DG, XG, CC, XC); B(L, C) ← ImageP ersistence(DL, XL, CC, XC); σ(G, C) ← InducedM atching(B(G, C), B(G), B(C)); σ(L, C) ← InducedM atching(B(L, C), B(L), B(C)); τ (L, G) = φ; U0, U1 = B(G)0, B(G)1 ; for ground truth V0, V1 = B(L)0, B(L)1 ; for prediction\n\n// Initialize matched refined intervals // Initialize unmatched refined intervals\n\n// Initialize unmatched refined intervals\n\nL0 = L1 = 0 ; for d ← 0 to 1 by 1 do\n\nforeach m0 ∈ σ(G, C)d do\n\nforeach m1 ∈ σ(L, C)d do\n\nif m0[2] = m1[2] then\n\npair\n\n// Initialize TopoMatch loss // Loop over dimension-d\n\n// Check for same image persistence\n\nAdd ((m0[0], m0[2], m1[0])) to τ (L, G)d; Remove (m0[0]) from Ud; Remove (m1[0]) from Vd; Remove (m1) from σ(L, C)d; p, q = m0[0], m1[0]; I0, I1 = VG(Index2Coord(p[0])), VG(Index2Coord(p[1])) ;\n\nindex to value\n\nJ0, J1 = VL(Index2Coord(q[0])), VL(Index2Coord(q[1])) ;\n\nindex to value\n\n// Map\n\n// Map\n\nLd = Ld + (I0 − J0)2 + (I1 − J1)2 ;\n\nintervals\n\n// Loss for matched\n\nbreak\n\nend\n\nend\n\nI0, I1 = VG(Index2Coord(p[0])), VG(Index2Coord(p[1])) ; // Map index\n\n; // Loss for unmatched intervals in ground\n\nI0, I1 = VL(Index2Coord(p[0])), VL(Index2Coord(p[1])) ;\n\n// Map index\n\n;\n\n// Loss for unmatched intervals in\n\n// Total TopoMatch loss\n\n20\n\nend foreach p ∈ Ud do\n\nto value Ld = Ld + (I0−I1)2 truth\n\n2\n\nend foreach p ∈ Vd do\n\nto value Ld = Ld + (I0−I1)2 2\nprediction\n\nend\n\nend L ← L0 + L1 ;\n\n43 44 end\n\nUnder review as a conference paper at ICLR 2023\n\n45 Procedure CubicalPersistence(I, relative, filtration, critical)\n\n46\n\n47\n\n48\n\n49\n\n50\n\n51\n\n52\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\n63\n\n64\n\n65\n\n66\n\n67\n\n68\n\n69\n\n70\n\n71\n\n72\n\n73\n\n74\n\n75\n\n76\n\n77\n\n78\n\n79\n\n80\n\n81\n\n82\n\n83\n\n84\n\n85\n\n86\n\n87\n\n88\n\nif relative=True then\n\nI ← AddBoundary(I);\n\n// Add image boundary\n\nend V , X, E ← F ilterCubeM ap(I, f iltration) ; // Valuemap, Indexmap & edges\n\nare computed using the CubeMap datastructure as in Wagner et al. (2012) // Initialize refined barcodes B(I)0, B(I)1 = φ ; C = φ ; // Initialize columns-to-reduce for the clearning trick if critical=True then\n\n// Initialize critical-edges for the clearing trick\n\nD = φ ;\n\nend U = U nionF ind(#cubes + 1) ; foreach e ∈ E do\n\nb0, b1 ← DualBoundary(X, e) ; x, y ← U.f ind(b0), U.f ind(b1); if x = y then\n\nAdd e to C , continue\n\n// Instantiate a Union-Find class // Compute refined intervals in dimension-1 // Find dual boundary of an edge\n\nend b = min(U.getbirth(x), U.getbirth(y)) ; if critical=True then Add e to D;\n\nend if (e, b) is valid then\n\nAdd (e, b) to B(I)1\n\nend U.union(x, y)\n\n// Retrieve birth\n\n// Check for positive interval\n\nend U = U nionF ind(#cubes) ; foreach e ∈ C do\n\nb0, b1 ← Boundary(X, e); x, y ← U.f ind(b0), U.f ind(b1); if x = y then continue\n\n// Instantiate a Union-Find class // Compute refined intervals in dimension-0 // Find boundary of an edge\n\nend b = max(U.getbirth(x), U.getbirth(y)) ; if (e, b) is valid then\n\n// Retrieve birth // Check for positive interval\n\nAdd (e, b) toB(I)0;\n\nend U.union(x, y)\n\nend if critical=True then\n\nreturn (B(I)0, B(I)1), D, V , X ; critical-edges, Valuemap & Indexmap\n\n// Return refined barcodes,\n\nelse\n\nreturn (B(I)0, B(I)1), C, V , X ; columns-to-reduce, Valuemap & Indexmap\n\n// Return refined barcodes,\n\nend\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n90\n\n91\n\n92\n\n93\n\n94\n\n95\n\n96\n\n97\n\n98\n\n99\n\n100\n\n101\n\n102\n\n103\n\n104\n\n105\n\n106\n\n107\n\n108\n\n109\n\n110\n\n111\n\n112\n\n113\n\n115\n\n116\n\n117\n\n118\n\n119\n\n120\n\n121\n\n122\n\n123\n\n124\n\n125\n\n126\n\n127\n\n128\n\n129\n\n130\n\n131\n\n132\n\n133\n\n134\n\n135\n\n136\n\n137\n\n138\n\n139\n\n140\n\n141\n\n142\n\n89 Procedure ImagePersistence(D, XI , C, XJ )\n\nB(I, J )0, B(I, J )1 = φ ; U = U nionF ind(#cubes) ; foreach e ∈ C do\n\nb0, b1 ← Boundary(XI , e); x, y ← U.f ind(b0), U.f ind(b1); if x = y then continue\n\n// Initialize image persistence pairs // Instantiate a Union-Find class // Compute pairs in dimension-0 // Find boundary of an edge\n\n114 Procedure InducedMatching(B(I, J ), B(I), B(J ))\n\nend // Retrieve birth b = max(U.getbirth(x), U.getbirth(y)); Add (e, b) to B(I, J )0 ; // All pairs for extended induced matching\n\n(see Sec. 2.3)\n\nU.union(x, y)\n\nend U = U nionF ind(#cubes + 1) ; foreach e ∈ D do\n\nb0, b1 ← DualBoundary(XJ , e); x, y ← U.f ind(b0), U.f ind(b1); if x = y then continue\n\n// Instantiate a Union-Find class // Compute pairs in dimension-1 // Find dual boundary of an edge\n\nend b = min(U.getbirth(x), U.getbirth(y)); Add (e, b) to B(I, J )1 ;\n\nmatching (see Sec. 2.3)\n\n// Retrieve birth // All intervals for extended induced\n\nU.union(x, y)\n\nend return (B(I, J )0, B(I, J )1) ;\n\n// Return image persistence pairs\n\nσ(I, J )0, σ(I, J )1 = φ ; for d ← 0 to 1 by 1 do\n\nforeach (a, b) ∈ B(I, J )d do\n\n// Initialize matched refined intervals // Loop over dimension-d // For each image persistence pair\n\nmi, mj = N one; foreach (c, d) ∈ B(I)d do\n\nif c = a then\n\nmi = (c, d); break\n\nend\n\nend if mi = N one then\n\ncontinue\n\nend foreach (c, d) ∈ B(J )d do\n\nif d = b then\n\nmj = (c, d); break\n\nend\n\nend if mj = N one then\n\ncontinue\n\n// Match left endpoints\n\n// Skip search if no match found\n\n// Match right endpoints\n\n// Skip search if no match found\n\nend Add (mi, (a, b), mj) to σ(I, J )d; Remove mi from B(I)d; Remove mj from B(J )d;\n\nend\n\nend return (σ(I, J )0, σ(I, J )1)\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nD COMPUTATIONAL COMPLEXITY\n\nFor a grayscale image represented by a matrix I ∈ RM,N , we have n = M N number of pixels and form a cubical grid complex of dimension d = 2. The computation of the filtration and the boundary matrix can be done efficiently using the CubeMap data structure (see Wagner et al. (2012)) with O(3dn + d2n) time and O(d2n) space complexity. Computing the barcodes by means of the reduction algorithm requires cubic complexity in the number of pixels O(n3) (see Otter et al. (2017)). Despite our empirical acceleration due to the Union-Find class and clearing tricks (as described in Bauer & Schmahl (2022); Bauer (2021)), the order complexity remains O(n3). We need O(n2) time complexity for computing the final matching and loss. It is noteworthy that Hu et al. (2019) also needs O(n3) time complexity to compute the barcode and O(n2) for the matching, whereas Shit et al. (2021) requires relatively lower complexity O(n) due to the overlap based loss formulation.\n\nE CONVERGENCE OF TOPOMATCH LOSS\n\n(a) CREMI\n\n(b) synMnist\n\n(c) ELEGANS\n\nFigure 18: Plot of the empirical convergence curves of our TopoMatch loss for the CREMI, MNIST, and ELEGANS datasets. We plot the TopoMatch contribution in the training loss for a varying number of epochs, which is dependent on the dataset size. We show that TopoMatch loss efficiently converges for the different datasets. The absolute magnitude of the loss varies from dataset to dataset because TopoMatch is a real interpretable measure of dim-0 and dim-1 topological features in the training images. E.g. CREMI has a substantially higher number of features, especially cycles, than Elegans, therefore, the absolute magnitude of the loss is likely higher.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nF WASSERSTEIN MATCHING\n\nThe pth Wasserstein distance is frequently used to measure the difference between persistence diagrams; it is given by\n\ndp(B1, B2) = inf γ\n\n(cid:32)\n\n(cid:88)\n\nq∈Dgm(B1)\n\n(cid:33)1/p\n\n∥q − γ(q)∥p\n\n∞\n\nfor p ≥ 1, where γ runs over all bijections Dgm(B1) → Dgm(B2) that respect the dimension. For a likelihood map L ∈ [0, 1]m×n and a ground truth G ∈ {0, 1}m×n, the authors of Hu et al. (2019) adopt this metric to define the Wasserstein loss\n\nLW(L, G) = min\n\nγ\n\n(cid:88)\n\n∥q − γ(q)∥2 2,\n\nq∈Dgm(L)\n\nwhere γ runs over all bijections Dgm(L) → Dgm(G) that respect the dimension. The bijection γ∗ achieving the minimum corresponds to the Wasserstein matching Dgm(L) → Dgm(G), which minimizes the total distance of matched points. For the represented topological features this means that the matching is purely based on their local contrast within their respective images. Furthermore, note that Dgm(G) contains exclusively the point (0, 1) since the entries of G are contained in {0, 1}. Hence, γ∗ matches points in Dgm(L) representing features in L with enough local contrast in descending order until Dgm(G) runs out of points. This procedure results in a matching of topological features, which potentially exhibit no spatial relation within their respective images (see Fig. 1,8c,19b and App. A) and can have a negative impact on the training of segmentation networks. To see this, we distinguish two cases for a fixed point q = (q1, q2) ∈ Dgm(L):\n\ncase 1: (false positive) q is matched but there is no spatially corresponding feature in G : Since q is matched to the point (0, 1) ∈ Dgm(G), the loss LW will be reduced by decreasing the value q1 and increasing the value q2. Hence, the segmentation network will learn to increase the local contrast of the feature described by the q (see Sec. 3.2), but it should be decreased.\n\n(a) L\n\nG\n\n(b) Wasserstein matching\n\nFigure 19: (a) A predicted likelihood map L and ground truth segmentation G. (b) visualizes the Wasserstein matching γ∗ (only the yellow cycles are matched), i.e. the top-left cycle in L is a false negative and the bottom-right cycle in L is a false positive.\n\ncase 2: (false negative) q is unmatched but there is a spatially corresponding feature in G: Since q is unmatched, the bijection γ∗ maps it to its closest point ((q1 + q2)/2, (q1 + q2)/2) on the diagonal ∆ and the loss LW will be reduced by increasing the value q1 and decreasing the value q2. Hence, the segmentation network will learn to decrease the local contrast of the feature described by q (see Sec. 3.2), but it should be increased.\n\nF.1 FREQUENCY OF INCORRECT WASSERSTEIN MATCHING\n\nNext, we study how frequently these two cases occur. Assuming that the TopoMatch matching is correct, we evaluate the quality of the Wasserstein matching on the CREMI dataset. Therefor, we choose a segmentation model to obtain label-prediction pairs for every image in the CREMI dataset and compute both matchings. Among the 37243 matched intervals in the barcodes of the predictions by the Wasserstein matching, only 224 have been matched correctly, i.e. it achieves a precision of 0.6%.\n\nF.2 WASSERSTEIN LOSS AS BETTI NUMBER ERROR\n\nFor a binarized output P and ground truth G, the Wasserstein loss and the Betti number error are closely related. A similar argumentation as in Sec. 3.3 for the TopoMatch loss shows that\n\nβerr(P , G) = 2LW(P , G).\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nA lower Betti number error of a model trained with our TopoMatch loss compared to a model trained with the Wasserstein loss asserts that the TopoMatch loss produces more faithful gradients during the training of segmentation networks. Note that, empirically, models trained with TopoMatch loss consistently outperform models trained with Wasserstein los with regard to the Betti number error (see Tables 1,3).\n\nG PERSISTENCE DIAGRAM AND BARCODES\n\n(a) Matching between Barcodes\n\n(b) Bijection between persistence diagram\n\nFigure 20: Illustrations of how to translate a matching between barcodes (a) into a bijection between persistence diagram (b) and vice versa. A red or blue line in (a) is a dot of the same color in (b). In (a), a green interval in between a blue and a red line indicates they are matched. In (b), a line connecting two points indicates that they are matched. For detail, please refer to Section 3.2.\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nH ADDITIONAL ABLATION EXPERIMENTS\n\nTable 3: α ablation on the synMnist dataset and the Roads dataset\n\ns r\nu - O —\n— —\n\ne c\ni D\nl c\n\nα\n\n0.0005 0.005 0.05 0.5\n\n0.05 0.1 0.25 0.75 0.5\n\nt e\n\nl a\n\n. 0.0005 0.005 0.05 0.5\n\nu H\n\ns r\nu O\n\ne c\ni D\nl c\n\n0.0005 0.005 0.05 0.5\n\n0.05 0.1 0.25 0.5 0.75\n\nl a\n\nt e\n\n. 0.0005 0.005 0.05 0.5\n\nu H\n\n-\n\ns d\na o\nR —\n— —\n\n-\n\n— —\n—\n\nt s\ni\n\nn\n\nM\n\nn y\ns -\n\n— —\n—\n\nDice\n\nclDice\n\nAcc.\n\nT.M.\n\nT.M.-0\n\nT.M.-1\n\nBetti\n\nBetti-0\n\nBetti-1\n\nARI\n\nVOI\n\n0.670 0.706 0.670 0.708 0.667 0.709 0.663 0.713\n\n0.664 0.701 0.663 0.697 0.667 0.701 0.668 0.704 0.663 0.696\n\n0.669 0.706 0.674 0.712 0.669 0.707 0.656 0.699\n\n0.866 0.907 0.871 0.920 0.849 0.916 0.796 0.888\n\n0.871 0.911 0.872 0.912 0.874 0.917 0.875 0.922 0.869 0.921\n\n0.872 0.909 0.870 0.908 0.867 0.916 0.785 0.862\n\n0.974 53.958 39.896 0.974 51.250 37.271 0.974 45.396 31.667 0.972 41.500 28.146\n\n0.975 77.292 61.688 0.975 81.188 65.979 0.975 79.188 64.125 0.975 65.500 51.042 0.975 78.625 63.750\n\n0.974 53.750 39.521 0.974 50.500 36.521 0.974 52.896 39.208 0.970 58.583 44.604\n\n0.962 0.962 0.955 0.939\n\n0.963 0.963 0.963 0.963 0.961\n\n0.963 0.962 0.960 0.935\n\n1.687 1.270 1.140 1.150\n\n1.616 1.642 1.342 1.270 1.196\n\n1.881 1.787 1.425 1.754\n\n0.843 0.458 0.265 0.291\n\n0.796 0.816 0.519 0.436 0.401\n\n1.001 0.911 0.502 0.562\n\n14.063 13.979 13.729 13.354\n\n15.604 15.208 15.063 14.458 14.875\n\n14.229 13.979 13.688 13.979\n\n0.844 0.812 0.875 0.859\n\n0.820 0.826 0.823 0.834 0.795\n\n0.880 0.876 0.923 1.192\n\n103.917 97.583 85.042 75.083\n\n79.375 24.542 74.042 23.542 62.833 22.208 55.792 19.292\n\n151.250 123.125 28.125 158.375 131.708 26.667 154.208 127.917 26.292 125.833 101.667 24.167 152.417 127.000 25.417\n\n102.500 95.833 99.875 105.417\n\n78.542 23.958 72.542 23.292 77.917 21.958 88.708 16.708\n\n2.302 1.596 1.348 1.428\n\n2.264 2.320 1.764 1.640 1.580\n\n2.650 2.498 1.802 1.968\n\n1.370 0.732 0.426 0.466\n\n1.328 1.384 0.826 0.700 0.622\n\n1.686 1.514 0.764 0.840\n\n0.932 0.864 0.922 0.962\n\n0.936 0.936 0.938 0.940 0.958\n\n0.964 0.984 1.038 1.128\n\n0.643 0.847 0.647 0.839 0.655 0.828 0.690 0.791\n\n0.588 0.895 0.599 0.885 0.622 0.879 0.615 0.873 0.618 0.874\n\n0.651 0.838 0.660 0.836 0.654 0.832 0.709 0.787\n\n0.844 0.537 0.873 0.481 0.868 0.491 0.805 0.612\n\n0.871 0.483 0.862 0.506 0.877 0.461 0.881 0.454 0.888 0.429\n\n0.880 0.461 0.864 0.504 0.893 0.425 0.814 0.589\n\nTable 4: Relative Frame ablation of our method on the Roads dataset\n\nα\n\n0.0005 0.005 0.05 0.5\n\ne v\n\ni t\na l\ne r\n\nv\n\ne 0.0005 0.005 0.005 0.5\n\ni t\na l\ne r\n- n\no n\n\nDice\n\nclDice\n\nAcc.\n\nT.M.\n\nT.M.-0\n\nT.M.-1\n\nBetti\n\nBetti-0\n\nBetti-1\n\nARI\n\nVOI\n\n0.670 0.706 0.670 0.708 0.667 0.709 0.663 0.713\n\n0.669 0.706 0.671 0.709 0.669 0.712 0.661 0.711\n\n0.974 53.958 39.896 0.974 51.250 37.271 0.974 45.396 31.667 0.972 41.500 28.146\n\n0.974 54.729 40.521 0.974 50.250 36.479 0.973 46.104 32.271 0.972 41.417 28.167\n\n14.063 13.979 13.729 13.354\n\n14.208 13.771 13.833 13.250\n\n103.917 97.583 85.042 75.083\n\n104.542 95.167 84.708 75.583\n\n79.375 24.542 74.042 23.542 62.833 22.208 55.792 19.292\n\n80.542 24.000 72.458 22.708 64.042 20.667 55.833 19.750\n\n0.643 0.847 0.647 0.839 0.655 0.828 0.690 0.791\n\n0.654 0.835 0.661 0.829 0.675 0.818 0.695 0.787\n\nTable 5: dimension-1 and dimensions-0,1 matching ablation for the Hu et al. method on the Roads dataset\n\nα\n\n0.0005 0.005 0.05 0.5\n\n1 -\n\nm d\n\ni\n\n,\n\n0 -\n\n1 0.0005 0.005 0.05 0.5\n\nm d\n\ni\n\nDice\n\nclDice\n\nAcc.\n\nT.M.\n\nT.M.-0\n\nT.M.-1\n\nBetti\n\nBetti-0\n\nBetti-1\n\nARI\n\nVOI\n\n0.669 0.706 0.674 0.712 0.669 0.707 0.656 0.699\n\n0.672 0.709 0.673 0.710 0.668 0.708 0.662 0.711\n\n0.974 53.750 39.521 0.974 50.500 36.521 0.974 52.896 39.208 0.970 58.583 44.604\n\n0.974 54.292 40.104 0.974 52.750 38.625 0.974 47.000 33.146 0.972 44.708 31.292\n\n14.229 13.979 13.688 13.979\n\n14.188 14.125 13.854 13.417\n\n102.500 95.833 99.875 105.417\n\n104.083 100.667 88.083 80.583\n\n78.542 23.958 72.542 23.292 77.917 21.958 88.708 16.708\n\n79.708 24.375 76.750 23.917 65.792 22.292 62.083 18.500\n\n0.651 0.838 0.660 0.836 0.654 0.832 0.709 0.787\n\n0.649 0.839 0.656 0.836 0.649 0.832 0.692 0.798\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nTable 6: Pretraining vs training from scratch of ours and the Hu et al. method on the Elegans dataset\n\nTraining\n\nI Ours f. scratch M\nOurs pretrained E\nHu et al. f. scratch R\nC Hu et al. pretrained\n\nn a\ng e\nl\n\ns Ours f. scratch Ours pretrained Hu et al. f. scratch Hu et al. pretrained\n\nE\n\nα\n\n0.05 0.05 0.05 0.05\n\n0.005 0.005 0.005 0.005\n\nDice\n\n0.882 0.889 0.880 0.895\n\n0.919 0.924 0.921 0.921\n\nclDice\n\n0.938 0.940 0.932 0.942\n\n0.960 0.963 0.959 0.962\n\nAcc.\n\n0.953 0.957 0.953 0.960\n\n0.983 0.984 0.984 0.984\n\nT.M.\n\n65.06 65.68 82.76 66.40\n\n1.700 1.950 2.150 2.075\n\nT.M.-0\n\nT.M.-1\n\n13.94 14.26 27.92 17.76\n\n1.050 1.200 1.425 1.250\n\n51.12 51.42 54.84 48.64\n\n0.650 0.750 0.725 0.825\n\nBetti\n\n45.72 64.40 85.60 75.36\n\n1.90 2.30 2.50 2.55\n\nBetti-0\n\nBetti-1\n\n27.16 27.96 55.28 34.96\n\n0.80 1.20 1.35 1.30\n\n18.56 36.44 30.32 40.40\n\n1.10 1.10 1.15 1.25\n\nARI\n\n0.919 0.905 0.905 0.909\n\n0.927 0.939 0.929 0.929\n\nVOI\n\n0.393 0.437 0.436 0.425\n\n0.359 0.313 0.350 0.349\n\nI DATASETS AND TRAINING SPLITS\n\nThe full training routine with the complete trainingsets and testsets will be available with our github repository 2. All our trainings are done on patches of 48 × 48 pixels. For the buildings dataset Mnih (2013), we downsample the images to 375×375 pixels and randomly choose 80 samples for training and 20 for testing. For each epoch, we randomly sample 8 patches from each sample. For the Colon dataset Carpenter et al. (2006); Ljosa et al. (2012), we downsample the images to 256 × 256 pixels; we randomly choose 20 samples for training and 4 for testing. For each epoch, we randomly sample 12 patches from each sample. For the CREMI dataset Funke et al. (2019), we downsample the images to 312 × 312 pixels; we choose 100 samples for training and 25 for testing. For each epoch, we randomly sample 4 patches from each sample. For the Elegans dataset Ljosa et al. (2012), we crop the images to 96 × 96 pixels; we randomly choose 80 samples for training and 20 for testing. For each epoch, we randomly sample 1 patch from each sample. For the synMnist dataset LeCun (1998), we synthetically modify the MNIST dataset to an image size of 48 × 48 pixels; please see our GitHub repository for details; we train on 4500 full, randomly chosen images and use 1500 for testing. For the Roads dataset Mnih (2013), we downsample the images to 375 × 375 pixels; we randomly choose 100 samples for training and 24 for testing. For each epoch, we randomly sample 8 patches from each sample.\n\nJ NETWORK SPECIFICATIONS\n\nWe use the following notation:\n\n1. In(input channels), Out(output channels), BI(output channels) present input, out-\n\nput, and bottleneck information (for U-Net);\n\n2. C(f ilter size, output channels) denote a convolutional layer followed by ReLU and\n\nbatch-normalization;\n\n3. U(f ilter size, output channels) denote a trans-posed convolutional layer followed by\n\nReLU and batch-normalization;\n\n4. ↓ 2 denotes maxpooling;\n\n5. ⊕ indicates concatenation of information from an encoder block.\n\nJ.1 UNET CONFIGURATION-I\n\nWe use this configuration for CREMI, synthMNIST, Colon and Elegans dataset. This is a lightweight U-net which has sufficient expressive power for these datasets.\n\nConvBlock : CB(3, out size) ≡ C(3, out size) → C(3, out size) →↓ 2\n\nUpConvBlock: UB(3, out size) ≡ U (3, out size) → ⊕ → C(3, out size)\n\nEncoder : CB(3, 256) → B(256)\n\nIN (1/3 ch) → CB(3, 16) → CB(3, 32) → CB(3, 64) → CB(3, 128) →\n\n2github/anonymous\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nDecoder : B(256) → UB(3, 256) → UB(3, 128) → UB(3, 64) → UB(3, 32) → UB(3, 16) → Out(1)\n\nJ.2 UNET CONFIGURATION-II\n\nWe had to choose a different U-Net architecture for the road and building dataset because we realized that a larger model is needed to learn useful features for this complex task.\n\nConvBlock : CB(3, out size) ≡ C(3, out size) → C(3, out size) →↓ 2\n\nUpConvBlock: UB(3, out size) ≡ U (3, out size) → ⊕ → C(3, out size)\n\nEncoder : CB(3, 1024) → B(1024)\n\nIN (3 ch) → CB(3, 64) → CB(3, 128) → CB(3, 256) → CB(3, 512) →\n\nDecoder : B(1024) → UB(3, 1024) → UB(3, 512) → UB(3, 256) → UB(3, 128) → UB(3, 64) → Out(1)\n\nK EVALUATION METRICS\n\nWe evaluate our experiments using a set of topological and pixel-based metrics. The metrics are computed with respect to the binarized predictions. Here, TopoMatch constitutes the most meaningful quantification, see section 3.3. We calculate the TopoMatch metric for dimension-0 (T.M.-0) and dimension-1 (T.M.-1) as well as their sum (T.M.). Furthermore, we implement the Betti number error for dimension-0 (Betti 0), dimension-1 (Betti 1), and their sum (Betti):\n\nβerr(P , G) =\n\n∞ (cid:88)\n\nd=0\n\n|βd(D(P )0.5) − βd(D(G)0.5)|\n\nIt computes the Betti numbers of both foregrounds and sums up their absolute difference in each dimension, i.e. it compares the topological complexity of the foregrounds. It is important to consider the dimensions separately since they have different relevance on different datasets. E.g., Roads has many 1-cycles, whereas Buildings has many 0-cycles (connected components).\n\nAdditionally, we use the traditional Dice metric and Accuracy, which describe the in total correctly classified pixels, as well as the clDice metric from Shit et al. (2021). Here, we calculate the clDice between the volumes and the skeleta, extracted using the skeletonize function of the skimage python-library. We compute all metrics on the individual test images of their respective size (without patching) and take the mean across the whole testset.\n\n28\n\nUnder review as a conference paper at ICLR 2023\n\nL BASIC DEFINITIONS AND TERMINOLOGY\n\nL.1 CUBICAL COMPLEXES\n\nA d-dimensional (cubical) cell in Rn is the Cartesian product c = (cid:81)n j=1 Ij of intervals Ij = [aj, bj] with aj ∈ Z, bj ∈ {aj, aj + 1} and d ∈ {0, . . . , n} is the number of non-degenerate intervals among {I1, . . . , Id}.\n\nIf c and d are cells and c ⊆ d, we call c a face of d of codimension dim(d) − dim(c). A face of codimension one is also called a facet. A d-dimensional (cubical) complex in Rn is a finite set of cubical cells in Rn with maximal dimension d that is closed under the face relation, i.e., if d ∈ K and c is a face of d, then c ∈ K. Furthermore we call a cubical complex K ′ ⊆ K a subcomplex of K.\n\nA filtration of a cubical complex K is given by a family (Kr)r∈R of subcomplexes of K, which satisfies:\n\n(1) Kr ⊆ Ks for all r ≤ s, (2) K = Kr for some r ∈ R.\n\nA filtered (cubical) complex K∗ is a cubical complex K together with a nested sequence of subcomplexes, i.e., a sequence of complexes\n\n∅ = K0 ⊆ K1 . . . ⊆ Km = K.\n\nA function f : K → R on a cubical complex is said to be order preserving if f (c) ≤ f (d) for a face c of a cell d.\n\nL.2 HOMOLOGY\n\nA chain complex C∗ consists of a family {Cd}d∈Z of vector spaces and a family of linear maps {∂d : Cd → Cd−1}d∈Z that satisfy ∂d−1 ◦ ∂d = 0.\n\nA map f : K → K ′ between cubical complexes is said to be cubical if it respects the face relation, i.e., f (c) must be a face of f (d) in K ′ if c is a face of d in K.\n\nL.3 HOMOLOGY OF CUBICAL COMOPLEXES\n\nHomology is a powerful concept involving local computations to capture information about the global structure of topological spaces. It assigns a sequence of abelian groups to a space which encode its topological features in all dimensions. A feature in dimension-0 describes a connected component, in dimension-1, it describes a loop, and in dimension-2, it describes a cavity. It also relates these features between spaces by inducing homomorphisms between their respective homology groups. We briefly introduce the homology of cubical complexes with coefficients in F2. For more details, we refer to Kaczynski et al. (2004). For d ∈ Z, we denote by Kd the set of d-dimensional cells in a cubical complex K. The F2-vector space Cd(K) freely generated by Kd is the chain group of K in degree d. We can think of the elements in Cd(K) as sets of d-dimensional cells and call them chains. These chain groups are connected by linear boundary maps ∂d : Cd(K) → Cd−1(K), which map a cell to the sum of its faces of codimension 1 and are extended linearly to all of Cd(K). The cubical chain complex C∗(K) is given by the pair ({Cd(K)}d∈Z, {∂d}d∈Z). We denote by Zd(K) = ker ∂d the subspace of cycles and by Bd(K) = im ∂d+1 the subspace of boundaries in Cd(K). Since ∂d−1 ◦ ∂d = 0, every boundary is a cycle and the homology group of K in degree d is defined by the quotient space Hd(K) := Zd(K)/Bd(K). In other words, Hd(K) consists of equivalence classes of d-cycles and two d-cycles\n\nFigure 21: (a) and (b) show cells and their boundary (red). (c) and (d) visualize two homologous 1-cycles (blue) in a cubical complex.\n\n(d) 1-cycle z2\n\n(c) 1-cycle z2\n\n(b) 2-cell\n\n(a) 1-cell\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nz1, z2 are equivalent (homologous) if their difference is a boundary. For convenience, we define H∗(K) = (cid:76) d∈Z Hd(K). Note that the homology groups still carry the structure of a F2-vector space and their dimension βd(K) = dimF2 (Hd(K)) is the dth Betti number of K.\n\nHomology does not only act on spaces; it also acts on maps between spaces. Therefor, a cubical map f : K → K ′ induces a linear map C∗(f ) : C∗(K) → C∗(K ′), by mapping a cell c ∈ K with dim(f (c)) = dim(c) to f (c) and extending this assignment linearly to all of C∗(K). Then C∗(f ) descends to a linear map H∗(f ) : H∗(K) → H∗(K ′) in homology since ∂∗ ◦ C∗(f ) = C∗(f ) ◦ ∂∗.\n\nL.4 PERSISTENCE MODULES\n\nA persistence module M consists of a family {Mr}r∈R of vector spaces, which are connected by linear transition maps maps Mr,s : Mr → Ms for all r ≤ s, such that\n\n(1) Mr,r = idMr for all r ∈ R, (2) Ms,t ◦ Mr,s = Mr,t for r ≤ s ≤ t.\n\nM is said to be pointwise finite-dimensional (p.f.d.) if Mr is finite-dimensional for every r ∈ R. A basic example of a persistence module is an interval module C(I) for a given interval I ⊆ R. It consists of vector spaces\n\nC(I)r =\n\n(cid:26)F2 0\n\nif r ∈ I, otherwise.\n\nand transition maps\n\nfor r ≤ s.\n\nC(I)r,s =\n\n(cid:26)idF2 0\n\nif r, s ∈ I, otherwise.\n\nA morphism Φ : M → N between persistence modules is a family {Φr : Mr → Nr}r∈R of linear maps, such that for all r ≤ s the following diagram commutes:\n\nMr\n\nΦr\n\nNr\n\nMr,s\n\nNr,s\n\nMs\n\nΦs\n\nNs\n\nWe call Φ an isomorphism (resp. monomorphism, epimorphism) of persistence modules if Φr is a isomorphism (resp. monomorphism, epimorphism) of vector spaces for all r ∈ R. For a family {Mi}i∈I of persistence modules, the direct sum (cid:76) ule consisting of vector spaces ((cid:76) ((cid:76)\n\ni∈I Mi is the persistence modi∈I (Mi)r for all r ∈ R and transition maps\n\ni∈I Mi)r = (cid:76)\n\ni∈I (Mi)r,s for all r ≤ s ∈ R.\n\ni∈I Mi)r,s = (cid:76)\n\nA multiset X consists of a set |X| together with a multiplicity function multX : |X| → N ∪ {∞}. Equivalently it can be represented by its underlying set ⨿X = (cid:83) {x}. We say X is finite if its underlying set ⨿X is finite and its cardinality #X is given by the cardinality of its underlying set.\n\n(cid:96)multX (x)\n\nx∈|X|\n\ni=1\n\nLet K∗ be a filtered cubical complex and L∗ a cell-wise refinement according to the compatible ordering c1, . . . , cl of the cells in K. The boundary matrix B ∈ Fl×l of L∗ is given entry-wise by\n\n2\n\nBi,j =\n\n(cid:26)1 0\n\nif σi is a facet of σj, otherwise.\n\nL.5 MATCHINGS\n\nA map f : X → Y between multisets is a map f : ⨿ X → ⨿Y between their underlying sets.\n\nA matching σ : X → Y between multisets is a bijection σ : X ′ → Y ′ for some multisets X ′, Y ′ that satisfy ⨿X ′ ⊆ ⨿X and ⨿Y ′ ⊆ ⨿Y . We call\n\n30\n\nUnder review as a conference paper at ICLR 2023\n\n• coim(σ) = X ′ the coimage of σ, • im(σ) = Y ′ the image of σ, • ker(σ) = X \\ X ′ the kernel and of σ, • coker(σ) = Y \\ Y ′ the cokernel of σ.\n\nFor a morphism Φ : M → N of persistence modules, the image of Φ is the persistence module im(Φ), with im(Φ)r = im(Φr) and transition maps im(Φ)r,s = Nr,s|im(Φr) : im(Φr) → im(Φs) for r, s ∈ R.\n\nLet M, N be persistence modules. We call M a (persistence) submodule of N if Mr is a subspace of Nr for every r ∈ R and the inclusions ir : Mr (cid:44)→ Nr assemble to a persistence map i = (ir)r∈R. In this case we write M ⊆ N . The composition of two matchings X σ1−→ Y σ2−→ Z is given by the composition of the bijections\n\n1 (Y ′) σ1−→ Y ′ σ2−→ σ2(Y ′), σ−1\n\nwith Y ′ = ⨿ im(σ1) ∩ ⨿ coim(σ2). A persistence module M is said to be staggered if every real number r ∈ R occurs at most once as endpoint of an interval in B(M ).\n\n31",
    "reference": "# Summary Of The Paper\n\nIn this paper, the authors propose a new loss and evaluation metric based on persistence theory for matching different image segmentations. The proposed loss can be quite different from the usual Wasserstein distance between persistence diagrams, and is rather based on the algebraic foundations of persistence theory, which turns out beneficial for practical purposes as the Wasserstein distances often induces matching errors. Finally the authors provide an exhaustive set of numerical experiments showcasing the efficiency of the new loss.\n\n# Strength And Weaknesses\n\nStrengths:\n---The proposed loss seems quite efficient from the numerical experiments\n---It is rather original as topology-based segmentations are often based on standard distances between persistence diagrams\n\nWeaknesses:\n---I think the writing of the paper could be improved, as I find it quite difficult to follow for someone that is not aware of topological data analysis and its algebraic foundations.\n---I think it would be interesting to compare against the simpler, more naive method of using the Wasserstein distances between L and min(L,G) and between min(L,G) and G. It seems that composing these two matchings would already provide better segmentations than the Wasserstein distance between L and G. Why are induced matchings better than this?\n---Optimizing losses based on persistent homology usually requires heavy mathematics (http://proceedings.mlr.press/v139/carriere21a.html, https://link.springer.com/article/10.1007/s10208-021-09522-y), so I was wondering if the gradient of TopoMatch is well defined also, and if it enjoys some kind of convergence guarantees?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity could be improved with more toy examples. The proposed loss is quite original though.\n\n# Summary Of The Review\n\nOverall, I think the paper is fine. The writing could be improved by adding some experiments from the supplementary inside the main body of the text and by simplifying a bit the theoretical section, but I think it is OK globally.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nON THE CONVERGENCE OF SGD UNDER THE OVERPARAMETER SETTING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWith the improvement of computing power, over-parameterized models get increasingly popular in machine learning. This type of model is usually with a complicated, non-smooth, and non-convex loss function landscape. However, when we train the model, simply using the first-order optimization algorithm like stochastic gradient descent (SGD) could acquire some good results, in both training and testing, albeit that SGD is known to not guarantee convergence for nonsmooth and non-convex cases. Theoretically, it was previously proved that in training, SGD converges to the global optimum with probability 1 − ε, but only for certain models and ε depends on the model complexity. It was also observed that SGD tends to choose a flat minimum, which preserves its training performance in testing. In this paper, we first prove that SGD could iterate to the global optimum almost surely under arbitrary initial value and some mild assumptions on the loss function. Then, we prove that if the learning rate is larger than a value depending on the structure of a global minimum, the probability of converging to this global optimum is zero. Finally, we acquire the asymptotic convergence rate based on the local structure of the global optimum.\n\n1\n\nINTRODUCTION\n\nWith the improvement of the computing power of computer hardware, an increasing number of over-parameterized models are deployed in the domain of machine learning. One of the most representative and successful models is what we called deep neural network (LeCun et al. (2015); Amodei et al. (2015); Graves et al. (2013); He et al. (2016); Silver et al. (2017)), which has achieved great empirical success in various application areas (Wu et al. (2016); Krizhevsky et al. (2017); Silver et al. (2017); Halla et al. (2022)). Meanwhile, deep neural networks are large in scale and have an optimization landscape that is in general non-smooth and non-convex (Wu et al., 2019; Brutzkus & Globerson, 2017). Training such a model should have been concerning. However, people could usually acquire very good results just through using first-order methods such as stochastic gradient descent (SGD). A large theoretical gap persists in understanding this process. Two main questions arise.\n\n1. Due to the over-parametrization and the highly complex loss landscape of deep neural networks, optimizing the deep networks to the global optimum is likely NP-hard (Brutzkus & Globerson, 2017; Blum & Rivest, 1992). Nevertheless, in practice, simple first-order methods, which does not have a convergence guarantee in the non-smooth and non-convex case (Liu et al., 2022a;b), are capable of finding a global optimum. This happens even more often on the training data (Zhang et al., 2021; Brutzkus & Globerson, 2017; Wu et al., 2019). It has been an open problem (Goodfellow et al., 2014) that, in this case, does SGD provably find the global optimum? Does the result generalize to more general model structures beyond neural networks?\n\n2. In general, over-parametrized models offer many global optimums. These global optimums have the same training loss of zero, and meanwhile drastically different test performance (Wu et al., 2018; Feng & Tu, 2021). Interestingly, studies find that SGD tends to converge to those generalizable ones (Zhang et al., 2021). In fact, it is observed empirically that SGD could usually find flat minima, which subsequently enjoys better generalization (Kramers, 1940; Dziugaite & Roy, 2017; Arpit et al., 2017; Kleinberg et al., 2018; Hochreiter & Schmidhuber, 1997; 1994). Why and how does SGD find a flat global minimum? The empirical finding has yet to be theoretically validated.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nRelated Works For the first question, in recent years, there have been a number of theoretical results that target to explain this phenomenon. Many of them focus on concrete neural network models, like two-layer networks with linear active function (Bartlett et al., 2018; Hardt & Ma, 2016). Several works need the inputs to be random Gaussian variables (Ge et al., 2018; Tian, 2017; Du et al., 2017; Zhong et al., 2017). Authors in Wu et al. (2019); Allen-Zhu et al. (2019) consider the non-smooth case, but its techniques is depending on the structure of the network. They prove when the number of nodes is enough large, the objective is “almost convex” and “semi-smooth”. The techniques unfortunately do not generalize to more general models. Another commonly used technique is to ignore the non-smoothness and apply the chain rule anyway on the non-smooth points (Bartlett et al., 2018). The derivation does provide some intuitions but they do not offer any rigorous guarantees, as the chain rule does not hold (Liu et al., 2022a;b). Even with these kinds of restrictions, existing works (Ge et al., 2018; Tian, 2017; Du et al., 2017; Bartlett et al., 2018; Vaswani et al., 2019; Chizat & Bach, 2018) only manage to find a high probability convergence result to the global optimum. The difference between this probability and 1 could depend on the structure of the model, like the number of nodes in the neural network, which raises further concerns on the tightness of the probability bound. It is currently lacking to analyze SGD for general models to obtain an almost surely convergence to the global optimum.\n\nFor the second question, most works investigate the flat minima in a qualitative way. A recent work is by Xie et al. (2020), which views the SGD process as a stochastic differential equation (SDE), and uses SDE to describe the process of the iteration escaping from the sharp minimum. Similar techniques are also used in the works by Wu et al. (2019); Feng & Tu (2021). Unfortunately, SGD can be viewed as an SDE only when the learning rate is sufficiently small, and for a normal learning rate trajectories formed by SGD and SDE could be arbitrarily different. Another technique used to study this problem is to use the linear stability (Wu et al., 2018; Feng & Tu, 2021), which considers a linear system near a global minimum. The behavior of SGD near some global minimum can then be characterized by the linear system of this global minimum. However, different from a deterministic system where the property near one point can be quantitative determine by the linearized system of this point, a stochastic system property near one point is determined by all points in Rd. Using this linearized function to fully represent SGD near some global minimum is thus not a rigorous argument.\n\nContributions\n\n1. Under several mild assumptions about the non-smooth and non-convex loss function, we provide the first proof that from an arbitrary initialization SGD could make the iteration converge to the global optimum almost surely, i.e., P (θn converges to a global optimum) = 1.\n\n2. Under the same set of assumptions and the same setting of SGD, we prove that if the learning rate is larger than a threshold, which depends on the sharpness of a global minimum, the probability which the iteration converges to this global optimum is strictly 0.\n\n3. With similar assumptions and the same setting, we acquire the asymptotic convergence rate of the iteration converging to the global optimum. By this result, we know that SGD achieves an arbitrary accuracy in polynomial time.\n\nTechnical Insight The basic intuition is as follows. We first understand the SGD as a Markov chain with the continuous state space. Then we aim to prove that the global optimum is the only absorbing state of this Markov chain. Concretely, due to the property of the sampling noise, this noise enjoys 0 variance when the optimization variable θ reaches the global optimum (Claim 2.1), i.e., Eξn ∥ ̃∇g(θ, ξn)− ̃∇g(θ)∥2 = 0 (notations are defined in the next section), which guarantees that once θn reaches the global optimum, it will not escape from the optimum. Meanwhile, in other local optimums, the positive variance makes θn jump out to this local optimum. Otherwise, as this Markov chain is a continuous state space Markov chain, an absorbing state with the measure 0 cannot become the real absorbing state (the probability of the θn reaching this absorbing state in every epoch is 0). Based on this, we need this absorbing state to have a flat-enough neighborhood (Assumption 2.2 in the new version), which deduces that θn that fall on this neighborhood tend to move closer to this absorbing state. Combining this absorbing state and this neighborhood statement, we can prove the distribution of θn will concentrate on the global optimum when as the iteration goes. Finally, this distribution will degenerate to the global optimum, that is, θn will converge to the global optimum.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nThis neighborhood is the key insight of proving the convergence of SGD. The neighborhood cannot be very sharp (have at most quadratic growth), which is the reason we made Assumption 2.2, item 1. It is actually reflected in Equation (8). A flat enough neighborhood can make the coefficient of the third term of (8) negative, which in turn makes the R(θn) (the Lyapunov function) to decrease with high probability (θn close to global optimum). Otherwise, if the neighborhood is sharp, this coefficient will become positive, which makes R(θn) increasing (θn away from global optimum).\n\n2 PROBLEM FORMULATION\n\nWe investigate SGD under the over-parametrization setting, under a few mild assumptions on the objective function. The setting and the assumptions, as well as some preliminaries that are relevant to the results, are provided in Section 2.1. We then present the sampling schemes in Section 2.2.\n\n2.1 OPTIMIZATION UNDER OVER-PARAMETRIZATION\n\nIn this paper, given a dataset D = {(xi, yi)}, xi, yi ∈ Rd, we consider a model ˆyi = f (θ, xi), and the mean-square error (MSE) loss, i.e.,\n\ng(θ) =\n\n1 N\n\nN (cid:88)\n\ni=1\n\ng(θ, xi), g(θ, xi) = (cid:0)f (θ, xi) − yi\n\n(cid:1)2\n\n.\n\n(1)\n\nThe goal of an optimization method, like SGD, is to obtain an optimum θ ∈ J ∗, where J ∗ = arg minθ∈Rd g(θ).\n\nIn the over-parametrization setting, this optimum is zero. To handle the non-smoothness, we recall the definition of Clarke subdifferential (Clarke, 1990), which is an important tool to design and operate SGD algorithms.\n\nDefinition 1 (Clarke subdifferential (Clarke, 1990)). Let ̄x ∈ Ω be given. The Clarke subdifferential of f at ̄x is defined by\n\n∂f ( ̄x) = co\n\n(cid:110)\n\nlim x→ ̄x\n\n∇f (x) : f is smooth at x\n\n(cid:111)\n\n,\n\nwhere co represents the convex hull. If f is furthermore smooth, it holds that ∂f (x) = {∇f (x)}. We use ̃∇f (x) to denote an arbitrary element in ∂f (x), and for convenient, we call ̃∇ as subgradient.\n\nThe Clarke subdifferential does not enjoy the chain rule and several techniques involved in regular gradient cannot be reused in our case. We provide a counterexample to illustrate this in Claim A.1.\n\nThis property and a few assumptions to eliminate pathological cases are described in the below assumption.\n\nAssumption 2.1. The loss function g(θ) satisfies the following conditions:\n\n1. g(θ) is continuous and smooth almost everywhere;\n\n2. The global optimum value of g(θ) is 0;\n\n3. The set of global optimum points J ∗ is composed of countably connected components Ji,\n\ni.e., J ∗ = (cid:83)+∞\n\ni=1 Ji (Ji ∩ Jj = ∅);\n\n4. There is a scalar c > 0, such that whenever g is smooth on θ1, θ2 then for any data point\n\n(xi, yi),\n\n(cid:13) (cid:13) ̃∇g(θ1, xi) − ̃∇g(θ2, xi)(cid:13)\n\n(cid:13) ≤ c max{∥θ1 − θ2∥, 1} .\n\nThis assumption describes the overall structure of the loss function g(θ). All 4 items in this Assumption are quite mild and are commonly used in optimization and learning.\n\nItems 1 and 2 are true under the MSE loss and the over-parametrization setting. Item 3 describes that the optimum is composed of countably many connected components and this item holds for almost all functions unless one delicately constructs a pathological counterexample Jin et al. (2022). In\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nthis paper, to make the presentation clear, we continue with the countably many points assumption J ∗ = (cid:83)+∞ i } to avoid the tedious arguments on continuum of optimums. Item 4 can be seen as a non-smooth extension of the traditional L-smooth condition, i.e., ∥∇g(x) − ∇g(y)∥ ≤ L∥x − y∥. It can be satisfied by many non-smooth functions, like ReLU and leaky-ReLU.\n\ni=1 {θ∗\n\nSimilar to the regular gradient, the subgradient is also zero at the optimum. Claim 2.1. For the MSE loss function (1), if the global optimum is 0, i.e., minθ∈Rd g(θ) = 0. Then the subgradient at the optimum points J ∗ is 0.\n\nProof. For any θ0 ∈ {θ | g is smooth at θ}, we can get that\n\n ̃∇g(θ0) = ∇g(θ0) =\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:0)f (θ0, xi) − yi\n\n(cid:1)∇f (θ0, xi) .\n\nThen for any θ∗ ∈ J ∗, we have\n\nlim θ0→θ∗\n\n ̃∇g(θ0) = lim\n\nθ0→θ∗\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:0)f (θ0, xi) − yi\n\n(cid:1)∇f (θ0, xi) = 0 ,\n\nwhere g is smooth at θ0. Then,\n\n∂g(θ∗) = co\n\n(cid:26)\n\nlim θ0→θ∗\n\n∇g(θ0) : f is smooth at z\n\n= co{0} .\n\n(cid:27)\n\nThis concludes that ̃∇g(θ∗) = 0.\n\nNotice that despite that g is non-smooth in general, in our setting, it is smooth on the optimum as described in the above claim. This distinguishes our setting from the line of literature on non-smooth optimization.\n\nTo make a global convergence, we need at least one θ∗ ∈ J ∗ to be not very “sharp”. That is, at the δθ∗ − neighboring of θ∗ the loss function holds L-smooth condition with the coefficient βθ∗ and an assumption as follow: Assumption 2.2. There exist θ∗ ∈ J ∗, rθ∗ ≥ 1, δ > 0, a neighboring area U (θ∗, δθ∗ ) of θ∗, such that for those θ ∈ U (θ∗, δθ∗ ) that ̃∇g(θ) holds\n\n1. For any mini-batch Ci, gCi(θ) holds the local one point L-smooth condition,\n\n∥ ̃∇g(θ)∥ < βθ∗ ∥θ − θ∗∥ (∀ θ ∈ U (θ∗, δθ∗ )).\n\ni.e.\n\n2. The loss function holds ̃∇g(θ)T (θ − θ∗) > αθ∗ ∥θ − θ∗∥rθ∗ +1 (∀θ ∈ U (θ∗, δθ∗ ))), for\n\nsome constant αθ∗ > 0.\n\nThe first item of this assumption is very mild. Due to Claim 2.1, we know g(θ) is smooth in θ∗, that is, limθ→θ∗ ̃∇g(θ) = ∇g(θ∗) = 0. Then item 1 is just to bound the speed of subgradient tend to 0 is not slower than a linear function (not too sharp as O((cid:112)∥θ − θ∗∥) or O(∥θ − θ∗∥0.9)). The second item of this assumption is very close to the local Kurdyka-Lojasiewicz condition, i.e. ∥∇g(θ)∥2r ≥ g(θ) − g(θ∗) (r ≥ 1)(θ ∈ U (θ∗, δθ∗ )) which is a typically mild condition used to substitute the local Polyak-Łojasiewicz condition (item 2 and the local Kurdyka-Lojasiewicz condition are totally equivalent for an unary function). This assumption is milder than several assumptions used in the previous works. It can be seen as the loss function has an rθ∗ + 1-order Taylor expansion on θ∗. Compared with the one point strongly convexity used in Li & Yuan (2017); Kleinberg et al. (2018), the positive Hessian matrix and local Polyak-Łojasiewicz condition in global optimum used in Wu et al. (2018); Jin et al. (2022), our assumption is much milder.\n\n2.2 TWO TYPES OF NOISE OF SGD\n\nIn the rest of this section we describe two types of SGD algorithms, by different sampling noise. The first type is with the traditional sampling noise while the second type is SGD with the sampling noise with global stable guarantee. They involve slightly different assumptions and the analysis of SGD also varies by the type of noise. Nevertheless, they conclude similar results as we will present in the next section.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n2.2.1 REGULAR SAMPLING NOISE\n\nWe start with the iterations of an (regular) SGD algorithm, that\n\nvn = ε0 ̃∇g(θn, ξn) , θn+1 = θn − vn ,\n\n(2)\n\nwhere {ξn} represents the sampling noise. That is, we have the noised sampling\n\n ̃∇g(θ, ξn) =\n\n1 |Ci|\n\n(cid:88)\n\n(cid:16)(cid:0)f (θ, ̄x) − ̄y(cid:1)2(cid:17)\n\n ̃∇\n\n,\n\n ̄x, ̄y∈Ci\n\nwhere Ci is a randomly selected mini-batch from the original data set. The next statement assumes that the subdgradient can be sampled without the sampling error being too large. It is necessary for an algorithm to use the gradient: Assumption 2.3. Let ξn be the sampling noise involved in the n-th iteration of SGD and ̃∇g(θ, ξn) be the noised sampling of the subgradient. For any θ ∈ Rd, it holds\n\nlim inf θ→∞\n\n∥∇g(θ)∥ > 0,\n\nand\n\n(cid:13) ̃∇g(θ, ξn)(cid:13) (cid:13) 2\n(cid:13) (cid:13) ̃∇g(θ)(cid:13) (cid:13) 2\n(cid:13) where M0 ≥ 0 is a constants decided by g. Meanwhile, we need lim inf θ→∞ ∥ ̃∇g(θ)∥ > max{4c\n\nlim sup θ→+∞\n\n< M0,\n\nEξn\n\n√\n\n√\n\nM0, 4c\n\nK0}.\n\n(cid:13) ̃∇g(θ, ξn) − ̃∇g(θ)(cid:13) (cid:13) 2\n(cid:13)\n\nFirst of this assumption is milder than the widely used bounded variance assumption, i.e., Eξn ≤ a (Li & Yuan, 2017; Kleinberg et al., 2018). Second part is to (cid:0)∥θ − θ1∥2 + combine the {θn} tend to ∞. For example, for a very simple loss functions g(θ) = 1 ∥θ − θ2∥2 + ∥θ − θ3∥2(cid:1), It hold our Assumption 2.3 but not hold bounded variance assumption. Meanwhile, this sampling immediately implies the below bound. Claim 2.2. For any bounded set Q that include J ∗, it holds\n\n3\n\nwhere GQ is a constant decided by Q.\n\nEξn\n\n(cid:13) ̃∇g(θ, ξn)(cid:13) (cid:13) 2\n(cid:13)\n\n≤ GQg(θ) (∀ θ ∈ Q) ,\n\nProof. For any smooth point in Q, the mini-batch gradient norm satisfies\n\n(cid:13) ̃∇gCi(θ)(cid:13) (cid:13) 2\n(cid:13)\n\n=\n\n≤\n\n4 |N0|2\n\nN (cid:88)\n\nxc∈Ci\n\n4 |N0|2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\nxc∈Ci\n\n(cid:0)f (θ, xc) − yc\n\n(cid:0)f (θ, xc) − yc\n\n(cid:1)2(cid:13)\n\n(cid:13) ̃∇f (θ, xc)(cid:13) 2\n(cid:13)\n\n≤\n\n(cid:1) ̃∇f (θ, xc)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 4N (cid:80)N\n\ni=1\n\n(cid:13) (cid:13) ̃∇f (θ, xi)(cid:13) 2\n(cid:13) N 2 0\n\ng(θ),\n\n(3)\n\nwhere N0 is the size of the mini-batch. Define\n\nhCi(θ) =\n\n4N (cid:80)N\n\ni=1\n\n(cid:13) (cid:13) ̃∇f (θ, xi)(cid:13) 2\n(cid:13) N 2 0\n\n.\n\nThrough Assumption 2.1, we know that h(θ) is bounded on smooth points. Then we have\n\n(cid:13) ̃∇gCi(θ, ξn)(cid:13) (cid:13) 2\n(cid:13)\n\n≤\n\n4N GQ N 2 0\n\ng(θ) (when g is smooth at θ) .\n\n(4)\n\nThen,\n\nEξn\n\n(cid:13) ̃∇g(θ, ξn)(cid:13) (cid:13) 2\n(cid:13)\n\n=\n\nN −1\n\nC N0−1 C N0\n\nN\n\n(cid:88)\n\n(cid:13) ̃∇gCi(θ)(cid:13) (cid:13) 2\n(cid:13)\n\n≤\n\nall Ci\n\n5\n\n4N GQC N0−1 0 C N0\n\nN 2\n\nN −1\n\nN\n\ng(θ) := GQg(θ) .\n\nUnder review as a conference paper at ICLR 2023\n\nFor the non-smooth point θ, we can prove for any sequence θ0 → θ (g is smooth at θ0), through Equation (4), there is\n\n(cid:13) (cid:13) (cid:13) lim\n\nθ0→θ\n\n(cid:13) 2\n ̃∇gCi(θ0, ξn) (cid:13) (cid:13)\n\n= lim θ0→θ\n\n(cid:13) ̃∇gCi (θ0, ξn)(cid:13) (cid:13) 2\n(cid:13)\n\n≤\n\n4N GQ N 2 0\n\nlim θ0→θ\n\ng(θ0) =\n\n4N GQ N 2 0\n\ng(θ) .\n\nRecall the following fact:\n\nIf ∥a1∥2 < s0, ∥a2∥2 < s0, . . . , ∥an∥2 < s0, the norm of their any convex combination\n\n∥ ̄a∥2 :=\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nn (cid:88)\n\ni=1\n\nλiai\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n<\n\n(cid:18) n\n\n(cid:88)\n\nλ2\n\ni\n\ni=1\n\n(cid:19)\n\ns0 ≤ s0 .\n\nThen we obtain\n\nThis concludes that\n\n∥ ̃∇gCi(θ)(cid:13) 2\n(cid:13)\n\n≤\n\n4N GQ N 2 0\n\ng(θ) .\n\nEξn\n\n(cid:13) ̃∇g(θ, ξn)(cid:13) (cid:13) 2\n(cid:13)\n\n≤ GQg(θ) .\n\nWe could observe that the noise variance Eξn ∥ ̃∇g(θ, ξn) − ̃∇g(θ)∥2 = 0 at the global optimum (Claim 2.1). Intuitively, the zero variance makes the θn stable in the global optimum, while for a local minimum or a saddle point the variance is nonzero in general. This is intuitively how SGD escapes from local minimum and saddle points.\n\nWe have to notice that the global optimum is a subset of the set where the noise variance equals 0. It is easy to prove that\n\nJ ∗ ⊆ (cid:8)θ | Eξn ∥ ̃∇(θ, ξn) − ̃∇(θ)∥2 = 0(cid:9) = J ∗∗ ,\n\nwhere J ∗∗ is equivalent to\n\nJ ∗∗ =\n\n(cid:92)\n\nCi\n\n(cid:110)\n\nθ | ̃∇\n\n(cid:16)(cid:0)f (θ, ̄x) − ̄y(cid:1)2(cid:17)\n\n(cid:111)\n\n= 0\n\n.\n\nOur techniques will eventually prove that the SGD with regular sampling noise converges to J ∗∗. This could be different than J ∗ in theory, but intuitively, for the over-parameter model and a large amount of data the model f (θ, x) is complex enough to make sure that other stationary points are sensitive to the mini-batch batch selection. As such making a point, that is not the global optimum, stationary to all batches simultaneously is almost impossible, i.e., J ∗∗/J ∗ = ∅. Nevertheless, in order to insure the rigor of the theory, we make an additional assumption only for the regular sampling noise. This assumption is lifted in the sampling noise with global stable guarantee.\n\nAdditional assumption for regular sampling noise For the sampling noise {ξn}, points that are stationary to all mini-batches must be in J ∗, i.e., J ∗ = J ∗∗. Meanwhile, for every mini-batch loss function gCi, the stationary point set of gCi is countable.\n\nIf one slightly modifies SGD by adding an additional Gaussian noise, we will prove that such sampling noise will enjoy a global stable guarantee. With this variant of SGD, the above assumption could be lifted. We now present our proposed variant of SGD.\n\n2.2.2 SAMPLING NOISE WITH GLOBAL STABLE GUARANTEE\n\nThe sampling noise we propose in this section is the regular noise in SGD plus an extra Gaussian noise, as\n\n(cid:0) ̃∇g(θn, ξn) + (cid:112)min{g(θn), K0}τnNn\n\n(cid:1) ,\n\nvn = ε0 θn+1 = θn − vn ,\n\n(5)\n\nwhere {ξn} again represents the sampling noise, K0 is a constant to prevent the noise from approaching infinity, {Nn} represents a mutually independent standard Gaussian noise, {τn} is a mutually independent Bernoulli variable, i.e., P (τn = 0) = p0, P (τn = 1) = 1 − p0, and {τn}, {ξn}, {Nn} are also mutually independent. The coefficient min{g(θn), K0} is to make sure the algorithm hold a positive noise variance Eξn,τn,Nn ∥vn∥2 > 0 in non-optimal stationary points. We use {τn} to reduce the scale of the problem, making the scale of the new noise equal to the scale of the mini-batch\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\ngradient ̃∇g(θn, ξn) and as the original sampling noise. For example, if the batch size is 100 and the scale of the original data set is 10000, then we can set p0 = 1 − 0.01, which makes the average scale (cid:1) guarantees of the noise min{g(θn), K0}τnNn that this algorithm has a positive variance in Rd/J ∗.\n\n(cid:1) also 100. The tail term (cid:112)min{g(θn), K0}τnNn\n\n3 MAIN RESULTS\n\nOur first main result states that SGD must converge to a global optimum with probability 1. This is a large improvement from previous results with only 1 − δ probability, where δ depends on the model. Our theorem answers the question raised in the introduction, affirmatively, that SGD could indeed obtain a global optimum even in this non-smooth non-convex over-parameter setting. The next two theorems discuss the cases of rθ∗ > 1 (higher than second-order local structure) and rθ∗ = 1 (second-order local structure) respectively. Theorem 3.1. Consider the SGD iteration in Equation (5), or alternatively Equation (2) with J ∗ = J ∗∗, and the MSE loss function (1). If Assumptions 2.1, 2.3 hold, and Assumption 2.2 holds with rθ∗ > 1, then for any 0 < ε0 < min{1/2cM0, 1/4cK0(1 − p0)}, and for any initialization θ1 ∈ Rd, {θn} converges to the set J ∗ almost surely, i.e.,\n\nlim n→∞\n\nd(θn, J ∗) = 0 a.s. ,\n\nwhere d(x, J ∗) = inf y{∥x − y∥, y ∈ J ∗} denotes the distance between point x and set J ∗. Meanwhile the value of the loss function converges to 0 almost surely, i.e.,\n\nlim n→∞\n\ng(θn) = 0 a.s. .\n\nFor each main result, we provide a proof sketch to illustrate our idea in deriving the result. A rigorous argument is deferred to the appendix.\n\nProof sketch. Our proof mainly relies on two techniques. The first technique is the Lyapunov It transfers the convergence of a high dimension vector θn to a one dimensional Lyamethod. punov function R(θn). The second technique is to use the idea of Markov process. We sketch these two steps and an additional step as follows.\n\nStep 1: In this step, we aim to prove that there exists at least one bounded set S0 such that there is no limit point of {θn} is in it almost surely. Through the Borel–Cantelli Lemma, it amounts to proving\n\n+∞ (cid:88)\n\nn=1\n\nP (cid:0)θn ∈ S0\n\n(cid:1) < +∞ .\n\n(6)\n\nIn order to prove Equation (6), we use the Lyapunov method, constructing a Lyapunov function R(θ) which holds a unique zero R(θ∗) = 0 and an open set ˆS0 which include θ∗ (exact forms of R(θ) and ˆS0 are provided in the appendix). We assign In as the characteristic function of the event {θn ∈ ˆS0}. Then we obtain the inequality\n\nI ( ˆS0) n+1 R(θn+1) − I ( ˆS0)\n\nn R(θn) ≤ −I ( ˆS0)\n\nn R\n\n2r\n\nr+1 (θn) + un ,\n\n(7)\n\nwhere un is defined in (12) with (cid:80)+∞\n\nn=1\n\nE(un) < +∞. Summing up Equation (6) yields\n\n+∞ (cid:88)\n\nn=1\n\nE (cid:0)I ( ˆS0)\n\nn R\n\n2r\n\nr+1 (θn)(cid:1) < E (cid:0)I ( ˆS0)\n\n1 R\n\n2r\n\nr+1 (θ1)(cid:1) +\n\n+∞ (cid:88)\n\nn=1\n\nE(un) < +∞ .\n\nSubsequently we could construct S0 := ˆS0/U (θ∗, δ′ r+1 (θn)(cid:1) < (cid:80)+∞ (cid:80)+∞ n=1\n\nE (cid:0)I ( ˆS0)\n\nE (cid:0)I (S0)\n\n0), for some small enough δ′ 0,\n\nto make r+1 (θn)(cid:1) < +∞. Then, as whenever θn ∈\n\nn R\n\nn=1\n\n2r\n\n2r\n\nˆS0/U (θ∗, δ′\n\nn R 0) we have R\n\n2r\n\nr+1 (θ) > ̃ε, we have\n\n+∞ (cid:88)\n\nn=1\n\nP (cid:0)θn ∈ S0\n\n(cid:1) <\n\n1 ̃ε\n\n+∞ (cid:88)\n\nn=1\n\nE (cid:0)I (S0)\n\nn R\n\n2r\n\nr+1 (θn)(cid:1) < +∞ .\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nAs such we conclude Equation (6), and through the Borel–Cantelli Lemma, we know that there is no limit point in S0 almost surely.\n\nStep 2: In this step, we aim to prove that for any bounded set S that has no intersection with J ∗, there is no limit point in it. The way we prove it is different for the two types of noise (2) and (5). Handling the sampling noise with global stable guarantee (5) is relatively simple. The Gaussian noise of (5) guarantees that it forms an irreducible Markov process. Then using the property of the irreducible Markov process directly will prove the statement. For (2), the situation becomes complicated where an argument of the regular sampling noise does not deduce an irreducible Markov process. We prove it using a delicate argument. We first prove that a max positive bounded invariant set D must hold its boundary set ∂D ∩ J ∗ ̸= ∅, and every trajectory started from this set must almost surely converge to some global optimum. Here a set is max positive invariant if any trajectories started in S0 will not escape S0 and for any points θ′′ /∈ J ∗ ∪ D, θ′′ ∪ D is not a positive invariant set. That means, for any point either almost every trajectory started with it converges to J ∗, or it holds a positive probability transfer to S0. For the first situation, this statement is satisfied. For the second situation, we can make a small enough positive measure set, such that for any θ ∈ S, there exists a δ′ 0, and some large enough k, P (θn+k ∈ S0 | θn = θ) > υ. Then we can get as desired\n\nυ\n\n+∞ (cid:88)\n\nn=1\n\nP (θn ∈ S) = υ\n\n=\n\n+∞ (cid:88)\n\n(cid:90)\n\nS\n\nn=k+1\n\n+∞ (cid:88)\n\n(cid:90)\n\nPn(dθ) < +∞ .\n\nn=k+1\n\nS(δ0,l0)\n\nPn−k(dθ) ≤\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\nS\n\nP (θn+k ∈ S(δ0,l0) | θn = θ)Pn−k(dθ)\n\nStep 3: In the previous step we actually proved that almost surely either θn → J ∗ or θn → +∞. Through the Kolmogorov 0-1 law, we know {θn converges} is a tail event. As such, P (θn → J ∗) ∈ {0, 1}. Meanwhile as P (θn → ∞) = 1 is impossible, P (θn → J ∗) could only take 1.\n\n√\n\n√\n\nM0, 4c\n\nIn step 3, we suspect that P (θn → ∞) = 1 is indeed impossible, even without the assumption lim inf θ→∞ ∥ ̃∇g(θ)∥ > max{4c K0}. In fact, as long as θn converges to J ∗ for any initialization θ1 in some neighboring domain of the optimum, it converges for all initialization. This is because for every initialization it either converges to the optimum or it has a positive probability to transfer to an arbitrary set with a positive measure. As the neighboring domain could be arbitrarily small, it is likely to exist. Theorem 3.2. Consider the SGD iteration in Equation (5), or alternatively Equation (2) with J ∗ = J ∗∗, and the MSE loss function (1). If Assumptions 2.1, 2.3 hold, and Assumption 2.2 holds with rθ∗ = 1, then for any 0 < ε0 < min{1/2cM0, αθ∗ /2(2 − p0)β2 θ∗ , 1/4cK0(1 − p0)}, where normal sampling noise 2 can be seen as p0 = 0, and for any θ1 ∈ Rd, θn converges to J ∗ almost surely, i.e.,\n\nlim n→∞\n\nd(θn, J ∗) = 0 a.s. ,\n\nwhere d(x, J ∗) = inf y{∥x − y∥, y ∈ J ∗} denotes the distance between point x and set J ∗. Meanwhile the value of the loss function converges to 0 almost surely, i.e.,\n\nlim n→∞\n\ng(θn) = 0 a.s. .\n\nProof sketch. This proof will be similar to the proof of Theorem 3.1. The difference is when rθ∗ = 1 the convergence towards a global optimum with second-order local structure is conditional on the selection of the initial learning rate ε0. The reason for this is the inequality\n\nI ( ˆS0) n+1 R(θn+1) − I ( ˆS0)\n\nn R(θn) ≤ −(cid:0)αθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\n(cid:1)I ( ˆS0)\n\nn R(θn) + un\n\n(8)\n\nholds only when the coefficient αθ∗ ε0 − 2(2 − p0)ε2 θ∗ > 0. By setting ε0 as the theorem the inequality and other arguments remain valid. This proof also agrees with our intuition that SGD converges to a sharper global optimum not as easy as a flat one (rθ∗ > 1).\n\n0β2\n\nRecall the second question raised in SGD was conjecturing if SGD tends to choose the flat minima (and so as to enjoy a better generalization). In the end of the above proof we find that SGD converges\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nto a sharper global optimum not as easy as a flat one. This observation is through positive results only, though. We wonder if the converse is also true, that is, if a global minimum is not flat, then SGD is unlikely to converge to that.\n\nIn the below theorem we answer the converse affirmatively. Is is proved that if ε0 is large enough, then the iteration will almost surely not converge to this optimum. Theorem 3.3. Consider the SGD iteration in Equation (5), or alternatively Equation (2) with J ∗ = J ∗∗, and the MSE loss function (1). If Assumptions 2.1, 2.3 hold, and Assumption 2.2 holds with rθ∗ = 1, then for any θ1 ∈ Rd, if ε0 > βθ∗ /2(2 − p0)α2 θ∗ , where normal sampling noise 2 can be seen as p0 = 0, the probability that θn converges to θ∗ is 0, i.e.,\n\n(cid:16)\n\nP\n\nlim n→∞\n\n∥θn − θ∗∥ = 0\n\n(cid:17)\n\n= 0 .\n\nProof sketch. The main idea is to prove that if the iteration always stays in a neighboring domain of θ∗, then the probability that this iteration converges to θ∗ is zero. The Lyapunov method is helpful in this case.\n\nStep 1: In this step, we aim to acquire a reverse inequality of (7). We first construct a Lyapunov function R(θ) and a domain S1 of θ∗, and an event Ai,n = {θn0 ∈ S1, n0 ∈ [i, n]} as well its characteristic function Ii,n. Then we can acquire an inequality\n\nIi,n\n\n(cid:0)R(θn+1) − R(θn)(cid:1) ≥ (cid:0)2(2 − p0)ε2\n\n0α2\n\nθ∗ − ε0βθ∗\n\n(cid:1)Ii,nR(θn) + Ii,nζn ,\n\n(9)\n\nwhere ζn is defined by equation 33. Notice that if (cid:0)2(2−p0)ε2 will be a variant of diffusion process.\n\n0α2\n\nθ∗ −ε0βθ∗\n\n(cid:1) > 0, then this inequality\n\nStep 2: In this step, we aim to prove when n approaches infinity, the iteration will transform a fixed part of itself out of S1. Through (9), we get\n\nE (cid:0)Ii,n+1R(θn+1)(cid:1) ≥\n\n(cid:32)\n\n1 + ˆp0 −\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n(cid:33)\n\nE (cid:0)Ii,nR(θn)(cid:1) .\n\nWe know if\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n< ˆp0 ,\n\nthen E (cid:0)Ii,n+1R(θn+1)(cid:1) will diverge to infinity, which is impossible to happen. As such, it must hold\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n≥ ˆp0 .\n\nStep 3: In this step, we will show that if E(Ii,+∞ ̸= 0) > 0, then Ii,nR(θn) will not converge to 0 almost surely. We prove it by contradiction and assume P (cid:0) limn→+∞ Ii,nR(θn) = 0(cid:1) = 1. That means for any ε′ → 1. Then\n\n→ 0, which concludes P\n\nIi,nR(θn) ≤ ε′\n\nIi,nR(θn) > ε′\n\n(cid:17)\n\n(cid:17)\n\n(cid:16)\n\n(cid:16)\n\n0 > 0, P\n\n0\n\n0\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n→ k′ε′\n\n0 .\n\nThis forms a contradiction. Step 4: In this final step, we will prove P (cid:0) limn→+∞ θn = θ∗(cid:1) = 0. We inspect the event {θn → θ∗}. If E(Ii,+∞ ̸= 0) > 0, then due to limn→+∞ Ii,ng(θn) − Ii,+∞g(θn) = 0 a.s., we could get P (cid:0) limn→+∞ Ii,+∞R(θn) = 0(cid:1) = 0. Then,\n\nP (cid:0){θn → θ∗} ∩ Ai,+∞\n\n(cid:1) = P (cid:0) lim\n\nn→+∞\n\nIi,+∞R(θn) = 0(cid:1) = 0 .\n\nOtherwise if E(Ii,+∞ ̸= 0) = 0, we have\n\nP (cid:0){θn → θ∗} ∩ Ai,+∞\n\n(cid:1) ≤ E(Ii,+∞ ̸= 0) = 0 .\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nAbsolutely, we have\n\nSubsequently we have\n\n{θn → θ∗} ⊂\n\n(cid:41)\n\nAi,+∞\n\n.\n\n(cid:40) +∞ (cid:91)\n\ni=1\n\nP (cid:0)θn → θ∗(cid:1) = P\n\n(cid:32)\n\n{θn → θ∗}\n\n(cid:40) +∞ (cid:91)\n\n(cid:92)\n\nm=1\n\n(cid:41)(cid:33)\n\nAi,+∞\n\n= P\n\n(cid:32) +∞ (cid:91)\n\ni=1\n\n{θn → θ∗}\n\n(cid:92)\n\nAi,+∞\n\n(cid:33)\n\n≤\n\n(cid:16)\n\nP\n\n+∞ (cid:88)\n\ni=1\n\n{θn → θ∗}\n\n(cid:92)\n\nAi,+∞\n\n(cid:17)\n\n= 0 .\n\nAs we have shown the asymptotic convergence of SGD, the natural question is how fast it converges. To provide the convergence rate, we will need a slightly stronger version of Assumption 2.2. We need, instead of just one θ∗, all θ∗, to satisfy the order r + 1 expansion. In this case, the supremum of the expansion order, among all optimum points, is denoted as ˆr = maxθ∗∈J ∗ ∞ := {θ∗ ∈ J ∗ | P (θn → θ∗) > 0}.\n\nrθ∗ , where J ∗\n\n∞\n\nOur next theorem provides the convergence rate of SGD. Theorem 3.4. Consider the SGD iteration in Equation (5), or alternatively Equation (2) with J ∗ = J ∗∗, and the MSE loss function (1). If Assumptions 2.1, 2.3 hold, and the variant of Assumption 2.2 described immediately preceding this statement holds with order ˆr + 1, then for any θ1 ∈ Rd, θn has an asymptotic convergence rate as\n\n(cid:40)\n\ng(θn)=\n\nO(cid:0)pn O(cid:0)n− 2\n\n(cid:1) a.s. , ˆr−1 (cid:1) a.s. ,\n\n0\n\nif ˆr = 1 , if ˆr > 1 ,\n\nwhere p0 < 1 is a constant decided by the learning rate ε0.\n\nProof sketch. The proof of this theorem is based on the proof of Theorem 3.1. We asymptotically bound of martingale difference (Lemma A.1) and with the bound apply the martingale convergence theorem. The asymptotic convergence rate follows.\n\nAs an immediate consequence of the convergence rate, the SGD algorithm could obtain an arbitrary accuracy in polynomial time. This validates the efficiency of SGD. Corollary 3.1. Consider the same setting as Theorem 3.4. For any θ1 ∈ Rd, the computational time for g(θn) to reach an η accuracy is\n\nO(cid:0)N0d · log( 1 O(cid:0)N0d · ( 1 η )\n\nη )(cid:1) a.s. , 2 (cid:1) a.s. ,\n\nˆr−1\n\nif ˆr = 1 ,\n\nif ˆr > 1 ,\n\n(cid:40)\n\nwhere N0 is the mini-batch size.\n\nREFERENCES\n\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\n\nparameterization. In International Conference on Machine Learning, 2019.\n\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep Speech 2: EndIn International Conference on Machine to-End speech recognition in English and Mandarin. Learning, 2015.\n\nDevansh Arpit, Stanisław Jastrz ̨ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International Conference on Machine Learning, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPeter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In International Conference on Machine Learning, 2018.\n\nAvrim L Blum and Ronald L Rivest. Training a 3-node neural network is NP-complete. Neural\n\nNetworks, 5(1):117–127, 1992.\n\nAlon Brutzkus and Amir Globerson. Globally optimal gradient descent for a ConvNet with Gaussian\n\ninputs. In International Conference on Machine Learning, 2017.\n\nLenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. Advances in neural information processing systems, 31, 2018.\n\nFrank H Clarke. Optimization and Nonsmooth Analysis. SIAM, 1990.\n\nSimon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-hidden-layer CNN: Don’t be afraid of spurious local minima. In International Conference on Machine Learning, 2017.\n\nGintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In Uncertainty in Artificial Intelligence, 2017.\n\nYu Feng and Yuhai Tu. The inverse variance–flatness relation in stochastic gradient descent is critical for finding flat minima. Proceedings of the National Academy of Sciences, 118(9):e2015617118, 2021.\n\nRong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape\n\ndesign. In International Conference on Learning Representations, 2018.\n\nIan J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network\n\noptimization problems. arXiv preprint arXiv:1412.6544, 2014.\n\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurIn International conference on acoustics, speech and signal processing,\n\nrent neural networks. 2013.\n\nRicardo Halla, Adeilson de Oliveira Souza, and Fábio José Pinheiro Sousa. Use of Artificial Neural Network for Analyzing the Contributions of Some Kinematic Parameters in the Polishing Process of Porcelain Tiles. Springer, 2022.\n\nMoritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231,\n\n2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\n\nnition. In Conference on Computer Vision and Pattern Recognition, 2016.\n\nSepp Hochreiter and Jürgen Schmidhuber. Simplifying neural nets by discovering flat minima.\n\nAdvances in Neural Information Processing Systems, 1994.\n\nSepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.\n\nRuinan Jin, Yu Xing, and Xingkang He. On the convergence of mSGD and AdaGrad for stochastic\n\noptimization. In International Conference on Learning Representations, 2022.\n\nBobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape local\n\nminima? In International Conference on Machine Learning, 2018.\n\nHendrik Anthony Kramers. Brownian motion in a field of force and the diffusion model of chemical\n\nreactions. Physica, 7(4):284–304, 1940.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification with deep convo-\n\nlutional neural networks. Communications of the ACM, 60(6):84–90, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n\n2015.\n\nGuo Lei, Cheng Dai-Zhan, and Feng De-Xing. Introduction to Control Theory: From Basic Con-\n\ncepts to Research Frontiers. Beijing: Science Press, 2005.\n\nYuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-\n\ntion. In Advances in Neural Information Processing Systems, 2017.\n\nWei Liu, Xin Liu, and Xiaojun Chen. Linearly constrained nonsmooth optimization for training\n\nautoencoders. SIAM Journal on Optimization, 32(3):1931–1957, 2022a.\n\nWei Liu, Xin Liu, and Xiaojun Chen. An inexact augmented Lagrangian algorithm for training leaky\n\nReLU neural network with group sparsity. Preprint arXiv:2205.05428, 2022b.\n\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 550(7676):354–359, 2017.\n\nYuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. In International Conference on Machine Learning, 2017.\n\nSharan Vaswani, Francis R. Bach, and Mark Schmidt. Fast and faster convergence of sgd for overparameterized models and an accelerated perceptron. In International Conference on Artificial Intelligence and Statistics, 2019.\n\nZhong-zhi Wang, Yun Dong, and Fangqing Ding. On almost sure convergence for sums of stochastic\n\nsequence. Communications in Statistics-Theory and Methods, 48(14):3609–3621, 2019.\n\nJingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On In International Conference on Machine\n\nthe noisy gradient descent that generalizes as SGD. Learning, 2019.\n\nLei Wu, Chao Ma, and E Weinan. How SGD selects the global minima in over-parameterized learning: A dynamical stability perspective. In Advances in Neural Information Processing Systems, 2018.\n\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine transarXiv preprint lation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016.\n\nZeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: In International Conference on\n\nStochastic gradient descent exponentially favors flat minima. Learning Representations, 2020.\n\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021.\n\nKai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees for one-hidden-layer neural networks. In International Conference on Machine Learning, 2017.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 COUNTER-EXAMPLE\n\nClaim A.1. The chain rule does not hold the Clarke subdifferential.\n\nProof. For a composite nonsmooth function, the chain rule may not hold at the nonsmooth point Liu et al. (2022b). We introduce an example as follows.\n\nConsider\n\nmin w1∈R,w2∈R,b1∈R,b2∈R\n\nf (w1, w2, b1, b2)\n\n:= ((w2σ (w1 + b1) + b2) + 1)2 + ((w2σ (2w1 + b1) + b2) − 1)2 .\n\n(10)\n\nLet w∗ 1, w∗ (w∗\n\n2 = 1, b∗ 1, b∗ 2, b∗\n\n1 = 0, w∗ 2), and\n\n1 = 0, b∗\n\n2 = 0, one can easily see that the SGD method will get stuck at\n\n∂f (w∗\n\n1, w∗ 1 + ε, w∗\n\n2, b∗ 2, b∗\n\n1, b∗ 1, b∗\n\n2) = (cid:8)(t, 0, s, 0)T : t ∈ [−4, 2], s ∈ [−2, 0](cid:9) , 2) = 5ε2 − 2ε + 2 < 2 = f (w∗\n\n1, w∗\n\n2, b∗\n\n1, b∗\n\nf (w∗\n\n2) for some small positive number ε .\n\nThen, observe that (w∗ see that (1, 2, −1, −1) is a global minimizer of equation 10, at which the function value is 0.\n\n2) is neither a local minimizer of equation 10. Moreover, one can\n\n1, w∗\n\n2, b∗\n\n1, b∗\n\nA.2 AUXILIARY LEMMAS\n\nLemma A.1. {Xn, Fn} that satisfies supn\n\n(Theorem 4.2.13, Lei et al. (2005)) Consider a Martingale difference column E(∥Xn+1∥2|Fn) < +∞ almost surely. Then it holds that\n\nβkXk = O\n\n(cid:16)(cid:112)\n\nSn ln\n\nn (cid:88)\n\nk=1\n\n1\n\n2 +η(Sn + e)\n\n(cid:17)\n\nalmost surely ,\n\n∀η > 0,\n\nk=1 β2 k.\n\nwhere Sn = (cid:80)n Lemma A.2. (Lemma 6 in Jin et al. (2022)) Suppose that {Xn} ∈ Rd is a non-negative sequence of random variables, then (cid:80)∞ n=0 Xn < +∞ holds almost surely if (cid:80)∞ Lemma A.3. (Wang et al., 2019) Suppose that {Xn} ∈ Rd is an L2 martingale difference sequence, and (Xn, Fn) is an adaptive process. Then it holds almost surely that (cid:80)∞\n\n(cid:1) < +∞.\n\nE (cid:0)Xn\n\nk=0 Xk < +∞ if\n\nn=0\n\n∞ (cid:88)\n\nn=1\n\nE(∥Xn∥2) < +∞,\n\nor\n\n∞ (cid:88)\n\nn=1\n\nE (cid:0)∥Xn∥2(cid:12)\n\n(cid:12)Fn−1\n\n(cid:1) < +∞ ,\n\nhappens almost surely.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA.3 PROOF OF LEMMA A.4.\n\nLemma A.4. Consider the SGD updates specified in equation 2 (or equation 2 with J ∗ = J ∗∗) and the MSE loss function equation 1. If Assumptions 2.1, 2.3 hold, then for any ε0 < min{1/2cM0, 1/4cK0(1 − p0)}, where normal sampling noise 2 can be seen as p0 = 0. Then for any θ1 ∈ Rd, the probability of θn diverge to the infinity is less than 1, i.e., P (θn → ∞) < 1.\n\nProof. We prove this Lemma by contradiction. We first assume P (θn → ∞) = 1, which means θn → ∞ almost surely. By the Lagrange’s mean value theorem, we have\n\ng(θn+1) − g(θn) = ̃∇g(θζn )T (θn+1 − θn) ,\n\nwhere ζn is a point between θn and θn+1. If ζn is a non-smooth point, then we can find at least one point in the set of ̃∇g(θζn). Therefore, we have\n\ng(θn+1) − g(θn) = ̃∇g(θζn)T (θn+1 − θn)\n\n= − ε0 ̃∇g(θn)T ̃∇g(θn, ξn) + (cid:0) ̃∇g(θζn ) − ̃∇g(θn)(cid:1)T (cid:13) ̃∇g(θζn ) − ̃∇g(θn)(cid:13) ≤ − ε0 ̃∇g(θn)T ̃∇g(θn, ξn) + (cid:13) ≤ − ε0 ̃∇g(θn)T ̃∇g(θn, ξn) + max{c, c · ε0∥ ̃∇g(θn, ξn)∥}ε0∥ ̃∇g(θn, ξn)∥ 0∥ ̃∇g(θn, ξn)∥2 . < − ε0 ̃∇g(θn)T ̃∇g(θn, ξn) + c · ε0∥ ̃∇g(θn, ξn)∥ + c · ε2\n\n(cid:13)∥θn+1 − θn∥\n\n(θn+1 − θn)\n\nThrough Assumption 2.3, we know that it hold Eξn ∥ ̃∇g(θn, ξn)∥2 ≤ M0∥ ∞. Then we take an expectation over the sampling noise, we have\n\n ̃∇g(θn)∥2 when θn →\n\nE (cid:0)g(θn+1)(cid:1) − E (cid:0)g(θn)(cid:1) < − ε0 E ∥ ̃∇g(θn)∥2 + c · ε0\n\n(cid:112)\n\nM0 E ∥ ̃∇g(θn)∥ + c · M0 · ε2\n\n0\n\nE ∥ ̃∇g(θn)∥2\n\n+ c(1 − p0)K0ε2\n\n< − (cid:0)ε0 − cM0ε2\n\n0\n\n0 + c(cid:112)(1 − p0)K0ε0 (cid:1) E ∥ ̃∇g(θn)∥2 + cε0\n\n(cid:112)\n\n+ c(cid:112)(1 − p0)K0ε0 .\n\nM0 E ∥ ̃∇g(θn)∥ + c(1 − p0)K0ε2\n\n0\n\n2 ε0 − cM0ε2\n\nSince 1 get P (∥ ̃∇g(θn)∥ > max{4c\n\n√\n\n0 > 0, and ∥ ̃∇g(θn)∥ > max{4c\n\nM0, 4c(cid:112)(1 − p0)K0}) → 1. This implies\n\n√\n\nM0, 4c(cid:112)(1 − p0)K0} when θn → ∞, we can\n\nWith this, we have\n\nE ∥ ̃∇g(θn)∥2 ≥ (cid:0) E ∥ ̃∇g(θn)∥(cid:1)2\n\nE (cid:0)g(θn+1)(cid:1) ≤ E (cid:0)g(θ1)(cid:1) − ˆk′\n\n1ε0\n\nn (cid:88)\n\nk=1\n\nE ∥ ̃∇g(θn)∥2 → −∞ ,\n\nwhich is impossible. We thus conclude that {θn} can not tend to infinity almost surely, i.e., P (θn → ∞) < 1.\n\nA.4 PROOF OF THEOREM 3.1.\n\nProof. For convenience, we abbreviate rθ∗ := r. Then we let\n\n(cid:40)(cid:18)\n\nl0 := min\n\nβθ∗ + 1 2r(r + 1)G(rθ∗ )\n\n0\n\nαθ∗ εr\n\n0\n\n(cid:19) r+1\n\nr−1\n\n(cid:41)\n\n, δθ∗\n\n,\n\nand construct a function\n\nR(θ) =\n\n \n\n\n\n∥θ − θ∗∥r+1, ∥θ − θ∗∥2, ˆk(∥θ − θ∗∥),\n\nif ∥θ − θ∗∥ ≤ max{1, δθ∗ } if ∥θ − θ∗∥ > ̄K0 if max{1, δθ∗ } < ∥θ − θ∗∥ ≤ ̄K0\n\n,\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nwhere ˆk(∥θ − θ∗∥) is the smooth connection between ∥θ − θ∗∥ (∥θ − θ∗∥ > ̄K0) and ∥θ − θ∗∥r+1 (∥θ − θ∗∥ ≤ max{1, δθ∗ }).\n\nThen through choosing feasible ˆk(θ − θ∗) and ˆK0, we can ensure that the Hessian matrix of R(θ) is bounded in Rd. Let the upper bound of the Hessian matrix be r(r + 1), i.e., xT Hθθx ≤ r(r + 1)∥x∥2 (∀x ∈ Rd, θ ∈ Rd).\n\nNext, we construct a set\n\nWe also define event A(l0) Lagrange’s mean value theorem, we obtain\n\nS(l0) = (cid:8)θ(cid:12)\n\n(cid:12)0 ≤ ∥θ − θ∗∥ < l0 n = {θn ∈ S(l0)} and the characteristic function I (l0)\n\n(cid:9) .\n\nn . Through the\n\nI (l0)\n\nn\n\n(cid:0)R(θn+1) − R(θn)(cid:1) = I (l0)\n\nn ∇R(θζn)T (θn+1 − θn) ,\n\nwhere θζn ∈ [θn+1, θn]. Note that\n\n∇R(θζn ) = ∇R(θn) + ∇R(θζn ) − ∇R(θn) ,\n\nand thus\n\nI (l0)\n\nn\n\n(cid:0)R(θn+1) − R(θn)(cid:1) ≤ −I (l0)\n\nn ∇R(θn)T vn + I (l0)\n\nn ∥∇R(θζn ) − ∇R(θn)∥∥θn+1 − θn∥ .\n\nHence, for any θ ∈ {θ|∥θ − θn∥ ≤ max{1, δθ∗ }} we have\n\n∇R(θ) = ∇(cid:0)∥θ − θ∗∥r+1(cid:1) = (r + 1)∥θ − θ∗∥r−1(θ − θ∗) .\n\nMoreover, if ∥θξn − θn∥ < max{1, δθ∗ }, we also have\n\n∥∇R(θζn ) − ∇R(θn)∥ ≤ r(r + 1)∥θn+1 − θn∥r ,\n\nand if ∥θξn − θn∥ ≥ max{l0, 1}, we have\n\n∥∇R(θζn ) − ∇R(θn)∥ ≤ r(r + 1)∥θζn − θn∥\n\n≤\n\n≤\n\nr(r + 1)\n\n∥θζn − θn∥r−1 ∥θζn − θn∥r\n\nr(r + 1) 1\n\n∥θn+1 − θn∥r .\n\nWith this, we have\n\nI (l0)\n\nn\n\n∇R(θζn ) − ∇R(θn)∥ ≤ r(r + 1)∥θn+1 − θn∥r = r(r + 1)∥vn∥r , (cid:0)R(θn+1) − R(θn)(cid:1) ≤ − I (l0) n R(θn) ≤ − I (l0) − (I (l0)\n\nn ∇R(θn)T vn + I (l0) n ∇R(θn)T vn + I (l0) n − I (l0)\n\nn r(r + 1)∥vn∥r+1 . n r(r + 1)∥vn∥r+1\n\nn+1)R(θn+1) .\n\nI (l0) n+1R(θn+1) − I (l0)\n\nTaking expectation of equation 11, we have\n\nE (cid:0)I (l0) (cid:16)\n\nn ∇R(θn)T vn I (l0)\n\nE (cid:0)∇R(θn)T vn\n\n(cid:1)\n\nn\n\n= E\n\n(cid:1)(cid:17)\n\n(cid:12) (cid:12)Fn\n\n(cid:16)\n\n= E\n\n= ε0 E\n\nn ε0 E (cid:0)∇R(θn)T ̃∇g(θn, ξn)(cid:1) + I (l0) I (l0) (cid:16)\n\nE (cid:0)∇R(θn)T ̃∇g(θn)(cid:1) .\n\nI (l0)\n\nn\n\nn ε0 E (cid:0)∇R(θn)T (cid:112)min{g(θn), K0}τnNn\n\n(11)\n\n(cid:1)(cid:17)\n\n(cid:1)(cid:12) (cid:12)Fn\n\nDefine ˆS to be the set of θ′, such that g(θ′) is not smooth. Then with Assumption 2.1, we have E\n\nθn∈ ˆS(h(θn)) = 0, where h is an arbitrary measurable function. Hence, when θn ∈ Rd/ ˆS,\n\nn ∇R(θn)T ̃∇g(θn) I (l0) n (r + 1)∥θn − θ∗∥r−1(θn − θ∗)T ̃∇g(θn) n (r + 1)∥θn − θ∗∥r−1αθ∗ ∥θn − θ∗∥r+1 = I (l0)\n\n= I (l0)\n\n≥ I (l0)\n\nn αθ∗ (r + 1)R\n\n2r\n\nr+1 (θn).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nTherefore, we have\n\nE (cid:0)I (l0)\n\nn ∇R(θn)T ̃∇g(θn)(cid:1) = E\n\nθn∈Rd/ ˆS\n\n(cid:0)I (l0)\n\nn ∇R(θn)T ̃∇g(θn)(cid:1)\n\n≥ E (cid:0)I (l0)\n\nn αθ∗ (r + 1)R\n\n2r\n\nr+1 (θn)(cid:1) ,\n\nand through Assumption 2.2, we get E (cid:0)I (l0)\n\nn r(r + 1)∥vn∥r+1(cid:1) (cid:16) E\n\nI (l0)\n\nn\n\n= r(r + 1)εr+1\n\n0\n\n+ r(r + 1)εr+1\n\n0\n\n+ r(r + 1)εr+1\n\n0\n\n(cid:16)\n\n(cid:16)\n\nE\n\nE\n\nI (l0)\n\nn\n\nI (l0)\n\nn\n\n= r(r + 1)εr+1\n\n0\n\n(cid:16)\n\nE\n\nI (l0)\n\nn\n\n+ r(r + 1)εr+1\n\n0\n\n(cid:16)\n\nE\n\nI (l0)\n\nn\n\n≤ r(r + 1)(2 − p0)εr+1\n\n0\n\n(cid:1)(cid:17)\n\nE (cid:0)∥ ̃∇g(θn, ξn)∥r+1(cid:12)\n\n(cid:12)Fn E (cid:0)∥(cid:112)min{g(θn), K0}τnNn∥r+1(cid:12) E (cid:0) ̃∇g(θn, ξn)T (cid:112)min{g(θn), K0}τnNn\n\n(cid:12)Fn\n\n(cid:1)(cid:17)\n\n(cid:1)(cid:17)\n\nE (cid:0)∥ ̃∇g(θn, ξn)∥r+1(cid:12)\n\n(cid:12)Fn E (cid:0)∥(cid:112)min{g(θn), K0}τnNn∥r+1(cid:12) E (cid:0)I (l0)\n\nn R(θn)(cid:1) ,\n\n(βθ∗ + 1)G(rθ∗ )\n\n0\n\n(cid:1)(cid:17)\n\n(cid:12)Fn\n\n(cid:1)(cid:17)\n\n(cid:12) (cid:12)Fn\n\n0\n\nwhere G(rθ∗ ) p0 = 0. Then, E (cid:0)I (l0)\n\nThat means\n\nHence,\n\nis defined in Claim 2.1, and results of equation 2 can be seen the situation which\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\nn R(θn)(cid:1) ≤ − αθ∗ ε0 E (cid:0)I (l0)\n\n2r\n\nr+1 (cid:1)\n\nn R(θn) + r(r + 1)(2 − p0)(βθ∗ + 1)εr+1 − E (cid:0)(I (l0)\n\nn+1)R(θn+1)(cid:1) .\n\nn − I (l0)\n\n0 G(rθ∗ )\n\n0\n\nE (cid:0)I (l0)\n\nn R\n\nr+1\n\n2 (θn)(cid:1)\n\nDue to θn ∈ S(l0), we know\n\n(cid:18)\n\nR(θn) <\n\nβθ∗ + 1\n\n2r(r + 1)(2 − p0)G(rθ∗ )\n\n0\n\nαθ∗ εr\n\n0\n\n(cid:19) r+1\n\nr−1\n\n.\n\nαθ∗ ε0I (l0)\n\nn R\n\n2r\n\nr+1 (θn) > 2r(r + 1)(2 − p0)αθ∗ εr+1\n\n0 G(rθ∗ )\n\n0\n\nI (l0) n R(θn) .\n\n≤ −\n\nE (cid:0)I (l0) n+1R(θn+1)(cid:1) − E (cid:0)I (l0) αθ∗ ε0 2\nn+1)R(θn+1)(cid:1), we observe that\n\nn R(θn)(cid:1) r+1 (θn)(cid:1) − E (cid:0)(I (l0)\n\nE (cid:0)I (l0)\n\nn R\n\n2r\n\nFor the term E (cid:0)(I (l0)\n\nn − I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) .\n\nE (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) = E (cid:0)(I (l0)\n\nn − I (l0)\n\nn I (l0)\n\nn+1)R(θn+1) − (I (l0)\n\nn+1 − I (l0)\n\nn I (l0)\n\nn )R(θn+1)(cid:1),\n\n(12)\n\nand\n\n(I (l0)\n\nn − I (l0) n+1 − I (l0)\n\nn I (l0) n I (l0)\n\nn+1)g(θn+1) ≥ l0(I (l0) n )g(θn+1) ≤ l0(I (l0)\n\nn − I (l0) n+1 − I (l0)\n\nn I (l0) n I (l0)\n\n(I (l0)\n\nn+1) ,\n\nn ) .\n\nTaking these into equation 12, we obtain\n\nE (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) ≥ E (cid:0)(I (l0) = l0 E (cid:0)I (l0)\n\nn − I (l0) n − I (l0)\n\nn+1)l0 − (I (l0) n I (l0) (cid:1) .\n\nn+1\n\nn+1 − I (l0)\n\nn I (l0)\n\nn )l0\n\n(cid:1)\n\nTaking equation 13 into equation 12, we have\n\nE (cid:0)I (l0)\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\nn R(θn)(cid:1) ≤ −\n\nαθ∗ ε0 2\n\n16\n\nE (cid:0)I (l0)\n\nn R2r(θn)(cid:1) − l0 E (cid:0)I (l0)\n\nn − I (l0)\n\nn+1\n\n(13)\n\n(cid:1) .\n\n(14)\n\nUnder review as a conference paper at ICLR 2023\n\nSumming equation 14 over n, we have\n\nE (cid:0)I (l0)\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\n1 R(θ1)(cid:1) ≤ −\n\nαθ∗ ε0 2\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (l0)\n\nk R\n\n2r\n\nr+1 (θn)(cid:1) − l0 E (cid:0)I (l0)\n\n1 − I (l0)\n\nn+1\n\n(cid:1) .\n\n(15)\n\nRearranging the equation, we have\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (l0)\n\nk R\n\n2r\n\nr+1 (θn)(cid:1) ≤\n\n2(l0 + g(θ1)) αθ∗ ε0\n\n< +∞ .\n\nNext we construct a subset of S(l0) as\n\nDefine event\n\nS(δ0,l0) = {θ(cid:12)\n\n(cid:12)0 < δ0 ≤ ∥θ − θ∗∥ < l0} .\n\nA(δ0,l0)\n\nn\n\n= {θn ∈ S(δ0,l0)}\n\nand the characteristic function be I (δ0,l0)\n\nn\n\n. Obviously, we have\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (δ0,l0)\n\nk\n\n2r\n\nr+1 (θk)(cid:1) <\n\nR\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (l0)\n\nk R\n\n2r\n\nr+1 (θk)(cid:1) ≤\n\n2(l0 + g(θ1)) αθ∗ ε0\n\n< +∞ .\n\nLet r0 := inf θ∈S(δ0,l0 ) R\n\n2r\n\nr+1 (θ) > 0, we have\n\nr0\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (δ0,l0)\n\nk\n\n(cid:1) <\n\n2(l0 + g(θ1)) αθ∗ ε0\n\n< +∞ ,\n\nthat is\n\n+∞ (cid:88)\n\nP (cid:0)θk ∈ S(δ0,l0)(cid:1) =\n\nk=1 Then we can obtain\n\n+∞ (cid:88)\n\nk=1\n\nE (cid:0)I (δ0,l0)\n\nk\n\n(cid:1) <\n\n2(l0 + g(θ1)) αθ∗ ε0r0\n\n< +∞ .\n\n(16)\n\nP (cid:0){θn} ∈ S(δ0,l0), i.o.(cid:1) = P\n\n(cid:18) +∞ (cid:92)\n\n+∞ (cid:91)\n\n(cid:0)θk ∈ S(δ0,l0)(cid:1)\n\n(cid:19)\n\nn=1\n\nk=n\n\n= lim\n\nn→+∞\n\nP\n\n(cid:0)θk ∈ S(δ0,l0)(cid:1)\n\n(cid:19)\n\n(cid:18) +∞ (cid:91)\n\nk=n\n\n≤ lim\n\nn→+∞\n\n+∞ (cid:88)\n\nk=n\n\nP (cid:0)θk ∈ S(δ0,l0)(cid:1) = 0 .\n\n(17)\n\n(18)\n\n(19)\n\nNote that equation 17 means the set S(δ0,l0) has no limit point of {θn} almost surely. Then if we use the SGD update rule equation 5 Since the noise is Gaussian, any θ ∈ Rd/J ∗ and for any k > 0, there is P (θn+k ∈ S(δ0,l0)|θn = θ) = ˆδ0 > 0. If we use SGD update rule equation 2, for any max positive invariant set D/J ∗, we know that there must exist a boundary set ∂D. Moreover, ∀θ′ ∈ ∂D, if θ′ ∈ Rd/D, then for any mini-batch Ci, we have ̃∇gCi(θ′) = 0. Otherwise we can find a sequence {θ′′ → θ′, /θ′′ ∈ D}, making the trajectories started from θ′′ close to the trajectory started from θ′. It forms a contradiction. Then due to J ∗∗ = J ∗, we know θ′ ∈ J ∗. That means D ∩ J ∗ ̸= ∅. If θ′ ∈ D, we can conclude all trajectories started from θ′ are a subset of ∂D. On the other hand, we can conclude ∂g is a close set. Through Heine ̆Borel theorem, it exists a finite open cover (cid:83)M n=1 On ⊃ ∂D, and every On holding an arbitrary small diameter. We let θ′ ∈ O1. Then we assign Tn as the lone time interval of one trajectory started from θ′ and back to Tn. If Tn → +∞, that means this trajectory must stay a infinity time in some Ok, that means exists a global optimum in Ok. Naturally, the trajectory will converge to this global optimum. If Tn is bounded, that means the trajectory will enter into O1 infinite times. Due to a mass of different mini-batch and the enough small diameter and f (θ) := P (θn+k ∈ Rd/D|θn = θ) = ˆδ0 > 0 is a continuous function, We get P (θn+k ∈ Rd/D|θn ∈ O1) = ˆδ0 > 0, it is contradiction about D is a\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\npositive invariant set. That means for any θ ∈ R/J ∗, either trajectories started from it will converge to some global optimum, either it has a positive probability to make sure it transfers to S(δ0,l0) after k steps. Then for any bounded set ˆS0 which has no intersection with J ∗, we first get rid of those points which will converge to J ∗. We know that f (θ) := P (θn+k ∈ S(δ0,l0)|θn = θ) = ˆδ0 > 0 is a continuous function. Then we can get for any bounded closed set ˆS0 which satisfied ˆS0 ∩ J ∗ = ∅, P (θn+k ∈ S(δ0,l0)|θn = θ) = ˆδ1 > 0. Then we aim to prove there is no limit point there is minθ∈ ˆS0 in ˆS0 almost surely by contradiction. We assume\n\n+∞ (cid:88)\n\nn=1\n\nP (cid:0)θn ∈ ˆS0\n\n(cid:1) = +∞ .\n\nThen,\n\n+∞ (cid:88)\n\nn=k+1\n\nP (θn ∈ S(δ0,l0)) =\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\nS(δ0,l0)\n\nPn(dθ)\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\nSRd\n\nP (θn+k ∈ S(δ0,l0)|θn = θ)Pn−k(dθ)\n\nP (θn+k ∈ S(δ0,l0)|θn = θ)Pn−k(dθ)\n\nPn−k(dθ) = ˆδ1\n\n+∞ (cid:88)\n\nn=1\n\nP (θn ∈ ˆS0)\n\n=\n\n≥\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\n+∞ (cid:88)\n\nˆS0\n\n(cid:90)\n\n≥ ˆδ1\n\nn=k+1\n\nˆS0\n\n= + ∞ .\n\nNote that this is in contradiction with equation 16 and thus (cid:80)+∞\n\nP (cid:0){θn} ∈ ˆS0, i.o.(cid:1) = P\n\n(cid:18) +∞ (cid:92)\n\n+∞ (cid:91)\n\nn=1\n\nk=n\n\nn=1 P (cid:0)θn ∈ ˆS0 (cid:19)\n\n(cid:0)θk ∈ ˆS0\n\n(cid:1)\n\n(cid:1) < +∞. Then,\n\n(20)\n\n= lim\n\nn→+∞\n\nP\n\n(cid:19)\n\n(cid:0)θk ∈ ˆS0\n\n(cid:1)\n\n(cid:18) +∞ (cid:91)\n\nk=n\n\n≤ lim\n\nn→+∞\n\n+∞ (cid:88)\n\nk=n\n\nP (cid:0)θk ∈ ˆS0\n\n(cid:1) = 0 .\n\nCombining equation 20 with equation 16, we can see that for any bounded set which does not include J ∗ = {θ|g(θ) = 0} has no limit point almost surely. This implies θn → J ∗ or θn → ∞. Since {{θn} is convergence} is a tail event. Then by the zero-one law, we know P ({θn} is convergence) = 0 or 1. That means {θn} either converges to J ∗ almost surely, or diverges to infinity almost surely. Through Lemma A.4, we know P (θn → ∞) < 1, thus {θn} can only converge to J ∗ almost surely.\n\nA.5 PROOF OF THEOREM 3.2\n\nProof. We define R(θ) = ∥θ − θ∗∥2, and a set\n\nS(l0) = {θ(cid:12)\n\n(cid:12)0 ≤ ∥θ − θ∗∥ < l0 := δθ∗ } .\n\nWe also define an event A(l0) mean value theorem, we have\n\nn = {θn ∈ S(l0)} and the characteristic function I (l0)\n\nn . By Lagrange’s\n\nI (l0)\n\nn\n\n(cid:0)R(θn+1) − R(θn)(cid:1) = I (l0)\n\nn ∇R(θζn)T (θn+1 − θn) ,\n\nwhere θζn ∈ [θn+1, θn].\n\nNote that ∇R(θζn ) = ∇R(θn) + ∇R(θζn ) − ∇R(θn), we have\n\nI (l0)\n\nn\n\n(cid:0)R(θn+1) − R(θn)(cid:1) ≤ −I (l0)\n\nn ∇R(θn)T vn + I (l0)\n\nn ∥∇R(θζn ) − ∇R(θn)∥∥θn+1 − θn∥ .\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nMoreover, we also have\n\n∥∇R(θζn) − ∇R(θn)∥ ≤ 2∥θn+1 − θn∥ = 2∥vn∥\n\nI (l0)\n\nn\n\nI (l0)\n\nn\n\n(cid:0)R(θn+1) − R(θn)(cid:1) ≤ − I (l0) (cid:0)R(θn+1) − R(θn)(cid:1) ≤ − I (l0) n R(θn) ≤ − I (l0) − (I (l0)\n\nn ∇R(θn)T vn + I (l0) n ∇R(θn)T vn + I (l0) n ∇R(θn)T vn + I (l0) n − I (l0)\n\nn+1)R(θn+1) .\n\nn 2∥vn∥2 n 2∥vn∥2 n 2∥vn∥2\n\nI (l0) n+1R(θn+1) − I (l0)\n\nTaking expectation of equation 21, we have\n\nE (cid:0)I (l0) (cid:16)\n\nn ∇R(θn)T vn I (l0)\n\nE (cid:0)∇R(θn)T vn\n\n(cid:1)\n\n= E\n\n(cid:1)(cid:17)\n\n(cid:12) (cid:12)Fn\n\nn (cid:16)\n\n= E\n\n= ε0 E\n\nn ε0 E (cid:0)∇R(θn)T ̃∇g(θn, ξn)(cid:1) + I (l0) I (l0) (cid:16)\n\nn ε0 E (cid:0)∇R(θn)T ̃∇g(θn)(cid:1) . I (l0)\n\nn ε0 E (cid:0)∇R(θn)T (cid:112)min{g(θn), K0}τnNn\n\n(21)\n\n(22)\n\n(cid:1)(cid:17)\n\n(cid:1)(cid:12) (cid:12)Fn\n\nWe define ˆS = {θ′| ̃∇g(θ) is not continue at θ′}. Then through Assumption 2.1, and note that E\nθn∈ ˆS(h(θn)) = 0, where h is an arbitrary measurable function, we have that the following when θn ∈ Rd/ ˆS.\n\nn ∇R(θn)T ̃∇g(θn) = 2I (l0) I (l0)\n\nn (θn − θ∗)T ̃∇g(θn) ≥ 2I (l0) n αθ∗ ∥θn − θ∗∥2 = 2I (l0)\n\nn αθ∗ ∥θn − θ∗∥2 n αθ∗ R(θn).\n\n≥ 2I (l0)\n\nTherefore, we have\n\nE (cid:0)I (l0)\n\nn ∇R(θn)T ̃∇g(θn)(cid:1) = E\n\nθn∈Rd/ ˆS ≥ 2 E (cid:0)I (l0)\n\n(cid:0)I (l0) n αθ∗ R(θn)(cid:1) .\n\nn ∇R(θn)T ̃∇g(θn)(cid:1)\n\nand through Assumption 2.2, we get\n\nE (cid:0)I (l0)\n\nn 2∥vn∥2(cid:1) = 2ε2\n\n0\n\n(cid:16)\n\nE\n\nI (l0)\n\nn\n\n(cid:1)(cid:17)\n\nE (cid:0)∥ ̃∇g(θn, ξn)∥2(cid:12)\n\n(cid:12)Fn E (cid:0)∥(cid:112)min{g(θn), K0}τnNn∥2(cid:12) E (cid:0) ̃∇g(θn, ξn)T (cid:112)min{g(θn), K0}τnNn (cid:1)(cid:17)\n\n(cid:12)Fn\n\n(cid:1)(cid:17)\n\nE (cid:0)∥ ̃∇g(θn, ξn)∥2(cid:12)\n\n(cid:12)Fn E (cid:0)∥(cid:112)min{g(θn), K0}τnNn∥2(cid:12)\n\n(cid:1)(cid:17)\n\n(cid:12)Fn\n\n+ 2ε2\n\n0\n\nE\n\n+ 4ε2\n\nE\n\n(cid:16)\n\n(cid:16)\n\nI (l0)\n\nn\n\nI (l0)\n\nn\n\n0 (cid:16)\n\n= 2ε2\n\n0\n\nE\n\nI (l0)\n\nn\n\n+ 2ε2\n\n0\n\nE\n\n(cid:16)\n\nI (l0)\n\nn\n\n(cid:1)(cid:17)\n\n(cid:12) (cid:12)Fn\n\n≤ 2(2 − p0)ε2\n\n0β2\n\nθ∗ E (cid:0)I (l0)\n\nn R(θn)(cid:1) ,\n\nwhere the situation of equation 2 can be seen as p0 = 0. Then we have\n\nE (cid:0)I (l0)\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\nn R(θn)(cid:1)\n\n≤ − cθ∗ ε0 E (cid:0)I (l0)\n\nn R(θn)(cid:1) + 2(2 − p0)ε2\n\n0β2\n\nθ∗ E (cid:0)I (l0)\n\nn R(θn)(cid:1) − E (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) ,\n\nand\n\nE (cid:0)I (l0)\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\nn R(θn)(cid:1) (cid:1) E (cid:0)I (l0)\n\n≤ − (cid:0)αθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\nn R(θn)(cid:1) − E (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) .\n\nFor the term E (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1), we first observe that\n\nE (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) = E (cid:0)(I (l0)\n\nn − I (l0)\n\nn I (l0)\n\nn+1)R(θn+1) − (I (l0)\n\nn+1 − I (l0)\n\nn I (l0)\n\nn )R(θn+1)(cid:1) ,\n\n(23)\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nand\n\n(I (l0)\n\nn − I (l0) n+1 − I (l0)\n\nn I (l0) n I (l0)\n\nn+1)g(θn+1) ≥ l0(I (l0) n )g(θn+1) ≤ l0(I (l0)\n\nn − I (l0) n+1 − I (l0)\n\nn I (l0) n I (l0)\n\n(I (l0)\n\nn+1) ,\n\nn ) .\n\nTaking these into equation 23, we have\n\nE (cid:0)(I (l0)\n\nn − I (l0)\n\nn+1)R(θn+1)(cid:1) ≥ E (cid:0)(I (l0) = l0 E (cid:0)I (l0)\n\nn − I (l0) n − I (l0)\n\nn I (l0) n+1)l0 − (I (l0) (cid:1) .\n\nn+1\n\nn+1 − I (l0)\n\nn I (l0)\n\nn )l0\n\n(cid:1)\n\n(24)\n\nSubstituting equation 24 into equation 23, we get\n\nE (cid:0)I (l0)\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\nn R(θn)(cid:1) (cid:1) E (cid:0)I (l0)\n\n≤ − (cid:0)αθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\nn R(θn)(cid:1) − l0 E (cid:0)I (l0)\n\nn − I (l0)\n\nn+1\n\n(cid:1) .\n\n(25)\n\nSumming equation 25 over n, we have\n\nE (cid:0)I (l0)\n\nn+1R(θn+1)(cid:1) − E (cid:0)I (l0)\n\n1 R(θ1)(cid:1)\n\n≤ − (cid:0)αθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\nn (cid:88)\n\n(cid:1)\n\nk=1\n\nE (cid:0)I (l0)\n\nk R(θn)(cid:1) − l0 E (cid:0)I (l0)\n\n1 − I (l0)\n\nn+1\n\n(cid:1) .\n\n(26)\n\nAs ε0 < αθ∗ /2(2 − p0)β2\n\nθ∗ , we have\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (l0)\n\nk R(θn)(cid:1) ≤\n\nNext, we construct a subset of S(l0) as\n\nl0 + g(θ1)\n\nαθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\n< +∞ .\n\nS(δ0,l0) = {θ(cid:12)\n\n(cid:12)0 < δ ≤ ∥θ − θ∗∥ < l0} .\n\nWe also define A(δ0,l0) have\n\nn\n\n= {θn ∈ S(δ0,l0)} and the characteristic function be I (δ0,l0)\n\nn\n\n. Notice that, we\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (δ0,l0)\n\nk\n\nR(θk)(cid:1) <\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (l0)\n\nk R(θk)(cid:1) ≤\n\nDenoter0 := inf θ∈S(δ0,l0 ) R(θ) > 0, then\n\nl0 + g(θ1)\n\nαθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\n< +∞ .\n\nr0\n\nn (cid:88)\n\nk=1\n\nE (cid:0)I (δ0,l0)\n\nk\n\n(cid:1) <\n\nl0 + g(θ1)\n\nαθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\n< +∞,\n\nthat is\n\n+∞ (cid:88)\n\nk=1\n\nP (cid:0)θk ∈ S(δ0,l0)(cid:1) =\n\n+∞ (cid:88)\n\nk=1\n\nE (cid:0)I (δ0,l0)\n\nk\n\n(cid:1) <\n\nWith this, we have\n\nl0 + g(θ1)\n\nαθ∗ ε0 − 2(2 − p0)ε2\n\n0β2\n\nθ∗\n\n< +∞.\n\n(27)\n\nP (cid:0){θn} ∈ S(δ0,l0), i.o.(cid:1) = P\n\n(cid:18) +∞ (cid:92)\n\n+∞ (cid:91)\n\n(cid:0)θk ∈ S(δ0,l0)(cid:1)\n\n(cid:19)\n\nn=1\n\nk=n\n\n= lim\n\nn→+∞\n\nP\n\n(cid:0)θk ∈ S(δ0,l0)(cid:1)\n\n(cid:19)\n\n(cid:18) +∞ (cid:91)\n\nk=n\n\n(28)\n\nP (cid:0)θk ∈ S(δ0,l0)(cid:1)\n\n+∞ (cid:88)\n\nk=n\n\n≤ lim\n\nn→+∞\n\n= 0 .\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nWe remark thatequation 28 implies the set S(δ0,l0) has no limit point of {θn} almost surely. Then if we use the SGD update rule equation 5, as the noise is Gaussian, for any θ ∈ Rd/J ∗ and any k > 0, there is P (θn+k ∈ S(δ0,l0)|θn = θ) = ˆδ0 > 0. If we use SGD update rule equation 2, for any max positive invariant set D/J ∗, we know that there must exist a boundary set ∂D. Moreover, ∀θ′ ∈ ∂D, if θ′ ∈ Rd/D, then for any mini-batch Ci, we have ̃∇gCi(θ′) = 0. Otherwise we can find a sequence {θ′′ → θ′, /θ′′ ∈ D}, making the trajectories started from θ′′ close to the trajectory started from θ′. It forms a contradiction. Then due to J ∗∗ = J ∗, we know θ′ ∈ J ∗. That means D ∩ J ∗ ̸= ∅. If θ′ ∈ D, we can conclude all trajectories started from θ′ are a subset of ∂D. On the other hand, we can conclude ∂g is a close set. Through Heine ̆Borel theorem, it exists a finite open cover (cid:83)M n=1 On ⊃ ∂D, and every On holding an arbitrary small diameter. We let θ′ ∈ O1. Then we assign Tn as the lone time interval of one trajectory started from θ′ and back to Tn. If Tn → +∞, that means this trajectory must stay a infinity time in some Ok, that means exists a global optimum in Ok. Naturally, the trajectory will converge to this global optimum. If Tn is bounded, that means the trajectory will enter into O1 infinite times. Due to a mass of different mini-batch and the enough small diameter and f (θ) := P (θn+k ∈ Rd/D|θn = θ) = ˆδ0 > 0 is a continuous function, We get P (θn+k ∈ Rd/D|θn ∈ O1) = ˆδ0 > 0, it is contradiction about D is a positive invariant set. That means for any θ ∈ R/J ∗, either trajectories started from it will converge to some global optimum, either it has a positive probability to make sure it transfers to S(δ0,l0) after k steps. Then for any bounded set ˆS0 which has no intersection with J ∗, we first get rid of those points which will converge to J ∗. We know that f (θ) := P (θn+k ∈ S(δ0,l0)|θn = θ) = ˆδ0 > 0 is a continuous function. Then we can get for any bounded closed set ˆS0 which satisfied ˆS0 ∩ J ∗ = ∅, there is minθ∈ ˆS0 P (θn+k ∈ S(δ0,l0)|θn = θ) = ˆδ1 > 0. Then we aim to prove there is no limit point in ˆS0 almost surely by contradiction. We assume\n\n+∞ (cid:88)\n\nn=1\n\nP (cid:0)θn ∈ ˆS0\n\n(cid:1) = +∞.\n\nThen we can get\n\n+∞ (cid:88)\n\nn=k+1\n\nP (θn ∈ S(δ0,l0)) =\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\nS(δ0,l0)\n\nPn(dθ)\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\nSRd\n\nP (θn+k ∈ S(δ0,l0)|θn = θ)Pn−k(dθ)\n\n=\n\n≥\n\n+∞ (cid:88)\n\n(cid:90)\n\nn=k+1\n\n+∞ (cid:88)\n\nˆS0\n\n(cid:90)\n\n≥ ˆδ1\n\nn=k+1\n\nˆS0\n\nP (θn+k ∈ S(δ0,l0)|θn = θ)Pn−k(dθ)\n\nPn−k(dθ) = ˆδ1\n\n+∞ (cid:88)\n\nn=1\n\nP (θn ∈ ˆS0)\n\n= + ∞ .\n\nThis is contradiction with equation 27, which implies\n\n+∞ (cid:88)\n\nn=1\n\nP (cid:0)θn ∈ ˆS0\n\n(cid:1) < +∞ .\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nHence, we can obtain\n\nP (cid:0){θn} ∈ ˆS0, i.o.(cid:1) = P\n\n(cid:18) +∞ (cid:92)\n\n+∞ (cid:91)\n\n(cid:19)\n\n(cid:0)θk ∈ ˆS0\n\n(cid:1)\n\nn=1\n\nk=n\n\n= lim\n\nn→+∞\n\nP\n\n(cid:19)\n\n(cid:0)θk ∈ ˆS0\n\n(cid:1)\n\n(cid:18) +∞ (cid:91)\n\nk=n\n\n+∞ (cid:88)\n\nk=n\n\n≤ lim\n\nn→+∞\n\n= 0 .\n\nP (cid:0)θk ∈ ˆS0\n\n(cid:1)\n\n(29)\n\nCombining equation 29 with equation 27, for any bounded set which does not include J ∗ = {θ|g(θ) = 0}, we can say that it has no limit point almost surely. That means θn → J ∗ or θn → ∞ almost surely. We know the event {θn is convergence} is a tail event. By zero-one law, we have P ({θn} is convergence) = 0 or 1. That means {g(θn)} either converges to J ∗ almost surely, or diverges to infinity almost surely. Through Lemma A.4, we know P (θn → ∞) < 1. That proves {θn} can only converge to J ∗ almost surely.\n\nA.6 PROOF OF THEOREM 3.3\n\nFirst we construct a function R(θ) = ∥θ − θ∗∥2. We can get that\n\nR(θn+1) − R(θn) = ∥θn+1 − θ∗∥2 − ∥θn − θ∗∥2 = (θn+1 − θn)T (θn+1 + θn − 2θ∗)\n\n= 2(θn − θ∗)T (θn+1 − θn) + ∥θn+1 − θn∥2 = −2(θn − θ∗)T vn + ∥vn∥2 = − 2(θn − θ∗)T (cid:0)ε0 ̃∇g(θn, ξn) + ε0\n\n(cid:1)\n\n(cid:112)min{g(θn), K0}τnNn (cid:13) 2\n(cid:13)\n\n.\n\n(cid:112)min{g(θn), K0}τnNn\n\n+ (cid:13)\n\n(cid:13)ε0 ̃∇g(θn, ξn) + ε0\n\n(cid:112)min{g(θn), K0}τnNn\n\n(30) (cid:1), we use the following trans-\n\n2(θn − θ∗)T (cid:0)ε0 ̃∇g(θn, ξn) + ε0\n\n(cid:1) + 2ε0 = 2ε0(θn − θ∗)T ̃∇g(θn) + 2ε0(θn − θ∗)T (cid:0) ̃∇g(θn, ξn) − ̃∇g(θn)(cid:1)\n\n(cid:112)min{g(θn), K0}τnNn\n\n(31)\n\nFor the term 2(θn − θ∗)T (cid:0)ε0 ̃∇g(θn, ξn) + ε0 formation:\n\nFor the term (cid:13)\n\n(cid:13)ε0 ̃∇g(θn, ξn) + ε0\n\n+ 2ε0\n\n(cid:112)min{g(θn), K0}τn(θn − θ∗)T Nn . (cid:112)min{g(θn), K0}τnNn\n\n(cid:13) 2\n(cid:13)\n\n, we can obtain\n\n= ε2\n\n= ε2\n\n(cid:13) (cid:13)ε0 ̃∇g(θn, ξn) + ε0 (cid:13) ̃∇g(θn, ξn)(cid:13) (cid:13) 2\n(cid:13) E\n\n(cid:16)(cid:13) (cid:13) ̃∇g(θn, ξn)(cid:13) (cid:13)\n\n0\n\n0\n\n(cid:112)min{g(θn), K0}τnNn\n\n(cid:13) 2\n(cid:13) (cid:112)min{g(θn), K0} ̃∇g(θn, ξn)T Nn + ε2 0τn (cid:17)\n\n+ ε2\n\n0p0 min{g(θn), K0} + ε2\n\n+ 2ε2 2(cid:12) (cid:12)Fn (cid:16)(cid:13) (cid:13) ̃∇g(θn, ξn)(cid:13) 2(cid:12) (cid:12)Fn (cid:13) (cid:112)min{g(θn), K0} ̃∇g(θn, ξn)T Nn (cid:16)(cid:13) (cid:13) (cid:13) ̃∇g(θn, ξn)(cid:13) (cid:13) ̃∇g(θn, ξn)(cid:13) 2\n(cid:13) (cid:13)\n\nnN 2\n\n− ε2\n\n+ ε2\n\n+ ε2\n\n0τ 2\n\nE\n\n(cid:17)\n\n0\n\n0\n\n0\n\nn min{g(θn), K0} − ε2\n\n+ 2ε2 0τn (cid:13) (cid:13) ̃∇g(θn)(cid:13) 2\n(cid:13)\n\n− ε2\n\n0\n\nE\n\n2(cid:12) (cid:12)Fn\n\n≥ ε2\n\n0\n\nnN 2\n\n0τ 2 (cid:13) (cid:13) ̃∇g(θn, ξn)(cid:13) 2\n(cid:13)\n\n+ 2ε2\n\n0τn\n\n(cid:112)min{g(θn), K0} ̃∇g(θn, ξn)T Nn .\n\nn min{g(θn), K0}\n\n0p0 min{g(θn), K0}\n\n(cid:17)\n\n(32)\n\nThen we construct a set\n\nS(ˆl0) = {θ|∥θ − θ∗∥ < l0 := δθ∗ }/{θ∗} . We also define event Ai,n = {θn0 ∈ S(ˆl0), n0 ∈ [i, n]}, and its characteristic function as Ii,n. We substitute equation 32 and equation 31 into equation 30, and multiple Ii,n, getting\n\nIi,n\n\n(cid:0)R(θn+1) − R(θn)(cid:1) ≥ (cid:0)2(2 − p0)ε2\n\n0α2\n\nθ∗ − ε0βθ∗\n\n(cid:1)Ii,nR(θn) + Ii,nζn,\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\nζn := 2ε0(θn − θ∗)T (cid:0) ̃∇g(θn, ξn) − ̃∇g(θn)(cid:1) + 2ε0 2(cid:12) (cid:12)Fn\n\n(cid:16)(cid:13) (cid:13) ̃∇g(θn, ξn)(cid:13) (cid:13)\n\n(cid:13) ̃∇g(θn, ξn)(cid:13) (cid:13) 2\n(cid:13)\n\n− ε2\n\n+ ε2\n\nE\n\n0\n\n0\n\n(cid:112)min{g(θn), K0}τn(θn − θ∗)T Nn\n\n(cid:17)\n\n+ ε2\n\n0τ 2\n\nnN 2\n\nn min{g(θn), K0}\n\n− ε2\n\n0p0 min{g(θn), K0}\n\nis a Martingale difference. Denote ˆp0 := (cid:0)R(θn+1) − R(θn)(cid:1) ≥ (cid:0)2(2 − p0)ε2 have\n\n0α2\n\nθ∗ − ε0βθ∗\n\n(33)\n\n(cid:1), we\n\nIi,n+1R(θn+1) − Ii,nR(θn) ≥ ˆp0Ii,nR(θn) + Ii,n\n\nˆζn − R(θn+1)(Ii,n − Ii,n+1) .\n\nThen,\n\nE (cid:0)Ii,n+1R(θn+1)(cid:1) − E (cid:0)Ii,nR(θn)(cid:1) ≥ ˆp0 E (cid:0)Ii,nR(θn)(cid:1) − E (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) ,\n\nwhich implies\n\nE (cid:0)Ii,n+1R(θn+1)(cid:1) ≥\n\n(cid:32)\n\n1 + ˆp0 −\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n(cid:33)\n\nE (cid:0)Ii,nR(θn)(cid:1) .\n\nAssuming\n\nwe have\n\nlim sup n→+∞\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n< ˆp0 ,\n\nNote that this contradicted the E (cid:0)Ii,n+1R(θn+1)(cid:1) ≤ ˆl0. Hence,\n\nE (cid:0)Ii,n+1R(θn+1)(cid:1) → +∞ .\n\nlim sup n→+∞\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n≥ ˆp0.\n\n(34)\n\nDefine an event Ai,+∞ := {θn0 ∈ S(ˆl0), n0 ≥ i}, and its characteristic function as Ii,+∞. We next prove P\n\nlimn→+∞ Ii,+∞R(θn) = 0\n\n= 0.\n\n(cid:17)\n\n(cid:16)\n\nWe assume P\n\n(cid:16)\n\nlimn→+∞ Ii,+∞R(θn) = 0\n\n(cid:17)\n\n= 1, and we can get P\n\n(cid:16)\n\nlimn→+∞ Ii,nR(θn) = 0\n\n(cid:17)\n\n=\n\n1. That means for any ε′ Then we get\n\n0 > 0, P\n\n(cid:16)\n\nIi,nR(θn) > ε′\n\n0\n\n(cid:17)\n\n→ 0, concluding P\n\n(cid:16)\n\nIi,nR(θn) ≤ ε′\n\n0\n\n(cid:17)\n\n→ 1.\n\nlim sup n→+∞\n\nE (cid:0)R(θn+1)(Ii,n − Ii,n+1)(cid:1) E (cid:0)Ii,nR(θn)(cid:1)\n\n= lim sup n→+∞\n\n(cid:82)\n\n(cid:82)\n\n= lim sup n→+∞\n\n<\n\nˆp0 2\n\n.\n\n(cid:82) E(R(θn+1) > ˆl0|θ = θ)Pi,n(dθ) R(θ)Pi,n(dθ) + (cid:82)\n\nR(θ)Pi,n(dθ)\n\nR(θ)>ε′ 0\n\nR(θ)ε′ 0\n\nR(θ)≤ε′ 0\n\nE(R(θn+1) > ˆl0|θ = θ)Pi,n(dθ) (cid:82)\n\nR(θ)Pi,n(dθ)\n\nR(θ)≤ε′ 0\n\nNote that this contradicted equation 34, which implies P Through inspecting the event {θn → θ∗}, we can get\n\n(cid:16)\n\nlimn→+∞ Ii,+∞R(θn) = 0\n\n(cid:17)\n\n= 0.\n\n{θn → θ∗} ⊂\n\n(cid:41)\n\nAi,+∞\n\n.\n\n(cid:40)+∞ (cid:91)\n\ni=1\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nThat means\n\nP (cid:0)θn → θ∗(cid:1) = P\n\n(cid:32)\n\n{θn → θ∗}\n\n(cid:40) +∞ (cid:91)\n\n(cid:92)\n\n(cid:41)(cid:33)\n\nAi,+∞\n\n(cid:32) +∞ (cid:91)\n\n{θn → θ∗}\n\nm=1\n\n(cid:33)\n\n(cid:92)\n\nAi,+∞\n\n= P\n\n= P\n\ni=1 (cid:32) +∞ (cid:91)\n\n(cid:110)\n\nIi,+∞R(θn) = 0\n\n(cid:33)\n\n(cid:111)\n\nlim n→+∞\n\nlim n→+∞\n\nIi,+∞R(θn) = 0\n\n(cid:17)\n\ni=1\n\n(cid:16)\n\nP\n\n≤\n\n+∞ (cid:88)\n\ni=1\n\n= 0 .\n\nA.7 PROOF OF THEOREM 3.4.\n\n∞ as {θ∗\n\nFirst we order J ∗ is smooth at θ and θ ∈ U (θ∗, δθ∗ )/{θ∗}). That means for any θ∗ inf θi̸=θj ∥θ∗ at most infinite {θ∗ bounded. Then we construct a function ̄R(θ) as follow:\n\ni }. Then Assumption 2.2 implies that ∀ θ∗ ∈ J ∗, there is ∥ ̃∇g(θ)∥ > 0 (g j ∥ ≥ ) = ∅. Furthermore, it means that there are\n\ni }. We assign this number as m. Due lim inf θ→+∞ ∥ ̃∇g∥ > 0, we know {δθ∗\n\nj ∥ := ˆδ0 ̸= 0 and U (θ∗\n\nj ∈ J ∗, there is ∥θ∗\n\n) ∩ U (θ∗\n\ni ̸= θ∗\n\ni − θ∗\n\ni − θ∗\n\ni , δθ∗\n\nj , δθ∗\n\n} is\n\nj\n\ni\n\ni\n\nThen we try to prove that there exists a function ˆR(θ) satisfies:\n\n ̄Rθ∗\n\ni\n\n(θ) = ∥θ − θ∗\n\ni ∥rθ∗\n\ni\n\n+1.\n\n1. For any θ ∈ Rd, there exist Hθθ such that θT Hθθ( ˆR)θ ≤ (cid:0) maxθ∗ +1, when θ near the θ∗ 2. ˆR(θ) = ∥θ − θ∗ i . 3. ˆR(θ) is bounded.\n\ni ∥rθ∗\n\ni\n\ni ∈J ∗\n\n∞\n\nrθ∗\n\ni (rθ∗\n\ni + 1)(cid:1)∥θ∥2.\n\nWe define indicator functions\n\nˆI (ri) θ∗ i\n\n:=\n\n(cid:26)1, 0,\n\nif ∥θ − θ∗∥ ≤ ri if ∥θ − θ∗∥ > ri\n\n,\n\nwhere ri is an undetermined coefficient. Clearly, function ˆI (ri) function fθ∗ ̄δ0 > 0, we can always find\n\n(x) = xrθ∗\n\n+1, (0 < x < ri) about the independent variable ∥θ − θ∗\n\nθ∗ i\n\ni\n\ni\n\n ̄Rθ∗\n\ni\n\n(θ) can be seen as an unary\n\ni ∥. Then for any\n\nrθ∗ (x) = r i\ni\n\nhθ∗\n\ni\n\n+1\n\n+\n\n(rθ∗\n\ni\n\n2rθ∗ + 1)2r i\ni\n\n2\n\n,\n\nto ensure there is a smooth connection (a parabola) between fθ∗ entirety after adding the smooth connection between fθ∗ fied j′′(x) < 1 and the connection point on hθ∗ hθ∗ ri := h−1 θ∗ i\n{U (θ∗\n\n(x) be an arbitrary constant value M , for different rθ∗\n\n(M ). Take K0 := minθ∗\n\n(K 0))} do not intersect. Then\n\nδθ∗ i\nθ∗ i\n\ni , ˆri(h−1\n\n ̄Rθ∗\n\ni ∈J ∗\n\n{ ˆI\n\n∞\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\n(x) and hθ∗\n\ni\n\nθ∗ i\n\n(x) and hθ∗ (x) as jθ∗\n\ni\n\n(x). Denote this (x), jθ∗ (x) satis-\n\n(x) is ˆri(ri) := ri + (rθ∗\n\n. Then let , we can always get an inverse solution\n\ni\n\ni\n\ni\n\nrθ∗ + 1)r i\ni\n\n(θ), 1}, there must exists K 0 < K0, such that sets\n\nˆR(θ) :=\n\n \n\n(cid:80)m\n\ni=1\n\n\n\nK 0,\n\n(K0)))\n\n(ˆri(h−1 θ∗ ˆI i\nθ∗ i\n\njθ∗\n\ni\n\n(∥θ − θ∗\n\ni ∥),\n\nif θ ∈ (cid:83)m others\n\ni=1 U (θ∗\n\ni , ˆri(h−1\n\nθ∗ i\n\n(K 0))\n\n,\n\n(35)\n\nis what we need. We next discuss this problem case by case according to the value of ˆr.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nThe first case is ˆr = 1 (from here to equation 38), we define an event\n\nA(ˆl0)\n\nn,θ∗ i\n\n= {θn ∈ U (θ∗\n\ni , h−1\n\nθ∗ i\n\n(K 0)} ,\n\nand the characteristic function be I (ˆl0)\n\nn,θi\n\n. Then we can get that\n\nI (ˆl0)\n\nn,θ∗ i\n\n(cid:0) ˆR(θn+1) − ˆR(θn)(cid:1) ≤ − In,θ∗\n\ni\n\nˆk1ε0 2\nk0\n\n∥ ̃∇ ˆR(θn)∥2 + ˆζn\n\nˆk1ε0 2\n\nˆR(θn) + I (ˆl0)\n\nn,θ∗ i\n\nˆζn,\n\n≤ −I (ˆl0)\n\nn,θ∗ i\n\nwhere {ˆζn} is a Martingale difference sequence defined as\n\nˆζn := ε0∥ ̃∇ ˆR(θn)∥2 − ̃∇ ˆR(θn)T vn + 2M0∥vn∥2 − 2M0 E(∥vn∥2|Fn) ,\n\nwhere k0, ˆk1 are two constants. We also define I (−ˆl0)\n\nn\n\n:= 1 − (cid:80)m\n\ni=1 I (ˆl0)\n\nn,θ∗ i\n\n, and obtain\n\nI (−ˆl0)\n\nn\n\n(cid:0) ˆR(θn+1) − ˆR(θn)(cid:1) ≤ I (−ˆl0)\n\nn K 0 ≤ I (−ˆl0)\n\nn\n\nˆR(θn)\n\nK 0 ˆR(θn)\n\n≤ I (−ˆl0)\n\nn\n\nˆR(θn)\n\n1rθ∗\n\ni\n\n+1 +\n\n(rθ∗ i\n\n+1)21\n\n2rθ∗ i\n\n2\n\n1\n\n≤ 3I (−ˆl0)\n\nn\n\nˆR(θn) .\n\nThrough calculating the sum of equation 36, equation 37, we obtain\n\n(36)\n\n(37)\n\nˆR(θn+1) − ˆR(θn) ≤ −\n\nk0\n\nE (cid:0) ˆR(θn+1)(cid:12)\n\n(cid:12)Fn\n\n(cid:1) ≤\n\n(cid:18)\n\n1 −\n\nˆk1ε0 2\nk0\n\nˆk1ε0 2\n\nˆR(θn) + 3I (−ˆl0)\n\nn\n\nˆR(θn) + ˆζ ′\n\nn ,\n\n+ 3I (−ˆl0)\n\nn\n\n(cid:19)\n\nˆR(θn) ,\n\nwhere ˆζ ′\n\ni=1 I (ˆl0)\n\nn,θ∗ i\n\nn := (cid:80)m (cid:32)\n\nˆζn. Denote k′ := k0\n\nˆk1ε0/2, we get\n\nE\n\nˆR(θn+1)\n\n(cid:81)n\n\nk=1\n\n(cid:0)1 − k′ε0 + 3I (−ˆl0)\n\nk\n\n(cid:1)\n\n(cid:33)\n\nFn\n\n≤\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nˆR(θn)\n\n(cid:81)n−1 k=1\n\n(cid:0)1 − k′ε0 + 3I (−ˆl0)\n\nk\n\n.\n\n(cid:1)\n\nThrough the upper martingale convergence theorem, we get\n\nˆR(θn) = O\n\n(cid:32) n−1 (cid:89)\n\n(cid:16)\n\nk=1\n\n1 − k′ε0 + 3I (−ˆl0)\n\nk\n\n(cid:33)\n\n(cid:17)\n\nalmost surely. By Theorem 3.1, we also (cid:80)+∞\n\nk=1 I (−ˆl0)\n\nk\n\n< +∞ almost surely, which means\n\nalmost surely. Denote\n\nwe have\n\nˆR(θn) = O\n\n(cid:16)(cid:0)1 − k′ε0\n\n(cid:1)n(cid:17)\n\np0 := 1 − k′ε0 < 1 ,\n\nˆR(θn) = O(cid:0)pn\n\n0\n\n(cid:1) almost surely .\n\n(38)\n\nThe second case is when ˆr > 1. Let\n\nˆl0 :=\n\nmin\n\n1≤i≤m, rθ∗ i\n\n>1\n\n(cid:40)\n\n(cid:40)(cid:18)\n\nmin\n\nβθ∗ + 1 2r(r + 1)G(rθ∗ )\n\n0\n\nαθ∗ εr\n\n0\n\n(cid:19)\n\n+1\n\n−1\n\nrθ∗ i\nrθ∗ i\n\n, δθ∗\n\ni\n\n, h−1 θ∗ i\n\n(K 0),\n\n(cid:41)(cid:41)\n\n,\n\nThen we construct a set\n\nS(ˆl0)\n\nθ∗ i\n\n= {θ(cid:12)\n\n(cid:12)0 ≤ ∥θ − θ∗\n\ni ∥ < ˆl0 .\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nWe also define event A(ˆl0) get that\n\nn,θ∗ i\n\n= {θn ∈ S(ˆl0)} and let the characteristic function be I (ˆl0)\n\nn,θi\n\n. Then we can\n\nn,θ∗ i\n\nI (ˆl0)\n\n(cid:0) ˆR(θn+1) − ˆR(θn)(cid:1) ≤ − In,θ∗\n\nˆk1ε0 2\nˆk1ε0 k0 2\nwhere {ˆζn} is a Martingale difference sequence defined as\n\n≤ − I (ˆl0)\n\nn,θ∗ i\n\ni\n\n∥ ̃∇ ˆR(θn)∥2 + ˆζn\n\nˆR\n\nˆr+1\n\n2 (θn) + I (ˆl0)\n\nn,θ∗ i\n\n(39)\n\nˆζn ,\n\nˆζn := ε0∥ ̃∇ ˆR(θn)∥2 − ̃∇ ˆR(θn)T vn + 2M0∥vn∥r+1 − 2M0 E(∥vn∥r+1|Fn) , := 1 − (cid:80)m\n\nand k0, ˆk1 are two constants. Define I (−ˆl0)\n\n, we get\n\nn\n\nI (−ˆl0)\n\nn\n\n(cid:0) ˆR(θn+1) − ˆR(θn)(cid:1) ≤ I (−ˆl0)\n\nˆR\n\nˆr+1\n\n2 (θn),\n\n(40)\n\nwhere ˆa0 is a constant. Through calculating the sum of equation 39 and equation 40, we get\n\ni=1 I (ˆl0) n K 0 ≤ ˆa0I (−ˆl0)\n\nn,θ∗ i\n\nn\n\nˆR(θn+1) − ˆR(θn) ≤ −\n\nˆR\n\nˆr+1\n\n2 (θn) + I (−ˆl0)\n\nn\n\nˆa0 ˆR\n\nˆr+1\n\n2 + ˆζ ′\n\nn,\n\nn := (cid:80)m\n\ni=1 I (ˆl0)\n\nn,θ∗ i\n\nk0\n\nˆk1ε0 2\nˆζn. We also have (cid:32)\n\nˆR(θn+1) ≤ ˆR(θn)\n\n1 − k′ ˆR\n\nˆr−1\n\n2 (θn) + I (−ˆl0)\n\nn\n\nˆa0 ˆR\n\nˆr−1\n\n2 +\n\n(cid:33)\n\n.\n\nn\n\nˆζ ′ ˆR(θn)\n\nwhere ˆζ ′\n\nThis\n\nˆR\n\n1−ˆr\n\n2 (θn+1) ≥ ˆR\n\n1−ˆr\n\n2 (θn)\n\n(cid:32)\n\n1 − k′ ˆR\n\nˆr−1\n\n2 (θn) + I (−ˆl0)\n\nn\n\nˆa0 ˆR\n\nˆr−1\n\n2 (θn) +\n\n(cid:33) 1−ˆr\n\n2\n\n.\n\nˆζn ˆR(θn)\n\nUsing the inequalities (1 + x)r0 ≥ 1 + r0x, (r0 < 0), we have\n\nˆR\n\n1−ˆr\n\n2 (θn+1) ≥ ˆR\n\n1−ˆr\n\n2 (θn) +\n\nk′(ˆr − 1) 2\n\n+\n\n(1 − r) 2\n\nI (−ˆl0)\n\nn\n\nˆa0 +\n\n(1 − r)ˆζn 2 ˆR 2 (θn)\n\nˆr+1\n\n.\n\nSumming this over n, we have\n\nˆR\n\n1−ˆr\n\n2 (θn+1) ≥ ˆR\n\n1−ˆr\n\n2 (θ1) +\n\nk′(ˆr − 1) 2\n\nn + (1 − ˆr)ˆa0\n\nn (cid:88)\n\nk=1\n\nI (−ˆl0)\n\nk\n\n+\n\nn (cid:88)\n\nk=1\n\n(1 − ˆr)ˆζk 2 ˆR 2 (θk)\n\nˆr+1\n\n.\n\nNote that (cid:80)+∞\n\nk=1 I (−ˆl0)\n\nk\n\n< +∞ almost surely, thus we have\n\nˆR\n\n1−ˆr\n\n2 (θn+1) ≥ Ω(n) +\n\nn (cid:88)\n\nk=1\n\n(1 − ˆr)ˆζk 2 ˆR 2 (θk)\n\nˆr+1\n\n, almost surely .\n\nˆζ ′\n\nn :=\n\n(1 − r)ˆζn 2 ˆR 2 (θn)\n\nˆr+1\n\n.\n\nDenote\n\nClearly,\n\nE (cid:0)∥ ˆζ ′\n\nn∥2|Fn\n\n(cid:1) =\n\nsup n\n\n(r − 1)2 4\n\nE\n\nsup n\n\n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nˆζk 2 (θk)\n\nˆr+1\n\nˆR\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:33)\n\nFn\n\n< +∞almost surely .\n\nBy Lemma A.1, we have\n\nThen,\n\nwhich implies\n\nn (cid:88)\n\nk=1\n\n(1 − ˆr)ˆζk 2 ˆR 2 (θk)\n\nˆr+1\n\n√\n\n= O(\n\nn ln(n))almost surely .\n\nˆR\n\n1−ˆr\n\n2 (θn) ≥ Ω(n)almost surely ,\n\ng(θn) = O(cid:0) ˆR(θn)(cid:1) = O(cid:0)n− 2\n\nˆr−1 (cid:1)almost surely .\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nA.8 PROOF OF COROLLARY 3.1\n\nWhen the loss function g(θn) attains the ε′ accuracy, according to Theorem 3.4,the overall number of SGD iteration is\n\n(cid:40)\n\nn=\n\nO(cid:0) log( 1 O(cid:0)( 1 ε′ )\n\nε′ )(cid:1) 2 (cid:1)\n\nr−1\n\nalmost surely , almost surely ,\n\nif ˆr = 1 if ˆr > 1 .\n\nThen we consider the computational time of a single step of SGD. Generally, the main timeconsuming part of one step is computing the gradient of loss function on a batch of datasets, which can be decomposed into computing N0 times of numerical differentiation, where the N0 is the size of the dataset. We assume time consumed of computing a function value is O(cid:0)1(cid:1). When a specific numerical differentiation scheme is given, such as ∂f (θ(1),··· ,θ(d),x) f (θ(1)\n\n|θ=θ0 ≈ , it’s obviously the computation time of numerical gradient is O(cid:0)d(cid:1).\n\n0 ,x)−f (θ0,x)\n\n0 ,··· ,θ(i)\n\n∂θi\n\n0 +h,··· ,θ(d) h\n\nIn summary, the whole computation time is\n\n(cid:40)\n\nO(cid:0)N0d · log( 1 O(cid:0)N0d · ( 1\n\nε′ )(cid:1) 2 (cid:1)\n\nr−1\n\nε′ )\n\nwhich is bounded by a polynomial time.\n\nalmost surely , almost surely ,\n\nif ˆr = 1 if ˆr > 1 ,\n\n27",
    "reference": "# Summary Of The Paper\n\nThis paper provides theoretical results for the asymptotic convergence of SGD algorithm under an over-parameterized setting. It shows a set of assumptions that can guarantee the global convergence of SGD almost surely in some non-convex setting.\n\n# Strength And Weaknesses\n\nThe strengths of this paper are the theoretical results for the global convergence of SGD almost surely. The authors consider the regular sampling scheme and propose a new scheme (sampling noise with global stable guarantee). They prove the asymptotic convergence for SGD under a set of assumptions. \n\nThe weaknesses of this paper are: \n- This paper did not explain the intuition why SGD converge globally very well. The assumptions are not clearly motivated. The authors should explain why they have two set of assumptions on the gradient/ sample gradient of g. One is Assumption 2.1 part 4, line 2 where there is a lower bound on the liminf of gradient, the other is Assumption 2.2 where we put an upper bound on the sample gradient. Please add a discussion why the theory needs these assumptions and make sure they do not contradict each other. In addition, Assumption 2.3 is not natural when it asks that the constants $c_\\theta, \\hat{c}_\\theta$ are bounded away from 0 and by a constant of $\\theta$. \n- The presentation of this paper is poor. There are many notations and variables that were mentioned before the authors define them in the draft. For example: $M_0$ and $a$ were referred in Assumption 2.1 but only defined until 2.2, $\\tilde{\\nabla} g$ has no definition, 'global stable guarantee' was referred before the explanation,... Most of the time, the sketch proofs are confusing and they did not help to understand the thought process to prove the theorems. \n\nQuestion: In Theorem 3.4, what is 'the variant of Assumption 2.3 described immediately preceding this statement'?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe results of this paper seem to be new and the approach is different from prior work. However, its clarity and presentation is not good.\n\n# Summary Of The Review\n\nAlthough this paper show interesting results, I am not fully convinced by the assumptions and the intuition/reasoning behind the proofs.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\nNot applicable"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nCHARACTERIZING NEURAL REPRESENTATION OF COGNITIVELY-INSPIRED RL AGENTS DURING AN EVIDENCE ACCUMULATION TASK\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nEvidence accumulation is thought to be fundamental for decision-making in humans and other mammals. It has been extensively studied in neuroscience and cognitive science with the goal of explaining how sensory information is sequentially sampled until sufficient evidence has accumulated to favor one decision over others. Neuroscience studies suggest that the hippocampus encodes a lowdimensional ordered representation of evidence through sequential neural activity. Cognitive modelers have proposed a mechanism by which such sequential activity could emerge through the modulation of recurrent weights with a change in the amount of evidence. This gives rise to neurons tuned to a specific magnitude of evidence which resemble neurons recorded in the hippocampus. Here we integrated a cognitive science model inside a Reinforcement Learning (RL) agent and trained the agent to perform a simple evidence accumulation tasks inspired by the behavioral experiments on animals. We compared the agent’s performance with the performance of agents equipped with GRUs and RNNs. We found that the agent based on a cognitive model was able to learn faster and generalize better while having significantly fewer parameters. We also compared the emergent neural activity across agents and found that in some cases, GRU-based agents developed similar neural representations to agents based on a cognitive model. This study illustrates how integrating cognitive models and artificial neural networks can lead to brain-like neural representations that can improve learning.\n\n1\n\nINTRODUCTION\n\nConverging evidence from cognitive science and neuroscience suggests that the brain represents physical and abstract variables in a structured form, as mental or cognitive maps. These maps are thought to play an essential role in learning and reasoning (Tolman, 1948; Ekstrom & Ranganath, 2018; Behrens et al., 2018). Cognitive maps are characterized by neurons that activate sequentially as a function of the magnitude of the variable they encode. For instance, neurons called place cells activate sequentially as a function of spatial distance from some landmark (Moser et al., 2015; Muller, 1996; Sheehan et al., 2021). Similarly, time cells activate sequentially as a function of elapsed time from some event (Pastalkova et al., 2008; MacDonald et al., 2011; Cruzado et al., 2020; Salz et al., 2016).\n\nSimilar sequential activity has also been observed for sound frequency (Aronov et al., 2017), probability (Knudsen & Wallis, 2021) and accumulated evidence (Nieh et al., 2021; Morcos & Harvey, 2016b). For example, in the “accumulating towers task” Nieh et al. (2021) trained mice to move along a virtual track and observe objects (towers) on the left- and right-hand sides. When mice arrived at the end of the track, to receive a reward they had to turn left or right, depending on which side had more towers. The difference in the number of towers here corresponds to the amount of evidence for turning left vs. turning right. Nieh et al. (2021) recorded activity of hundreds of individual neurons from mice hippocampus, part of the brain commonly thought to play a key role in navigation in physical and abstract spaces (Bures et al., 1997; Eichenbaum, 2014; Moser et al., 2015). The results indicated the existence of cells tuned to a particular difference in the number of towers, such that a population of neurons tiles the entire evidence axis (Nieh et al., 2021) (see also Morcos &\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nHarvey (2016b)). This provides valuable insight into how abstract variables are represented in the brain.\n\nCognitive scientists have developed elaborate models of evidence accumulation to explain the response time in a variety of behavioral tasks (Laming, 1968; Link, 1975; Ratcliff, 1978). These models hypothesize that the brain contains an internal variable that represents the progress towards the decision. A neural-level cognitive model proposed that the brain could implement this process using a framework based on the Laplace transform (Howard et al., 2018). The Laplace framework gives rise to map-like representations and it has been successful in describing the emergence of sequentially activated time cells (Shankar & Howard, 2012) and place cells (Howard et al., 2014; Howard & Hasselmo, 2020).\n\nArtificial neural networks (ANNs) are commonly thought to have a distributed representation that does not have a map-like structure. While ANNs excel in many domains, they still struggle at many tasks that humans find relatively simple. Unlike humans, ANNs typically require a large number of training examples and fail to generalize to examples that are outside the training distribution (Bengio, 2017; LeVine, 2017; Marcus, 2020). Using cognitive models informed by neural data as an inductive bias for ANNs is an important direction that can help not only advance the current AI systems but also improve our understanding of cognitive mechanisms in the brain.\n\nHere we integrate the Laplace framework into reinforcement learning (RL) agents. The Laplace framework is based on recurrent neurons with analytically computed weights. We use the Laplace domain to generate a map-like representation of the amount of evidence. This representation is then fed into a trainable RL module based on the A2C architecture (Mnih et al., 2016). We compare map-based agents to standard RL agents that use simple recurrent neural networks (RNNs) and Gated Recurrent Units (GRUs) (Chung et al., 2014) in terms of performance and similarity of the neural activity to neural activity recorded in the brain.\n\nContributions of this work are as follows:\n\n• We integrated a cognitive model for evidence accumulation based on the Laplace transform\n\ninto an RL agent.\n\n• We showed that symbolic operations in the Laplace domain give rise to individual neurons that are tuned to the magnitude of the evidence, just like neurons in neuroscience studies (Nieh et al., 2021; Morcos & Harvey, 2016a).\n\n• We found that agents based on the Laplace framework learn faster and generalize better than agents based on commonly used RNNs. This indicates that RL agents were able to efficiently use the brain-like sequential representation of evidence.\n\n• We found that GRUs performed much better than RNNs, suggesting that gating plays an important role in constructing a neural representation of time-varying latent variables. This is consistent with the cognitive modeling work, which uses gating to convert a representation of elapsed time into a representation of accumulated evidence.\n\nFigure 1: Schematic of the accumulating towers environment. In this simple example, two towers appeared on the right, and one tower appeared on the left, so the agent has to turn right once it reaches the end of the track. Each tower is encoded with a single pixel value.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 METHODS\n\n2.1 ENVIRONMENTS\n\nInspired by the neuroscience studies from Nieh et al. (2021); Morcos & Harvey (2016b), we designed a simple version of the accumulating towers task. To evaluate robustness and generality of the proposed approach, aside from default version of the task, we also designed two other tasks, namely range count task and exact count task. The environments are provided as a part of the supplementary material, and they will be made publicly available and made open source together with the entire code used to implement the artificial agents and generate all of the results in the manuscript.\n\n2.1.1 ACCUMULATING TOWERS TASK\n\nAgents had to navigate down a virtual track composed of only three inputs: left, right and middle. In each episode, agents start from the beginning of the virtual track and observe towers (represented by the input value changing from 0 to 1) on each side of the environment (Fig. 1). Agents had four available actions: left, right, forward, and backward. Positions of towers were decided randomly in each episode. Similar to the neuroscience studies, the maximum number of towers on one side was 14. Once the agent arrived at the end of the track, the middle input changed from zero to one signifying that it hit the wall. In order to receive the reward, the agent had to turn left or right, depending on which side had more towers. We set the magnitude of the reward to 10, penalize the agents for hitting the wall at the end of the track or going backward with a -1 negative reward, and penalize the agent for hitting the side walls with a -0.1 negative reward.\n\n2.1.2 RANGE COUNT TASK\n\nIn this task, agents observed the same environment as in the accumulating towers task, but in order to obtain a reward, agents had to turn left only if the number of towers on the left-hand side was larger than five and right in every other case (regardless of the number of towers on the right-hand side).\n\n2.1.3 EXACT COUNT TASK\n\nIn the counting task, agents again observed the same environment as in the accumulating towers task, but the maximum number of towers on each side was 5 instead of 14, and agents had to turn left only if the number of towers on the left-hand side was exactly 5 and right in every other case.\n\n2.2 RNN IMPLEMENTATION OF EVIDENCE ACCUMULATION USING THE LAPLACE DOMAIN\n\nEvidence accumulation implemented through the Laplace domain is a key component of the cognitively-inspired RL agent (Fig. 2). We will first describe the Laplace framework for functions of time and then convert a representation of time into a representation of evidence and show how it can be implemented as an RNN.\n\nFigure 2: The agent architecture. We compare simple RNN, GRU and Laplace-based RNN described here.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\na\n\nb\n\nc\n\nFigure 3: Example of the Laplace and inverse Laplace transform with and without modulatory input. (a) In the absence of modulatory input (α = 1) the impulse response of the Laplace transform decays exponentially with decay rate s. The impulse response of the inverse Laplace transform has a unimodal shape. Note that if time t was shown on the log-scale, the unimodal curves would be equally wide and equidistant. (b, c) α modulates the decay rate of F and it is proportional to the change in the count (b) or distance (c). This makes units in ̃f develop unimodal basis functions that are tuned to count or distance rather than to time and peak at n∗ or x∗ respectively.\n\n2.2.1 LAPLACE FRAMEWORK\n\nWe define the Laplace transform of function f (t) from −∞ to the present t:\n\n(cid:90) t\n\nF (s; t) =\n\ne−s(t−t′)f (t′)dt′.\n\n(1)\n\n0 We restrict variable s to real positive values. 1\n\nThe above equation can be expressed in a differential form where s appears as a rate constant of a leaky integrator:\n\ndF (s; t) dt\n\n= −sF (s; t) + f (t).\n\n(2)\n\nFig. 3a shows the impulse response of the above equation for several values of s.\n\nTo convert a representation of time into a representation of numerosity n(t) (how many times some input was observed) extend Eq. 2 and modulate s with a rate of change α expressed as a time derivative of numerosity (α = dn/dt):\n\ndF (s; t) dt\n\n=\n\ndn dt\n\n(−sF (s; t) + f (t)) .\n\n(3)\n\nBy reorganizing terms in the above equation and applying the chain rule we can rewrite the equation as a function of n, instead of t (Fig. 3b):\n\ndF (s; n) dn\n\n= −sF (s; n) + f (n),\n\n(4)\n\nand we set f (n) = δ(0).\n\nNote that the same approach can convert a function of time into a function of any variable, which derivative can be learned from the environment. For instance, if α = dx/dt is velocity, then the network would represent traveled distance x (Fig. 3c).\n\n1The Laplace transform defines s as a complex variable. This choice would result in exponentially growing and oscillatory neural activity, causing numerical instabilities when computing the inverse Laplace transform.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nInverting the Laplace transform reconstructs the input as a function of the internal variable n∗, which corresponds to n. The inverse, which we denote as ̃f (n∗; t) can be computed using the Post inversion formula (Post, 1930):\n\n ̃f (n∗; n) = L−1\n\nk F (s; n) =\n\n(−1)k k!\n\nsk+1 dk\n\ndsk F (s; n),\n\n(5)\n\nwhere n∗ := k/s and k → ∞. As we show below, the reconstruction gives rise to units tuned to a particular n. By solving ∂ ̃fn∗;n/∂n = 0 we see that ̃f (n∗; n) peaks at n∗ = n. For s being a continuous variable and k → ∞, the width of the peak is infinitesimally small, providing a perfect reconstruction of the observed quantity.\n\n2.2.2 DISCRETE IMPLEMENTATION\n\nFor a neural network implementation, we discretize the Laplace and inverse Laplace transform for both s and t. To select values s in an informed way, we compute the impulse response of ̃fn∗;n:\n\n ̃fn∗;n =\n\n1 u(t)\n\nkk+1 k!\n\n(cid:18) u(t) n∗\n\n(cid:19)k+1\n\ne−k u(t) n∗ ,\n\n(6)\n\nwhere u = (cid:80)t i=0 α(ti). When s is discrete and k is finite, ̃fn∗;n is a set of unimodal basis functions (when s is continuous and k → ∞, those unimodal basis functions turn into delta functions with spacing → 0). The coefficient of variation of ̃fn∗;n is independent of n∗ and n: c = 1/ k + 1. This implies that the width of the unimodal basis functions increases linearly with their peak time. When observed as a function of log(n), the width of the unimodal basis functions is constant. This property of the Post inversion formula is relevant for modeling human perception due to the WeberFechner law (Fechner, 1860/1912; Portugal & Svaiter, 2011). This law states that the relationship between the perceived magnitude of the stimulus and its true magnitude is logarithmic, motivating the use of logarithmic units such as decibel and candela. To ensure equidistant spacing of unimodal basis functions along the log-axis we space n∗ logarithmically. This results in dramatic conservation of resources, especially when representing large quantities since the number of units in ̃fn∗;n grows as a function of log(n) rather than n. Note that fixing the values of n∗ and choosing k also fixes values of s since s = k/n∗.\n\n√\n\nWe now write a discrete-time approximation of Eq. equation 3 as an RNN with a diagonal connectivity matrix and a linear activation function:\n\nFs;t = W Fs;t−1 + ft,\n\n(7)\n\nwhere W = diag(e−α(t)s∆t). A discrete approximation of the inverse Laplace transform, ̃fn∗;t, can be implemented by multiplying Fs;t with a derivative matrix L−1 k computed for some finite value of k.\n\n2.2.3 SUBTRACTION OF FUNCTIONS USING THE LAPLACE DOMAIN\n\nIn the accumulating towers task, Eq. 4 can enable the agent to learn to represent the number of towers on each side. However, the latent variable that should determine the agent’s decision is not the number of towers on each side but the difference between those numbers (the agent needs to turn towards the side which had more towers). This is a non-trivial problem since the number of towers is not represented as a scalar but as a function over n. Fortuitously, the Laplace domain enables access to a number of useful operations, including subtraction of two functions (Howard et al., 2015). To show this, let us define f (a) and g(a) as functions representing two distributions of possible values for the number a in the range 0 to amax. Outside this range, the functions are assumed to vanish. We define the operation of subtraction of these two distributions [f − g](a) to be the cross-correlation of the two functions:\n\n[f − g](a) ≡\n\nf (x′)g(a + x′)dx′.\n\n(8)\n\nTo illustrate that the above operation results in subtraction of two functions, consider a simple case where each of the functions is a delta function: f = δ(a1) and g = δ(a2). Then [f − g] is a delta function at a1 − a2. To implement cross-correlation in the Laplace domain we can turn Eq. 8\n\n0\n\n5\n\n(cid:90) ∞\n\nUnder review as a conference paper at ICLR 2023\n\na\n\nb\n\nc\n\nFigure 4: Agent performance on (a) accumulating towers task, (b) range count task, and (c) exact count task. In each task proposed architectures (either ̃fsub or Fsub) learned the task faster than GRU despite having almost three orders of magnitude fewer parameters.\n\ninto convolution by reflecting g(a) around amax: gr = g(amax − a). Point-wise product of the Laplace transforms of two functions, f (a) and g(a), corresponds to their convolution in the time domain. Point-wise multiplication of the Laplace transform of f (a) and gr(a) corresponds to crosscorrelation of f (a) and g(a) in the time domain, which is equivalent to their subtraction [f − g](a). Note that for subtraction we need to consider both positive and negative values. Since we only use positive values of s, we are not able to directly represent the negative axis. To work around this, we compute both [f − g](a) and [g − f ](a).\n\n2.3 AGENT ARCHITECTURE\n\nThe agents received three inputs from the environment that were fed into the recurrent layer (Fig. 2). The recurrent layer was either RNN, GRU or an RNN based on the Laplace framework as described above. When the Laplace framework was used we had three independent ̃f modules. Each module had 20 n∗ values spaced logarithmically from 5 to 100. The value of parameter k was set to 8. Parameter k controls the sharpness of the unimodal basis functions and this value was chosen to ensure no gaps between them. Note also that the input into ̃f was delivered to α, which controls recurrent weights of the population of units F . This is a conceptual difference in comparison to other RNNs, where recurrent weights are tuned separately for each unit. The strength of the proposed approach is that the population of neurons encodes a function over a latent variable that the network needs to learn such that ̃f directly represents the count of the objects. In addition to computing ̃f , we also computed the subtraction ̃fsub of each pair of ̃f . This was done by computing the product in the Laplace domain between each pair of F as described in the previous section. The total number of units was 180 (20 units per module, 3 independent modules and 6 subtraction modules). When other RNNs were used, the dense layer was mapped to 180 recurrent units.\n\nThe output of the recurrent layer was passed to an actor network and a critic network. Both actor and critic consist of a single layer fully connected neural network. We set the discount rate to γ = 0 (since in this task, the reward was immediately available to agents after they made a correct turn). For all agents, we explored two different learning rates 0.001 and 0.0001.\n\n3 RESULTS\n\n3.1 PERFORMANCE OF RL AGENTS\n\nWe trained and evaluated agents in three different RL environments: accumulating towers, range count and exact count. We compared several agents based on the Laplace framework: the proposed agent using the inverse Laplace transform and the subtraction ( ̃fsub), without the subtraction ( ̃f ),\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\na\n\nb\n\nc\n\nd\n\nFigure 5: Neural activity of ( ̃fsub) agents after 100k episodes of accumulating towers task. Similar to plots in Nieh et al. (2021); Morcos & Harvey (2016b), neurons are sorted by peak activity. Each row is normalized such that the activity ranges from 0 to 1.\n\na\n\nb\n\nc\n\nd\n\nFigure 6: Same as Fig. 5 but for GRU agents.\n\nwith the subtraction but without the inverse Laplace transform (Fsub), and without both subtraction and the inverse Laplace transform (F ). This was done to evaluate the importance of different components of the Laplace framework. We also compared agents based on existing RNNs, including a simple RNN and GRU, as well as versions of those agents but with frozen recurrent weights. The control experiment with frozen recurrent weights was conducted because the Laplace agents have analytically computed recurrent weights s which are modulated on a population level with α as shared weight.\n\nd=10000 d=3000 ̃fsub 10.000 ± 0.000 10.000 ± 0.000 ̃f 9.925 ± 0.041 9.925 ± 0.065 10.000 ± 0.000 10.000 ± 0.000 Fsub 8.900 ± 0.272 8.675 ± 0.361 F\n4.700 ± 0.576 4.600 ± 0.281 RNN 9.600 ± 0.146 8.425 ± 0.504 GRU −201.0 ± 0.000 −601.0 ± 0.000 RNNF ROZEN GRUF ROZEN −14.473 ± 5.653 −149.3 ± 44.77 −449.4 ± 131.3\n\nd=300 10.000 ± 0.000 9.903 ± 0.066 10.000 ± 0.000 8.995 ± 0.287 4.475 ± 0.456 9.98 ± 0.000 −21.0 ± 0.000\n\n# Parameters 724 244 724 244 34024 100624 724 724\n\nTable 1: Results of agents trained of the accumulating towers task. The table shows mean reward +/- standard error across four runs after 100k episodes of training in d=300 steps long environment. Validation was done in 300, 3000 and 10000 steps long environments.\n\nThe agents were trained and evaluated in 300 steps long environment. We trained four different agents for each of the eight models and performed 100 validation runs every 1000 episodes.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n ̃fsub ̃f Fsub F\nRNN GRU\n\nd=300 d=3000 d=10000 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 9.325 ± 0.074 9.525 ± 0.096 9.400 ± 0.122 9.375 ± 0.147 9.350 ± 0.115 9.150 ± 0.075 −3.225 ± 8.033 −45.550 ± 44.875 −145.2 ± 131.6 4.300 ± 0.696 4.15 ± 0.557 9.125 ± 0.758 −601.0 ± 0.000 −201.0 ± 0.000 RNNF ROZEN −31.00 ± 0.000 −601.0 ± 0.000 −201.0 ± 0.000 GRUF ROZEN −31.00 ± 0.000\n\n# Parameters 724 244 724 244 34024 100624 724 724\n\nTable 2: Results of agents trained of the range count task. The table shows mean reward +/- standard error across four runs after 100k episodes of training in d=300 steps long environment. Validation was done in 300, 3000 and 10000 steps long environments.\n\n ̃fsub ̃f Fsub F\nRNN GRU\n\nd=10000 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 8.275 ± 0.246 8.225 ± 0.222 8.175 ± 0.219 RNNF ROZEN −21.10 ± 8.574 −148.7 ± 45.27 −448.8 ± 131.8 GRUF ROZEN −21.23 ± 8.465 −148.7 ± 45.27 −448.8 ± 131.8\n\nd=300 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 7.975 ± 0.129 7.975 ± 0.222 9.375 ± 0.375\n\nd=3000 10.000 ± 0.000 10.000 ± 0.000 10.000 ± 0.000 8.250 ± 0.075 8.000 ± 0.146 8.050 ± 0.545\n\n# Parameters 724 244 724 244 34024 100624 724 724\n\nTable 3: Results of agents trained of the exact count task. The table shows mean reward +/- standard error across four runs after 100k episodes of training in d=300 steps long environment. Validation was done in 300, 3000 and 10000 steps long environments.\n\n3.2 AGENTS BASED ON THE COGNITIVE MODEL LEARNED THE TASKS FASTER THAN\n\nGRU-BASED AGENTS\n\nWhile in some cases other agents had a better start, ̃fsub agents were the first to learn the tasks and converge to a reward value of 10 (see supplemental video of ̃fsub agent performing the accumulating towers task). This indicates that the cognitive model provided a good representation, and once the A2C algorithm learned to use it, it was able to perform perfectly (Fig. 4 and the first column in Table 1, Table 2, and Table 3; see also Fig. 13, Fig. 14, and Fig. 15 for results on a wider training range). In accumulating towers task and exact count task, Fsub agents converged next. While the difference between ̃fsub and Fsub agents is only in the inverse Laplace transform which is just a linear projection, this result suggests that the sequential map-like activation in ̃fsub was more useful in learning to perform the task with perfect accuracy than the Laplace representation with exponentially decaying traces. ̃f and F agents performed above chance (agents reached the end of the track and made a correct decision in more than 50% of cases), but did not reach the performance of 10 during 100k episodes on accumulating towers task suggesting that the subtraction operation which constructed a map-like representation for evidence was important for learning.\n\nGRU agents managed to reach performance close to 10, indicating that they can learn the tasks as well (see supplemental video of GRU agent performing the accumulating towers task). On the other hand, the RNN agents did not learn the tasks in 100k episodes indicating that gating was important for correct performance. It is important to note that the cognitive model also constructed the representation of evidence by a gating mechanism. Similarly, GRU can learn to modulate the range of change in neural activity by the amount of change in the evidence. Frozen models were not able to learn the task, failing to even reach the end of the track and make a random decision as indicated by the total reward being negative.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\n3.3 AGENTS BASED ON THE COGNITIVE MODEL WERE ROBUST TO CHANGES IN THE\n\nENVIRONMENT\n\nTo test the ability of agents to generalize, we also evaluated them on 3000 and 10000 steps long tracks without ever training them on tracks of that length (second and third column in Table 1, Table 2 and Table 3). Agents based on the cognitive model showed great resilience to this kind of rescaling. This is not surprising since the representation was designed to change as a function of change in the amount of evidence, so rescaling the environment did not have any impact. On the other hand, the performance of GRU agents dropped at unseen track lengths but remained well above chance. This suggests that GRU agents were able to learn to modulate their activity by the change in the amount of evidence: when there was no new evidence, there was little to no change in the activity, making the impact of track rescaling relatively small.\n\nWe also evaluated agents in environments that had 1 to 30 towers on each side instead of 1 to 14 towers (Table 4). While all agents suffered some loss in reward, GRU and ̃fsub seem most resilient to this change, indicating potential similarity in the form of those two representations.\n\n3.4 NEURAL ACTIVITY INSIDE THE RECURRENT LAYER RESEMBLES ACTIVITY IN MICE\n\nHIPPOCAMPUS\n\nWe visualized the neural activity of each of the four agents for each of 8 models after 100k episodes of training on the accumulating towers task. As expected, neurons in ̃fsub agents activated sequentially as a function of evidence resembling the activity in neural recordings from the hippocampus (Nieh et al., 2021; Morcos & Harvey, 2016b) (Fig. 5). ̃f neurons also showed some tuning to the amount of evidence, but since they were able to only count objects on each side (and not subtract them), the tuning is blurry (Fig. 7). Neurons in F and Fsub agents showed gradual changes as a function of evidence rather than tuning to a particular magnitude of evidence reflecting the exponential dynamics of the Laplace transform (Fig. 8 and Fig. 9 respectively).\n\nSome of the neurons in GRU agents showed tuning to the magnitude of evidence with often prominent asymmetry between positive and negative amount of evidence (Fig. 6). (See supplemental video of changes in the neural representation during training of GRU agent.) In comparison to neurons in RNN agents (Fig. 11), GRUs had a firing pattern significantly more dependent on the amount of evidence. Neurons in frozen GRU agents also showed some tuning to the magnitude of evidence (Fig. 10), although less than neurons in trained GRU agents.\n\n4 CONCLUSIONS\n\nWe evaluated different artificial agents on three tasks 1) evidence accumulation task that mimicked the procedure of a recent neuroscience study (Nieh et al., 2021; Morcos & Harvey, 2016b), 2) range count task, and 3) exact count task. We used a simple setup with only three inputs and compared agents based on a cognitive model with agents based on simple RNN and GRU.\n\nAgents based on a cognitive model were able to learn faster and generalize better despite having almost three orders of magnitude fewer parameters than GRU-based agent (244 vs 100624). This is an indicator that the A2C algorithm was able to use the neural representation from the cognitive model. This representation also resembled data from neural recordings in Nieh et al. (2021); Morcos & Harvey (2016b) characterized with the sequential activation as a function of the amount of evidence.\n\nAgents based on the cognitive model represented perceptual information on a logarithmic scale, consistent with the Weber-Fechner law. This type of representation saves resources and can explain human behavioral outputs in perceptual tasks.\n\nWhile we focused on experiments that involve numerosity, the proposed architecture can represent any latent variables (e.g., size, distance or luminosity) as a log-compressed number line. The only condition is that the latent variables change over time such that their time derivative can be extracted from the input signal (see Fig. 3 for examples of log-compressed number lines for discrete and continuous signals).\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nDmitriy Aronov, Rhino Nevers, and David W Tank. Mapping of a non-spatial dimension by the\n\nhippocampal–entorhinal circuit. Nature, 543(7647):719–722, 2017.\n\nTimothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B Baram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map? organizing knowledge for flexible behavior. Neuron, 100(2):490–509, 2018.\n\nYoshua Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2017.\n\nJ Bures, AA Fenton, Yu Kaminsky, and L Zinyuk. Place cells and place navigation. Proceedings of\n\nthe National Academy of Sciences, 94(1):343–350, 1997.\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n\nNathanael A Cruzado, Zoran Tiganj, Scott L Brincat, Earl K Miller, and Marc W Howard. Conjunctive representation of what and when in monkey hippocampus and lateral prefrontal cortex during an associative memory task. Hippocampus, 30(12):1332–1346, 2020.\n\nHoward Eichenbaum. Time cells in the hippocampus: a new dimension for mapping memories.\n\nNature Reviews Neuroscience, 15(11):732–744, 2014.\n\nArne D Ekstrom and Charan Ranganath. Space, time, and episodic memory: The hippocampus is\n\nall over the cognitive map. Hippocampus, 28(9):680–687, 2018.\n\nGustav Fechner. Elements of Psychophysics. Vol. I. Houghton Mifflin, 1860/1912.\n\nMarc W Howard and Michael E Hasselmo. Cognitive computation using neural representations of\n\ntime and space in the laplace domain. arXiv preprint arXiv:2003.11668, 2020.\n\nMarc W Howard, Christopher J MacDonald, Zoran Tiganj, Karthik H Shankar, Qian Du, Michael E Hasselmo, and Howard Eichenbaum. A unified mathematical framework for coding time, space, and sequences in the hippocampal region. Journal of Neuroscience, 34(13):4692–707, 2014. doi: 10.1523/JNEUROSCI.5808-12.2014.\n\nMarc W Howard, Karthik H Shankar, and Zoran Tiganj. Efficient neural computation in the laplace domain. In Cognitive computations workshop at Advances in neural information processing systems, 2015.\n\nMarc W Howard, Andre Luzardo, and Zoran Tiganj. Evidence accumulation in a laplace domain\n\ndecision space. Computational brain & behavior, 1(3):237–251, 2018.\n\nEric B Knudsen and Joni D Wallis. Hippocampal neurons construct a map of an abstract value space.\n\nCell, 184(18):4640–4650, 2021.\n\nDonald Richard John Laming. Information theory of choice-reaction times. 1968.\n\nSteve LeVine. Artificial intelligence pioneer says we need to start over. Arlington, VA: Axios, 2017.\n\nSteve W Link. The relative judgment theory of two choice response time. Journal of Mathematical\n\nPsychology, 12(1):114–135, 1975.\n\nC. J. MacDonald, K. Q. Lepage, U. T. Eden, and H. Eichenbaum. Hippocampal “time cells” bridge\n\nthe gap in memory for discontiguous events. Neuron, 71(4):737–749, 2011.\n\nGary Marcus. The next decade in ai: four steps towards robust artificial intelligence. arXiv preprint\n\narXiv:2002.06177, 2020.\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pp. 1928–1937. PMLR, 2016.\n\nAri S Morcos and Christopher D Harvey. History-dependent variability in population dynamics\n\nduring evidence accumulation in cortex. Nature Neuroscience, 19(12):1672–1681, 2016a.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nAri S Morcos and Christopher D Harvey. History-dependent variability in population dynamics\n\nduring evidence accumulation in cortex. Nature neuroscience, 19(12):1672–1681, 2016b.\n\nMay-Britt Moser, David C Rowland, and Edvard I Moser. Place cells, grid cells, and memory. Cold\n\nSpring Harbor perspectives in biology, 7(2):a021808, 2015.\n\nRobert Muller. A quarter of a century of place cells. Neuron, 17(5):813–822, 1996.\n\nEdward H Nieh, Manuel Schottdorf, Nicolas W Freeman, Ryan J Low, Sam Lewallen, Sue Ann Koay, Lucas Pinto, Jeffrey L Gauthier, Carlos D Brody, and David W Tank. Geometry of abstract learned knowledge in the hippocampus. Nature, pp. 1–5, 2021.\n\nEva Pastalkova, Vladimir Itskov, Asohan Amarasingham, and Gyorgy Buzsaki. Internally generated\n\ncell assembly sequences in the rat hippocampus. Science, 321(5894):1322–1327, 2008.\n\nRD Portugal and Benar Fux Svaiter. Weber-fechner law and the optimality of the logarithmic scale.\n\nMinds and Machines, 21(1):73–81, 2011.\n\nE. Post. Generalized differentiation. Transactions of the American Mathematical Society, 32:723–\n\n781, 1930.\n\nR. Ratcliff. A theory of memory retrieval. Psychological Review, 85:59–108, 1978.\n\nDaniel M Salz, Zoran Tiganj, Srijesa Khasnabish, Annalyse Kohley, Daniel Sheehan, Marc W Howard, and Howard Eichenbaum. Time cells in hippocampal area ca3. Journal of Neuroscience, 36(28):7476–7484, 2016.\n\nK. H. Shankar and M. W. Howard. A scale-invariant internal representation of time. Neural Com-\n\nputation, 24(1):134–193, 2012.\n\nDaniel J Sheehan, Stephen Charczynski, Blake A Fordyce, Michael E Hasselmo, and Marc W Howard. A compressed representation of spatial distance in the rodent hippocampus. bioRxiv, 2021.\n\nEdward C Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\n ̃fsub ̃f Fsub F\nRNN GRU\n\nd=300 d=3000 9.892 ± 0.067 9.850 ± 0.056 7.060 ± 0.220 7.175 ± 0.343 8.845 ± 0.151 8.775 ± 0.137 7.293 ± 0.257 7.625 ± 0.410 4.538 ± 0.449 4.800 ± 0.429 9.725 ± 0.054 9.895 ± 0.016 RNNF ROZEN −21.000 ± 0.000 −201.0 ± 0.000 GRUF ROZEN −14.555 ± 5.582 −149.6 ± 44.5\n\nd=10000 9.925 ± 0.041 6.800 ± 0.242 8.525 ± 0.248 7.350 ± 0.109 4.775 ± 0.461 9.025 ± 0.219 -601.0 ± 0.000 -449.7 ± 131.0\n\n# Parameters 724 244 724 244 34024 100624 724 724\n\nTable 4: Mean reward +/- standard error across four runs on accumulating towers task after 100k episodes of training in d=300 steps long environment and 1 to 14 towers shown to agents. Validation was done in 300, 3000 and 10000 steps long environments and with 1 to 30 towers shown to agents.\n\na\n\nb\n\nc\n\nd\n\nFigure 7: Neural activity (top row) and psychometric curves (bottom row) of ̃f agents after 100k episodes of accumulating towers task. Top row conveys the same information as Fig. 5. Since ̃f did not have perfect accuracy, we also show the psychometric curves. Dots in the psychometric curves indicate actions taken by the agent at different amounts of evidence.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\na\n\nb\n\nc\n\nd\n\nFigure 8: Same as Fig. 7 but for F agents\n\na\n\nb\n\nc\n\nd\n\nFigure 9: Same as Fig. 7 but for Fsub agents.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\na\n\nb\n\nc\n\nd\n\nFigure 10: Same as Fig. 7 but for GRUf rozen agents. Note that some psychometric curves are empty, indicating that those agents did not learn to reach the end of the environment.\n\na\n\nb\n\nc\n\nd\n\nFigure 11: Same as Fig. 7 but for RNN agents.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\na\n\nb\n\nc\n\nd\n\nFigure 12: Same as Fig. 7 but for RNNf rozen agents. Note that psychometric curves are empty, indicating that the agents did not learn to reach the end of the environment.\n\nFigure 13: Agent performance on the accumulating towers task (same data as in Fig. 4a but different scale).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Agent performance on the range count task (same data as in Fig. 4b but different scale).\n\nFigure 15: Agent performance on the exact count task (same data as in Fig. 4c but different scale).\n\n16",
    "reference": "# Summary Of The Paper\n\nThe paper presents a novel framework for reinforcement learning, specifically for evidence accumulation tasks that involve counting quantities and deciding which is greater. The general approach is to use cognitive models as inductive biases by learning a function representing the evidence and giving the resulting representations to an RL algorithm. The paper includes a discretized version of the Laplace transform-based framework to use with a neural network.\n\nThe framework is as follows: the Laplace transform is rewritten as a differential equation in terms of the numerosity of the stimulus and discretized; an inverse is also derived. This is meant to enable learning of a counted quantity. To subtract one quantity from another and determine which one is higher, the paper presents a cross-correlation mechanism. \n\nThis framework integrates into an RL algorithm (here A2C) by having environment inputs go into a recurrent layer, have that layer learn a direct representation of the count of the objects and their difference (in theory), and then pass the output of this layer to the actor and critic networks. \n\nExperiments are done on an \"accumulating towers\" task. There are several ablations removing various parts of the Laplace framework, as well as RNN and GRU layers as baselines. Task reward statistics show that the Laplace and inverse Laplace transforms with subtraction do very well, as does GRU. Other ablations have mid-range performance, and RNN does quite poorly. Frozen versions of RNN and GRU are unsurprisingly very poor.\n\nWhen evaluating on harder versions of the task, Laplace with subtraction continues to do well, as does GRU. The paper attributes this to the gating mechanisms present in both methods.\n\nFinally, there is some analysis of neural activity and its connections to human cognitive maps. Results show that Laplace with subtraction appears to activate sequentially as a function of evidence. GRU and other versions of the Laplace framework also shows some tuning to \"magnitude\"/\"amount of evidence\".\n\n# Strength And Weaknesses\n\n**Strengths**\n- Introduction is strong and motivates the paper well. The idea of cognitive models providing inductive biases is not novel but this is a very interesting instantiation of it\n- The connections to human cognitive maps are more convincing than a lot of \"human cognitive construct + our cool new method\" connections that ML papers attempt to make, especially because of follow-up evidence in the paper such as the logarithmic representation. While even this one is ultimately conceptual (and the logarithmic representation appears to be a design choice rather than an emergent property, if I understand correctly), I find it compelling.\n- The paper is well-packaged - it makes several claims and demonstrates them, and the methods solve exactly the problem motivated and outlined. To some degree, the final contribution about gating could use more expansion, but I realize it's more a validation of prior work and I appreciate the acknowledgement of the similarities/power of existing work\n\n**Weaknesses**\n- The Laplace framework presented here seems very tuned to this specific task, or at least counting-based tasks. The paper acknowledges in \"Future Work\" that this is by no means the only type of latent variable and encourages work on others, but as of right now the framework is engineered in terms of counting - or at least, that's the sense I get reading it. It's still an interesting method, but GRUs and RNNs are much more general modules; it would be helpful to have (brief!) discussion of either 1) the domain limitations of these equations or 2) the potential to easily adapt this method to other tasks. I see how this formulation may extend to some other quantities, but I don't know for sure. \n- The discussion I described above would make the comparisons in this paper make more sense, but for actual acceptance I'm not sure that would be enough. This is an interesting idea and novel to my knowledge, but without seeing experiments in other domains, it's not clear how much value it adds. A more general approach covering a larger domain (potentially through multiple frameworks like this one), and experimental results on those, would be more convincing.\n- From a pure utility perspective, it's unclear how much value this method provides over GRU (not to mention, it has a narrower domain if I understand correctly). Because the performances are so similar, some statistics differentiating them would help.\n- The word \"deep\" is used in the title and throughout the paper, but the version of A2C experimented on uses single-layer FCNs for actor and critic. The paper isn't making claims about scale or ability to handle large real-world tasks or complicated environments, so this claim seems not only false but unnecessary to the story.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity**\n- Section 2 is quite clear given the math in it; while it did take a couple reads to build intuition for the Laplace framework, it wasn't overly daunting or impossible\n- Results section could use better claims-based signposting, rather than topic-based, or just a general info dump as most of it is now (which is not to say that long section doesn't read well; it does). Three of the four contributions are claims that will be demonstrated, so the results section may benefit from section headings based on those claims. \n\n**Quality**\n- Experiment section is carefully considered. The experiments designed to support each claim in the contributions are intuitive and convincingly designed\n- The paper doesn't leave any loose ends or unanswered questions. It feels like a finished piece of work.\n\n**Originality**\n- The high-level concept of cognitive models as inductive biases (or some such integration) is familiar, but I believe this particular approach for evidence accumulation is novel. I think the contribution list is quite an accurate and complete summary of the parts of this paper that are novel.\n\nNits:\n- \"Amount of evidence\" is strange phrasing here. It seems from the rest of the paper and the place cell example that this means *n* itself, but the phrase sounds like it means, how much evidence we have that the value is *n* as opposed to other values.\n\n# Summary Of The Review\n\nI am recommending a weak acceptance for this paper. It is a good paper and I enjoyed reading it, but I am not convinced of its significance from a general modeling perspective given the limitations I think I see in its domain. I also am not totally sure I understood all the math, so I may be missing limitations; I may also be missing strengths. \n\nAll that said, the quality of idea and execution is far above a workshop paper and it is a very compelling read, so I wouldn't consider it fair to recommend rejection just because it's an interesting and well-demonstrated idea rather than the next big SOTA system like a lot of papers. I would raise my score if I were convinced that this opens a lot of future work directly (not just in high-level concept) or enables improvement on new tasks out of the box, and if I were convinced that it provides meaningful improvement over existing methods.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nMEMONAV: WORKING MEMORY MODEL FOR VISUAL NAVIGATION\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, the node features on the topological map are stored in the short-term memory (STM), as these features are dynamically updated. The MemoNav retains the informative fraction of the STM via a forgetting module to improve navigation efficiency. To learn a global representation of 3D scenes, we introduce long-term memory (LTM) that continuously aggregates the STM. Afterward, a graph attention module encodes the retained STM and the LTM to generate working memory (WM). After encoding, the WM contains the informative features in the retained STM and the scene-level feature in the LTM and is finally used to generate actions. Consequently, the synergy of these three types of memory increases navigation performance by selectively retaining goal-relevant information and learning a highlevel scene feature. When evaluated on multi-goal tasks, the MemoNav outperforms the SoTA methods at all difficulty levels in both Gibson and Matterport3D scenes. The MemoNav also achieves consistent improvements on traditional 1-goal tasks. Moreover, the qualitative results show that our model is less likely to be trapped in a deadlock.\n\n1\n\nINTRODUCTION\n\nThis paper studies the image-goal navigation (ImageNav) problem, which aims to steer an agent towards a destination with the goal image in unseen environments. ImageNav has recently received much attention due to its promising applications in robots navigating in the open world.\n\nScene memory is essential for imageNav as it provides indispensable historical information for decision-making in unseen environments (Savinov et al., 2018). During navigation, this memory typically stores both scene features and the agent’s navigation history (Kwon et al., 2021). These two types of information in turn help the agent generate more reasonable navigation actions by lessening the negative impact of partial observability (Parisotto & Salakhutdinov, 2018). In literature, various memory mechanisms have been introduced for ImageNav, which can be classified into three categories according to memory structure: (a) metric map-based methods (Chaplot et al., 2020a; Chen et al., 2019) that reconstruct local top-down maps and aggregate them into a global map, (b) stacked memory-based methods (Pashevich et al., 2021; Mezghani et al., 2021; Fang et al., 2019) that stack the past observations in chronological order, and (c) topological map-based methods (Kwon et al., 2021; Chaplot et al., 2020b; Beeching et al., 2020; Savinov et al., 2018) that store sparse landmark features in graph nodes. The topological map-based methods benefit from the memory sparsity of topological maps and have achieved impressive performance in ImageNav.\n\nHowever, existing topological map-based methods still suffer from two major limitations: (a) Unawareness of useful nodes. They generally use all node features for generating actions without considering the contribution of each node, thus being easily misled by redundant nodes that are uninformative of the goal. (b) Local representation. Each node feature only represents a small local area in a large scene, which restricts the agent’s ability to learn a higher-level semantic and geometric representation of the scene.\n\nTo overcome the above two limitations, we present a novel ImageNav method named MemoNav, which is motivated by the classical concept of working memory in cognitive neuroscience (Cowan, 2008) and in loose analogy with the working memory model in human navigation (Blacker et al.,\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n2017). The MemoNav learns three types of scene representations: (a) Short-term memory (STM) represents the local and transient features of the nodes in a topological map. (b) Long-term memory (LTM) is a global node that learns a scene-level representation by continuously aggregating STM. (c) Working memory (WM) learns goal-relevant features about 3D scenes and is used by a policy network to generate actions. The WM is formed by encoding the informative fraction of the STM and the LTM.\n\nBased on the above three representations, the MemoNav navigation pipeline contains five steps: (1) STM generation. The map update module stores landmark features on the map as the STM. (2) Selective forgetting. To incorporate goal-relevant STM into the WM, a forgetting module temporarily removes nodes whose attention scores assigned by a memory decoder rank below a predefined percentage. After forgetting, the navigation pipeline will not compute the forgotten node features at subsequent time steps. (3) LTM generation. To assist the STM, we add a global node to the map as the LTM. The global node links to all map nodes and continuously aggregates the features of these nodes at each time step. (4) WM generation. A graph attention module encodes the retained STM and the LTM to generate the WM. The WM utilizes the goal-relevant information in the retained STM and the scene-level feature in the LTM, thus enabling the agent to utilize informative scene representations to improve navigation performance. (5) Action generation. Two Transformer decoders use the embeddings of the goal image and the current observation to decode the WM. Then, the decoded features are used to generate navigation actions.\n\nConsequently, with the synergy of the three representations, the MemoNav noticeably outperforms the SoTA method (Kwon et al., 2021) in the Gibson scenes (Xia et al., 2018), increasing the navigation success rate by approximately 2.9%, 1.4%, 2.4%, and 1.7% on 1, 2, 3, and 4-goal test datasets, respectively. The comparison in the Matterport3D scenes (Chang et al., 2017) shows that the MemoNav exhibits better transferability.\n\nThe main contributions of this paper are as follows:\n\n• We propose the MemoNav, which learns three types of scene representations (STM, LTM,\n\nand WM) to improve navigation performance in the ImageNav task.\n\n• We use a forgetting module to retain the STM, thereby reducing redundancy in the map and improving navigation efficiency. We also introduce a global node as the LTM. The LTM connects to all nodes in the STM and learns a scene-level representation that provides the agent with a global view.\n\n• We adopt a graph attention module to generate WM from the retained STM and the LTM. This module flexibly adjusts weights used for aggregating node features, which helps the agent use adaptive WM to improve performance.\n\n• The experimental results demonstrate that our model outperforms the SoTA baseline on both\n\n1-goal and multi-goal tasks in two popular scene datasets.\n\n2 RELATED WORK\n\nImageNav methods. Since an early attempt (Zhu et al., 2017) to train agents in a simulator for ImageNav, rapid progress has been made on this task (Beeching et al., 2020; Chen et al., 2021; Wasserman et al., 2022; Al-Halah et al., 2022). Several methods have utilized topological scene representations for visual navigation, of which SPTM (Savinov et al., 2018) is an early work. NTS (Chaplot et al., 2020b) and VGM (Kwon et al., 2021) incrementally build a topological map during navigation and generalize to unseen environments without exploring the scenes in advance. These methods utilize all features in the map, while the MemoNav flexibly utilizes the informative fraction of these features. Another line of work (Yadav et al., 2022; Majumdar et al., 2022) has introduced self-supervised learning to enhance the scene representations, achieving a promising navigation success rate. In contrast, we enhance the scene representations using a global node that aggregates the agent’s local observation features.\n\nMemory mechanisms for reinforcement learning. Several studies (Ritter et al., 2021; Lampinen et al., 2021; Sukhbaatar et al., 2021; Loynd et al., 2020) draw inspiration from memory mechanisms of the human brain and design reinforcement learning models for reasoning over long time horizons. Ritter et al. (Ritter et al., 2021) proposed an episodic memory storing state transitions for navigation\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Example episodes for 1-goal and three-goal tasks. In the 1-goal task (left), the agent searches for the goal during exploring the scene. In the three-goal task (right), the agent continues to navigate to two more successive goals.\n\ntasks. Lampinen et al. (Lampinen et al., 2021) presented hierarchical attention memory as a form of “mental time-travel” (Tulving, 1985), which means recovering goal-oriented information from past experiences. Unlike this method, our model retains such information via a novel forgetting module. Expire-span (Sukhbaatar et al., 2021) predicts life spans for each memory fragment and permanently deletes expired ones. Our model is different from it in that we restore forgotten memory if the agent returns to visited places. The most similar work is Working Memory Graph (Loynd et al., 2020) which is also inspired by working memory in cognitive neuroscience. However, its memory capacity is fixed. In contrast, our model retains a certain proportion of short-term memory in the working memory and adjusts the memory capacity when the navigation episode contains more goals.\n\n3 BACKGROUND\n\n3.1 TASK DEFINITION\n\nThe objective of ImageNav (Figure 1) is to learn a policy π to reach a target, given an image Itarget that contains a view of the target and a series of observations {It} captured during the navigation. At the beginning of navigation, the agent receives an RGB image Itarget of the target. At each time step, the agent captures an RGB-D panoramic image It of the current location and generates a navigational action. Following (Kwon et al., 2021), any additional sensory data (e.g. GPS and IMU) are not available.\n\n3.2 BRIEF REVIEW OF VISUAL GRAPH MEMORY\n\nThe MemoNav is mainly built upon VGM (Kwon et al., 2021), which is briefly introduced below.\n\nVGM incrementally builds a topological map G = (V, E) from the agent’s past observations where V and E denote nodes and edges, respectively. The node features (short-term memory) V ∈ Rd×Nt are generated from observations by a pretrained encoder Floc where d denotes the dimension of feature and Nt the number of nodes at time t.\n\nVGM uses a graph convolutional network (GCN) to encode the topological map, obtaining the encoded memory M = GCN(V ). Before encoding, VGM obtains the target embedding etarget = Fenc(Itarget) and fuses each node feature with this embedding using a linear layer.\n\nThe encoded memory is then decoded by two Transformer (Vaswani et al., 2017) decoders, Dcur and Dtarget. Dcur takes the current observation embedding ecur = Fenc(Icur) as the query and the feature vectors of the encoded memory M as the keys and values, generating a feature vector fcur. Similarly, Dtarget takes the target embedding etarget as the query and generates ftarget. Lastly, an LSTM-based policy network takes as input the concatenation of fcur, ftarget and ecur to output an action distribution.\n\n4 METHOD\n\nThe MemoNav comprises three key components: the forgetting module, the global node, and the GATv2 memory encoder. We show the pipeline of the MemoNav in Figure 2 and describe these components in the remainder of this section.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Overview of MemoNav. (a) The memory update module builds a topological map using et, the embedding of the current image It. (b) The node features in the map form the STM while a global node that links to each node acts as the LTM. (c) The forgetting module temporarily excludes a fraction of STM whose attention scores rank below a threshold p. (d) The retained STM and the LTM are then encoded by (e) a graph attention module to generate the WM Mw. (f) The WM is decoded by two Transformer decoders (details in the Appendix). (g) Lastly, the output of the decoding process is input to a policy network to generate navigation actions.\n\n4.1 SELECTIVE FORGETTING MODULE\n\nWe devise a forgetting module that instructs the agent to forget uninformative experiences. Here, “forgetting” means that nodes with attention scores lower than a threshold are temporarily excluded from the navigation pipeline. This means of forgetting via attention is evidenced by research (Fukuda & Vogel, 2009) revealing that the optimal performance of working memory relies on humans’ ability to focus attention on task-relevant information. In Figure 3, we visualize the attention scores for the short-term memory (STM) calculated in Dtarget of VGM. The figure shows that the agent assigns high scores to nodes close to targets while paying little attention to remote ones. This phenomenon indicates that not all node features in the STM are helpful and that it is more efficient to use a small number of them for navigation.\n\nThe forgetting module retains a fraction of STM according to the attention scores {αi}Nt i=1 in Dtarget. These scores denote to what extent the goal embedding etarget attends to each node feature in the STM. After Dtarget finishes decoding, the agent temporarily “forgets” a fraction of nodes whose scores rank below a predefined percentage p. In other words, these nodes will be disconnected from their neighbors, and not be processed by the navigation pipeline at the subsequent time steps. If the agent returns to a forgotten node, this node will be added to the map and processed by the pipeline again. In a multi-goal task, once the agent has reached a target, all forgotten nodes will be restored, as these nodes are probably useful for finding the next target.\n\nThe forgetting module is used in a plug-and-play manner, which means that this module is not activated during training and switched on when evaluating and deploying. p is default to 20%. With this module, the agent can selectively retain the informative STM, while avoiding misleading experiences.\n\n4.2 LONG-TERM MEMORY GENERATION\n\nIn addition to STM, the information in the long-term memory (LTM) also forms part of working memory (WM) (Ericsson & Kintsch, 1995). Inspired by ETC (Ainslie et al., 2020) and LongFormer\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: VGM calculates redundant node features. The figure visualizes the attention scores in Dtarget for an multi-goal episode of VGM. We can see that only a small number of nodes are useful for finding a single goal.\n\n(Beltagy et al., 2020), we add a zero-initialized trainable global node nglobal ∈ Rd to the topological map as the LTM (the orange nodes in Figure 2). RecBERT (Hong et al., 2021) adopts a recurrent state token similar to the proposed LTM, but their functions are different. The token in RecBERT aggregates visual-linguistic clues while the LTM in our model summarizes the agent’s past observations by continuously fusing the STM via memory encoding (the encoder is described in the next subsection).\n\nThe LTM possesses two advantages: it learns a scene-level feature and facilitates feature fusing. A recent study (Ramakrishnan et al., 2022) suggests that embodied agents need to learn higher-level representations of the 3D environment to overcome the partial observability caused by limited field-ofview sensors. From this viewpoint, the LTM stores a high-level scene representation by aggregating local node features. Moreover, another merit of the LTM is facilitating feature fusing. The topological map is divided into several sub-graphs when forgotten nodes are removed. Consequently, direct message passing between the nodes separated in different sub-graphs no longer exists. The LTM links to every node in the map, acting as a bypath that facilitates feature fusing between these isolated sub-graphs.\n\n4.3 WORKING MEMORY GENERATION\n\nThe third type of scene representation WM learns goal-relevant features that are used to generate actions. To learn adaptive WM, we utilize a graph attention module GATv2 (Brody et al., 2022) to encode the retained STM and the LTM since GATv2 is powerful when different nodes have different rankings of neighbors. GATv2 derives adaptive weights from node features for neighboring nodes, instead of from the Laplacian matrix. This design is suitable for generating WM, especially in multi-goal tasks since the STM features that contain information about the target or a path leading to the target should obtain high weights while those of irrelevant places should receive lower weights. In addition, the node features in the STM are dynamically updated during navigation, as the agent replaces these features with new ones when revisiting these nodes. Therefore, it is more suitable to adaptively change the weights used to aggregate the STM features into the WM.\n\nThe WM generation is formulated as: Mw = {V ′, nt global}) where Mw represents the generated working memory, and V ′ the encoded STM. {·, ·} denotes that the LTM (a vector) is prepended to the retained STM (a sequence of vectors). Note that the time step superscript of nglobal means that the LTM is recurrent through time.\n\nglobal} = GATv2({V , nt−1\n\nAfter GATv2 encoding, the WM aggregates the goal-relevant information in the retained STM as well as the scene-level representation in the LTM. Lastly, the decoders Dcur and Dtarget take Mw as keys and values, generating fcur and ftarget, which are further used to generate actions.\n\n5 EXPERIMENTS\n\n5.1 DATASETS\n\nAll experiments are conducted in the Habitat (Savva et al., 2019) simulator with the Gibson (Xia et al., 2018) or Matterpot3D (Chang et al., 2017) scene dataset. We adopt the same action space as in VGM (Kwon et al., 2021).\n\n1-goal dataset. In the Gibson scenes, all models are trained with 72 scenes and evaluated on a public 1-goal 1 dataset (Mezghani et al., 2021) that comprises 14 unseen scenes. Following the setting in\n\n1The 1-goal difficulty level here denotes the hard level in the public test dataset\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nVGM (Kwon et al., 2021), 1007 out of 1400 episodes in this public dataset are used for the evaluation on 1-goal tasks, but the 1400 episodes are still used for ablation studies.\n\nMulti-goal dataset. Multi-goal evaluation is more suitable for evaluating memory mechanisms used by navigation methods. By letting the agent return to visited places we can test whether memory mechanisms help the agent plan a short path. If not, the agent will probably waste its time re-exploring the scene or traveling randomly. However, recent ImageNav methods seldom conduct multi-goal evaluations. To further investigate how memory mechanisms assist the agent in navigation, we follow MultiON (Wani et al., 2020) to collect 700-episode multi-goal test datasets in the Gibson scenes (an example in Figure 1). In a multi-goal task, the agent navigates to an ordered sequence of goals.\n\nWe follow five rules to set sequential goals for each sample: (1) No obstacles appear inside a circle with a radius of 0.75 meters centered at each goal. (2) The distance between two successive goals is no more than 10 meters. (3) All goals are placed on the same layer without altitude differences. (4) All goals are reachable from each other. (5) The final goal is placed near a certain previous one with the distance between them smaller than 1.5 meters. Please refer to A.4.4 for more details.\n\nIn the Matterpot3D scenes, we sample 1008 episodes for each difficulty level from the multi-goal test datasets used in Multi-ON (Wani et al., 2020). The difficulty of an episode is indicated by the number of goals. All compared methods and our model are trained on the Gibson 1-goal dataset and tested on other difficulty levels.\n\nEvaluation Metrics. In 1-goal tasks, the success rate (SR) and success weighted by path length (SPL) (Anderson et al., 2018) are used. In a multi-goal task, two metrics are borrowed from (Wani et al., 2020): The progress (PR) is the fraction of goals successfully reached, equal to the SR for 1-goal tasks; Progress weighted by path length (PPL) indicates navigation efficiency and is defined as\n\nP P L =\n\n1 E\n\nE (cid:88)\n\nP rogressi\n\ni=1\n\nli max (pi, li)\n\n, where E is the total number of test episodes, li and pi are\n\nthe shortest path distance to the final goal via midway ones, and the actual path length taken by the agent, respectively.\n\n5.2 COMPARED METHODS AND TRAINING DETAILS\n\nWe compare the MemoNav with previous methods utilizing various types of memory. Random is an agent that samples actions from a uniform distribution and navigates with oracle stopping. CNNLSTM (Zhu et al., 2017) uses no maps but a hidden vector as implicit memory. ANS (Chaplot et al., 2020a) is a metric map-based model adapted for ImageNav in the experiments of VGM (Kwon et al., 2021). NTS (Chaplot et al., 2020b) incrementally builds a topological map without pre-exploring and adopts a hierarchical navigation strategy. VGM (Kwon et al., 2021) (see Section 3.2) is the baseline the MemoNav is built on. ZER (Al-Halah et al., 2022) is used to handle multiple navigation subtasks. SLING (Wasserman et al., 2022) uses mode switch to finish last-mile navigation. OVRL (Yadav et al., 2022) introduces offline visual representation learning before training agents.\n\nFollowing VGM (Kwon et al., 2021), for all methods except ANS and NTS, the agent is first trained using imitation learning, minimizing the negative log-likelihood of the ground-truth actions. Next, the agent is finetuned with proximal policy optimization (PPO) (Schulman et al., 2017) to improve the exploratory ability. The reward setting and auxiliary losses remain the same as in VGM. Each model is trained for five runs on a TITAN RTX GPU. The evaluation results for ANS and NTS are borrowed from the VGM paper (Kwon et al., 2021) 2\n\n5.3 QUANTITATIVE RESULTS\n\nComparison on Gibson. Table 1 shows that the MemoNav outperforms all the baselines in SR on the four difficulty levels. CNNLSTM exhibits the poorest performance as its limited memory provides insufficient information about the goal area. The MemoNav outperforms the metric map-based method ANS which requires pre-built maps, enjoying a large improvement in SR and SPL.\n\n2ANS is designed for exploration while NTS is not opensourced, so it is not straightforward to reproduce\n\nthem for multi-goal tasks.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Evaluation results on the Gibson (G) and Matterport3D (M) test datasets. We report the averages and standard deviations (in parentheses) by repeating experiments five times. (G*: evaluation on 4200 validation trajectories, SR: success rate (%), SPL: success weighted by path length (%)), PR: progress (%), PPL: progress weighted by path length (%))\n\nScene\n\nMethods\n\n1-goal\n\n2-goal\n\n3-goal\n\n4-goal\n\nSR\n\nSPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\n17.4(0.7) 21.6(1.2) 30.0(−) 43.0(−) 71.8(1.7) 74.7(1.5)\n\n22.0 41.3 55.4 62.2\n\n5.0(0.2) 14.6(1.1) 11.0(−) 26.0(−) 57.9(2.2) 58.1(1.4)\n\n18.8 26.9 37.4 36.6\n\nRandom CNNLSTM ANS NTS VGM MemoNav (ours)\n\nZER (ViewAug) OVRL SLING+OVRLGD MemoNav (ours)\n\nRandom CNNLSTM VGM MemoNav (ours)\n\nG\n\nG*\n\nM\n\n4.9(0.6) 14.5(1.7) -\n- 45.6(2.9) 47.0(2.4)\n\n0.8(0.1) 9.6(0.9) -\n- 18.7(1.5) 18.3(1.1)\n\n3.1(0.3) 9.0(1.1) -\n- 33.4(2.8) 35.8(2.9)\n\n0.4(0.1) 2.8(0.2) -\n- 8.3(0.8) 8.6(0.4)\n\n1.8(0.2) 6.4(0.9) -\n- 25.8(1.5) 27.5(1.0)\n\n0.2(0.0) 1.6(0.1) -\n- 5.0(0.4) 5.1(0.2)\n\n- -\n- -\n\n- -\n- -\n\n- -\n- -\n\n- -\n- -\n\n- -\n- -\n\n- -\n- -\n\n- -\n- -\n\n- -\n- -\n\n14.5(0.5) 16.2(1.3) 24.4(1.1) 26.0(1.0)\n\n5.5(0.3) 9.8(1.1) 16.3(0.7) 16.1(0.4)\n\n8.0(0.2) 10.8(0.9) 15.8(1.7) 18.2(0.7)\n\n0.9(0.0) 2.6(0.2) 4.7(0.3) 5.2(0.2)\n\n6.3(0.2) 7.7(0.8) 12.0(0.2) 13.3(0.2)\n\n0.5(0.0) 1.4(0.2) 2.8(0.2) 2.8(0.1)\n\nTable 2: Network component ablation results. Row 1 is the baseline model VGM (Kwon et al., 2021), and row 7 is our full model. The averages over five runs are reported in this table while the standard deviations are placed in . (G: the GATv2-based memory encoder, L: the long-term memory, F: forgetting nodes whose attention scores rank below 20%)\n\nComponents\n\n1-goal\n\n2-goal\n\n3-goal\n\n4-goal\n\nG\n\nL\n\nF\n\nSR\n\nSPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\n✓\n\n55.8(3.3) 1\n58.9(1.8) 2\n✓ 56.8(3.2) 3\n✓ ✓ 59.4(1.7) 4\n5 ✓ ✓ ✓ 60.7(1.9)\n\n47.1(1.9) 48.4(1.2) 47.0(2.1) 48.7(0.5) 49.0(1.0)\n\n45.6(2.9) 46.3(1.6) 45.8(2.9) 46.3(1.9) 47.3(2.4)\n\n18.7(1.5) 18.3(1.0) 18.6(1.2) 18.8(0.9) 18.9(1.1)\n\n33.4(2.8) 35.3(2.1) 33.7(2.9) 35.4(1.1) 35.8(1.4)\n\n8.3(0.8) 8.3(0.9) 8.0(0.9) 8.3(0.7) 8.6(0.4)\n\n25.8(2.7) 26.5(1.6) 25.9(3.3) 26.8(1.3) 27.5(1.0)\n\n5.0(0.4) 4.7(0.5) 5.1(0.5) 4.7(0.6) 5.1(0.2)\n\nCompared with the VGM, our model exhibits a noticeable performance gain, increasing the SR by 2.9%, 1.4%. 2.4%, and 1.7% on the 1, 2, 3, and 4-goal tasks respectively while using 20% fewer node features. This result indicates that the MemoNav benefits from the informative scene memory and the high-level scene representation contained in the WM, obtaining a higher success rate.\n\nThe MemoNav is also tested using 90-degree FoV sensors instead of panoramas (see G* in Table 1. The evaluation results on 4200 trajectories (G*) used by ZER also show that the MemoNav outperforms ZER, OVRL, and SLING.\n\nComparison on Matterport3D. We conduct an evaluation on the Matterport3D scenes to test the models’ ability to generalize to other scene types. Table 1 shows that our method achieves consistent performance improvements on this unseen scene dataset. Compared with VGM, the MemoNav witnesses performance gains in the success rate at the three difficulty levels, improving the SR/PR by 1.6% , 2.4%, and 1.3% on the 1, 2, and 3-goal tasks, respectively. These results show that our method is more capable of generalizing to new scene styles.\n\n5.4 ABLATION STUDIES AND ANALYSIS\n\nWe conduct ablation studies in the Gibson scenes to analyze the impact of each proposed component.\n\nPerformance gain of each proposed component. We ablate the three key components described in Section 4 and show the results in Table 2. Comparing row 2 with row 1, we can see that the LTM brings a noticeable improvement in SR/PR over the baseline. Comparing row 3 with row 1, applying the forgetting module achieves slight improvements in the SR/PR. However, its cooperation with the LTM witnesses larger increases (row 4 vs. row 1). More importantly, compared with the baseline (row 1), the synergy of the three components (row 5) increases the SR/PR by 0.049 (8.8%), 0.014 (3.1%),\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Ablation study of LTM. Row 1 is our default model. Row 2 shows the impact of replacing the LTM with a random feature in the STM. Row 3 is a variant that uses LTM to aggregates STM but does not incorporate the LTM into the WM. We report the averages and standard deviations (in parentheses) of five runs.\n\nMethods\n\n1-goal\n\n2-goal\n\n3-goal\n\n4-goal\n\nSR\n\nSPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\n1 2\n3\n\nMemoNav w/ random replace w/o decoding LTM\n\n60.7(2.1) 59.5(0.9) 59.8(1.6)\n\n49.0(1.9) 47.1(1.1) 48.4(1.3)\n\n47.0(2.4) 45.9(2.3) 46.0(2.4)\n\n18.3(1.1) 17.3(1.3) 17.5(1.0)\n\n35.8(1.4) 35.0(1.3) 35.6(1.5)\n\n8.6(0.4) 6.7(0.4) 7.9(0.3)\n\n27.5(1.0) 26.8(1.5) 27.0(1.0)\n\n5.1(0.2) 4.1(0.3) 4.8(0.4)\n\n(a) SPL/PPL vs. p\n\n(b) SR/PR vs. p\n\n(c) Average steps vs. p\n\nFigure 4: Navigation performance versus forgetting threshold p. (Averaged over five runs)\n\n0.024 (7.2%), 0.017 (6.6%) at the 1, 2, 3 and 4-goal levels respectively. These results demonstrate that the three components help to solve long-time navigation tasks with multiple sequential goals.\n\nImportance of LTM. Table 3 presents the results of ablation experiments for the LTM (described in Section 4.2). The first ablation experiment (row 2) replaces the LTM feature with a randomly selected STM feature each time the GATv2 encoding is finished, so that the MemoNav cannot use the scene-level representation in the LTM. Row 2 shows that the navigation performance becomes worse at all difficulty levels compared with the full model (row 1). Furthermore, when the LTM feature is not incorporated into the WM (row 3), the performance also deteriorates. In summary, the LTM stores a scene-level feature essential for improving the success rate.\n\nCorrelation between navigation performance and forgetting threshold. We evaluate our model with different forgetting thresholds p (see Section 4.1). The results are shown in Figure 4. For clarity, the figure shows the performance differences between our full model (with the forgetting module, the LTM, and the GATv2 encoder) and the variants. The four levels exhibit different trends. When p increases (i.e. a larger fraction of STM is not incorporated into the WM), our model witnesses first increases and then drops in the SR/PR and SPL/PPL at the 3 and 4-goal levels while enjoying slight gains in these metrics at the 1 and 2-goal levels. At the 1-goal level, our model obtains the highest SR and SPL when p = 80%, which means that a large fraction of node features in the map are useless when the navigation task is easy. A similar trend can also be seen at the 2-goal level. In contrast, our model exhibits an increase in the PR and PPL at the 4-goal level when p rises from 0% to 40%. However, if p rises to 80%, the two metrics see a precipitous decline, and the agent takes more than 20 steps to complete the tasks. These results suggest that excluding redundant STM from the WM improves navigation performance in multi-goal tasks. However, excluding an excessive fraction forces the agent not to utilize what it has explored, thus undermining the navigation performance.\n\n5.5 VISUALIZATION RESULTS\n\nTo observe how the MemoNav improves the navigation performance, we show example episodes of CNNLSTM, VGM and our model in the Gibson scenes in Fig 5 (See Appendix for more). We can see that CNNLSTM takes numerous steps to explore and tends to fail in complex scenes. VGM tends to spend a large proportion of time going in circles in narrow pathways. In contrast, the trajectories of the MemoNav are shorter and smoother. For instance, the 2nd and 3rd columns in Figure 5 show\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Visualization of example episodes. we compare selected episodes of CNNLSTM, VGM, and MemoNav at four difficulty levels in the Gibson scenes and visualize the top-down views. The number of navigation steps (the upper limit is 500) are shown at the bottom of each top-down view. Best viewed in color.\n\nFigure 6: Histograms of normalized path lengths of successful episodes for VGM (Kwon et al., 2021) and our model (averaged over 5 runs). Our model has more short trajectories while VGM has a larger proportion of long episodes.\n\nthat the VGM agent is trapped in a bottleneck connecting two rooms, while our agent plans efficient routes. This comparison shows that the MemoNav is more capable of escaping from deadlock.\n\nWe next draw a histogram of the normalized path lengths of successful episodes for VGM and our model to further investigate to what degree our model escapes from deadlock. The normalized path length is calculated as lnorm = li/ max(pi, li) − 1 where pi and li are defined in Section 5.1. lnorm indicates how many extra steps the agent takes compared to the shortest path. A larger lnorm represents a less efficient navigation episode. The histogram is shown in Figure 6. The figure shows that the histogram of our model exhibits a less heavier-tailed distribution, especially at the 4-goal level. This distribution means that our model has fewer episodes with an extremely large number of steps and agrees with the qualitative comparison.\n\n6 CONCLUSION AND FUTURE WORK\n\nThis paper proposes MemoNav, a novel memory model for ImageNav. This model flexibly retains the informative fraction of the short-term navigation memory via a forgetting module. We also introduce an extra global node as long-term memory to learn a scene-level representation. The retained short-term memory and the long-term memory are encoded by a graph attention module to generate the working memory that is used for generating action. The experimental results show that the MemoNav outperforms the baselines in multi-goal tasks, exhibits better transferability, and is more capable of escaping from deadlock. For future work, we will try to design trainable forgetting modules to better assess which fraction of the agent’s memory is informative.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nWe provide our source code and model configuration files in the supplementary material. We also add the python script used for generating our multi-goal Gibson datasets to the supplementary material. Any detail about data processing can be found in this script. For example, the rules used to generate multi-goal samples (described in Section 5.1) are coded in the script.\n\nLICENSES FOR REFERENCED DATASETS\n\nGibson: http://svl.stanford.edu/gibson2/assets/GDS_agreement.pdf\n\nMatterprt3D: http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf\n\nREFERENCES\n\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268–284, 2020.\n\nZiad Al-Halah, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Zero experience required: Plug & play modular transfer learning for semantic visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17031–17041, 2022.\n\nPeter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018.\n\nAlan Baddeley. Working memory: theories, models, and controversies. Annual review of psychology,\n\n63:1–29, 2012.\n\nEdward Beeching, Jilles Dibangoye, Olivier Simonin, and Christian Wolf. Learning to plan with uncertain topological maps. In ECCV 2020-16th European Conference on Computer Vision, pp. 1–24, 2020.\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n\narXiv preprint arXiv:2004.05150, 2020.\n\nKara J Blacker, Steven M Weisberg, Nora S Newcombe, and Susan M Courtney. Keeping track of where we are: Spatial working memory in navigation. Visual Cognition, 25(7-8):691–702, 2017.\n\nShaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=F72ximsx7C1.\n\nAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In 2017 International Conference on 3D Vision (3DV), pp. 667–676. IEEE, 2017.\n\nDevendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, and Ruslan Salakhutdinov. Learning to explore using active neural slam. In International Conference on Learning Representations (ICLR), 2020a.\n\nDevendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta. Neural topological slam for visual navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12875–12884, 2020b.\n\nKevin Chen, Junshen K Chen, Jo Chuang, Marynel V ́azquez, and Silvio Savarese. Topological planning with transformers for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11276–11286, 2021.\n\nTao Chen, Saurabh Gupta, and Abhinav Gupta. Learning exploration policies for navigation. In International Conference on Learning Representations, 2019. URL https://openreview. net/pdf?id=SyMWn05F7.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nNelson Cowan. What are the differences between long-term, short-term, and working memory?\n\nProgress in brain research, 169:323–338, 2008.\n\nK Anders Ericsson and Walter Kintsch. Long-term working memory. Psychological review, 102(2):\n\n211, 1995.\n\nKuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for embodied agents in long-horizon tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 538–547, 2019.\n\nKeisuke Fukuda and Edward K Vogel. Human variation in overriding attentional capture. Journal of\n\nNeuroscience, 29(27):8726–8733, 2009.\n\nYicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln bert: A recurrent vision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1643–1653, 2021.\n\nObin Kwon, Nuri Kim, Yunho Choi, Hwiyeon Yoo, Jeongho Park, and Songhwai Oh. Visual graph memory with unsupervised representation for visual navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15890–15899, 2021.\n\nAndrew Lampinen, Stephanie Chan, Andrea Banino, and Felix Hill. Towards mental time travel: a hierarchical memory for reinforcement learning agents. Advances in Neural Information Processing Systems, 34, 2021.\n\nRicky Loynd, Roland Fernandez, Asli Celikyilmaz, Adith Swaminathan, and Matthew Hausknecht. Working memory graphs. In International Conference on Machine Learning, pp. 6404–6414. PMLR, 2020.\n\nArjun Majumdar, Gunnar A Sigurdsson, Robinson Piramuthu, Jesse Thomason, Dhruv Batra, and Gaurav S Sukhatme. Ssl enables learning from sparse rewards in image-goal navigation. In International Conference on Machine Learning, pp. 14774–14785. PMLR, 2022.\n\nLina Mezghani, Sainbayar Sukhbaatar, Thibaut Lavril, Oleksandr Maksymets, Dhruv Batra, Piotr Bojanowski, and Karteek Alahari. Memory-augmented reinforcement learning for image-goal navigation. arXiv preprint arXiv:2101.05181, 2021.\n\nEmilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement\n\nlearning. In International Conference on Learning Representations, 2018.\n\nAlexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15942–15952, 2021.\n\nSanthosh Kumar Ramakrishnan, Tushar Nagarajan, Ziad Al-Halah, and Kristen Grauman. EnIn International Conference on Learning\n\nvironment predictive coding for visual navigation. Representations, 2022. URL https://openreview.net/forum?id=DBiQQYWykyy.\n\nSamuel Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matthew Botvinick, and David Raposo. Rapid task-solving in novel environments. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=F-mvpFpn_0q.\n\nNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for\n\nnavigation. In International Conference on Learning Representations, 2018.\n\nManolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9339–9347, 2019.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learning, pp. 9902–9912. PMLR, 2021.\n\nEndel Tulving. Memory and consciousness. Canadian Psychology/Psychologie canadienne, 26(1):1,\n\n1985.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nSaim Wani, Shivansh Patel, Unnat Jain, Angel Chang, and Manolis Savva. Multion: Benchmarking semantic map memory using multi-object navigation. Advances in Neural Information Processing Systems, 33:9700–9712, 2020.\n\nJustin Wasserman, Karmesh Yadav, Girish Chowdhary, Abhinav Gupta, and Unnat Jain. Last-mile\n\nembodied visual navigation. In 6th Annual Conference on Robot Learning, 2022.\n\nFei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9068–9079, 2018.\n\nKarmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-Pierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and Oleksandr Maksymets. Offline visual representation learning for embodied navigation. arXiv preprint arXiv:2204.13226, 2022.\n\nYuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE international conference on robotics and automation (ICRA), pp. 3357–3364. IEEE, 2017.\n\nA APPENDIX\n\nA.1 LIMITATIONS\n\nWhile the MemoNav witnesses a large improvement in the navigation success rate in multi-goal navigation tasks, it still encounters limitations. The proposed forgetting module is a post-processing method, as it obtains the attention scores of the decoder before deciding which nodes are to be forgotten. Future work can explore trainable forgetting modules. The second limitation is that our forgetting module does not reduce memory footprint, since the features of the forgotten nodes still exist in the map for localization. Moreover, the forgetting threshold in our experiments is fixed. Future work can merge our idea with Expire-span (Sukhbaatar et al., 2021) to learn an adaptive forgetting threshold.\n\nA.2 POTENTIAL IMPACT\n\nThe notable potential of negative societal impact from this work: our model is trained on 3D scans of the Gibson scenes which only contain western styles. This inadequacy of diverse scene styles may render our model biased and incompatible with indoor environments in unseen styles. As a result, our model may be only available in a small fraction of real-life scenes. If our model is transferred to out-of-distribution scenes, the agent may take more steps and even bump on walls frequently.\n\nA.3 REPRESENTATIVE MODELS OF WORKING MEMORY IN HUMAN BRAIN\n\nCowan et al. (Cowan, 2008) proposed a typical model describing the relationships among longterm memory (LTM), short-term memory (STM), and working memory (WM) of the human brain. According to their definitions, LTM is a large knowledge base and a record of prior experience; STM reflects faculties of the human mind that hold a limited amount of information in a very accessible state temporarily; WM includes part of STM and other processing mechanisms that help to utilize STM. Cowan et al. designed a framework depicting how WM is formed from STM and LTM (shown in Figure 7). This framework demonstrates that STM is derived from a temporarily activated subset\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 7: The memory model by Cowan et al. (Cowan, 2008). This figure is borrowed and adapted from its original paper.\n\nof LTM. This activated subset may decay as a function of time unless it is refreshed. The useful fraction of STM is incorporated into WM via an attention mechanism to avoid misleading distractions. Subsequent work by Baddeley et al. (Baddeley, 2012) suggests that the central executive manipulates memory by incorporating not only part of STM but also part of LTM to assist in making a decision.\n\nWe draw inspiration from the work by Cowan et al. (Cowan, 2008) and Baddeley et al. (Baddeley, 2012) and reformulate the agent’s navigation experience as the three types of memory defined above.\n\nThe parallel between the MemoNav and the two relevant models above is shown in the following list:\n\n• The map node features are termed “STM”, since they are local and transient.\n\n• The topological map of the MemoNav maintains a 100-node queue to store map nodes. This design simulates STM that holds a limited amount of information in a very accessible state temporarily in the human brain.\n\n• The MemoNav introduces a global node aggregating prior observation features stored in the\n\ntopological map, thereby simulating LTM which acts as a large knowledge base.\n\n• The MemoNav utilizes a forgetting mechanism to remove a fraction of STM with attention scores lower than a threshold. This mechanism acts as a simple way of decaying STM.\n\n• The forgetting mechanism helps WM include part of STM.\n\n• The MemoNav incorporates the retained STM and the LTM into WM, which is subsequently used to generate navigation actions. This design simulates the working memory model by (Baddeley, 2012).\n\nA.4\n\nIMPLEMENTATION DETAILS\n\nA.4.1\n\nIMPLEMENTATION OF MEMONAV\n\nThe structure of the memory decoding module (Figure 2(f) in the main paper) in the MemoNav remains the same as in the VGM (Kwon et al., 2021) and is shown in Figure 8. We maintain the module hyper-parameters specified in the supplementary of the VGM paper. The forgetting module on the MemoNav requires the attention scores generated in the decoder Dtarget. Therefore, our model needs to calculate the whole navigation pipeline before deciding which fraction of the STM should be retained. This lag means that the retained STM is incorporated into the WM at the next time step. The pseudo-code of the MemoNav is shown in Algorithm 1\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 8: The structure of the memory decoding module displayed in Figure 2 (f) of the main paper. Mw denotes the working memory. The attention scores α generated in Dtarget are used in the forgetting module to retain informative STM.\n\nAlgorithm 1: The implementation of the MemoNav Data: Empty topological map G = {V, E}, target image Itarget, current time step t, forgetting percentage p, trainable observation encoder Fenc, GATv2-based encoder GATv2, Transformer decoders Dtarget and Dcur, LSTM-based policy network LSTM\n\nResult: Navigation action at\n\n1 Long-term memory nglobal ← 0 ∈ Rd; 2 Attention scores for graph nodes V : α ← 0 ∈ R|V |; 3 while not AgentCallStop () do\n\n// Step 1: Update the topological map It ← GetCurrentPanorama(); G. UpdateMap(It); // Step 2: Retain the informative fraction of the STM Forgotten number n ← Floor (p · |V|); Sorted indices i ← Argsort(α); Forgotten indices if orgotten ← i [0 : n]; G. RemoveNodes(if orgotten); // Step 3: Memory encoding and decoding V ∈ R|V|×d ← G. GetNodeFeatures (); Working memory Mw ← GATv2({V , nglobal}); ecur ← Fenc(It), etarget ← Fenc(Itarget); fcur ← Dcur (ecur, Mw) , ftarget ← Dtarget(etarget, Mw); α ← Dtarget. GetAttScores() // Step 4: Action generation x ← LSTM(FC([fcur, ftarget, ecur])); p (at | x) = σ(FC(x)); at ← SampleFromDistribution(p(at | x));\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\n16\n\n17 18 end\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 9: Reward curves of the MemoNav and VGM during the RL training phase.\n\nA.4.2 REPRODUCTION OF CNNLSTM\n\nWe reproduce CNNLSTM (Zhu et al., 2017) following the description in its original paper, but we also make some modifications to keep the comparison fair. We replace the ResNet-50 in CNNLSTM with the pretrained RGB-D encoder of VGM (Kwon et al., 2021). We also add positional embeddings to the encoded RGB-D observations to contain temporal information. Moreover, we concatenate the encoded RGB-D observations with the goal image embedding and project the concatenated feature (1024D) to a 512D feature, so that CNNLSTM can utilize the information of the goal image. The projected features of four consecutive frames are further condensed and then input to a policy network as in (Zhu et al., 2017). To use the two auxiliary tasks proposed in VGM (Kwon et al., 2021), we also introduce the linear projection layers (Linear-ReLU-Linear) used in VGM to process the embedded goal image and embedded current observation. With these modifications, the comparison between CNNLSTM and MemoNav (ours) is fair.\n\nA.4.3 TRAINING DETAILS\n\nWe follow the two-step training routine and maintain the training hyper-parameters in the VGM paper (Kwon et al., 2021). Firstly, CNNLSTM, VGM, and MemoNav (ours) are all trained using imitation learning for 20k steps. Afterward, we finetune these models using PPO (Schulman et al., 2017) for 10M steps. Due to the performance fluctuation intrinsic to reinforcement learning, the model at the 10M-th step is probably not the best. Therefore, we evaluate all checkpoints in the step range [9M, 10.4M] and select the best one.\n\nA recent study (Yadav et al., 2022) has pointed out that training via PPO does not converge till 500M frames and results at 10M are highly sensitive to initialization. For better comparing the MemoNav and VGM, these two methods are trained for 10M more steps. The reward curves during the training phase is shown in Figure 9. This figure shows that the reward gain obtained by MemoNav becomes larger when the number of training steps increases from 10M to 20M.\n\nA.4.4 EXTRA INFORMATION ABOUT THE MULTI-GOAL DATASETS\n\nWe follow the format of the public 1-goal dataset in (Mezghani et al., 2021) and create 2-goal, 3goal, and 4-goal test datasets on the Gibson (Xia et al., 2018) scenes. We generate 50 samples for each of the 14 test scenes. In each sample, we randomly choose target positions while still following five rules: (1) no obstacles appear inside a circle with a radius of 0.75 meters centered at each target; (2) the distance between two successive targets is no more than 10 meters; (3) all targets are placed on the same layer without altitude differences. (4) all targets are reachable from each other. (5) The final target is placed near a certain previous one with the distance between them smaller than 1.5 meters. The distributions of the total geodesic distances for the three difficulty levels are shown in Figure 10.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 10: Histograms of geodesic distances for the multi-goal test datasets.\n\nA.4.5 COMPUTE REQUIREMENTS\n\nWe utilize an RTX TITAN GPU for training and evaluating our models. The imitation learning phase takes 1.5 days to train while the reinforcement learning takes 5 days.\n\nThe computation in the GATv2-based encoder and the two Transformer decoders occupy the largest proportion of the run-time of the MemoNav. The computation complexity of the encoder and the decoders are O(|V|d2 + |E|d) and O (|V|d), respectively. Using the forgetting module with a percentage threshold p, the computation complexity of the MemoNav can be flexibly decreased by reducing the number of nodes to p|V|.\n\nA.5 EXTRA ABLATION STUDIES\n\nWe conduct additional ablation experiments on the forgetting module in the MemoNav to further investigate how its design affects the agent’s navigation performance. The results are shown in Table 4.\n\nOrigin of attention scores used by forgetting module. The forgetting module in the MemoNav removes the uninformative STM according to the attention scores generated in Dtarget, as described in Section 4.2.1. We change these scores to those generated in Dcur. The result (row 2) shows that using the attention scores generated in Dcur leads to slightly worse performance.\n\nWe also remove STM according to the average of the scores generated in Dcur and those in Dtarget. The result (row 3) demonstrates that this variant obtains lower SR and SPL in the 1-goal setting but larger PR and PPL in harder tasks (i.e., 3 and 4-goal tasks). We hypothesize that this phenomenon occurs because averaging scores can include context information close to the agent, and not just the goal. Easy tasks require less memory, so including too many scene features around the agent leads to frequent changes in the output actions, thereby undermining the performance in 1 and 2-goal settings. In contrast, harder tasks require the agent to utilize its context information to plan a short path to visited areas, so retaining memory informative for both the current position and the goal benefits navigation performance.\n\nEffectiveness of forgetting module. This ablation investigates whether it is effective to exclude the STM with attention scores ranking below the predefined percentage p. ”Effectiveness” here means that the forgetting module does retain useful node features. In this experiment,the forgetting module excludes a random fraction of the STM. We test this ablation model over five random seeds and report the average metrics. The result (row 3) shows that incorporating a random fraction of the STM into the WM leads to decreases in all metrics, which validates the effectiveness of our design for retaining informative STM.\n\nTo better understand how the forgetting module works, we conduct an additional statistical experiment. In this experiment, five distance metrics are calculated: (a) distance from a node to the agent, (b) distance from a node to the goal, (c) distance from a node to the oracle shortest path, (d) distance from a node to the shortest path segments closer to the agent, and (e) distance from a node to the shortest path segments closer to the current goal. Then the histograms of these five metrics are drawn according to the metrics records for each forgotten/retained node at each time step so that we can see how far away these nodes are from the agent, the goal, and the shortest path. Please see Figure 11 to better understand the definitions of distance metrics (c)(d)(e).\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 11: The visualization of distance metrics (c), (d), and (e).\n\nTable 4: Extra ablation studies of the forgetting module.\n\nMethods\n\n1-goal\n\n2-goal\n\n3-goal\n\n4-goal\n\nSR\n\nSPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\n1 2\n3 4\n\nMemoNav w. Dcur att. scores w. averaged scores w. Random STM\n\n60.7(2.1) 60.4(1.4) 60.1(0.6) 59.7(2.0)\n\n49.0(1.9) 48.6(1.4) 48.4(0.6) 44.6(1.7)\n\n47.0(2.4) 46.9(1.8) 46.9(1.2) 44.9(1.3)\n\n18.3(1.1) 18.2(1.0) 18.4(0.6) 15.3(0.9)\n\n35.8(1.4) 35.7(1.5) 36.7(1.2) 33.1(1.2)\n\n8.6(0.4) 8.6(0.3) 8.8(0.4) 8.1(0.4)\n\n27.5(1.0) 28.4(1.4) 28.4(1.5) 25.7(1.1)\n\n5.1(0.2) 5.2(0.2) 5.4(0.2) 4.8(0.2)\n\nWe evaluate the MemoNav on the 3-goal Gibson task and draw the histograms on per-goal basis with the average results of five different seeds, as shown in Figure 12. The figure provides two interesting findings:\n\n• The distance distribution patterns for forgotten nodes (green) and retained ones (orange) vary across goals. The distributions of the distances from forgotten nodes to goals (column 2) and to shortest path segments near goal (column 5) become uniform when the agent navigates to the third goal. In contrast, these two histograms for the retained nodes become sharper and the peaks shift to smaller distance values. This phenomenon occurs because the forgetting module selectively retains map nodes closer to and informative for the current goal.\n\n• The forgetting module has a larger impact on the distance metrics when the navigation task becomes more difficult. Specifically, when the current goal index is 1 (i.e. the task is easy), the averages of the distance metrics for forgotten nodes and retained nodes are close. When the goal index rises to 3 (i.e. the task becomes harder), a larger proportion of the retained nodes are close to the goal, the shortest path, and the shortest path segments near goal while a larger proportion of the forgotten nodes are close to the agent, and the shortest path segments near agent. This difference suggests that MemoNav improves navigation performance by focusing more on the region along the shortest path and around the goal area.\n\nThese results empirically validate that the information useful for path planning are not totally lost by the forgetting module.\n\nExtra Validation of LTM. In order to observe how the learned scene-level representation in the LTM benefits the MemoNav, we replace the LTM feature with the average of all STM features. After replacing, the LTM no longer contains a scene-level representation, but it still facilitates message passing. The results in Table 5 shows that the MemoNav without the scene-level representation in the LTM witnesses drops in all metrics. This experiment suggests that the learned scene-level representation in the LTM contains important information that helps to improve navigation performance.\n\nA.6 EXTRA VISUALIZATION RESULTS\n\nMore examples of multi-goal episodes are displayed in Figure 13(a). The agent efficiently explores the scenes and finds sequential goals using the informative fraction of the node features in the map.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Histograms of the five distance metrics defined in Section A.5. The data of these metrics is collected by evaluating the MemoNav on the 3-goal task in the Gibson scenes and averaged over five runs. The upper row (green) and lower row (orange) belong to the forgotten nodes and retained ones, respectively.\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Extra ablation studies of the LTM.\n\nMethods\n\n1-goal\n\n2-goal\n\n3-goal\n\n4-goal\n\nSR\n\nSPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\nPR\n\nPPL\n\n1 2\n\nMemoNav w. averaging STM as LTM\n\n60.7(2.1) 58.1(1.6)\n\n49.0(1.9) 47.2(1.5)\n\n47.0(2.4) 45.1(1.6)\n\n18.3(1.1) 17.6(0.8)\n\n35.8(1.4) 34.4(1.3)\n\n8.6(0.4) 8.1(0.3)\n\n27.5(1.0) 26.4(0.8)\n\n5.1(0.2) 4.5(0.2)\n\nThese examples show that the MemoNav agent focuses high attention only on a small fraction of nodes and excludes nodes that are far away from the current goal. For example, in the 3-goal example, the agent forgets the topmost node when navigating to the 1st goal since this node is the farthest from the goal; the agent forgets the nodes at the bottom left corner when navigating to the 2nd and 3rd goals since these nodes are remote and uninformative. The comparison with the baseline for these examples is recorded in the supplementary videos.\n\nFailure case analysis. We present examples of failed episodes in Figure 13(b) and record the proportions of various failure modes at all difficulty levels. The failure modes can mainly be categorized into four types: Stopping mistakenly, Missing the goal, Not close enough, and Overexploring. The mode Stopping mistakenly means that the agent implements stop at the wrong place. The mode Missing the goal means that the agent has observed the goal but passes it. The mode Not close enough means that the agent attempts to reach the goal it sees but implements stop outside the successful range. The mode Over-exploring means that the agent spends too much time exploring open areas without any goals. The largest probability lies in Over-exploring cases, most of which occur when the agent explores a large proportion of the scene but still fails to get close to the target area in a limited time.\n\nVisualization comparison between MemoNav with and without forgetting. We compare the trajectories of the MemoNav with and without the forgetting module to observe how this module affects the characteristics (e.g. smoothness and length) of the trajectories. Figure 14 shows that the trajectories of MemoNav using the forgetting module are smoother and shorter. Without the forgetting module, the trajectories tends to possess many tiny sharp turns and are longer. We hypothesize that a fraction of the STM is uninformative for navigation. This fraction may acts as noise and make the policy network frequently change its output actions. The forgetting module adaptively excludes this disturbing fraction of STM so that the policy network is able to use helpful navigation memory for decision-making.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a)\n\n(b)\n\nFigure 13: (a) Examples of multi-goal navigation episodes. Each example shows both the topological map and the trajectory. The graph nodes are incrementally added to the map by the agent and selectively retained by the forgetting module in the MemoNav. The yellow downward arrow denotes the current localized node of the agent. (b) Examples of failed episodes. The agent encounters four major failure mode: Stopping mistakenly, Missing the goal, Not close enough, and Over-exploring.\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 14: Visualization comparing the MemoNav with and without the forgetting module. We compare selected episodes at four difficulty levels in the Gibson scenes and visualize the top-down views. The number of navigation steps (the upper limit is 500) are shown at the bottom of each top-down view. Best viewed in color.\n\n21",
    "reference": "# Summary Of The Paper\n\nThis paper builds upon the VGM (Kwon et al., 2021) for image navigation. By forgetting some less informative nodes in the topological map through the attention mechanism, the short-term memory (STM) can be updated dynamically. The selected STMs are fused to obtain the scene-level feature with Long-term memory (LTM), and which are encoded through graph attention to generate the Working Memory (WM). The WM is finally used to generate the actions for navigation. The experiments of both 1-goal and multi-goal tasks are conducted in two public datasets Gibson and Matterpot3D in Habitat simulator.\n\n# Strength And Weaknesses\n\nStrengths:\n1.\tThe paper is generally well-written and easy to follow.\n2.\tThe experiments and analysis are extensive with the ablation experiments and baselines.\n3.\tThe selective forgetting module seems capable of retaining the informative nodes in the topological map and forget the redundant nodes, which may be helpful for improving the SR/PR for 1-goal and multi-goal tasks.\n\nWeaknesses:\n1.\tThe MemoNav seems to be trained in the 1-goal setting, while used in both 1-goal and multi-goal tasks, is it suitable to use this model to evaluate the multi-goal task? It is worth discussing training different models (under multi-goal settings) to validate for multi-goal tasks.\n2.\tThe proposed MemoNav follows the VGM method of constructing the topological maps of the environments. Considering the topological maps can somehow memorize the observations during navigation as well, the advantages of proposed memory based modules are not that effective compared to the baseline, also the close SPL/PPL results are obtained compared the baseline VGM under 1-goal and multi-goal tasks. \n3.\tIt seems the regularity and relevance are not that significant in Figure 4. The 2-goal task shows a sharp drop in PP at p=0.4, and a dramatic increase at p=0.6, what are the reasons for this phenomenon?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is well-written, and the contributions of the paper are easy to follow. Compared with the current methods, this paper contributes to node selection and encoding, but less in constructing topological maps. The code is submitted, the paper should be reproducible.\n\n# Summary Of The Review\n\nI think this is a practical work, the motivation and idea are interesting, and the experimental results are also sufficient. However, I worry about the that novelty of the technical contribution over baseline (VGM) seems limited. And compared to the results of VGM, the improvement of the proposed method seems marginal in some parts.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nEVA3D: COMPOSITIONAL 3D HUMAN GENERATION FROM 2D IMAGE COLLECTIONS\n\nFangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, Ziwei Liu (cid:0) S-Lab, Nanyang Technological University {fangzhou001, zhaoxi001, yushi001, liang.pan, ziwei.liu}@ntu.edu.sg\n\nFigure 1: EVA3D generates high-quality and diverse 3D humans with photo-realistic RGB renderings and detailed geometry. Only 2D image collections are used for training.\n\nABSTRACT\n\nInverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512 × 256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves stateof-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to “inversegraphics” diverse human bodies with a clean framework. Our code is publicly available at https://github.com/hongfz16/EVA3D.\n\n1\n\nINTRODUCTION\n\nInverse graphics studies inverse-engineering of projection physics, which aims to recover the 3D world from 2D observations. It is not only a long-standing scientific quest, but also enables nu-\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nmerous applications in VR/AR and VFX. Recently, 3D-aware generative models (Chan et al., 2021; Or-El et al., 2022; Chan et al., 2022) demonstrate great potential in inverse graphics by learning to generate 3D rigid objects (e.g. human/animal faces, CAD models) from 2D image collections. However, human bodies, as articulated objects, have complex articulations and diverse appearances. Therefore, it is challenging to learn 3D human generative models that can synthesis animatable 3D humans with high-fidelity textures and vivid geometric details.\n\nTo generate high-quality 3D humans, we argue that two main factors should be properly addressed: 1) 3D human representation; 2) generative network training strategies. Due to the articulated nature of human bodies, a desirable human representation should be able to explicitly control the pose/shape of 3D humans. With an articulated representation, a 3D human is modeled in its canonical pose (canonical space), and can be rendered in different poses and shapes (observation space). Moreover, the efficiency of the representation matters in high-quality 3D human generation. Previous methods (Noguchi et al., 2022; Bergman et al., 2022) fail to achieve high resolution generation due to their inefficient human representations.\n\nIn addition, training strategies could also highly influence 3D human generative models. The issue mainly comes from the data characteristics. Compared with datasets used by Noguchi et al. (2022) (e.g. AIST (Tsuchida et al., 2019)), fashion datasets (e.g. DeepFashion (Liu et al., 2016)) are more aligned with real-world human image distributions, making a favorable dataset choice. However, fashion datasets mostly have very limited human poses and highly imbalanced viewing angles. This imbalanced 2D data distribution could hinder 3D GAN learning, leading to difficulties in novel view/ pose synthesis. Therefore, a proper training strategy is in need to alleviate the issue.\n\nIn this work, we propose EVA3D, an unconditional high-quality 3D human generative model from sparse 2D human image collections only. To facilitate that, we propose a compositional human NeRF representation to improve the model efficiency. We divide the human body into 16 parts and assign each part an individual network, which models the corresponding local volume. Our human representation mainly provides three advantages. 1) It inherently describes the human body prior, which supports explicit control over human body shapes and poses. 2) It supports adaptively allocating computation resources. More complex body parts (e.g. heads) can be allocated with more parameters. 3) The compositional representation enables efficient rendering and achieves high-resolution generation. Rather than using one big volume (Bergman et al., 2022), our compositional representation tightly models each body part and prevents wasting parameters on empty volumes. Moreover, thanks to the part-based modeling, we can efficiently sample rays inside local volumes and avoid sampling empty spaces. With the compact representation together with the efficient rendering algorithm, we achieve high-resolution (512 × 256) rendering and GAN training without using super-resolution modules, while existing methods can only train at a native resolution of 1282. Moreover, we carefully design training strategies to address the human pose and viewing angle imbalance issue. We analyze the head-facing angle distribution and propose a pose-guided sampling strategy to help effective 3D human geometry learning.\n\nQuantitative and qualitative experiments are performed on two fashion datasets (Liu et al., 2016; Fu et al., 2022) to demonstrate the advantages of EVA3D. We also experiment on UBCFashion (Zablotskaia et al., 2019) and AIST (Tsuchida et al., 2019) for comparison with prior work. Extensive experiments on our method designs are provided for further analysis. In conclusion, our contributions are as follows: 1) We are the first to achieve high-resolution high-quality 3D human generation from 2D image collections; 2) We propose a compositional human NeRF representation tailored for efficient GAN training; 3) Practical training strategies are introduced to address the imbalance issue of real 2D human image collections. 4) We demonstrate applications of EVA3D, i.e. interpolation and GAN inversion, which pave way for further exploration in 3D human GAN.\n\n2 RELATED WORK\n\n3D-Aware GAN. Generative Adversarial Network (GAN) (Goodfellow et al., 2020) has been a great success in 2D image generation (Karras et al., 2019; 2020). Many efforts have also been put on 3D-aware generation. Nguyen-Phuoc et al. (2019); Henzler et al. (2019) use voxels, and Pan et al. (2020) use meshes to assist the 3D-aware generation. With recent advances in NeRF (Mildenhall et al., 2020; Tewari et al., 2021), many have build 3D-aware GANs based on NeRF (Schwarz et al., 2020; Niemeyer & Geiger, 2021; Chan et al., 2021; Deng et al., 2022). To increase the generation\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nresolution, Gu et al. (2021); Or-El et al. (2022); Chan et al. (2022) use 2D decoders for super resolution. Moreover, it is desirable to lift the raw resolution, by improving the rendering efficiency, for more detailed geometry and better 3D consistency (Skorokhodov et al., 2022; Xiang et al., 2022; Schwarz et al., 2022; Zhao et al., 2022). We also propose an efficient 3D human representation to allow high resolution training.\n\nHuman Generation. Though great success has been achieved in generating human faces, it is still challenging to generate human images for the complexity in human poses and appearances (Sarkar et al., 2021b; Lewis et al., 2021; Sarkar et al., 2021a; Jiang et al., 2022c). Recently, Fu et al. (2022); Fr ̈uhst ̈uck et al. (2022) scale-up the dataset and achieve impressive 2D human generation results. For 3D human generation, Chen et al. (2022) generate human geometry using 3D human dataset. Some also attempt to train 3D human GANs using only 2D human image collections. Grigorev et al. (2021); Zhang et al. (2021) use CNN-based neural renderers, which cannot guarantee 3D consistency. Noguchi et al. (2022) use human NeRF (Noguchi et al., 2021) for this task, which only trains at low resolution. Bergman et al. (2022); Zhang et al. (2022a) propose to increase the resolution by super-resolution, which still fails to produce high-quality results. Hong et al. (2022b); Zhang et al. (2022b) generate 3D avatars and motions from text inputs.\n\n3D Human Representations. 3D human representations serve as fundamental tools for human related tasks. Loper et al. (2015); Pavlakos et al. (2019b) create parametric human models, for explicit modeling of 3D humans. To model human appearances, Habermann et al. (2021); Shysheya et al. (2019); Yoon et al. (2021); Liu et al. (2021) further introduce UV maps. Parametric modeling gives robust control over the human model, but less realism. Palafox et al. (2021) use implicit functions to generate realistic 3D human body shapes. Embracing the development of NeRF, the number of works about human NeRF has also exploded (Peng et al., 2021b; Zhao et al., 2021; Peng et al., 2021a; Xu et al., 2021; Noguchi et al., 2021; Weng et al., 2022; Chen et al., 2021; Su et al., 2021; Jiang et al., 2022a;b; Wang et al., 2022). Hong et al. (2022a) propose to learn modalinvariant human representations for versatile down-stream tasks. Cai et al. (2022) contribute a largescale multi-modal 4D human dataset. Some propose to model human body in a compositional way (Mihajlovic et al., 2022; Palafox et al., 2022; Su et al., 2022), where several submodules are used to model different body parts, and are more efficient than single-network ones.\n\nCompositional NeRF. The compositional representation has been long studied for its effectiveness and efficiency. It has also been applied to NeRF for object, scene and human modeling. Tancik et al. (2022); Kundu et al. (2022) model outdoor scene NeRF in an compositional way by splitting scenes into block or object levels. Yang et al. (2021); Driess et al. (2022); Wang et al. (2021) decompose multi-objects in a scene for further editing. Compositional NeRF generation has also been studied in prior arts (Niemeyer & Geiger, 2021; BR et al., 2022).\n\n3 METHODOLOGY\n\n3.1 PREREQUISITES\n\nNeRF (Mildenhall et al., 2020) is an implicit 3D representation, which is capable of photorealistic novel view synthesis. NeRF is defined as {c, σ} = FΦ(x, d), where x is the query point, d is the viewing direction, c is the emitted radiance (RGB value), σ is the volume density. To get the RGB value C(r) of some ray r(t) = o + td, namely volume rendering, we have the following formulation, C(r) = (cid:82) tf σ(r(s))ds) is the accumulated transmittance along the ray r from tn to t. tn and tf denotes the near and far bounds. To get the estimation of C(r), it is discretized as\n\nT (t)σ(r(t))c(r(t), d)dt , where T (t) = exp(− (cid:82) t\n\ntn\n\ntn\n\nˆC(r) =\n\nN (cid:88)\n\ni=1\n\nTi(1 − exp(−σiδi))ci, where Ti = exp(−\n\ni−1 (cid:88)\n\nj=1\n\nσjδj), δi = ti+1 − ti.\n\n(1)\n\nFor better geometry, Or-El et al. (2022) propose to replace the volume density σ(x) with SDF values d(x) to explicitly define the surface. SDF can be converted to the volume density as σ(x) = α−1sigmoid (−d(x)/α), where α is a learnable parameter. In later experiments, we mainly use SDF as the implicit geometry representation, which is denoted as σ for convenience.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Rendering Process of the Compositional Human NeRF Representation. For shape and pose specified by SMPL(β, θ), local bounding boxes are constructed. Rays that intersect with bounding boxes are sampled and transferred to the canonical space using inverse LBS. Subnetworks corresponding to bounding boxes are queried, results of which are integrated to produce final renderings.\n\nSMPL (Loper et al., 2015), defined as M (β, θ), is a parametric human model, where β, θ controls body shapes and poses. In this work, we use the Linear Blend Skinning (LBS) algorithm of SMPL for the transformation from the canonical space to observation spaces. Formally, point x in the canonical space is transformed to an observation space defined by pose θ as x′ = (cid:80)K k=1 wkGk(θ, J )x, where K is the joint number, wk is the blend weight of x against joint k, Gk(θ, J ) is the transformation matrix of joint k. The transformation from observation spaces to the canonical space, namely inverse LBS, takes a similar formulation with inverted transformation matrices.\n\n3.2 COMPOSITIONAL HUMAN NERF REPRESENTATION\n\nThe compositional human NeRF representation is defined as FΦ, corresponding to a set of local bounding boxes B. For each body part k, we use a subnetwork Fk ∈ FΦ to model the local bounding max} ∈ B, as shown in Fig. 2 b). For some point xi in the canonical coordinate with box {bk direction di and falling inside the k-th bounding box, the corresponding radiance ck i and density σk is queried by\n\nmin, bk\n\ni\n\n{ck\n\ni , σk\n\ni } = Fk( ˆxk\n\ni , di), where ˆxk\n\ni =\n\nmin + bk\n\nmax)\n\n2xi − (bk bk\n\nmax − bk\n\nmin\n\n.\n\n(2)\n\nIf the point xi falls in multiple bounding boxes Ai, a window function (Lombardi et al., 2021) is applied to linearly blend queried results. The blended radiance ci and density σi of xi is calculated as\n\n{ci, σi} =\n\n1 (cid:80) ωa\n\n(cid:88)\n\na∈Ai\n\nωa{ck\n\ni , σk\n\ni }, where ωa = exp(−m( ˆxk\n\ni (x)n + ˆxk\n\ni (y)n + ˆxk\n\ni (z)n)).\n\n(3)\n\nm, n are chosen empirically. Different from Palafox et al. (2022); Su et al. (2022), we only query subnetworks whose bounding boxes contain query points. It increases the efficiency of the query process and saves computational resources.\n\nTaking advantages of the compositional representation, we also adopt an efficient volume rendering algorithm. Previous methods need to sample points, query, and integrate for every pixel of the canvas, which wastes large amounts of computational resources on backgrounds. In contrast, for the compositional representation, we have pre-defined bounding boxes to filter useful rays, which is also the key for our method being able to train on high resolution.\n\nAs shown in Fig. 2, for the target pose θ, shape β and camera setup, our rendering algorithm R(FΦ, β, θ, cam) is described as follows. Firstly, ray r(t) = o + td is sampled for each pixel on the canvas. Then we transform the pre-defined bounding boxes B to the target pose θ using transformation matrices Gk defined by SMPL. Rays that intersect with the transformed bounding boxes are kept for further rendering. Others are marked to be the background color. For ray r(t) = o + td that intersects with single or multiple bounding boxes, we get the near and far bounds tn, tf .\n\n4\n\na)SampleRays&PointsatObservationSpaceSMPL(β,θ)b)InverseLBStoCanonicalSpaceF!F\"F#...c)TransformtoLocalBoundingBox&QuerySubnetworks[]c!\"σ!\"[]c$σ$InverseLBSd)IntegrateQueriedResultsAlongRaysω!∫[]c!\"σ!\"ω\"[]c%σ%x!x\"x\"!#x\"!$x\"\"%[]c&σ&[]c’σ’∫Published as a conference paper at ICLR 2023\n\nFigure 3: 3D Human GAN Framework. With the estimated SMPL and camera parameters distribution pest, 3D humans are randomly sampled and rendered conditioned on z ∼ pz. The renderings are used for adversarial training against real 2D human image collections preal.\n\nN points are randomly sampled on each ray as\n\n(cid:20)\n\nti ∼ U\n\ntn +\n\ni − 1 N\n\n(tf − tn), tn +\n\n(cid:21)\n\n(tf − tn)\n\n.\n\ni N\n\n(4)\n\nNext, we transform sampled points back to the canonical space using inverse LBS. Similar to Zheng et al. (2021), we inverse not only the pose transformation, but also the shape/ pose blend shapes BS(β), BP (θ) to be able to generalize to different body shapes. For sampled point r(ti), the nearest k points N = {v1...vk} are found among the vertices of the posed SMPL mesh M (β, θ). The transformation of point r(ti) from the observation space to the canonical space is defined as\n\n(cid:21)\n\n(cid:20)x0 i\n1\n\n=\n\n(cid:88)\n\nvj ∈N\n\nωj (cid:80) ωj\n\n(Mj)−1\n\n(cid:21)\n\n(cid:20)r(ti) 1\n\n, where Mj =\n\n(cid:32) K\n\n(cid:88)\n\nk=1\n\nwj\n\nkGk\n\n(cid:33) (cid:20)\n\nI Bj 0\n\nS + Bj I\n\nP\n\n(cid:21)\n\n.\n\n(5)\n\nωj = 1/∥r(ti) − vj∥ is the inverse distance weight. Mj is the transformation matrix of the SMPL vertex vj. Then we query the compositional human NeRF representation F with point x0 i to get its corresponding radiance ci and density σi as defined in Eq. 2 and 3. Finally, we integrate the queried results for the RGB value of ray r(t), as defined in Eq. 1.\n\n3.3\n\n3D HUMAN GAN FRAMEWORK\n\nWith the compositional human NeRF representation, we construct a 3D human GAN framework as shown in Fig. 3. The generator is defined as G(z, β, θ, cam; ΦG) = R(FΦG (z), β, θ, cam). Similar to pi-GAN (Chan et al., 2021), each subnetwork of FΦ consists of stacked MLPs with SIREN activation (Sitzmann et al., 2020). To generate fake samples, z ∼ pz is sample from normal distribution. {β, θ, cam} ∼ pest are sampled from the estimated distribution from 2D image collections. We use off-the-shelf tools (Pavlakos et al., 2019a; Kocabas et al., 2020) to estimate {β, θ, cam} for the 2D image collections. Unlike ENARF-GAN(Noguchi et al., 2022), where these variables are sampled from the distribution of motion datasets (Mahmood et al., 2019), the real 2D image collections do not necessarily share the similar pose distribution as that of motion datasets, especially for fashion datasets, e.g. DeepFashion, where the pose distribution is imbalanced. Finally, the fake samples If = G(z, β, θ, cam; ΦG), along with real samples Ir ∼ preal are sent to discriminator D(I; ΦD) for adversarial training. For more implementation details, please refer to the supplementary material.\n\n3.4 TRAINING\n\nDelta SDF Prediction. Real-world 2D human image collections, especially fashion datasets, usually have imbalanced pose distribution. For example, as shown in Fig. 6, we plot the distribution of facing angles of DeepFashion. Such heavily imbalanced pose distribution makes it hard for the network to learn correct 3D information in an unsupervised way. Therefore, we propose to introduce strong human prior by utilizing the SMPL template geometry dT (x) as the foundation of our human representation. Instead of directly predicting the SDF value d(x), we predict an SDF offset ∆d(x) from the template (Yifan et al., 2022). Then dT (x) + ∆d(x) is used as the actual SDF value of point x.\n\nPose-guided Sampling. To facilitate effective 3D information learning from sparse 2D image collections, other than introducing a 3D human template, we propose to balance the input 2D images\n\n5\n\np!\"#βθcam~SMPL(β,θ)camp$~zmapping~p%!&’DiscriminatorReal/Fake?θPose-GuidedSamplingQueryIntegrateSampleRayInverseLBSPublished as a conference paper at ICLR 2023\n\nFigure 4: Generation Results of EVA3D. The 3D-aware nature and inherent human prior of EVA3D enable explicit control over rendering views, human poses, and shapes.\n\nbased on human poses. The intuition behind the pose-guided sampling is that different viewing angles should be sampled more evenly to allow effective learning of geometry. Empirically, among all human joints, we use the angle of the head to guide the sampling. Moreover, facial areas contain more information than other parts of the head. Front-view angles should be sampled more than other angles. Therefore, we choose to use a Gaussian distribution centered at the front-view angle μθ, with a standard deviation of σθ. Specifically, M bins are divided on the circle. For an image with the head angle falling in bin m, its probability pm of being sampled is defined as\n\npm =\n\n1 √\n\n2π\n\nσθ\n\n(cid:32)\n\nexp\n\n−\n\n1 2\n\n(cid:18) θm − μθ σθ\n\n(cid:19)2(cid:33)\n\n, where θm =\n\n2πm M\n\n.\n\n(6)\n\nWe visualize the balanced distribution in Fig. 6. The network now has higher chances of seeing more side-views of human bodies, which helps better geometry generation.\n\nLoss Functions. For the adversarial training, we use the non-saturating GAN loss with R1 regularization (Mescheder et al., 2018), which is defined as\n\nLadv(ΦG, ΦD) = Ez∼pz,{β,θ,cam}∼pest [f (D(G(z, β, θ, cam; ΦG); ΦD))]\n\n+ EIr∼preal[f (−D(Ir; ΦD)) + λ|∇D(Ir; ΦD)|2],\n\n(7)\n\n(8)\n\nwhere f (u) = −log(1 + exp(−u)). Other than the adversarial loss, some regularization terms are introduced for the delta SDF prediction. Firstly, we want minimum offset from the template mesh to maintain plausible human shape, which gives the minimum offset loss Loff = Ex[∥∆d(x)∥2 2]. Secondly, to ensure that the predicted SDF values are physically valid (Gropp et al., 2020), we penalize the derivation of delta SDF predictions to zero Leik = Ex[∥∇(∆d(x))∥2 2]. The overall loss is defined as L = Ladv + λoffLoff + λeikLeik, where λ∗ are loss weights defined empirically.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\nDatasets. We conduct experiments on four datasets: DeepFashion (Liu et al., 2016), SHHQ (Fu et al., 2022), UBCFashion (Zablotskaia et al., 2019) and AIST (Tsuchida et al., 2019). The first two are sparse 2D image collections, meaning that each image has different identities and poses are sparse, which makes them more challenging. The last two are human video datasets containing different poses/ views of the same identities, which is easier for the task but lacks diversity.\n\nComparison Methods. We compare with three baselines. ENARF-GAN (Noguchi et al., 2022) makes the first attempt at human NeRF generation from 2D image collections. EG3D (Chan et al., 2022) and StyleSDF (Or-El et al., 2022) are state-of-the-art methods for 3D-aware generation, both requiring super-resolution modules to achieve high-resolution generation.\n\nEvaluation Metrics. To evaluate the quality of rendered images, we adopt Frechet Inception Distance (FID) (Heusel et al., 2017) and Kernel Inception Distance (KID) (Bi ́nkowski et al., 2018).\n\n6\n\na)RenderViewsControlb)HumanPoseControlc)HumanShapesControlPublished as a conference paper at ICLR 2023\n\nFigure 5: Qualitative Comparison Between EVA3D and Baseline Methods. Rendered 2D images and corresponding meshes are placed side-by-side. Both the 2D renderings and 3D meshes generated by our method achieve the best quality among SOTA methods. Zoom in for the best view.\n\nFollowing ENARF-GAN, we use Percentage of Correct Keypoints (PCKh@0.5) (Andriluka et al., 2014) to evaluate the correctness of generated poses. Note that PCKh@0.5 can only be calculated on methods that can control generated poses, i.e. ENARF-GAN and EVA3D. To evaluate the correctness of geometry, we use an off-the-shelf tool (Ranftl et al., 2022) to estimate depth from the generated images and compare it with generated depths. 50K samples, padded to square and resized to the same resolution (DeepFashion, SHHQ, UBCFashion at 5122; AIST at 2562), are used to compute FID and KID. PCKh@0.5 and Depth are evaluated on 5K samples.\n\n4.2 QUALITATIVE EVALUATIONS\n\nGeneration Results and Controlling Ability of EVA3D. As shown in Fig. 4 a), EVA3D is capable of generating high-quality renderings in novel views and remain multi-view consistency. Due to the inherent human prior in our model design, EVA3D can control poses and shapes of the generated 3D human by changing β and θ of SMPL. We show novel pose and shape generation results in Fig. 4 b)& c). We refer readers to the supplementary PDF and video for more qualitative results.\n\nComparison with Baseline Methods. We show the renderings and corresponding meshes generated by baselines and our method in Fig. 5. EG3D trained on DeepFashion, as well as StyleSDF trained on SHHQ, generate reasonable RGB renderings and geometry. However, without explicit human modeling, complex human poses make it hard to align and model 3D humans in observation spaces, which leads to distorted generation. Moreover, because of the use of super resolution, their geometry is only trained under low resolution (642) and therefore lacks details. EG3D trained on SHHQ and StyleSDF trained on DeepFashion fail to capture 3D information and collapse to the trivial solution of painting on billboards. Limited by the inefficient representation and computational resources, ENARF-GAN can only be trained at a resolution of 1282, which leads to low-quality rendering results. Besides, lacking human prior makes ENARF-GAN hard to capture correct 3D information of human from sparse 2D image collections, which results in broken meshes. EVA3D, in contrast, generates high-quality human renderings on both datasets. We also succeeded in learning reasonable 3D human geometry from 2D image collections with sparse viewing angles and poses, thanks to the strong human prior and the pose-guided sampling strategy. Due to space limitations, we only show\n\n7\n\nDeepFashionSHHQa)EG3Db)StyleSDFc)ENARF-GANd)OursPublished as a conference paper at ICLR 2023\n\nTable 1: Comparison with State-of-the-Art Methods. * The training code of ENARF-GAN is implemented based on the official inference code.\n\nMethods, Resolution\n\nDeepFashion\n\nSHHQ\n\nFID↓ KID↓\n\nPCK↑ Depth↓\n\nFID↓ KID↓\n\nPCK↑ Depth↓\n\nEG3D, 5122 StyleSDF, 5122 ENARF-GAN*, 1282 Ours, 5122\n\n26.38 92.40 77.03 15.91\n\nMethods, Resolution\n\nEG3D, 5122 StyleSDF, 5122 ENARF-GAN*, 1282 Ours, 5122\n\n0.014 0.136 0.114 0.011\n\n- -\n43.74 87.50 UBCFashion\n\n0.0779 0.0359 0.1151 0.0272\n\n32.96 14.12 80.54 11.99\n\n0.033 0.010 0.102 0.009\n\n- -\n40.17 88.95\n\n0.0296 0.0300 0.1241 0.0177\n\nAIST\n\nFID↓ KID↓\n\nPCK↑ Depth↓\n\nFID↓ KID↓\n\nPCK↑ Depth↓\n\n23.95 18.52 -\n12.61\n\n0.009 0.011 -\n0.010\n\n- -\n- 99.17\n\n0.1163 0.0311 -\n0.0090\n\n34.76 199.5 73.07 19.40\n\n0.022 0.225 0.075 0.010\n\n- -\n42.85 83.15\n\n0.1165 0.0236 0.1128 0.0126\n\nTable 2: Ablation Study. †Depth is evaluated with SMPL depth. We report Depth×103 for simplicity.\n\nMethods\n\nFID↓ †Depth↓ Baseline, 2562 31.14 + Composite, 5122 17.81 + Delta SDF, 5122 15.62 + Pose-guide, 5122 15.91\n\n3.57 5.02 3.69 3.04\n\nTable 3: Trade-Off Between RGB and Geometry.\n\nDistribution FID↓ †Depth↓ Original σθ = 15◦ σθ = 30◦ σθ = 45◦ σθ = 60◦ Uniform\n\n15.62 15.91 19.05 19.56 25.08 25.82\n\n3.69 3.04 2.58 2.65 2.91 2.92\n\nFigure 6: PDF of Different PoseGuided Sampling Distributions.\n\nresults of DeepFashion and SHHQ here. For visual comparisons on UBCFashion and AIST, please refer to the supplementary material.\n\n4.3 QUANTITATIVE EVALUATIONS\n\nAs shown in Tab. 1, our method leads all metrics in four datasets. EVA3D outperforms ENARFGAN in all settings thanks to our high-resolution training ability. EG3D and StyleSDF, as the SOTA methods in the 3D generation, can achieve reasonable scores in some settings (e.g. StyleSDF achieves 18.52 FID on UBCFashion) for their super-resolution modules. But they also fail on some datasets (e.g. StyleSDF fails on AIST with 199.5 FID) for complexity in human poses. In the contrast, EVA3D achieves the best FID/KID scores under all settings. Moreover, unlike EG3D or StyleSDF, EVA3D can control the generated pose and achieve higher PCKh@0.5 score than ENARF-GAN. For the geometry part, we also achieve the lowest depth error, which shows the importance of natively high-resolution training.\n\n4.4 ABLATION STUDIES\n\nAblation on Method Designs. To validate the effectiveness of our designs on EVA3D, we subsequently add different designs on a baseline method, which uses one large network to model the canonical space. Experiments are conducted on DeepFashion. The results are reported in Tab. 2. Limited by the inefficient representation, the “Baseline” can only be trained at 256 × 128, which results in the worst FID score. Adding compositional design (“+Composite”) makes the network efficient enough to be trained at a higher resolution of 512 × 256 and achieve higher generation quality. We further introduce human prior by predicting delta SDF (“+Delta SDF”), which gives the best FID score. Finally, using the pose-guided sampling (“+Pose-guide”), we further decrease the depth error. We refer readers to the supplementary material for qualitative evaluations of ablation studies.\n\nAnalysis on Pose-Guided Sampling. We analyze the importance of the sampling strategy in 3D human GAN training. Three types of distributions pest are experimented, including the original dataset distribution (“Original”), pose-guided Gaussian distribution (“σθ = ∗”), and pose-guided uniform distribution (“Uniform”). The results are reported in Tab. 3. Firstly, uniform sampling is not a good strategy for that the information density is different between different parts of human.\n\n8\n\n00.010.020.030.040.05060120180240300Oriσ!=15σ!=30σ!=45σ!=60UniPublished as a conference paper at ICLR 2023\n\nFigure 7: Applications of EVA3D. a) Interpolation on the latent space gives smooth transition between two samples. b) Inversion result (right) of the target image (left).\n\nSecondly, the original distribution gives the best visual quality but the worst geometry. It could result in the trivial solution of painting on billboards. Thirdly, the pose-guided Gaussian sampling can avoid damaging visual quality too much and improve geometry learning. As the standard deviation σθ increases, FID increases while the depth error decreases. Therefore, it is a trade-off between In our final experiments, we choose σθ = 15◦ which is a visual quality and geometry quality. satisfying balance between the two factors.\n\n4.5 APPLICATIONS\n\nInterpolation on Latent Space. As shown in Fig. 7 a), we linearly interpolate two latent codes to generate a smooth transition between them, showing that the latent space learned by EVA3D is semantically meaningful. More results are provided in the supplementary video.\n\nInversion. We use Pivotal Tuning Inversion (PTI) (Roich et al., 2021) to inverse the target image and show the results in Fig. 7 b). Reasonable novel view synthesis results can be achieved. The geometry, however, fails to capture geometry details corresponding to RGB renderings, which can be caused by the second stage generator fine-tuning of PTI. Nevertheless, we demonstrate the potential of EVA3D in more related downstream tasks. For more results and comparison with baseline methods, please refer to the supplementary material.\n\n5 DISCUSSION\n\nTo conclude, we propose a high-quality unconditional 3D human generation model EVA3D that only requires 2D image collections for training. We design a compositional human NeRF representation for efficient GAN training. To train on the challenging 2D image collections with sparse viewing angles and human poses, e.g. DeepFashion, strong human prior and pose-guided sampling are introduced for better GAN learning. On four large-scale 2D human datasets, we achieve state-of-the-art generation results at a high resolution of 512 × 256.\n\nLimitations: 1) There still exists visible circular artifacts in the renderings, which might be caused by the SIREN activation. A better base representation, e.g. tri-plane of EG3D, and a 2D decoder might solve the issue. 2) The estimation of SMPL parameters from 2D image collections is not accurate, which leads to a distribution shift from the real pose distribution and possibly compromises generation results. Refining SMPL estimation during training would make a good future work. 3) Limited by our tight 3D human representation, it is hard to model loose garments, accessories or body parts (like hair). The apparent geometric line artifact around the neck and shoulder areas of samples from DeepFashion training (see Fig. 5) could be caused by the compositional representation having trouble modeling long hair hanging down. Using separate modules to handle loose parts might be a promising direction. 4) It is known that state-of-the-art 3D-aware generation methods (Chan et al., 2022; Or-El et al., 2022) have not achieved comparable quality with that of 2D generation (Karras et al., 2020; 2021). To investigate if that is still the case in terms of human generation, we further train StyleGAN2 (Karras et al., 2020) on DeepFashion. StyleGAN2 achieves 6.52 FID, which is much lower than our FID of 15.91. This indicates that 3D-aware generation still has a long way to develop.\n\nACKNOWLEDGMENTS\n\nThis study is supported by NTU NAP, MOE AcRF Tier 2 (MOE-T2EP20221-0012), and under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).\n\n9\n\na)InterpolationonLatentSpaceb)TargetInversionResultPublished as a conference paper at ICLR 2023\n\nETHICS STATEMENT\n\nAlthough the results of EVA3D are yet to the point where they can fake human eyes, we still need to be aware of its potential ethical issues. The generated 3D humans might be misused to create contents that are misleading. EVA3D can also be used to invert real human images, which can be used to create fake videos of real humans and cause negative social impacts. Moreover, the generated 3D humans might be biased, which is caused by the inherent distribution of training datasets. We make our best effort to demonstrate the impartiality of EVA3D in Fig. 1.\n\nREPRODUCIBILITY STATEMENT\n\nOur method is thoroughly described in Sec. 3. Together with implementation details included in the supplementary material, the reproducibility is ensured. Moreover, Our code is publicly available at https://github.com/hongfz16/EVA3D.\n\nREFERENCES\n\nMykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on computer Vision and Pattern Recognition, pp. 3686–3693, 2014.\n\nAlexander W Bergman, Petr Kellnhofer, Yifan Wang, Eric R Chan, David B Lindell, and Gordon Wetzstein. Generative neural articulated radiance fields. arXiv preprint arXiv:2206.14314, 2022.\n\nMikołaj Bi ́nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd\n\ngans. arXiv preprint arXiv:1801.01401, 2018.\n\nMallikarjun BR, Ayush Tewari, Xingang Pan, Mohamed Elgharib, and Christian Theobalt. gcorf:\n\nGenerative compositional radiance fields. arXiv preprint arXiv:2210.17344, 2022.\n\nZhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Humman: Multi-modal 4d human dataset for versatile sensing and modeling. arXiv preprint arXiv:2204.13686, 2022.\n\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic In Proceedings of the\n\nimplicit generative adversarial networks for 3d-aware image synthesis. IEEE/CVF conference on computer vision and pattern recognition, pp. 5799–5809, 2021.\n\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16123–16133, 2022.\n\nJianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629, 2021.\n\nXu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael J Black, Andreas Geiger, and Otmar\n\nHilliges. gdna: Towards generative detailed neural avatars. arXiv, 2022.\n\nYu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. Gram: Generative radiance manifolds for 3d-aware image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10673–10683, June 2022.\n\nDanny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object dynamics with compositional neural radiance fields. arXiv preprint arXiv:2202.11855, 2022.\n\nAnna Fr ̈uhst ̈uck, Krishna Kumar Singh, Eli Shechtman, Niloy J Mitra, Peter Wonka, and Jingwan Lu. Insetgan for full-body image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7723–7732, 2022.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy, Wayne Wu, and Ziwei Liu. Stylegan-human: A data-centric odyssey of human generation. arXiv preprint arXiv:2204.11823, 2022.\n\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139–144, 2020.\n\nArtur Grigorev, Karim Iskakov, Anastasia Ianina, Renat Bashirov, Ilya Zakharkin, Alexander Vakhitov, and Victor Lempitsky. Stylepeople: A generative model of fullbody human avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5151– 5160, 2021.\n\nAmos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regu-\n\nlarization for learning shapes. arXiv preprint arXiv:2002.10099, 2020.\n\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d-aware\n\ngenerator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985, 2021.\n\nMarc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time deep dynamic characters. ACM Transactions on Graphics (TOG), 40(4): 1–16, 2021.\n\nPhilipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escaping plato’s cave: 3d shape from adversarial rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9984–9993, 2019.\n\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\n\nFangzhou Hong, Liang Pan, Zhongang Cai, and Ziwei Liu. Versatile multi-modal pre-training for human-centric perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16156–16166, 2022a.\n\nFangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint arXiv:2205.08535, 2022b.\n\nBoyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5605–5615, 2022a.\n\nWei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural\n\nhuman radiance field from a single video. arXiv preprint arXiv:2203.12575, 2022b.\n\nYuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, and Ziwei Liu. Text2human: Text-driven controllable human image generation. ACM Transactions on Graphics (TOG), 41(4):1–11, 2022c. doi: 10.1145/3528223.3530104.\n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401–4410, 2019.\n\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8110–8119, 2020.\n\nTero Karras, Miika Aittala, Samuli Laine, Erik H ̈ark ̈onen, Janne Hellsten, Jaakko Lehtinen, and\n\nTimo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021.\n\nMuhammed Kocabas, Nikos Athanasiou, and Michael J Black. Vibe: Video inference for human body pose and shape estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5253–5263, 2020.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAbhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser. Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation. In CVPR, 2022.\n\nKathleen M Lewis, Srivatsan Varadharajan, and Ira Kemelmacher-Shlizerman. Tryongan: Bodyaware try-on via layered interpolation. ACM Transactions on Graphics (TOG), 40(4):1–10, 2021.\n\nLingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neural free-view synthesis of human actors with pose control. ACM Transactions on Graphics (TOG), 40(6):1–16, 2021.\n\nZiwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n\nStephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of volumetric primitives for efficient neural rendering. ACM Transactions on Graphics (TOG), 40(4):1–13, 2021.\n\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. ACM transactions on graphics (TOG), 34(6):1–16, 2015.\n\nNaureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 5442–5451, 2019.\n\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do In International conference on machine learning, pp. 3481–3490. PMLR,\n\nactually converge? 2018.\n\nMarko Mihajlovic, Shunsuke Saito, Aayush Bansal, Michael Zollhoefer, and Siyu Tang. Coap: Compositional articulated occupancy of people. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13201–13210, 2022.\n\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, pp. 405–421. Springer, 2020.\n\nThu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. HoloIn Proceedings of the\n\ngan: Unsupervised learning of 3d representations from natural images. IEEE/CVF International Conference on Computer Vision, pp. 7588–7597, 2019.\n\nMichael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11453–11464, 2021.\n\nAtsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural articulated radiance field. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5762–5772, 2021.\n\nAtsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Unsupervised learning of efficient\n\ngeometry-aware neural articulated representations. arXiv preprint arXiv:2204.08839, 2022.\n\nRoy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira KemelmacherIn ProShlizerman. Stylesdf: High-resolution 3d-consistent image and geometry generation. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13503– 13513, 2022.\n\nPablo Palafox, Aljaˇz Boˇziˇc, Justus Thies, Matthias Nießner, and Angela Dai. Npms: Neural para-\n\nmetric models for 3d deformable shapes. arXiv preprint arXiv:2104.00702, 2021.\n\nPablo Palafox, Nikolaos Sarafianos, Tony Tung, and Angela Dai. Spams: Structured implicit parametric models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12851–12860, 2022.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nXingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo. Do 2d gans know 3d shape? unsupervised 3d shape reconstruction from 2d image gans. arXiv preprint arXiv:2011.00844, 2020.\n\nGeorgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10975–10985, 2019a.\n\nGeorgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10975–10985, 2019b.\n\nSida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Animatable neural radiance fields for human body modeling. arXiv e-prints, pp. arXiv– 2105, 2021a.\n\nSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9054–9063, 2021b.\n\nRen ́e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022.\n\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based\n\nediting of real images. arXiv preprint arXiv:2106.05744, 2021.\n\nKripasindhu Sarkar, Vladislav Golyanik, Lingjie Liu, and Christian Theobalt. Style and pose control for image synthesis of humans from a single monocular view. arXiv preprint arXiv:2102.11263, 2021a.\n\nKripasindhu Sarkar, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. Humangan: A gener-\n\native model of humans images. arXiv preprint arXiv:2103.06902, 2021b.\n\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems, 33:20154– 20166, 2020.\n\nKatja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. Voxgraf: Fast 3d-\n\naware image synthesis with sparse voxel grids. arXiv preprint arXiv:2206.07695, 2022.\n\nAliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, et al. Textured In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern neural avatars. Recognition, pp. 2387–2397, 2019.\n\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems, 33:7462–7473, 2020.\n\nIvan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. Epigraf: Rethinking training\n\nof 3d gans. arXiv preprint arXiv:2206.10535, 2022.\n\nShih-Yang Su, Frank Yu, Michael Zollh ̈ofer, and Helge Rhodin. A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose. Advances in Neural Information Processing Systems, 34:12278–12291, 2021.\n\nShih-Yang Su, Timur Bagautdinov, and Helge Rhodin. Danbo: Disentangled articulated neural body\n\nrepresentations via graph neural networks. arXiv preprint arXiv:2205.01666, 2022.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8248–8258, 2022.\n\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Yifan Wang, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Advances in neural rendering. arXiv preprint arXiv:2111.05849, 2021.\n\nShuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019, pp. 501–510, Delft, Netherlands, November 2019.\n\nShaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu Tang. Arah: Animatable volume rendering\n\nof articulated human sdfs. In European conference on computer vision, volume 4, 2022.\n\nZiyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, and Michael Zollhofer. Learning compositional radiance fields of dynamic human heads. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5704–5713, 2021.\n\nChung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira KemelmacherShlizerman. Humannerf: Free-viewpoint rendering of moving people from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16210–16220, 2022.\n\nJianfeng Xiang, Jiaolong Yang, Yu Deng, and Xin Tong. Gram-hd: 3d-consistent image generation at high resolution with generative radiance manifolds. arXiv preprint arXiv:2206.07255, 2022.\n\nHongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion. Advances in Neural Information Processing Systems, 34, 2021.\n\nBangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In International Conference on Computer Vision (ICCV), October 2021.\n\nWang Yifan, Lukas Rahmann, and Olga Sorkine-hornung. Geometry-consistent neural shape representation with implicit displacement fields. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=yhCp5RcZD7.\n\nJae Shin Yoon, Lingjie Liu, Vladislav Golyanik, Kripasindhu Sarkar, Hyun Soo Park, and Christian Theobalt. Pose-guided human animation from a single image in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15039–15048, 2021.\n\nPolina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based\n\nnetwork for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019.\n\nJianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu, Yichun Shi, Guoxian Song, Zhongcong Xu, Xinchao Wang, and Jiashi Feng. Avatargen: a 3d generative model for animatable human avatars. arXiv preprint arXiv:2208.00561, 2022a.\n\nJichao Zhang, Enver Sangineto, Hao Tang, Aliaksandr Siarohin, Zhun Zhong, Nicu Sebe, and Wei Wang. 3d-aware semantic-guided generative model for human synthesis. arXiv preprint arXiv:2112.01422, 2021.\n\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022b.\n\nFuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. arXiv preprint\n\nHumannerf: Generalizable neural human radiance field from sparse inputs. arXiv:2112.02789, 2021.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nXiaoming Zhao, Fangchang Ma, David G ̈uera, Zhile Ren, Alexander G Schwing, and Alex Colburn. Generative multiplane images: Making a 2d gan 3d-aware. In European Conference on Computer Vision, pp. 18–35. Springer, 2022.\n\nZerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. IEEE transactions on pattern analysis and machine intelligence, 44(6):3170–3184, 2021.\n\n15",
    "reference": "# Summary Of The Paper\n\nThis paper tackles the problem of 3D-aware human generation from 2D images. To generate at a high resolution, EVA3D proposes to \nuse compositional multiple NeRFs to do the generation. To overcome difficulties in training with imbalanced human dataset, the paper proposes several training strategies. Results on several human dataset demonstrate the effectiveness of the proposed method.\n\n# Strength And Weaknesses\n\n## Strengths\n\nThe proposed approach is interesting and effective. Even though some techniques are not new, e.g., LBS mapping is commonly used in human reconstruction, to integrate them into generative model training is novel.\n\n## Weakness\n\nI do not find major issues in the paper. However, there are some questions I wish authors can clarify or add to make this paper stronger. See below.\n\n## Questions\n\n**1. Clarification about \"Baseline\" in Tab. 2**\n\nCan authors clarify the difference between the \"Baseline\" in Tab. 2 and StyleSDF in Tab. 1? I am a little bit confused because the performance differs quite a lot. My understanding is that Tab. 2's \"Baseline\" includes the mapping with LBS. Is this correct?\n\n**2. For the effectiveness of composition**\n\nIn Tab. 2, to demonstrate the efficacy of composition, authors show the results between $512^2$ and $256^2$. This comparison is great to show the power of composition for high resolution. However, I feel like it may be a more direct comparison if authors can show the results from just $256^2$ with composition.\n\n**3. For the issues arising from composition**\n\nFrom qualitative results, e.g., Fig. 4 in the supplementary, it seems like the composition can impose apparent artifacts around the edge of the bounding box. For example, there are apparent lines around the neck. It would be great if authors can provide more discussion and analysis of this.\n\n**4. For the training strategies**\n\nThe proposed training strategies, i.e., pose-guided training and LBS mapping from observation space to canonical space, are essentially generalizable to all baseline methods (StyleSDF and EG3D). Meanwhile, the Delta prediction can also be applied to StyleSDF. It would build a stronger paper if authors can utilize these training strategies in baselines's training. \n\nThis is somehow related to the effectiveness of composition in Question 2: the above experiments would give clearer view of how the composition works compared to baselines's representations.\n\n**5. Lacked references**\n\n- Schwarz et al., VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids. NeurIPS 2022.\n- Zhao et al., Generative multiplane images: making a 2D GAN 3D-aware. ECCV 2022.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- Clarify: the paper is clearly written and easy to follow.\n- Quality: the results are of high quality.\n- Novelty: it is novel to integrate techniques mentioned in the paper into a generative model training pipeline.\n- Reproducibility: authors state that the code will be public.\n\n# Summary Of The Review\n\nThe proposed approach is interesting. The paper is clearly written. Meanwhile, effectiveness of some techniques can be further discussed.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Details Of Ethics Concerns\n\nN/A"
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nRECEDING NEURON IMPORTANCES FOR STRUCTURED PRUNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nStructured pruning efficiently compresses networks by identifying and removing unimportant neurons. While this can be elegantly achieved by applying sparsityinducing regularisation on BatchNorm parameters, an L1 penalty would shrink all scaling factors rather than just those of superfluous neurons. To tackle this issue, we introduce a simple BatchNorm variation with bounded scaling parameters, based on which we design a novel regularisation term that suppresses only neurons with low importance. Under our method, the weights of unnecessary neurons effectively recede, producing a polarised bimodal distribution of importances. We show that neural networks trained this way can be pruned to a larger extent and with less deterioration. We one-shot prune VGG and ResNet architectures at different ratios on CIFAR and ImagenNet datasets. In the case of VGG-style networks, our method significantly outperforms existing approaches particularly under severe pruning. Source code is available at: https://anonymous. 4open.science/r/receding-neuron-importances-40C9.\n\n1\n\nINTRODUCTION\n\nModern deep neural network architectures (Simonyan & Zisserman, 2014; He et al., 2016) achieve state-of-the-art performance but require significant computational resources which makes their deployment onto edge devices difficult. Even though it has been shown that it is possible to train less over-parametrised models from scratch and obtain a similar performance (Frankle & Carbin, 2018), it remains a non-trivial task to actually find such a winning subnetwork.\n\nIn this work, we focus on structured one-shot pruning as a means to network compression which is typically composed of three stages – i) training a large model to convergence, ii) removing parameters with low importance, and iii) fine-tuning the remaining network. Unstructured pruning, which works on a weight level (Yang et al., 2019; Frankle & Carbin, 2018), can remove a much higher number of parameters but produces sparse weight matrices which cannot be efficiently utilised without specialised hardware (Han et al., 2016). In contrast, by removing entire neurons structured pruning finds efficient structures akin to an implicit architecture search (Liu et al., 2018).\n\nStructured pruning methods attribute an importance score to each neuron, which enables their ranking and ultimately the decision of which to dispose (Li et al., 2016; Molchanov et al., 2016a). To this end, BatchNorm layers (Ioffe & Szegedy, 2015) become very appealing as they explicitly learn parameters which uniformly scale the outputs of each neuron. This scaling parameter can be used as a proxy for the importance the model attributes to a neuron, as a value of zero would effectively suppress an output. Furthermore, one can regularise these layers to obtain neuron level sparsity whilst maintaining classification performance (Liu et al., 2017; Zhuang et al., 2020). Such methods typically define neuron importance as the absolute value of its scaling parameter, an approach which limits the design of regularisers. Because the measure is only half-bounded one cannot easily define levels of importance without looking at the overall distribution - making it difficult to target specific neurons. An example is the L1 regulariser (Liu et al., 2017) which shrinks all parameters with a constant gradient, even ones with high importance. Ideally, one would design a regulariser which creates sparsity by only shrinking unimportant neurons, leaving the others untouched.\n\nIn this work, we create such a regulariser and show it outperforms existing approaches at a rate that increases with the amount of neurons pruned. Our contributions are two-fold: we first introduce a simple variation of BatchNorm, which linearly transforms channels using bounded scalers. This\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nlayer maintains the same performance as the original, while offering a bounded importance score for neurons. Building on this measure we then define a novel regularisation, focused on shrinking only neurons with lesser weight, by having its gradient decay exponentially for higher importances. Our method significantly outperforms related approaches for VGG models, and we show that severe degradation can be attributed to over-pruning early layers of the network.\n\n2 RELATED WORK\n\nNeural network compression through pruning is most commonly divided into structured and unstructured approaches. Unstructured pruning has gained a lot of attention in recent years (Frankle & Carbin, 2018) as it challenges conventional wisdom over the role of over-parametrisation and weight initialisation in the optimisation of deep neural networks (Frankle et al., 2020). While these methods can achieve superior theoretical compression rates (Renda et al., 2020), their use remains impractical without specialised hardware that can take advantage of sparsity (Han et al., 2016). Structured pruning on the other hand removes entire neurons from an architecture thus achieving real memory and computational efficiencies (Liu et al., 2018).\n\nAt the heart of structured pruning lies the task of identifying unimportant neurons to remove from a network. Quantifying importance can be based on numerous criteria including filter norms (Li et al., 2016; He et al., 2018), reconstruction errors (He et al., 2017; Luo et al., 2017; Molchanov et al., 2016b; Yu et al., 2018), redundancy (He et al., 2019; Suau et al., 2020; Wang et al., 2018) and BatchNorm parameters (Liu et al., 2017; Zhuang et al., 2020). Our work belongs in the latter category as we focus on deriving an importance score solely based on channel scaling parameters.\n\nIn addition to defining importance measures, one can add regularisation during training to nudge networks into utilising their capacity more sparingly. Most relevant to our work are methods which apply sparsity regularisations on BatchNorm parameters (Liu et al., 2017; Zhuang et al., 2020). The most popular approach is Network Slimming (Liu et al., 2017), which constrains the BatchNorm scaling parameters using the L1 penalty. A drawback of this method is that it shrinks all parameters with an equal gradient irrespective of their importance. This issue is also addressed by Zhuang et al. (2020) who propose a regulariser that explicitly maximises the polarisation of the BatchNorm scaler distribution. While the method effectively increases the margin between important and unimportant neurons, it does so by both shrinking and expanding weights. Another method designed for nonlinear shrinking is Yang et al. (2019), who propose the ratio of the L1 and L2 norms as a sparsity regulariser. Even though this method is not based on BatchNorm, but is targeted at filter weights, we include it in our comparison as it has a similar motivation to our work.\n\nIn terms of pruning setup the most popular method is one-shot pruning (Li et al., 2016; Zhuang et al., 2020; Yang et al., 2019) where all desired neurons are removed at once and the remaining network is fine-tuned. Iterative approaches (Han et al., 2015) periodically prune and fine-tune until a target ratio is met. Recent works aim to eliminate the need of fine-tuning (Chen et al., 2021) altogether or prune models after intialisation in a data-free manner (Lee et al., 2018; Wang et al., 2020).\n\n3 SIGMOID BATCHNORM\n\nWe introduce a variation of BatchNorm, which uses a single learnable parameter per channel and offers a bounded importance score for filters. In its original formulation, BatchNorm (Ioffe & Szegedy, 2015) first normalises each input channel x using batch statistics mean μB and standard deviation σB, then applies an affine transformation using learnable parameters γ and β.\n\nBN (x, γ, β) = γ\n\nx − μB (cid:112)σ2 B + ε\n\n+ β\n\n(1)\n\nWhile BatchNorm has become ubiquitous in Deep Learning, the reasons behind its effectiveness are not fully understood (Santurkar et al., 2018). The proliferation of BatchNorm variations (Ba et al., 2016; Wu & He, 2018; Ulyanov et al., 2016) suggests its benefits arise from normalising activations rather than the affine transformation following it. We empirically show that the transformation can be replaced to be linear without loss of performance.\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\ny\n\nt i\ns n\ne D\n\n6\n\n4\n\n2\n\n0\n\nγ\n\nβ\n\nVGG-19 ResNet-50\n\n6\n\n4\n\n2\n\n0\n\n−1 −0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\n−1 −0.5\n\n0\n\n0.5\n\n1\n\n1.5\n\nFigure 1: Distribution of parameters across all BatchNorm layers of pre-trained models available in PyTorch (Paszke et al., 2017). The scale γ stays in the interval [0, 1], the offset β remains close to 0.\n\nWe look at the empirical distributions of BatchNorm parameters to understand the effects of the affine transformation. While the scale γ and offset β are unbounded, in practice, these will be suppressed due to weight decay. We can observe this in pre-trained models: Figure 1 shows the parameter distributions for all BatchNorm layers of a VGG-19 (Simonyan & Zisserman, 2014) and ResNet-50 (He et al., 2016) model. The BatchNorm parameters in both networks are well aligned as almost all γ ∈ [0, 1], while a large proportion of β are concentrated around 0.\n\nBecause BatchNorm operates on a channel level, it is of particular interest to structured pruning, as it offers a natural place to look for measures that quantify filter importance. In line with our observation about the distribution of β, previous approaches (Liu et al., 2017; Zhuang et al., 2020) define filter importance as the magnitude of γ and ignore the offset. While this has proven to work in practice, one could argue it would be desirable to construct an importance score that incorporates all the information available. Additionally, the utility of such a measure could be greatly improved if it would be bounded - offering a better understanding by knowing the minimum and maximum importance a filter could have.\n\nGiven the aforementioned considerations, we propose a simple alteration to BatchNorm, termed σBN , which keeps the normalisation scheme, but changes the channel transformation to use a bounded scaling parameter. We remove the offset β and bound the scale parameter by applying the sigmoid function to γ before multiplication:\n\nσBN (x, γ) = σ(γ)\n\nx − μB (cid:112)σ2 B + ε\n\n,\n\nwhere σ(γ) =\n\n1 1 + e−γ\n\n(2)\n\nIn Table 1 we see the performance of vanilla BatchNorm (BN) compared with our variation σBN. Our variation performs comparable for VGG-16 and ResNet-56 on both CIFAR datasets. For completeness we also add the performance of BN and σBN with and without the bias term β. We can observe that the presence or absence of β does not have a profound impact on accuracy. With σBN, we now have a normalisation layer that uses a single, bounded parameter to quantify the importance of a filter. We will use this property to decide which filters to suppress and ultimately prune.\n\nCIFAR10 BN-β BN σBN σBN+β\n\nCIFAR100 BN-β BN σBN σBN+β\n\nVGG-16 93.58 93.57 93.48 ResNet-56 92.87 93.34 93.11\n\n93.35 93.33\n\nVGG-16 72.81 72.99 72.75 ResNet-56 70.70 70.94 71.27\n\n73.12 71.12\n\nTable 1: No significant difference in classification accuracy can be seen between any BatchNorm variation, including toggles of the bias term for vanilla BN and σBN. Results are averaged over three runs and have standard deviation of ≈0.2. Normalisation layers are trained without regularisation.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n0.25\n\n0.2\n\n0.15\n\n0.1\n\n0.05\n\n0.0\n\n∇γ(RS)\n\nNeuron Importances\n\nb=0 b=1 b=2 b=3\n\ns t\n\nn u\no C\n\n103 102 101 0\n\ns t\n\nn u\no C\n\n103 102 101 0\n\nL1\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n|γ|\n\nRNI\n\n0\n\n0.5\n\nσ(γ)\n\n1\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nσ(γ)\n\nFigure 2: Left: The gradient of the RNI loss with respect to γ for different hyper-parameter choices. b shifts the range of neuron importances which the regulariser will affect. Right: Histograms of resulting neuron importances after using L1 or RNI regularisation.\n\n4 RECEDING NEURON IMPORTANCE\n\nStructured pruning aims to compress a network by identifying and removing unimportant neurons. We define θ to be the parameters of a neural network and refer to the subset associated with BatchNorm as θBN . Let I(θi) be a measure of importance for neuron i, which can be computed from any model parameters associated with that neuron, such as its filters in convolutional layers (Li et al., 2016; Yang et al., 2019). In BatchNorm based pruning, I is derived only from θBN , and is commonly defined as I(θBN ) = |γi| (Liu et al., 2017; Zhuang et al., 2020). In this paper we use the measure induced by our σBN layer such that I(θBN ) = σ(γi). As in previous work, we want to solve the following risk minimisation problem:\n\ni\n\ni\n\nmin θ\n\n1 N\n\nL (θ, X) + λR(θ) + λSRS(θBN )\n\n(3)\n\nwhere X = {xi, yi}N i=1 is a labelled dataset with N training samples, and λ, λS are scalar weightings for the regularisation terms R and RS. R(θ) is a regularisation against over-fitting, such as weight decay, and is usually applied over all network parameters, including normalisation layers. RS is targeting only θBN such that I exhibits the following property during pruning:\n\nL (θ\\{θi}, X) > L (θ\\{θj}, X) ,\n\nif\n\nI(θi) < I(θj)\n\n(4)\n\nSuch a property creates an ordering which ensures that removing a neuron θi with lower importance will have less impact on model performance, than removing a more important θj. We will assess the quality of such an ordering by looking at the model performance after pruning and fine-tuning. However, an ordering is only partially evaluated when removing a single subset of neurons - since the order of the remaining neurons is not taken into account. Therefore, for a comprehensive estimate of the global ordering quality, it is necessary to prune a model at multiple thresholds.\n\nGenerally we want RS to induce sparsity over θBN such that outputs of unimportant neurons are entirely suppressed during training. Pruning such a network would result in less damage and consequently a quicker recovery during fine-tuning. The typical choice of Rs is the L1 norm, which shrinks all scaling parameters with a constant gradient. Such a regularisation strategy simultaneously subdues both important and unimportant neurons with equal weighting. From an optimisation perspective, L1 will create a bias such that at convergence, any equilibrium would require the classification loss to negate the constant gradient of the regulariser. Such a bias might lead to sub-optimal solutions, and can be mitigated by having the regularisation reduce its strength once a suitable, sparse distribution of importances has been found. We show that a better regularisation term can be\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\ndesigned by focusing on suppressing unimportant neurons, whilst leaving important ones untouched. Without an idea of bounds for I(θBN ) it is difficult to decide on a threshold between important and unimportant neurons. Using the importance measure from our σBN layers we propose a regularisation loss whose strength decays exponentially as I(θBN ) reaches its maximum. We introduce the Receding Neuron Importance regularisation (RNI) on batch normalisation parameters γ:\n\nRs(γ, b) = σ(γ + b) · (1 − log(σ(γ + b))),\n\nwith ∇σ(γ+b)(Rs) = − log(σ(γ + b))\n\nand ∇γ(Rs) = − log(σ(γ + b))σ(γ + b)(1 − σ(γ + b))\n\n(5)\n\nThe hyper-parameter b controls the range defining which neuron importances to target. Its effects can be visualised in Figure 2 (Left): b shifts the peak of the gradient after which an exponential decay occurs. As b increases only neurons with lesser importance are affected by the regularisation. With the weights of such neurons receding towards zero, the final distribution of importances will have a polarised bimodal shape - only important and zero-weight neurons will be left (Right).\n\n5 EXPERIMENTS\n\nIn this section, we will evaluate our sparsity regularisation under a one-shot pruning setting on the CIFAR10/100 and ImageNet datasets using VGG and ResNet models. In most structured pruning literature evaluation is performed only under relatively benign pruning ratios, which mostly preserve the performance of the original model and can make it difficult to compare related methods. In our evaluations we observe that the difference between approaches becomes noticeable when pruning ratios are increased. Under more inauspicious circumstances we show that our method outperforms existing state-of-the-art approaches and can, in some instances, significantly mitigate the detrimental effects of severe pruning. We also compare the pruned VGG/ResNet networks with similarly sized MobileNetV2 models and show that pruning outperforms these compact architectures.\n\n5.1 EXPERIMENTAL SETUP\n\nPruning. Under a one-shot pruning scenario, a model is trained, then pruned and subsequently finetuned to recuperate performance. An appropriate regularisation strength is selected such that it does not impede training while also producing enough sparsity to prune the desired amount of neurons. For practical reasons we train a model only once and then evaluate it under several pruning ratios. We prune networks in a global fashion, removing filters based on their importance score and without regard to the resulting distribution over the layers. To avoid the pruning of all neurons within a layer, a minimum of three filters will be preserved. While this avoids the extreme case of layer collapse, if a method disproportionately prunes the neurons of a single layer it could still irreparably damage the network. From a technical perspective, pruning the filters of a layer changes its parameter count but also that of the following layer, since the expected input dimensionality has changed. While this has no further implications for VGG style architectures, it does pose some challenges for networks with residual connections. Similar to related works, in order to avoid mismatched dimensions in ResNets, we do not prune skip connections or the last layer in residual connections.\n\nRelated Methods. We compare our approach with L1 Slimming (Liu et al., 2017), Polarization Regularisation (Zhuang et al., 2020), Deep Hoyer (Yang et al., 2019) and Uniform Channel Scaling (UCS) (Zhuang et al., 2020). The same experimental settings are used for all methods during training, pruning and fine-tuning. An exception is the UCS baseline, which does not use any sparsity regularisation and prunes in a local manner by removing the same percentage of filters across all layers. All networks of related approaches are built using vanilla BatchNorm, and with the same initialisation scheme as Liu et al. (2017). Models trained with σBatchNorm layers are initialised with γ drawn from a standard Normal distribution and will not use weight decay during training. The learning rate of σBatchNorm is set to be higher by factor of 10 than that of the rest of the network - we found this to help with polarisation.\n\nHyper-parameters. The same settings as Liu et al. (2017) are used for the CIFAR10 and CIFAR100 datasets. VGG-16 and ResNet-56 models are both trained and fine-tuned for 160 epochs, with the same learning rate schedule and with a batch size of 64. Models are optimised using SGD with momentum, weight decay of 10−4 and an initial learning rate of 10−1 which reduces stepwise by\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\na factor of 10−1 at epochs 80 and 120. The respective sparsity regularisations are applied on all layers of the models. Datasets are augmented using only random crops and horizontal flips. On ImageNet, we evaluate methods on the ResNet-50 bottleneck architecture and regularise only its prunable layers. Models are trained for 90 epochs and fine-tuned for 30, using a batch size of 512. The initial learning rate is 10−1 for training and 10−2 for fine-tuning, and is scheduled to decrease after 1/3 and 2/3 of the respective training time. For our RNI experiments the following hyper-parameters are used: λS = 10−3, b = 3 for VGG-16, λS = 10−4,b = 0 for ResNet-56, and λS = 10−5, b = 3 for ResNet-50.\n\nCIFAR10\n\nCIFAR100\n\n94%\n\n92%\n\n90%\n\n88%\n\n86%\n\n84%\n\n82%\n\ny c\na r\nu c\nc A\n\nSlimming Polarization DeepHoyer UCS RNI\n\n70%\n\n60%\n\n50%\n\n40%\n\n30%\n\n50% 60% 70% 80% 90% Filters Pruned\n\n50% 60% 70% 80% 90% Filters Pruned\n\nFigure 3: Accuracy of fine-tuned VGG-16 models at varying pruning thresholds. Performance differences between methods become more pronounced at higher ratios. RNI degrades gracefully compared to related methods.\n\n5.2 RESULTS\n\nApart from ImageNet, all methods are evaluated on three random seeds and their mean performance is reported. For clarity, we will omit the standard deviation as we found the results to be very similar across runs. Two scenarios are examined in more detail as they sit on opposite ends of the difficulty spectrum: pruning 50% and 90% of filters. The 50% mark is the default setting in the structured pruning literature and we expect all methods to recuperate most of their baseline performance. This pruning level also ensures a fair comparison between methods as it acts as an anchor for hyperparameter choices. At 90% of filters pruned, we see a significant degradation in performance and a far larger differentiation between methods and models.\n\nIn Figure 3 we have an overview of how VGG networks degrade as pruning ratios increase. On both datasets our method deteriorates gracefully compared to the abrupt declines of related approaches. While on CIFAR10 all methods have an easier time maintaining their performance, on CIFAR100 degradation becomes immediately visible due to the increased difficulty of the dataset. Surprisingly, the locally pruning UCS baseline performs worst on CIFAR10 but outperforms all methods, apart from ours, on CIFAR100. This shows that correctly identifying unimportant neurons in a global manner becomes harder as the difficulty of the dataset increases. In our ablation studies we will show that this deterioration can be directly linked to excessive pruning of early layers in VGGs.\n\nCIFAR10. This task is relatively simple and we expect excess model capacity to be prunable without much performance loss, as seen in Table 2. The only significant degradation happens for VGGs at the 90% mark, where UCS and DeepHoyer lose over 10% accuracy compared to the baseline. ResNets are more robust and are lose less than 4% accuracy while reducing roughly 80% of FLOPs.\n\nCIAFR100. Due to the increased difficulty, in Table 3 we see a larger deterioration for models trained on the CIFAR100 dataset. Here networks require more capacity, thus the number of necessary filters should be higher, and the identification of superfluous ones more difficult. With 90% of filters removed, VGG networks experience severe degradation as methods fail to identify which neurons to keep. In this scenario our approach considerably outperforms state-of-the-art methods by 30% classification accuracy. The only exception is the UCS baseline, which by design maintains the stability of a model, as it will never prune excessive amounts of filters in any given layer. ResNets again prove to be more robust to pruning and show less variation in performance between methods.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nVGG-16\n\nBaseline 50%\n\n90%\n\nResnet-56 Baseline 50%\n\n90%\n\nAcc. Acc. FLOPs Acc. FLOPs\n\nAcc. Acc. FLOPs Acc. FLOPs\n\nSlimming 93.71 93.96 60.25 89.15 11.40 Polarization 93.91 94.27 64.15 88.37 14.69 8.09 DeepHoyer 93.66 93.96 61.32 80.82 93.84 93.02 25.19 83.69 UCS 1.13 93.53 93.64 48.24 90.96 8.40 RNI\n\nSlimming 93.68 93.45 54.07 89.90 19.55 Polarization 93.34 93.50 52.69 89.43 18.89 93.44 93.51 51.07 89.94 20.84 DeepHoyer 93.78 93.33 50.39 88.78 14.79 UCS 93.53 93.01 56.13 90.26 21.54 RNI\n\nTable 2: Results on CIFAR10 for different pruning ratios, showing absolute accuracy (%) and remaining FLOPs (%) relative to the unpruned baseline. Due to the simplicity of the task, models can be significantly pruned without deterioration.\n\nVGG-16\n\nBaseline 50%\n\n90%\n\nResnet-56 Baseline 50%\n\n90%\n\nAcc. Acc. FLOPs Acc. FLOPs\n\nAcc. Acc. FLOPs Acc. FLOPs\n\n3.76 Slimming 73.54 71.61 61.12 29.66 2.59 Polarization 73.47 71.23 63.82 25.62 4.98 73.19 71.66 67.23 25.42 DeepHoyer 73.80 70.84 25.19 54.39 UCS 1.13 72.28 72.46 36.72 57.92 6.31 RNI\n\nSlimming 71.94 70.06 56.97 63.92 15.29 Polarization 71.80 70.08 40.78 62.42 15.47 71.88 71.14 49.22 63.49 19.51 DeepHoyer 71.90 70.72 50.40 63.65 14.79 UCS 71.30 70.65 48.24 63.95 16.34 RNI\n\nTable 3: Results on CIFAR100 for low and high ratios of filters pruned. Shown are the absolute classification accuracy (%) and remaining FLOPs (%) relative to the unpruned baseline. Due to the increased complexity of the classification task, it becomes more difficult to remove excess capacity. The VGG architecture breaks down significantly after sever pruning.\n\nImageNet. On ImageNet in Table 4 the results are similar, however we observe bigger losses due to the increased dataset difficulty. Under these circumstances our method still outperforms prior state-of-the-art, but is worse than the UCS benchmark. This again indicates that for more difficult datasets, global methods over-prune the wrong layers, something the UCS baseline will not do by design. Nevertheless, ResNets show remarkable robustness to pruning, which we attribute to the fact that their skip connections act as a fail-safe and limit the effect of over-pruned layers.\n\n90%\n\nAcc.\n\nAcc. FLOPs Acc. FLOPs\n\nResNet-50 Baseline 50%\n\nCompact Networks. At higher pruning ratios the parameter count of models is significantly reduced, raising the question of whether they would outperform networks specifically designed to be lightweight. Compact networks such as MobileNetV2 Sandler et al. (2018) have an adjustable width multiplier allowing the instantiation of models with arbitrary number of parameters. We thus conduct additional baseline experiments comparing the RNI pruned VGG/ResNet models from Tables 2 and 3 to non-sparse MobileNets of the same size. For each of the RNI pruned models we find a width multiplier such that the corresponding MobileNet has a similar number of parameters, and then optimise it using the same training setup, but without a sparsity loss. From the results in Table 5 we can see that our pruned models can outperform similar sized MobileNets in almost every setting, proving the practical utility of higher pruning ratios.\n\nTable 4: ImageNet results showing absolute accuracy (%) and remaining FLOPs (%) relative to the unpruned baseline.\n\n72.56 41.12 51.74 16.50 73.11 46.12 49.43 17.00 68.72 57.56 50.98 24.14 73.28 44.78 58.13 16.94 72.31 48.13 54.08 17.66\n\nSlimming Polarization DeepHoyer UCS RNI\n\n75.74 75.12 75.36 76.14 74.75\n\nSummary. Because of the evaluation over datasets with varying degrees of complexity, we can make several observations about how models degrade under a spectrum of pruning ratios. Firstly, at 50% of filters pruned differences between approaches are barely visible since all methods are able to regain most of their pre-pruning performance. Differences become obvious only at higher ratios, where our method clearly surpasses prior state-of-the-art. Unsurprisingly, residual networks are more robust to pruning than VGG architectures, since their skip connections can propagate the signal even if essential residual connections have been mistakenly over-pruned. This however comes at the cost\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nNetwork\n\nDataset\n\nPruning Ratio\n\n50%\n\n#P.\n\n90% #P.\n\nVGG-16 [RNI] MobileNetV2\n\nCIFAR10 CIFAR10\n\n93.64 3.40M 90.96 167K 93.38 3.43M 89.67 177K\n\nVGG-16 [RNI] MobileNetV2\n\nCIFAR100 72.46 3.23M 57.92 158K CIFAR100 75.16 3.12M 52.72 163K\n\nResNet-56 [RNI] CIFAR10 CIFAR10 MobileNetV2\n\n93.01 91.21\n\n401K 90.26 112K 408K 88.55 124K\n\nResNet-56 [RNI] CIFAR100 70.65 CIFAR100 71.05 MobileNetV2\n\n429K 63.95 128K 452K 52.72 163K\n\nTable 5: Comparison between non-sparse MobileNetV2 models and RNI pruned VGG/ResNets on CIFAR datasets. For each setting the width multiplier has been chosen such that the MobileNet will have the same number of parameters as the pruned networks.\n\nof pruning efficiency, as seen by the diminished reduction of FLOPs compared to VGG models. To our surprise we find that while UCS is not suited for simple classification tasks, it outperforms prior global pruning methods on more complex datasets. Because of its design, the UCS baseline avoids any risk of excessively pruning individual layers, making models less prone to sudden performance deterioration. This leads to the conjecture that the other related methods over-prune certain layers, which leads to severe degradation in VGG models. In our ablative analysis we show that this is indeed the case, and make the realisation that our method is the only one which does not significantly prune early layers in a network. This offers an explanation why RNI noticeably outperforms similar approaches at high pruning ratios. Finally, from additional evaluations on compact architectures we see that pruned VGG/ResNet models outperform similarly sized MobileNets, which further proves the utility of pruning as a means to obtaining compressed networks.\n\nλS=10−3\n\nλS=10−4\n\nAcc.\n\ny\n\nt i\ns n\ne D\n\n15\n\n10\n\n5\n\n0\n\n0\n\n0.5\n\nσ(γ)\n\nb=0 b=1 b=2 b=3 b=4\n\n1\n\n4\n\n2\n\n0\n\n50% 40% 30% 20% 10% 0%\n\n0\n\n0.5\n\nσ(γ)\n\n1\n\n0\n\n1\n\nλS=10−3 λS=10−4\n\n3\n\n4\n\n2 b\n\nFigure 4: Effects of hyper-parameters λS and b for VGG-16 on CIFAR100. Left, Middle: Filter importance distributions after regularised training. Increasing λS or lowering b will produce sparser models. Right: One-shot performance at a 90% pruning ratio. Both over- and under-regularising can lead to performance degradation, however the search space for b is limited.\n\n5.3 ABLATIVE ANALYSIS\n\nIn this section we will perform two ablation studies to better understand the behaviour of the Receding Neuron Importance regulariser. We first analyse the effects of different hyper-parameters choices on neuron sparsity and one-shot pruning performance. Following that we investigate how much each layer of a VGG network will be pruned under comparable approaches.\n\nSparsity. In addition to the regularisation strength λS, our approach introduces the shifting parameter b. We show that these two parameters are complementary in controlling the gradient of the regularisation loss and effective at producing sparsity. While λS scales the amplitude of the gradi-\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nent, b will set the range at which neuron importances start to recede. Figure 4 (Left, Middle) shows how λS and b choices create different distributions of neuron importances for a VGG model on CIFAR100. Sparsity can be measured as the density around the zero importance limit, and increases with higher λS or lower b. The interplay between these two parameters creates flexibility in the level of sparsity produced and the distribution of the remaining neuron importances. This ability to create a high variety of distributions comes at the cost of relative hyper-parameter sensitivity with respect to pruning accuracy, however this limitation only becomes visible at high pruning thresholds. Figure 4 (Right) shows the one-shot pruning accuracy at a 90% pruning ratio, of models trained with selected hyper-parameters. We can observe that both under- and over-regularised models will suffer from severe pruning. Unsurprisingly, inducing a strong degree of sparsity during training does not translate into a highly prunable model.\n\nCIFAR10\n\nCIFAR100\n\nSlimming Polarization DeepHoyer\n\nUCS RNI\n\n100%\n\n75%\n\n50%\n\n25%\n\n0%\n\n0 1 2 3 4 5 6 7 8 9 101112\n\n0 1 2 3 4 5 6 7 8 9 101112\n\nFigure 5: Percentage of filters removed from each layer when pruning a VGG-16 model at a 90% ratio. The deeper half of layers contain over 70% of the total amount of filters in the network. RNI stands out by not over-pruning layers in the first half.\n\nPruning Locations. Figure 5 shows what percentage of filters are removed from each layer during pruning. UCS prunes in a local manner, removing the same fraction of filters from each layer, as can be seen from the unchanging colour-coding. It is worth reminding that in a VGG-style architecture the number of filters periodically doubles as the network becomes deeper.This has the consequence that global pruning methods will achieve their target in a large part through the amount of filters pruned from the deeper layers. These are less sensitive to pruning, as there will still be a high absolute number of filters left. Consequently, removing a large percentage of neurons from the first layers will not add much to the global target, but will significantly limit their expressive power. Thus, it is reasonable to assume that filters in the early layers of a network have a higher importance, since the capacity of these layers is already limited.\n\nFrom Figure 5 we can see that deeper layers are indeed pruned more heavily than ones closer to the input. There are however some exceptions: on the CIFAR10 dataset, the only method pruning a significant amount of filters from the first layer is DeepHoyer - which also achieves the lowest accuracy based on the results from Table 2. On the CIFAR100 dataset Network Slimming and Neuron Polarization start excessively pruning from the 5th layer onward. RNI is the only approach that starts to heavily prune only in the latter half of the network. Our method, along with UCS, are the only ones that do not suffer catastrophic damage after pruning 90% of the total number of filters.\n\n6 CONCLUSION\n\nIn this work we introduce RNI, a novel sparsity regularisation for structured pruning, under which only the weights of unimportant neurons is designed to recede. To this end, we propose σBatchNorm, a BatchNorm variation with bounded scaling factors, enabling the construction of such targeted regularisation functions without sacrificing performance. Our method consistently outperforms state-of-the-art methods for VGG/ResNet architectures on CIFAR and ImageNet datasets. We show that accurately comparing methods requires evaluation on a spectrum of pruning ratios, as differences become more pronounced at higher levels. For VGG architectures our method significantly reduces the performance degradation from severe pruning compared to prior art. In future work we would like to explore more applications of the RNI regulariser as well as a way to schedule its hyper-parameters to obtain a pre-determined sparsity ratio.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREPRODUCIBILITY STATEMENT\n\nThis paper is entirely reproducible. All experimental details including setup and hyper-parameters are described in depth in Sections 5.1 and 5.2. All reported results have been run on publicly available datasets. Furthermore, a runnable PyTorch repository has been linked and attached as supplementary material.\n\nREFERENCES\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n\narXiv:1607.06450, 2016.\n\nTianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances in Neural Information Processing Systems, 34, 2021.\n\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\n\nnetworks. arXiv preprint arXiv:1803.03635, 2018.\n\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neural arXiv preprint arXiv:2009.08576,\n\nnetworks at initialization: Why are we missing the mark? 2020.\n\nSong Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for\n\nefficient neural networks. arXiv preprint arXiv:1506.02626, 2015.\n\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating\n\ndeep convolutional neural networks. arXiv preprint arXiv:1808.06866, 2018.\n\nYang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of the IEEE international conference on computer vision, pp. 1389–1397, 2017.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nNamhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning\n\nbased on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.\n\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n\nefficient convnets. arXiv preprint arXiv:1608.08710, 2016.\n\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. LearnIn Proceedings of the IEEE\n\ning efficient convolutional networks through network slimming. International Conference on Computer Vision, pp. 2736–2744, 2017.\n\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\n\nnetwork pruning. arXiv preprint arXiv:1810.05270, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nJian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional\n\nneural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016a.\n\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional\n\nneural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016b.\n\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nAlex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural\n\nnetwork pruning. arXiv preprint arXiv:2003.02389, 2020.\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510–4520, 2018.\n\nShibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-\n\nization help optimization? arXiv preprint arXiv:1805.11604, 2018.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\n\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\n\nXavier Suau, Nicholas Apostoloff, et al. Filter distillation for network compression. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 3129–3138. IEEE, 2020.\n\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-\n\ngredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.\n\nChaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by\n\npreserving gradient flow. arXiv preprint arXiv:2002.07376, 2020.\n\nDong Wang, Lei Zhou, Xueni Zhang, Xiao Bai, and Jun Zhou. Exploring linear relationship in\n\nfeature map subspace for convnets compression. arXiv preprint arXiv:1803.05729, 2018.\n\nYuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on\n\ncomputer vision (ECCV), pp. 3–19, 2018.\n\nHuanrui Yang, Wei Wen, and Hai Li. Deephoyer: Learning sparser neural network with differen-\n\ntiable scale-invariant sparsity measures. arXiv preprint arXiv:1908.09979, 2019.\n\nRuichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao, ChingYung Lin, and Larry S Davis. Nisp: Pruning networks using neuron importance score propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9194– 9203, 2018.\n\nTao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li. Neuron-level structured pruning using polarization regularizer. Advances in Neural Information Processing Systems, 33, 2020.\n\n11",
    "reference": "# Summary Of The Paper\n\nThe paper proposes a new way to measure the 'importance' of a neuron, by re-parameterizing the scaling parameters of the batchnorm layers. The proposed method is to use a sigmoid on the scaling parameter, to ensure that it lies in [0,1], and the value of the sigmoid reflects the importance. Using this value (value closer to 1 means important), the 'unimportant' neurons are pruned in a one-shot manner. The paper also proposes a new regularizer for this scaling parameter, to control the amount of neurons to be pruned. The results on simpler image classification tasks like CIFAR-10, -100 suggest that this method indeed outperforms other methods, particularly at higher pruning levels. However, on more complex task like Imagenet, it seems to perform worse than the UCS method. Further, the algorithm is only evaluated on image tasks (CIFAR and ImageNet), and two architectures (ResNet and VGG).\n\n# Strength And Weaknesses\n\nStrengths:\n1. The proposed measure and regularizer seem to be a more principled magnitude based pruning method than existing methods that directly work on the magnitude and apply an L1 regularizer. \n\n2. The results on CIFAR-10, 100 suggest that indeed this method has potential, since it outperforms existing methods on higher pruning ratios.\n\nConcerns:\n1. The proposed method is only tested on 2 architectures: ResNet and VGG, and on image tasks CIFAR and ImageNet. \n\n- Can the method be applied to Transformer architecture on language tasks? \n- The paper applies the method only for one-shot pruning. Can this be applied in multiple iterations, pruning a certain fraction of neurons in each round?\n- Can this method be used to prune pre-trained networks? For example, take a pretrained network and fine-tune if for a few epochs on a downstream task, and then apply this pruning method.\n\n2. Although the method performs well on simpler tasks like CIFAR, it performs worse than UCS on difficult tasks like ImageNet. The paper says that this is because \"This again indicates that for more difficult datasets, global methods over-prune the wrong layers\". I think this is a major limitation. However, I think this might be remedied, for example, by a per-layer re-normalization of the importance scores, so that a similar ratio of neurons are pruned in every layer.\n\n3. (Minor) The paper promotes the proposed method by claiming that it measures the 'importance' of neurons by bounding the scaling parameter of batchnorm in the range [0, 1]. However, I am not fully convinced by this. Consider the case when a neuron has a low scaling parameter, but in its outgoing weights to the next layer, a few (say 10%) of the weights are large. That means that even though the output magnitude of that neuron is low, some neurons in the next layer give high weightage to its output. In that case, this neuron can still be important. In short, what I mean to say is that just the magnitude of a neuron's output might not necessarily capture its 'importance'. While this method still seems more principled than previous methods at using the scaling parameter's magnitude for pruning, I do not agree that it captures the importance of the neuron. I would suggest clarifying this in the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is clearly written. I have not tried to run the provided code to reproduce the results.\n\n# Summary Of The Review\n\nI think the proposed method to measure the 'importance' of a neuron is more principled than previous methods that use magnitude, but I think the method can be more refined to deal with the problem of over-pruning the wrong layers for complex tasks, and by adding more experiments to show how the method performs for different architectures and tasks.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Published as a conference paper at ICLR 2023\n\nLIQUID STRUCTURAL STATE-SPACE MODELS\n\nRamin Hasani †∗ CSAIL, MIT\n\nMathias Lechner † CSAIL, MIT\n\nTsun-Hsuan Wang CSAIL, MIT\n\nMakram Chahine CSAIL, MIT\n\nAlexander Amini CSAIL, MIT\n\nDaniela Rus CSAIL, MIT\n\nABSTRACT\n\nA proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on an extensive series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structured SSM, such as S4, is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structured statespace model, dubbed Liquid-S4, improves generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition dataset, Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4’s kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n1\n\nINTRODUCTION\n\nLearning representations from sequences of data requires expressive temporal and structural credit assignment. In this space, the continuous-time neural network class of liquid time-constant networks (LTC) (Hasani et al., 2021b) has shown theoretical and empirical evidence for their expressivity and their ability to capture the cause and effect of a given task from high-dimensional sequential demonstrations (Lechner et al., 2020a; Vorbach et al., 2021; Wang et al., 2022; Hasani et al., 2022; Yin et al., 2022). Liquid networks are nonlinear state-space models (SSMs) with an input-dependent state transition module that enables them to learn to adapt the dynamics of the model to incoming inputs, at inference, as they are dynamic causal models (Friston et al., 2003). Their complexity, however, is bottlenecked by their differential equation numerical solver that limits their scalability to longer-term sequences. How can we take advantage of LTC’s generalization and causality capabilities and scale them to competitively learn long-range sequences without gradient issues, compared to advanced recurrent neural networks (RNNs) (Rusch & Mishra, 2021a; Erichson et al., 2021; Gu et al., 2020a), convolutional networks (CNNs) (Lea et al., 2016; Romero et al., 2021b; Cheng et al., 2022), and attention-based models (Vaswani et al., 2017)?\n\nIn this work, we set out to leverage the elegant formulation of structured state-space models (S4) (Gu et al., 2022a) to obtain linear liquid network instances that possess the approximation capabilities of both S4 and LTCs. This is because structured SSMs are shown to largely dominate advanced RNNs, CNNs, and Transformers across many data modalities such as text, sequence of pixels, audio, and time series (Gu et al., 2021; 2022a;b; Gupta, 2022). structured SSMs achieve such impressive performance by using three main mechanisms: 1) High-order polynomial projection operators (HiPPO)\n\n∗correspondence to: rhasani@mit.edu Code: https://github.com/raminmh/liquid-s4.\n\n† indicates authors with equal contributions.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\n(Gu et al., 2020a) that are applied to state and input transition matrices to memorize signals’ history, 2) diagonal plus low-rank parametrization of the obtained HiPPO (Gu et al., 2022a), and 3) an efficient (convolution) kernel computation of an SSM’s transition matrices in the frequency domain, transformed back in time via an inverse Fourier transformation (Gu et al., 2022a).\n\nTo combine S4 and LTCs, instead of modeling sequences by linear state-space models of the form ̇x = A x + B u, y = C x, (as done in structured and diagonal SSMs (Gu et al., 2022a;b)), we propose to use a linearized LTC state-space model (Hasani et al., 2021b), given by the following ̇x = (A + B u) x + B u, y = C x. We show that this dynamical system can also dynamics: be efficiently solved via the same parametrization of S4, giving rise to an additional convolutional Kernel that accounts for the similarities of lagged signals. We call the obtained model LiquidS4. Through extensive empirical evaluation, we show that Liquid-S4 consistently leads to better generalization performance compared to all variants of S4, CNNs, RNNs, and Transformers across many time-series modeling tasks. In particular, we achieve SOTA performance on the Long Range Arena benchmark (Tay et al., 2020b). To sum up, we make the following contributions:\n\n1. We introduce Liquid-S4, a new state-space model that encapsulates the generalization and causality capabilities of liquid networks as well as the memorization and scalability of S4. 2. We achieve state-of-the-art performance on pixel-level sequence classification, text, speech recognition, and all six tasks of the long-range arena benchmark with an average accuracy of 87.32%. On the full raw Speech Command recognition dataset, Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameters. Finally, on the BIDMC vital signs dataset, Liquid-S4 achieves SOTA in all modes.\n\n2 RELATED WORKS\n\nLearning Long-Range Dependencies with RNNs. Sequence modeling can be performed autoregressively with RNNs which possess persistent states (Little, 1974) originated from Ising (Brush, 1967) and Hopfield networks (Hopfield, 1982; Ramsauer et al., 2020). Discrete RNNs approximate continuous dynamics step-by-steps via dependencies on the history of their hidden states, and continuous-time (CT) RNNs use ordinary differential equation (ODE) solvers to unroll their dynamics with more elaborate temporal steps (Funahashi & Nakamura, 1993).\n\nCT-RNNs can perform remarkable credit assignment in sequence modeling problems both on regularly sampled, irregularly-sampled data (Pearson et al., 2003; Li & Marlin, 2016; Belletti et al., 2016; Roy & Yan, 2020; Foster, 1996; Amig ́o et al., 2012; Kowal et al., 2019), by turning the spatiotemproal dependencies into vector fields (Chen et al., 2018), enabling better generalization and expressivity (Massaroli et al., 2020; Hasani et al., 2021b). Numerous works have studied their characteristics to understand their applicability and limitations in learning sequential data and flows (Lechner et al., 2019; Dupont et al., 2019; Durkan et al., 2019; Jia & Benson, 2019; Grunbacher et al., 2021; Hanshu et al., 2020; Holl et al., 2020; Quaglino et al., 2020; Kidger et al., 2020; Hasani et al., 2020; Liebenwein et al., 2021; Gruenbacher et al., 2022).\n\nHowever, when these RNNs are trained by gradient descent (Rumelhart et al., 1986; Allen-Zhu & Li, 2019; Sherstinsky, 2020), they suffer from the vanishing/exploding gradients problem, which makes difficult the learning of long-term dependencies in sequences (Hochreiter, 1991; Bengio et al., 1994). This issue happens in both discrete RNNs such as GRU-D with its continuous delay mechanism (Che et al., 2018) and Phased-LSTMs (Neil et al., 2016), and continuous RNNs such as ODE-RNNs (Rubanova et al., 2019), GRU-ODE (De Brouwer et al., 2019), Log-ODE methods (Morrill et al., 2020) which compresses the input time-series by time-continuous path signatures (Friz & Victoir, 2010), and neural controlled differential equations (Kidger et al., 2020), and liquid time-constant networks (LTCs) (Hasani et al., 2021b).\n\nNumerous solutions have been proposed to resolve these gradient issues to enable long-range dependency learning. Examples include discrete gating mechanisms in LSTMs (Hochreiter & Schmidhuber, 1997; Greff et al., 2016; Hasani et al., 2019), GRUs (Chung et al., 2014), continuous gating mechanisms such as CfCs (Hasani et al., 2021a), hawks LSTMs (Mei & Eisner, 2017), IndRNNs (Li et al., 2018), state regularization (Wang & Niepert, 2019), unitary RNNs (Jing et al., 2019), dilated RNNs (Chang et al., 2017), long memory stochastic processes (Greaves-Tunnell & Harchaoui, 2019), recurrent kernel networks (Chen et al., 2019), Lipschitz RNNs (Erichson et al., 2021), symmetric skew decomposition (Wisdom et al., 2016), infinitely many updates in iRNNs\n\n2\n\nPublished as a conference paper at ICLR 2023\n\n(Kag et al., 2019), coupled oscillatory RNNs (coRNNs) (Rusch & Mishra, 2021a), mixed-memory RNNs (Lechner & Hasani, 2021), and Legendre Memory Units (Voelker et al., 2019).\n\nLearning Long-range Dependencies with CNNs and Transformers. RNNs are not the only solution to learning long-range dependencies. Continuous convolutional kernels such as CKConv (Romero et al., 2021b) and (Romero et al., 2021a), and circular dilated CNNs (Cheng et al., 2022) have shown to be efficient in modeling long sequences faster than RNNs. There has also been a large series of works showing the effectiveness of attention-based methods for modeling spatiotemporal data. A large list of these models is listed in Table 6. These baselines have recently been largely outperformed by the structured state-space models (Gu et al., 2022a).\n\nState-Space Models. SSMs are well-established frameworks to study deterministic and stochastic dynamical systems (Kalman, 1960). Their state and input transition matrices can be directly learned by gradient descent to model sequences of observations (Lechner et al., 2020b; Hasani et al., 2021b; Gu et al., 2021). In a seminal work, Gu et al. (2022a) showed that with a couple of fundamental algorithmic methods on memorization and computation of input sequences, SSMs can turn into the most powerful sequence modeling framework to-date, outperforming advanced RNNs, temporal and continuous CNNs (Cheng et al., 2022; Romero et al., 2021b;a) and a wide variety of Transformers (Vaswani et al., 2017), available in Table 6 by a significant margin.\n\nThe key to their numerical performance is their derivation of higher-order polynomial projection (HiPPO) matrix (Gu et al., 2020a) obtained by a scaled Legendre measure (LegS) inspired by the Legendre Memory Units (Voelker et al., 2019) to memorize input sequences. Their efficient runtime and memory are derived from their normal plus-low rank representation.It was also shown recently that diagonal SSMs (S4D) (Gupta, 2022) could be as performant as S4 in learning long sequences when parametrized and initialized properly (Gu et al., 2022b;c). Concurrent with our work, there is also a new variant of S4 introduced as simplified-S4 (S5) (Smith et al., 2022) that tensorizes the 1-D operations of S4 to gain a more straightforward realization of SSMs. Here, we introduce LiquidS4, which is obtained by a more expressive SSM, namely liquid time-constant (LTC) representation (Hasani et al., 2021b) which achieves SOTA performance across many benchmarks.\n\n3 SETUP AND METHODOLOGY\n\nIn this section, we first revisit the necessary background to formulate our liquid structured statespace models. We then set up and sketch our technical contributions.\n\n3.1 BACKGROUND: STRUCTURED STATE-SPACE MODELS (S4)\n\nWe aim to design an end-to-end sequence modeling framework built by SSMs. A continuous-time SSM representation of a linear dynamical system is given by the following set of equations:\n\n ̇x(t) = A x(t) + B u(t),\n\ny(t) = C x(t) + D u(t).\n\n(1)\n\nHere, x(t) is an N -dimensional latent state, receiving a 1-dimensional input signal u(t), and computing a 1-dimensional output signal y(t). A(N ×N ), B(N ×1), C(1×N ) and D(1×1) are system’s parameters. For the sake of brevity, throughout our analysis, we set D = 0 as it can be added eventually after construction of our main results in the form of a skip connection (Gu et al., 2022a).\n\nDiscretization of SSMs. In order to create a sequence-to-sequence model similar to a recurrent neural network (RNN), we discretize the continuous-time representation of SSMs by the trapezoidal rule (bilinear transform) as follows (sampling step = δt) (Gu et al., 2022a):\n\nxk = A xk−1 + B uk,\n\nyk = C xk\n\nThis is obtained via the following modifications to the transition matrices:\n\nA = (I −\n\nδt 2\n\nA)−1(I +\n\nδt 2\n\nA),\n\nB = (I −\n\nδt 2\n\nA)−1 δt B, C = C\n\n(2)\n\n(3)\n\nWith this transformation, we constructed a discretized seq-2-seq model that can map the input uk to output yk, via the hidden state xk ∈ RN . A is the hidden transition matrix, B and C are input and output transition matrices, respectively.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nCreating a Convolutional Representation of SSMs. The system described by Eq. 2 and Eq. 3, can be trained via gradient descent to learn to model sequences, in a sequential manner which is not scalable. To improve this, we can write the discretized SSM in Eq. 2 as a discrete convolutional kernel. To construct the convolutional kernel, let us unroll the system of Eq. 2 in time as follows, assuming a zero initial hidden states x−1 = 0:\n\nx0 = Bu0,\n\nx1 = ABu0 + Bu1,\n\n2\n\nx2 = A\n\nBu0 + ABu1 + Bu2, 2\n\ny0 = CBu0,\n\ny1 = CABu0 + CBu1,\n\ny2 = CA\n\nBu0 + CABu1 + CBu2,\n\nThe mapping u0,k → yk can now be formulated into a convolutional kernel explicitly:\n\nyk = CA\n\nk\n\nBu0 + CA\n\nk−1\n\nBu1 + . . . CABuk−1 + CBuk,\n\ny = K ∗ u\n\nK ∈ RL := KL(C, A, B) := (cid:0)CA\n\ni\n\nB(cid:1)\n\ni∈[L] = (cid:0)CB, CAB, . . . , CA\n\nL−1\n\nB(cid:1)\n\n. . .\n\n. . .\n\n(4)\n\n(5)\n\n(6)\n\nEq. 5 is a non-circular convolutional kernel. Gu et al. (2022a) showed that under the condition that K is known, it can be solved very efficiently by a black-box Cauchy kernel computation pipeline.\n\nComputing S4 Kernel Efficiently: Gu et al. (2022a) showed that the S4 convolution kernel could be computed efficiently using the following elegant parameterization tricks:\n\n• To obtain better representations in sequence modeling schemes by SSMs, instead of randomly initializing the transition matrix A, we can use the normal plus low-Rank (NPLR) matrix below, called the Hippo Matrix (Gu et al., 2020a) which is obtained by the scaled Legendre measure (LegS) (Gu et al., 2021; 2022a):\n\n(HiPPO Matrix)\n\nAnk = −\n\n \n\n\n\n(2n + 1)1/2(2k + 1)1/2 n + 1 0\n\nif n > k if n = k if n < k\n\n(7)\n\n• The NPLR representation of this matrix is the following (Gu et al., 2022a):\n\nA = V ΛV ∗ − P Q⊤ = V (Λ − (V ∗P ) (V ∗Q)∗) V ∗ (8) Here, V ∈ CN ×N is a unitary matrix, Λ is diagonal, and P , Q ∈ RN ×r are the lowrank factorization. Eq. 7 is normal plus low rank with r = 1 (Gu et al., 2022a). With the decomposition in Eq. 8, we can obtain A over complex numbers in the form of diagonal plus low-rank (DPLR) (Gu et al., 2022a).\n\n• Vectors Bn and Pn are initialized by Bn = (2n + 1) 1\n\n2 and Pn = (n + 1/2) 1\n\n2 (Gu et al.,\n\n2022b). Both vectors are trainable.\n\n• Furthermore, it was shown in Gu et al. (2022b) that with Eq. 8, the eigenvalues of A might be on the right half of the complex plane, resulting in numerical instability. To resolve this, Gu et al. (2022b) recently proposed to use the parametrization Λ − P P ∗ instead of Λ − P Q∗.\n\n• Computing the powers of A in direct calculation of the S4 kernel K is computationally expensive. S4 computes the spectrum of K instead of direct computations, which reduces the problem of matrix powers to matrix inverse computation (Gu et al., 2022a). S4 then computes this convolution kernel via a black-box Cauchy Kernel efficiently, and recovers K by an inverse Fourier Transform (iFFT) (Gu et al., 2022a).\n\n3.2 LIQUID STRUCTURAL STATE-SPACE MODELS\n\nIn this work, we construct a convolutional kernel corresponding to a linearized version of LTCs (Hasani et al., 2021b); an expressive class of continuous-time neural networks that demonstrate attractive generalizability out-of-distribution and are dynamic causal models (Vorbach et al., 2021; Friston et al., 2003; Hasani et al., 2020). In their general form, the state of a liquid time-constant network at each time-step is given by the set of ODEs described below (Hasani et al., 2021b):\n\ndx(t) dt\n\n= −\n\n(cid:104)\n\nA + B ⊙ f (x(t), u(t), t, θ)\n\n⊙x(t) + B ⊙ f (x(t), u(t), t, θ).\n\n(9)\n\n(cid:105)\n\n(cid:124)\n\n(cid:123)(cid:122) Liquid time-constant\n\n(cid:125)\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nIn this expression, x(N ×1)(t) is the vector of hidden state of size N , u(m×1)(t) is an input signal with m features, A(N ×1) is a time-constant state-transition mechanism, B(N ×1) is a bias vector, and ⊙ represents the Hadamard product. f (.) is a bounded nonlinearity parametrized by θ.\n\nOur objective is to show how the liquid time-constant (i.e., an input-dependent state transition mechanism in state-space models can enhance its generalization capabilities by accounting for the covariance of the input samples. To do this, we linearize the LTC formulation of Eq. 9 in the following to better connect the model to SSMs. Let’s dive in:\n\nLinear Liquid Time-Constant State-Space Model. A Linear LTC SSM can be presented by the following coupled bilinear (first order bilinear Taylor approximation (Penny et al., 2005)) equation:\n\n ̇x(t) = (cid:2)A + IN B u(t)] x(t) + B u(t),\n\ny(t) = C x(t)\n\n(10)\n\nSimilar to Eq. 1, x(t) is an N -dimensional latent state, receiving a 1-dimensional input signal u(t), and computing a 1-dimensional output signal y(t). A(N ×N ), B(N ×1), and C(1×N ). Note that D is set to zero for simplicity. In Eq. 10, JN is an N × N unit matrix that adds B u(t) element-wise to A. This dynamical system allows the coefficient (state transition compartment) of state vector x(t) to be input dependent which, as a result, allows us to realize more complex dynamics.\n\nDiscretization of Liquid-SSMs. We can use a forward Euler transformation to discretize Eq. 10 into the following discretization:\n\nxk = (cid:0)A + B uk\n\n(cid:1) xk−1 + B uk,\n\nyk = C xk\n\n(11)\n\nThe discretized parameters would then correspond to: A = I + δt 2 A, B = δt B, and C = C, which are function of the continuous-time coefficients A, B, and C, and the discretization step δt. Given the properties of the transition matrices A and B, and ranges of δt, we could use the more stable bilinear discretization of matrices A and B, of Eq. 3 as well, as the Forward Euler discretization and the bilinear transformation of A and B presented in Eq. 3 stay close to each other (Appendix D).\n\nCreating a Convolutional Representation of Liquid-SSMs. Similar to Eq. 4, we first unroll the Liquid-SSM in time to construct a convolutional kernel of it. By assuming x−1 = 0, we have:\n\nx0 = Bu0,\n\ny0 = CBu0\n\n2 x1 = ABu0 + Bu1+ B\n\nu0u1,\n\n2 y1 = CABu0 + CBu1+CB\n\nu0u1\n\n(12)\n\n2 x2 = A\n\nBu0 + ABu1 + Bu2+ AB\n\n2\n\nu0u1 + AB\n\n2\n\n2 u0u2 + B\n\nu1u2 + B\n\n3\n\nu0u1u2\n\ny2 = CA\n\n2\n\n2 Bu0 + CABu1 + CBu2+ CAB\n\n2 u0u1 + CAB\n\n2 u0u2 + CB\n\nu1u2 + CB\n\n3\n\nu0u1u2,\n\n. . .\n\nThe resulting expressions of the Liquid-SSM at each time step consist of two types of weight configurations: 1. Weights corresponding to the mapping of individual time instances of inputs independently, shown in black in Eq. 12, and 2. Weights associated with all orders of auto-correlation of the input signal, shown in violet in Eq. 12. The first set of weights corresponds to the convolutional kernel of the simple SSM, shown by Eq. 5 and Eq. 6, whereas the second set leads to the design of an additional input correlation kernel, which we call the liquid kernel. These kernels generate the following input-output mapping:\n\nyk = CA\n\nk\n\nBu0 + CA\n\nk−1\n\nBu1 + . . . CABuk−1 + CBuk +\n\nP (cid:88)\n\n(cid:88)\n\nCA\n\n(k+1−p−i)\n\np\n\nB\n\nuiui+1 . . . up\n\n(13)\n\np=2\n\nuiui+1 ... up∈Π(k+1,p)\n\nfor i ∈ Z and i ≥ 0, →\n\ny = K ∗ u + Kliquid ∗ ucorrelations.\n\nHere, Π(k + 1, p) represents (cid:0)k+1 (cid:1) permuted indices. For instance, let us assume we have a 1dimensional input signal u(t) of length L = 100 on which we run the liquid-SSM kernel. We set the hyperparameters P = 4. This value represents the maximum order of the correlation terms we\n\np\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 1 LIQUID-S4 KERNEL - The S4 convolution kernel (highlighted in black) is used from Gu et al. (2022a) and Gu et al. (2022b). Liquid kernel computation is highlighted in purple.\n\nInput: S4 parameters Λ, P , B, C ∈ CN , step size ∆, liquid kernel order P, inputs seq length L,\n\nliquid kernel sequence length ̃L\n\nOutput: SSM convolution kernel K = KL(A, B, C) and SSM liquid kernel Kliquid =\n\nK ̃L(A, B, C) for A = Λ − P P ∗ (Eq. 6) C\n\nI − A\n\nL(cid:17)∗\n\n(cid:16)\n\n1: (cid:101)C ←\n\n▷ Truncate SSM generating function (SSMGF) to length L\n\n(cid:21) (cid:20)k00(ω) k01(ω) k10(ω) k11(ω)\n\n2:\n\n(cid:104)\n\n(cid:105)∗ (cid:16) 2\n\n1−ω\n\n(cid:17)−1\n\n←\n\n(cid:101)C P\n\n[B P ] (cid:2)k00(ω) − k01(ω)(1 + k11(ω))−1k10(ω)(cid:3)\n\n1+ω − Λ\n\n∆\n\n▷ Black-box Cauchy kernel\n\n▷ Woodbury Identity\n\n▷ Evaluate SSMGF at all roots of unity ω ∈ ΩL ▷ Inverse Fourier Transform ▷ Liquid-S4 Kernel as shown in Eq. 14\n\n(cid:105)\n\np−1 (L− ̃L,L)\n\n∗ J ̃L\n\n▷ J ̃L is a backward identity matrix\n\n▷ Liquid-S4 Kernel of Eq. 14 with A reduced to Identity.\n\n1+ω\n\n3: ˆK(ω) ← 2 4: ˆK = { ˆK(ω) : ω = exp(2πi k 5: K ← iFFT( ˆK) 6: if Mode == KB then 7:\n\nfor p in {2, . . . , P} do\n\nL )}\n\n(cid:104)\n\n8:\n\nKliquid=p =\n\nK(L− ̃L,L) ⊙ B\n\nKliquid.append(Kliquid=p)\n\nend for\n\n9: 10: 11: else if Mode == PB then 12:\n\nfor p in {2, . . . , P} do\n\nKliquid=p = C ⊙ B Kliquid.append(Kliquid=p)\n\np−1 (L− ̃L,L)\n\n13:\n\n14: 15: 16: end if\n\nend for\n\nwould want to take into account to output a decision. This means that the signal ucorrelations in Eq. 13 will contain all combinations of 2 order correlation signals (cid:0)L+1 (cid:1), uiujuk and 4 order signals (cid:0)L+1 (cid:1), uiujukul. The kernel weights corresponding to this auto-correlation signal are given in Appendix A. This additional kernel takes the temporal similarities of incoming input samples into consideration. This way, Liquid-SSM gives rise to a more general sequence modeling framework. The liquid convolutional kernel, Kliquid is as follows:\n\n(cid:1), uiuj, 3 order (cid:0)L+1\n\n4\n\n2\n\n3\n\nKliquid ∈ R ̃L := KL(C, A, B) := (cid:0)CA\n\n( ̃L−i−p)\n\nB\n\np(cid:1)\n\ni∈[ ̃L], p∈[P]\n\n= (cid:0)CA\n\n ̃L−2\n\n2\n\nB\n\n, . . . , CB\n\np(cid:1)\n\n(14)\n\nHow to compute Liquid-S4 kernel efficiently? Kliquid possess similar structure to the S4 kernel. In particular, we have: Proposition 1. The Liquid-S4 kernel for each order p ∈ P, Kliquid, can be computed by the anti-diagonal transformation (flip operation) of the product of the S4 convolution kernel, K = (cid:0)CB, CAB, . . . , CA\n\nB(cid:1), and a vector B\n\n∈ RN .\n\nL−1\n\np−1\n\nThe proof is given in Appendix. Proposition 1 indicates that the Liquid-s4 kernel can be obtained from the precomputed S4 kernel and a Hadamard product of that kernel with the transition vector B powered by the chosen liquid order. This is illustrated in Algorithm 1, lines 6 to 10, corresponding to a mode we call KB, which stands for Kernel × B.\n\nAdditionally, we introduce a simplified Liquid-S4 kernel that is easier to compute while is as expressive as or even better performing than the KB kernel. To obtain this, we set the transition matrix A in Liquid-S4 of Eq. 14, with an identity matrix, only for the input correlation terms. This way, the Liquid-s4 Kernel for a given liquid order p ∈ P reduces to the following expression:\n\n(Liquid-S4 - PB)\n\nKliquid=p ∈ R ̃L := KL(C, B) := (cid:0)CB\n\np(cid:1)\n\ni∈[ ̃L], p∈[P]\n\n(15)\n\nWe call this kernel Liquid-S4 - PB, as it is obtained by powers of the vector B. The computational steps to get this kernel is outlined in Algorithm 1 lines 11 to 15.\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nTable 1: Performance on Long Range Arena Tasks. Numbers indicate validation accuracy (standard deviation). The accuracy of models denoted by * is reported from (Tay et al., 2020b). Methods denoted by ** are reported from (Gu et al., 2022a). The rest of the models’ performance results are reported from the cited paper. See Appendix for accuracy on test set.\n\nModel (input length)\n\nListOps 2048\n\nIMDB 2048\n\nCIFAR 1024\n\nPathfinder 1024\n\nPath-X Avg. 16384\n\nRandom∗ Transformer∗ (Vaswani et al., 2017) Local Att.∗ (Tay et al., 2020b) Sparse Transformer∗ (Child et al., 2019) Longformer∗ (Beltagy et al., 2020) Linformer∗ (Wang et al., 2020) Reformer∗ (Kitaev et al., 2019) Sinkhorn Trans.∗ (Tay et al., 2020a) BigBird∗ (Zaheer et al., 2020) Linear Trans.∗ (Katharopoulos et al., 2020) Performer∗ (Choromanski et al., 2020)\n\nFNet∗∗ (Lee-Thorp et al., 2021) Nystr ̈omformer∗∗ (Xiong et al., 2021) Luna-256∗∗ Ma et al. (2021) H-Transformer-1D∗∗ (Zhu & Soricut, 2021)\n\nCDIL (Cheng et al., 2022)\n\nDSS (Gupta, 2022) S4 (original)∗∗ (Gu et al., 2022a) S4-LegS (Gu et al., 2022b) S4-FouT (Gu et al., 2022b) S4-LegS/FouT (Gu et al., 2022c) S4D-LegS (Gu et al., 2022b) S4D-Inv (Gu et al., 2022b) S4D-Lin (Gu et al., 2022b) S5 original (Smith et al., 2022) S5 new (Smith et al., 2023)\n\nLiquid-S4-KB (ours) Liquid-S4-PB (ours)\n\n10.00 36.37 15.82 17.07 35.63 16.13 37.27 33.67 36.05 16.13 18.01\n\n35.33 37.15 37.25 49.53\n\n44.05\n\n50.00 64.27 52.98 63.58 62.85 65.90 56.10 61.20 64.02 65.90 65.40\n\n65.11 65.52 64.57 78.69\n\n86.78\n\nAAN 4000\n\n50.00 57.46 53.39 59.59 56.89 53.09 53.40 53.83 59.29 53.09 53.82\n\n59.61 79.56 79.29 63.99\n\n85.36\n\n10.00 42.44 41.46 44.24 42.22 42.34 38.07 41.23 40.83 42.34 42.77\n\n38.67 41.58 47.38 46.05\n\n66.91\n\n50.00 71.40 66.63 71.71 69.71 75.30 68.50 67.45 74.87 75.30 77.05\n\n77.80 70.94 77.72 68.78\n\n91.70\n\n57.6 58.35 59.60 (0.07) 57.88 (1.90) 60.45 (0.75) 60.47 (0.34) 60.18 (0.35) 60.52 (0.51) 61.00 62.15\n\n62.55 (0.10) 62.75 (0.20) p = 5\n\n76.6 76.02 86.82 (0.13) 86.34 (0.31) 86.78 (0.26) 86.18 (0.43) 87.34 (0.20) 86.97 (0.23) 86.51 89.31\n\n88.97 (0.02) 89.02 (0.04) p=6\n\n87.6 87.09 90.90 (0.15) 89.66 (0.88) 90.30 (0.28) 89.46 (0.14) 91.09 (0.01) 90.96 (0.09) 88.26 91.40\n\n91.10 (0.05) 91.20 (0.01) p=2\n\n85.8 87.26 88.65 (0.23) 89.07 (0.19) 89.00 (0.26) 88.19 (0.26) 87.83 (0.37) 87.93 (0.34) 86.14 88.00\n\n89.37 (0.22) 89.50 (0.40) p=3\n\n84.1 86.05 94.20 (0.25) 94.46 (0.26) 94.44 (0.08) 93.06 (1.24) 93.78 (0.25) 93.96 (0.60) 87.57 95.33\n\n94.50 (0.08) 94.80 (0.20) p=2\n\n50.00 x\nx x\nx x\nx x\nx x\nx\n\nx x\nx x\n\nx\n\n85.0 88.10 96.35 x\nx 91.95 92.80 x\n85.25 98.58\n\n96.10 96.66 p=2\n\n36.67 54.39 46.06 51.24 53.46 50.55 50.56 51.23 55.01 50.46 51.18\n\n54.42 57.46 59.37 61.41\n\n74.96\n\n79.45 80.48 86.09 77.90 78.50 84.89 85.50 78.39 82.46 87.46\n\n87.09 87.32\n\nComputational Complexity of the Liquid-S4 Kernel. The computational complexity of the S4Legs Convolutional kernel solved via the Cauchy Kernel is ̃O(N + L), where N is the state-size, and L is the sequence length [Gu et al. (2022a), Theorem 3]. Liquid-S4 both in KB and PB modes can be computed in ̃O(N + L + pmax ̃L). The added time complexity in practice is tractable. This is because we usually select the liquid orders, p, to be less than 10 (typically pmax = 3, and ̃L which is the number of terms we use to compute the input correlation vector, ucorrelation, is typically two orders of magnitude smaller than the sequence length.\n\n4 EXPERIMENTS WITH LIQUID-S4\n\nIn this section, we present an extensive evaluation of Liquid-S4 on sequence modeling tasks with very long-term dependencies and compare its performance to a large series of baselines ranging from advanced Transformers and Convolutional networks to many variants of state-space models. In the following, we first outline the baseline models we compare against. We then list the datasets we evaluated these models on and finally present results and discussions.\n\nBaselines. We consider a broad range of advanced models to compare Liquid-S4 with. These baselines include transformer variants such as vanilla and Sparse Transformers, a Transformer model with local attention, Longformer, Linformer, Reformer, Sinkhorn, BigBird, Linear Transformer, and Performer. We also include architectures such as FNets, Nystro ̈mformer, Luna-256, H-Transformer1D, and Circular Diluted Convolutional neural networks (CDIL). We then include a full series of state-space models and their variants such as diagonal SSMs (DSS), S4, S4-legS, S4-FouT, S4LegS/FouT, S4D-LegS, S4D-Inv, S4D-Lin and the Simplified structured state-space models (S5).\n\nDatasets. We first evaluate Liquid-S4’s performance on the well-studied Long Range Arena (LRA) benchmark (Tay et al., 2020b), where Liquid-S4 outperforms other S4 and S4D variants in every task with an average accuracy of 87.32%. LRA dataset includes six tasks with sequence lengths ranging from 1k to 16k. Concurrent work We then report Liquid-S4’s performance compared to other S4, and S4D variants, as well as other models, on the BIDMC Vital Signals dataset (Pimentel et al.,\n\n7\n\nPublished as a conference paper at ICLR 2023\n\n2016; Goldberger et al., 2000). BIDMC uses bio-marker signals of length 4000 to predict Heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2). We also experiment with the sCIFAR dataset that consists of the classification of flattened images in the form of 1024-long sequences into ten classes.\n\nHR\n\nModel\n\nBIDMC\n\nTable 2: Performance on BIDMC Vital Signs dataset. Numbers indicate RMSE on the test set. Models denoted by * is reported from (Gu et al., 2022b). The rest of the models’ performance results are reported from the cited paper.\n\nFinally, we perform Raw Speech Command (SC) recognition with full 35 labels as conducted very recently in the updated S4 article (Gu It is essential to deet al., 2022a). note that there is a modified speech command dataset that restricted the dataset to only ten output classes and is used in a couple of works (see for example Kidger et al. (2020); Gu et al. (2021); Romero et al. (2021b;a)). Aligned with the updated results reported in Gu et al. (2022a) and Gu et al. (2022b), we choose not to break down this dataset and use the full-sized benchmark. SC dataset contains sequences of length 16k to be classified into 35 commands. Gu et al. (2022a) introduced a new test case setting to assess the performance of models (trained on 16kHz sequences) on sequences of length 8kHz. S4 and S4D perform exceptionally well in this zero-shot test scenario.\n\nUnICORNN (Rusch & Mishra, 2021b) coRNN (Rusch & Mishra, 2021a) CKConv∗ NRDE (Morrill et al., 2021) LSTM (Rusch & Mishra, 2021b) Transformer∗ XGBoost (Tan et al., 2021) Random Forest (Tan et al., 2021) Ridge Regress. (Tan et al., 2021)\n\nS4-LegS∗ (Gu et al., 2022b) S4-FouT∗ (Gu et al., 2022b) S4D-LegS∗ (Gu et al., 2022b) S4-(LegS/FouT)∗ (Gu et al., 2022b) S4D-Inv∗ (Gu et al., 2022b) S4D-Lin∗ (Gu et al., 2022b)\n\n0.090 (0.006) 0.068 (0.003) 0.102 (0.001) 0.080 (0.007) 0.110 (0.001) 0.114 (0.003)\n\n0.247 (0.062) 0.301 (0.030) 0.248 (0.036) 0.163 (0.008) 0.254 (0.022) 0.226 (0.008)\n\n0.332 (0.013) 0.339 (0.020) 0.367 (0.001) 0.344 (0.032) 0.373 (0.024) 0.379 (0.006)\n\n0.869 -\n1.051 1.29 -\n3.02 1.52 1.74 4.16\n\n1.06 1.45 1.214 1.49 2.28 2.61 1.67 1.85 3.86\n\nLiquid-S4-KB (ours) Liquid-S4-PB (ours)\n\n0.310 (0.001) 0.303 (0.002) p=3\n\n0.162 (0.001) 0.158 (0.001) p=2\n\n0.068 (0.002) 0.066 (0.002) p=4\n\n1.39 1.81 2.05 2.97 10.7 12.2 4.72 5.69 17.3\n\nSPO2\n\nRR\n\n4.1 Results on Long Range Arena\n\nTable 6 depicts a comprehensive list of baselines benchmarked against each other on six long-range sequence modeling tasks in LRA. We observe that Liquid-S4 instances (all use the PB kernel with a scaled Legendre (LegS) configuration) with a small liquid order, p, ranging from 2 to 6, consistently outperform all baselines in all six tasks, establishing the new SOTA on LRA with an average performance of 87.32%. In particular, on ListOps, Liquid-S4 improves S4-LegS performance by more than 3%, on character-level IMDB by 2.2%, and on 1-D pixel-level classification (CIFAR) by 0.65%, while establishing the-state-of the-art on the hardest LRA task by gaining 96.66% accuracy. Liquid-S4 performs on par with improved S4 and S4D instances on both AAN and Pathfinder tasks. The performance of SSM models is generally well-beyond what advanced Transformers, RNNs, and Convolutional networks achieve on LRA tasks, with the Liquid-S4 variants standing on top.\n\nFigure 1: Performance vs Liquid Order in Liquid-S4 for A) ListOps, and B) IMDB datasets. More in Appendix. (n=3)\n\nThe impact of increasing Liquid Order p. Figure 1 illustrates how increasing the liquid order, p, can improve performance on ListOps and IMDB tasks from LRA (More results in Appendix).\n\n4.2 Results on BIDMC Vital Signs\n\nTable 2 demonstrates the performance of a variety of classical and advanced baseline models on the BIDMC dataset for all three heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2) level prediction tasks. We observe that Liquid-s4 with a PB kernel of order p = 3, p = 2, and p = 4, perform better than all S4 and S4D variants. It is worth denoting that Liquid-S4 is built by the same parametrization as S4-LegS (which is the official S4 model reported in the updated S4 report (Gu et al., 2022a)). In RR, Liquid-S4 outperforms S4-LegS by a significant margin of 36%. On SpO2, Liquid-S4 performs 26.67% better than S4-Legs. On HR, Liquid-S4 outperforms S4-Legs by 8.7% improvement in performance.\n\n8\n\nABPublished as a conference paper at ICLR 2023\n\n4.3 Results on Image Classification\n\nSimilar to the previous tasks, a Liquid-S4 network with PB kernel of order p = 3 outperforms all variants of S4 and S4D while being significantly better than Transformer and RNN baselines as summarized in Table 3.\n\n4.4 Results on Speech Commands\n\nTable 4 demonstrates that Liquid-S4 with p = 2 achieves the best performance amongst all benchmarks on the 16KHz testbed. LiquidS4 also performs competitively on the halffrequency zero-shot experiment, while it does not realize the best performance. Although the task is solved to a great degree, the reason could be that liquid kernel accounts for covariance terms. This might influence the learned representations in a way that hurts performance by a small margin in this zero-shot experiment. The hyperparameters are given in Appendix.\n\nTable 3: Performance on sCIFAR. Numbers indicate Accuracy (standard deviation). The baseline models are from Table 9 of Gu et al. (2022b).\n\nModel\n\nAccuracy\n\nTransformer (Trinh et al., 2018) FlexConv (Romero et al., 2021a) TrellisNet (Bai et al., 2018) LSTM r-LSTM (Trinh et al., 2018) UR-GRU (Gu et al., 2020b) HiPPO-RNN (Gu et al., 2020a) LipschitzRNN (Erichson et al., 2021)\n\nS4-LegS (Gu et al., 2022b) S4-FouT (Gu et al., 2022b) S4-(LegS/FouT) (Gu et al., 2022b)\n\nS4D-LegS (Gu et al., 2022b) S4D-Inv (Gu et al., 2022b) S4D-Lin (Gu et al., 2022b) S5 Smith et al. (2022)\n\nLiquid-S4-KB (ours) Liquid-S4-PB (ours)\n\n62.2 80.82 73.42 63.01 72.2 74.4 61.1 64.2\n\n91.80 (0.43) 91.22 (0.25) 91.58 (0.17)\n\n89.92 (1.69) 90.69 (0.06) 90.42 (0.03) 89.66\n\n91.86 (0.08) 92.02 (0.14) p=3\n\nIt is essential to denote that there is a modified speech command dataset that restricts the dataset to only ten output classes, namely SC10, and is used in a couple of works (see for example (Kidger et al., 2020; Gu et al., 2021; Romero et al., 2021b;a)). Aligned with the updated results reported in (Gu et al., 2022a) and (Gu et al., 2022b), we choose not to break down this dataset and report the full-sized benchmark in the main paper. Nevertheless, we conducted an experiment with SC10 and showed that even on the reduced dataset, with the same hyperparameters, we solved the task with a SOTA accuracy of 98.51%. The results are presented in Table 7.\n\n5 CONCLUSIONS\n\nWe showed that the performance of structured state-space models could be considerably improved if they are formulated by a linear liquid time-constant kernel, namely Liquid-S4. Liquid-S4 kernels are obtainable with minimal effort, with their kernel computing the similarities between time-lags of the input signals in addition to the main S4 diagonal plus low-rank parametrization. Liquid-S4 kernels with smaller parameter counts achieve SOTA performance on all six tasks of the Long-range arena dataset, on BIDMC heart rate, respiratory rate, and blood oxygen saturation, on sequential 1-D pixellevel image classification, and on Speech command recognition. As a final note, our experimental evaluations suggest that for challenging multivariate time series and modeling complex signals with long-range dependencies, SSM variants such as Liquid-S4 dominate other baselines, while for image and text data, a combination of SSMs and attention might enhance model quality.\n\nTable 4: Performance on Raw Speech Command dataset with Full 35 Labels.Numbers indicate Accuracy on test set. The baseline models are reported from Table 11 of (Gu et al., 2022b).\n\nModel\n\nParameters\n\n16000Hz\n\n8000Hz\n\nInceptionNet (Nonaka & Seita, 2021) ResNet-18 XResNet-50 ConvNet\n\nS4-LegS (Gu et al., 2022b) S4-FouT (Gu et al., 2022b) S4-(LegS/FouT) (Gu et al., 2022b)\n\nS4D-LegS (Gu et al., 2022b) S4D-Inv (Gu et al., 2022b) S4D-Lin (Gu et al., 2022b)\n\nLiquid-S4-KB (ours) Liquid-S4-PB (ours)\n\n481K 216K 904K 26.2M\n\n307K 307K 307K\n\n306K 306K 306K\n\n224K 224K\n\n61.24 (0.69) 77.86 (0.24) 83.01 (0.48) 95.51 (0.18)\n\n96.08 (0.15) 95.27 (0.20) 95.32 (0.10)\n\n95.83 (0.14) 96.18 (0.27) 96.25 (0.03)\n\n96.52 (0.04) 96.78 (0.05) p=2\n\n05.18 (0.07) 08.74 (0.57) 07.72 (0.39) 07.26 (0.79)\n\n91.32 (0.17) 91.59 (0.23) 90.72 (0.68)\n\n91.08 (0.16) 91.80 (0.24) 91.58 (0.33)\n\n91.30 (0.19) 90.00 (0.25) p=2\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis research was supported in part by the AI2050 program at Schmidt Futures (Grant G-22-63172) and the United States Air Force Artificial Intelligence Accelerator under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein. We are very grateful.\n\nREFERENCES\n\nZeyuan Allen-Zhu and Yuanzhi Li. Can sgd learn recurrent neural networks with provable general-\n\nization? In Advances in Neural Information Processing Systems, pp. 10331–10341, 2019.\n\nJos ́e M Amig ́o, Roberto Monetti, Thomas Aschenbrenner, and Wolfram Bunk. Transcripts: An algebraic approach to coupled time series. Chaos: An Interdisciplinary Journal of Nonlinear Science, 22(1):013105, 2012.\n\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In Inter-\n\nnational Conference on Learning Representations, 2018.\n\nFrancois W Belletti, Evan R Sparks, Michael J Franklin, Alexandre M Bayen, and Joseph E Gonzalez. Scalable linear causal inference for irregularly sampled time series with long range dependencies. arXiv preprint arXiv:1603.03336, 2016.\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n\narXiv preprint arXiv:2004.05150, 2020.\n\nYoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient\n\ndescent is difficult. IEEE transactions on neural networks, 5(2):157–166, 1994.\n\nStephen G Brush. History of the lenz-ising model. Reviews of modern physics, 39(4):883, 1967.\n\nShiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. Advances in neural information processing systems, 30, 2017.\n\nBenjamin Charlier, Jean Feydy, Joan Alexis Glaun`es, Franc ̧ois-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1–6, 2021.\n\nZhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):1–12, 2018.\n\nDexiong Chen, Laurent Jacob, and Julien Mairal. Recurrent kernel networks. In Advances in Neural\n\nInformation Processing Systems, pp. 13431–13442, 2019.\n\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in neural information processing systems, pp. 6571–6583, 2018.\n\nLei Cheng, Ruslan Khalitov, Tong Yu, and Zhirong Yang. Classification of long sequential data using circular dilated convolutional neural networks. arXiv preprint arXiv:2201.02143, 2022.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\n\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n\nEdward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous modeling of sporadically-observed time series. In Advances in Neural Information Processing Systems, pp. 7377–7388, 2019.\n\nEmilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes.\n\nIn Advances in\n\nNeural Information Processing Systems, pp. 3134–3144, 2019.\n\nConor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows.\n\nIn\n\nAdvances in Neural Information Processing Systems, pp. 7509–7520, 2019.\n\nBenjamin N. Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W. Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021.\n\nGrant Foster. Wavelets for period analysis of unevenly sampled time series. The Astronomical\n\nJournal, 112:1709, 1996.\n\nKarl J Friston, Lee Harrison, and Will Penny. Dynamic causal modelling. Neuroimage, 19(4):\n\n1273–1302, 2003.\n\nPeter K Friz and Nicolas B Victoir. Multidimensional stochastic processes as rough paths: theory\n\nand applications, volume 120. Cambridge University Press, 2010.\n\nKen-ichi Funahashi and Yuichi Nakamura. Approximation of dynamical systems by continuous\n\ntime recurrent neural networks. Neural networks, 6(6):801–806, 1993.\n\nAry L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov, Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. circulation, 101(23):e215–e220, 2000.\n\nAlexander Greaves-Tunnell and Zaid Harchaoui. A statistical investigation of long memory in lan-\n\nguage and music. In International Conference on Machine Learning, pp. 2394–2403, 2019.\n\nKlaus Greff, Rupesh K Srivastava, Jan Koutn ́ık, Bas R Steunebrink, and J ̈urgen Schmidhuber. Lstm: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10): 2222–2232, 2016.\n\nSophie Gruenbacher, Mathias Lechner, Ramin Hasani, Daniela Rus, Thomas A Henzinger, Scott A Smolka, and Radu Grosu. Gotube: Scalable statistical verification of continuous-depth models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, No 6, pp. 6755–6764, 2022.\n\nSophie Grunbacher, Ramin Hasani, Mathias Lechner, Jacek Cyranka, Scott A Smolka, and Radu Grosu. On the verification of neural odes with stochastic guarantees. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, No 13, pp. 11525–11535, 2021.\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 2020a.\n\nAlbert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800–3809. PMLR, 2020b.\n\nAlbert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R ́e. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021.\n\nAlbert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured\n\nstate spaces. In International Conference on Learning Representations, 2022a.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nAlbert Gu, Ankit Gupta, Karan Goel, and Christopher R ́e. On the parameterization and initialization\n\nof diagonal state space models. arXiv preprint arXiv:2206.11893, 2022b.\n\nAlbert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R ́e. How to train your hippo: State space models with generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037, 2022c.\n\nAnkit Gupta. Diagonal state spaces are as effective as structured state spaces. arXiv preprint\n\narXiv:2203.14343, 2022.\n\nYAN Hanshu, DU Jiawei, TAN Vincent, and FENG Jiashi. On robustness of neural ordinary differ-\n\nential equations. In International Conference on Learning Representations, 2020.\n\nRamin Hasani, Alexander Amini, Mathias Lechner, Felix Naser, Radu Grosu, and Daniela Rus. Response characterization for auditing cell dynamics in long short-term memory networks. In 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2019.\n\nRamin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu. The natural lottery ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 2020 International Conference on Machine Learning. JMLR. org, 2020.\n\nRamin Hasani, Mathias Lechner, Alexander Amini, Lucas Liebenwein, Max Tschaikowski, arXiv preprint\n\nGerald Teschl, and Daniela Rus. Closed-form continuous-depth models. arXiv:2106.13898, 2021a.\n\nRamin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu. Liquid timeconstant networks. Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):7657– 7666, May 2021b.\n\nRamin Hasani, Mathias Lechner, Alexander Amini, Lucas Liebenwein, Aaron Ray, Max Tschaikowski, Gerald Teschl, and Daniela Rus. Closed-form continuous-time neural networks. Nature Machine Intelligence, pp. 1–12, 2022.\n\nSepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen [in german] diploma thesis.\n\nTU M ̈unich, 1991.\n\nSepp Hochreiter and J ̈urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n\n1735–1780, 1997.\n\nPhilipp Holl, Vladlen Koltun, and Nils Thuerey. Learning to control pdes with differentiable physics.\n\narXiv preprint arXiv:2001.07457, 2020.\n\nJohn J Hopfield. Neural networks and physical systems with emergent collective computational\n\nabilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.\n\nJunteng Jia and Austin R Benson. Neural jump stochastic differential equations. In Advances in\n\nNeural Information Processing Systems, pp. 9843–9854, 2019.\n\nLi Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation, 31(4): 765–783, 2019.\n\nAnil Kag, Ziming Zhang, and Venkatesh Saligrama. Rnns incrementally evolving on an equilibrium In International Conference on\n\nmanifold: A panacea for vanishing and exploding gradients? Learning Representations, 2019.\n\nRE Kalman. A new approach to linear filtering and prediction problems. J. Basic Eng., Trans.\n\nASME, D, 82:35–45, 1960.\n\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ̧ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156–5165. PMLR, 2020.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nPatrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696– 6707, 2020.\n\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\n\nIn\n\nInternational Conference on Learning Representations, 2019.\n\nDaniel R Kowal, David S Matteson, and David Ruppert. Functional autoregression for sparsely\n\nsampled data. Journal of Business & Economic Statistics, 37(1):97–109, 2019.\n\nColin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks: In European Conference on Computer Vision, pp.\n\nA unified approach to action segmentation. 47–54. Springer, 2016.\n\nMathias Lechner and Ramin Hasani. Mixed-memory rnns for learning long-term dependencies in\n\nirregularly sampled time series. OpenReview, 2021.\n\nMathias Lechner, Ramin Hasani, Manuel Zimmer, Thomas A Henzinger, and Radu Grosu. Designing worm-inspired neural networks for interpretable robotic control. In 2019 International Conference on Robotics and Automation (ICRA), pp. 87–94. IEEE, 2019.\n\nMathias Lechner, Ramin Hasani, Alexander Amini, Thomas A Henzinger, Daniela Rus, and Radu Grosu. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10): 642–652, 2020a.\n\nMathias Lechner, Ramin Hasani, Daniela Rus, and Radu Grosu. Gershgorin loss stabilizes the recurrent neural network compartment of an end-to-end robot learning scheme. In 2020 International Conference on Robotics and Automation (ICRA). IEEE, 2020b.\n\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with\n\nfourier transforms. arXiv preprint arXiv:2105.03824, 2021.\n\nShuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5457–5466, 2018.\n\nSteven Cheng-Xian Li and Benjamin M Marlin. A scalable end-to-end gaussian process adapter for irregularly sampled time series classification. In Advances in neural information processing systems, pp. 1804–1812, 2016.\n\nLucas Liebenwein, Ramin Hasani, Alexander Amini, and Daniela Rus. Sparse flows: Pruning continuous-depth models. Advances in Neural Information Processing Systems, 34:22628–22642, 2021.\n\nWilliam A Little. The existence of persistent states in the brain. Mathematical biosciences, 19(1-2):\n\n101–120, 1974.\n\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441–2453, 2021.\n\nStefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting\n\nneural odes. Advances in Neural Information Processing Systems, 33:3952–3963, 2020.\n\nHongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating multivariate point process. In Advances in Neural Information Processing Systems, pp. 6754–6764, 2017.\n\nJames Morrill, Patrick Kidger, Cristopher Salvi, James Foster, and Terry Lyons. Neural cdes for\n\nlong time series via the log-ode method. arXiv preprint arXiv:2009.08295, 2020.\n\nJames Morrill, Cristopher Salvi, Patrick Kidger, and James Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pp. 7829–7838. PMLR, 2021.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nDaniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network training for long or event-based sequences. In Advances in neural information processing systems, pp. 3882–3890, 2016.\n\nNaoki Nonaka and Jun Seita. In-depth benchmarking of deep neural network architectures for ecg\n\ndiagnosis. In Machine Learning for Healthcare Conference, pp. 414–439. PMLR, 2021.\n\nRonald Pearson, Gregory Goney, and James Shwaber. Imbalanced clustering for microarray time-\n\nseries. In Proceedings of the ICML, volume 3, 2003.\n\nWill Penny, Zoubin Ghahramani, and Karl Friston. Bilinear dynamical systems. Philosophical\n\nTransactions of the Royal Society B: Biological Sciences, 360(1457):983–993, 2005.\n\nMinh Q Phan, Yunde Shi, Raimondo Betti, and Richard W Longman. Discrete-time bilinear representation of continuous-time bilinear state-space models. Advances in the Astronautical Sciences, 143:571–589, 2012.\n\nMarco AF Pimentel, Alistair EW Johnson, Peter H Charlton, Drew Birrenkott, Peter J Watkinson, Lionel Tarassenko, and David A Clifton. Toward a robust estimation of respiratory rate from pulse oximeters. IEEE Transactions on Biomedical Engineering, 64(8):1914–1923, 2016.\n\nAlessio Quaglino, Marco Gallieri, Jonathan Masci, and Jan Koutn ̃Ak. Snode: Spectral discretization of neural odes for system identification. In International Conference on Learning Representations, 2020.\n\nHubert Ramsauer, Bernhard Sch ̈afl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael K Kopp, et al. Hopfield networks is all you need. In International Conference on Learning Representations, 2020.\n\nDavid W Romero, Robert-Jan Bruintjes, Jakub Mikolaj Tomczak, Erik J Bekkers, Mark Hoogendoorn, and Jan van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In International Conference on Learning Representations, 2021a.\n\nDavid W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. In International Conference on\n\nCkconv: Continuous kernel convolution for sequential data. Learning Representations, 2021b.\n\nDP Roy and L Yan. Robust landsat-based crop time series modelling. Remote Sensing of Environ-\n\nment, 238:110810, 2020.\n\nYulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, pp. 5321–5331, 2019.\n\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-\n\npropagating errors. nature, 323(6088):533–536, 1986.\n\nKonstantin T. Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies. In International Conference on Learning Representations, 2021a.\n\nT Konstantin Rusch and Siddhartha Mishra. UnICORNN: A recurrent model for learning very long time dependencies. In International Conference on Machine Learning, pp. 9168–9178. PMLR, 2021b.\n\nAlex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory\n\n(lstm) network. Physica D: Nonlinear Phenomena, 404:132306, 2020.\n\nJimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for\n\nsequence modeling. arXiv preprint arXiv:2208.04933, 2022.\n\nJimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Ai8Hw3AXqks.\n\n14\n\nPublished as a conference paper at ICLR 2023\n\nChang Wei Tan, Christoph Bergmeir, Franc ̧ois Petitjean, and Geoffrey I Webb. Time series extrinsic\n\nregression. Data Mining and Knowledge Discovery, 35(3):1032–1060, 2021.\n\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In\n\nInternational Conference on Machine Learning, pp. 9438–9447. PMLR, 2020a.\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020b.\n\nTrieu Trinh, Andrew Dai, Thang Luong, and Quoc Le. Learning longer-term dependencies in rnns with auxiliary losses. In International Conference on Machine Learning, pp. 4965–4974. PMLR, 2018.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\nAaron Voelker, Ivana Kaji ́c, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019.\n\nCharles Vorbach, Ramin Hasani, Alexander Amini, Mathias Lechner, and Daniela Rus. Causal navigation by continuous-time neural networks. Advances in Neural Information Processing Systems, 34, 2021.\n\nCheng Wang and Mathias Niepert. State-regularized recurrent neural networks. In International\n\nConference on Machine Learning, pp. 6596–6606, 2019.\n\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\n\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n\nTsun-Hsuan Wang, Wei Xiao, Tim Seyde, Ramin Hasani, and Daniela Rus.\n\nInterpreting neural\n\npolicies with disentangled tree representations. arXiv preprint arXiv:2210.06650, 2022.\n\nScott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. Advances in neural information processing systems, 29:4880– 4888, 2016.\n\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr ̈omformer: A nystr ̈om-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, No 16, pp. 14138– 14148, 2021.\n\nLianhao Yin, Tsun-Hsuan Wang, Makram Chahine, Tim Seyde, Mathias Lechner, Ramin Hasani, and Daniela Rus. Cooperative flight control using visual-attention–air-guardian. arXiv preprint arXiv:2212.11084, 2022.\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283–17297, 2020.\n\nZhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for In Proceedings of the 59th Annual Meeting of the Association for Computational sequences. Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801–3815, 2021.\n\n15\n\nPublished as a conference paper at ICLR 2023\n\nA EXAMPLE LIQUID-S4 KERNEL\n\nKliquid ∗ ucorrelations =\n\n(cid:104)\n\nCA\n\n(k−1)\n\n2 B\n\n2 , . . . , CB\n\n, . . . , CA\n\n(k−2)\n\n3 B\n\n3 , . . . , CB\n\n, . . . , CA\n\n(k−3)\n\n4 B\n\n4(cid:105)\n\n, . . . , CB\n\n∗ (16)\n\n(cid:104)\n\nu0u1, . . . , uk−1uk, . . . , u0u1u2, . . . , uk−2uk−1uk, . . . , u0u1u2u3, . . . , uk−3uk−2uk−1uk\n\n(cid:105)T\n\nHere, ucorrelations 2 )+(k+1 R(k+1\n\n3 )+(k+1\n\n4 ).\n\nis a vector of\n\nlength (cid:0)k+1\n\n2\n\n(cid:1) + (cid:0)k+1\n\n(cid:1) + (cid:0)k+1\n\n(cid:1), and the kernel Kliquid ∈\n\n3\n\n4\n\nB PROOF OF PROPOSITION 1\n\nProposition. The Liquid-S4 kernel for each order p ∈ P, Kliquid, can be computed by the anti-diagonal transformation (flip operation) of the product of the S4 convolution kernel, K = (cid:0)CB, CAB, . . . , CA\n\nB(cid:1), and a vector B\n\n∈ RN .\n\nL−1\n\np−1\n\nProof. This can be shown by unrolling the S4 convolution kernel and multiplying its components with B , performing an anti-diagonal transformation to obtain the corresponding liquid S4 kernel:\n\np−1\n\nK = (cid:0)CB, CAB, CA\n\n2\n\nB, . . . , CA\n\nL−1\n\nB(cid:1)\n\nFor p = 2 (correlations of order 2), S4 kernel should be multiplied by B. The resulting kernel would be:\n\n2\n\n(cid:0)CB\n\n2 , CAB\n\n, CA\n\n2\n\n2 B\n\n, . . . , CA\n\nL−1\n\n2(cid:1)\n\nB\n\nWe obtain the liquid kernel by flipping the above kernel to be convolved with the 2-term correlation terms (p=2):\n\nKliquid=2 = (cid:0)CA\n\nL−1\n\n2 B\n\n, . . . , CA\n\n2\n\n2 B\n\n2 , CAB\n\n2(cid:1)\n\n, CB\n\nSimilarly, we can obtain liquid kernels for higher liquid orders and obtain the statement of the proposition.\n\nC HYPERPARAMETERS\n\nLearning Rate. Liquid-S4 generally requires a smaller learning rate compared to S4 and S4D blocks.\n\nSetting ∆tmax and ∆tmin We set ∆tmax for all experiments to 0.2, while the ∆tmin was set based on the recommendations provided in (Gu et al., 2022c) to be proportional to ∝\n\n1\n\nseq length .\n\nCausal Modeling vs. Bidirectional Modeling Liquid-S4 works better when it is used as a causal model, i.e., with no bidirectional configuration.\n\ndstate We observed that Liquid-S4 PB kernel performs best with smaller individual state sizes dstate. For instance, we achieve SOTA results in ListOps, IMDB, and Speech Commands by a state size set to 7, significantly reducing the number of required parameters to solve these tasks.\n\nChoice of Liquid-S4 Kernel In all experiments, we choose our simplified PB kernel over the KB kernel due to the computational costs and performance. We recommend the use of PB kernel.\n\nChoice of parameter p in liquid kernel. In all experiments, start off by setting p or the liquidity order to 2. This means that the liquid kernel is going to be computed only for correlation terms of order 2. In principle, we observe that higher p values consistently enhance the representation learning capacity of Liquid-S4 modules, as we showed in all experiments. We recommend p = 3 as a norm to perform experiments with Liquid-S4.\n\n16\n\nPublished as a conference paper at ICLR 2023\n\nTable 6: Performance on Long Range Arena Tasks. Numbers for Liquid-S4 kernels indicate test accuracy (standard deviation). The rest of the models’ performance results are reported from the cited paper. Liquid-S4 is used with its PB kernel.\n\nModel (input length)\n\nS4-LegS (Gu et al., 2022b) S4D-LegS (Gu et al., 2022b) S4D-Inv (Gu et al., 2022b)\n\nLiquid-S4-KB (ours) Liquid-S4-PB (ours)\n\nListOps 2048\n\n59.60 (0.07) 60.47 (0.34) 60.18 (0.35)\n\n62.30 (0.10) 62.60 (0.20) p = 5\n\nIMDB 2048\n\n86.82 (0.13) 86.18 (0.43) 87.34 (0.20)\n\n88.80 (0.05) 88.90 (0.10) p=6\n\nAAN 4000\n\nCIFAR 1024\n\nPathfinder 1024\n\nPath-X Avg. 16384\n\n90.90 (0.15) 89.46 (0.14) 91.09 (0.01)\n\n90.95 (0.10) 91.15 (0.09) p=4\n\n88.65 (0.23) 88.19 (0.26) 87.83 (0.37)\n\n89.40 (0.15) 89.45 (0.33) p=3\n\n94.20 (0.25) 93.06 (1.24) 93.78 (0.25)\n\n94.60 (0.10) 94.90 (0.25) p=2\n\n96.35 91.95 92.80\n\n95.98 96.36 p=2\n\n86.09 84.89 85.50\n\n87.01 87.21\n\nThe kernel computation pipeline uses the PyKeops package (Charlier et al., 2021) for large tensor computations without memory overflow.\n\nAll reported results are validation accuracy (similar to Gu et al. (2022a)) performed with 2 to 3 different random seeds, except for the BIDMC dataset, which reports accuracy on the test set.\n\nTable 5: Hyperparameters for obtaining best performing models. BN= Batch normalization, LN = Layer normalization, WD= Weight decay.\n\nDepth Features H State Size Norm Pre-norm Dropout LR\n\nBatch Size Epochs WD\n\nListOps Text (IMDB) Retrieval (AAN) Image (CIFAR) Pathfinder Path-X\n\nSpeech Commands\n\nBICMD (HR) BICMD (RR) BICMD (SpO2)\n\nsCIFAR\n\n9 4\n6 6\n6 6\n\n6\n\n6 6\n6\n\n6\n\n128 128 256 512 256 320\n\n128\n\n128 128 128\n\n512\n\n7 7\n64 512 64 64\n\n7\n\n256 256 256\n\n512\n\nBN BN BN LN BN BN\n\nBN\n\nLN LN LN\n\nLN\n\nTrue True False False True True\n\nTrue\n\nTrue True True\n\nFalse\n\n0.01 0.1 0.2 0.1 0.0 0.0\n\n0.0\n\n0.0 0.0 0.0\n\n0.1\n\n0.002 0.003 0.005 0.01 0.0004 0.001\n\n0.008\n\n0.005 0.01 0.01\n\n0.01\n\n12 8\n16 16 4\n8\n\n10\n\n32 32 32\n\n50\n\n30 50 20 200 200 60\n\n50\n\n500 500 500\n\n200\n\n0.03 0.01 0.05 0.03 0.03 0.05\n\n0.05\n\n0.01 0.01 0.01\n\n0.03\n\nTable 7: Performance on Raw Speech Command dataset with the reduced ten classes (SC10) dataset.Numbers indicate validation accuracy. The accuracy of baseline models is reported from Table 5 of (Gu et al., 2022a). x stands for infeasible computation on a single GPU or not applicable as stated in Table 10 of (Gu et al., 2022a). The hyperparameters for Liquid-S4 are the same as the ones reported for Speech Commands Full Dataset reported in Table 5.\n\nModel\n\nTransformer Performer ODE-RNN NRDE ExpRNN LipschitzRNN CKConv WaveGAN-D LSSL (Gu et al., 2021) S4-LegS (Gu et al., 2022a)\n\nLiquid-S4 (ours)\n\nSC10\n\n16kHz\n\n8kHz\n\nx 30.77 x\n16.49 11.6 x\n71.66 96.25 x\n98.32\n\n98.51 p=2\n\nx 30.68 x\n15.12 10.8 x\n65.96 x\nx 96.30\n\n95.9 p=2\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nFigure 2: Performance vs Liquid Order in Liquid-S4. (n=3)\n\nD ON THE DISCRETIZATION OF THE LIQUID KERNEL.\n\nHow do we perform the discretization of A + Bu(t). The dynamical system presented in Eq. 10, is a continuous-time (CT) bilinear state-space model (SSM). Ideally, we want that the discretization of a CT bilinear SSM to 1) satisfy the first-order form of the model, and 2) preserve the bilinear model structure. This is challenging and only possible via a limited number of methods:\n\n1) The most straightforward approach is to use Forward Euler with first-order error: ̇x = xk+1−xk δt + O(δt). Now by plugging this in Eq. 10, we get A = I + δtA, B = δtB. Such discretization satisfies the conditions above. For this discretization to stay stable, for s = σ ± iω an eigenvalue of the continuous transition matrix A, and λ = 1 + sδt, an eigenvalue of the discrete model, Re(s) ≤ 0 or |λ| = |1 + sδt| ≤ 1, thus (1 + σδt)2 + ω2δt2 ≤ 1. This condition implies that selecting a small enough δt ensures the system’s stability, but for cases where δt is large, the system might go unstable.\n\nOne can show that based on the properties of the transition matrices A and B, and the range of selected δt a bilinear transformation of discrete matrices A and B, would be very close to that of our Forward Euler discretization. This means that:\n\n|AForward Euler − AApprox bilinear| < γ\n\n0 < γ <\n\nδt 2\n\n(17)\n\n2) Adams-Bashforth Method: The second-order Adams-Bashforth will apply the transformation xk+1 = xk + 3δt 2 f (k − 1) + O(δt2), where f(k) is the right-hand-side of Eq. 8 at time t = kδt. This method also satisfies the two conditions we required (Phan et al., 2012).\n\n2 f (k) − δt\n\nOne must denote that computing a bilinear transform (https://en.wikipedia.org/wiki/ Bilinear_transform) of a continuous-time bilinear SSM while preserving the first-order structure of the model is an open problem. Ideally, we can apply this transformation on A+Bu(t). However, it is challenging to preserve the first-order form of the equation while keeping the bilinear (liquid) structure of the model described in Eq. 10.\n\nIn our case, we use the bilinear transform form of A, B, and C presented in (Eq. 3) for the discrete weights of the system of (Eq. 11), as this approximation is close to that of the Forward Euler. This implies that the continuous system in (Eq. 10) could be transformed directly to Eq. 11 by a forward Euler transformation. Furthermore, due to the range of δt and properties of A and B, the bilinear transformed matrices presented in Eq. 3, would be close to the direct forward Euler system.\n\n18\n\nLower is betterLower is betterLower is betterHigher is betterHigher is betterABCDEPublished as a conference paper at ICLR 2023\n\nE ON THE AUTOREGRESSIVE MODE OF PB KERNEL\n\nIn autoregressive (AR) mode, with PB (or any other conditioned kernel), we obtain A, B, and C no matter what the conditions are.\n\nMore specifically, in the autoregressive mode of PB we can use Eq. 13. In Eq. 13, for computing the black parts we can reuse the AR mode of the plain S4 model and only have computed the new (violet) parts. As the violet parts consist of p multiplications of input terms (and the corresponding matrices) computing the AR mode is feasible. This adds a complexity of O(p) to the inference in the AR mode of PB, but because p is much smaller than L (past sequence length), it can significantly speed up the inference time compared to the convolution counterpart.\n\nMoreover, one of the properties of LTCs that was never studied and introduced before this work is their ability to account for the pairwise correlation of inputs which became apparent once we unrolled the system’s dynamics in this work. We believe that the pairwise correlation of inputs is a property that the PB kernel also possesses. Whether the kernel loses the expressivity and robustness attributes of LTCs, we have to investigate in future work.\n\nF WHY DOES PB KERNEL OUTPERFORM KB KERNEL?\n\nOne possible reason why PB outperforms KB could be the fact that we limit the correlation terms with the truncation with order p. This limitation arises from how S4 blocks are constructed as a stack of many 1D blocks which does not computationally allow us to exploit the benefit of higher-order correlation terms due to the high computational complexity. This limitation might also reduce the expressivity of the KB kernel, but not PB, as they do not have a dependency on A for the correlation terms.\n\nA potential solution would be a Liquid-S5 instance, where we could directly use a parallel scan introduced in the concurrent work S5 (Smith et al., 2023), over the linear LTC system (which is a time-varying SSM). This is possible because we could precompute the state transitions at each time step. This way we would not need to truncate the kernel and obtain all correlation terms for free. This is an exciting extension to Liquid-S4 which we are exploring in future work\n\n19",
    "reference": "# Summary Of The Paper\n\nThe paper introduces an approach for capturing long-term dependencies of complex time series. The paper incorporates some ideas from the classical dynamical system modeling literature into machine learning.\n\n# Strength And Weaknesses\n\nStrengths:\ni) The proposed method is very comprehensive. It is tailored in fine details including computational feasibility.\nii) The paper reports a large list of experiments, compares against competitive modern machine learning approaches such as transformers, and appears to outperform all of them.\n\nWeaknesses:\ni) The method involves calculation of iFFT somewhere in the pipeline. I wonder how much it could preserve end-to-end differentiability of the eventual loss function if it is used together with encoders, for instance in high-dimensional sequences such as videos.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe paper is written in a clear and precise scientific language. The proposed method is novel in its own context, even though probably not as novel for the signal processing domain. The paper gives sufficient amount of details to reproduce the experiments.\n\n# Summary Of The Review\n\nSolid piece of work. The approach is a bit different from the commonplace approaches of the core machine learning community, which could make its accessibility a bit questionable. On the other hand, this could also be an opportunity for the community to get exposed to ideas from different domains.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  },
  {
    "input": "Under review as a conference paper at ICLR 2023\n\nHOW USEFUL ARE GRADIENTS FOR OOD DETECTION REALLY?\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nOne critical challenge in deploying machine learning models in real-life applications is out of distribution (OOD) detection. Given a predictive model which is accurate on in distribution (ID) data, an OOD detection system can further equip the model with the option to defer prediction when the input is novel and the model has low confidence. Notably, there has been some recent interest in utilizing gradient information in pre-trained models for OOD detection. While these methods are competitive, we argue that previous works conflate their performance with the necessity of gradients. In this work, we provide an in-depth analysis and comparison of gradient based methods and elucidate the key components that warrant their OOD detection performance. We further demonstrate that a general, non-gradientbased family of OOD detection methods are just as competitive, casting doubt on the usefulness of gradients for OOD detection.\n\n1\n\nINTRODUCTION\n\nRecent advances in algorithms, models, and training infrastructure have brought about unprecedented performance of machine learning (ML) methods, across a wide range of data types and tasks. Despite their demonstrated potential on benchmark settings and domains, one obstacle which limits ML methods’ applicability in real-world applications is the uncertainty or confidence of the predictions. Without any deliberate mechanisms, ML models will output a prediction for any given input, and the question of whether this prediction can be trusted will be especially critical in many high-risk decision-making settings (e.g. self-driving cars (Agarwal et al., 2021), physical sciences (Char et al., 2021; Boyer et al., 2021), and healthcare (Zhou et al., 2020)). This risk is further exacerbated with deep learning where the interpretability of models are often limited (Rudin, 2019)).\n\nIt is unrealistic for one to expect to train a model that has perfect predictions for all possible inputs, partly because real-world datasets are limited in their scope. Thus in lieu of trying to make predictions for all test inputs, one can attempt to first detect whether the input is covered by the support of the training data. This is the motivation behind OOD detection. Among the diverse approaches to OOD detection for image recognition, a recent line of work has suggested utilizing the information in gradients to derive efficient and performant methods for OOD detection (Liang et al., 2017; Lee & AlRegib, 2020; Agarwal et al., 2020; Lee & AlRegib, 2021; Huang et al., 2021; Sun et al., 2022; Kokilepersaud et al., 2022).\n\nWe motivate our work by first exploring the claim that gradients are useful for OOD detection. Through a comparison with various extensions of gradient-based scores, we analyze the key components that actually drive the performance of these methods, and we argue that gradient computations are not essential in deriving performant post hoc OOD detectors. Rather, these methods ultimately rely on the magnitude of the learned feature embedding and the predicted output distribution. We thereby refute many of the intuitions that previous works motivate their methods with. Based on our analysis, we advocate for the study of a more general, non-gradient-based framework for producing performant score functions and provide a comprehensive empirical evaluation of various instantiations of the score within this framework.\n\nThe rest of this paper is structured as follows. We first provide a formal statement of the problem setting and the related works in Section 2. We then introduce existing and new gradient-based detectors and discuss how both can be simplified into intuitive forms (Section 3). Following this, we perform empirical evaluations of the methods (Section 4) and discuss their implications in Section 5.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Output Score Components The probability output components (V components from Section 4.2) are shown on a 3-class probability simplex where each of the 3 vertices signifies probability 1 to a single class. From left to right, these are the output score terms used for EXGRAD, MSP, and GRADNORM. Lighter to darker shades indicate lower to higher values.\n\n2 PRELIMINARIES AND RELATED WORK\n\n∈\n\n[C] given an input x\n\nProblem Setting and Notation We focus on the classification setting where the task is to predict a Rd, where C is the total number of possible classes and d is class label y ∈\nN (xi, yi) i=1 in order to the dimensionality of the input. We assume we have access to training data RC. Here, θ is a collection of all of the train the parameters θ of a deep neural network fθ : Rd weights from each layer in the network, thus, θ = , where l denotes the index of each [L] layer, and we assume the network has L layers in total.\n\nWl : l\n\n→ ∈\n\n}\n\n{\n\n}\n\n{\n\nGiven an input x, the network predicts a probability vector, p, via the softmax function of the network (x)/T f (k′ )\n\n(cid:17) , where the superscripts denote the index\n\noutputs: p(k)(x) = P(Y = k\n\nf (k) θ\n(cid:16)\n\n(x)/T\n\n(cid:80)C\n\nexp\n\n(cid:17)\n\n(cid:16)\n\nx) = |\n\nk′=1 exp\n\nθ\n\nof the vector and T is the temperature. If not otherwise specified, we will assume T = 1. Although p(x) depends on both θ and x , we will always exclude the former and often exclude the latter when it is clear from context or unimportant. Lastly, we often abuse notation and use Y p(x) to mean Y is sampled from the categorical distribution parameterized by p(x).\n\n∼\n\nRd, The OOD Detection Problem In a real-world scenario, the model may be given an input, ̃x during deployment that is substantially different from any of the datapoints in the training set. For example, a classifier trained to identify numeric digits may be given an image of a cat. Since the model’s prediction p( ̃x) cannot be trusted, it would be advantageous to flag such instances and defer prediction. This is the problem that OOD detection addresses.\n\n∈\n\n: Rd\n\nMore formally, the goal in OOD detection is to derive a binary classifier which labels whether a given input, ̃x, is ID (in distribution) or OOD. This goal is commonly addressed by learning a score function R which quantifies the degree to which the input is OOD. Existing works have approached S\nfrom various perspectives, which generally vary by where this problem of learning the mapping is extracted from. Here, we introduce broad groupings of methodologies to the signal to generate provide context for our work, and we refer the reader to Yang et al. (2021); Salehi et al. (2021) for an in-depth survey of the field.\n\n→\n\nS\n\nS\n\nOne class of methods focuses on the input space and learns score functions based on characteristics that can be derived from the input features. For example, distance-based methods are based on the intuition that OOD data should lie “far away” from ID data and define with distances between the input point and reference points that are representative of ID (Lee et al., 2018; Techapanurak et al., 2020; Van Amersfoort et al., 2020). Meanwhile, density-based methods utilize probabilistic models to describe the density of ID data and argue that OOD points should occur in areas of low densities. Thus generative models are often used and the score function is often derived with the predicted likelihood of the test input point (Ren et al., 2019; Serr`a et al., 2019; Zisselman & Tamar, 2020).\n\nS\n\nAnother class of methods aims to directly influence a predictive model’s behavior to OOD data through explicit training. These methods often assume access to OOD examples during training and incorporates them into a predictive model’s training procedure to maximize the separability between ID and OOD inputs. This is usually achieved by setting aside actual samples from an OOD test distribution (Hendrycks et al., 2018; Yu & Aizawa, 2019; Liu et al., 2020), or if they are not available, by synthesizing OOD examples via adversarial training, perturbations, or sampling from boundaries or low density regions (Lakshminarayanan et al., 2016; Lee et al., 2017; Vernekar et al., 2019).\n\n2\n\n<latexit sha1_base64=\"DaxXQ7jPEJOGQPn7QwBDslmNTco=\">AAACd3icbVDLTttAFJ24vKGQlCUSjBqowoLIrlDbTSUKG5YgNYAUh2g8uU5GmYc1c42ILH9Pv4Ztq34KO+zEC0K40khH58y5jxMlUjj0/f8178PS8srq2vrG5tbH7Z1649ONM6nl0OFGGnsXMQdSaOigQAl3iQWmIgm30fii1G8fwDph9G+cJNBTbKhFLDjDgurXf4VCI1iER8wOQ5eqfjb+GeT3FzRUDEdRnCX5fdYaH+etgJ4skMeHeb/e9Nv+tOgiCCrQJFVd9Ru1/XBgeKpAI5fMuW7gJ9jLmEXBJeQbYeogYXzMhtAtoGYKXC+b3prTo4IZ0NjY4mmkU3bOMXgQias8jzPTaz1jyrmJiopO5S3urVaS72ndFOMfvUzoJEXQfLZInEqKhpbB0oGwwFFOCsC4FcUtlI+YZbyId35K2RuNkW5+Ma4iK4YjLBMN3ua3CG6+toNv7dPr0+bZeZXtGtkjn0mLBOQ7OSOX5Ip0CCd/yBP5S/7Vnr0D74vXmn31apVnl8yVF7wA6OjCxA==</latexit>PCk=1p(k)(1p(k))<latexit sha1_base64=\"imqY+FHwiM00V8X8IWrzr/eqqbQ=\">AAACW3icbVBNrxIxFC2DH4joA40rE9MIJM8NmTEv770l0Y1LTOQjASSdcgcaOu2kvUMgk/kF79e41V/iwv9iB2Yh4E2anpzTc3vvCRMpLPr+74pXffT4ydPas/rzxouXV83Wq5HVqeEw5FpqMwmZBSkUDFGghEligMWhhHG4+Vzo4y0YK7T6hvsE5jFbKREJztBRi2Z3JhSCQdhh1pnFbLfYUHfhOoyyJP+eXW8+5J180Wz7Pf9Q9BIEJWiTsgaLVuXdbKl5GoNCLpm108BPcJ4xg4JLyOuz1ELC+IatYOqgYjHYeXbYJ6ddxyxppI07CumBPXEstyKxpWd3NP2rZyy2dh+HrlOxij3XCvJ/2jTF6H6eCZWkCIofB4lSSVHTIjy6FAY4yr0DjBvhdqF8zQzjLsLTX4reqLW0p4PxODRitcYi0eA8v0sw+tgLbns3X2/a/U9ltjXylrwn1yQgd6RPvpABGRJOHsgP8pP8qvzxql7daxyfepXS85qclPfmL9oOuF8=</latexit>maxkp(k)<latexit sha1_base64=\"W4v5XvBEaYoQ/eDzx5ssjJB1VXc=\">AAACiHicbVBNaxsxEJW3X6n75bTHQhF1CumhZreEJjkUQnzpMS11ErAco5VnbWF9LNJsiBH7v/pXeum1/RnV2nuokw4IHu/NG828vFTSY5r+7CT37j94+GjncffJ02fPX/R2X557WzkBI2GVdZc596CkgRFKVHBZOuA6V3CRL4eNfnENzktrvuOqhInmcyMLKThGatr7xqRBcAg3GPaYr/Q0LD9n9dWQMgUFMhXNSFnhuAhZHYY1/UCZ5rjIi1DWV2F/+b6mzMn5Aplrevfqaa+fDtJ10bsga0GftHU23e28YTMrKg0GheLej7O0xEngDqVQUHdZ5aHkYsnnMI7QcA1+EtbH1/RdZGa0sC4+g3TNbjlm17L0redmY/pXD1x7v9J5nNTc5W9rDfk/bVxhcTQJ0pQVghGbRYpKUbS0SZrOpAOBahUBF07GW6hY8BhkzHv7l2Y2Wqv89mJC5+tgm0Sz2/ndBecfB9mnwcHXg/7JaZvtDnlN3pJ9kpFDckK+kDMyIoL8IL/Ib/In6SZpcpgcb1qTTut5RbYqOf0LoWjJ2g==</latexit>PCk=11Cp(k)Under review as a conference paper at ICLR 2023\n\nMany works have instead turned attention to the information that can be extracted from a predictive model that is fully trained on ID data. With the intuition that an ideal predicted distribution should have low uncertainty (heavily concentrated on the predicted class) for an ID point and have high uncertainty (close to a uniform distribution) for an OOD input, some methods focus on generating scores with the predicted distribution of pre-trained models Hendrycks & Gimpel (2016); Linmans et al. (2020); Liu et al. (2020). Meanwhile, some works have examined the information available when backpropagating gradients of a loss function. Liang et al. (2017); Agarwal et al. (2020) utilize information in the gradient w.r.t. the input, while Lee & AlRegib (2020); Huang et al. (2021) examine gradients w.r.t. the model parameters. Huang et al. (2021), in particular, proposed GRADNORM, an efficient post hoc score function which simply measures the norm of model parameter gradients and thus requires no additional training or hyperparameter tuning. Despite this, GRADNORM achieves State-of-the-Art (SotA) performance among post hoc methods. We explain GRADNORM in more detail in the next section.\n\n3 GRADIENT BASED OOD METHODS\n\nLee & AlRegib (2020) initially proposed using model gradients as a signal to detect OOD data. Specifically, they provide a fully trained network with an input point and backpropagate the cross entropy loss between a uniform probability distribution and the network’s predicted class probabilities. We summarize their intuition for this algorithm in the following hypothesis: Hypothesis 3.1. The Feature-Extraction Hypothesis If learning a point ( ̃x, ̃y) requires a large change in a well-trained network, then ̃x must have been novel, because most of a deep network is dedicated to feature extraction, which by assumption should already perform well on ID data.\n\nMeanwhile, GRADNORM contends the opposite case: they argue that trying to fit the predicted distribution of ID data to a uniform distribution label will necessitate higher gradients to the model parameters than with OOD data. We note that this intuition relies on the predicted distributions actually displaying high entropy for OOD inputs and low entropy for ID inputs.\n\nDespite conflicting intuitions, both works ultimately propose taking the KL divergence (identically, the cross-entropy loss) w.r.t. the uniform distribution and measuring a norm of the model gradients as a score function. Through extensive empirical ablations, GRADNORM hones in on the L1 norm of the last layer gradients of the KL-divergence w.r.t. the uniform distribution as the score function,\n\nWe can expand the divergence term and simplify this score into the following expression:\n\n.\n\n1 (cid:13) (cid:13) (cid:13) (cid:13)\n\n.\n\n1 (cid:105)(cid:13) (cid:13) (cid:13)\n\np\n\n(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\nSGN (x) =\n\n∂DKL(u\n\n||\n\nsoftmax(fθ(x)) ∂WL\n\nSGN (x) =\n\nEY\n\nunif\n\n∼\n\n∇WL log p(Y )\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n(1)\n\n(2)\n\nThat is, GRADNORM measures the norm of the expected gradient according to a uniform labeling distribution. Note that\n\n∇WL denotes the gradient w.r.t. the parameters in the final layer of fθ.\n\nAnother Gradient Approach We note that Eq.2 is not a unique measure that leverages gradients. To aid our analysis, we also consider the following variant:\n\nSEG(x) = EY\n\np(x)\n\n∼\n\n∇θ log p(Y )\n\n.\n\n(3)\n\nWe will show later that this score also has interpretable properties, and helps us understand the central components of gradient-based score functions. Due to the outer positioning of the expectation, we refer to the score function in Eq. 3 as EXGRAD. Two key differences in this score are 1) the label distribution of Y comes from the model’s own predicted distribution, p, not the uniform distribution, and 2) this calculates the expected norm of the gradient, whereas Eq. 2 calculates the norm of the expected gradient. Other gradient-based scores are readily plausible, and we visit a suite of scores with empirical comparisons in Section 4.1.\n\n(cid:104)\n\n(cid:20)(cid:13) (cid:13) (cid:13)\n\n3.1 DECOMPOSITION OF GRADIENT METHODS\n\nTo give further grounds for their method, Huang et al. (2021) analyze Eq. 2 and show that it can be decomposed into two terms: one that is characterized by the size of the magnitude of the encoding\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nfed to the last layer of the network, and one that is characterized by the output of the network. In particular, they derive\n\nSGN (x) =\n\n1 T ∥\n\nh\n\n∥1\n\n1 C −\n\np(k)\n\nC\n\n(cid:88)k=1 (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(4)\n\nwhere h is the encoding being fed to the last layer of the network. Following their notation, we will denote U to be the part of the score characterized by h and V to be the part characterized by the C\nnetwork output, i.e. here U = T U V . Huang et al. k=1 (2021) show that U and V have the same trend where their values are large for ID points and lower for OOD points. While each can be used as OOD detectors alone, the product of the two terms results in stronger performance.\n\n∥1, V = 1\n\nSGN = 2\n\n1 C −\n\n, and\n\np(k)\n\n(cid:80)\n\nh\n\n(cid:12) (cid:12)\n\n(cid:12) (cid:12)\n\n∥\n\n2\n\nAlthough not mentioned in their analysis, the V term in GRADNORM’s score is simply the total variation (TV) distance between a discrete uniform distribution and the model’s predicted distribution. This characterization of V gives insight as to why GRADNORM works. When the input image is in distribution, the network will likely have higher confidence, making the TV distance between p and the discrete uniform large.\n\nDecomposition of EXGRAD Doing a similar analysis, our alternative gradient-based detector, EXGRAD, can be broken down in a similar way. In particular,\n\nSEG(x) =\n\n2 T\n\nU V\n\nU =\n\nh\n\n∥\n\n∥1\n\np(k)(1\n\np(k))\n\n−\n\n(5)\n\nC\n\nV =\n\n(cid:88)k=1\n\nThe full derivation is shown in Appendix B. Like with GRADNORM, the V term for EXGRAD turns Bernoulli(p(k)) be the random variable corresponding out to be an interpretable quantity. Let Bk ∼ to the event that x belongs to class k. Then, V = k=1 Var(Bk). Intuitively, when the input image is in distribution and there is high confidence on a single class, the variance of each Bernoulli random (cid:80) variable will be low. Note that this is the opposite trend of the TV distance and comparison of these scores can be seen in Figure 1.\n\n∥1. A visual\n\nh\n\n∥\n\nC\n\nThis general U V -style score will be of particular interest to us throughout this paper, and we will often refer to it as an “Encoding-Output” composition as it relies on both the networks encoding for the image (at least at the penultimate layer) and its output. We note that recent work on the “Familiarity Hypothesis” in Dietterich & Guyer (2022), as well as work on the role of the feature norm in open-set recognition and OOD detection in Vaze et al. (2022) further motivates the study of U and suggests a plausible alternative theory to the gradient-based “Feature-Extraction Hypothesis”.\n\n4 EMPIRICAL EVALUATION\n\nThe previous section introduced variations to existing gradient-based scores and decomposed the scores into interpretable components. This naturally begs the questions, which scores actually perform well, and what should their favorable performance be attributed to? In this section, we explore these questions by comparing the performance of multiple gradient-based scores against simpler, non-gradient based approaches on OOD detection tasks in image classification. Specifically, the goals of these experiments are as follows:\n\n1. to investigate whether different gradient-based scores are useful for OOD detection or whether only particular variants perform well (i.e. is it essential that the gradient-based detector is derived by taking the derivative of the KL divergence?);\n\n2. to examine the plausibility of the feature-extraction hypothesis in explaining the strong\n\nperformance of gradient-based OOD detection methods;\n\n3. to investigate the claim that gradient-based approaches offer unique performance advantages\n\nin the context of post hoc OOD detection tasks.\n\nIndeed, we show that SotA OOD detection performance is achievable by leveraging information solely from the predicted class distribution and latent encoding, calling into question the additional utility gained from gradient based score functions. Furthermore, we find that there is significant variability in performance across tasks for different gradient-based score functions. Moreover, we\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nshow that gradient-based methods are often worse than computationally simpler approaches that require no backpropagation. Lastly, we perform experiments that challenge the plausibility of one emerging theory that attempts to explain the role of gradients in SotA OOD methods.\n\n4.1\n\nINVESTIGATING THE PERFORMANCE OF GRADIENT-BASED SCORE VARIANTS\n\nExperimental Set Up A key motivation of our work is in the design of post hoc OOD detection mechanisms. In light of this, for each OOD method, we take a deep neural network pre-trained on a particular dataset (the ID dataset) and use data from one of the remaining datasets as OOD data. These models were obtained from a popular open-source library 1 of pre-trained neural network image classifier models and achieve competitive performance on ID data.\n\nWe investigate the performance of seven gradient-based OOD scores, one of which has been previously described in the literature (GRADNORM) and another which we described in detail in Section 3 (EXGRAD). We include additional experimental results on natural variants that can be derived by simple design choices in implementation, specifically by interchanging norms and expectations, choice of norm and choice of distribution to generate the synthetic label at test time. Finally, we compare these gradient-based methods against two non-gradient based approaches inspired by Huang et al. (2021) and our analysis in Section 3. We refer to these scores as “V term” scores.\n\nWe used MNIST Deng (2012), SVHN Netzer et al. (2011) and CIFAR-10 Krizhevsky (2009) as our base datasets for the first experimental setup, and use 10,000 samples from the test split of each ( ̃x) which dataset to form our ID and OOD datasets. Each OOD method defines a score function ( ̃x) > ε]. We then calculate AUROCs for each method, we then use to define an OOD classifier I[ varying the threshold ε. Table 1 describes the mean and standard deviation of the AUROC for each method across the six ID-OOD dataset combinations. Full experimental results showing performance for each ID-OOD combination are available in Appendix A. The columns “Deep” and “Shallow” refer to variants of each method that perform gradients w.r.t. all parameters of the network (Deep) or w.r.t. just the parameters of the final layer (Shallow), i.e. the weights of the layer that generate logits.\n\nS\n\nS\n\nAnalysis of Results We first observe that, on average over the 6 ID-OOD benchmark tasks, EXGRAD outperforms the previous SotA gradient-based post hoc OOD method, GRADNORM. Moreover, no method consistently dominates all other methods across ID-OOD splits (see Appendix A). This suggests that the singular focus in the literature on backpropagating KL divergence losses is unwarranted.\n\nOur next observation is that, in instances where extending gradients to the entire network produces change in performance, such changes are at best modest gains, and can in fact hurt performance. This aligns with results found in Huang et al. (2021). Given that the last layer of a deep neural network is merely taking linear combinations of learned features, this observation serves as further evidence against the Feature-Extraction Hypothesis.\n\nLastly, we note that the highest performing score functions are the non-gradient-based approaches. In particular, the two score functions inspired from the decomposition analyses improve beyond their best gradient-based variants by 2.9 and 0.7 percentage points respectively.\n\nThese observations provide the basis for a general framework to achieve high-performing post hoc OOD detectors that do not rely on calculating test-time gradients: combining a norm of learned encodings with a function of the predicted class distribution. In the following section, we provide additional experimental details that leverage this proposed template to further illustrate the ability to achieve high performance without relying on test-time gradients.\n\n4.2 EXPLORING ENCODING-OUTPUT COMPOSITIONS\n\nThe ImageNet Benchmark In addition to the previous experimental set up, in this section we also evaluate on the large-scale ImageNet benchmark proposed by Huang & Li (2021). For this benchmark, the ImageNet-1k dataset (Deng et al., 2009) is used for the ID dataset. This ID dataset is different from MNIST, CIFAR10, and SVHN in that it is composed of higher resolution images and has C = 1,000 classes instead of C = 10. The iNaturalist (Van Horn et al., 2017), SUN (Xiao et al., 2010), Places (Zhou et al., 2017), and Textures (Cimpoi et al., 2014) datasets are used as OOD datasets. This benchmark uses the 50,000 images in the ImageNet validation set as the ID data and\n\n1https://github.com/aaron-xichen/pytorch-playground\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: AUROC for Gradient-Based OOD Detectors on Small-Scale Experiments The leftmost column shows the definition of each score function. The Gradient Depth column signifies whether gradients were taken with respect to all parameters (“Deep”) or just the last layer’s parameters (“Shallow”). For each row, the mean AUROC is reported with standard deviations in parentheses, . Rows both calculated across the 6 ID-OOD dataset combinations from are sorted by mean AUROC. Note that the last two rows are the highest performing score functions and correspond to scores that do not involve any gradient calculations.\n\nMNIST, CIFAR-10, SVHN\n\n}\n\n{\n\nTable 2: AUROC scores The table shows AUROC scores for several detectors grouped by category. The leftmost group uses only the output of the network (V only), the second is the product of these ∥1 V , and the last measures with the 1-norm of the encoding passed to the last layer of the network grouping is detectors that leverage gradients of the last layer of the network. The highest AUROC found for each OOD task is bolded. We separately report the average of the ImageNet baselines and all of the other baselines. Note that we do show h\n\n∥1 T V in this table since it is exactly GRADNORM.\n\nh\n\n∥\n\n∥\n\n10,000 images for each of the OOD datasets (except for Textures which uses 5,640). The pre-trained model is from Google BiT-S2 (Kolesnikov et al., 2020) and uses the ResNetv2-101 architecture (He\n\n2https://github.com/google-research/big_transfer\n\n6\n\nScoreExpressionAUROCGradientDepth∥EY∼p(cid:2)∇θlogp(Y)(cid:3)∥220.725(±0.086)Deep∥EY∼p(cid:2)∇θlogp(Y)(cid:3)∥220.741(±0.081)ShallowEY∼Uniform(cid:2)∥∇θlogp(Y)∥1(cid:3)0.825(±0.120)DeepEY∼Uniform(cid:2)∥∇θlogp(Y)∥1(cid:3)0.850(±0.135)ShallowEY∼Uniform(cid:2)∥∇θlogp(Y)∥22(cid:3)0.867(±0.148)DeepEY∼Uniform(cid:2)∥∇θlogp(Y)∥22(cid:3)0.887(±0.107)Shallow(cid:13)(cid:13)∇θEY∼Uniform(cid:2)logp(Y)(cid:3)(cid:13)(cid:13)1(GRADNORM)0.892(±0.087)Deep(cid:13)(cid:13)∇θEY∼Uniform(cid:2)logp(Y)(cid:3)(cid:13)(cid:13)1(GRADNORM)0.906(±0.092)ShallowEY∼phlogp(Y)p(Y)∥∇θlogp(Y)∥22i0.910(±0.090)ShallowEY∼p(cid:2)∥∇θlogp(Y)∥22(cid:3)0.919(±0.041)ShallowEY∼p(cid:2)∥∇θlogp(Y)∥22(cid:3)0.921(±0.034)DeepEY∼phlogp(Y)p(Y)∥∇θlogp(Y)∥22i0.921(±0.106)DeepEY∼p(cid:2)∥∇θlogp(Y)∥1(cid:3)(EXGRAD)0.925(±0.047)ShallowEY∼p(cid:2)∥∇θlogp(Y)∥1(cid:3)(EXGRAD)0.926(±0.053)DeepPCk=1p(k)(1−p(k))(EXGRADVterm)0.933(±0.063)N/APCk=1(cid:12)(cid:12)1C−p(k)(cid:12)(cid:12)(GRADNORMVterm)0.935(±0.064)N/ADatasetVOnly∥h∥1VGradient-BasedIDOODMSPEnergyVarSumTVMSPEnergyVarSumGradNormExGradMNISTCIFAR100.9710.9570.9720.9740.9070.9280.8670.8960.963SVHN0.9870.9890.9880.9900.9780.9800.9490.9710.975CIFAR10MNIST0.9240.8310.9250.9280.9130.8600.8950.9130.921SVHN0.9430.9260.9440.9480.9940.9570.9950.9950.919SVHNMNIST0.8130.8070.8130.8110.7480.7400.6370.7350.840CIFAR100.9550.9530.9560.9550.9360.9140.8440.9260.931Average0.9320.910.9330.9350.9130.8960.8650.9060.925ImageNetiNaturalist0.8760.8850.8840.8850.8920.9170.7480.9040.769SUN0.7820.8520.7900.8300.8180.9100.7870.8900.666Places0.7670.8130.7710.7890.7950.8720.7290.8490.689Textures0.7440.7580.7500.7610.7780.8170.7290.8110.651Average0.7920.8270.7990.8160.8210.8790.7480.8640.694Under review as a conference paper at ICLR 2023\n\net al., 2016). Our code for these experiments was built on top of the code from Huang et al. (2021)3, and more details about this baseline can be found in their paper.\n\nExperimenting with the Encoding-Output Composition In what follows, we test a variety of detectors that adhere to the encoding-output composition described in Section 3.1. In particular, each of the methods we test here are a product of a U term, which is a function of the encoding h, and a V term, which is a function of the outputted probability vector p. We test all combinations of U\n\nEnergy, VarSum, MSP, TV\n\nand V\n\n1,\n\nh\n\n∈ {\n\n∥\n\n∥1}\n\n∈ {\n\n. }\n\nHere, U = 1 denotes not using any encoding of the input and only using the output score (i.e. V term). For the V term, TV is the TV distance between p and a discrete uniform distribution (V term from Eq. 4). The scores for Energy (Liu et al., 2020) and MSP (Hendrycks & Gimpel, 2016) are as follows:\n\nSEnergy = T log\n\nC\n\nef (k)(x)/T\n\n(cid:88)k=1\n\nSMSP = max\n\n[C]\n\nk\n\n∈\n\np(k)\n\nNote that the energy score we use here is the negative version of the one originally introduced in Liu et al. (2020). Like before, we assume that T = 1 for all experiments.\n\nC\n\nk=1 p(k)(1\n\nVarSum is a term inspired by the decomposition of EXGRAD (i.e the V term from Eq. 5). Because p(k)) is anti-correlated with every other score, we make VarSum a correlated V = p(k)). Each of the V terms explored version of this. In particular, VarSum = 1 here are visualized in Figure 1, with the exception of Energy, since logits cannot be deduced from probabilities alone.\n\nk=1 p(k)(1\n\n(cid:80)\n\n(cid:80)\n\n−\n\n−\n\n−\n\nC\n\nTable 2 displays the OOD detection performance for each of these methods in both the small-scale benchmark setting (with MNIST, CIFAR10, SVHN datasets) and the large-scale benchmark setting with the ImageNet dataset. The small-scale experiments assume the same setting as Section 4.1.\n\nAnalysis of Results It is immediately clear that there is a significant difference between the ImageNet baseline and the other baselines, and as such, we average the AUROCs separately. Starting with the small-scale setting, we find the best performers to be methods which look at the outputted predicted distribution only. Interestingly, the two best scores appear to be VarSum and the TV distance, which to the best of our knowledge, have not been recommended for OOD detection by previous works.\n\nLiterature generally warns against exclusively using the model’s predicted distribution for OOD detection (Hein et al., 2019; Kirsch et al., 2021) since even OOD points can produce confident predictions with probabilities that are highly concentrated on a single class. These results indicate that this issue is milder in smaller scale models (MNIST, SVHN), and exacerbated for large models which are highly over-parameterized (CIFAR10, ImageNet). This result is generally in line with observations in calibration, which point out the over-confidence of neural network models as they become more over-parameterized (Guo et al., 2017; Wang et al., 2021).\n\nFollowing this, it seems that information about the penultimate layer encoding is important for high performing encoders in the ImageNet baselines. Besides VarSum, every possible V improves across all ImageNet baselines when multiplied by ∥1. It is unclear why VarSum and EXGRAD do not follow this trend and have subpar performance; however, we believe it may be related to how the sum of variance landscape changes with a dramatic increase in classes.\n\nh\n\n∥\n\nWe believe that at the time of writing this paper GRADNORM is the current state-of-the-art post hoc method for all considered benchmarks. However, importantly, we find that ENERGY is a strictly better detector than GRADNORM on the ImageNet benchmark. These results further strengthen our claim that gradients do not necessarily provide unique benefits for OOD detection performance. It is perhaps only the encoding-output decomposition that results in strong performance, rather than some unique property of gradients.\n\n∥1 ×\n\nh\n\n∥\n\n4.3 EXPLORING ENCODING CHOICES\n\nWhile the previous section limited the feature encoding to the L1 norm, in this section, we study the effect of encoding design choice by varying the norm applied. Although Huang et al. (2021) do an\n\n3https://github.com/deeplearning-wisc/gradnorm_ood\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: Average AUROC over ImageNet Benchmark Each cell of the heatmap shows the average AUROC for a different configuration of probability output score (y-axis) and order of the norm on the encoding fed to the last layer (x-axis).\n\nablation which changes the order of the norm, they do this ablation on the gradients themselves. By focusing on the norm of h, we are able to have more control over the score and the resulting detector.\n\nh\n\nFigure 2 displays the average AUROC performance in the ImageNet benchmark for 48 different pairs of U V scores, where U Energy, TV, MSP, VarSum . Heatmaps for every experiment are shown in Appendix C. The results indicate that the choice of encoding does have a significant effect in a score’s OOD detection performance. In particular, Energy achieves an AUROC of 0.917, which is a drastic improvement over any score in Table 2. It is also better than some previously proposed non-post hoc methods such as MOS (Huang & Li, 2021), which achieves an average AUROC of 0.901.\n\n0, 0.1, 0.3, 0.5, 0.8, 1, 2, 3, 4, 5, 6,\n\n∥p : p\n\n∥0.3 ×\n\n, and V\n\n∞}}\n\n∈ {∥\n\n∈ {\n\n∈ {\n\nh\n\n∥\n\n}\n\nWe note that this result is not exactly fair since the parameter scan was done over the benchmark test set. Nevertheless, this is an encouraging result for what could possibly be achieved by a method that considers both image encoding and network output. In particular, we believe that devising more sophisticated U terms than the naive ones tested here could be a promising direction for improving OOD detection performance within this framework.\n\n4.4 CHALLENGING THE FEATURE-EXTRACTION HYPOTHESIS\n\nAs described above, one emerging theory that attempts to explain the role of gradients in OOD detection is the Feature-Extraction Hypothesis. Quoting Lee & AlRegib (2020),\n\nGradient-based optimization involves larger updates when there is a larger gap between predictions and correct labels for given inputs. It implies that the model requires more significant adjustments to its parameters, as it has not learned enough features to represent the inputs or relationships between learned features and classes for correct prediction.\n\nWe challenge the plausibility of the Feature-Extraction Hypothesis in explaining the role of gradients with the following observation: the gradient calculated at a singleton test point ̃x can be associated with a learning problem that requires no feature learning in order to minimise the loss. In particular, we note that previous gradient based scores involve calculating gradients for optimisation problems of the form minθ EY [l(Y, p( ̃x))]. Notably, degenerate mappings4 lie in the solution space for such optimisation problems, which by definition cannot extract features from input data. In order to investigate the plausibility of the Feature Extraction Hypothesis, we define a score function that by construction excludes degenerate mappings in its solution space. Specifically, we define the score\n\nlog p(Y ID\n\ni )(xID i )\n\n,\n\n(6)\n\nSBG(x) = EY\n\np\n\n∼\n\n∇θ\n\nlog p(Y )(x) +\n\n(cid:32)\n\nC\n\ni=1 (cid:88)\n\n(cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1(cid:35)\n\n(cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ni , Y ID\n\nwhere (xID i ) is an in distribution training point belonging to class i. We refer to the OOD classifier based on this score function as BATCHGRAD. In particuar, we note that the loss function inside the parentheses of equation 6 defines an optimisation problem that, for sufficiently expressive networks, cannot have degenerate mappings as its solution. This is because the 2nd term inside\n\n4Note that by “true mapping”, we mean a mapping that is not degenerate, and by “degenerate mapping” we\n\nmean a mapping that maps to the same value for all inputs.\n\n8\n\n00.10.30.50.8123456infNormEnergyTVMSPVarSum0.8200.8240.9170.9130.8970.8790.7680.6890.6510.6340.6250.6110.8030.7790.8920.8940.8810.8640.7390.6560.6170.6000.5920.5790.7940.8370.8510.8430.8300.8210.7850.7650.7550.7500.7480.7420.3800.6190.7640.7740.7660.7480.5480.4100.3790.3780.3830.409Under review as a conference paper at ICLR 2023\n\nDeep BATCHGRAD\n\nAUROC 0.787 (±0.224)\n\nShallow BATCHGRAD 0.925 (±0.044)\n\nTable 3: BATCHGRAD Small-Scale Experimental Results This table shows AUROC results, calculated across the 6 ID-OOD dataset combinations from\n\nMNIST, CIFAR-10, SVHN\n\n{\n\n. }\n\nthe parentheses penalizes choices of θ that parameterise a degenerate mapping. Note that this is in contrast to other loss functions, such as those used in EXGRAD and GRADNORM, where degenerate mappings are in the solution spaces of their associated optimisation problems. The motivation for the BATCHGRAD loss function is to design an experiment where gradients are guaranteed to point towards a true (i.e. non-degenerate) mapping, giving a better chance of observing the Feature Extraction Hypothesis in action, should it be true. In particular, if the Feature Extraction Hypothesis were true, then we would be especially likely to see a reduction in OOD detection performance when restricting gradients to the final layer of BATCHGRAD. However, as shown in the results in Table 3 we find that the opposite holds: the variant of BATCHGRAD with gradients restricted to just the last layer is more informative for OOD detection than when using the gradients w.r.t. the whole network. This observation is consistent with previous experiments showing the sufficiency, and at times improvement, of restricting gradients to final layer parameters. This experimental result, in combination with analysis demonstrating the significance of the last layer gradients, leads us to conclude that the Feature-Extraction Hypothesis is not an appropriate explanation for the high performance of gradient-based OOD detection methods.\n\n5 DISCUSSION\n\nIn this work, we experimentally investigated gradient-based OOD detection methods. While using gradient-based approaches can result in strong performance, we find previous explanations attributing performance to gradients unsatisfactory. Although prior works have focused on the interpretation of taking the gradient with respect to the KL divergence between the outputted distribution and a discrete uniform distribution (Lee & AlRegib, 2020; Huang et al., 2021), we find that other gradient-based scores that do not have this interpretation also perform well, especially on smaller scale problems.\n\nWe also question the idea that the gradient space of the neural network holds key information used for OOD detection. Our experiments provide evidence against the hypothesis that gradient-based methods are informed by large changes needed in the network to capture unseen, OOD images. Moreover, we show that we can derive better performing detectors that are agnostic to gradients and only use the encoding-output decomposition discussed in Huang et al. (2021). As such, we believe the strength of GRADNORM comes not from its leverage of gradients, but solely from the fact that it fuses information about network encodings and outputted distributions. Hence, while it is possible that gradients contain useful information for OOD detection, we do not believe that previous methods leverage information from gradients that cannot be derived more easily through other means.\n\nFuture Work Our hope is that the insights provided in this work can be used to further improve OOD detection. In particular, we believe that more advanced methods can be developed that fuse together information from the network encoding and scores derived from the model’s predicted distribution. For example, we evaluated the choice of input encodings by varying the order of the norm applied on the last hidden layer outputs. Investigating the utility of other hidden layer features or auxiliary encoders and studying what properties of an encoding are helpful in detecting OOD data could provide further insights to devising stronger OOD detectors.\n\nAnother interesting direction for investigation is what role task difficulty and model capacity play in how these detectors perform. We found that when the in distribution task was easier, top performing detectors only depended on network outputs; however, it was essential to use both outputs and input encodings for the more complex ImageNet tasks. As mentioned in Section 4.2, existing works in uncertainty quantification and calibration have noted the unreliability of a deep model’s predicted class distribution. While calibration is orthogonal to the scope of this work, we believe there could be fundamental ties in determining the boundary between solely relying on the output predicted distribution for OOD detection (i.e. utilizing the V term only), and additionally requiring extracted information from the input space via input encodings (utilizing the U term). We leave investigating this direction for future work.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nReproduciblity Statement Our experimental setup follows closely that of our main baseline Huang et al. (2021). We have followed their code as provided in the public github repo https://github. com/deeplearning-wisc/gradnorm_ood, which includes the pre-trained models and data processing. For the ”small scale” experiments (any experiments involving MNIST, CIFAR10, SVHN datasets), we use the pre-trained models as they are provided in the public github repo https://github.com/aaron-xichen/pytorch-playground, and follow the rest of the protocol as provided in the earlier repo.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAgarwal, C., D’souza, D., and Hooker, S. Estimating example difficulty using variance of gradients.\n\narXiv preprint arXiv:2008.11600, 2020.\n\nAgarwal, T., Arora, H., and Schneider, J. Learning urban driving policies using deep reinforcement learning. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pp. 607–614. IEEE, 2021.\n\nBoyer, M., Wai, J., Clement, M., Kolemen, E., Char, I., Chung, Y., Neiswanger, W., and Schneider, J. Machine learning for tokamak scenario optimization: combining accelerating physics models and empirical models. Bulletin of the American Physical Society, 2021.\n\nChar, I., Chung, Y., Boyer, M., Kolemen, E., and Schneider, J. A model-based reinforcement learning approach for beta control. In APS Division of Plasma Physics Meeting Abstracts, volume 2021, pp. PP11–150, 2021.\n\nCimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3606–3613, 2014.\n\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\n\nDeng, L. The mnist database of handwritten digit images for machine learning research. IEEE Signal\n\nProcessing Magazine, 29(6):141–142, 2012.\n\nDietterich, T. G. and Guyer, A. The familiarity hypothesis: Explaining the behavior of deep open set\n\nmethods. Pattern Recognition, 132:108931, 2022.\n\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In\n\nInternational Conference on Machine Learning, pp. 1321–1330. PMLR, 2017.\n\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European\n\nconference on computer vision, pp. 630–645. Springer, 2016.\n\nHein, M., Andriushchenko, M., and Bitterwolf, J. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nHendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples\n\nin neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\nHendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. arXiv\n\npreprint arXiv:1812.04606, 2018.\n\nHuang, R. and Li, Y. Mos: Towards scaling out-of-distribution detection for large semantic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8710–8719, 2021.\n\nHuang, R., Geng, A., and Li, Y. On the importance of gradients for detecting distributional shifts in\n\nthe wild. Advances in Neural Information Processing Systems, 34, 2021.\n\nKirsch, A., Mukhoti, J., Amersfoort, J. v., Torr, P. H., and Gal, Y. On pitfalls in ood detection:\n\nPredictive entropy considered harmful. 2021.\n\nKokilepersaud, K., Prabhushankar, M., AlRegib, G., Corona, S. T., and Wykoff, C. Gradient-based severity labeling for biomarker classification in oct. In 2022 IEEE International Conference on Image Processing (ICIP), pp. 3416–3420. IEEE, 2022.\n\nKolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pp. 491–507. Springer, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nKrizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009.\n\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty\n\nestimation using deep ensembles. arXiv preprint arXiv:1612.01474, 2016.\n\nLee, J. and AlRegib, G. Gradients as a measure of uncertainty in neural networks. In 2020 IEEE\n\nInternational Conference on Image Processing (ICIP), pp. 2416–2420. IEEE, 2020.\n\nLee, J. and AlRegib, G. Open-set recognition with gradient-based representations. In 2021 IEEE\n\nInternational Conference on Image Processing (ICIP), pp. 469–473. IEEE, 2021.\n\nLee, K., Lee, H., Lee, K., and Shin, J. Training confidence-calibrated classifiers for detecting\n\nout-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.\n\nLee, K., Lee, K., Lee, H., and Shin, J. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.\n\nLiang, S., Li, Y., and Srikant, R. Enhancing the reliability of out-of-distribution image detection in\n\nneural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nLinmans, J., van der Laak, J., and Litjens, G. Efficient out-of-distribution detection in digital\n\npathology using multi-head convolutional neural networks. In MIDL, pp. 465–478, 2020.\n\nLiu, W., Wang, X., Owens, J. D., and Li, Y. Energy-based out-of-distribution detection. arXiv\n\npreprint arXiv:2010.03759, 2020.\n\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Reading digits in natural images\n\nwith unsupervised feature learning. 2011.\n\nRen, J., Liu, P. J., Fertig, E., Snoek, J., Poplin, R., DePristo, M. A., Dillon, J. V., and Lakshminarayanan, B. Likelihood ratios for out-of-distribution detection. arXiv preprint arXiv:1906.02845, 2019.\n\nRudin, C. Stop explaining black box machine learning models for high stakes decisions and use\n\ninterpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.\n\nSalehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M. H., and Sabokrou, M. A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges. arXiv preprint arXiv:2110.14051, 2021.\n\nSerr`a, J., ́Alvarez, D., G ́omez, V., Slizovskaia, O., N ́u ̃nez, J. F., and Luque, J. Input complexity and out-of-distribution detection with likelihood-based generative models. arXiv preprint arXiv:1909.11480, 2019.\n\nSun, J., Yang, L., Zhang, J., Liu, F., Halappanavar, M., Fan, D., and Cao, Y. Gradient-based novelty detection boosted by self-supervised binary classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8370–8377, 2022.\n\nTechapanurak, E., Suganuma, M., and Okatani, T. Hyperparameter-free out-of-distribution detection\n\nusing cosine similarity. In Proceedings of the Asian Conference on Computer Vision, 2020.\n\nVan Amersfoort, J., Smith, L., Teh, Y. W., and Gal, Y. Uncertainty estimation using a single deep deterministic neural network. In International Conference on Machine Learning, pp. 9690–9700. PMLR, 2020.\n\nVan Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset-supplementary material. Reptilia, 32(400):1–3, 2017.\n\nVaze, S., Han, K., Vedaldi, A., and Zisserman, A. Open-set recognition: A good closed-set classifier is all you need. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=5hLP5JY9S2d.\n\nVernekar, S., Gaurav, A., Abdelzad, V., Denouden, T., Salay, R., and Czarnecki, K. Out-of-distribution\n\ndetection in classifiers via generation. arXiv preprint arXiv:1910.04241, 2019.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nWang, D.-B., Feng, L., and Zhang, M.-L. Rethinking calibration of deep neural networks: Do not be\n\nafraid of overconfidence. Advances in Neural Information Processing Systems, 34, 2021.\n\nXiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 3485–3492. IEEE, 2010.\n\nYang, J., Zhou, K., Li, Y., and Liu, Z. Generalized out-of-distribution detection: A survey. arXiv\n\npreprint arXiv:2110.11334, 2021.\n\nYu, Q. and Aizawa, K. Unsupervised out-of-distribution detection by maximum classifier discrepancy. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\n\nZhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6): 1452–1464, 2017.\n\nZhou, H., Cheng, C., Lipton, Z. C., Chen, G. H., and Weiss, J. C. Mortality risk score for critically ill patients with viral or unspecified pneumonia: Assisting clinicians with covid-19 ecmo planning. In International Conference on Artificial Intelligence in Medicine, pp. 336–347. Springer, 2020.\n\nZisselman, E. and Tamar, A. Deep residual flow for out of distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13994–14003, 2020.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nA ADDITIONAL EXPERIMENTAL DETAILS\n\nThe tables below shows detailed breakdowns of AUROC results for the small-scale experiments. Note that the top row describes the ID dataset, and the two entries beneath describe the OOD datasets.\n\nID Dataset MNIST OOD Dataset 0.843 Deep BATCHGRAD Shallow BATCHGRAD 0.851\n\nSVHN\n\nCIFAR 0.947 0.922\n\nMNIST SVHN CIFAR 0.974 0.975\n\n0.950 0.963\n\nCIFAR SVHN MNIST 0.505 0.920\n\n0.502 0.921\n\nB DERIVATION OF EXGRAD DECOMPOSITION\n\nWe will now rewrite Eq. 3 into a more digestible form. Assume that we only take the derivative with D are the parameters for the last layer, respect to the last layer of the network, assume that W and let the output of the last layer be fθ(h) = W h, where h is the encoding taken in by the last layer. Although we have not considered bias terms in this formulation, note that they can easily be included by adding another dimension to h with the value 1. For readability, we write p instead of p(h) and f (k) instead of f (k)(h).\n\nRC\n\n∈\n\n×\n\nFirst, fix a class k, and note that\n\n∂ ∂f (k)\n\nθ\n\nlog\n\n(cid:32)\n\nθ /T\n\nef (k) k′=1 ef (k′ )/T\n\nC\n\nθ\n\n(cid:80)\n\n(cid:33)\n\n=\n\n=\n\n=\n\n∂ ∂f (k)\n\nθ (cid:32)\n\nf (k) θ\nT −\n\nlog\n\nef (k′ )\n\nθ\n\n/T\n\n(cid:33)\n\n(cid:88)k\n\nθ\n\nef (k) k ef (k′ )\n\nθ (cid:33)\n\n−\n\n1 1\nT (cid:32) 1\nT\n\n1\n\n(cid:16)\n\np(k) (cid:80)\n\n−\n\n(cid:17)\n\n14\n\nScoreExpressionGradientSVHNMNISTCIFARMNISTCIFARSVHNCIFARSVHNMNIST∥EY∼p(cid:2)∇θlogp(Y)(cid:3)∥22Deep0.5950.7300.8390.7860.6690.733∥EY∼p(cid:2)∇θlogp(Y)(cid:3)∥22Shallow0.6510.7960.8380.7750.6350.749EY∼Uniform(cid:2)∥∇θlogp(Y)∥1(cid:3)Deep0.6920.8160.7900.7170.9930.940EY∼Uniform(cid:2)∥∇θlogp(Y)∥1(cid:3)Shallow0.6060.8170.9390.8570.9950.886EY∼Uniform(cid:2)∥∇θlogp(Y)∥22(cid:3)Deep0.5910.8950.9860.9570.9540.819EY∼Uniform(cid:2)∥∇θlogp(Y)∥22(cid:3)Shallow0.7090.9190.9760.9030.9940.819(cid:13)(cid:13)∇θEY∼Uniform(cid:2)logp(Y)(cid:3)(cid:13)(cid:13)1Deep0.7590.9370.9110.8140.9890.943(cid:13)(cid:13)∇θEY∼Uniform(cid:2)logp(Y)(cid:3)(cid:13)(cid:13)1Shallow0.7350.9260.9710.8960.9950.913EY∼phlogp(Y)p(Y)∥∇θlogp(Y)∥22iShallow0.7490.9330.9830.9280.9940.870EY∼p(cid:2)∥∇θlogp(Y)∥22(cid:3)Shallow0.8480.9080.9640.9550.9150.925EY∼p(cid:2)∥∇θlogp(Y)∥22(cid:3)Deep0.8620.9360.9560.9470.9160.910EY∼phlogp(Y)p(Y)∥∇θlogp(Y)∥22iDeep0.7100.9260.9930.9680.9840.947EY∼p(cid:2)∥∇θlogp(Y)∥1(cid:3)Shallow0.8400.9310.9750.9630.9190.921EY∼p(cid:2)∥∇θlogp(Y)∥1(cid:3)Deep0.8300.9470.9770.9650.9210.918PCk=1p(k)(1−p(k))N/A0.8130.9560.9880.9720.9440.926PCk=1(cid:12)(cid:12)1C−p(k)(cid:12)(cid:12)N/A0.8110.9550.9900.9740.9480.928Under review as a conference paper at ICLR 2023\n\nFor a class k′\n\n= k,\n\n∂ ∂f (k′)\n\nθ\n\nlog\n\n(cid:32)\n\nθ\n\nef (k) k′=1 ef (k′ )\n\nC\n\nθ (cid:33)\n\n=\n\n−\n\nθ\n\n/T\n\nef (k′ ) k ef (k′ )\n\nθ\n\n/T\n\n(cid:80)\n\n= −\n\np(k′) (cid:80) T\n\nBuilding on this,\n\n∂ ∂W\n\nlog\n\n(cid:32)\n\nθ /T\n\nef (k) k′=1 ef (k′)\n\nC\n\nθ\n\n=\n\n1 T\n\nh\n\n/T (cid:33)\n\n(cid:80)\n\n=\n\np(1),\n\n−\n\np(2),\n\n−\n\n. . . , 1\n\n−\n\np(k),\n\n. . .\n\n(cid:2)\n\n− −\n\np(1)h(1) p(1)h(2) ...\n\n1 T  \n\n\n− −\n\np(2)h(1) p(2)h(2) ...\n\n. . . . . .\n\n...\n\n(1 (1\n\n− −\n\n(cid:3) p(k))h(1) p(k))h(2) ...\n\n. . . . . .\n\n...\n\n\n\n \n\nThe L1 norm of the gradient with respect to W is just the sum of the absolute values of each entry.\n\nD\n\ni=1 (cid:88)\n\n(1\n\n\n\n−\n\np(k))\n\n+\n\nhi| |\n\n\n\np(k′)\n\nhi| |\n\n\n\n=k\n\n(cid:88)k′\n\nD\n\n=\n\ni=1 (cid:16) (cid:88) = 2(1\n\n(1\n\np(k)) |\n\nhi|\n\n−\n\n+ (1\n\np(k))\n\nhi| |\n\n−\n\n(cid:17)\n\np(k))\n\nh\n\n∥1\n\n∥\n\n−\n\nPutting it all together,\n\n(x) = Ek\n\np\n\n∼\n\nS\n\n∇W log\n\nθ /T\n\nef (k) k′=1 ef (k′ )\n\nC\n\nθ\n\n(cid:32)\n\n(cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) p\n(cid:104)\n\n∼\n\n(1\n\nC\n\n2 T\n\nEk\n\n2 T ∥\n\nh\n\n∥1\n\n=\n\n=\n\np(k))\n\n(cid:80) h\n∥\n\n∥1\n\n−\n\n(cid:105) p(k))\n\np(k)(1\n\n−\n\nc=1 (cid:88)\n\n1(cid:35)\n\n/T (cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nC ADDITIONAL HEATMAPS\n\nIn this section, we present additional heatmaps (similar to Figure 2) which indicate the AUROC for each of the methods evaluated in Section 4.3, separately for each of the ID-OOD dataset settings tested in the experiments section. The title of each plot indicates the ID and OOD dataset in order, i.e. “ID x OOD”. Each cell of the heatmap shows the AUROC for a different configuration of probability output score (y-axis) and order of the norm on the encoding fed to the last layer (x-axis).\n\n15\n\n00.10.30.50.8123456infNormEnergyTVMSPVarSum0.7660.5910.6850.7150.7340.7400.7520.7560.7570.7570.7570.7560.7530.5670.6650.7030.7270.7350.7510.7540.7550.7550.7550.7470.7600.5870.6860.7200.7410.7480.7610.7640.7650.7650.7650.7580.5600.5060.5660.6000.6260.6370.6560.6580.6560.6530.6490.624SVHN x MNIST̸ ̸\nUnder review as a conference paper at ICLR 2023\n\n16\n\n00.10.30.50.8123456infNormEnergyTVMSPVarSum0.9090.7060.8390.8800.9050.9140.9300.9360.9380.9390.9390.9350.9210.7010.8450.8920.9180.9260.9400.9450.9460.9470.9470.9440.9320.7300.8730.9110.9300.9360.9470.9500.9510.9520.9520.9500.6590.6160.7140.7740.8240.8440.8880.9010.9060.9070.9060.871SVHN x CIFAR1000.10.30.50.8123456infNormEnergyTVMSPVarSum0.9830.7040.9480.9710.9780.9800.9830.9830.9830.9830.9830.9810.8170.4960.8470.9330.9650.9710.9800.9810.9810.9800.9800.9740.8600.5530.8930.9530.9730.9780.9840.9840.9840.9840.9840.9790.2940.4110.7770.8880.9370.9490.9650.9670.9660.9650.9640.955MNIST x SVHN00.10.30.50.8123456infNormEnergyTVMSPVarSum0.9470.6960.8900.9140.9240.9280.9330.9330.9330.9320.9320.9260.7490.4970.7810.8520.8860.8960.9110.9130.9120.9110.9090.8940.7740.5420.8110.8700.8980.9070.9200.9210.9200.9190.9170.9050.3310.4260.7220.8100.8540.8670.8880.8910.8900.8880.8850.866MNIST x CIFAR10Under review as a conference paper at ICLR 2023\n\n17\n\n00.10.30.50.8123456infNormEnergyTVMSPVarSum0.9510.9750.9710.9650.9600.9570.9480.9440.9410.9400.9380.9330.9940.9940.9950.9960.9960.9950.9940.9910.9880.9860.9830.9530.9890.9940.9950.9950.9950.9940.9880.9840.9800.9760.9730.9470.9930.9930.9950.9950.9950.9950.9940.9920.9890.9860.9830.938CIFAR10 x SVHN00.10.30.50.8123456infNormEnergyTVMSPVarSum0.8720.9480.9140.8910.8700.8600.8340.8190.8090.8010.7950.7630.9770.9670.9590.9480.9280.9130.8340.7580.6870.6230.5680.3180.9640.9690.9600.9480.9270.9130.8440.7800.7220.6700.6260.4340.9740.9650.9540.9400.9150.8950.7870.6810.5860.5030.4360.197CIFAR10 x MNIST00.10.30.50.8123456infNormEnergyTVMSPVarSum0.8610.8160.9290.9320.9260.9170.8230.7280.6780.6540.6430.6250.8630.7710.9070.9140.9120.9040.8160.7150.6600.6340.6220.6010.8730.8860.9030.9010.8960.8920.8710.8550.8460.8410.8380.8320.2800.5340.7320.7530.7560.7480.5340.3600.3310.3330.3400.375ImageNet x iNaturalUnder review as a conference paper at ICLR 2023\n\n18\n\n00.10.30.50.8123456infNormEnergyTVMSPVarSum0.8210.7850.9110.9170.9150.9100.8490.7780.7350.7120.7000.6740.7860.7260.8780.8900.8930.8900.8270.7540.7100.6860.6730.6450.7760.7940.8300.8290.8230.8180.7920.7740.7630.7570.7540.7470.2540.5470.7440.7690.7830.7870.7150.5630.5010.4820.4770.481ImageNet x SUN00.10.30.50.8123456infNormEnergyTVMSPVarSum0.7890.7620.8810.8850.8800.8720.8010.7350.6990.6810.6710.6510.7400.6970.8420.8530.8540.8490.7770.7040.6660.6470.6370.6170.7620.7760.8060.8060.8000.7950.7720.7550.7450.7390.7360.7290.3130.5350.6920.7130.7260.7290.6500.5240.4840.4740.4730.480ImageNet x Places00.10.30.50.8123456infNormEnergyTVMSPVarSum0.8080.9320.9440.9200.8650.8170.6000.5150.4910.4860.4870.4930.8220.9220.9420.9200.8670.8110.5360.4500.4340.4340.4380.4530.7650.8930.8650.8370.8000.7780.7040.6770.6680.6640.6630.6610.6740.8590.8870.8620.7990.7290.2940.1920.2010.2220.2410.299ImageNet x Textures",
    "reference": "# Summary Of The Paper\n\nThis paper evaluates different methods for OOD detection which use and do not use gradient information. They decompose several methods as in [1] into two components related to the encoding and output probability vectors. \nThrough their decompositions they propose different OOD detection methods not based on gradients which outperform benchmarks such as grad norm [1] in some settings.\n\n[1] Huang et al. 2021\n\n# Strength And Weaknesses\n\n- The paper is written clearly and the contributions are clearly stated.\n- Some conclusions are not fully supported/could use additional explanations\nQuestions/comments\n- The results have high variance (cf. Fig 2), in this case the paper would greatly benefit from studying the performance of the proposed OOD methods on additional datasets. Conversely, are the authors proposed that using ||h||_{0.3} x energy should be the OOD score to use across different tasks/models?\n- Eq 6 requires a more through explanation. Why does it imply learning a true mapping? \n- Can the authors elaborate on their explanation why varsum and exgrad have subpar performance? \n- The authors don't include AUPR results which should be included. While AUC-ROC shows us the tradeoff between true positive and false positive predictions, and the AUC-PR shows us the tradeoff between precision (low false positive) and recall (low false negative) predictions.\n- Do the authors believe that the feature extraction / familiarity hypothesis [1] holds for the last layer? \n\n[1] Dietterich and Guyer 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\n- The paper is written clearly and the contributions are clearly stated, and the results are interesting.\n\n# Summary Of The Review\n\nThe paper is very interesting and beautifully written. However, some statements and results could benefit from additional clarity. I would be glad to see this paper accepted if the authors can address all the reviewer’s questions (above) as it has an interesting take on ongoing OOD detection discussions in the literature.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work."
  }
]