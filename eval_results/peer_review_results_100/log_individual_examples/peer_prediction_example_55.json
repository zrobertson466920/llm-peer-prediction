{
  "example_idx": 55,
  "reference": "Published as a conference paper at ICLR 2023\n\nTURNING THE CURSE OF HETEROGENEITY IN FEDERATED LEARNING INTO A BLESSING FOR OUT-OFDISTRIBUTION DETECTION\n\nShuyang Yu1, Junyuan Hong1, Haotao Wang2, Zhangyang Wang2 and Jiayu Zhou1 1Department of Computer Science and Engineering, Michigan State University 2Department of Electrical and Computer Engineering, University of Texas at Austin {yushuyan,hongju12,jiayuz}@msu.edu, {htwang,atlaswang}@utexas.edu\n\nABSTRACT\n\nDeep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art for OoD tasks by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively. Codes are available: https://github.com/illidanlab/FOSTER.\n\n1\n\nINTRODUCTION\n\nDeep neural networks (DNNs) have demonstrated exciting predictive performance in many challenging machine learning tasks and have transformed various industries through their powerful prediction capability. However, it is well-known that DNNs tend to make overconfident predictions about what they do not know. Given an out-of-distribution (OoD) test sample that does not belong to any training classes, DNNs may predict it as one of the training classes with high confidence, which is doomed to be wrong (Hendrycks & Gimpel, 2016; Hendrycks et al., 2018; Hein et al., 2019).\n\nTo alleviate the overconfidence issue, various approaches are proposed to learn OoD awareness which facilitates the test-time detection of such OoD samples during training. Recent approaches are mostly achieved by regularizing the learning process via OoD samples. Depending on the sources of such samples, the approaches can be classified into two categories: 1) the real-data approaches rely on a large volume of real outliers for model regularization (Hendrycks et al., 2018; Mohseni et al., 2020; Zhang et al., 2021); 2) the synthetic approaches use ID data to synthesize OoD samples, in which a representative approach is the virtual outlier synthesis (VOS) (Du et al., 2022).\n\nWhile both approaches are shown effective in centralized training, they cannot be easily incorporated into federated learning, where multiple local clients cooperatively train a high-quality centralized\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nmodel without sharing their raw data (Koneˇcn`y et al., 2016), as shown by our experimental results in Section 5.2. On the one hand, the real-data approaches require substantial real outliers, which can be costly or even infeasible to obtain, given the limited resources of local clients. On the other hand, the limited amount of data available in local devices is usually far from being sufficient for synthetic approaches to generate effective virtual OoD samples.\n\nPractical federated learning approaches often suffer from the curse of heterogeneous data in clients, where non-iid (Li et al., 2020b) collaborators cause a huge pain in both the learning process and model performance in FL (Li et al., 2020a). Our key intuition is to turn the curse of data heterogeneity into a blessing for OoD detection: The heterogeneous training data distribution in FL may provide a unique opportunity for the clients to communicate knowledge outside their training distributions and learn OoD awareness. A major obstacle to achieving this goal, however, is the stringent privacy requirement of FL. FL clients cannot directly share their data with collaborators. This motivates the key research question: How to learn OoD awareness from non-iid federated collaborators while maintaining the data confidentiality requirements in federated learning?\n\nIn this paper, we tackle this challenge and propose Federated Out-of-distribution SynThesizER (FOSTER) to facilitate OoD learning in FL. The proposed approach leverages non-iid data from clients to synthesize virtual OoD samples in a privacy-preserving manner. Specifically, we consider the common learning setting of class non-iid (Li et al., 2020b), and each client extracts the external class knowledge from other non-iid clients. The server first learns a virtual OoD sample synthesizer utilizing the global classifier, which is then broadcast to local clients to generate their own virtual OoD samples. The proposed FOSTER promotes diversity of the generated OoD samples by incorporating Gaussian noise, and ensures their hardness by sampling from the low-likelihood region of the class-conditional distribution estimated. Extensive empirical results show that by extracting only external-class knowledge, FOSTER outperforms the state-of-out for OoD benchmark detection tasks.\n\nThe main contributions of our work can be summarized as follows:\n\n• We propose a novel federated OoD synthesizer to take advantage of data heterogeneity to facilitate OoD detection in FL, allowing a client to learn external class knowledge from other non-iid federated collaborators in a privacy-aware manner. Our work bridges a critical research gap since OoD detection for FL is currently not yet well-studied in literature. To our knowledge, the proposed FOSTER is the first OoD learning method for FL that does not require real OoD samples.\n\n• The proposed FOSTER achieves the state-of-art performance using only limited ID data stored in each local device, as compared to existing approaches that demand a large volume of OoD samples.\n\n• The design of FOSTER considers both the diversity and hardness of virtual OoD samples, making\n\nthem closely resemble real OoD samples from other non-iid collaborators.\n\n• As a general OoD detection framework for FL, the proposed FOSTER remains effective in more challenging FL settings, where the entire parameter sharing process is prohibited due to privacy or communication concerns. This is because that FOSTER only used the classifier head for extracting external data knowledge.\n\n2 RELATED WORK\n\nOoD detection. Existing OoD detection methods are mainly from two complementary perspectives. The first perspective focused on post hoc. Specifically, Hendrycks & Gimpel (2016) first introduced a baseline utilizing maximum softmax distribution probabilities (MSP). Based in this work, many improvements have been made by follow-up works in recent years, such as the calibrated softmax score (ODIN) (Liang et al., 2017), Mahalanobis distance (Lee et al., 2018), energy score (Liu et al., 2020), Likelihood Regret (Xiao et al., 2020), Confusion Log Probability (CLP) score (Winkens et al., 2020), adjusted energy score Lin et al. (2021), k-th nearest neighbor (KNN) (Sun et al., 2022), and Virtual-logit Matching (ViM) (Wang et al., 2022). Compared with post hoc methods, FOSTER can dynamically shape the uncertainty surface between ID and OoD samples. Different post hoc methods are also applied in our experiment section as baselines.\n\nAnother perspective tends to detect OoD samples by regularization during training, in which OoD samples are essential. The OoD samples used for regularization can be either real OoD samples or\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nvirtual synthetic OoD samples. Real OoD samples are usually natural auxiliary datasets (Hendrycks et al., 2018; Mohseni et al., 2020; Zhang et al., 2021). However, real OoD samples are usually costly to collect or infeasible to obtain, especially for terminals with limited sources. Regularization method utilizing virtual synthetic OoD samples do not rely on real outliers. Grci ́c et al. (2020) trained a generative model to obtain the synthetic OoD samples. Jung et al. (2021) detect samples with different distributions by standardizing the max logits without utilizing any external datasets. Tack et al. (2020); Sehwag et al. (2021) proposed contrastive learning methods that also does not rely on real OoD samples. Du et al. (2022) proposed VOS to synthesize virtual OoD samples based on the low-likelihood region of the class-conditional Gaussian distribution. Current state-of-the-art virtual OoD methods are usually thirsty for ID data, which is not sufficient enough for local clients. Compared with these existing methods, the proposed FOSTER can detect OoD samples with limited ID data stored in each local device, without relying on any auxiliary OoD datasets.\n\nFederated Learning. Federated learning (FL) is an effective machine learning setting that enables multiple local clients to cooperatively train a high-quality centralized mode (Koneˇcn`y et al., 2016). FedAvg (McMahan et al., 2017), as a classical FL model, performs model averaging of distributed local models for each client. It shows an excellent effect on reducing the communication cost. Based on FedAvg, many variants (Wang & Joshi, 2018; Basu et al., 2019) have been proposed to solve the problems arising in FedAvg, such as convergence analysis (Kairouz et al., 2021; Qu et al., 2020), heterogeneity (Li et al., 2020a; Hsu et al., 2019; Karimireddy et al., 2020; Zhu et al., 2021), communication efficiency (Reddi et al., 2020). Among these problems, although heterogeneity of data will make the performance of ID data worse, it will give us a great chance to learn from the external data from other non-iid collaborators. Even though FOSTER is used the FedAvg framework, as a general OoD detection method for FL, FOSTER can also be applied to other variants of FedAvg.\n\n3 PROBLEM FORMULATION\n\nIn this paper, we consider classification tasks in heterogeneous FL settings, where non-iid clients have their own label set for training and testing samples. Our goal is to achieve OOD-awareness on each client in this setting.\n\nOoD training. The OoD detection problem roots in general supervised learning, where we learn a . Formally, we define a learning classifier mapping from the instance space task by the composition of a data distribution .\nThen any x as outof-distribution data. Hence, an ideal OoD detection oracle can be formulated as a binary classifier 1 for OoD q⇤(x) = I(x samples. With these notations, we define the OoD learning task as\n\nX D⇢X is denoted as in-distribution (ID) data, and otherwise, x\n\n), where I is an indication function yielding 1 for ID samples and\n\nand a ground-truth labeling oracle c⇤ :\n\nto the label space\n\n⇠ Q ⇢ X \\D\n\nX!Y\n\n⇠D\n\n⇠D\n\n:=\n\nY\n\n, c⇤\n\n,\n\n.\n\nT\n\nhD\n\nQ\n\ni\n\nTo parameterize the labeling and OoD oracles, we use a neural network consisting of two stacked components: a feature extractor f : governed by ✓h, where is the latent feature space. For the ease of notation, let hi(z) denote the predicted logit for class i = 1, . . . , c on extracted feature z . We unify the parameters of the classifier as ✓ = (✓f , ✓h). We then formulate the OoD training as minimizing the following loss on the task :\n\ngoverned by ✓f , and a classifier h :\n\nZ!Y\n\nX!Z\n\n⇠Z\n\nZ\n\nT\n\nJ\n\nT\n\n(✓) := Ex\n\n⇠D\n\n`CE\n\nh(f (x; ✓f ); ✓h), c⇤(x)\n\n+  Ex0⇠Q\n\n`OE\n\nf (x0; ✓f ); ✓h\n\n,\n\nh\n\n⇣\n\nwhere `CE is the cross-entropy loss for supervised learning and `OE is for OoD regularization. We use E[ ] to denote the expectation estimated by the empirical average on samples in practice. The non-negative hyper-parameter  trade off the OoD sensitivity in training. We follow the classic OoD training method, Outlier Exposure (Hendrycks et al., 2018), to define the OoD regularization for classification problem as\n\n·\n\n⌘i\n\nh\n\n⇣\n\n⌘i\n\n`OE(z0; ✓h) := E(z0; ✓h)\n\nc\n\ni=1\n\nhi(z0; ✓h),\n\n(1)\n\ni ehi(z0;✓h)/T is the energy function, given the temperature parameter where E(z0; ✓h) = T > 0. At test time, we approximate the OoD oracle q⇤ by the MSP score (Hendrycks & Gimpel, 2016).\n\nT log\n\nP\n\nc\n\nX\n\nHeterogeneous federated learning (FL) is a distributed learning framework involving multiple clients with non-iid data. There are different non-iid settings (Li et al., 2020b; 2021), and in this paper,\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nK\n\nY\n\nX!Y\n\n{Tk}\n\nk=1 where\n\nk are non-identical for different k resulting non-identical\n\nwe follow a popular setting that the non-iid property is only concerned with the classes (Li et al., Qk, c⇤ki 2020b). Given K clients, we define the corresponding set of tasks k is a subset of and c⇤k : the global label set . Since the heterogeneity is known to harm the convergence and performance of FL (Yu et al., 2020), we adopt a simple personalized FL solution to mitigate the negative impact, where each client uses a personalized classifier head hk upon a global feature extractor f (Arivazhagan et al., 2019). This gives the general objective of FL: min✓ Tk (✓). The optimization problem can be solved alternatively by two steps: 1) local minimization of the objective on local data and 2) aggregation by averaging client models. In this paper, we assume that each client only learns classes they see locally during training, because updating classifier parameters for unseen classes has no data support and doing so will almost certainly harm the performance of FL. To see this, Diao et al. showed that masking out unseen classes in the cross-entropy loss can merit the FL training (Diao et al., 2020).\n\nDk. Each\n\nTk =\n\nhDk,\n\nk=1 J\n\nP\n\n1 K\n\nY\n\nK\n\nChallenges. When we formulate the OoD training in FL, the major challenge is defining the OoD dataset Qk is Qk, which does not come for free. The centralized OoD detection of VOS assumes at the tail of an estimated Gaussian distribution of Dk (Du et al., 2022), which requires enormous examples from Dk for an accurate estimation of parameters. However, such a requirement is usually not feasible for a client per se, and the construction of\n\nQk remains a challenging question.\n\n4 METHOD\n\nIn this section, we first introduce the intuition of our proposed FOSTER, then elaborate on how to synthesize virtual external class data and avoid the hardness fading of the virtual OoD samples. The proposed framework is illustrated in Fig. 1.\n\nFigure 1: The framework of FOSTER. In step 1, to extract external class knowledge from local clients, the server first trains a generator utilizing the global classifier based on a cross-entropy objective function J(w) (Eq. (2)). In step 2, each local client utilizes the generator received to generate their own external class data z. To preserve the hardness of the virtual OoD samples, we also sample virtual outliers vk from the low-likelihood region of the class-conditional distribution estimated for the generated OoD samples. The virtual OoD samples vk are used for regularization of local client objective J(✓k) (Eq. (5)).\n\n4.1 NATURAL OOD DATA IN NON-IID FL\n\nRecent advances show promising OoD detection performance by incorporating OoD samples during the training phase, and however, OoD detection in FL is largely overlooked. In FL, each client does not have access to a large volume of real OoD samples because it can be costly or even infeasible to obtain such data for resource-constrained devices. As such, an OoD training method for FL that relies on few or even no real OoD examples is strongly desired. Novel to this work, we notice that data from classes out of the local class set, namely external-class data, are natural OoD samples w.r.t. the local data and can serve as OoD surrogate samples in OoD training. As shown in Fig. 2, training w/ external-class data achieves better OoD detection performance than normal training and\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nVOS, since the score of ID and real OoD data is well separated. Besides, compared to the real OoD dataset adopted in prior arts, external-class samples are likely to be nearer to the ID data, since they are sampled from similar feature distributions (refer to (a) and (b) in Fig. 2 ).\n\n(a) Normal training.\n\n(b) Training w/ VOS. Figure 2: The density of negative energy score for OoD detection evaluation using dataset Textures. We use 5 ID classes, and 5 external classes of CIFAR-10.\n\n(c) Training w/ external-class data.\n\n4.2 SYNTHESIZING EXTERNAL-CLASS DATA FROM GLOBAL CLASSIFIER\n\nThough using external-class data as an OoD surrogate is attractive and intuitive, it is not feasible in FL to directly collect them from other non-iid clients, due to privacy concerns and high communication costs on data sharing.\n\nWe thereby propose to generate samples from the desired classes leveraging the encoded class information in the global classifier head. Given the global classifier H : parameterized by Z!Y ✓h to generate samples from specified classes on clients’ demand. As such, we solve the following optimization problem:\n\ng , we utilize a w-governed conditional generative network Gw :\n\nY!Z\n\nmin w\n\nJ(w) := Ey\n\np(y)Ez\n\n⇠\n\nGw(z\n\ny,✏)\n\n|\n\n⇠\n\n`OE(H(z; ✓h\n\ng ), y)\n\n,\n\n(2)\n\nh\n\ni\n\n|\n\ny), we use a Gaussian noise vector ✏\n\nwhere p(y) is the ground-truth prior which is assumed to be a uniform distribution here. We follow the common practice of the generative networks (Zhu et al., 2021) to let y be a one-hot encoding vector, where the target class entry is 1 and others are 0. To encourage the diversity of the generator outputs G(z (0, I) to reparameterize the one-hot encoding vector during the generating process, following the prior practice (Kingma & Welling, 2013). Thus, Gw(z is the global label set. The generator training process can refer to Fig. 1 Step 1. Then for local training (see Fig. 1 Step 2), by downloading the global generator as a substitute of samples given an arbitrary external class set ̄ Y\nsamples as z\n\nQk, each local client indexed by k can generate virtual OoD k. In the feature space, we denote the virtual\n\n(0, I)) given y\n\ny, ✏) given y\n\nGw(y, ✏\n\n, where\n\nY\\Y\n\nk =\n\n⇠N\n\n⇠N\n\n⇠Y\n\ny)\n\nk.\n\n⌘\n\nGw(z\n\nY\n\n✏\n\n|\n\n|\n\n⇠\n\n|\n\n ̄ Y\n\n⇠\n\n4.3 FILTERING VIRTUAL EXTERNAL-CLASS SAMPLES\n\nAlthough synthesized features are intuitively conditioned on external class, the quality of generated OoD samples may vary by iterations likely because of the lack of two properties: (1) Diversity. Like traditional generative models (Srivastava et al., 2017; Thanh-Tung & Tran, 2020), the trained conditional generator may suffer from mode collapse (Mao et al., 2019) in a class, where generator can only produce a small subsets of distribution. As a result, the effective synthesized OoD samples will be mostly unchanged and OoD training will suffer from the lack of diverse samples. (2) Hardness. For a client, its internal and external classes may co-exist with another client, which will enlarge the between-class margins gradually. As the FL training proceeds, the class-conditioned synthesis OoD samples will become increasingly easier to be memorized, namely, overfit by the model. In other words, the hardness of OoD examples declines over time.\n\n(1) Encourage OoD diversity by tail sampling. As mode collapse happens in the high-density area of a class, samples that approximate the class but have larger variance are preferred for higher diversity. For this purpose, we seek to find samples of low but non-zero probability from the distribution of the external classes. Specifically, for each client, we first assume that the set of virtual OoD Nk i=1 forms a class-conditional multivariate representations k is the Gaussian mean of samples from\n\n(0, I) k), where μc\n\nyi ⇠ |\nyk = c) =\n\n ̄ k,✏ Y\n(μc\n\n⇠N k, ⌃c\n\nzki ⇠\n\nyi,✏ )\n\nG(z\n\n{\n\n}\n\n|\n\nGaussian distribution p(zk|\n\nN\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nthe external class set ̄ k is the tied covariance matrix. The parameters of the Y\nclass-conditional Gaussian can be estimated using the empirical mean and variance of the virtual external class samples:\n\nk for client k, and ⌃c\n\nˆμc\n\nk =\n\n1 N c k\n\nzki,\n\nˆ⌃k =\n\n1 Nk\n\ni:yi=c X\n\nc X\n\ni:yi=c X\n\n(zki \n\nˆμc\n\nk) (zki \n\nˆμc\n\nk)T ,\n\n(3)\n\nwhere Nk is the number of samples, and N c set. Then, we select the virtual outliers falling into the ✏-likelihood region as:\n\nk is the number of samples of class c in the virtual OoD\n\nc\n\nk =\n\nvc k|\n\n{\n\nV\n\n\"0 <\n\nexp\n\n⇣\n\n1\n\n2 (vc\n\n1\n\nk (vc\n\nˆμc\n\nk  (2⇡)d/2\n\nk)T ˆ⌃ ˆ⌃k|\n\n|\n\n1/2\n\nk \n\nˆμc\n\nk)\n\n<\", vc\n\nk ⇠\n\nG(\n\n·|\n\n⌘\n\ny = c, ✏)\n\n, }\n\n(4)\n\nwhere \"0 ensures the sample is not totally random, a small \" pushes the generated vc mean of the external class in favor of the sampling diversity.\n\nk away from the\n\nAlgorithm 1 Federated Out-of-Distribution Synthesizer (FOSTER)\n\nK\n\n1: Input: Tasks {\n\nTk}\n\nk=1; Global parameters ✓g, local parameters Global generator parameter w; Learning rate ↵, , local steps T , ID batch size B, OE batch size BOE.\n\n✓k}\n\nk=1;\n\n{\n\nK\n\nServer selects active clients for all user k\n\nin parallel do\n\nA\n\nInitialize local parameters ✓k for t = 1, . . . , T do\n\n✓\n\n2A\n\nB\n\nuniformly at random, then broadcast ✓, w to\n\n.\n\nA\n\n(xi, yi)\n\ni=1 ⇠D\n\nG(z {\nEstimate the multivariate Gaussian distributions based on ZOE by Eq. (3). Filter virtual external class samples according to Eq. (4).\n\nk, ZOE =\n\n(0, I)\n\nyi,✏ )\n\n⇠N\n\nzki\n\nyi\n\n⇠\n\n⇠\n\n}\n\n{\n\n}\n\n|\n\n|\n\nk,✏\n\n ̄ Y\n\nBOE i=1 .\n\n2: repeat 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14:\n\n✓k \n\n✓k \n\nr✓k J(✓k)..\n\nend for Client k sends ✓k back to the server.\n\nOptimize Eq. (5)\n\nend for\n\n1\n\n|A|\n\n✓k.\n\nk\n\n2A\n\nServer updates ✓g for t = 1, . . . , T do w\n\n15: w\n16: end for 17: 18: until training stop\n\n↵\n\nP\n\nrwJ(w)..\n\nOptimize Eq. (2)\n\n(2) Increase the hardness by soft labels. To defend the enlarged margin between internal and external classes, we control the condition inputs to the generator such that generated samples are  to closer to the internal classes. Given an one-hot encoding label vector y of class c, we assign 1 the c-th entry, and a random value within (0, ) to the rest of the positions, where \n\n(0, 0.5).\n\nIn summary, given an observable ˆ\n\nmin ✓k\n\nJ(✓k) :=\n\n|\n\n1 ˆ\nDk| Xxi2\n\nˆ Dk\n\n\"\n\nDk, we formulate the local optimization of FOSTER as: `CE(hk(f (xi; ✓f k), c⇤(xi)) +  `OE(vk)\n\nk); ✓h\n\n1\n\n|Vk| Xvk2Vk\n\n,\n\n(5)\n\n#\n\nand the overall framework of our algorithm is summarized in Algorithm 1. The major difference from FedAvg is that we introduce a generator for OoD outlier synthesis. Since the generator is trained on the server, the computation overhead for the client is marginal, with only the inference of low-dimensional vectors. As compared to VOS, the samples generated from external classes are more likely to approximate the features from real images due to the supervision of real external class prototypes from the classifier head.\n\n2\n\n5 EXPERIMENTS\n\nIn this section, we first introduce the experiment setup and then show empirical results demonstrating the effectiveness of the proposed FOSTER.\n\n6\n\n Published as a conference paper at ICLR 2023\n\nID Datasets for training. We use CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), STL10 (Coates et al., 2011), and DomainNet (Peng et al., 2019) as ID datasets. Both CIFAR-10 and CIFAR100 are large datasets containing 50, 000 training images and 10, 000 test images. Compared with CIFAR, STL10 is a small dataset consisting of only 5,000 training images and 8,000 test images. DomainNet are consist of images from 6 different domains. We use DomainNet to explore how FOSTER performs in the case of feature non-iid among different clients.\n\nOoD Datasets for evaluation. We use Textures (Cimpoi et al., 2014), Places365 (Zhou et al., 2017), LSUNC (Yu et al., 2015), LSUN-Resize (Yu et al., 2015) and iSUN (Xu et al., 2015) as the OoD datasets for evaluation. When ID dataset is CIFAR10, we also evaluate on CIFAR-100 to check near-OoD detection performance, since CIFAR-10 and CIFAR100 datasets have similarities, although their classes are disjoint.\n\n(a) CIFAR-10.\n\n(b) CIFAR-100.\n\nFigure 3: Visualization of generated external class samples and ID samples.\n\nBaselines. We compare the proposed FOSTER with both Post hoc and Virtual synthetic OoD detection methods that have been mentioned in Section 2: a) Post hoc OoD detection methods: Energy score (Liu et al., 2020), MSP (Hendrycks & Gimpel, 2016), ODIN (Liang et al., 2017). b) Synthetic OoD detection method: VOS (Du et al., 2022). For a fair comparison, the training method of FL for all of the above approaches including the proposed FOSTER is FedAvg (McMahan et al., 2017) with a personalized classifier head, and we note that our framework can be extended to other FL variants. All the approaches only use ID data without any auxiliary OoD dataset for training.\n\nMetrics for OoD detection and classification. To evaluate the classification performance on ID samples, we report the test accuracy (Acc) for each client’s individual test sets, whose classes match their training sets. For OoD detection performance, we report the area under the receiver operating characteristic curve (AUROC), and the area under the PR curve (AUPR) for ID and OoD classification. In FL setting, all three metrics are the mean results of all the clients.\n\nHeterogeneous federated learning. For CIFAR-10 and CIFAR-100, the total client number is 100, for STL10, the total client number is 50. For DomainNet, the total client number is 12 (2 for each domain). To model class non-iid data of the training datasets, we follow a uniform partition mode and assign partial classes to each client. We distribute 3 classes per client for CIFAR-10 and STL10, 5 classes for DomainNet, and 10 classes for CIFAR-100, unless otherwise mentioned.\n\n5.1 VISUALIZATION OF GENERATED EXTERNAL CLASS SAMPLES\n\nIn Fig. 3, we visualize the generated external class samples and ID samples of a client using TSNE for both CIFAR-10 and CIFAR-100. Without accessing the raw external-class data from the other users, our generator, trained merely from the shared classifier head, yields samples that are strictly out of the local distribution without any overlap. We also obtain a consistent conclusion from CIFAR-100, which has as many as 90 external classes. The enormous external classes diversify the OoD set and therefore we observe a larger gain of OoD detection accuracy (a 2.9% AUROC increase versus the best baseline) compared to other benchmarks in Table 1. The observation also motivates our design of the tail sampling to encourage diversity.\n\n5.2 BENCHMARK RESULTS\n\nFOSTER outperforms existing methods. We compare FOSTER with other competitive baselines in Table 1. The proposed FOSTER shows stronger OoD detection performance on all three training sets, while preserving a high test accuracy. VOS is another regularization method using virtual OoD samples, which even shows worse results than post hoc methods. The virtual OoD data synthesized by VOS is based on a large amount of ID samples. For the FL setting, when data stored in each device is limited, these synthesized OoD samples based on ID data will no longer be effective, which\n\n7\n\nPublished as a conference paper at ICLR 2023\n\ndeteriorates the OoD detection performance. For FOSTER, the virtual OoD samples are based on the external class knowledge extracted from other clients, which are close to real OoD samples. Thus, they are effective in improving the OoD detection performance while preserving the test accuracy.\n\nID dataset\n\nCIFAR-10\n\nCIFAR-100\n\nSTL10\n\nMethod Energy MSP ODIN VOS FOSTER Energy MSP ODIN VOS FOSTER Energy MSP ODIN VOS FOSTER\n\nAcc \"\n0.9431 0.9431 0.9431 0.9426 0.9432 0.8129 0.8129 0.8129 0.8063 0.8218 0.8236 0.8236 0.8236 0.8264 0.8410\n\n\"\n\nAUROC 0.7810 0.8829 0.8842 0.7970 0.9091 0.8056 0.8606 0.8657 0.8372 0.8945 0.7529 0.7410 0.7418 0.7370 0.7671\n\nAUPR \"\n0.9262 0.9691 0.9689 0.9342 0.9785 0.9575 0.9782 0.9789 0.9666 0.9838 0.9228 0.9309 0.9306 0.9126 0.9425\n\nTable 1: Our FOSTER outperforms competitive baselines. numbers are best performers.\n\n\"\n\nindicates larger value is better. Bold\n\nNear OoD detection. We evaluate the model training with CIFAR10 datasets on both near OoD (CIFAR100) and far OoD datasets. The results are shown in Table 2, and the best results are highlighted. The proposed FOSTER outperforms baselines for all of the evaluation OoD datasets, especially the near OoD dataset CIFAR100. By synthesizing virtual external class samples, FOSTER has access to virtual near OoD samples during training, which is also an advantage of FOSTER over other baselines.\n\nDatasets\n\nEnergy MSP ODIN VOS FOSTER\n\nTextures\n\nLSUN-C\n\nPlaces365\n\nLSUN-Resize AUROC AUPR AUROC AUPR AUROC AUPR AUROC AUPR 0.7080 0.8868 0.8221 0.9411 0.7009 0.9065 0.8376 0.9519 0.8107 0.9375 0.8964 0.9754 0.9043 0.9774 0.9154 0.9825 0.8124 0.9367 0.8976 0.9752 0.9062 0.9773 0.9166 0.9825 0.7346 0.8993 0.8267 0.9447 0.7270 0.9196 0.8451 0.9541 0.8458 0.9544 0.9253 0.9842 0.9332 0.9863 0.9316 0.9870\n\niSUN\n\nCIFAR-100\n\nAUROC AUPR AUROC AUPR 0.8289 0.9462 0.7883 0.9248 0.9103 0.9805 0.8604 0.9615 0.9114 0.9805 0.8614 0.9613 0.8397 0.9499 0.8086 0.9379 0.9238 0.9849 0.8952 0.9742\n\nTable 2: Near and far OoD detection for CIFAR10. The proposed FOSTER outperforms baselines for all of the evaluation OoD datasets, especially near OoD dataset CIFAR100.\n\nOoD detection for feature non-iid clients. We explore whether our FOSTER can still work well when feature non-iid also exists among different clients on DomainNet. Under this problem setting, different clients not only have different classes, but may also come from different domains. According to the results shown in Table 3, although the results are not that significant compared with feature iid settings, FOSTER still outperforms the baselines. For feature non-iid settings, the external class knowledge extracted from clients from different domains is not so consistent compared with feature iid cases. However, our experimental results also show that in this case, there is still some invariant external class information across different domains that can be extracted by our FOSTER to help improve the OoD detection performance.\n\nTable 3: Our FOSTER outperforms competitive baselines under feature non-iid setting.\n\nAUROC 0.6745 0.6871 0.6871 0.6796 0.6960\n\nAUPR \"\n0.8953 0.9048 0.9047 0.8988 0.9075\n\nMethod Energy MSP ODIN VOS FOSTER\n\nAcc \"\n0.7237 0.7237 0.7237 0.7340 0.7348\n\n\"\n\n5.3 QUALITATIVE STUDIES\n\nEffects of active client number. We investigate the effects of active client number on CIFAR-10. The number of clients is fixed to be 100, while the number of active clients is set to be 20, 50 and 100, respectively. According to the results in Table 4, FOSTER shows better OoD detection performance than baselines in all cases of active users. With the increase of active clients, the OoD performance of FOSTER remains stable, which means our proposed FOSTER is not sensitive to the number of active users.\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nActive num\n\nMethod Acc\n\nAUROC\n\nAUPR\n\n\"\n\n\"\n\n\"\n\nClasses / client\n\nMethod Acc\n\nAUROC\n\nAUPR\n\n\"\n\n\"\n\n\"\n\n20\n\n50\n\n100\n\n0.9399 Energy MSP 0.9399 ODIN 0.9399 VOS 0.9410 FOSTER 0.9401 0.9432 Energy MSP 0.9432 ODIN 0.9432 0.9430 VOS FOSTER 0.9429 0.9431 Energy MSP 0.9431 ODIN 0.9431 VOS 0.9426 FOSTER 0.9432\n\n0.7760 0.8560 0.8562 0.7545 0.9011 0.7592 0.8869 0.8879 0.7946 0.8947 0.7810 0.8829 0.8842 0.7970 0.9091\n\n0.9363 0.9674 0.9674 0.9173 0.9776 0.9185 0.9728 0.9727 0.9311 0.9750 0.9262 0.9691 0.9689 0.9342 0.9785\n\n10\n\n5\n\n3\n\n0.8129 Energy MSP 0.8129 ODIN 0.8129 0.8063 VOS FOSTER 0.8218 0.8976 Energy MSP 0.8976 ODIN 0.8976 0.8974 VOS FOSTER 0.8981 0.9383 Energy MSP 0.9383 ODIN 0.9383 VOS 0.9393 FOSTER 0.9397\n\n0.8056 0.8606 0.8657 0.8372 0.8945 0.7735 0.8776 0.8831 0.7927 0.9081 0.7215 0.8682 0.8723 0.7636 0.8865\n\n0.9575 0.9782 0.9789 0.9666 0.9838 0.9157 0.9704 0.9714 0.9289 0.9778 0.8684 0.9586 0.9592 0.8990 0.9697\n\nTable 4: Ablation study on the number of active clients:FOSTER is not sensitive to the number of active users.\n\nTable 5: Ablation study on ID class number: the advantage of the proposed FOSTER over other baselines is not affected by the number of ID class number.\n\nEffects of ID class number. We investigate the effects of ID class number on CIFAR-100. We set the classes distributed per client (classes / client) to be 10, 5 and 3, respectively. According to the results in Table 5, the advantage of the proposed FOSTER over other competitive baselines is not affected by the number of ID classes. When the number of ID classes decreases, for FOSTER the maximum changes in AUROC and AUPR are 2.16% and 0.81%, respectively. VOS, as another virtual synthetic OoD detection method, with the decrease of ID classes, AUROC and AUPR drop by 7.36% and 6.76%, respectively, which is a much larger variation compared with our method. Thus, the ID class number has a large impact on VOS, while almost has no effect on FOSTER. Effects of the p.d.f. filter. We report the effects of the p.d.f. filter as mentioned in Section 4.3 on CIFAR-10 in Table 6. The generator without a p.d.f. filter is outperformed by baselines. The phenomenon occurs because not all generated external class samples are of high quality, and some of them may even deteriorate OoD detection performance. Since we add Gaussian noise during the process, some randomly generated external class samples might overlap with ID samples. Thus, we build a class-condition Gaussian distribution for external classes, and adopt a p.d.f. filter to select diverse virtual OoD samples which do not overlap with the ID clusters. According to this table, filtering out low-quality OoD samples improves AUROC and AUPR by 4.44% and 1.18%, respectively.\n\nMethod Energy MSP ODIN VOS FOSTER w/o pdf filter FOSTER w/ pdf filter\n\nAcc \"\n0.9431 0.9431 0.9431 0.9426 0.9425 0.9432\n\n\"\n\nAUROC 0.7810 0.8829 0.8842 0.7970 0.8647 0.9091\n\n\"\n\nAUPR 0.9262 0.9691 0.9689 0.9342 0.9667 0.9785\n\nMethod FOSTER FOSTER w/ soft label\n\nAcc \"\n0.8410 0.8294\n\nAUROC 0.7671 0.7872\n\n\"\n\n\"\n\nAUPR 0.9425 0.9501\n\nTable 7: Ablation study on random soft labels: soft label strategy increase the hardness of generated virtual OoD samples.\n\nTable 6: Ablation study on pdf filter: pdf filter plays an effective role in selecting diverse, highquality virtual OoD samples. Effects of the random soft label strategy. We study the effects of the random soft label strategy on STL10, and set  = 0.2. As shown in Table 7, after replacing the one-hot label with the random soft label as the input for the generator, we improve AUROC and AUPR by 2.01% and 0.75% respectively, while preserving a similar ID classification test accuracy. That is because soft label contains knowledge from ID classes make the generated external class samples closer to ID samples.\n\n6 CONCLUSION\n\nIn this paper, we study a largely overlooked problem: OoD detection in FL. To turn the curse of heterogeneity in FL into a blessing that facilitates OoD detection, we propose a novel OoD synthesizer without relying on any real external samples, allowing a client to learn external class knowledge from other non-iid federated collaborators in a privacy-preserving manner. Empirical results showed that the proposed approach achieves state-of-the-art performance in non-iid FL.\n\n9\n\nPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThis material is based in part upon work supported by the National Science Foundation under Grant IIS-2212174, IIS-1749940, Office of Naval Research N00014-20-1-2382, and National Institute on Aging (NIA) RF1AG072449. The work of Z. Wang is in part supported by the National Science Foundation under Grant IIS-2212176.\n\nREFERENCES\n\nManoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Feder-\n\nated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.\n\nDebraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd with quantization, sparsification and local computations. Advances in Neural Information Processing Systems, 32, 2019.\n\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3606–3613, 2014.\n\nAdam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215–223. JMLR Workshop and Conference Proceedings, 2011.\n\nEnmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient\n\nfederated learning for heterogeneous clients. arXiv preprint arXiv:2010.01264, 2020.\n\nXuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don’t know by virtual\n\noutlier synthesis. arXiv preprint arXiv:2202.01197, 2022.\n\nMatej Grci ́c, Petra Bevandi ́c, and Siniša Šegvi ́c. Dense open-set recognition with synthetic outliers\n\ngenerated by real nvp. arXiv preprint arXiv:2011.11094, 2020.\n\nMatthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why ReLU networks yield highconfidence predictions far away from the training data and how to mitigate the problem. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 41–50, 2019.\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\n\nexamples in neural networks. arXiv preprint arXiv:1610.02136, 2016.\n\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier\n\nexposure. arXiv preprint arXiv:1812.04606, 2018.\n\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data\n\ndistribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.\n\nSanghun Jung, Jungsoo Lee, Daehoon Gwak, Sungha Choi, and Jaegul Choo. Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 15425–15434, 2021.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning, 14(1–2):1–210, 2021.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.\n\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes.\n\narXiv preprint\n\narXiv:1312.6114, 2013.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.\n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,\n\nmethods, and future directions. IEEE Signal Processing Magazine, 37(3):50–60, 2020a.\n\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems, 2:429–450, 2020b.\n\nXiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning\n\non non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021.\n\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution\n\nimage detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nZiqian Lin, Sreya Dutta Roy, and Yixuan Li. Mood: Multi-level out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15313–15323, 2021.\n\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection.\n\nAdvances in Neural Information Processing Systems, 33:21464–21475, 2020.\n\nQi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking generative adversarial networks for diverse image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1429–1437, 2019.\n\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 1273–1282. PMLR, 2017.\n\nSina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizable out-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5216–5223, 2020.\n\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1406–1415, 2019.\n\nZhaonan Qu, Kaixiang Lin, Jayant Kalagnanam, Zhaojian Li, Jiayu Zhou, and Zhengyuan Zhou. Federated learning’s blessing: Fedavg has linear speedup. arXiv preprint arXiv:2007.05690, 2020.\n\nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.\n\nVikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised\n\noutlier detection. arXiv preprint arXiv:2103.12051, 2021.\n\nAkash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan: Reducing mode collapse in gans using implicit variational learning. Advances in neural information processing systems, 30, 2017.\n\nYiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest\n\nneighbors. arXiv preprint arXiv:2204.06507, 2022.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nJihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learning on distributionally shifted instances. Advances in neural information processing systems, 33:11839–11852, 2020.\n\nHoang Thanh-Tung and Truyen Tran. Catastrophic forgetting and mode collapse in gans. In 2020\n\nInternational Joint Conference on Neural Networks (IJCNN), pp. 1–10. IEEE, 2020.\n\nHaoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtuallogit matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4921–4930, 2022.\n\nJianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of\n\ncommunication-efficient sgd algorithms. arXiv preprint arXiv:1808.07576, 2018.\n\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam, Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.\n\nZhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score for variational auto-encoder. Advances in neural information processing systems, 33:20685–20696, 2020.\n\nPingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint arXiv:1504.06755, 2015.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nTao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov. Salvaging federated learning by local adaptation.\n\narXiv preprint arXiv:2002.04758, 2020.\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,\n\n2016.\n\nJingyang Zhang, Nathan Inkawhich, Yiran Chen, and Hai Li. Fine-grained out-of-distribution\n\ndetection with mixup outlier exposure. arXiv preprint arXiv:2106.03917, 2021.\n\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017.\n\nZhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In International Conference on Machine Learning, pp. 12878–12889. PMLR, 2021.\n\n12",
  "translations": [
    "# Summary Of The Paper\n\nIn this paper, the authors study a largely overlooked problem: OOD detection in FL. To turn the curse of heterogeneity in FL into a blessing that facilitates OOD detection, the authors propose a novel OOD synthesizer without relying on any real external samples, allowing a client class knowledge from other non-iid federated collaborators in a privacy-preserving manner. Empirical results showed that the proposed approach achieves SOTA performance in non-iid FL.\n\nFrom the view of the problem setting, this paper studies a very important problem, which contains enough significance in the field of FL and OOD detection. From the technical part, the authors propose a novel federated OOD synthesizer to take advantage of data heterogeneity to facilitate OOD detection in FL, allowing a client to learn external class knowledge from other non-iid federated collaborators in a privacy-aware manner. This work bridges a critical research gap since OOD detection for FL is currently not yet well-studied in literature. The proposed FOSTER is the first OOD learning method for FL that does not require real OOD samples.\n\n# Strength And Weaknesses\n\nPros:\n\n1.  The problem setting is very important, filling up a gap between FL and OOD detection. This study is significant in the fields of FL and OOD detection.\n\n2. From the technical part, the authors propose a novel federated OOD synthesizer to take advantage of data heterogeneity to facilitate OOD detection in FL, allowing a client to learn external class knowledge from other non-iid federated collaborators in a privacy-aware manner. This work bridges a critical research gap since OOD detection for FL is currently not yet well-studied in literature. The proposed FOSTER is the first OOD learning method for FL that does not require real OOD samples. Note that, it is not trivial to directly use the OOD techniques in FL, which is the major technical contribution of this paper.\n\n3. Experiments cover many aspects regarding this paper (like CIFAR10/100 and near OOD detection).\n\nCons:\n\n1. The problem setting should be presented explicitly. What the data you have and what the aim this paper wants to do should be demonstrated in a separated paragraph or subsection.\n\n2. The generated OOD data is somehow different from the true OOD data. Relevant discussions are needed.\n\n3. What is the validation dataset used to find the best hyperparameters of your method? Validation datasets are very important to the OOD detection. We cannot select validation datasets containing OOD data. \n\n4. ImageNet benchmark should be used to verify the effectiveness of the proposed method.\n\n5. To ensure the completeness of the ablation study, performance of Foster w/o pdf and soft label should be reported.\n\n6. In section 3, it is wired to say that a sample belongs to (using \\in) a distribution. For example, x \\in D. It should be x ~ D.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThis paper is clearly demonstrated. The quality and novelty are enough in terms of significance and technical contributions. This paper can be reproducible based on the algorithm provided.\n\n# Summary Of The Review\n\nFrom the view of the problem setting, this paper studies a very important problem, which contains enough significance in the field of FL and OOD detection. From the technical part, the authors propose a novel federated OOD synthesizer to take advantage of data heterogeneity to facilitate OOD detection in FL, allowing a client to learn external class knowledge from other non-iid federated collaborators in a privacy-aware manner. This work bridges a critical research gap since OOD detection for FL is currently not yet well-studied in literature. The proposed FOSTER is the first OOD learning method for FL that does not require real OOD samples.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThis paper presents a novel approach called the Federated Out-of-Distribution Synthesizer (FOSTER) to tackle the challenge of out-of-distribution (OoD) detection in federated learning (FL). The authors highlight the difficulties of obtaining real OoD samples in FL environments characterized by heterogeneous client data. FOSTER leverages this heterogeneity to synthesize virtual OoD samples, which are used to improve the robustness of classifiers against misclassifications. Experimental results demonstrate that FOSTER outperforms existing state-of-the-art methods on benchmark datasets such as CIFAR-10, CIFAR-100, and STL10.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to synthesizing OoD samples without the need for real data, addressing a significant gap in the current literature on OoD detection in FL. The methodology is well-structured, with a clear explanation of how the synthesis process works and how it preserves privacy. However, a potential weakness is that the paper could benefit from a more extensive evaluation of the method across a wider range of datasets and settings to fully establish its robustness and generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-organized, making it easy for readers to follow the authors' rationale and methodology. The quality of writing is high, with precise terminology and a coherent flow of ideas. The novelty of the approach is significant, as it presents a new way to utilize client heterogeneity in FL for OoD detection. Reproducibility is facilitated by the detailed description of the FOSTER algorithm, although the authors could enhance this aspect by providing access to code or datasets used in their experiments.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning and out-of-distribution detection by introducing FOSTER, a method that effectively synthesizes virtual OoD samples. The experimental results are promising, showcasing FOSTER's superiority over existing methods. However, further validation across diverse datasets would strengthen the claims made.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to out-of-distribution (OoD) detection in federated learning (FL) through the introduction of the Federated Out-of-Distribution Synthesizer (FOSTER). The methodology capitalizes on the inherent data heterogeneity of clients in FL, utilizing external-class data to synthesize virtual OoD samples while preserving privacy. Experimental results reveal that FOSTER significantly outperforms state-of-the-art methods in OoD detection across various datasets, including CIFAR-10 and DomainNet, while maintaining high classification accuracy for in-distribution samples.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach, which effectively transforms the challenge of data heterogeneity into a beneficial factor for OoD detection. The ability to synthesize OoD samples without sharing raw data maintains the privacy of clients, which is paramount in FL. Furthermore, the experimental design is robust, with clear comparisons to existing techniques and demonstrated improvements in key metrics such as AUROC and AUPR. However, the paper also has notable weaknesses, including a reliance on the availability and quality of external-class data, which may not always be diverse or representative. Additionally, the computational overhead associated with synthesizing virtual samples could pose challenges for very resource-constrained clients, and the variability in the quality of synthetic samples may affect performance if not properly managed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making the complex concepts accessible to the reader. The quality of writing is high, with a logical flow that guides the reader through the problem statement, methodology, and results. The novelty of the approach is significant, as it introduces a new way to leverage the challenges of FL for a critical application in OoD detection. Regarding reproducibility, the methodology is sufficiently detailed, allowing for potential implementation by other researchers.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of federated learning and out-of-distribution detection through its innovative use of data heterogeneity. While it presents strong empirical results and a clear methodology, it also highlights challenges related to data dependency and computational costs that require further consideration.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper titled \"Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection\" by Shuyang Yu et al. introduces the Federated Out-of-Distribution Synthesizer (FOSTER), a novel methodology designed to address the challenges of out-of-distribution (OoD) detection in federated learning (FL) scenarios characterized by data heterogeneity. The key innovation of this work lies in synthesizing virtual external-class OoD samples from non-iid federated data without requiring the sharing of raw data, thus maintaining privacy. The methodology employs a class-conditional generator that utilizes the global classifier to generate synthetic samples while ensuring diversity and quality. Experimental results demonstrate that FOSTER outperforms state-of-the-art methods in various datasets, achieving improvements in classification accuracy and OoD detection metrics.\n\n# Strength And Weaknesses\nThe primary strength of the paper is its innovative approach to leveraging data heterogeneity in federated learning for OoD detection, which is often seen as a challenge rather than an asset. The proposed method is theoretically well-founded, with a clear description of the loss functions and training procedures, enhancing its reproducibility. Additionally, the empirical results show significant improvements over existing methods, indicating the practical applicability of the proposed technique. However, one potential weakness is the reliance on the quality of the global classifier, which may affect the performance of the synthetic sample generation. The paper could also benefit from a more extensive discussion on the limitations and potential biases introduced by the synthetic sampling process.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured sections and a logical flow that facilitates understanding of the methodology and results. The mathematical formulations are clearly presented, allowing for easy comprehension of the proposed approach. The novelty of the work is significant, as it presents a fresh perspective on utilizing federated learning's inherent data heterogeneity for OoD detection, a topic that has not been extensively addressed in the literature. The reproducibility of the experiments is supported by detailed descriptions of the datasets, metrics, and experimental settings, although providing access to code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of federated learning and out-of-distribution detection by introducing FOSTER, a method that effectively turns data heterogeneity into an advantage. The theoretical foundations and empirical validations are solid, although some discussion on limitations would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FOSTER, a novel framework designed to enhance out-of-distribution (OoD) detection in federated learning (FL) by leveraging data heterogeneity. FOSTER operates without the need for real OoD samples, instead utilizing synthetic data to simulate these scenarios, thus addressing the challenges of data acquisition in real-world applications. The experimental results demonstrate that FOSTER significantly outperforms existing state-of-the-art methods across multiple datasets, showcasing improvements in metrics such as AUROC and ID accuracy, particularly in near OoD detection contexts.\n\n# Strength And Weaknesses\nFOSTER's main strengths lie in its innovative approach to managing data heterogeneity, which allows for effective OoD detection without compromising privacy. The ability to generate synthetic OoD samples is a notable advantage, as it circumvents the challenges associated with obtaining real data. However, the reliance on synthetic data raises concerns regarding the representativeness of these samples, potentially leading to performance issues in real-world applications. While the framework shows robust performance across various metrics, it lacks an in-depth analysis of conditions where it might underperform compared to conventional methods. Additionally, the paper could benefit from broader testing across diverse real-world applications to strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, presenting a clear problem formulation and a comprehensive evaluation of the proposed method. The inclusion of ablation studies is a valuable aspect that enhances the understanding of FOSTER's components. However, the theoretical foundations could be elaborated upon to bolster the claims made. The methodology is described in sufficient detail, allowing for reproducibility, although further insights into the impact of varying parameters would be beneficial.\n\n# Summary Of The Review\nFOSTER introduces a novel approach to OoD detection in federated learning, demonstrating high performance while ensuring data privacy. Despite its strengths, the reliance on synthetic data and the lack of extensive real-world validation raise concerns about the generalizability and robustness of the framework.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the challenge of out-of-distribution (OoD) detection in federated learning (FL) settings by proposing a novel methodology named Federated Out-of-Distribution Synthesizer (FOSTER). FOSTER synthesizes virtual OoD samples using class-conditional generative models trained on heterogeneous data from multiple clients, thereby reducing reliance on real OoD samples. The authors empirically validate their approach, demonstrating that FOSTER significantly outperforms existing methods on several benchmark datasets, achieving enhancements in area under the receiver operating characteristic (AUROC) scores and in-distribution (ID) accuracy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to leveraging data heterogeneity in FL as a resource for generating OoD samples, thereby addressing a common limitation of existing methods that require large amounts of real OoD data. The methodological framework is well-structured, ensuring privacy preservation while enhancing OoD detection capabilities. However, a notable weakness is the dependency on the quality of the generative model; if the generative model fails to accurately capture external class distributions, the effectiveness of FOSTER may be compromised, particularly in environments with significant data skew.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and presents a logical flow from problem statement to methodology and empirical results. The quality of the experimental validation is commendable, with a thorough comparison against various benchmarks, although the potential limitations related to the generative model could be more explicitly addressed. The novelty of the approach is significant, as it reframes data heterogeneity from a challenge into an advantage for OoD detection. The reproducibility of the results is facilitated by the detailed methodology, although access to the generative model implementation details would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a compelling and innovative approach to addressing OoD detection within federated learning by synthesizing virtual samples from heterogeneous client data. While the methodology shows great promise, careful consideration of the generative model's quality is essential to fully realize its potential in diverse data environments.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper presents a novel framework, Federated Adversarial Training Synthesizer (FATS), designed to enhance model robustness against adversarial attacks in the context of federated learning (FL). The authors capitalize on the heterogeneous and non-iid nature of client data to collaboratively generate synthetic adversarial examples without sharing private data directly. Their empirical results demonstrate that FATS significantly outperforms existing adversarial training methods in both centralized and federated settings, leading to improved accuracy and robustness metrics across various datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its timely approach to addressing the critical intersection of federated learning and adversarial robustness. By transforming the inherent challenges of heterogeneous data into a strategic advantage, the authors contribute valuable insights to the field. The extensive empirical evaluations effectively illustrate the proposed method's superiority over baseline techniques. However, the paper could enhance its theoretical foundation regarding the generation of synthetic adversarial examples and should expand its experimental scope to include a wider variety of datasets and adversarial strategies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, although some aspects of the theoretical framework could benefit from more detailed explanations. The quality of the writing is high, but additional context on the computational efficiency of the proposed method would enhance clarity, particularly in light of the resource constraints often faced in federated learning. The novelty of the approach is significant, as it introduces a unique method for synthesizing adversarial data while ensuring privacy. However, the reproducibility of the results could be improved with more detailed descriptions of the experimental setup and the specific datasets used.\n\n# Summary Of The Review\nThis paper makes a noteworthy contribution to federated learning by proposing a novel method for adversarial training that effectively utilizes heterogeneous data. While the concept is innovative and the empirical results are compelling, the paper would benefit from clearer theoretical justification and a broader range of experimental evaluations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the Federated Out-of-Distribution Synthesizer (FOSTER), a novel method designed to enhance out-of-distribution (OoD) detection in federated learning (FL) without requiring real OoD samples. FOSTER leverages data heterogeneity as an advantage, claiming to significantly improve performance metrics such as AUROC by up to 2.49% across various datasets. The authors assert that their method synthesizes class-conditional virtual OoD samples effectively, preserves privacy, and maintains high communication efficiency, while also providing extensive empirical validation of their claims.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to synthesizing OoD samples within the federated learning framework, which is a timely and relevant topic. However, the claims of substantial performance improvements appear to be overstated, as the actual enhancements over existing methods are relatively minor. While the methodology is presented as a comprehensive solution, it lacks sufficient empirical evidence to support its broad applicability across diverse scenarios. Additionally, the communication efficiency benefits are comparable to existing methods, which detracts from the significance of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly presents the proposed methodology and experimental results. However, the claims regarding improvements and innovations may lead to misinterpretations due to the inflated presentation of results. The reproducibility of the work is not adequately addressed, as the details surrounding the implementation and specific experimental conditions are insufficiently detailed, which may hinder other researchers from replicating the findings effectively.\n\n# Summary Of The Review\nWhile the paper proposes a novel framework (FOSTER) for improving OoD detection in federated learning, the actual contributions and performance enhancements are modest compared to existing methods. The claims made regarding the significance and effectiveness of the approach lack sufficient empirical support and may exaggerate the practical impacts of the methodology.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection\" proposes a novel method called Federated Out-of-Distribution Synthesizer (FOSTER) aimed at addressing the challenge of out-of-distribution (OoD) detection in federated learning (FL) settings, particularly when real OoD samples are scarce or unavailable. The methodology involves synthesizing virtual OoD samples from heterogeneous, non-iid data among clients using class-conditional generators and enhancing sample diversity through Gaussian noise. The experimental results demonstrate that FOSTER outperforms existing state-of-the-art methods across several datasets, including CIFAR-10, CIFAR-100, and STL10, with notable improvements in AUROC and ID accuracy metrics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to leveraging data heterogeneity in FL for OoD detection, which is a significant advancement given the limitations of traditional methods that depend on real OoD samples. The proposed FOSTER framework shows robust performance across different datasets and configurations, indicating its applicability in real-world scenarios. However, a potential weakness is the reliance on synthetic data, which, while necessary given the constraints of real data, may raise questions about the generalizability of the results across different domains or more complex scenarios. Furthermore, the paper could benefit from a more detailed analysis of the computational efficiency and scalability of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology, making it accessible to readers. The quality of the writing is high, with a logical flow of ideas supported by empirical results. The novelty of the approach is significant, as it reframes a common challenge in FL into an opportunity for improved detection capabilities. In terms of reproducibility, the authors provide sufficient detail about the experimental setup and benchmarks, although additional information on the implementation specifics would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of federated learning and out-of-distribution detection through its innovative FOSTER framework. It demonstrates strong empirical results and offers a new perspective on utilizing data heterogeneity, although some concerns regarding the generalizability of synthetic samples could be explored further.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper presents a novel approach to out-of-distribution (OoD) detection in federated learning (FL) by leveraging data heterogeneity. The authors introduce a method called FOSTER, which synthesizes virtual samples from non-iid clients, positing that these samples can effectively substitute for real OoD data. The primary findings suggest that FOSTER improves OoD detection performance while maintaining privacy and communication efficiency. The methodology is evaluated using metrics like AUROC and AUPR, and the paper claims that the synthesized samples can capture the characteristics of genuine OoD data without requiring actual OoD samples.\n\n# Strength And Weaknesses\nThe paper's strength lies in its innovative perspective on the utility of heterogeneous data in FL, challenging prevailing assumptions that such heterogeneity is detrimental. The synthesis of virtual samples as a substitute for real OoD data is a novel contribution that opens up new avenues for research. However, the paper raises several critical questions regarding the assumptions made, such as the alignment of external class knowledge with OoD detection tasks, the effectiveness of synthesized samples, and the generalizability of the proposed method across different datasets. The lack of empirical evidence supporting some of these claims limits the strength of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but the clarity of some assumptions and methodologies could be improved. The novelty of leveraging data heterogeneity in the context of OoD detection is significant, though the reproducibility is questionable due to insufficient details on the synthesis process and evaluation metrics. The paper could benefit from a more comprehensive discussion of baseline comparisons and the robustness of the privacy-preserving mechanisms employed.\n\n# Summary Of The Review\nOverall, the paper presents a fresh approach to OoD detection in federated learning by utilizing data heterogeneity, yet it falls short in providing robust empirical evidence to fully support its claims. While the novelty is commendable, the assumptions made regarding the effectiveness of synthesized samples and the generalizability of results warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents FOSTER (Federated Out-of-Distribution Synthesizer), a novel method for out-of-distribution (OoD) detection in federated learning (FL) contexts, capitalizing on data heterogeneity among clients. The authors propose a technique that synthesizes virtual OoD samples using non-iid data from clients, thus addressing the challenge of requiring extensive real OoD samples in traditional methods. The empirical results demonstrate that FOSTER outperforms existing baseline methods in both OoD detection and classification accuracy across various datasets, indicating its effectiveness in a federated learning environment.\n\n# Strength And Weaknesses\nStrengths of the paper include the innovative approach of synthesizing OoD samples from non-iid data, which is particularly relevant in the context of federated learning where real OoD data is scarce. The privacy-preserving aspect of the method is a significant advantage, as it aligns well with the principles of federated learning. However, one potential weakness is the reliance on the quality of the generated synthetic samples, which may vary based on the diversity of the non-iid data available among clients. Furthermore, the paper could benefit from a more extensive discussion of the limitations and potential edge cases of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The methodology is described in sufficient detail, allowing for reproducibility, although additional details on the training procedures and hyperparameter settings would enhance this aspect. The novelty of synthesizing OoD samples in a federated learning context is significant and presents a fresh perspective on OoD detection, which is often overlooked in current literature.\n\n# Summary Of The Review\nOverall, the paper makes a commendable contribution to the field of federated learning and out-of-distribution detection by proposing a novel method that effectively utilizes data heterogeneity. The empirical results support the claims made by the authors, although further exploration of the method's limitations would strengthen the paper.\n\n# Correctness\n5/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing the robustness of machine learning models against adversarial attacks through the introduction of a new adversarial training framework. The authors propose an innovative methodology that integrates a multi-faceted loss function designed to increase the model's generalization capabilities. Experimental results demonstrate that their approach significantly outperforms existing adversarial training techniques on various benchmark datasets, leading to improved model accuracy and resilience against adversarial examples.\n\n# Strength And Weaknesses\n**Strengths**:\n- **Innovative Approach**: The proposed adversarial training framework introduces a unique loss function that combines multiple objectives, which could potentially lead to more robust models.\n- **Theoretical Insights**: The paper provides a solid theoretical justification for the proposed methodology, helping to clarify how the different components interact to enhance robustness.\n- **Comprehensive Evaluation**: The authors conduct extensive experiments on multiple datasets, showcasing the effectiveness of their method against a wide range of adversarial attacks.\n\n**Weaknesses**:\n- **Clarity of Presentation**: Some sections, particularly those detailing the theoretical derivation of the loss function, could benefit from improved clarity and organization to aid reader comprehension.\n- **Limited Comparison**: Although the authors compare their method with several existing techniques, additional comparisons with state-of-the-art methods in the field would strengthen their claims.\n- **Scalability Concerns**: The paper does not adequately address how the proposed approach would perform in larger-scale applications or in real-world scenarios, which raises questions about its practicality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe overall clarity of the paper is moderate, with some sections requiring more detailed explanations to ensure that readers can fully understand the proposed ideas. The quality of the methodology is high, with a well-structured approach to both theoretical and empirical evaluations. The novelty of the proposed method is significant, as it introduces a fresh perspective on adversarial training. However, reproducibility may be hindered due to insufficient details regarding implementation specifics and evaluation metrics.\n\n# Summary Of The Review\nThe paper offers a promising contribution to the field of adversarial machine learning by introducing an innovative adversarial training framework that demonstrates improved robustness against attacks. While the theoretical foundations and empirical results are strong, the paper could benefit from enhanced clarity and broader comparisons with existing state-of-the-art methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper addresses the challenge of out-of-distribution (OoD) detection in federated learning (FL) environments, where traditional methods often fail due to the lack of available real OoD samples and the inherent data heterogeneity among clients. The authors propose the Federated Out-of-Distribution Synthesizer (FOSTER), a novel framework that synthesizes virtual external-class OoD samples from the non-iid data of other clients, thus circumventing the need for real OoD samples. FOSTER not only achieves state-of-the-art performance with limited in-distribution data but also enhances the diversity and difficulty of the synthesized samples, effectively leveraging the heterogeneity of the data in FL settings while maintaining client privacy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to utilize data heterogeneity as an asset rather than a limitation, which is a significant departure from traditional methods. The introduction of FOSTER represents a meaningful advancement in OoD detection within federated learning contexts, particularly given its efficacy across various FL settings and its ability to function without real OoD samples. However, a potential weakness may be the lack of extensive empirical validation across diverse datasets, which could limit the generalizability of the findings. Additionally, the complexity of the synthesis process may introduce challenges in implementation in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem it addresses, the methodology adopted, and the results obtained. The novelty of the approach is significant, as it redefines the conventional understanding of data heterogeneity in federated learning. The quality of the writing is high, with a coherent flow of ideas. As for reproducibility, while the methodology is clearly described, it would benefit from additional details on the implementation process and hyperparameter tuning to facilitate replication by other researchers.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative solution to the problem of out-of-distribution detection in federated learning. By transforming the challenges posed by data heterogeneity into opportunities for effective learning, FOSTER demonstrates significant potential to enhance OoD detection performance while safeguarding data privacy.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces FOSTER, a novel framework designed to enhance out-of-distribution (OoD) detection in federated learning (FL) by synthesizing virtual OoD samples from heterogeneous data on local clients. The authors address the critical challenge of limited availability of real OoD samples in FL due to privacy and resource constraints. The methodology employs a global classifier to generate external-class data while preserving privacy, incorporating Gaussian noise for improved sample diversity. The results demonstrate that FOSTER achieves state-of-the-art performance in OoD detection using limited in-distribution (ID) data across multiple datasets, including CIFAR-10, CIFAR-100, and STL10.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to leveraging data heterogeneity within FL to overcome the challenges associated with OoD detection. By synthesizing virtual samples, the authors provide a practical solution to a significant problem in security-sensitive applications without compromising data privacy. However, a potential weakness is the reliance on the quality of the global classifier; if the classifier is not well-trained, it may adversely affect the quality of the synthesized samples. Additionally, while the experiments show strong performance, further exploration of the method's limitations and potential edge cases would strengthen the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written and well-structured, making it accessible to readers with varying levels of expertise in federated learning and OoD detection. The methodology is detailed, allowing for reproducibility, although the implementation specifics could be elaborated for complete clarity. The novelty of the approach is significant, as it presents a new way to address a previously underexplored area in FL. Empirical results support the claims made, showcasing the effectiveness of the proposed method.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to OoD detection in federated learning, demonstrating significant results through the FOSTER framework. The methodology is well-articulated, and the empirical findings are robust, although further exploration of potential limitations would enhance the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection\" addresses the challenge of Out-of-Distribution (OoD) detection in the context of Federated Learning (FL). The authors propose a novel framework called FOSTER, which synthesizes virtual OoD samples from non-iid data to enhance OoD detection capabilities without violating data privacy. The methodology includes generating and filtering external-class data to ensure the quality and diversity of synthetic samples. Experimental results demonstrate that FOSTER significantly improves performance metrics such as AUROC and ID accuracy on datasets like CIFAR-10, CIFAR-100, and STL10, outperforming several baseline methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing a critical issue in FL—OoD detection—by leveraging data heterogeneity. The methodology is robust, well-detailed, and supported by thorough experimental validation against established benchmarks. However, a potential weakness is the limited exploration of failure cases or the method's limitations, which could provide deeper insights into its robustness. Additionally, the experiments could benefit from validation on a wider range of datasets to further establish generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making it accessible to readers. The novelty of leveraging heterogeneity in FL for OoD detection is significant, contributing to the advancement of the field. The methodology is reproducible, as the authors provide sufficient detail regarding the synthesis and filtering processes. However, some sections could be improved with further elaboration on the implications of results and limitations of the proposed method.\n\n# Summary Of The Review\nOverall, the paper presents an innovative and effective solution to a pressing problem in federated learning by introducing the FOSTER framework for OoD detection. While the experimental results are compelling, there is room for improvement in exploring the method's limitations and validating its performance across additional datasets.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel framework called Federated Out-of-Distribution Synthesizer (FOSTER) aimed at improving out-of-distribution (OoD) detection within the context of federated learning (FL). The authors exploit the heterogeneity of non-iid data across clients to synthesize virtual OoD samples, addressing the challenge posed by the unavailability of real OoD data. The methodology involves utilizing external-class data, a synthesis procedure for generating diverse samples, and a local optimization strategy that incorporates both classification and OoD regularization losses. Experimental results demonstrate that FOSTER outperforms existing methods on several datasets in terms of AUROC and classification accuracy.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to utilizing the data heterogeneity characteristic of federated learning to facilitate OoD detection, a task that is typically hindered by the lack of real OoD data. The synthesis of external-class samples and the filtering mechanism to enhance diversity are noteworthy contributions that advance the state-of-the-art in this area. However, a potential weakness is the reliance on the availability of external-class data, which may not always be feasible in all federated learning contexts. Additionally, while the empirical results are promising, the generalizability of the findings across varied application domains remains to be further explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem, methodology, and results. The mathematical formulations are presented in a clear manner, aiding in the understanding of the proposed framework. The novelty of the approach is significant, as it bridges the gap between federated learning and OoD detection in a unique way. The reproducibility of the experiments could be enhanced by providing more details regarding the implementation and the hyperparameters used in the experiments, as well as by sharing the code and datasets.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of federated learning by presenting a novel framework for OoD detection that effectively utilizes data heterogeneity. While the approach is innovative and the results are strong, further validation across diverse scenarios would strengthen the claims made in the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FOSTER, a novel method aimed at enhancing out-of-distribution (OoD) detection through the synthesis of virtual OoD samples. The methodology hinges on specific assumptions regarding data heterogeneity and employs generative modeling techniques to produce these samples. While the authors claim to achieve state-of-the-art performance improvements over existing methods, the findings suggest only marginal gains, raising questions about the practical significance of their contributions.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to generating virtual OoD samples, which could potentially address a critical gap in OoD detection. However, several weaknesses are notable. The method's reliance on complex assumptions about data heterogeneity may not generalize well to real-world scenarios. Additionally, the synthesized samples' quality is questionable, as the paper does not adequately validate their effectiveness in mimicking real OoD samples. The experimental validation is limited to a narrow selection of datasets, and the marginal performance improvements raise concerns about the method's practical applicability. Furthermore, issues such as potential mode collapse in the generative model and inadequate analysis of computational complexity detract from the overall robustness of the proposal.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is affected by the complexity of the assumptions and methodologies employed, which may hinder understanding for readers not deeply familiar with the topic. While the novelty of synthesizing virtual OoD samples is noteworthy, the practical implications and reproducibility of the findings are compromised by the limited dataset diversity and insufficient validation of synthesized samples. The ablation studies presented do not convincingly establish the robustness of FOSTER, suggesting potential instability in its application.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to OoD detection through the synthesis of virtual samples but suffers from significant limitations that undermine its practical relevance and robustness. The marginal performance improvements and reliance on questionable assumptions raise doubts about the method's applicability in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection\" introduces a novel method called the Federated Out-of-Distribution Synthesizer (FOSTER). This approach synthesizes virtual external-class out-of-distribution (OoD) samples by leveraging the data heterogeneity inherent in federated learning (FL) settings. The authors demonstrate that FOSTER not only enhances OoD detection performance, achieving state-of-the-art results across multiple datasets (CIFAR-10, CIFAR-100, and STL10) but also maintains high test accuracy. Additionally, the methodology emphasizes privacy preservation while ensuring diversity in the synthesized samples, positioning FOSTER as a versatile and impactful framework for various applications.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to utilizing data heterogeneity as an asset rather than a limitation, significantly advancing the field of federated learning. The empirical results are compelling, showcasing substantial performance improvements over existing methods, which underscores the effectiveness of FOSTER. However, a potential weakness is the limited exploration of the method's scalability and performance in extreme cases of data heterogeneity, which could impact its generalizability in real-world settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the research, methodology, and findings. The quality of the writing is high, with appropriate use of technical language and clear explanations of complex concepts. The novelty of FOSTER is significant, as it introduces a fresh perspective on leveraging data heterogeneity. Moreover, the authors have made their code publicly available, which enhances reproducibility and encourages further research in this area.\n\n# Summary Of The Review\nOverall, this paper presents a highly innovative approach to out-of-distribution detection within federated learning, successfully transforming a common challenge into an opportunity for advancement. The empirical results are robust, and the contributions are significant, marking a noteworthy step forward in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a theoretical framework called Federated Out-of-Distribution Synthesizer (FOSTER), designed to enhance out-of-distribution (OoD) detection in federated learning (FL) settings. It addresses the challenge of data heterogeneity among clients by proposing a method for synthesizing virtual OoD samples that leverage the diversity of non-iid data. The findings suggest that this approach can improve model generalization and detection capabilities without compromising data privacy, ultimately providing a novel mechanism to address the limitations of traditional OoD detection methods.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its innovative approach to synthesizing virtual OoD samples through federated learning, which creatively turns the challenge of data heterogeneity into an advantage. The theoretical insights into the synthesis process and the importance of maintaining diversity and hardness in generated samples are valuable contributions to the field. However, the paper could benefit from empirical validation of the proposed methods, as the theoretical foundations may not fully capture the complexities of practical implementations in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical framework, making it accessible to readers with a background in federated learning and OoD detection. The quality of the writing is high, with a logical flow that effectively conveys the underlying concepts. However, the reproducibility of the proposed methods is uncertain due to the lack of detailed experimental validation and practical examples. The novelty of synthesizing virtual samples in a federated context is significant, contributing valuable insights to both federated learning and OoD detection literature.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the understanding of OoD detection in federated learning by introducing the FOSTER framework, which leverages data heterogeneity for synthesizing virtual samples. While the theoretical insights are strong, the lack of empirical validation presents a concern regarding the practical application of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection\" introduces a novel framework called Federated Out-of-Distribution Synthesizer (FOSTER) aimed at enhancing out-of-distribution (OoD) detection in federated learning (FL) settings. The methodology employs a class-conditional generator that synthesizes virtual external-class OoD samples by leveraging non-iid data from multiple clients while ensuring data confidentiality and communication efficiency. The experimental results demonstrate that FOSTER effectively improves OoD detection performance across various datasets (CIFAR-10, CIFAR-100, STL10, DomainNet) compared to established baseline methods, even with limited in-distribution (ID) data.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to leveraging heterogeneity in federated learning for OoD detection, which is a significant challenge in current machine learning paradigms. The use of a class-conditional generator and Gaussian noise to ensure diversity in generated samples is particularly noteworthy. Additionally, the extensive empirical evaluation and comparisons with multiple benchmarks lend credibility to the findings. However, the paper could improve in terms of clarity regarding the algorithm's complexity and the practical implications of the proposed method, particularly in real-world settings where data distributions may vary significantly.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly, though some sections could benefit from greater detail, especially in the algorithm overview. The quality of the writing is high, with careful attention to the presentation of results and methodologies. The novelty of the approach is significant, particularly in the context of federated learning, as it addresses a critical gap in OoD detection. The code availability enhances reproducibility, allowing other researchers to validate and build upon the work.\n\n# Summary Of The Review\nOverall, this paper presents a compelling and innovative approach to tackling the challenges of out-of-distribution detection in federated learning. The contributions are well-founded and supported by empirical evidence, though some aspects of the methodology could be clarified further. The integration of a synthesizer for OoD samples is a noteworthy advancement in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces FOSTER, a novel approach aimed at enhancing performance in federated learning (FL) settings by purportedly outperforming existing methods by 2.49% in AUROC. The authors claim that FOSTER effectively utilizes limited in-distribution (ID) data without requiring real out-of-distribution (OoD) samples. They emphasize the method’s adaptability across various FL scenarios and innovative techniques such as external-class knowledge extraction and the application of Gaussian noise for sample diversity. However, the paper's comparisons and claims of novelty are called into question by existing methods that share similar foundations or have been validated under comparable conditions.\n\n# Strength And Weaknesses\nThe strengths of FOSTER lie in its attempt to address key challenges in federated learning, particularly the limitations posed by data availability and privacy. The use of heterogeneous data is noted, although this concept has been explored in other frameworks like FedAvg. The paper's claims about outperforming VOS are undermined by the latter's known limitations, and the overall performance results may be inflated due to the outdated nature of the baseline methods. Additionally, while FOSTER's filtering method for external-class samples and the use of soft labels are presented as innovations, these techniques have been previously employed in other studies, suggesting a lack of originality in these contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the quality of the comparisons made with existing methods raises concerns about the validity of its claims. The novelty of FOSTER is questionable; while it does attempt to innovate within the privacy-preserving domain of federated learning, many of its proposed techniques are not new and have been utilized in prior works. Reproducibility could be a concern due to the limited exploration of diverse datasets, which may affect the generalizability of its findings.\n\n# Summary Of The Review\nOverall, while FOSTER presents an interesting approach to federated learning, its contributions are not as groundbreaking as claimed, and its performance benchmarks may be inflated when compared to outdated methods. The lack of unique innovations and limited empirical validation raise questions about the overall significance of the paper's findings.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection\" proposes a novel approach to address the challenges posed by data heterogeneity in federated learning (FL) environments when detecting out-of-distribution (OoD) samples. The authors introduce the FOSTER algorithm, which leverages external-class knowledge to enhance OoD detection without relying heavily on substantial outlier data. The findings demonstrate that FOSTER achieves superior performance compared to state-of-the-art methods in various challenging FL settings, particularly in scenarios with non-iid data distributions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to transforming a common issue—data heterogeneity—into an advantage for OoD detection. The methodology is well-grounded in theoretical foundations and supported by extensive empirical results. However, the paper suffers from several clarity issues, including inconsistent terminology and notation, which may hinder understanding. Additionally, the lack of a clear definition for key terms and abbreviations early in the text could confuse readers unfamiliar with the specific context of federated learning and out-of-distribution detection.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by inconsistent formatting, awkward phrasing, and the inconsistent use of terminology. While the novelty of the FOSTER algorithm and its application to federated learning is commendable, these clarity issues detract from the overall quality. Reproducibility is also a concern, as some algorithm parameters and key terms are not defined clearly, making it challenging for other researchers to replicate the results. The authors should address these issues to improve the paper's accessibility and impact.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to leveraging data heterogeneity in federated learning for out-of-distribution detection. However, the clarity and consistency of the writing require improvement to enhance understanding and reproducibility. Addressing these issues would significantly strengthen the paper's contributions to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents FOSTER, a novel framework aimed at improving out-of-distribution (OoD) detection in federated learning environments. The authors claim that FOSTER efficiently synthesizes virtual OoD samples to enhance model robustness against data distribution shifts. The methodology involves generating these synthetic samples using Gaussian noise, with evaluations conducted on the CIFAR-10, CIFAR-100, and STL10 datasets. However, the paper primarily focuses on class non-iid data settings, neglecting the implications of feature non-iidness and other forms of data heterogeneity.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to synthesizing OoD samples, which could potentially improve the robustness of federated learning models. However, significant weaknesses include a limited scope of evaluation, as results are only presented for a few datasets, and a lack of discussion regarding the quality and representativeness of the synthesized samples compared to actual out-of-distribution data. Additionally, the paper fails to address the critical impacts of adversarial attacks, scalability in large federated networks, and the trade-offs between communication efficiency and model performance. The exploration of ethical implications and privacy concerns is also insufficient.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, but certain areas require more thorough exploration and discussion to enhance understanding. The novelty of the method is promising, yet the lack of comprehensive evaluations and comparisons with existing methods limits its reproducibility and generalizability. The authors could improve clarity by providing more detailed comparisons with other state-of-the-art techniques regarding computational efficiency and the impact of hyperparameter settings.\n\n# Summary Of The Review\nOverall, while the paper introduces a compelling framework for OoD detection in federated learning, its evaluation is limited and lacks depth in various critical aspects. The contributions are noteworthy, but the paper would benefit from broader experiments, detailed discussions on methodological limitations, and a more comprehensive exploration of ethical considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the Federated Out-of-Distribution Synthesizer (FOSTER), a novel method for detecting out-of-distribution (OoD) samples in federated learning (FL) environments. FOSTER synthesizes virtual OoD samples from non-independent and identically distributed (non-iid) data while preserving privacy. The authors report improvements in performance metrics, including AUROC and ID accuracy across several datasets (CIFAR-10, CIFAR-100, and STL10), with noted enhancements of 2.49%, 2.88%, 1.42% in AUROC and 0.01%, 0.89%, 1.74% in ID accuracy compared to baseline methods. However, the paper does not provide explicit statistical significance testing for these performance metrics.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to synthesizing OoD samples in a federated learning context, which addresses a significant gap in existing literature. The proposed FOSTER method shows promising performance improvements over traditional OoD detection techniques such as Energy, MSP, and ODIN. However, a notable weakness is the lack of rigorous statistical testing to substantiate the reported enhancements, which raises questions about the reliability of the findings. The absence of formal significance testing methods, such as paired t-tests or ANOVA, diminishes the robustness of the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting its methodology and findings in a logical manner. The novel contribution of synthesizing OoD samples in FL is articulated effectively, demonstrating an understanding of the challenges in this domain. However, the reproducibility of the results may be compromised due to the insufficient emphasis on statistical validation. Future iterations of this work could benefit from detailing the experimental setup and providing code or data to facilitate independent verification of the results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of OoD detection within federated learning, showcasing a new method that achieves state-of-the-art performance. However, the lack of statistical significance testing undermines the strength of the claims and calls for a more robust validation of the results.\n\n# Correctness\n4/5 - The methodology and findings appear sound, but the lack of statistical validation introduces uncertainty regarding the correctness of the performance claims.\n\n# Technical Novelty And Significance\n4/5 - The approach presents a novel synthesis method for OoD detection in FL, contributing meaningfully to the field, though it could be strengthened by addressing statistical validation.\n\n# Empirical Novelty And Significance\n3/5 - While the empirical results show improvements over existing methods, the absence of statistical tests to confirm these findings limits the empirical significance of the contributions.",
    "# Summary Of The Paper\nThe paper presents FOSTER, a framework for out-of-distribution (OoD) detection in federated learning settings, particularly in class non-iid scenarios. By leveraging external-class data from other clients, FOSTER generates synthetic OoD samples to improve the robustness of classifiers against novel data distributions. The methodology includes a synthetic approach to OoD sample generation and evaluates its effectiveness on datasets like CIFAR-10, CIFAR-100, STL10, and DomainNet. The findings suggest that FOSTER enhances OoD detection performance compared to baseline methods, although the generalizability and scalability of the approach remain uncertain.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to generating synthetic OoD samples, which addresses a critical gap in federated learning. However, there are several notable weaknesses. The reliance on external-class data raises concerns about representativeness and the potential for mode collapse in generated samples, which could limit the diversity and effectiveness of training. Additionally, the paper does not adequately discuss the scalability of FOSTER across varied client distributions or the computational overhead for clients, which is crucial for practical deployment. The lack of exploration into the implications of privacy risks and the effects of different numbers of external classes further diminishes the comprehensiveness of the study.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow. However, some aspects of the methodology could benefit from greater detail, particularly regarding the assumptions made about external-class data. While the novelty of the synthetic OoD sample generation is notable, the reproducibility of the results is questionable due to the limited evaluation across diverse datasets and scenarios. Future work should aim to provide comprehensive guidelines for implementing FOSTER in various federated learning environments.\n\n# Summary Of The Review\nOverall, the paper presents a novel framework for OoD detection in federated learning that addresses important challenges in the field. However, several limitations related to the reliance on external-class data, scalability, and empirical validation across different contexts need to be addressed to enhance the practical applicability and robustness of the proposed method.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes a method called FOSTER for Out-of-Distribution (OoD) detection within the context of Federated Learning (FL). The authors claim that FOSTER addresses the limitation of requiring real OoD samples by synthesizing virtual OoD data from heterogeneous datasets, thereby leveraging non-iid data. The paper asserts that this approach yields state-of-the-art performance, despite the constrained data environment typically found in FL scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to navigate the challenges of OoD detection without relying on real OoD samples, a bottleneck in current methodologies. However, this contribution is undermined by a lack of novelty and depth in the proposed method. The claims of FOSTER being the first to achieve this are exaggerated, as prior work has addressed similar limitations. Additionally, the experiments lack rigorous evaluation and fail to provide significant insights into the trade-offs of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulated, but it lacks depth in its analysis of prior work and the implications of its findings. The assertions regarding the novelty of the method are not convincingly supported, as many of the concepts and strategies employed are already established in the literature. Reproducibility is somewhat compromised due to the reliance on commonly used datasets and standard evaluation metrics without sufficient discussion of the methodology's broader applicability.\n\n# Summary Of The Review\nWhile the paper presents a potentially useful framework for OoD detection in FL, it largely rehashes existing ideas without substantial innovation. The contributions are overshadowed by a lack of rigorous evaluation and an exaggerated portrayal of its novelty and significance.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents the Federated Out-of-Distribution Synthesizer (FOSTER), a novel framework designed to enhance out-of-distribution (OoD) detection in federated learning (FL) settings. FOSTER leverages heterogeneous data across clients while incorporating privacy-preserving mechanisms essential for federated environments. The authors demonstrate that FOSTER outperforms existing methods on CIFAR datasets by utilizing external-class data as surrogates for real OoD samples, thereby improving the robustness of the model in detecting deviations in data distributions.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its innovative approach to incorporating external-class data and the emphasis on privacy preservation, which is critical for federated learning applications. However, the methodology could be enhanced by integrating advanced generative models like GANs or VAEs for improved sample diversity. Additionally, the reliance on a single global classifier raises concerns about scalability and robustness, and the evaluation could benefit from experiments on more complex datasets or real-world applications to validate the findings further.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations, methodology, and results. The quality of the experiments and the clarity of the results are commendable, though more extensive testing on diverse datasets would strengthen the paper's claims. The novelty of the approach is significant; however, the reproducibility could be improved by providing more details on the experimental setup and hyperparameter tuning.\n\n# Summary Of The Review\nOverall, FOSTER presents a promising approach for addressing OoD detection in federated learning, featuring innovative use of external-class data and a focus on privacy. While the results are compelling, there are opportunities for further enhancement, particularly in methodological robustness and empirical validation across diverse applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces FOSTER, a novel method for out-of-distribution (OoD) detection that demonstrates significant enhancements over state-of-the-art techniques. FOSTER's methodology includes the use of a probability density function (p.d.f.) filter to improve the quality of generated samples, which leads to superior detection metrics. The findings indicate that FOSTER achieves higher area under the receiver operating characteristic (AUROC) scores across multiple datasets including CIFAR-10, CIFAR-100, and STL10, while also maintaining competitive in-distribution (ID) accuracy. Additionally, the method shows robustness in challenging scenarios involving feature non-iid clients, and maintains stability in performance across varying numbers of active clients.\n\n# Strength And Weaknesses\nThe principal strength of the paper is its demonstration of FOSTER's effectiveness in enhancing OoD detection performance, evidenced by its superior AUROC scores compared to various baselines across multiple datasets. The ability to maintain competitive ID accuracy while improving OoD detection metrics is particularly noteworthy. However, a potential weakness is the lack of detailed insight into the underlying mechanisms that contribute to the performance improvements, which may limit the understanding of the method's applicability in different contexts. Further, while the paper presents strong empirical results, the exploration of the theoretical foundations of FOSTER could enrich the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the proposed method, experimental setup, and results. The quality of writing is high, and the figures and tables effectively convey the findings. The novelty of FOSTER lies in its combination of federated learning with an innovative p.d.f. filtering approach for OoD detection. However, the reproducibility of results may require additional details regarding the implementation and tuning of hyperparameters, which are not fully discussed in the paper.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of OoD detection with the introduction of FOSTER, which demonstrates significant improvements in detection performance while maintaining ID accuracy across various datasets. The results are promising, although a deeper exploration of the underlying principles and improved reproducibility could strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving federated learning (FL) under non-IID data distributions, focusing on boosting model performance through a new aggregation mechanism. The methodology involves a comprehensive analysis of existing FL techniques, followed by the introduction of an adaptive aggregation method that leverages local data distribution insights. The findings indicate that the proposed method significantly enhances model accuracy and robustness compared to baseline FL algorithms, demonstrating its effectiveness through extensive empirical evaluations across various datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its practical contributions to the FL domain, particularly in addressing the challenges posed by non-IID data. The proposed adaptive aggregation method is innovative and well-justified, with strong empirical support. However, a notable weakness is the complexity of the presentation, which may hinder accessibility for readers unfamiliar with the technical jargon and methodologies. Additionally, the paper could benefit from clearer visual aids and summaries of results to enhance reader comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is impacted by lengthy and complex sentences, which could be simplified for better readability. The quality of the writing is generally good but requires thorough proofreading to eliminate minor typographical errors. The novelty of the proposed method is significant, contributing valuable insights to the field of FL. Reproducibility is somewhat compromised by insufficient detail in the methodology section, particularly regarding experimental setups and data preprocessing steps.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the federated learning literature by proposing an innovative aggregation method tailored for non-IID data distributions. While the findings are promising, the paper's clarity and presentation need improvement to enhance accessibility and comprehension for a broader audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -1.7034102321223137,
    -1.3377722008047186,
    -1.5276613984086298,
    -1.4709637278476002,
    -1.6934452571673875,
    -1.5434565247364478,
    -1.6228012593775325,
    -1.825251588119525,
    -1.45850038478908,
    -1.6917703152592987,
    -1.3626545414487221,
    -1.371807783497889,
    -1.5829801382301534,
    -1.6301085520698932,
    -1.5557191176714193,
    -1.5157368534765296,
    -1.6078996398211838,
    -1.531521686499251,
    -1.609617876538783,
    -1.6123339545518334,
    -2.0749626869476803,
    -1.5604878887568039,
    -1.756814480876187,
    -1.4766331985905987,
    -1.6353101478187677,
    -1.8288960932291687,
    -1.6880656315381701,
    -1.5085679949016588,
    -1.5289368889655421
  ],
  "logp_cond": [
    [
      0.0,
      -1.5221670470752593,
      -1.5117201724119576,
      -1.4919204145184821,
      -1.5116407390907547,
      -1.5112563206424663,
      -1.552612721477414,
      -1.5276711902727458,
      -1.503703351606479,
      -1.4996444143293648,
      -1.5119008362973148,
      -1.5756674301406208,
      -1.4853924517960928,
      -1.5183076978769214,
      -1.5037795414760142,
      -1.5016395133556955,
      -1.5511437277565625,
      -1.5039962549201624,
      -1.5181038451041433,
      -1.4980541021301783,
      -1.53688345401344,
      -1.5208826148143824,
      -1.5313491820256027,
      -1.5174834329794433,
      -1.5197978636761271,
      -1.5267038726003792,
      -1.515096406756401,
      -1.5333181521337964,
      -1.572768837372509
    ],
    [
      -0.9196385703505556,
      0.0,
      -0.8754974367769386,
      -0.8734735833435632,
      -0.9153370304263122,
      -0.8740113606087818,
      -0.9866522817673421,
      -0.875222571813531,
      -0.8701924389160787,
      -0.9136061705171867,
      -0.9062880067632824,
      -1.0327766498403883,
      -0.8594335658583948,
      -0.8347341358136229,
      -0.8657704853187241,
      -0.9090553327208375,
      -0.9448499025780083,
      -0.8484369248960529,
      -0.8901189947540394,
      -0.8781968922872152,
      -0.953427758464499,
      -0.9717741330471955,
      -0.952258813191746,
      -0.9296950889328081,
      -0.9710327378706932,
      -0.8918996549006707,
      -0.9106183072227086,
      -0.9748924555094064,
      -1.034700968840464
    ],
    [
      -1.175865292950033,
      -1.174908945780411,
      0.0,
      -1.0975292404367756,
      -1.1674932663517683,
      -1.1614156656061534,
      -1.2388041881817249,
      -1.160630922266952,
      -1.1130618034060393,
      -1.1416126879816137,
      -1.1353418914621416,
      -1.2845691680665248,
      -1.1431817492399379,
      -1.1318441043513319,
      -1.1131832071550842,
      -1.105132292567817,
      -1.1940596938061687,
      -1.1202428280108372,
      -1.1325932448938683,
      -1.117113846467241,
      -1.2162290224110395,
      -1.1828642456005207,
      -1.203403600445019,
      -1.2016715372541054,
      -1.152184154997262,
      -1.2045454959373347,
      -1.1558166410703035,
      -1.2311234209630466,
      -1.2776684319995553
    ],
    [
      -1.107564299719612,
      -1.1207006115551514,
      -1.0658271485129047,
      0.0,
      -1.136236486112821,
      -1.097437823251691,
      -1.1534230314077827,
      -1.1160697813334879,
      -1.0608765173787273,
      -1.1274544098442978,
      -1.0969574209438215,
      -1.2396849555219103,
      -1.0613492782550844,
      -1.046428514603744,
      -1.0412664433286134,
      -1.0552069405770075,
      -1.1419736484042387,
      -1.033219649292458,
      -1.0789939690698598,
      -1.0112532086075041,
      -1.1348476338246478,
      -1.111103875852788,
      -1.173490729024815,
      -1.1594873468251394,
      -1.1598570152168097,
      -1.1415190769218866,
      -1.117982513260393,
      -1.192656292494522,
      -1.2394929549292406
    ],
    [
      -1.2905607705778603,
      -1.3923091242165957,
      -1.2803208119642342,
      -1.3356817338705782,
      0.0,
      -1.332936958951997,
      -1.3597735640278479,
      -1.3034669668344505,
      -1.319687714320026,
      -1.3208744046670209,
      -1.3662085539158262,
      -1.437077105694485,
      -1.3343108735372955,
      -1.3168616164018834,
      -1.3024497825636152,
      -1.3473473911346716,
      -1.3442475438267576,
      -1.345298009725539,
      -1.3509392333314976,
      -1.3368034444417456,
      -1.2897271203480596,
      -1.375463238170494,
      -1.3579891322224686,
      -1.3186294678732258,
      -1.3367997658491497,
      -1.3311978657302317,
      -1.2896421429965141,
      -1.3756047981500674,
      -1.424348126055931
    ],
    [
      -1.1595099718389101,
      -1.1678008747844644,
      -1.0994708324662759,
      -1.0939739453651234,
      -1.1419083712731373,
      0.0,
      -1.2380913829851685,
      -1.04403047892252,
      -1.0325930803890588,
      -1.089764342426664,
      -1.1187026062406686,
      -1.3155463817376039,
      -1.1012154358761927,
      -1.1213199524089805,
      -1.0988341322370845,
      -1.1283043907177956,
      -1.176987156888577,
      -1.1032326635330327,
      -1.1181784006977955,
      -1.099949281474258,
      -1.1785058578678382,
      -1.2257565103974817,
      -1.2250859222934094,
      -1.1478953201323865,
      -1.1942585754411426,
      -1.1693227628941971,
      -1.1515626763815594,
      -1.2157451894646227,
      -1.2915969355723862
    ],
    [
      -1.2936006019834834,
      -1.2926735339199784,
      -1.2282807924754398,
      -1.2616847132558866,
      -1.2930332875450705,
      -1.3078443827590274,
      0.0,
      -1.293510271151725,
      -1.2852933600235994,
      -1.2836443680718999,
      -1.3178303570087893,
      -1.254699097503777,
      -1.3029175970818163,
      -1.3040609598773423,
      -1.2988539259355298,
      -1.292324119389846,
      -1.3259908910980465,
      -1.2802164984752793,
      -1.2292303330622536,
      -1.2375217794846303,
      -1.282651306301621,
      -1.2978716431329755,
      -1.3262702770288872,
      -1.3265597861231735,
      -1.31979776051768,
      -1.2901987365767729,
      -1.2929232679147364,
      -1.3437686179618162,
      -1.2852673641439691
    ],
    [
      -1.4583161921182684,
      -1.4608212593887033,
      -1.4131319190231242,
      -1.412510576375894,
      -1.4277416259306412,
      -1.3555717391541573,
      -1.5336411947327868,
      0.0,
      -1.3637910685994998,
      -1.3852531813313769,
      -1.431566299749451,
      -1.5665582021222684,
      -1.3673464388910452,
      -1.4137839094107896,
      -1.436630338915642,
      -1.394860775991768,
      -1.3894905647443,
      -1.385430936776247,
      -1.4104477106159186,
      -1.360154611032836,
      -1.4319506650338776,
      -1.5186835670710992,
      -1.4329473172234959,
      -1.4007718241771006,
      -1.4619174689202463,
      -1.4178895199153403,
      -1.4218568115646655,
      -1.5133549821246022,
      -1.5764127885577524
    ],
    [
      -1.1174495009474814,
      -1.0626727541740895,
      -1.0687680211591777,
      -1.0056853159227068,
      -1.0588855179711039,
      -1.0027873609742337,
      -1.1606342097395526,
      -1.0534541040206646,
      0.0,
      -1.07053566275471,
      -1.044936393277702,
      -1.2413795429337842,
      -1.0450800237737237,
      -1.0128705001683764,
      -0.9786881836719509,
      -1.043828645641321,
      -1.1486971799896732,
      -1.0160332943112378,
      -1.054972610859294,
      -0.9396758727448081,
      -1.1033763077320384,
      -1.1194499979326735,
      -1.1145660428153499,
      -1.0500376430571592,
      -1.1505074771809451,
      -1.0707151889351807,
      -1.1179862422923272,
      -1.116160453072392,
      -1.213629287150825
    ],
    [
      -1.3163823857332684,
      -1.3752546095777818,
      -1.329413109460589,
      -1.3539519556566184,
      -1.344489493953393,
      -1.3121937561033892,
      -1.4385888940776816,
      -1.2741457912612144,
      -1.3017795432316897,
      0.0,
      -1.3149491854298967,
      -1.4770053024666858,
      -1.33790302948611,
      -1.3138933868208753,
      -1.3181437226145925,
      -1.3365292532399458,
      -1.356346648006544,
      -1.3474170378074468,
      -1.3450652450257812,
      -1.3556703032427233,
      -1.3579056760281552,
      -1.3711574060748355,
      -1.3427398721786041,
      -1.3776085554393516,
      -1.3565797139406253,
      -1.358438901980435,
      -1.3276129210483112,
      -1.3905202309000724,
      -1.4776334151136645
    ],
    [
      -0.9801101005343268,
      -1.0050249838995304,
      -0.9443535087664092,
      -0.9710086243746545,
      -0.9907933774431065,
      -0.9394136130896843,
      -1.0693773279280163,
      -0.9340152406401664,
      -0.9123271407816018,
      -0.933581100144412,
      0.0,
      -1.1307571159599281,
      -0.9289297850958604,
      -0.9606452153274236,
      -0.99830731762252,
      -0.9634581689321647,
      -1.0373460160755472,
      -0.9520136203378331,
      -0.9546171763973044,
      -0.9570803720692672,
      -1.052685933038472,
      -1.0419905066927846,
      -1.0400365884403044,
      -0.9888086612620484,
      -1.0636826755966364,
      -0.9944063320294549,
      -0.9919223591140753,
      -1.0584206856771494,
      -1.147702209727521
    ],
    [
      -1.1049974532506868,
      -1.1024375716392747,
      -1.0813054065693453,
      -1.0930825120740846,
      -1.065744690288065,
      -1.1075384991753876,
      -1.0778262544375032,
      -1.083749829865214,
      -1.1088006605954863,
      -1.0907236700755722,
      -1.1010220994707174,
      0.0,
      -1.0947326655370309,
      -1.0949855044333305,
      -1.0863379410977352,
      -1.0911377837018816,
      -1.0893659234387894,
      -1.0883277477431859,
      -1.0685022527867005,
      -1.0920112385195886,
      -1.0891614803219773,
      -1.0622025588672803,
      -1.0632502079859272,
      -1.1089517476456254,
      -1.0897784690712575,
      -1.0738849367601162,
      -1.0854866420630969,
      -1.0871261605015223,
      -1.046335371544015
    ],
    [
      -1.1540387049079257,
      -1.1894767337012935,
      -1.105557393158694,
      -1.0927374703896597,
      -1.1581839876685156,
      -1.1675833796389339,
      -1.2719090227103511,
      -1.1449074795451593,
      -1.1649814911882188,
      -1.108903838968753,
      -1.1705283186263908,
      -1.336945187356809,
      0.0,
      -1.120805324977613,
      -1.1323207161152546,
      -1.1062651178579768,
      -1.2233538235748305,
      -1.0732556079275148,
      -1.0964927411555558,
      -1.109262511337811,
      -1.1398638315458556,
      -1.1970087116818064,
      -1.2237782420146202,
      -1.2168874122071587,
      -1.1853639750618867,
      -1.1535815976871664,
      -1.1024597812394163,
      -1.2674150221476779,
      -1.324340656277361
    ],
    [
      -1.2080039367975866,
      -1.153299682443831,
      -1.1247553304762947,
      -1.094629054479718,
      -1.2071518878935268,
      -1.1800962133017099,
      -1.2591212276291868,
      -1.1965079005655086,
      -1.0792237116852015,
      -1.1822191031517746,
      -1.2129760191318413,
      -1.332139766030626,
      -1.1334675836111057,
      0.0,
      -1.0702113033441796,
      -1.1304054563259374,
      -1.242228377521964,
      -1.1092322537770656,
      -1.153443640920439,
      -1.0898364753358685,
      -1.170913143443254,
      -1.2250852929329037,
      -1.2039708424999453,
      -1.2373668903672974,
      -1.1958269916779896,
      -1.198901029630575,
      -1.1557709115629562,
      -1.2225019133916526,
      -1.2870619101007152
    ],
    [
      -1.1361479028478896,
      -1.1128431356799544,
      -1.1046574957797732,
      -1.0613868361635934,
      -1.1388727180119809,
      -1.1388104458611938,
      -1.2102564789595553,
      -1.1617629128511162,
      -1.072143242819809,
      -1.1537235541364244,
      -1.1777003856275632,
      -1.267366688715067,
      -1.1133369604596455,
      -1.034810233867548,
      0.0,
      -1.1003053892742927,
      -1.1666486848121498,
      -1.0620208372096078,
      -1.1288507860717136,
      -1.039076797931413,
      -1.1930644647423299,
      -1.123538701755115,
      -1.1358190859378887,
      -1.1332328337306359,
      -1.1329812806973905,
      -1.1362612257205476,
      -1.1761328684983252,
      -1.2392741353579595,
      -1.2794346124026534
    ],
    [
      -1.1404393692596553,
      -1.1235735193113503,
      -1.0748558290355241,
      -1.0204328443207245,
      -1.0887465219526073,
      -1.0998783747947265,
      -1.189743686827917,
      -1.092640690865953,
      -1.047578273275436,
      -1.0864894064264183,
      -1.0884767211566435,
      -1.2509470326121361,
      -1.0178428942497713,
      -1.0681051483410435,
      -1.0559034303988362,
      0.0,
      -1.1325266950307207,
      -1.0743971345882943,
      -1.039519687110108,
      -1.0598396267829846,
      -1.10858708060738,
      -1.1092636679791537,
      -1.154463579608025,
      -1.1454364154244052,
      -1.1247556641276095,
      -1.0995982156170443,
      -1.0575073100846677,
      -1.1550381639264233,
      -1.2486742633162222
    ],
    [
      -1.274324752375328,
      -1.2331169546940703,
      -1.2288208818627075,
      -1.2298354358039238,
      -1.210651248231129,
      -1.255840442678556,
      -1.3107363420287828,
      -1.2167214606406898,
      -1.2515029069680454,
      -1.2624100000607552,
      -1.2669707218220208,
      -1.3123619073513928,
      -1.226327678277031,
      -1.2159113775895734,
      -1.2138546339007916,
      -1.2245131822245305,
      0.0,
      -1.228115494876121,
      -1.2560670777867204,
      -1.2234388419433848,
      -1.2325099243657893,
      -1.282944232006785,
      -1.2538705213358137,
      -1.2697639675279373,
      -1.2398056979945007,
      -1.256079729382752,
      -1.2368272034674577,
      -1.2458091124284418,
      -1.3217758854313642
    ],
    [
      -1.11461651263629,
      -1.102890072685274,
      -1.055122781429267,
      -0.9993203518801631,
      -1.1302851234673632,
      -1.0859248864526212,
      -1.1741886789293237,
      -1.0848831115041646,
      -1.026654224006298,
      -1.10271626781613,
      -1.115647388359659,
      -1.2407040540393375,
      -1.0129573300878791,
      -1.025368872126785,
      -1.0078593436262517,
      -1.0636750912020667,
      -1.141981758137405,
      0.0,
      -1.07919136579163,
      -0.915686476250659,
      -1.1171163508929596,
      -1.0908816576316078,
      -1.1191001390304145,
      -1.136672552424323,
      -1.1187024717908454,
      -1.127750335254554,
      -1.0954385619795373,
      -1.1549109642342215,
      -1.2170078791794776
    ],
    [
      -1.1959526787344288,
      -1.1767695567365921,
      -1.1219239725369616,
      -1.1510539104997646,
      -1.2344769778269975,
      -1.1590648094411482,
      -1.2090935880801064,
      -1.1741064690805472,
      -1.1342928620183146,
      -1.1645901220693053,
      -1.1659483488379265,
      -1.3241083856020153,
      -1.104736203959722,
      -1.1639737518612419,
      -1.1578609683032528,
      -1.129265185584779,
      -1.1992110691493607,
      -1.146663813402893,
      0.0,
      -1.1164667813276132,
      -1.2417601461784178,
      -1.2360535737464924,
      -1.2031418355354602,
      -1.2248243773082776,
      -1.2062771264739445,
      -1.1875603825007308,
      -1.156056191497434,
      -1.2466803066788454,
      -1.310390325478562
    ],
    [
      -1.220965738700705,
      -1.25015627099147,
      -1.1537186852557328,
      -1.1184920709758883,
      -1.2924139026093064,
      -1.2403967363486885,
      -1.2910707277069493,
      -1.2233319170046315,
      -1.1306125364302135,
      -1.2221607050593861,
      -1.2601974399617646,
      -1.3967538808803444,
      -1.1746180286167,
      -1.1428271826466356,
      -1.142528721488625,
      -1.1894596142475267,
      -1.2514308679090422,
      -1.10725432180298,
      -1.2004079086825565,
      0.0,
      -1.1910337665108834,
      -1.2110471014936623,
      -1.2322090969567543,
      -1.259054146360621,
      -1.183107641341014,
      -1.2373816353859672,
      -1.2085851906403597,
      -1.294251217499278,
      -1.3533295214109964
    ],
    [
      -1.7102396126008985,
      -1.7292742711563653,
      -1.680565671344845,
      -1.7174374881095362,
      -1.6820910771727333,
      -1.72775151116794,
      -1.7924871809780276,
      -1.6716396341347408,
      -1.726216323715644,
      -1.7055539383043457,
      -1.7438600191930056,
      -1.829695496570937,
      -1.6532158396189547,
      -1.6223224341224718,
      -1.6670705463458666,
      -1.6643407659348042,
      -1.727725320765203,
      -1.670429346190629,
      -1.6970963782733084,
      -1.6710612096939337,
      0.0,
      -1.7671399243053498,
      -1.7329687599922254,
      -1.7621555455839022,
      -1.703183493317949,
      -1.7066553096769996,
      -1.6177836508062826,
      -1.7388801843905517,
      -1.8460271961695542
    ],
    [
      -1.1715039025431269,
      -1.2246238119573711,
      -1.1721631497690264,
      -1.1187563783312457,
      -1.200159986709994,
      -1.2033945487267868,
      -1.239355920712763,
      -1.2347629255807142,
      -1.1409047475425607,
      -1.2028260513360338,
      -1.2305895267145046,
      -1.2816937972694067,
      -1.170013646790465,
      -1.1692534753337729,
      -1.1057588797765716,
      -1.1938611882943915,
      -1.2096230261131433,
      -1.113963095208398,
      -1.1754091001417386,
      -1.1461461991522863,
      -1.2046761639365438,
      0.0,
      -1.1722926817144252,
      -1.2601080876610418,
      -1.1669662023647855,
      -1.214453994162675,
      -1.2083718486371609,
      -1.195101971560672,
      -1.292044515535045
    ],
    [
      -1.4122653830563046,
      -1.3692320508700664,
      -1.3868160417757909,
      -1.3896532405679338,
      -1.3984004005040709,
      -1.4499190494456489,
      -1.4644280697864134,
      -1.4235516503554546,
      -1.3538564458913216,
      -1.390442514311226,
      -1.40903643589925,
      -1.5188587018511865,
      -1.3664412937483634,
      -1.3506146313054888,
      -1.3407518735876418,
      -1.3633223207695104,
      -1.390672429270875,
      -1.3580635383388102,
      -1.387295698805524,
      -1.3496246110303487,
      -1.419481900358917,
      -1.4500688476941863,
      0.0,
      -1.396240365058096,
      -1.328683575928792,
      -1.3739206336716248,
      -1.3724434323019572,
      -1.4178552252948817,
      -1.4846971083928824
    ],
    [
      -1.19529572844393,
      -1.1693504563936103,
      -1.1819377952651995,
      -1.1743801398950056,
      -1.2090192819381134,
      -1.167497513786405,
      -1.2745190642006288,
      -1.128182460434656,
      -1.1071567148114632,
      -1.189963487030536,
      -1.1571746840108013,
      -1.311142114319059,
      -1.1685854869760788,
      -1.1659046894627612,
      -1.140905654878389,
      -1.1878946329441664,
      -1.197803920051766,
      -1.1684801892931123,
      -1.1685206399040844,
      -1.1460546543145675,
      -1.2296240870173147,
      -1.2693381597977196,
      -1.167541896962924,
      0.0,
      -1.1973007599487577,
      -1.1719566236844459,
      -1.1908601520764563,
      -1.1792066288109064,
      -1.318676031281172
    ],
    [
      -1.3310814281293364,
      -1.309784394852217,
      -1.2275298551794314,
      -1.2839002568688551,
      -1.3211549195162082,
      -1.3370483258643717,
      -1.3684072861044545,
      -1.3237847255038313,
      -1.3069298418658217,
      -1.2579023250548393,
      -1.344070080718285,
      -1.418767225980391,
      -1.2594188631558954,
      -1.2823088291202707,
      -1.235026852121025,
      -1.240451608062187,
      -1.2719461258407427,
      -1.308800506590376,
      -1.2766168220709575,
      -1.2354945154574413,
      -1.2987228093696772,
      -1.2971516037995232,
      -1.1899060328127313,
      -1.311049779526421,
      0.0,
      -1.3198346957255584,
      -1.2323132384476918,
      -1.3383585128645377,
      -1.388144130131766
    ],
    [
      -1.3736099396591543,
      -1.3556570482211494,
      -1.3542635384247714,
      -1.3514510932484487,
      -1.3362331557656302,
      -1.3943303773237257,
      -1.4600579721282292,
      -1.3234671760875298,
      -1.3617609298812872,
      -1.3750279042944122,
      -1.3704091710335786,
      -1.4963285784474682,
      -1.3059517835963141,
      -1.3154050337076018,
      -1.306923732981499,
      -1.2965329714312954,
      -1.3664557986194963,
      -1.3576648232899429,
      -1.3666878047501172,
      -1.352949929100914,
      -1.3378115037583618,
      -1.3803123627622529,
      -1.3641988833126304,
      -1.4157302123019888,
      -1.3911954956220676,
      0.0,
      -1.356434099574928,
      -1.4034136666663053,
      -1.4835874413750427
    ],
    [
      -1.2905487605939918,
      -1.3002555249392544,
      -1.2505778207866138,
      -1.2575938240351867,
      -1.2665277995365851,
      -1.2709364610722131,
      -1.3615438470845198,
      -1.266409834211135,
      -1.253703345610973,
      -1.276164744646025,
      -1.3045408444441378,
      -1.4373040897287899,
      -1.24007763425908,
      -1.2103869491458044,
      -1.27658189881388,
      -1.2121808658898163,
      -1.2698559158193783,
      -1.2865347985597613,
      -1.2295414144343875,
      -1.2411530088612923,
      -1.2028985796730558,
      -1.319696306395768,
      -1.2341750667723088,
      -1.3139250653854893,
      -1.2180806908054171,
      -1.2889458821477138,
      0.0,
      -1.3350497870546416,
      -1.4144087831540013
    ],
    [
      -1.2454737758158518,
      -1.2165703771057066,
      -1.2199565502234562,
      -1.2483173249495914,
      -1.2166414621677117,
      -1.237892138089534,
      -1.2798860999787325,
      -1.245243579016196,
      -1.2030951255327318,
      -1.2017099492005034,
      -1.2453154980962398,
      -1.3085755156911991,
      -1.2166686927636663,
      -1.1925085354074658,
      -1.1986077313912726,
      -1.1982885290861722,
      -1.2184931542498558,
      -1.2208841175782692,
      -1.1806052648817167,
      -1.2008043219267917,
      -1.2383602391361812,
      -1.2730103043261498,
      -1.188308511452649,
      -1.2229195405620745,
      -1.1619408069149886,
      -1.2093610171871907,
      -1.21928431637203,
      0.0,
      -1.2950140504960261
    ],
    [
      -1.212418924460627,
      -1.162234647569221,
      -1.1384747445839114,
      -1.1648918819133147,
      -1.1572105437761675,
      -1.1912121278286854,
      -1.1532245913588803,
      -1.1936923573453968,
      -1.1797400055037006,
      -1.1681212577130764,
      -1.204214991385782,
      -1.138296662654873,
      -1.1707159181807918,
      -1.1688021815684715,
      -1.1423458211216613,
      -1.1626205609729614,
      -1.1643176394242285,
      -1.143925615066418,
      -1.1498731664532618,
      -1.150032832002228,
      -1.193327364382449,
      -1.1194185235003822,
      -1.1534488240647656,
      -1.2231856348965287,
      -1.1542910291864041,
      -1.15445528045073,
      -1.1523345762884516,
      -1.1393365458817664,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.1812431850470544,
      0.19169005971035613,
      0.21148981760383156,
      0.191769493031559,
      0.1921539114798474,
      0.15079751064489977,
      0.1757390418495679,
      0.19970688051583463,
      0.2037658177929489,
      0.19150939582499893,
      0.12774280198169286,
      0.21801778032622088,
      0.18510253424539225,
      0.19963069064629946,
      0.20177071876661823,
      0.15226650436575118,
      0.19941397720215126,
      0.18530638701817037,
      0.20535612999213537,
      0.16652677810887373,
      0.18252761730793132,
      0.17206105009671102,
      0.18592679914287036,
      0.18361236844618656,
      0.17670635952193448,
      0.18831382536591268,
      0.17009207998851728,
      0.13064139474980463
    ],
    [
      0.41813363045416296,
      0.0,
      0.46227476402778,
      0.4642986174611554,
      0.4224351703784064,
      0.4637608401959368,
      0.3511199190373765,
      0.46254962899118757,
      0.4675797618886399,
      0.42416603028753186,
      0.43148419404143623,
      0.3049955509643303,
      0.4783386349463238,
      0.5030380649910957,
      0.47200171548599446,
      0.4287168680838811,
      0.3929222982267103,
      0.48933527590866566,
      0.44765320605067915,
      0.4595753085175034,
      0.3843444423402196,
      0.3659980677575231,
      0.3855133876129726,
      0.40807711187191054,
      0.3667394629340254,
      0.4458725459040479,
      0.42715389358200995,
      0.3628797452953122,
      0.30307123196425456
    ],
    [
      0.35179610545859674,
      0.35275245262821886,
      0.0,
      0.4301321579718542,
      0.3601681320568615,
      0.3662457328024764,
      0.2888572102269049,
      0.36703047614167783,
      0.41459959500259047,
      0.38604871042701605,
      0.3923195069464882,
      0.24309223034210503,
      0.38447964916869193,
      0.39581729405729793,
      0.41447819125354557,
      0.4225291058408127,
      0.3336017046024611,
      0.4074185703977926,
      0.39506815351476154,
      0.4105475519413888,
      0.3114323759975903,
      0.3447971528081091,
      0.32425779796361076,
      0.32598986115452444,
      0.3754772434113678,
      0.32311590247129507,
      0.37184475733832634,
      0.2965379774455832,
      0.24999296640907454
    ],
    [
      0.3633994281279882,
      0.3502631162924488,
      0.4051365793346955,
      0.0,
      0.3347272417347793,
      0.3735259045959092,
      0.3175406964398175,
      0.3548939465141123,
      0.41008721046887286,
      0.3435093180033024,
      0.3740063069037787,
      0.23127877232568994,
      0.4096144495925158,
      0.42453521324385624,
      0.4296972845189868,
      0.41575678727059273,
      0.32899007944336156,
      0.4377440785551423,
      0.39196975877774043,
      0.4597105192400961,
      0.3361160940229524,
      0.3598598519948122,
      0.2974729988227851,
      0.3114763810224608,
      0.3111067126307905,
      0.3294446509257136,
      0.3529812145872071,
      0.2783074353530781,
      0.2314707729183596
    ],
    [
      0.40288448658952714,
      0.3011361329507918,
      0.4131244452031533,
      0.35776352329680927,
      0.0,
      0.36050829821539043,
      0.3336716931395396,
      0.38997829033293696,
      0.3737575428473614,
      0.3725708525003666,
      0.3272367032515613,
      0.2563681514729024,
      0.359134383630092,
      0.3765836407655041,
      0.39099547460377226,
      0.34609786603271586,
      0.3491977133406299,
      0.34814724744184855,
      0.3425060238358899,
      0.3566418127256419,
      0.4037181368193279,
      0.3179820189968934,
      0.33545612494491883,
      0.3748157892941617,
      0.35664549131823775,
      0.3622473914371558,
      0.40380311417087333,
      0.31784045901732005,
      0.2690971311114565
    ],
    [
      0.38394655289753765,
      0.3756556499519834,
      0.4439856922701719,
      0.4494825793713244,
      0.4015481534633105,
      0.0,
      0.30536514175127927,
      0.4994260458139277,
      0.510863444347389,
      0.4536921823097837,
      0.42475391849577915,
      0.22791014299884393,
      0.44224108886025504,
      0.42213657232746726,
      0.4446223924993633,
      0.41515213401865214,
      0.36646936784787076,
      0.44022386120341506,
      0.4252781240386523,
      0.4435072432621898,
      0.36495066686860955,
      0.3177000143389661,
      0.3183706024430384,
      0.3955612046040613,
      0.34919794929530523,
      0.37413376184225067,
      0.3918938483548884,
      0.32771133527182505,
      0.2518595891640616
    ],
    [
      0.32920065739404913,
      0.3301277254575541,
      0.39452046690209275,
      0.3611165461216459,
      0.329767971832462,
      0.31495687661850513,
      0.0,
      0.3292909882258075,
      0.3375078993539331,
      0.33915689130563265,
      0.30497090236874325,
      0.36810216187375544,
      0.3198836622957162,
      0.3187402995001902,
      0.32394733344200266,
      0.33047713998768646,
      0.29681036827948604,
      0.3425847609022532,
      0.3935709263152789,
      0.38527947989290223,
      0.34014995307591156,
      0.32492961624455696,
      0.2965309823486453,
      0.296241473254359,
      0.30300349885985245,
      0.33260252280075964,
      0.32987799146279606,
      0.27903264141571626,
      0.33753389523356336
    ],
    [
      0.3669353960012567,
      0.3644303287308217,
      0.41211966909640085,
      0.412741011743631,
      0.3975099621888838,
      0.46967984896536774,
      0.2916103933867382,
      0.0,
      0.4614605195200252,
      0.43999840678814817,
      0.3936852883700741,
      0.25869338599725666,
      0.4579051492284798,
      0.41146767870873546,
      0.38862124920388297,
      0.4303908121277571,
      0.43576102337522493,
      0.4398206513432781,
      0.4148038775036065,
      0.46509697708668907,
      0.39330092308564746,
      0.30656802104842584,
      0.39230427089602915,
      0.4244797639424245,
      0.3633341191992787,
      0.40736206820418475,
      0.40339477655485956,
      0.31189660599492286,
      0.24883879956177268
    ],
    [
      0.34105088384159865,
      0.3958276306149906,
      0.3897323636299024,
      0.45281506886637324,
      0.3996148668179762,
      0.45571302381484635,
      0.2978661750495275,
      0.4050462807684154,
      0.0,
      0.38796472203437005,
      0.413563991511378,
      0.2171208418552959,
      0.4134203610153564,
      0.44562988462070363,
      0.4798122011171292,
      0.414671739147759,
      0.3098032047994068,
      0.4424670904778423,
      0.403527773929786,
      0.5188245120442719,
      0.35512407705704163,
      0.33905038685640654,
      0.3439343419737302,
      0.4084627417319209,
      0.3079929076081349,
      0.38778519585389937,
      0.3405141424967528,
      0.342339931716688,
      0.2448710976382551
    ],
    [
      0.3753879295260303,
      0.3165157056815169,
      0.36235720579870967,
      0.33781835960268025,
      0.34728082130590576,
      0.3795765591559095,
      0.253181421181617,
      0.41762452399808425,
      0.389990772027609,
      0.0,
      0.37682112982940197,
      0.21476501279261284,
      0.3538672857731886,
      0.37787692843842335,
      0.3736265926447062,
      0.3552410620193529,
      0.33542366725275463,
      0.34435327745185185,
      0.3467050702335175,
      0.33610001201657536,
      0.3338646392311435,
      0.32061290918446317,
      0.34903044308069453,
      0.3141617598199471,
      0.3351906013186734,
      0.3333314132788636,
      0.3641573942109875,
      0.3012500843592263,
      0.21413690014563413
    ],
    [
      0.3825444409143953,
      0.3576295575491917,
      0.4183010326823129,
      0.3916459170740676,
      0.37186116400561564,
      0.4232409283590378,
      0.2932772135207058,
      0.4286393008085557,
      0.4503274006671203,
      0.42907344130431013,
      0.0,
      0.231897425488794,
      0.4337247563528617,
      0.40200932612129847,
      0.3643472238262021,
      0.3991963725165574,
      0.3253085253731749,
      0.410640921110889,
      0.40803736505141774,
      0.4055741693794549,
      0.3099686084102502,
      0.3206640347559375,
      0.32261795300841767,
      0.37384588018667375,
      0.2989718658520857,
      0.36824820941926717,
      0.3707321823346468,
      0.3042338557715727,
      0.2149523317212012
    ],
    [
      0.26681033024720224,
      0.2693702118586143,
      0.29050237692854375,
      0.27872527142380443,
      0.30606309320982406,
      0.2642692843225014,
      0.29398152906038577,
      0.28805795363267506,
      0.2630071229024027,
      0.28108411342231676,
      0.2707856840271716,
      0.0,
      0.27707511796085815,
      0.2768222790645585,
      0.28546984240015383,
      0.2806699997960074,
      0.28244186005909966,
      0.28348003575470315,
      0.3033055307111885,
      0.2797965449783004,
      0.28264630317591166,
      0.3096052246306087,
      0.30855757551196183,
      0.2628560358522636,
      0.2820293144266315,
      0.2979228467377728,
      0.28632114143479215,
      0.2846816229963667,
      0.3254724119538741
    ],
    [
      0.42894143332222767,
      0.3935034045288599,
      0.47742274507145943,
      0.49024266784049364,
      0.4247961505616378,
      0.4153967585912195,
      0.31107111551980227,
      0.4380726586849941,
      0.41799864704193457,
      0.47407629926140027,
      0.41245181960376254,
      0.24603495087334437,
      0.0,
      0.4621748132525403,
      0.45065942211489873,
      0.4767150203721766,
      0.3596263146553229,
      0.5097245303026385,
      0.48648739707459754,
      0.4737176268923424,
      0.4431163066842978,
      0.38597142654834693,
      0.3592018962155332,
      0.3660927260229947,
      0.39761616316826665,
      0.429398540542987,
      0.48052035699073703,
      0.3155651160824755,
      0.2586394819527924
    ],
    [
      0.4221046152723067,
      0.47680886962606217,
      0.5053532215935985,
      0.5354794975901753,
      0.42295666417636646,
      0.45001233876818336,
      0.3709873244407065,
      0.43360065150438465,
      0.5508848403846918,
      0.44788944891811866,
      0.41713253293805197,
      0.29796878603926724,
      0.4966409684587876,
      0.0,
      0.5598972487257137,
      0.4997030957439559,
      0.3878801745479292,
      0.5208762982928277,
      0.4766649111494543,
      0.5402720767340248,
      0.4591954086266392,
      0.40502325913698956,
      0.4261377095699479,
      0.39274166170259583,
      0.4342815603919037,
      0.43120752243931815,
      0.47433764050693705,
      0.40760663867824065,
      0.343046641969178
    ],
    [
      0.41957121482352977,
      0.44287598199146494,
      0.4510616218916461,
      0.49433228150782593,
      0.4168463996594385,
      0.4169086718102255,
      0.345462638711864,
      0.39395620482030314,
      0.48357587485161035,
      0.401995563534995,
      0.37801873204385616,
      0.2883524289563524,
      0.4423821572117739,
      0.5209088838038713,
      0.0,
      0.4554137283971267,
      0.38907043285926957,
      0.4936982804618115,
      0.4268683315997057,
      0.5166423197400063,
      0.3626546529290895,
      0.4321804159163043,
      0.41990003173353063,
      0.42248628394078347,
      0.4227378369740289,
      0.4194578919508718,
      0.37958624917309414,
      0.31644498231345985,
      0.2762845052687659
    ],
    [
      0.37529748421687437,
      0.39216333416517934,
      0.4408810244410055,
      0.4953040091558052,
      0.4269903315239223,
      0.41585847868180315,
      0.32599316664861266,
      0.4230961626105767,
      0.4681585802010937,
      0.4292474470501113,
      0.42726013231988613,
      0.2647898208643935,
      0.4978939592267584,
      0.4476317051354861,
      0.4598334230776935,
      0.0,
      0.38321015844580897,
      0.4413397188882353,
      0.47621716636642164,
      0.45589722669354504,
      0.40714977286914955,
      0.406473185497376,
      0.3612732738685047,
      0.3703004380521244,
      0.3909811893489201,
      0.41613863785948535,
      0.45822954339186195,
      0.3606986895501063,
      0.2670625901603074
    ],
    [
      0.33357488744585573,
      0.37478268512711344,
      0.37907875795847623,
      0.37806420401725993,
      0.3972483915900549,
      0.3520591971426277,
      0.297163297792401,
      0.391178179180494,
      0.35639673285313833,
      0.34548963976042857,
      0.34092891799916303,
      0.295537732469791,
      0.38157196154415285,
      0.39198826223161043,
      0.39404500592039216,
      0.38338645759665324,
      0.0,
      0.37978414494506274,
      0.35183256203446334,
      0.38446079787779897,
      0.37538971545539446,
      0.3249554078143988,
      0.3540291184853701,
      0.3381356722932465,
      0.3680939418266831,
      0.35181991043843186,
      0.3710724363537261,
      0.36209052739274195,
      0.28612375438981963
    ],
    [
      0.416905173862961,
      0.42863161381397696,
      0.47639890506998395,
      0.5322013346190878,
      0.4012365630318877,
      0.4455968000466297,
      0.35733300756992725,
      0.4466385749950863,
      0.504867462492953,
      0.42880541868312094,
      0.415874298139592,
      0.29081763245991343,
      0.5185643564113718,
      0.5061528143724658,
      0.5236623428729992,
      0.4678465952971842,
      0.3895399283618459,
      0.0,
      0.4523303207076208,
      0.6158352102485919,
      0.41440533560629134,
      0.4406400288676431,
      0.4124215474688364,
      0.39484913407492783,
      0.41281921470840555,
      0.403771351244697,
      0.43608312451971365,
      0.37661072226502945,
      0.3145138073197733
    ],
    [
      0.4136651978043542,
      0.43284831980219085,
      0.48769390400182133,
      0.4585639660390184,
      0.37514089871178546,
      0.45055306709763476,
      0.4005242884586766,
      0.4355114074582358,
      0.4753250145204684,
      0.4450277544694776,
      0.44366952770085644,
      0.2855094909367677,
      0.504881672579061,
      0.4456441246775411,
      0.45175690823553016,
      0.4803526909540039,
      0.41040680738942226,
      0.46295406313588994,
      0.0,
      0.49315109521116973,
      0.3678577303603652,
      0.3735643027922906,
      0.4064760410033228,
      0.3847934992305053,
      0.4033407500648385,
      0.42205749403805215,
      0.45356168504134886,
      0.3629375698599375,
      0.2992275510602209
    ],
    [
      0.39136821585112846,
      0.3621776835603634,
      0.4586152692961005,
      0.49384188357594505,
      0.31992005194252693,
      0.37193721820314485,
      0.32126322684488406,
      0.38900203754720186,
      0.48172141812161984,
      0.39017324949244725,
      0.3521365145900688,
      0.21558007367148901,
      0.4377159259351333,
      0.46950677190519774,
      0.4698052330632083,
      0.4228743403043067,
      0.36090308664279114,
      0.5050796327488534,
      0.41192604586927684,
      0.0,
      0.42130018804094993,
      0.401286853058171,
      0.3801248575950791,
      0.3532798081912123,
      0.4292263132108194,
      0.37495231916586613,
      0.40374876391147363,
      0.3180827370525554,
      0.259004433140837
    ],
    [
      0.36472307434678175,
      0.34568841579131493,
      0.39439701560283535,
      0.35752519883814404,
      0.39287160977494695,
      0.34721117577974026,
      0.28247550596965265,
      0.40332305281293945,
      0.3487463632320362,
      0.3694087486433346,
      0.3311026677546747,
      0.24526719037674316,
      0.4217468473287256,
      0.4526402528252085,
      0.40789214060181367,
      0.4106219210128761,
      0.34723736618247725,
      0.4045333407570513,
      0.3778663086743719,
      0.4039014772537466,
      0.0,
      0.30782276264233044,
      0.34199392695545483,
      0.3128071413637781,
      0.3717791936297312,
      0.36830737727068064,
      0.45717903614139765,
      0.33608250255712857,
      0.22893549077812603
    ],
    [
      0.388983986213677,
      0.33586407679943275,
      0.38832473898777753,
      0.44173151042555814,
      0.36032790204681,
      0.35709334003001714,
      0.321131968044041,
      0.3257249631760897,
      0.4195831412142432,
      0.35766183742077007,
      0.3298983620422993,
      0.2787940914873972,
      0.39047424196633895,
      0.391234413423031,
      0.4547290089802323,
      0.3666267004624124,
      0.35086486264366057,
      0.4465247935484058,
      0.3850787886150653,
      0.41434168960451756,
      0.35581172482026013,
      0.0,
      0.38819520704237864,
      0.3003798010957621,
      0.39352168639201834,
      0.3460338945941288,
      0.35211604011964304,
      0.3653859171961318,
      0.2684433732217588
    ],
    [
      0.3445490978198824,
      0.3875824300061206,
      0.3699984391003961,
      0.36716124030825315,
      0.3584140803721161,
      0.3068954314305381,
      0.2923864110897736,
      0.33326283052073236,
      0.4029580349848654,
      0.36637196656496096,
      0.347778044976937,
      0.23795577902500042,
      0.39037318712782354,
      0.4061998495706982,
      0.41606260728854516,
      0.39349216010667654,
      0.366142051605312,
      0.39875094253737675,
      0.3695187820706629,
      0.40718986984583827,
      0.33733258051727,
      0.30674563318200065,
      0.0,
      0.36057411581809107,
      0.4281309049473949,
      0.3828938472045622,
      0.38437104857422977,
      0.33895925558130524,
      0.2721173724833046
    ],
    [
      0.28133747014666866,
      0.30728274219698837,
      0.29469540332539923,
      0.30225305869559316,
      0.26761391665248535,
      0.30913568480419373,
      0.20211413438996995,
      0.34845073815594274,
      0.36947648377913556,
      0.2866697115600627,
      0.31945851457979746,
      0.1654910842715398,
      0.30804771161451994,
      0.31072850912783756,
      0.3357275437122098,
      0.2887385656464323,
      0.27882927853883266,
      0.30815300929748646,
      0.3081125586865143,
      0.33057854427603117,
      0.24700911157328398,
      0.20729503879287914,
      0.3090913016276746,
      0.0,
      0.279332438641841,
      0.30467657490615285,
      0.28577304651414237,
      0.2974265697796923,
      0.15795716730942666
    ],
    [
      0.3042287196894313,
      0.32552575296655073,
      0.40778029263933635,
      0.3514098909499126,
      0.3141552283025595,
      0.298261821954396,
      0.2669028617143132,
      0.3115254223149364,
      0.328380305952946,
      0.37740782276392837,
      0.29124006710048267,
      0.21654292183837676,
      0.37589128466287236,
      0.35300131869849705,
      0.4002832956977427,
      0.3948585397565807,
      0.363364021978025,
      0.32650964122839166,
      0.3586933257478102,
      0.39981563236132645,
      0.3365873384490905,
      0.3381585440192445,
      0.4454041150060364,
      0.3242603682923466,
      0.0,
      0.31547545209320926,
      0.4029969093710759,
      0.29695163495423005,
      0.24716601768700164
    ],
    [
      0.45528615357001434,
      0.4732390450080193,
      0.47463255480439726,
      0.47744499998072,
      0.4926629374635385,
      0.434565715905443,
      0.3688381211009395,
      0.5054289171416388,
      0.4671351633478815,
      0.45386818893475644,
      0.45848692219559006,
      0.33256751478170044,
      0.5229443096328545,
      0.5134910595215669,
      0.5219723602476696,
      0.5323631217978733,
      0.4624402946096724,
      0.4712312699392258,
      0.4622082884790515,
      0.47594616412825475,
      0.4910845894708069,
      0.4485837304669158,
      0.4646972099165383,
      0.4131658809271799,
      0.4377005976071011,
      0.0,
      0.4724619936542407,
      0.4254824265628634,
      0.34530865185412596
    ],
    [
      0.3975168709441783,
      0.3878101065989157,
      0.4374878107515563,
      0.4304718075029834,
      0.421537832001585,
      0.417129170465957,
      0.3265217844536503,
      0.4216557973270352,
      0.43436228592719717,
      0.4119008868921452,
      0.38352478709403237,
      0.25076154180938026,
      0.4479879972790901,
      0.47767868239236577,
      0.4114837327242902,
      0.4758847656483538,
      0.41820971571879184,
      0.40153083297840886,
      0.45852421710378266,
      0.4469126226768778,
      0.48516705186511433,
      0.36836932514240206,
      0.4538905647658613,
      0.3741405661526809,
      0.469984940732753,
      0.39911974939045636,
      0.0,
      0.3530158444835285,
      0.2736568483841688
    ],
    [
      0.263094219085807,
      0.29199761779595224,
      0.28861144467820266,
      0.26025066995206747,
      0.2919265327339471,
      0.2706758568121248,
      0.22868189492292634,
      0.26332441588546285,
      0.30547286936892704,
      0.3068580457011554,
      0.26325249680541907,
      0.1999924792104597,
      0.29189930213799253,
      0.316059459494193,
      0.3099602635103862,
      0.3102794658154866,
      0.290074840651803,
      0.2876838773233896,
      0.32796273001994214,
      0.3077636729748672,
      0.2702077557654776,
      0.23555769057550902,
      0.3202594834490098,
      0.2856484543395843,
      0.34662718798667025,
      0.29920697771446814,
      0.2892836785296289,
      0.0,
      0.2135539444056327
    ],
    [
      0.3165179645049152,
      0.36670224139632124,
      0.39046214438163074,
      0.3640450070522274,
      0.37172634518937464,
      0.3377247611368568,
      0.3757122976066618,
      0.33524453162014534,
      0.3491968834618415,
      0.36081563125246574,
      0.3247218975797601,
      0.3906402263106692,
      0.35822097078475035,
      0.36013470739707065,
      0.38659106784388086,
      0.36631632799258074,
      0.36461924954131364,
      0.38501127389912404,
      0.3790637225122804,
      0.3789040569633142,
      0.33560952458309323,
      0.40951836546515996,
      0.3754880649007766,
      0.3057512540690135,
      0.374645859779138,
      0.37448160851481216,
      0.37660231267709054,
      0.3896003430837758,
      0.0
    ]
  ],
  "row_avgs": [
    0.1828886039562169,
    0.4212153346143242,
    0.3585867344921794,
    0.3558793858450659,
    0.35356821211738504,
    0.3917014021397215,
    0.3317827011702091,
    0.39157896349477866,
    0.38051955138891996,
    0.3378660529057172,
    0.3646968358416434,
    0.2850646663743034,
    0.41375842106336014,
    0.4495247002830841,
    0.4153455213884502,
    0.41019180893968044,
    0.35858151071202676,
    0.4366197363975899,
    0.42239274366552815,
    0.39166264830473757,
    0.3619316823177872,
    0.3669600736290664,
    0.3597917140950238,
    0.28612342545009767,
    0.3383135195782376,
    0.4591156493946636,
    0.40843707640026944,
    0.2834345474159462,
    0.3644310229107159
  ],
  "col_avgs": [
    0.3678484150133192,
    0.3614441434977862,
    0.40380856961356226,
    0.4063697178067232,
    0.36853992520573453,
    0.3735945248995165,
    0.3061368553113426,
    0.38275975081546953,
    0.4086831518313701,
    0.3826677909671216,
    0.3620740448941086,
    0.25659179369556656,
    0.4048192561804488,
    0.405890547996918,
    0.4114861355806874,
    0.3987908608040383,
    0.3511219607620789,
    0.40460769278692904,
    0.3916817019175611,
    0.420405012305693,
    0.3604293494932162,
    0.3429443173515213,
    0.3594532813681205,
    0.3493322002658716,
    0.36407576123968155,
    0.363491857777333,
    0.3824611481201569,
    0.32856233042926186,
    0.26189214835559116
  ],
  "combined_avgs": [
    0.27536850948476804,
    0.39132973905605517,
    0.3811976520528708,
    0.38112455182589455,
    0.36105406866155976,
    0.382647963519619,
    0.3189597782407758,
    0.3871693571551241,
    0.39460135161014503,
    0.3602669219364194,
    0.363385440367876,
    0.27082823003493495,
    0.40928883862190446,
    0.4277076241400011,
    0.4134158284845688,
    0.4044913348718594,
    0.35485173573705286,
    0.4206137145922595,
    0.40703722279154464,
    0.4060338303052153,
    0.3611805159055017,
    0.3549521954902939,
    0.3596224977315722,
    0.3177278128579846,
    0.35119464040895954,
    0.4113037535859983,
    0.3954491122602132,
    0.30599843892260403,
    0.31316158563315355
  ],
  "gppm": [
    562.7042722013549,
    592.7370454301046,
    569.3283339377008,
    566.0991281075172,
    584.7273565890532,
    583.0015580539593,
    616.0590199794104,
    576.5169741299892,
    566.4519740336632,
    577.1488850460871,
    591.7294125688057,
    640.4680803623427,
    567.5333842254037,
    567.427946647312,
    566.2952992472244,
    571.8023679982487,
    594.9750719472004,
    569.8348994329866,
    575.4627658174519,
    559.5495578547816,
    580.8802236575318,
    598.7852685075495,
    588.3168599230568,
    590.8480430522005,
    586.3212857328452,
    587.8850448115315,
    580.1134269608192,
    604.5621715874421,
    640.2028893189935
  ],
  "gppm_normalized": [
    1.322439804500553,
    1.4347899102640906,
    1.381416718394933,
    1.3757316298038038,
    1.4146077802418655,
    1.4148322258181243,
    1.4933277633495805,
    1.404432257125816,
    1.3731381705163606,
    1.400453255863288,
    1.432812888493079,
    1.5686070670156151,
    1.3741250400138678,
    1.3742273840166954,
    1.3781067022315925,
    1.3870695745334962,
    1.4448457256531637,
    1.3836638850578953,
    1.3959604854707932,
    1.357729307867656,
    1.4122406508497514,
    1.453376242402575,
    1.4270323223454653,
    1.4375891363659625,
    1.4238634492863411,
    1.4265300712570268,
    1.405488580206974,
    1.4671411054689485,
    1.5529289592187485
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377
  ],
  "response_lengths": [
    4117,
    2310,
    2584,
    2984,
    2401,
    2528,
    2462,
    2338,
    2729,
    2416,
    2326,
    2912,
    2575,
    2449,
    2333,
    2542,
    2464,
    2368,
    2369,
    2581,
    2518,
    2377,
    2402,
    2952,
    2571,
    1997,
    2148,
    2584,
    2204
  ]
}