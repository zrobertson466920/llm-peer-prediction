{
  "example_idx": 95,
  "reference": "Under review as a conference paper at ICLR 2023\n\nLOCAL KL CONVERGENCE RATE FOR STEIN VARIATIONAL GRADIENT DESCENT WITH REWEIGHTED KERNEL\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe study the convergence properties of Stein Variational Gradient Descent (SVGD) algorithm for sampling from a non-normalized probabilistic distribution p∗(x) ∝ exp(−f∗(x)). Compared with Kernelized Stein Discrepancy (KSD) convergence analyzed in previous literature, KL convergence as a more convincing criterion can better explain the effectiveness of SVGD in real-world applications. In the population limit, SVGD performs smoothed gradient descent with kernel integral operator. Notably, SVGD with smoothing kernels suffers from gradient vanishing in low-density areas, which makes the error term between smoothed gradient and the Wasserstein gradient not controllable. In this context, we introduce a reweighted kernel to amplify the smoothed gradient in low-density areas, which leads to a bounded error term. When the p∗(x) satisfies log-Sobolev inequality, we develop the convergence rate for SVGD in KL divergence with the reweighted kernel. Our analysis points out the defects of conventional smoothing kernels in SVGD and provides the convergence rate for SVGD in KL divergence.\n\n1\n\nINTRODUCTION\n\nSampling from non-normalized distributions is a crucial task in statistics. In particular, in Bayesian inference, Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) are considered two mainstream lines to handle the intractable integration of posterior distributions. On the one hand, although methods based on MCMC, e.g., Langevin Monte Carlo (Durmus & Moulines, 2019; Welling & Teh, 2011) (LMC) and Metropolis-adjusted Langevin algorithm (Xifara et al., 2014) (MALA), are able to provide approximate target distributions with arbitrarily small error (Wibisono, 2018), the sample efficiency is low due to the lack of repulsive force between samples (Duncan et al., 2019; Korba et al., 2020). On the other hand, VI-based sampling methods (Blei et al., 2017; Ranganath et al., 2014) can improve the sampling efficiency by reformulating inference as an optimization problem. However, restricting the search space of the optimization problem to some parametric distributions in VI usually causes a huge gap between its solution and the target distribution p∗.\n\nInspired by conventional VI, a series of recent works analyze LMC as the optimization problem of Kullback-Leibler (KL) divergence (Wibisono, 2018; Bernton, 2018; Durmus et al., 2019), i.e.,\n\narg min p∈P2(Rd)\n\nHp∗ (p) := DKL(p∥p∗) =\n\n(cid:90)\n\np(x) ln\n\np(x) p∗(x)\n\ndx\n\n(1)\n\nwhere P2(Rd) is the set of Radon-Nikodym derivatives of probability measures ν over Lebesgue measure such that p(x) = dν(x)/dx, (cid:82) ∥x∥2p(x)dx < ∞. LMC is considered as a discrete scheme of the gradient flow of the relative entropy by driving particles with stochastic and energy-induced force. Besides, to take the best of both MCMC and VI, Stein Variational Gradient Descent (Liu & Wang, 2016) (SVGD) was proposed as a non-parametric VI method. It replaces the stochastic force in LMC with the interaction between particles and approximates the target distribution by a driving force in Reproducing Kernel Hilbert space (RKHS). It means the gradient flow of SVGD is defined by the functional derivative projection of Eq. 1 to RKHS. The empirical performance of SVGD and its variants have been largely demonstrated in various tasks such as learning deep probabilistic models (Liu & Wang, 2016; Pu et al., 2017), Bayesian inference (Liu & Wang, 2016; Feng et al., 2017; Detommaso et al., 2018), and reinforcement learning (Liu et al., 2017).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nIn addition to rich applications, there is a lot of work on the theoretical analysis of SVGD. For example, Kernelized Stein Discrepancy (KSD) convergence properties of SVGD under asymptotic and non-asymptotic settings are investigated by Liu (2017); Lu et al. (2019) and Korba et al. (2020); Salim et al. (2021; 2022), respectively. However, different from the convergence of KL divergence in the analysis of LMC (Cheng & Bartlett, 2018; Vempala & Wibisono, 2019), KSD convergence cannot deduce the effectiveness of SVGD in some real-world applications, e.g., posterior sampling (Welling & Teh, 2011) and non-convex learning (Raginsky et al., 2017). Then, to provide KL convergence, some other works (Duncan et al., 2019; Korba et al., 2020) present a linear convergence of SVGD with Stein log-Sobolev inequality (SLSI) (Duncan et al., 2019). Nonetheless, different from the clear meaning and criteria of standard log-Sobolev inequality (LSI) in the analysis of LMC (Vempala & Wibisono, 2019), the establishment of SLSI requires the property of the coupling of designed smoothing kernels and the target distribution, which can hardly be verified in commonly used kernels (Duncan et al., 2019). In addition, SLSI in higher dimensions is more challenging to hold.\n\nTo fill these gaps, in this paper, we aim to provide the convergence rate of SVGD (in the infinite particle regime) in terms of KL objective, when p∗ = e−f∗ satisfies standard LSI. Specifically, we first point out that the SVGD with smoothing kernel, e.g., RBF kernel, suffers from gradient vanishing in low-density areas due to the extra pt(x) scaling. Then, we denote the importance of reweighted kernels by dividing pt(x) or p∗(x), where the scaling of smoothed gradients can be normalized. With the reweighting scaling p−1/2 (y) for kernel k(x, y) and regularity conditions, SLSI in higher dimensions can be nearly established with an additional term controlled by kernel approximation error. Finally, by choosing a proper reweighted smoothing kernel, the KL divergence of SVGD dynamics obtains a local linear convergence rate to any neighborhood of p∗(x) under mild assumptions when the initialization p0(x) is relatively close to p∗(x).\n\n(x)p−1/2\n\n∗\n\n∗\n\nThe main contributions of the paper are as follows:\n\n• We introduce reweighted kernels to SVGD which replaces traditional smoothing kernels\n\nand overcomes the gradient vanishing problem in low-density areas.\n\n• We study the KL convergence rate of SVGD algorithm. Under the standard LSI and some mild assumptions, we show SVGD with a reweighted kernel has a local linear convergence rate to any neighborhood of p∗(x).\n\n2 PRELIMINARIES\n\nIn this section, we first introduce important notations used in the following sections. Then, we explain how to optimize functionals on Wasserstein space by continuous updates in the infinite particle regime. After that, we show that the key condition LSI on the target distribution to obtain the KL convergence rate of LMC. However, the convergence rate of SVGD dynamics is non-trivial with this assumption.\n\nIn following sections, bold letters x, y, z denote vectors in Rd, and B(x, r) means the Notations. open ball centered at x with radius r > 0. For function f : Rd → R, ∇f (·) and ∇2f (·) refer to its gradient and Hessian matrix respectively. For function f : Rd → Rd, ∇f (·) and ∇ · f (·) present the Jacobian matrix and divergence. For function with multiple variables, ∇i means the gradient w.r.t i-th variable. The distributions are assumed to be absolutely continuous with respect to the Lebesgue measure, which produces density function p. The probability density function of the target posterior is denoted by p∗. The density at time t is pt. Notation ∥ · ∥ denotes 2-norm for both vectors and matrix. In Hilbert space H equipped with the inner product ⟨·, ·⟩H, the norm is induced as ∥ · ∥H. The set P2(X ) is consist of probability measure μ on X with finite second order moment. ̃k denotes a function smoother, such as exp(−∥x∥2), max{0, 1 − ∥x∥}.\n\n2.1 OPTIMIZATION IN THE WASSERSTEIN SPACE\n\nSampling algorithms can be considered as optimizing some given functionals in the Wasserstein space as Eq. 1. Generally, they only update particles, which causes the evolution of the particles’ distribution. Such an evolution finally affects the objective functional.\n\nIn particular, given initial distribution x0 ∼ p0(x) and function class H, suppose the update of xt is dxt = φt(xt), (2)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nwhere φt : Rd → Rd ∈ H. By the continuity equation, we have the differential equation for pt(x),\n\ndpt(x) dt\n\n= −∇ · (pt(x)φt(x)).\n\nFor any suitable functional F w.r.t. pt, its evolution can be presented as\n\nd dt\n\nF(pt) =\n\n(cid:90)\n\nRd\n\nδF δp\n\n(pt)∂tptdx =\n\n(cid:90)\n\nRd\n\n∇\n\nδF δp\n\n(pt) · φt(x)dpt.\n\nwhere δF/δp denotes the L2(Rd)-functional derivative (Villani, 2009; Duncan et al., 2019). Assuming that F = Hp∗ , we have Proposition 2.1. The evolution of KL divergence with Eq. 2 is dHp∗ (pt) dt\n\npt(x)φt(x)⊤∇ ln\n\np∗(x) pt(x)\n\n= −\n\ndx.\n\n(3)\n\n(cid:90)\n\nProposition 2.1 is a direct result of the functional derivative for KL divergence, where δF/δp = ln p + 1 + ln p∗ (Chapter 15 of Villani (2009)). By choosing φt which decreases KL divergence via Eq. 3, we can optimize pt to approach p∗.\n\n2.2 LOG-SOBOLEV INEQUALITY\n\nIn the Wasserstein space optimization literature, LSI is particularly crucial to obtaining the convergence rate, which is an analogue to Polyak-Lojasiewicz (PL) inequality in Euclidean space. LSI applies to a wider class of measures than log-concave distributions and can be checked by BakryEmery criterion Bakry & Émery (1985). Specifically, bounded perturbation and Lipschitz mapping can preserve the establishment of LSI Vempala & Wibisono (2019), where log-concavity would be failed. For example, subtracting some small Gaussians from a strongly log-concave distribution will destroy the log-concavity of the original distribution, while it still satisfies LSI as long as the Gaussians we subtract are small enough. When the target distribution p∗ satisfies μ-LSI, it denotes (cid:2)g2(cid:3) ln Ep∗\n\n(cid:2)g2 ln g2(cid:3) − Ep∗\n\n∥∇g∥2(cid:105)\n\n(cid:2)g2(cid:3) ≤\n\nEp∗\n\nEp∗\n\n(4)\n\n(cid:104)\n\n,\n\n2 μ\n\nfor any differentiable function g ∈ L2(ν). Such an inequality usually provides some connection between the sufficient descent of functional evolution and its exact values.\n\nCoupling log-Sobolev inequality with Langevin dynamics. rithm, Langevin dynamics Vempala & Wibisono (2019) chooses\n\nIn particular, the most popular algo-\n\nφt(x) = ∇ ln\n\np∗(x) pt(x)\n\n,\n\ndpt(x) dt\n\n(cid:18)\n\n= ∇ ·\n\npt(x)∇ ln\n\n(cid:19)\n\npt(x) p∗(x)\n\nto decrease the KL functional (Eq. 3) which is equivalent to the particles’ update,\n\ndxt = −∇f∗(x)dt +\n\n2dBt,\n\n(5)\n\n√\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nwhere Bt is standard Brownian motion. The introduction of randomness can also convert ∇ ln pt(x) to a tractable form and the dynamics becomes (cid:90)\n\ndHp∗ (pt) dt\n\n= −\n\npt(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx\n\n(6)\n\nwhere the absolute value of RHS in Eq. 6 is called relative Fisher information. Taking g2 = pt/p∗, we have\n\nHp∗ (pt) ≤\n\n1 2μ\n\n(cid:90)\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\npt(x)\n\n∇ ln\n\ndx.\n\n(7)\n\nThen LSI provides a lower bound for gradient norm, leading to sufficient descent for KL divergence. Combining Eq. 7 and Eq. 6, we have\n\ndHp∗ (pt) dt\n\n≤ −2μHp∗ (pt),\n\n(8)\n\nfor some μ > 0. By applying Gronwall’s lemma, Eq. 8 yields Hp∗ (pt) ≤ Hp∗ (p0) exp(−2μt), which indicates the linear convergence rate of Langevin dynamics. Nonetheless, coupling LSI with SVGD is challenging due to the introduction of RKHS.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nKL divergence descent with asymptotic SVGD. As shown in Lemma 3.2 of Liu & Wang (2016), the key point of continuous SVGD is to minimize the RHS of Eq. 3 in the unit ball of the RKHS H as follows\n\nS(pt, p∗) = max φ∈H\n\n(cid:90)\n\npt(x)φ(x)⊤∇ ln\n\np∗(x) pt(x)\n\ndx,\n\nsuch that ∥φ∥H ≤ 1\n\n(9)\n\nwhere S(pt, p∗) is called Kernelized Stein’s discrepancy (KSD). Assume that the RKHS is associated with a kernel k(x, y) : Rd × Rd → R, such that k(x, y) = ⟨Φ(x), Φ(y)⟩H0 , and H is the d-times Cartesian product space of H0 that contains Frobenius-normalized linear functions from H to Rd, i.e., ⟨φt, φt⟩H ≤ 1. Note that Eq. 10 defines a functional gradient in RKHS, which indicates the steepest KL decreasing direction with a normalized functional vector. Using integration by parts, Eq. 9 can be rewritten as\n\n(cid:90)\n\nφt = arg max\n\nφ∈H\n\npt(x)(φ(x)⊤∇ ln p∗(x) + ∇φ(x))dx,\n\nsuch that ∥φ∥H ≤ S(pt, q).\n\n(10)\n\nThe explicit form for φt(x) is presented as follows. Proposition 2.2. Assume that φt satisfies Eq. 10. Then we have (cid:90)\n\nφt(x) =\n\npt(y) [∇ ln p∗(y)k(x, y) + ∇1k(y, x)] dy,\n\n(11)\n\nwhere φt(x) can be estimated by particle samples from pt.\n\nProposition 2.2 (as Lemma 3.2 proved in Liu & Wang (2016)) makes Eq. 2 become a practically tractable algorithm by Monte Carlo estimation of Eq. 11, where particle samples are from pt(y). Note that Eq. 2 and Eq. 11 naturally lead to the algorithm of SVGD.\n\nCombining Proposition 2.1 and 2.2, the dissipation of the KL divergence along continuous SVGD can be obtained,\n\ndHp∗ (pt) dt\n\n= −\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nRd\n\nk(x, y)pt(x)pt(y) ·\n\n∇ ln\n\n(cid:20)\n\npt(x) p∗(x)\n\n· ∇ ln\n\n(cid:21)\n\npt(y) p∗(y)\n\ndydx.\n\n(12)\n\nWhen the kernel is strictly positive definite, the RHS of Eq. 12 is negative, leading to the decrease of KL divergence. Unlike the sufficient descent bounded by the functional value in Eq. 7, LSI cannot be conducted on the RHS of Eq. 12 due to the kernelization, which also causes the KL convergence rate to be unknown. Instead, previous works (Liu, 2017; Lu et al., 2019) provided an O(1/t) KSD convergence as\n\n(cid:90) t\n\n0\n\nS(ps, p∗)ds ≤\n\nHp∗ (p0) t\n\n.\n\n(13)\n\nmin 0≤s≤t\n\nS(pt, p∗) ≤\n\n1 t\n\n3 KL CONVERGENCE OF SVGD\n\nIn this section, we first show, compared with KSD convergence proved in most previous works on SVGD Liu (2017); Lu et al. (2019); Korba et al. (2020); Salim et al. (2021; 2022), KL convergence is more powerful in explaining the practical performance of real-world applications. After that, we explain Stein log-Sobolev Inequality (SLSI) the necessary condition for analyzing KL convergence of SVGD can hardly be verified. In order to investigate more reasonable conditions, our assumptions are proposed and validated empirically in some simple cases. Finally, we provide the main theorem, i.e., the KL convergence of SVGD under these mild assumptions. Due to the page limit, we left the comparison with previous works by list in Appendix A.\n\n3.1 SAMPLING TASKS REQUIRE KL CONVERGENCE\n\nSampling algorithms are widely used in real-world applications for solving corresponding machine learning problems. Although different tasks usually require different criteria for convergence analysis, most of these criteria can be deduced by KL convergence.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nIn Bayesian learning Welling & Teh (2011), people expect to capture parameter uncertainty via Markov chain Monte Carlo (MCMC) techniques. Specifically, they prefer to sample the parameter vector w from a posterior distribution presented as\n\np(w|z) ∝ p(w)\n\nn (cid:89)\n\ni=1\n\np(zi|w),\n\nwhere z := {zi}N\n\ni=1 denotes a given dataset. The update of w in (Neal et al., 2011) (cid:33)\n\n(cid:32)\n\n∆w =\n\nηt∇ log p(w) + ηt\n\nN (cid:88)\n\n∇ log p(zi|w) + (cid:112)2ηt\n\n(cid:90) 1\n\ndBt\n\n,\n\n(14)\n\nis equivalent to minimizing the KL divergence w.r.t between p(w|z) and p∗(w|z) where\n\ni=1\n\n0\n\np∗(w|z) = arg min\n\np′\n\nHp(w) (cid:81)n\n\ni=1 p(zi|w)(p′).\n\nIt means the convergence of Bayesian learning is directly dependent on KL convergence. Another important application of sampling algorithms is to minimize the expected excess risk as\n\nE [F ( ˆw)] − F ∗\n\n(15)\n\nwhere F denotes the objective function of the stochastic optimization under the unknown data distribution P\n\nF (w) = EP [f (w, z)] =\n\nf (w, z)P (dz),\n\n(cid:90)\n\nand F ∗ denotes inf w∈Rd F (w). When F is L-smooth, previous gradient-based MCMC methods would like to analyze the convergence with the general framework by E [F ( ˆwk)] − F ∗ =E [F ( ˆwk)] − E [F ( ˆw∗)] + E [F ( ˆw∗)] − F ∗\n\nz\n\n(cid:90)\n\n(cid:90)\n\n=\n\n≤\n\nP (n)(dz)\n\n(cid:20)(cid:90)\n\nRd\n\nF (w) ̃pk,z(w)dw −\n\nF (w)p∗,z(w)dw\n\n(cid:21)\n\n+ E [F ( ˆw∗)] − F ∗\n\n(cid:90)\n\nRd\n\nP (n)(dz) [LC · W2( ̃pk,z, p∗,z)] (cid:125)\n\n(cid:123)(cid:122) Training error\n\n(cid:124)\n\n+E [F ( ˆw∗)] − F ∗\n\n(16) where the n-tuple (data) z = {z1, z2, . . . , zn} of i.i.d. samples are drawn from P . It means the minimization of Wasserstein 2 distance between ̃pt,z (MCMC samples at time t) and p∗,z(x) ∝ p(w|z) leads to convergence of expected excess risk. The aforementioned results can be directly deduced by the KL convergence (Raginsky et al., 2017; Xu et al., 2018) with\n\nW2( ̃pk,z, p∗,z) ≤ C ·\n\n(cid:32)\n\n(cid:113)\n\nHp∗,z ( ̃pk,z) +\n\n(cid:18) Hp∗,z ( ̃pk,z) 2\n\n(cid:19)1/4(cid:33)\n\n.\n\nUnfortunately, the connection between W2( ̃pk,z, p∗,z) and S( ̃pk,z, p∗,z) depends on the choice of RKHS, which is highly specialized and non-general. From a theoretical perspective, when the RKHS is over-smooth with a sufficient large bandwidth, k(x, y) = σ−d exp(−∥x − y∥2/2σ2) with large σ, the corresponding kernelized gradient tend to diminish, i.e., limσ→∞ Ept[k(x, y)∇ ln pt(y) p∗(y) ] = 0. That means the KSD can be arbitrarily small with improper RKHS choices. In this condition, the convergence of KSD does not make much sense about the quality of pt, since it can be simply controlled by some special RKHS.\n\n3.2 KL CONVERGENCE OF SVGD WITH DIFFERENT ASSUMPTIONS\n\nTo investigate KL convergence of SVGD, some previous works (Duncan et al., 2019; Korba et al., 2020; Salim et al., 2021) introduce the following assumption. Assumption 1. The probability density p∗ satisfies Stein log-Sobolev inequality (SLSI) with a constant μ > 0, if for any pt ∈ P2(Rd), it has\n\nS(pt, p∗).\n\n(17)\n\nHp∗ (pt) ≤\n\n1 2μ\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nWe can immediately obtain\n\nHp∗ (pt) ≤\n\n1 2μ\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nRd\n\nk(x, y)pt(x)pt(y) ·\n\n∇ ln\n\n(cid:20)\n\npt(x) p∗(x)\n\n· ∇ ln\n\n(cid:21)\n\npt(y) p∗(y)\n\ndydx,\n\n(18)\n\nand a linear KL convergence can be achieved by combining Eq. 12 and Eq. 18 for SVGD (Duncan et al., 2019; Korba et al., 2020; Salim et al., 2021). However, the verification of this assumption is highly non-trivial because we cannot test all pt ∈ P2(Rd). Only when the designed RKHS is overly regular, the RHS of Eq. 18 can be estimated. In the meanwhile, an overly regular kernel, i.e.,\n\n(cid:90)\n\np∗(x)∇ ln p∗(x)·∇ ln p∗(x)k(x, x)−2∇ ln p∗(x)·∇1k(x, x)+∇1k(x, x)·∇2k(x, x)dx < ∞,\n\n(19) will make SLSI fail, which is indicated in (Duncan et al., 2019). Besides, Eq. 19 holds for the most widely used smoothing kernels, such as Radial basis function (RBF) kernel. The contradiction between Eq. 18 and Eq. 19 makes the current analysis in KL divergence highly restricted.\n\nIn this condition, we expect more reasonable assumptions to investigate KL convergence of SVGD. Similar to (Arbel et al., 2019), we have additional assumptions on trajectory of pt. Specifically, we assume the following.\n\n[A1] p∗ satisfies μ-log-Sobolev Inequality (Eq. 4) and f∗ is L-smooth, i.e., for any x, y ∈ Rd,\n\n∥∇f∗(x) − ∇f∗(y)∥ ≤ L∥x − y∥.\n\n[A2] ft is L-smooth where pt = e−ft. [A3] pt is warm: supx∈Rd pt(x)/p∗(x) ≤ β for some constant β ≥ 1.\n\nAssumption [A1] and [A2] are similar to the convexity geometry and L-smoothness in conventional Euclidean optimization. Assumption [A3] restricts the domain of our proof: the tail of pt should be lighter than p∗, which is widely used in Langevin dynamics. Compared with SLSI, due to the decoupling of requirements of the target distribution p∗ and designed kernels k, we can verify these assumptions in several ways. The establishment of LSI of the target distribution p∗ can be checked by the criterion mentioned in Section 2.2. For the trajectory assumptions, i.e., [A2] and [A3], we provide the empirical validation by showing the estimation of density ratio and smoothness of pt in some simple cases with the growth of t (Fig 1). Then, we have the following theorem.\n\nTheorem 3.1. Suppose Assumption [A1]-[A3] are satisfied, and chi-square Dχ(p0, p∗) ≤ 1/4. For any ε > 0, if we set reweighted kernel k:\n\nk(x, y) = (p∗(x))−1/2 kσ(x, y) (p∗(y))−1/2 ,\n\n(cid:90)\n\nRd\n\n∥y∥4 · ̃k(y)dy ≤ M,\n\nand\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 −\n\nwhere\n\nkσ(x, y) = ̃kσ(x − y) = σ−d ̃k(σ−1(x − y)), (cid:90)\n\nkσ(x, x − y)dy\n\n≤\n\nRd\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 √\n\n,\n\n2\n\n2\n\n(20)\n\n(cid:32)\n\nσ = min\n\n1,\n\nε\n\n12LM\n\n√\n\nβ\n\n(cid:18)\n\n·\n\n16Cd +\n\n9βCd 2M\n\n+ 6(cid:112)βL + 3CL + βCL\n\n(cid:19)−1(cid:33)\n\n,\n\n(21)\n\nC = (cid:82) (cid:112)p∗(x)dx , then the KL divergence between pt and p∗ satisfies (cid:18)\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n(cid:18)\n\nHp∗ (pt) ≤ max\n\n0,\n\nHp∗ (p0) −\n\n· exp\n\n−\n\n64ε μ\n\nμt 16\n\n(cid:19)\n\n+\n\n64ε μ\n\n.\n\n(22)\n\nRemark 1. It should be noted that C < ∞ is proven in Lemma B.1. Although we require the trajectory of the algorithm to satisfy some assumptions, the actual requirements are much looser. For example, we allow the coefficients L of smoothness in Assumption [A2] and the maximum density ratio β in Assumption [A3] to increase with the number of iteration t growth. Even if the rate of growth is polynomial, O(1/ε) convergence rate can still be obtained by decreasing σt in the reweighted kernel in Eq. 21, which is shown in Remark 3. Assumption [A3] is actually introduced to control Rényi divergence between pt and p∗ will not be infinity in some region near the target, which can be easily obtained in Langevin dynamics.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Illustration of warmness and smoothness evolution, where p0 ∼ N (0, 0.25), p∗ ∼ N (0, 5).\n\nThis theorem demonstrates that, by introducing reweighted kernel k(x, y) and controlling the variance σ of smoothing kernel ̃k, SVGD initialized in a local region will provide a linear convergence rate to any ε-neighborhood of the target distribution. To validate this result, we provide experiments in synthetic data in Appendix D, and illustrate SVGD with reweighted kernels usually achieves a lower KL divergence compared with traditional SVGD. It should be noticed that commonly used kernels, e.g., RBF kernel and Bump kernel, are proper ̃k. Besides, the linear convergence shows all the parameters will not deteriorate the convergence of SVGD when σ is small enough.\n\n4 REWEIGHTED KERNEL FOR KL CONVERGENCE\n\nIn this section, we mainly explain why we should introduce reweighted kernels in Theorem 3.1. The intuition can be split into 2 parts: (1) the infeasibility of the usage of LSI due to the kernel approximation error; (2) the tractable kernel approximation error form with a reweighted kernel.\n\n4.1 KERNEL APPROXIMATION\n\nIntuitively, to measure the error between Wasserstein gradient and its kernelized one, the most direct idea is to control the error of relative Fisher information and make use of LSI. However, this idea will encounter some fatal bottlenecks.\n\nThe bottleneck of SVGD analysis with Eq. 7 If we directly upper bound the descent of KL divergence by Eq. 7, we have\n\ndHp∗ (pt) dt\n\n≤ −\n\n(cid:90)\n\n1 2\n\npt(x)\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx\n\n(cid:125)\n\n(cid:124)\n\n+\n\n(cid:90)\n\nRd\n\n1 2\n(cid:124)\n\n(cid:123)(cid:122) sufficient descent (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x)\n\n(cid:90)\n\npt(y)k(x, y)∇ ln\n\ndy − ∇ ln\n\np∗(y) pt(y)\n\n(cid:123)(cid:122) kernel approximation error\n\n(23)\n\np∗(x) pt(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n, dx\n\n(cid:125)\n\nwhere the kernel approximation error can hardly be upper bounded due to pt(y) in integration. It means if we directly plug some smoothing kernels into the iteration of SVGD, kernel approximate error may dominate RHS of Eq. 23 and cause SVGD to converge to a limit different from the target distribution with an uncontrollable bias.\n\nFailures of Smoothing Kernels. Smoothing Kernels (kernel smoothers), such as radial basis function kernel, are widely used in SVGD, due to their universal approximation capability to smooth functions (Park & Sandberg, 1991; Micchelli et al., 2006). Assume that kσ(x, y) = ̃kσ(x − y) = σ−d ̃k(σ−1(x − y)) is a smoothing kernel with parameter σ > 0, where σ is called the bandwidth of the kernel. The variance σ2 in the smoothing kernel tends to control the smoothness of the estimated gradient. A large σ makes kernelized gradient well-estimated with finite samples while the kernel approximation error is large. In the population limit, where randomness from pt(y) is ignored, the optimal smoothing kernel should be Dirac delta function δx(y), where the kernel bandwidth is sufficiently small to estimate Wasserstein gradient. For each point x, the kernelized gradient becomes pt(x)∇ ln p∗(x) pt(x) , which means that for those low-density areas pt(x) → 0, the kernelization suffers\n\n7\n\n025050075010001250150017502000iterations2345678suppt(x)/p*(x)Warmness# of Particles = 100# of Particles = 500# of Particles = 1000025050075010001250150017502000iterations1.01.52.02.53.03.54.04.5Lipschitz constant for logptSmoothness# of Particles = 100# of Particles = 500# of Particles = 1000Under review as a conference paper at ICLR 2023\n\n(a) Wasserstein Gradient\n\n(b) With Smoothing kernel\n\n(c) With Reweighted kernel\n\nFigure 2: −∇ ln pt(x) for data distribution N (0, diag(1, 0.52))\n\nIllustration of gradient vanishing with smoothing kernel. The vector field illustrates\n\nfrom gradient vanishing. Such biased Wasserstein gradient estimation in smoothing kernel SVGD will have an additional pt term, which hampers the sufficient descent of each iteration and even the convergence rate. This indicates that SVGD is not compatible with smoothing kernels in low-density parts. Ideally, the kernel should be reweighted by some 1/(cid:112)pt(x)pt(y), which can balance the vanishing scaling in the current SVGD. However, pt is unknown in general, so this reweighting cannot provide algorithmic insight to improve SVGD.\n\nTo solve this problem, a very intuitive idea is to balance the order of pt, we may require kernel k to be related to pt through the following proposition. Proposition 4.1. If we use the kernel k(x, y) = (pt(x))−1/2 kσ(x, y) (pt(y))−1/2, by choosing delta function as the smoothing kernel k0(x, y) = δx(y), kernel approximation error is 0 and SVGD is equivalent to the Wasserstein gradient flow.\n\nUnfortunately, the reweighting strategy with pt(x) makes the iteration of SVGD (Eq. 11) computationally intractable as pt(x) and ∇pt(x) are unknown in general. If we consider the local convergence of SVGD, when pt(x) is approaching p∗(x), we can expect a lower kernel approximation error by replacing p−1/2\n\nin Proposition 4.1 with p−1/2\n\n(·) as follows\n\n∗\n\nt\n\nk(x, y) = (p∗(x))−1/2 kσ(x, y) (p∗(y))−1/2\n\nwhere kσ denotes the smoothing kernel, and it satisfies\n\n(cid:90)\n\nRd\n\n∥y∥4 · kσ(x, x − y)dy < ∞ and\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:90)\n\n1 −\n\nRd\n\nkσ(x, x − y)dy\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n1 √\n\n2\n\n.\n\n2\n\n(24)\n\n(25)\n\nNotice that many popular kernels satisfies Eq. 25, e.g., standard RBF kernel, Bump function (Eq. 26), etc.\n\n ̃kσ(z) =\n\n1 σ\n\n(cid:32)\n\nexp\n\n−\n\n(cid:33)\n\n1 1 − ∥z∥2/σ2\n\n∥z∥ ∈ B(0, σ).\n\n(26)\n\nTherefore, the dynamics of KL divergence in such a reweighted kernel is\n\ndHp∗ (pt) dt\n\n= −\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nRd\n\n(cid:34)\n\nkσ(x, y) ·\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n·\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\n(cid:35)\n\npt(y) p∗(y)\n\ndydx.\n\n(27)\n\nSimilar to the requirement of a delta function in 4.1, a small σ is preferred in our setting, since our analysis is based on the population limit of pt(x). Besides, our analysis would indicate the kernel choice to obtain the convergence.\n\nCompared with utilizing smoothing kernel directly, we may expect a smaller kernel approximation error by introducing kernels as Eq. 24. We also validate this phenomenon by Figure 2, which has shown the gradient vanishing phenomenon with the smoothing kernel. Figure 2 (a) is the vector field of Wasserstein gradient ∇ ln p∗(x)/pt(x), which is a linear function in Gaussian case. However, when using a smoothing kernel the low-density area in Figure 2 (b) has almost no gradient, which makes the particle in this area stuck. Our proposed reweighted kernel amplifies the gradient in low-density areas, the resulting gradient is similar to the Wasserstein gradient.\n\n8\n\n420244202442024420244202442024Under review as a conference paper at ICLR 2023\n\n4.2 ERROR REWEIGHTING\n\nTo achieve a better kernel approximation error with reweighted smoothing kernel Eq. 24, we require a corresponding local version of log-Sobolev inequality to control the sufficient descent of the evolution of the KL divergence (Eq. 27).\n\nLocal version of log-Sobolev inequality. In this context, the sufficient descent term should be reformulated to obtain a well-behaved kernel approximation error. The choice of g in Eq. 38 may convert the kernel approximation error to a tractable form. By choosing g(x) = pt(x) p∗(x) , we can find the upper bound of KL divergence by χ2 version of Rényi information.\n\nLemma 4.2. Suppose p∗ satisfies the μ-log-Sobolev inequality (LSI) with a constant μ > 0. When any probability density function pt satisfies Dχ2 (pt, p∗) ≤ 1/2, we have\n\nμ 4\n\nHp∗ (pt) ≤\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx.\n\n(28)\n\nLemma 4.2 indicates that the KL divergence is also bounded by the Wasserstein gradient of χ2 divergence. If pt/p∗ is bounded, Eq. 28 provides a tighter upper bound of KL divergence compared with that in Eq. 7, especially for the tail part. Thus, controlling the error term in this form has more potential. With such a construction, the decreasing of KL divergence satisfies\n\nd dt\n\nHp∗ (pt) ≤ −\n\n1 2\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ ln\n\npt(x) p∗(x) (cid:123)(cid:122) sufficient descent\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx\n\n(cid:125)\n\n(cid:124)\n\n+\n\n1 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:90)\n\nRd\n\n(cid:124)\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:90)\n\n−\n\nRd\n\nkσ(x, y)\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy\n\n(cid:123)(cid:122) kernel approximation error\n\n(29)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\n.\n\n(cid:125)\n\nLemma 4.3. Assume that ln p∗ and ln pt are L-smooth; pt is warm: supx∈Rd pt(x)/p∗(x) ≤ β and (cid:16) Hp∗ (p0) ≤ (2β)−1. Then by choosing σ = min\n\n, we have\n\n1, O\n\n(cid:17)(cid:17)\n\n(cid:16)\n\nε β1.5L+βL2\n\n(cid:90)\n\nRd\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:90)\n\n−\n\nRd\n\nkσ(x, y)\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ndx ≤ 4ε +\n\n(cid:90)\n\n1\n\n4\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx,\n\nwhere kσ satisfies requirements in Eq. 25.\n\nIn this condition, we nearly establish the \"Stein log-Sobolev Inequality\" with arbitrary small ε by combining Eq. 27, Eq. 28, and Eq. 29 as follows\n\nμ 8\n\nHp∗ (pt) ≤\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nRd\n\n(cid:34)\n\nkσ(x, y) ·\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n·\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\n(cid:35)\n\npt(y) p∗(y)\n\ndydx + 4ε.\n\n(30)\n\nTherefore, reweighted kernels can be considered as a sufficient condition for establishing local \"Stein log-Sobolev Inequality\" near the target distribution p∗.\n\n5 CONCLUSIONS\n\nIn this paper, we prove the local linear convergence for SVGD with reweighted kernel in KL divergence. In particular, our analysis is based on the smoothing kernel for SVGD algorithm and we point out that the conventional smoothing kernel fails to provide valid gradient scaling in low-density areas. Thus, we highlight that the reweighting is necessary for smoothing kernels in SVGD algorithm. With (p∗(x)p∗(y))−1/2 weighting for k(x, y), we provides the KL convergence rate for SVGD algorithm locally for log-Sobolev p∗(x). Our analysis provide new insights on the kernel design in SVGD, especially for gradient amplification in the low-density area.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMilton Abramowitz. Abramowitz and stegun: Handbook of mathematical functions. US Department\n\nof Commerce, 10, 1972.\n\nMichael Arbel, Anna Korba, Adil Salim, and Arthur Gretton. Maximum mean discrepancy gradient\n\nflow. Advances in Neural Information Processing Systems, 32, 2019.\n\nDominique Bakry and Michel Émery. Diffusions hypercontractives. In Seminaire de probabilités\n\nXIX 1983/84, pp. 177–206. Springer, 1985.\n\nEspen Bernton. Langevin monte carlo and jko splitting. In Conference On Learning Theory, pp.\n\n1777–1798. PMLR, 2018.\n\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.\n\nJournal of the American statistical Association, 112(518):859–877, 2017.\n\nXiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. In Algorithmic\n\nLearning Theory, pp. 186–211. PMLR, 2018.\n\nGianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert Scheichl. A stein variational newton method. Advances in Neural Information Processing Systems, 31, 2018.\n\nAndrew Duncan, Nikolas Nüsken, and Lukasz Szpruch. On the geometry of stein variational gradient\n\ndescent. arXiv preprint arXiv:1912.00894, 2019.\n\nAlain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted langevin\n\nalgorithm. Bernoulli, 25(4A):2854–2882, 2019.\n\nAlain Durmus, Szymon Majewski, and Bła ̇zej Miasojedow. Analysis of langevin monte carlo via\n\nconvex optimization. The Journal of Machine Learning Research, 20(1):2666–2711, 2019.\n\nYihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized stein variational\n\ngradient descent. arXiv preprint arXiv:1707.06626, 2017.\n\nAnna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymptotic analysis for stein variational gradient descent. Advances in Neural Information Processing Systems, 33, 2020.\n\nMichel Ledoux. Concentration of measure and logarithmic sobolev inequalities. In Seminaire de\n\nprobabilites XXXIII, pp. 120–216. Springer, 1999.\n\nQiang Liu. Stein variational gradient descent as gradient flow. Advances in neural information\n\nprocessing systems, 30, 2017.\n\nQiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference\n\nalgorithm. Advances in neural information processing systems, 29, 2016.\n\nYang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng. Stein variational policy gradient. arXiv\n\npreprint arXiv:1704.02399, 2017.\n\nJianfeng Lu, Yulong Lu, and James Nolen. Scaling limit of the stein variational gradient descent:\n\nThe mean field regime. SIAM Journal on Mathematical Analysis, 51(2):648–671, 2019.\n\nCharles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine\n\nLearning Research, 7(12), 2006.\n\nRadford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,\n\n2(11):2, 2011.\n\nJooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks.\n\nNeural computation, 3(2):246–257, 1991.\n\nYuchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, and Lawrence Carin. Vae learning via stein variational gradient descent. Advances in Neural Information Processing Systems, 30, 2017.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nMaxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pp. 1674–1703. PMLR, 2017.\n\nRajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial\n\nintelligence and statistics, pp. 814–822. PMLR, 2014.\n\nAdil Salim, Lukang Sun, and Peter Richtárik. Complexity analysis of stein variational gradient\n\ndescent under talagrand’s inequality t1. arXiv preprint arXiv:2106.03076, 2021.\n\nAdil Salim, Lukang Sun, and Peter Richtarik. A convergence theory for svgd in the population In International Conference on Machine Learning, pp.\n\nlimit under talagrand’s inequality t1. 19139–19152. PMLR, 2022.\n\nSantosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm:\n\nIsoperimetry suffices. Advances in neural information processing systems, 32, 2019.\n\nCédric Villani. Optimal transport: old and new, volume 338. Springer, 2009.\n\nMax Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681–688. Citeseer, 2011.\n\nAndre Wibisono. Sampling as optimization in the space of measures: The langevin dynamics as a composite optimization problem. In Conference on Learning Theory, pp. 2093–3027. PMLR, 2018.\n\nTatiana Xifara, Chris Sherlock, Samuel Livingstone, Simon Byrne, and Mark Girolami. Langevin diffusions and the metropolis-adjusted langevin algorithm. Statistics & Probability Letters, 91: 14–19, 2014.\n\nPan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of langevin dynamics based algorithms for nonconvex optimization. Advances in Neural Information Processing Systems, 31, 2018.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nA CONVERGENCE RATE COMPARISON IN SECTION 3\n\nThe assumptions are listed as follows\n\n[AS1] p∗ satisfies μ-log-Sobolev Inequality (Eq. 4). [AS2] p∗ satisfies Talagrand 1 Inequality. [AS3] The Stein log-Sobolev inequality with constant λ, i.e., KL (p∥p∗) ≤ 1 [AS4] f∗ is L-smooth, i.e., for any x, y ∈ Rd,\n\n2λ\n\nD (p∥p∗)2\n\n∥∇f∗(x) − ∇f∗(y)∥ ≤ L∥x − y∥.\n\n[AS5] ft is L-smooth where pt = e−ft. [AS6] pt is warm: supx∈Rd pt(x)/p∗(x) ≤ β for some constant β ≥ 1. [AS7] pt SVGD satisfy:\n\n(cid:90)\n\nk(x, x)pt(x)dx < ∞.\n\n[AS8] Kernel regularization assumption 1:\n\n(cid:26) 1 2\n\nsup x\n\n∥∇ log p∗∥Lip k(x, x) + 2∇xx′\n\n(cid:27)\n\n< ∞.\n\n[AS9] Kernel regularization assumption 2:\n\n∥k (x, ·)∥H ≤ B and\n\n∥∇xk (x, ·)∥Hd ≤ B\n\n[AS10] Bounded moment assumptions:\n\n(cid:90)\n\nsup t\n\n∥x∥ pt(x)dx < ∞.\n\n(31)\n\n(32)\n\n(33)\n\n(34)\n\nWe compare our theoretical results with all previous work where Stein discrepancy is abbreviated as SD.\n\nTable 1: Comparison of convergence rate for sampling algorithms\n\nAlgorithm\n\nULA Vempala & Wibisono (2019)\n\nSVGD Liu (2017) SVGD Liu (2017)\n\nAssumptions\n\n[AS1],[AS4]\n\n[AS7] [AS4],[AS7],[AS8]\n\nSVGD Korba et al. (2020) SVGD Korba et al. (2020)\n\n[AS3],[AS4],[AS7] [AS4],[AS7],[AS9],[AS10]\n\nSVGD Salim et al. (2021)\n\n[AS4],[AS2],[AS7],[AS9]\n\nOurs\n\n[AS1],[AS4],[AS5],[AS6]\n\nCriterion Asymptotic\n\nRate\n\nKL\n\nKL SD\n\nKL SD\n\nSD\n\nKL\n\nNo\n\nYes Yes\n\nYes No\n\nNo\n\nYes\n\nO(e−t)\n\nN/A O(1/t)\n\nO(e−t) O(1/t)\n\nO(1/t)\n\nO(e−t)\n\nB IMPORTANT LEMMAS IN SECTION 4\n\nIn order to facilitate the lemma in Chapter 4, we first revisit the dynamics of SVGD. Remark 2. With a slight abuse of notation, pt of SVGD follows the continuity equation:\n\nin the sense of distribution (rather than almost sure), where\n\n∂tpt + ∇ · (ptφt) = 0\n\n(cid:90)\n\nφt (x) =\n\n(k (x, y) ∇ ln p∗ (y) + ∇yk (x, y)) dpt(y).\n\nNotice that Eq. 35 holds in the sense of distribution here means\n\n(cid:90)\n\n(cid:90)\n\n∂tpt(x)v(x)dx =\n\n∇v(x) · φt(x)dpt(x)\n\n(35)\n\n(36)\n\n(37)\n\nfor all v(x) ∈ C∞\n\nc (Rd).\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nB.1 PROOF OF PROPOSITION 4.1\n\nProof. When k(x, y) = (pt(x))−1/2 kσ(x, y) (pt(y))−1/2 and k0(x, y) = δx(y), we have\n\nk(x, y) =\n\nδx(y) (pt(x))1/2 (pt(y))1/2\n\n=\n\nδx(y) pt(y)\n\n.\n\nThus,\n\nφt(x) =\n\n(cid:90)\n\nRd\n\npt(y)\n\nδx(y) pt(y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy = ∇ ln\n\npt(x) p∗(x)\n\n,\n\nwhich indicates that φt(x) is exactly the Wasserstein gradient and the kernel approximation error is 0.\n\nB.2 PROOF OF LEMMA 4.2\n\nProof. With LSI, all smooth function g : Rd → R with Ep∗ [g2] < ∞,\n\nEp∗\n\n(cid:2)g2 ln g2(cid:3) − Ep∗\n\n(cid:2)g2(cid:3) ln Ep∗\n\n(cid:2)g2(cid:3) ≤\n\n2 μ\n\nEp∗\n\n(cid:104)\n\n∥∇g∥2(cid:105)\n\n.\n\nSuppose g = pt/p∗, we have Ep∗ [g2] = Dχ2(pt, p∗) + 1 < ∞ and\n\n(cid:90)\n\nRd\n\n(cid:90)\n\n−\n\np∗(x) ·\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x) (cid:18) pt(x) p∗(x)\n\nln\n\n(cid:19)2\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x)\n\ndx\n\n(cid:34)(cid:90)\n\ndx · ln\n\np∗(x) ·\n\np∗(x) ·\n\n(cid:18) pt(x) p∗(x)\n\n(cid:19)2\n\n(cid:35)\n\ndx\n\nRd\n\n≤\n\n2 μ\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx =\n\n2 μ\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx.\n\nRd\n\n(cid:90)\n\nRd\n\nWith the fact ln x ≤ x − 1 and ln x ≥ 1 − 1\n\nx when x ≥ 0, we have\n\n(cid:34)(cid:90)\n\nln\n\nRd\n\np∗(x) ·\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x)\n\n(cid:35)\n\ndx\n\n≤\n\n(cid:90)\n\nRd\n\npt(x) ·\n\n(cid:18) pt(x) p∗(x)\n\n(cid:19)\n\n− 1\n\ndx\n\n(cid:90)\n\n=\n\nRd\n\np2 t (x) p∗(x)\n\n·\n\npt(x)\n\np∗(x) − 1\n\npt(x) p∗(x)\n\ndx ≤\n\n(cid:90)\n\nRd\n\np2 t (x) p∗(x)\n\n· ln\n\n(cid:19)\n\n(cid:18) pt(x) p∗(x)\n\ndx.\n\nPlugging the previous inequality into LHS of Eq. 39, we have\n\n(38)\n\n(39)\n\n(40)\n\np∗(x) ·\n\nLHS ≥\n\n(cid:90)\n\nRd (cid:90)\n\n−\n\n(cid:19)2\n\ndx\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x) (cid:18) pt(x) p∗(x)\n\nln\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x) (cid:90)\n\ndx ·\n\np∗(x) ·\n\nRd\n\np∗(x) ·\n\np∗(x) ·\n\nRd\n\n(cid:90)\n\n=2\n\nRd\n\n(cid:90)\n\n−\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x) (cid:32)(cid:18) pt(x) p∗(x)\n\nRd\n\n(cid:18) pt(x) p∗(x) (cid:33)\n\nln\n\n(cid:19)2\n\np2 t (x) p∗(x) (cid:19)\n\ndx −\n\n· ln\n\n(cid:19)\n\n(cid:18) pt(x) p∗(x)\n\ndx\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n− 1\n\ndx ·\n\n(cid:90)\n\nRd\n\np2 t (x) p∗(x)\n\n· ln\n\n(cid:18) pt(x) p∗(x)\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x) (cid:19)\n\ndx\n\nln\n\n(cid:19)\n\n(cid:18) pt(x) p∗(x)\n\ndx\n\n(41)\n\n≥ (cid:0)2 − Dχ2 (pt, p∗) − 1(cid:1) ·\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x)\n\nln\n\n(cid:19)\n\n(cid:18) pt(x) p∗(x)\n\ndx\n\n≥\n\n1 2\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:19)2\n\n(cid:18) pt(x) p∗(x)\n\nln\n\n(cid:19)\n\n(cid:18) pt(x) p∗(x)\n\ndx,\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nwhere the last inequality follows from Dχ2 (pt, p∗) ≤ 1/2. Besides, we have\n\n(cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\n=\n\np∗(x) ·\n\n(cid:18) pt(x) p∗(x)\n\n(cid:19)2\n\nln\n\n(cid:18) pt(x) p∗(x) (cid:19)\n\n− 1\n\n· pt(x) ln\n\n(cid:19)\n\n(cid:18) pt(x) p∗(x) pt(x) p∗(x)\n\ndx =\n\n(cid:90)\n\nRd\n\npt(x) p∗(x)\n\n· pt(x) ln\n\npt(x) p∗(x)\n\ndx\n\ndx +\n\n(cid:90)\n\nRd\n\npt(x) ln\n\npt(x) p∗(x)\n\ndx ≥ Hp∗ (pt),\n\n(42)\n\nwhere the last inequality follows from (x − 1) ln x ≥ 0 for all x ≥ 0. Combining Eq. 39, Eq. 41 and Eq. 42, we complete the proof, and obtain\n\nμ 4\n\nHp∗ (pt) ≤\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx.\n\n(43)\n\nB.3 PROOF OF LEMMA 4.3\n\nBefore providing the Kernel Approximation Error (KAE) in Lemma 4.3, we need to introduce some lemmas.\n\nLemma B.1. Assume that p(x) is L-smooth and log-Sobolev, then for any α > 0\n\n(cid:90)\n\np(x)αdx < ∞\n\nProof. By Ledoux (1999), when p(x) is log-Sobolev, there exists c > 0 such that\n\nthen\n\n(cid:90)\n\n(cid:90)\n\np(x)ec∥x∥2\n\ndx < ∞\n\neln p(x)+c∥x∥2\n\ndx < ∞\n\nThus, there exists cx > 0, for sufficient large ∥x∥ > cx,\n\nln p(x) < −c∥x∥2\n\nand\n\n(cid:90)\n\n(cid:90)\n\npα(x)dx <\n\n∥x∥>cx\n\n∥x∥>cx\n\ne−αc∥x∥2\n\ndx <\n\n(cid:114)\n\n2πd cα\n\nFor\n\n(cid:90)\n\n(cid:90)\n\npα(x)dx ≤\n\n∥x∥≤cx\n\n∥x∥≤cx\n\npα(0)eαL∥x∥2\n\ndx < pα(0)C(d, cx, αL)\n\nwhere C(d, cx, αL) is a constant depend on d, cx, αL.\n\nLemma B.2. (A Variant of Lemma. 11 in Vempala & Wibisono (2019)) Suppose p(x) = e−f (x), x ∈ Rd, f is L-smooth and p(x) satisfies\n\nthen we have\n\n(cid:90)\n\nRd\n\n(cid:112)p(x)dx ≤ C/2,\n\n(cid:90) (cid:112)p(x) ∥∇f (x)∥2 dx ≤ LCd;\n\nProof. Since f (x) is L-smooth, we have for any x\n\n∇2f (x) ⪯ LI\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nUsing integration by parts, we have\n\n(cid:90)\n\ne−f (x)/2∥∇f (x)∥2dx = 4\n\n= 4\n\n(cid:90)\n\n(cid:90)\n\ne−f (x)/2∥∇f (x)/2∥2dx\n\ne−f (x)/2∆(f (x)/2)dx\n\n(cid:90)\n\n≤ 2Ld\n\ne−f (x)/2dx = LCd\n\n(44)\n\n(45)\n\n(46)\n\nProof. In the following, we mainly focus on providing the upper bound of Kernel Approximation Error (KAE), and have\n\n(cid:90)\n\nKAE =\n\nRd\n\n(cid:90)\n\n≤2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nRd\n\n(cid:90)\n\nRd\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:90)\n\n−\n\nRd\n\nkσ(x, x − y)\n\npt(x − y) (cid:112)p∗(x − y)\n\n∇ ln\n\npt(x − y) (cid:112)p∗(x − y)\n\ndy\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\nkσ(x, x − y)\n\n(cid:34)\n\npt(x) (cid:112)p∗(x)\n\n(cid:90)\n\n(cid:18)\n\n(cid:90)\n\n1 −\n\n+ 2\n\nRd\n\nRd\n\nkσ(x, x − y)dy\n\npt(x) p∗(x)\n\n−\n\npt(x) (cid:112)p∗(x)\n\npt(x − y) (cid:112)p∗(x − y) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\n∇ ln\n\n(cid:19)2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ ln\n\npt(x − y) p∗(x − y)\n\n(cid:35)\n\ndy\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\ndx.\n\n(47)\n\nwhere the first equation follows from the change of variable. With the requirement of k, we have\n\nkσ(x, y) = ̃kσ(x − y) =\n\n ̃k\n\n1 σ\n\n(cid:18) (x − y) σ\n\n(cid:19)\n\n,\n\n(cid:90)\n\nRd\n\n∥y∥4 · ̃k(y)dy ≤ M\n\nand\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:90)\n\n1 −\n\nRd\n\nkσ(x, x − y)dy\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n≤\n\n1 √\n\n.\n\n2\n\n2\n\n(48)\n\nIn this condition, suppose y = σz in Eq. 47, xz := x − σz, then we have\n\nKAE ≤4\n\n(cid:90)\n\nRd\n\n(cid:124)\n\n(cid:90)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nRd\n\n(cid:34)\n\n ̃k(z)\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n∇ ln\n\npt(xz) p∗(xz)\n\n(cid:35)\n\ndz\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\n(cid:125)\n\n(49)\n\n+\n\n1 4\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\n(cid:123)(cid:122) Term 1 (cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx.\n\npt(x) p∗(x)\n\nFor any x ∈ Rd, we have\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n=\n\npt(x) (cid:112)p∗(x)\n\n(∇f∗(x) − ∇ft(x)) where pt(x) = e−ft(x), p∗(x) = e−f∗(x).\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\nPlugging such an equation into Eq. 49, we have\n\n(cid:90)\n\nTerm 1 =\n\n(cid:90)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:32)\n\nRd\n\nRd\n\n(cid:34)(cid:32)\n\n ̃k(z)\n\npt(x) (cid:112)p∗(x)\n\n∇f∗(x) −\n\n(cid:33)\n\n∇f∗(xz)\n\npt(xz) (cid:112)p∗(xz) (cid:33)(cid:35)\n\n∇ft(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(xz)\n\ndz\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\n−\n\n(cid:90)\n\n≤2\n\nRd\n\npt(x) (cid:112)p∗(x) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nRd\n\n(cid:90)\n\n ̃k(z)\n\n(cid:32)\n\n(cid:90)\n\n+ 2\n\n(cid:90)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nRd \n\n(cid:90)\n\n ̃k(z)\n\nRd\n\n ̃k(z)dz ·\n\n(cid:90)\n\n≤2\n\n\n\nRd\n\nRd\n\n\n\n(cid:90)\n\npt(x) (cid:112)p∗(x) (cid:32)\n\n∇f∗(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n(cid:33)\n\n∇f∗(xz)\n\ndz\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\npt(x) (cid:112)p∗(x)\n\n∇ft(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n(cid:33)\n\n∇ft(xz)\n\ndz\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\n(cid:90)\n\nRd\n\n ̃k(z)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇f∗(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n∇f∗(xz)\n\n(cid:33)(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\ndz\n\n dx\n\n(cid:90)\n\n+ 2\n\n\n\nRd\n\nRd\n\n ̃k(z)dz ·\n\n(cid:90)\n\nRd\n\n ̃k(z)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇ft(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(xz)\n\n(cid:33)(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\ndz\n\n dx\n\n(cid:90)\n\n(cid:90)\n\n≤3\n\nRd\n\nRd\n\n ̃k(z)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇ft(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(xz)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:124)\n\n(cid:123)(cid:122) Term 1.1\n\ndzdx\n\n(cid:125)\n\n(cid:90)\n\n(cid:90)\n\n+ 3\n\nRd\n\nRd\n\n ̃k(z)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇f∗(x) −\n\npt(xz) (cid:112)p∗(xz)\n\n∇f∗(xz)\n\n(cid:124)\n\n(cid:123)(cid:122) Term 1.2\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n(cid:125)\n\n(50) where the first inequality follows from Minkowski inequality, the second inequality follows from Cauchy–Schwarz inequality, and the third inequality follows from Eq. 48.\n\nConsider Term 1.1, we have,\n\nTerm 1.1 =\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nRd\n\n ̃k(z)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇ft(x) −\n\n+\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(x) −\n\npt(xz) (cid:112)p∗(xz)\n\npt(xz) (cid:112)p∗(xz) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ft(xz)\n\n∇ft(x)\n\ndzdx\n\n(cid:90)\n\n(cid:90)\n\n≤2\n\nRd\n\nRd\n\n ̃k(z)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:90)\n\n(cid:90)\n\n+ 2\n\nRd\n\nRd\n\n ̃k(z)\n\npt(x) (cid:112)p∗(x) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(xz) (cid:112)p∗(xz)\n\n(cid:33)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n(∇ft(x) − ∇ft(xz))\n\ndzdx\n\n(cid:90)\n\n(cid:90)\n\n≤2\n\nRd\n\nRd\n\n ̃k(z)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n(cid:33)\n\n∇ft(x)\n\n(cid:90)\n\n+ 2\n\nRd\n\n ̃k(z)L2σ2 ∥z∥2\n\n(cid:33)2\n\n(cid:32)\n\n(cid:90)\n\nRd\n\npt(xz) (cid:112)p∗(xz)\n\ndxdz\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n(cid:90)\n\n(cid:90)\n\n≤2\n\nRd\n\nRd\n\n ̃k(z)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n(cid:33)\n\n∇ft(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx + 2σ2βL2(M + 1),\n\n(51)\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nwhere the second inequality follows from L-smoothness of pt(x) and the Fubini’s theorem, and the third inequality follows from pt warm (Assumption 3) and the fact (cid:82) Rd ∥y∥2 · ̃k(y)dy ≤ D in Eq. 48.\n\nIn the following, we focus on the first term of RHS of Eq. 51, and have\n\npt(x) (cid:112)p∗(x)\n\n= exp\n\n(cid:18) f∗(x) 2\n\n(cid:19)\n\n− ft(x)\n\n.\n\nFor each x ∈ Rd, suppose high dimensional Rd can be divided into two parts:\n\n(cid:26)\n\nBl(x) =\n\nz\n\nf∗(x) 2\n\n− ft(x) ≤\n\n(cid:26)\n\nBu(x) =\n\nFor z ∈ Bl(x), we have\n\nf∗(x) 2\n\n− ft(x) ≥\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nz\n\nf∗(xz) 2\n\nf∗(xz) 2\n\n(cid:27)\n\n− ft(xz)\n\n,\n\n(cid:27)\n\n− ft(xz)\n\n.\n\n(52)\n\n(53)\n\npt(xz) (cid:112)p∗(xz) pt(xz) (cid:112)p∗(xz) pt(xz) (cid:112)p∗(xz) pt(xz) (cid:112)p∗(xz)\n\n≤\n\n≤\n\n≤\n\n·\n\n·\n\n·\n\n−\n\npt(x) (cid:112)p∗(x) (cid:20)(cid:18) f∗(xz)\n\n=\n\npt(xz) (cid:112)p∗(xz) (cid:19)\n\n− ft(xz)\n\n−\n\n2\n\n(cid:18) f∗(x) 2\n\n− ft(x)\n\n(cid:18)\n\n·\n\n1 − exp\n\n(cid:18)(cid:18) f∗(x)\n\n2 (cid:19)(cid:21)\n\n(cid:19)\n\n− ft(x)\n\n−\n\n(cid:18) f∗(xz) 2\n\n(cid:19)(cid:19)(cid:19)\n\n− ft(xz)\n\n(cid:20)\n\n−\n\n(cid:20)\n\n−\n\nσ 2\n\nσ 2\n\n(∇f∗(x) − ∇f∗(xz) + f∗(x − σz))⊤ z + σ∇ft(xz)⊤z +\n\n(cid:21)\n\n∥z∥2\n\n3Lσ2 4\n\n∇f∗(xz)⊤z + σ∇ft(xz)⊤z +\n\n(cid:21)\n\n,\n\n∥z∥2\n\n5Lσ2 4\n\n(54) where the first inequality follows from 1 − e−x ≤ x for any x ≥ −1, the second and third inequality follows from L-smoothness of f∗ and ft. Therefore, we have\n\n(cid:33)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n(cid:20)\n\n·\n\n−\n\nσ 2\n\n∇f∗(xz)⊤z + σ∇ft(xz)⊤z +\n\n(cid:21)\n\n∥z∥2\n\n5Lσ2 4\n\n· ∇ft(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n−\n\n∇f∗(xz)⊤z + σ∇ft(xz)⊤z +\n\n(cid:21)2\n\n∥z∥2\n\n5Lσ2 4\n\n(cid:112)pt(xz)dzdx\n\n ̃k(z)σ 2\n\n·\n\n(pt(xz))1.5 p∗(xz)\n\n· ∥∇ft(x) − ∇ft(xz) + ∇ft(xz)∥2 dzdx\n\nσ · ̃k(z) ∥z∥2 (cid:112)pt(xz) ·\n\n(cid:18) 1 2\n\n∥∇f∗(xz)∥2 +\n\n3 2\n\n∥∇ft(xz)∥2 +\n\n5L2σ2 2\n\n(cid:19)\n\n∥z∥2\n\ndzdx\n\nσ · ̃k(z) ·\n\n(pt(xz))1.5 p∗(xz)\n\n(cid:16)\n\n∥∇ft(xz)∥2 + L2σ2 ∥z∥2(cid:17)\n\n·\n\ndzdx\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nBl\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nBl\n\n(cid:90)\n\n(cid:90)\n\n≤\n\n≤\n\n ̃k(z)\n\n ̃k(z)\n\n ̃k(z) 2σ\n\n(cid:32)\n\npt(x) (cid:112)p∗(x)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) pt(xz) (cid:13) (cid:112)p∗(xz) (cid:13) (cid:13) (cid:20)\n\nσ 2\n\nBl (cid:90)\n\nRd (cid:90)\n\n+\n\nBl\n\nRd (cid:90)\n\n(cid:90)\n\n≤\n\nBl (cid:90)\n\nRd\n\nBl\n\nRd (cid:90)\n\n+\n\n√\n\nβσ 2\n\n≤\n\n(cid:90)\n\n·\n\n(cid:90)\n\n ̃k(z)∥z∥2\n\n+\n\n3σ 2\n\n·\n\n+ βσ ·\n\nRd (cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\nRd (cid:90)\n\n ̃k(z)∥z∥2\n\nRd\n\n(cid:90)\n\n ̃k(z)\n\nRd\n\n(cid:112)p∗(xz) ∥∇f∗(xz)∥2 dxdz\n\n(cid:112)pt(xz) ∥∇ft(xz)∥2 dxdz +\n\n(cid:112)pt(xz) ∥∇ft(xz)∥2 dxdz + βL2σ3 ·\n\n ̃k(z)∥z∥2\n\n(cid:112)pt(xz)dxdz\n\n ̃k(z)∥z∥4\n\n(cid:90)\n\nRd\n\n(cid:112)pt(xz)dxdz\n\n(cid:90)\n\n·\n\nRd\n\n5L2σ3 2\n(cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\n≤(cid:112)βCdL(M + 1) · σ + 3(cid:112)βCdL(M + 1) · σ +\n\n(cid:112)βCL2M · σ3 + 3β1.5CdL · σ +\n\nβ1.5CL2(M + 1) · σ3.\n\n1 2\n\n(55) It can be noticed that the first inequality follows from Eq. 54, the second and the third inequalities follow from Cauchy–Schwarz inequality, the fourth inequality follows from the β-warm during\n\n5 4\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nthe update, the fourth inequality follows from the β-warm during the update, Lemma B.2 and the following fact\n\n(cid:90)\n\n(cid:90)\n\n ̃k(z)dz ≤\n\n ̃k(z)dz.\n\nRd Besides, the constant C is provided by Lemma B.1 as\n\nBl\n\n(cid:90)\n\nRd\n\n(cid:112)p∗(x)dx ≤ C.\n\npt(xz) (cid:112)p∗(xz)\n\n1 − exp\n\n(cid:20)(cid:18) f∗(x)\n\n·\n\n(cid:18)\n\n−\n\n=\n\nFor z ∈ Bu(x), we have pt(x) (cid:112)p∗(x) pt(x) (cid:112)p∗(x) pt(x) (cid:112)p∗(x) pt(x) (cid:112)p∗(x) pt(x) (cid:112)p∗(x)\n\n(cid:20) σ 2\n(cid:20) σ 2\n\n≤\n\n≤\n\n≤\n\n·\n\n·\n\n·\n\n2\n\nSimilar to Eq. 55, we have\n\n(cid:18)(cid:18) f∗(xz)\n\n2\n\n(cid:19)\n\n− ft(x)\n\n−\n\n(cid:19)\n\n− ft(xz)\n\n−\n\n(cid:18) f∗(x) 2\n\n(cid:19)(cid:19)(cid:19)\n\n− ft(x)\n\n(cid:18) f∗(xz) 2\n\n(cid:19)(cid:21)\n\n− ft(xz)\n\n(∇f∗(xz) − ∇f∗(x) + ∇f∗(x))⊤ z − σ∇f ⊤\n\nt (x)z +\n\n∇f ⊤\n\n∗ (x)z − σ∇f ⊤\n\nt (x)z +\n\n(cid:21)\n\n.\n\n∥z∥2\n\n5Lσ2 4\n\n(56)\n\n(cid:21)\n\n∥z∥2\n\n3Lσ2 4\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nBu\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nBu\n\n(cid:90)\n\n(cid:90)\n\nBu (cid:90)\n\nRd (cid:90)\n\n+\n\n≤\n\n≤\n\nRd (cid:90)\n\n(cid:90)\n\n≤\n\nBu (cid:90)\n\nRd (cid:90)\n\n+\n\n√\n\n≤\n\nRd\n\nBu\n\n(cid:90)\n\n·\n\nβσ 2\n5L2σ3 2\n\nRd\n\n+\n\n ̃k(z)\n\npt(x) (cid:112)p∗(x)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) pt(x) (cid:13) (cid:112)p∗(x) (cid:13) (cid:13) (cid:20) σ 2\n ̃k(z)σ 2\n\n∇f ⊤\n\n·\n\n ̃k(z)\n\n ̃k(z) 2σ\n\nBu\n\n(cid:33)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n∇ft(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n(cid:20) σ 2\n\n·\n\n∇f ⊤\n\n∗ (x)z − σ∇f ⊤\n\nt (x)z +\n\n(cid:21)\n\n∥z∥2\n\n5Lσ2 4\n\n· ∇ft(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n∗ (x)z − σ∇f ⊤\n\nt (x)z +\n\n(cid:21)2\n\n∥z∥2\n\n5Lσ2 4\n\n· (cid:112)pt(x)dzdx\n\n(pt(x))1.5 p∗(x)\n\n· ∥∇ft(x)∥2 dzdx\n\nσ · ̃k(z) ∥z∥2 (cid:112)pt(x) ·\n\n(cid:18) 1 2\n\n∥∇f∗(x)∥2 +\n\n3 2\n\n∥∇ft(x)∥2 +\n\n5L2σ2 2\n\n(cid:19)\n\n∥z∥2\n\ndzdx\n\n ̃k(z)σ 2\n\n·\n\n(pt(x))1.5 p∗(x) (cid:90)\n\n· ∥∇ft(x)∥2 dzdx\n\n ̃k(z)∥z∥2\n\n(cid:112)p∗(x) ∥∇f∗(x)∥2 dxdz +\n\nRd\n\n ̃k(z)∥z∥4\n\n(cid:90)\n\n·\n\nRd\n\n(cid:90)\n\nRd\n\n(cid:112)pt(x)dxdz +\n\nRd (cid:90)\n\n(cid:90)\n\n·\n\n ̃k(z)\n\nRd\n\nRd\n\n3σ 2\n\n(cid:90)\n\n·\n\n ̃k(z)∥z∥2\n\n(cid:90)\n\nRd\n\n(cid:112)pt(x) ∥∇ft(x)∥2 dxdz\n\n(cid:112)pt(x) ∥∇ft(x)∥2 dxdz\n\n≤(cid:112)βCdL(M + 1) · σ + 3(cid:112)βCdL(M + 1) · σ +\n\n(cid:112)βCL2M · σ3 +\n\n3 2\n\nβ1.5CdL · σ.\n\nPlugging Eq. 55, Eq. 57 into Eq. 51, we have\n\nTerm 1.1 ≤16(cid:112)βCdL(M + 1) · σ + 9β1.5CdL · σ + 2βL2(M + 1) · σ2\n\n+ 5(cid:112)βCL2M · σ3 + β1.5CL2(M + 1) · σ3.\n\n(57)\n\n(58)\n\nWith the same techniques in Eq. 51, we have\n\nTerm 1.2 ≤ 2\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nB(0,r)\n\n ̃k(z)\n\n(cid:32)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n−\n\npt(xz) (cid:112)p∗(xz)\n\n(cid:33)\n\n∇f∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx + 5L2σ2r2.\n\n(59)\n\n18\n\nβσ 2\n5 4\n\nUnder review as a conference paper at ICLR 2023\n\nSimilar to Eq. 55, when z ∈ Bl(x), we have (cid:33)\n\n(cid:32)\n\n ̃k(z)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:90)\n\n(cid:90)\n\nRd\n\nBl\n\n(cid:90)\n\n(cid:90)\n\npt(x) (cid:112)p∗(x)\n\n−\n\npt(xz) (cid:112)p∗(xz) (cid:18) 1 2\n\nσ · ̃k(z) ∥z∥2 (cid:112)pt(xz) ·\n\n∥∇f∗(xz)∥2 +\n\n∇f∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n3 2\n\n∥∇ft(xz)∥2 +\n\n5L2σ2 2\n\n(cid:19)\n\n∥z∥2\n\ndzdx\n\nBl (cid:90)\n\nRd\n\nBl\n\n≤\n\n≤\n\nRd (cid:90)\n\n+\n\n√\n\nβσ 2\n\n+\n\n3σ 2\n\n·\n\n+ βσ ·\n\nRd (cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\nσ · ̃k(z) ·\n\n(pt(xz))1.5 p∗(xz)\n\n(cid:16)\n\n∥∇f∗(xz)∥2 + L2σ2 ∥z∥2(cid:17)\n\n·\n\ndzdx\n\n(cid:90)\n\n·\n\n(cid:90)\n\n ̃k(z)∥z∥2\n\n(cid:112)p∗(xz) ∥∇f∗(xz)∥2 dxdz\n\nRd (cid:90)\n\n ̃k(z)∥z∥2\n\n(cid:90)\n\n ̃k(z)\n\nRd\n\n(cid:112)pt(xz) ∥∇ft(xz)∥2 dxdz +\n\n5L2σ3 2\n(cid:90) (cid:112)pt(xz) ∥∇f∗(xz)∥2 dxdz + βL2σ3 ·\n\nRd\n\n(cid:90)\n\n·\n\nRd\n\n ̃k(z)∥z∥4\n\n(cid:90)\n\nRd\n\n(cid:112)pt(xz)dxdz\n\n ̃k(z)∥z∥2\n\n(cid:112)pt(xz)dxdz\n\n(cid:90)\n\nRd\n\nRd\n\n≤(cid:112)βCdL(M + 1) · σ + 3(cid:112)βCdL(M + 1) · σ +\n\n(cid:112)βCL2M · σ3 + 3β1.5CdL · σ +\n\n5 4\n\n1 2\n\nβ1.5CL2(M + 1) · σ3,\n\n(60)\n\nwhere the last inequality utilizes additional β-warm condition comparing with Eq. 55.\n\npt(xz) (cid:112)p∗(xz) (cid:18) 1 2\n\nSimilar to Eq. 57, when z ∈ Bu(x), we have (cid:33)\n\n(cid:32)\n\n(cid:90)\n\n(cid:90)\n\n ̃k(z)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n−\n\n∇f∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndzdx\n\n≤\n\n≤\n\nRd\n\nBu\n\n(cid:90)\n\n(cid:90)\n\nBu (cid:90)\n\nRd (cid:90)\n\n+\n\n√\n\n+\n\nRd\n\nBu\n\n(cid:90)\n\n·\n\nβσ 2\n5L2σ3 2\n\nRd\n\nσ · ̃k(z) ∥z∥2 (cid:112)pt(x) ·\n\n∥∇f∗(x)∥2 +\n\n3 2\n\n∥∇ft(x)∥2 +\n\n5L2σ2 2\n\n(cid:19)\n\n∥z∥2\n\ndzdx\n\n ̃k(z)σ 2\n\n·\n\n(pt(x))1.5 p∗(x) (cid:90)\n\n· ∥∇f∗(x)∥2 dzdx\n\n ̃k(z)∥z∥2\n\n(cid:112)p∗(x) ∥∇f∗(x)∥2 dxdz +\n\nRd\n\n ̃k(z)∥z∥4\n\n(cid:90)\n\n·\n\nRd\n\n(cid:90)\n\nRd\n\n(cid:112)pt(x)dxdz +\n\nRd (cid:90)\n\n(cid:90)\n\n·\n\n ̃k(z)\n\nRd\n\nRd\n\n3σ 2\n\n(cid:90)\n\n·\n\n ̃k(z)∥z∥2\n\n(cid:90)\n\nRd\n\n(cid:112)pt(x) ∥∇ft(x)∥2 dxdz\n\n(cid:112)pt(x) ∥∇f∗(x)∥2 dxdz\n\n≤(cid:112)βCdL(M + 1) · σ + 3(cid:112)βCdL(M + 1) · σ +\n\n(cid:112)βCL2M · σ3 +\n\n3 2\n\nβ1.5CdL · σ.\n\nβσ 2\n5 4\n\n(61)\n\n(62)\n\nCombining Eq. 60, Eq. 61 with Eq. 59, we have\n\nTerm 1.2 ≤16(cid:112)βCdL(M + 1) · σ + 9β1.5CdL · σ + 2σ2βL2(M + 1) + 5(cid:112)βCL2M · σ3 + β1.5CL2(M + 1) · σ3.\n\nWithout loss of generality, we suppose σ ≤ 1 and M ≥ 1. Plugging Eq. 58 and Eq. 62 into Eq. 50, we have\n\nTerm 1 ≤12LM (cid:112)βσ ·\n\n(cid:18)\n\n16Cd +\n\n9βCd 2M\n\n+ 6(cid:112)βL + 3CL + βCL\n\n(cid:19)\n\n,\n\n(63)\n\nwhich means if we set (cid:32)\n\nσ = min\n\n1,\n\nε\n\n12LM\n\n√\n\nβ\n\n(cid:18)\n\n·\n\n16Cd +\n\n9βCd 2M\n\n+ 6(cid:112)βL + 3CL + βCL\n\n(cid:19)−1(cid:33)\n\n,\n\n(64)\n\nKAE satisfies\n\nKAE ≤ 4ε +\n\n1 4\n\n(cid:90)\n\nRd\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx,\n\nand the proof is completed.\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nC THE MAIN THEOREM IN SECTION 3\n\nBefore providing the main theorem, i.e., Theorem 3.1, we need to introduce some lemmas.\n\nLemma C.1. Suppose Assumption [A1]–[A3] are satisfied, and p0 is near to the target p∗ satisfying Dχ(p0, p∗) ≤ 1/4, for any time T = −C ln ε where ε ≤ (16βC)−2, we have\n\nDχ(pT , p∗) ≤ 1/2.\n\nProof. We denote Dχ2 (pt, p∗) as chi-square distance between pt and p∗. We have the following functional derivative\n\ndDχ(pt, p∗) dt\n\n(cid:90)\n\n=\n\nRd\n\nδDχ(pt, p∗) δp\n\n(pt)∂tptdx =\n\n(cid:90)\n\nRd\n\n2\n\npt(x) p∗(x)\n\n∂tpt(x)dx.\n\n(65)\n\nCombining the result with Remark 2, we have\n\ndDχ(pt, p∗) dt\n\n= −\n\n= −\n\n(cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\n2∇\n\n2∇\n\npt(x) p∗(x) pt(x) p∗(x)\n\n·\n\n·\n\n(cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\npt(y)k(x, y) · ∇ ln\n\npt(y) p∗(y)\n\ndydx\n\np∗(y)k(x, y) · ∇\n\npt(y) p∗(y)\n\ndydx\n\n(66)\n\nPlugging Eq. 20 to the previous equation, we have\n\ndDχ(pt, p∗) dt\n\n= − 2\n\n= − 2\n\n(cid:18)(cid:90)\n\n(cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\npt(x) (cid:112)p∗(x) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x)\n\n∇\n\n· ∇\n\npt(x) p∗(x) (cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n(cid:90)\n\n·\n\nRd\n\nk(x, y)(cid:112)p∗(y) · ∇\n\npt(y) p∗(y)\n\ndydx\n\ndx − 2\n\n(cid:90)\n\nRd\n\npt(x) (cid:112)p∗(x) − (cid:112)p∗(x) · ∇\n\n· ∇\n\n·\n\npt(x) p∗(x) (cid:19)\n\npt(x) p∗(x)\n\ndx\n\n(67)\n\nk(x, y)(cid:112)p∗(y) · ∇\n\npt(y) p∗(y)\n\npt(x)\n\n∇\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx\n\nRd (cid:90)\n\n≤ −\n\nRd\n\n(cid:90)\n\n+\n\nRd\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n(cid:90)\n\nk(x, y)(cid:112)p∗(y) · ∇\n\npt(y) p∗(y)\n\n− (cid:112)p∗(x) · ∇\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx\n\nRd\n\n≤β · KAE\n\nwhere the last inequality follows from the pt warm assumption. Suppose we control the KAE by Eq. 64, which means ∂tDχ(pt, p∗) ≤ 4βε, and time T = −C ln ε which leads pt to the target region, i.e., KL(pt∥p∗) ≤ ε by the linear convergence, when ε is small enough, e.g., ε ≤ (16βC)−2 and Dχ(p0, p∗) ≤ 1/4, we have\n\nDχ(pT , p∗) ≤ Dχ(p0, p∗) + 4βεT = Dχ(p0, p∗) − 4Cβε ln ε ≤ Dχ(p0, p∗) + 4Cβ\n\n√\n\nε ≤ 1/2.\n\nHence, the proof is completed.\n\nWith these Lemmas, we provide the main theorem proof in the following.\n\nProof. Suppose Hp∗ (pt) := KL(pt∥p∗) for abbreviation. According to the time derivative of KL divergence along any flow, we have\n\nd dt\n\nHp∗ (pt) =\n\n(cid:90)\n\nRd\n\nδHp∗ δp\n\n(pt) ∂tptdx.\n\nTherefore, along Remark 2, we have\n\nd dt\n\nHp∗ (pt) =\n\n(cid:90)\n\n∇ ln\n\n(cid:90)\n\nRd (cid:90)\n\npt(x) p∗(x)\n\n· φt(x)pt(x)dx\n\n(cid:20)\n\n= −\n\nRd\n\nRd\n\nk(x, y)pt(x)pt(y) ·\n\n∇ ln\n\n20\n\npt(x) p∗(x)\n\n· ∇ ln\n\n(cid:21)\n\npt(y) p∗(y)\n\ndydx,\n\n(68)\n\n(69)\n\nkσ(x, y)\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy\n\n(70)\n\nRd\n\nUnder review as a conference paper at ICLR 2023\n\nwhich follows from Eq. 36. By taking\n\nk(x, y) = (p∗(x))−1/2 kσ(x, y) (p∗(y))−1/2 ,\n\nEq. 69 satisfies\n\nd dt\n\nHp∗ (pt) = −\n\n= −\n\n−\n\n= −\n\n(cid:90)\n\nRd\n\n(cid:90)\n\nRd\n\npt(x) (cid:112)p∗(x)\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:90)\n\ndx ·\n\nRd\n\nkσ(x, y)\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy\n\n∇ ln\n\npt(x) p∗(x)\n\n·\n\n(cid:34)(cid:90)\n\nRd\n\nkσ(x, y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy\n\npt(y) (cid:112)p∗(y) (cid:35)\n\ndx\n\n∇ ln\n\npt(x) p∗(x)\n\n+\n\npt(x) (cid:112)p∗(x) (cid:90)\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\nRd\n\n(cid:90)\n\n−\n\nRd\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\n∇ ln\n\npt(x) p∗(x)\n\npt(x) (cid:112)p∗(x) (cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x) (cid:34)(cid:90)\n\ndx\n\n·\n\npt(x) p∗(x) (cid:35)\n\ndx.\n\n∇ ln\n\npt(x) p∗(x)\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx\n\npt(x) (cid:112)p∗(x) (cid:90)\n\n−\n\n≤ −\n\n1 2\n\n+\n\n1 2\n\nRd\n\n(cid:90)\n\nRd\n\n(cid:124)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) (cid:112)p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:90)\n\n−\n\nRd\n\nkσ(x, y)\n\npt(y) (cid:112)p∗(y)\n\n∇ ln\n\npt(y) p∗(y)\n\ndy\n\n(cid:123)(cid:122) Kernel Approximation Error(KAE)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\ndx\n\n.\n\n(cid:125)\n\nthe decreasing of KL divergence of SVGD at time t satisfies\n\nd dt\n\nHp∗ (pt) ≤ −\n\n≤ −\n\n(cid:90)\n\n1 4\nμ 16\n\nHp∗ (pt) + 4ε,\n\np∗(x) ·\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\npt(x) p∗(x)\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx + 4ε\n\n(71)\n\nRd\n\nwhere the first inequality follows from Lemma 4.3 and the second one follows from Lemma 4.2. However, Lemma 4.2 requires a local condition of pt which we proved in Lemma C.1. By applying Gronwall’s lemma, Eq. 71 implies the desired bound\n\nHp∗ (pt) ≤ max\n\n0,\n\nHp∗ (p0) −\n\n(cid:18)\n\n(cid:18)\n\n64ε μ\n\n(cid:19)(cid:19)\n\n(cid:18)\n\n· exp\n\n−\n\n(cid:19)\n\nμt 16\n\n+\n\n64ε μ\n\n.\n\n(72)\n\nHence, the proof is completed.\n\nRemark 3. Actually, instead of the constant upper bound of the density ratio provided in Assumption [A3], we allow the upper bound of the density ratio to be upper-bounded as\n\nsupx∈Rd pt(x)/p∗(x) ≤ P (t).\n\nwhere P (t) denotes a polynomial function. Without loss of generality, we suppose P (t) ≤ (t + 1)q. In this condition, reweighted SVGD can achieve an O(1/ε) when we choose\n\nσt = min (cid:0)1, e−t−1(cid:1) .\n\n(73)\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\nIn the following, we will show how this choice affects the kernel approximation error shown in Lemma 4.3. Similar to Eq. 63, we can obtain the following inequality\n\nTerm 1 ≤12LM (cid:112)P (t)σt ·\n\n(cid:18)\n\n16Cd +\n\n9P (t)Cd 2M\n\n+ 6(cid:112)P (t)L + 3CL + P (t)CL\n\n(cid:19)\n\n≤192LM Cd · (cid:112)P (t)σt + 54LCd · P (t)1.5σt\n\n+ 36L2M · (cid:112)P (t)σt + 12L2M C · P (t)1.5σt.\n\n(74)\n\nWe will easily obtain that\n\nP 1.5(t)σt = (1 + t)1.5q · e−(t+1) ≤\n\n(1 + 1.5q)1.5q · e−1−1.5q t + 1\n\n,\n\nand Term 1 ≤ (cid:0)192LM Cd + 54LCd + 36L2M + 12L2M C(cid:1) · (1 + 1.5q)1.5q · e−1−1.5q · (t + 1)−1.\n\nFor abbreviation, we suppose\n\nC∗ = (cid:0)192LM Cd + 54LCd + 36L2M + 12L2M C(cid:1) · (1 + 1.5q)1.5q · e−1−1.5q.\n\nThen, Similar to Eq. 71, we have\n\nd dt\n\nHp∗ (pt) ≤ −\n\n≤ −\n\n(cid:90)\n\n1 4\nμ 16\n\np∗(x) ·\n\nRd\n\nHp∗ (pt) +\n\n∇ ln\n\npt(x) p∗(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\ndx +\n\n4C∗ 1 + t\n\n(75)\n\n(cid:13) pt(x) (cid:13) (cid:13) p∗(x) (cid:13) 4C∗ 1 + t\n\n.\n\nIt means the KL divergence Hp∗ (pt) satisfies\n\nHp∗ (pt) ≤ 4C∗ · exp\n\n−\n\n(cid:18)\n\nμ(t + 1) 16\n\n(cid:19)\n\n· Ei\n\n(cid:19)\n\n(cid:18) μ(t + 1) 16\n\n+ Hp∗ (p0) · exp\n\n(cid:19)\n\n(cid:18) −μt 16\n\n(76)\n\nwhere Ei is denoted as\n\nEi(x) = −\n\n(cid:90) ∞\n\n−x\n\nexp(−t) t\n\ndt.\n\nAccording to Abramowitz (1972), we have\n\ne−xEi(x) ≤ −\n\n1 2\n\nln(1 −\n\n2 x\n\n)\n\n(cid:18)\n\n4C∗ · exp\n\n−\n\nμ(t + 1) 16\n\n(cid:19)\n\n· Ei\n\n(cid:19)\n\n(cid:18) μ(t + 1) 16\n\n≤ − 2C∗ ln(1 −\n\n32 μ(t + 1)\n\n) ≤\n\n64C∗ μ(t + 1) − 32\n\n,\n\nwhere the last inequality follows from ln(x) ≥ 1 − 1/x. Hence, by requiring RHS of Eq. 76 to be smaller than ε, we have t ≥ 64C∗/(με) + 32/μ.\n\nIn the following, we will show that a convergence rate can be obtained by approximating an unknown normalizing constant of the target distribution p∗ (similar to Theorem 3.1). Proposition C.2. Suppose Assumption [A1]-[A3] are satisfied, chi-square Dχ(p0, p∗) ≤ 1/4 and PDF of target distribution p∗(x) can be estimated by ˆp∗(x) satisfying\n\np∗(x) =\n\ne−f∗(x) C∗\n\n,\n\nˆp∗(x) =\n\ne−f∗(x) ˆC\n\nwhere\n\n0 < C∗, ˆC < ∞.\n\nFor any ε > 0, if we set reweighted kernel k:\n\nˆk(x, y) = (ˆp∗(x))−1/2 kσ(x, y) (ˆp∗(y))−1/2 ,\n\n(cid:90)\n\nRd\n\n∥y∥4 · ̃k(y)dy ≤ M,\n\nand\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 −\n\nkσ(x, y) = ̃kσ(x − y) = σ−d ̃k(σ−1(x − y)), (cid:90)\n\nkσ(x, x − y)dy\n\n≤\n\nRd\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n1 √\n\n,\n\n2\n\n2\n\n(77)\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nwhere\n\n(cid:32)\n\nσ = min\n\n1,\n\nε\n\n12LM\n\n√\n\nβ\n\n(cid:18)\n\n·\n\n16Cd +\n\n9βCd 2M\n\n+ 6(cid:112)βL + 3CL + βCL\n\n(cid:19)−1(cid:33)\n\n,\n\n(78)\n\nthen the KL divergence between pt and p∗ satisfies\n\nHp∗ (pt) ≤ max\n\n0,\n\nHp∗ (p0) −\n\n(cid:18)\n\n(cid:18)\n\n(cid:19)(cid:19)\n\n64ε μ\n\n(cid:32)\n\n· exp\n\n−\n\n(cid:33)\n\nμt 16\n\n·\n\nˆC C∗\n\n+\n\n64ε μ\n\n.\n\n(79)\n\nProof. We suppose the following reweighted kernels\n\nk(x, y) = (p∗(x))−1/2 kσ(x, y) (p∗(y))−1/2 ˆk(x, y) = (ˆp∗(x))−1/2 kσ(x, y) (ˆp∗(y))−1/2\n\nlead to different update rules which are\n\n(cid:90)\n\nφt (x) =\n\n(k (x, y) ∇ ln p∗ (y) + ∇yk (x, y)) dpt(y)\n\n(cid:90)\n\n= −\n\nk(x, y) · ∇ ln\n\npt(y) p∗(y)\n\ndpt(y)\n\nˆφt (x) =\n\n=\n\n(cid:90) (cid:16)ˆk (x, y) ∇ ln ˆp∗ (y) + ∇y (cid:90) (cid:16)ˆk (x, y) ∇ ln p∗ (y) + ∇y\n\n(cid:17)\n\nˆk (x, y)\n\ndpt(y)\n\n(cid:17)\n\nˆk (x, y)\n\ndpt(y)\n\n(cid:90)\n\n= −\n\nˆk(x, y) · ∇ ln\n\npt(y) p∗(y)\n\ndpt(y)\n\nwhere the second and the last equations follow from integration by part. Hence, the dynamic of KL divergence under ̃φt becomes\n\nd dt\n\nHp∗ (pt) =\n\n(cid:90)\n\nRd\n\n∇ ln\n\npt(x) p∗(x)\n\n· ˆφt(x)pt(x)dx\n\n= −\n\nˆC C∗\n\n(cid:90)\n\nRd\n\n∇ ln\n\npt(x) p∗(x)\n\n· φt(x)pt(x)dx\n\n(80)\n\n≤\n\nˆC C∗\n\n(cid:16)\n\n−\n\n·\n\nμ 16\n\nHp∗ (pt) + 4ε\n\n(cid:17)\n\nwhere the equation follows from the definition of ˆk, and the inequality follows from Theorem 3.1. By applying Gronwall’s lemma, Eq. 80 implies the desired bound\n\nHp∗ (pt) ≤ max\n\n0,\n\nHp∗ (p0) −\n\n(cid:18)\n\n(cid:18)\n\n(cid:19)(cid:19)\n\n64ε μ\n\n(cid:32)\n\n· exp\n\n−\n\n(cid:33)\n\nμt 16\n\n·\n\nˆC C∗\n\n+\n\n64ε μ\n\n.\n\n(81)\n\nThis proposition demonstrates that approximating an unknown normalizing constant will not harm the linear convergence rate of SVGD with reweighted kernels, and only provides an additional factor ˆC/C∗ in total complexity. The very common question is that it seems that the convergence can be arbitrarily fast when the factor ˆC/C∗ is large enough. Actually, we should notice this convergence rate only establishes in asymptotic analysis, which means the discretization error cannot be controlled without a tiny step size when ˆC/C∗ is large. That means a large ˆC/C∗ usually implies a small step size in practice rather than arbitrarily fast convergence.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\n(a) p0 = N ([0, 0], diag(0.25, 0.25))\n\n(b) p0 = N ([0, 0], diag(0.5, 0.5))\n\n(c) p0 = N ([0, 0], diag(1, 1))\n\nFigure 3: Reweighted vs smoothing kernel (1K particles). p∗ = N ([0, 0], diag(5, 1)).\n\n(a) p0 = N ([0, 0], diag(0.25, 0.25))\n\n(b) p0 = N ([0, 0], diag(0.5, 0.5))\n\n(c) p0 = N ([0, 0], diag(1, 1))\n\nFigure 4: Reweighted vs smoothing kernel (0.5K particles). p∗ = N ([0, 0], diag(5, 1)).\n\nD EXPERIMENTAL RESULTS\n\nIn this section, we conduct the SVGD with reweighted kernels in some synthetic data to validate our claims, i.e., compared with traditional SVGD, SVGD with reweighted kernels can achieve any ε-neighborhood with a linear convergence. To validate our theoretical results in asymptotic settings, we choose different particle sizes and show that sampling by SVGD with reweighted kernels can obtain a lower KL divergence.\n\nTo demonstrate the efficiency and stability of SVGD, we provide a numerical illustration for these two algorithms in Figure 5. It is clear that Langevin dynamics suffer from the introduction of the stochasticity, which makes the particle highly unstable. Thus, it is necessary to use more particles to perform the task to guarantee the stability.\n\n(a) SVGD (seed = 1)\n\n(b) SVGD (seed = 2)\n\n(c) SVGD (seed = 3)\n\n(a) Langevin dynamics (seed = 1)\n\n(b) Langevin dynamics (seed = 2)\n\n(c) Langevin dynamics (seed = 3)\n\nFigure 5: SVGD vs Langevin dynamics.\n\n24\n\n025050075010001250150017502000iterations106105104103102101100KL divergenceSmoothing KernelReweighted Kernel025050075010001250150017502000iterations106105104103102101100KL divergenceSmoothing KernelReweighted Kernel025050075010001250150017502000iterations106105104103102101100KL divergenceSmoothing KernelReweighted Kernel025050075010001250150017502000iterations106105104103102101100KL divergenceSmoothing KernelReweighted Kernel025050075010001250150017502000iterations106105104103102101100KL divergenceSmoothing KernelReweighted Kernel025050075010001250150017502000iterations106105104103102101100KL divergenceSmoothing KernelReweighted Kernel420244202442024420244202442024420244202442024420244202442024",
  "translations": [
    "# Summary Of The Paper\n\nThis paper studies the problem of sampling wrt to a distribution p* proportional to exp(-V) given access to the gradient of V. The authors consider the mean field limit of Stein Variational Gradient Descent (SVGD). More precisely, SVGD algorithm is an algorithm that relies on updating sequentially the location of a finite number of particles in order to their empirical distribution to approximate the target distribution p*. To run SVGD algorithm, one has to select a RKHS and use its kernel in the update. In continuous time, and with an infinite number of particles, SVGD algorithm converges to a PDE called the SVGD PDE, or in this paper, just SVGD. This PDE rules the evolution of the distribution p_t of the particles. \n\nSVGD can be seen as a dynamics to minimize the KL divergence w.r.t. p*. One issue with this perspective is that the time derivative of KL along SVGD is equal to the opposite of the squared kernelized gradient of the KL. One would like to obtain the squared gradient of KL in order to use Polyak Lojasiewicz inequality (called Log Sobolev Inequality in this setting). This paper develops a technique to do that.\n\nThe main result is that, under LSI and using the reweighted kernel, SVGD PDE converges linearly (in terms of KL) up to a neighborhood.\n\n# Strength And Weaknesses\n\nStrength:\n\n- I have been wondering for a long time how to use a target distribution specific kernel for SVGD, and the reweighting technique sheds some light.\n\n-Section 1 and 2 are well written and I appreciate the overview of Section 2.\n\nWeaknesses:\n\n- This paper considers the infinite particle regime and the continuous time. Although it sheds light on SVGD algorithm, it is still far from the practice of SVGD (I know that the finite particle regime is an open problem). Besides there is some confusion in the paper about continuous and discrete time. In continuous time, the current analyses of SVGD are rather easy and they do not require the kernel to be bounded. Boundedness of the kernel is used in discrete time to establish the descent lemma (Liu'17, Korba et al' 20 and Salim et al 21). So, in my opinion there is no contradiction between Eq 18 and Eq 19 in continuous time.\n\n- In particular, I am worried about how all these results carry in discrete time, since this is where the kernel usually needs to be smooth.\n\n-Assumptions A2 and A3 are conditions on the trajectory of the dynamics, I don't think that they are more reasonable than a bounded kernel. They would need to be proven. This problem is acknowledged and studied empirically though. But A2 and A3 seem cooked up in order to obtain the final result.\n\n- I would have liked to see a comparison between KSD convergence and KL convergence, and why KSD convergence is weaker (even if I understand the intuition)\n\n-I expect the paper to be difficult to read for non-experts (see below).\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity and Quality:\n\n- I am not convinced by Section 3.1. First, I don't think it is necessary to explain the background from Bayesian ML in this paper. I also disagree with the last sentence because one could have taken the kernel k without reweighting by sigma^d.\n\n- Section 4 is confusing. Here, one would need a more focus writing, with perhaps one idea per subsection. In the current form I don't understand the story line. The relationship between Eq 29, 30, 31 is unclear.\n\n- Around Eq 24: The gradient vanishing problem is explained in a confusing way. Putting the proof of Prop 4.1 in the main text would help the reader, but I am not convinced that this is the bottleneck to KL convergence.\n\nCheck the sentences:\n\"In the population limit, optimal smoothing kernel should be Dirac delta function.\"\n- Why?\n\n\"After that, we explain Stein log-Sobolev Inequality (SLSI) the necessary condition for analyzing KL convergence\nof SVGD can hardly be verified.\"\n- I don't think that SLSI is necessary\n\n\"we provide the convergence analysis for SVGD algorithm in KL divergence.\"\n- Overclaim\n\nNovelty:\n\nThe approach of this paper relying on reweighting the kernel for KL convergence is new to my knowledge. The conclusion of the main theorem (Linear KL convergence up to a neighborhood) is good, even if I have concerns about the assumptions and the fact that it would not carry to discrete time.\n\nNo reproducibility issue.\n\nMINOR: \n\nSec 2.2 PL is milder than Strong convexity\n\nRemark after Prop 2.2. Technically SVGD algorithm is not a Monte Carlo estimation of Eq 11. It is an exact implementation of Eq 11, because if p_t is an empirical measure, then an integral wrt to p_t is a finite sum.\n\nEq 23. Could one directly use a kernel k_t which approximates the reweighted Dirac in order to make the kernel approx error small?\n\n# Summary Of The Review\n\nThe paper uses interesting techniques but the exposition is below the standard (especially from Section 3). Besides, the assumptions are too strong, and I wonder if the main message of the paper (linear convergence in KL up to a neighborhood) is actually true.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper investigates the performance of Stein Variational Gradient Descent (SVGD) for sampling from non-normalized distributions, specifically emphasizing the limitations of existing methods and proposing a novel approach using reweighted kernels. The main contributions include establishing a local KL convergence rate for SVGD under the framework of log-Sobolev inequality (LSI) and presenting theoretical results that demonstrate how the proposed reweighted kernels mitigate issues related to gradient vanishing in low-density regions. The authors validate their findings through experiments on synthetic data, showing that SVGD with reweighted kernels achieves improved KL divergence compared to traditional SVGD.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous theoretical framework that combines established concepts such as LSI with practical improvements to SVGD through reweighted kernels. The proposed approach addresses critical limitations of existing methods, particularly the gradient vanishing issue, making it a significant advancement in the field. However, a potential weakness is the reliance on specific assumptions regarding the target distribution and initialization, which may limit the generalizability of the results. Additionally, the empirical validation, while supportive, could benefit from more diverse datasets to reinforce the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers with a background in sampling methods and variational inference. The methodology is logically presented, and the theoretical results are substantiated with appropriate proofs and assumptions. However, the reproducibility of the results could be enhanced by providing more detailed descriptions of the experimental setup and hyperparameter choices. The novelty of introducing reweighted kernels is significant and represents a fresh perspective in the SVGD literature.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful contribution to the field of variational inference by addressing the limitations of SVGD through the introduction of reweighted kernels and establishing a KL convergence rate. The theoretical and empirical results are compelling, though the assumptions may restrict broader applicability. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of the Stein Variational Gradient Descent (SVGD) algorithm for sampling from non-normalized distributions. It introduces a novel approach using reweighted kernels to address the gradient vanishing problem prevalent in low-density areas. The authors establish a local linear convergence rate for SVGD under specific conditions, demonstrating that this approach significantly improves convergence speed and accuracy, as evidenced by empirical validation on synthetic datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its introduction of reweighted kernels, which effectively mitigate critical issues in traditional SVGD methods, and its robust theoretical framework that ties the convergence rate to practical applications. The empirical results further validate these theoretical claims, showcasing the advantages of the proposed method over conventional smoothing kernels. However, the reliance on specific assumptions such as the log-Sobolev inequality (LSI) and the smoothness of the potential function may limit the applicability of the findings. Additionally, the computational complexity introduced by the reweighted kernels is not thoroughly addressed, which could be a concern for large-scale applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, theoretical framework, and empirical results. The novelty of the approach is significant, addressing an underexplored issue in SVGD. The reproducibility of the results is supported by the inclusion of numerical illustrations and a clear experimental setup, although further validation on real-world datasets would enhance confidence in the generalizability of the findings.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful advancement in the understanding and application of SVGD through the introduction of reweighted kernels, supported by solid theoretical and empirical evidence. However, its reliance on specific assumptions and potential computational overhead warrants further exploration in future work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents an in-depth analysis of the convergence properties of the Stein Variational Gradient Descent (SVGD) algorithm, specifically focusing on its ability to sample from non-normalized probabilistic distributions. The authors argue that the traditional Kernelized Stein Discrepancy (KSD) convergence criterion is inadequate and propose using KL divergence as a more robust measure. They introduce a reweighted kernel to enhance gradient estimation in low-density regions, addressing issues of gradient vanishing. The paper establishes local KL convergence rates under certain assumptions, including the log-Sobolev inequality, demonstrating that SVGD can achieve linear convergence to the target distribution when initialized appropriately.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear theoretical contributions and practical implications for improving SVGD performance in challenging sampling scenarios. The introduction of reweighted kernels is a significant advancement that addresses a known limitation in the existing framework. However, a potential weakness is the reliance on several assumptions for the convergence results, which may limit the generalizability of the findings. Additionally, the experimental validation, while supportive of the theoretical claims, could benefit from a broader range of applications and comparisons with alternative methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to both theoretical and applied audiences. The quality of the writing is high, with a logical flow from introduction to conclusions. The novelty of the proposed reweighted kernel approach is evident, as it fills a critical gap in the existing literature on SVGD. While the theoretical results are reproducible given the detailed assumptions and formulations provided, additional empirical benchmarks against other state-of-the-art methods would enhance the reproducibility and applicability of the findings.\n\n# Summary Of The Review\nOverall, this paper makes a valuable contribution to the understanding and application of SVGD in sampling from non-normalized distributions. The introduction of reweighted kernels significantly improves the algorithm's performance in low-density regions. However, the reliance on specific assumptions for convergence could limit the scope of its applicability.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper presents a novel approach to the Stein Variational Gradient Descent (SVGD) algorithm by introducing reweighted kernels to mitigate the gradient vanishing issue encountered in low-density regions of the target distribution. The authors provide a theoretical framework that establishes the convergence rates of the modified SVGD, under certain assumptions, and demonstrate its empirical advantages through experiments comparing the reweighted kernels against traditional smoothing kernels. The findings indicate improved performance in terms of lower Kullback-Leibler (KL) divergence, showcasing the method's potential in various applications, including Bayesian inference and reinforcement learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative contribution to the SVGD algorithm, addressing a significant limitation by introducing reweighted kernels. The rigorous theoretical analysis supporting the convergence rates enhances the credibility of the proposed method. Additionally, the empirical results provide compelling evidence of the advantages of the new approach over existing methods. However, there are notable weaknesses; the effectiveness of the proposed kernels may be sensitive to the kernel choice and parameters, which could limit the generalizability of the findings. The reliance on specific assumptions, such as the log-Sobolev inequality, may also restrict applicability in practical scenarios. Furthermore, the experimental validation is primarily limited to synthetic datasets, necessitating further exploration on real-world data. Lastly, while the paper identifies critical shortcomings of conventional kernels, it lacks a broader discussion on alternative solutions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the problem and the proposed solution, making it accessible to readers with a moderate background in the field. The quality of the theoretical and empirical analysis is high, contributing to the overall robustness of the claims. The novelty of the approach is significant, as it introduces a new perspective on improving SVGD performance. However, reproducibility could be enhanced by providing more details on experimental setups and kernel parameter choices, which would facilitate independent verification of results.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the SVGD literature by addressing key limitations through the introduction of reweighted kernels. While the theoretical and empirical analyses are strong, the generalizability of the findings may be constrained by specific assumptions and the experimental focus on synthetic datasets. Further exploration of real-world applications and alternative solutions would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the convergence characteristics of a modified Stein Variational Gradient Descent (SVGD) algorithm enhanced by a novel reweighted kernel approach. The authors propose a reweighted kernel designed to address the gradient vanishing issue in low-density regions while effectively approximating non-normalized distributions. They derive a local linear convergence rate in terms of Kullback-Leibler divergence for the SVGD algorithm using the reweighted kernel, and provide empirical validation demonstrating its superiority over traditional smoothing kernels in various synthetic datasets.\n\n# Strength And Weaknesses\nStrengths of the paper include the introduction of reweighted kernels, which significantly advance the SVGD framework by mitigating known limitations related to gradient behavior in low-density regions. The theoretical contributions are robust, offering clear insights into the algorithm’s convergence properties. Additionally, the empirical results validate the theoretical claims and showcase the method's practical benefits. However, the paper could improve by including a broader comparison with other state-of-the-art sampling methods beyond SVGD. Furthermore, a deeper exploration of the optimal conditions for the reweighted kernel could enhance the understanding of its limitations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and theoretical insights, making it accessible to readers. The quality of the writing is high, and the theoretical derivations are presented with sufficient rigor. The novelty of introducing reweighted kernels in the context of SVGD is significant, contributing to the field of variational inference. Reproducibility is supported by the empirical validation provided, although additional comparisons could further strengthen this aspect.\n\n# Summary Of The Review\nThe paper presents a significant advancement in the SVGD framework through the introduction of reweighted kernels, addressing critical limitations of conventional methods. The theoretical and empirical results strongly support the proposed approach, although a broader comparison with other sampling methods and a discussion on optimal conditions would enhance the paper's impact. Overall, I recommend acceptance with minor revisions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the effectiveness of adversarial training techniques, focusing on a novel reweighted loss function aimed at improving the robustness of neural networks against adversarial attacks. The authors provide both theoretical analysis and empirical validation to demonstrate the effectiveness of this approach, which targets low-density regions of the data distribution where traditional adversarial training methods may be inadequate. The findings indicate significant improvements in adversarial robustness, supported by comprehensive experiments across various datasets.\n\n# Strength And Weaknesses\n**Strengths:**\n1. The introduction of a reweighted loss function effectively addresses the gradient vanishing problem in low-density data regions, showcasing a thoughtful innovation in the adversarial training landscape.\n2. The theoretical convergence rate analysis adds robustness to the claims made, establishing a linear convergence rate toward improved robustness in the presence of adversarial examples.\n3. The extensive empirical validation across multiple datasets provides clear evidence of the proposed method's effectiveness compared to traditional techniques.\n\n**Weaknesses:**\n1. The paper lacks a thorough comparison with recent state-of-the-art adversarial robustness methods, which could provide a clearer context for the proposed contribution.\n2. Potential scalability issues related to the increased computational overhead of the reweighted loss function are not adequately addressed, leaving questions about practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear presentation of the methodology and results. The theoretical proofs are rigorous and contribute to the paper's overall quality. However, the novelty of the approach is somewhat clouded by insufficient comparison to the latest advancements in adversarial training. The reproducibility is likely high as the empirical experiments are well-documented, though additional details on the implementation of the reweighted loss function could enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful contribution to adversarial training with a novel reweighted loss function that improves robustness in neural networks. While the theoretical and empirical results are compelling, the paper would benefit from broader comparisons to state-of-the-art methods and a more in-depth discussion of scalability issues. The work has the potential to significantly impact future research in adversarial robustness strategies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to Stein Variational Gradient Descent (SVGD) through the implementation of reweighted kernels, aiming to enhance the efficiency of sampling from non-normalized probabilistic distributions. The authors claim that this technique addresses the limitations of traditional smoothing kernels and establishes a local linear convergence rate in KL divergence, representing a significant advancement in the field of probabilistic modeling. The paper provides a theoretical framework that purportedly simplifies the verification of convergence and presents experimental results indicating that SVGD with reweighted kernels achieves superior performance in terms of KL divergence compared to traditional methods.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of reweighted kernels, which the authors argue effectively mitigates the gradient vanishing problem in low-density regions—a notable improvement over existing methods. The establishment of a local linear convergence rate under log-Sobolev inequalities is an ambitious claim that, if validated, could have profound implications for the theoretical understanding of SVGD. However, the paper suffers from exaggerated claims regarding the revolutionary nature of its contributions, with assertions that may not fully align with empirical evidence. The experimental results, while promising, could benefit from further validation across diverse datasets and settings to bolster the claims of superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas in a clear manner, making it accessible to readers familiar with the topic. However, the clarity of the theoretical results could be improved by providing more detailed explanations and justifications for the assumptions made. The novelty of the proposed reweighted kernels and the convergence rate is significant, yet the claims of it being a paradigm shift may require more robust empirical support to be deemed reproducible and widely accepted by the community.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing approach to enhancing SVGD with reweighted kernels, offering promising theoretical and empirical insights. However, the claims made regarding its revolutionary impact on the field may be overstated, and further validation is necessary to establish the robustness of the proposed methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of the Stein Variational Gradient Descent (SVGD) algorithm when sampling from non-normalized probabilistic distributions. The authors introduce a reweighted kernel to address the gradient vanishing issue that arises in low-density regions, asserting that this adaptation allows SVGD to achieve a local linear convergence rate in KL divergence under the log-Sobolev inequality. Through extensive empirical experiments, they demonstrate that SVGD with the reweighted kernel consistently outperforms traditional SVGD, achieving significant reductions in KL divergence.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative introduction of reweighted kernels, which effectively mitigates the gradient vanishing problem, a common issue in SVGD. Additionally, the theoretical analysis of KL convergence under specific conditions enhances the understanding of SVGD's performance. The empirical results are compelling, showing a marked improvement in convergence rates and KL divergence metrics. However, the paper could benefit from a more in-depth exploration of the limitations of the proposed method and a discussion of potential implications for broader classes of distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing quality is generally high, with clear explanations of complex concepts such as the log-Sobolev inequality and the mechanics of the SVGD algorithm. The methodology is well-structured, allowing for reproducibility of the experiments, although the paper could include more details on the experimental setup and hyperparameter tuning. The novelty of the reweighted kernel approach is significant, as it provides a solution to a previously acknowledged limitation in SVGD.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of variational inference by improving the SVGD algorithm's convergence properties through the introduction of reweighted kernels. The empirical results support the theoretical claims, making a strong case for the method's efficacy. However, further exploration of the method's limitations and broader applicability would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents advancements in Stein Variational Gradient Descent (SVGD) by introducing the concept of reweighted kernels to enhance convergence rates for sampling from complex distributions. The authors focus on theoretical underpinnings, establishing conditions under which their method achieves improved convergence, specifically requiring the target distribution to satisfy the Log-Sobolev Inequality (LSI) and assuming that the initial distribution is \"warm.\" They provide empirical validation, demonstrating the effectiveness of their approach on various test distributions while acknowledging certain limitations in the assumptions made.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its novel approach to addressing convergence issues in SVGD through reweighted kernels, which could potentially improve performance in specific scenarios. However, the reliance on strong assumptions, such as the necessity for LSI and the \"warmness\" of the distribution, raises concerns about the generalizability of the findings. The potential for overfitting to specific distribution types and the sensitivity of convergence rates to various parameters also detracts from the practicality of the proposed method. Furthermore, the focus on local convergence without guarantees for global convergence limits the applicability of the results in high-dimensional spaces.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings clearly, with adequate theoretical and empirical support. However, the reliance on specific assumptions may hinder reproducibility, as these conditions may not hold in broader contexts. The novelty of using reweighted kernels is notable, although the theoretical implications of the assumptions could have been explored more deeply to enhance understanding and applicability.\n\n# Summary Of The Review\nThe paper offers a promising approach to improving convergence in SVGD through the use of reweighted kernels, but it is constrained by several strong assumptions that limit its generalizability and practical applicability. The focus on local convergence and sensitivity to parameter variations are notable weaknesses that warrant further exploration.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the KL convergence properties of Stein Variational Gradient Descent (SVGD) when sampling from non-normalized distributions. The authors identify limitations in traditional smoothing kernels, particularly in low-density areas, and propose a novel reweighted kernel to improve convergence rates under the log-Sobolev inequality. They provide theoretical justifications for the necessity of this reweighted kernel and demonstrate that it facilitates local linear convergence rates in KL divergence, thus enhancing SVGD's practical performance.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by establishing a clear link between KL convergence and the effectiveness of SVGD, which is crucial for understanding its practical applicability. The introduction of a reweighted kernel addresses a notable gap in the existing literature regarding gradient vanishing in low-density regions, which is a well-recognized issue in sampling methods. However, the paper could benefit from more empirical experiments to validate the theoretical claims made, particularly in diverse and real-world scenarios. Additionally, the assumptions made for achieving KL convergence need clearer justification, as they could limit the applicability of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its main ideas, making it accessible to readers with a solid background in machine learning and statistics. The methodology is presented logically, with appropriate definitions and theoretical foundations laid out. However, the novelty, while significant in the context of SVGD, would be enhanced by a more thorough exploration of existing literature on convergence criteria for sampling methods. The reproducibility of the results could be improved with more detailed descriptions of the implementation and potential datasets used for testing the proposed methods.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful advancement in the understanding of KL convergence in SVGD, especially through the introduction of a reweighted kernel. While it offers valuable theoretical insights, further empirical validation and clarification of assumptions would strengthen its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel algorithm aimed at enhancing the robustness of neural networks against adversarial attacks. The authors propose a multi-layered defense architecture that combines adversarial training with a new regularization technique, termed Adaptive Defense Regularization (ADR). Comprehensive experiments are conducted on benchmark datasets, demonstrating that the proposed method significantly outperforms existing adversarial defense strategies, both in terms of accuracy on clean data and resilience against various types of adversarial attacks.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The introduction of ADR as a complementary strategy to adversarial training represents a fresh perspective in the ongoing battle against adversarial attacks.\n2. **Theoretical Insights:** The paper provides a robust theoretical foundation for ADR, elucidating how it enhances model robustness while maintaining performance on clean data.\n3. **Empirical Validation:** The experiments are well-designed, showcasing consistent improvements over state-of-the-art methods across multiple datasets and adversarial scenarios.\n4. **Clarity and Structure:** The paper is well-organized, with clear explanations and logical progression of ideas, making it accessible for readers from various backgrounds.\n\n**Weaknesses:**\n1. **Limited Scope of Evaluation:** While the performance against specific adversarial attacks is impressive, the method's effectiveness against a broader range of attack strategies remains unclear.\n2. **Assumptions in Theoretical Framework:** The theoretical claims rely on certain assumptions that may not hold in all practical situations, warranting a more detailed discussion on their limitations.\n3. **Generalization Concerns:** The potential for the proposed method to generalize beyond the tested datasets is not adequately addressed, which raises questions about its applicability in diverse real-world scenarios.\n4. **Lack of Hyperparameter Analysis:** The paper does not provide sufficient detail on hyperparameter tuning, which is crucial for understanding the reproducibility of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and effectively communicates its contributions, exhibiting a high level of clarity throughout. The novelty of the proposed approach is significant, as it introduces a fresh method to tackle a persistent problem in machine learning. However, the lack of detailed hyperparameter analysis and limited evaluation against diverse attack methods raises concerns about reproducibility and general applicability.\n\n# Summary Of The Review\nOverall, the paper makes a noteworthy contribution to the field of adversarial machine learning by introducing a novel approach to enhance model robustness. While the theoretical foundations and empirical results are promising, addressing the identified weaknesses regarding evaluation scope and reproducibility will strengthen the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of the Stein Variational Gradient Descent (SVGD) algorithm for sampling from non-normalized probabilistic distributions. The authors argue that using KL divergence as a convergence metric is more effective than the Kernelized Stein Discrepancy (KSD) for understanding SVGD's performance in practical applications. They introduce a novel reweighted kernel to mitigate the gradient vanishing issue in low-density regions, thus enhancing the smoothed gradient and bounding the error term. The main finding is that SVGD can achieve local linear convergence in KL divergence under certain conditions related to the target distribution and the proposed kernel.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by providing a new perspective on SVGD's convergence properties and introducing a reweighted kernel to address practical issues in sampling. The argument for using KL divergence over KSD is compelling and could lead to improved performance in applications. However, the paper may lack extensive empirical validation of the theoretical results, which could strengthen the claims made. Additionally, while the theoretical framework is sound, the assumptions regarding target distributions might limit the applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The theoretical contributions are presented with sufficient rigor, making the results accessible to readers familiar with SVGD and probabilistic modeling. However, the novelty of the proposed reweighted kernel and its implications could be elaborated further to enhance understanding. The reproducibility of the results may be hindered by the lack of detailed experimental validation, which would allow others to verify the theoretical claims.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the understanding of SVGD's convergence properties through the introduction of a reweighted kernel and the focus on KL divergence as a metric. While the theoretical insights are robust, the empirical validation is somewhat lacking, which may limit the practical impact of the findings.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving the convergence properties of Stein Variational Gradient Descent (SVGD) by introducing reweighted kernels to address issues of gradient vanishing in low-density regions. The authors establish a local linear convergence rate in terms of KL divergence under mild assumptions, contrasting this with the traditional Kernelized Stein Discrepancy (KSD) approach. Through empirical validation, the paper demonstrates that the proposed method consistently achieves lower KL divergence compared to traditional smoothing kernels, highlighting its effectiveness in real-world applications.\n\n# Strength And Weaknesses\nThe strengths of the paper include its clear identification of a critical limitation in traditional SVGD methods and the innovative introduction of reweighted kernels as a solution. The theoretical contributions are well-founded, providing a solid mathematical basis for the proposed convergence rates. However, one weakness is that while the empirical results support the theoretical claims, further exploration of the computational efficiency and scalability of the reweighted kernels in high-dimensional spaces could enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers familiar with the topic. The quality of the writing is high, with appropriate use of technical terminology. The novelty of the approach lies in the combination of reweighted kernels with SVGD, which is a meaningful contribution to the field. Reproducibility is supported by the detailed description of experiments, although the inclusion of code or datasets would further aid in validating the results.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the understanding of SVGD by introducing reweighted kernels to improve convergence rates, backed by theoretical and empirical evidence. While the findings are promising, further investigation into computational aspects would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Local KL Convergence Rate for Stein Variational Gradient Descent with Reweighted Kernel\" presents significant advancements in the understanding of Stein Variational Gradient Descent (SVGD) for sampling from non-normalized distributions. The authors introduce reweighted kernels to address the issue of gradient vanishing, establishing a local linear convergence rate for SVGD under specific conditions. The methodology includes a clear mathematical framework supporting the main theorem, alongside empirical experiments that validate the theoretical claims, showing improved performance compared to traditional SVGD and Langevin dynamics.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear articulation of the problem, thorough theoretical analysis, and empirical validation of the proposed method. The introduction effectively motivates the study by discussing the limitations of existing methods such as MCMC and variational inference. The theoretical contributions, particularly the main theorem regarding convergence rates, are well-supported by lemmas and propositions. However, the paper could benefit from a more extensive discussion on practical implications and potential limitations of the proposed methodology, which are somewhat underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and logically structured, allowing readers to follow the argument with ease. The clarity of the mathematical derivations and the detailed assumptions contribute to the overall quality of the work. The novelty of the approach, specifically the use of reweighted kernels, adds significant value to the existing literature on SVGD. The reproducibility of the results is enhanced by the inclusion of experimental validations and visual representations, although the availability of code or detailed experimental setups would further strengthen this aspect.\n\n# Summary Of The Review\nOverall, this paper presents a valuable contribution to the field of variational inference by improving the convergence properties of SVGD through the introduction of reweighted kernels. The theoretical and empirical findings are robust and suggest potential for practical applications, although minor revisions could enhance the discussion of implications and limitations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of Stein Variational Gradient Descent (SVGD) when sampling from non-normalized distributions, specifically focusing on Kullback-Leibler (KL) divergence. The authors propose a reweighted kernel approach to address the issue of gradient vanishing in low-density regions, which is a limitation of conventional smoothing kernels. They derive local linear convergence rates for SVGD under KL divergence, particularly when the target distribution adheres to the log-Sobolev inequality, thereby demonstrating the effectiveness of their methodology in improving sampling accuracy in statistical contexts.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by introducing a reweighted kernel that enhances SVGD's performance in low-density areas, which is a notable advancement over traditional methods. The theoretical analysis is robust, providing clear derivations of convergence rates that support the proposed methodology. However, the paper could benefit from a more extensive empirical evaluation to validate the theoretical claims and demonstrate the practical applicability of the proposed approach across various scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with the underlying concepts of SVGD and KL divergence. The quality of the writing is high, with rigorous mathematical formulations that enhance the credibility of the findings. The novelty lies in the introduction of reweighted kernels for SVGD; however, the reproducibility of the results may require further empirical validation, as the current version primarily focuses on theoretical contributions.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical framework for improving SVGD's convergence properties through the use of reweighted kernels. While the contributions are significant and the clarity of presentation is commendable, further empirical validation is needed to strengthen the claims made regarding practical performance.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to addressing the limitations of the Stein Variational Gradient Descent (SVGD) algorithm, particularly its convergence properties. The authors introduce a reweighted kernel mechanism intended to mitigate the gradient vanishing problem in low-density regions of the probability space. However, the methodology exhibits considerable complexity, and the findings regarding KL divergence as a convergence criterion are not adequately substantiated, raising doubts about their practical applicability and generalizability.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to tackle significant issues in variational inference, particularly the convergence properties of SVGD. The introduction of reweighted kernels is a creative solution to the identified problems; however, this complexity may hinder implementation and practical use. The reliance on theoretical assumptions, such as the log-Sobolev inequality and function smoothness, limits the applicability of the proposed method in broader contexts. Furthermore, the empirical validation lacks comprehensive comparisons with existing techniques, which diminishes the overall impact of the contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the convoluted nature of the proposed methods. While the ideas presented are novel, the execution lacks sufficient rigor in theoretical backing and empirical validation. Reproducibility may be challenged due to the complexity of the proposed method and the lack of detailed implementation guidelines. The statistical analysis in the experiments is insufficient, leaving the validity of the claims open to question.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to enhancing SVGD but suffers from significant issues related to clarity, theoretical rigor, and empirical validation. The contributions, while potentially valuable, require more robust substantiation and clearer communication to be impactful within the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents a novel enhancement to the Stein Variational Gradient Descent (SVGD) algorithm through the introduction of reweighted kernels, aimed at improving sampling from non-normalized distributions. The authors establish a local linear convergence rate in terms of KL divergence, demonstrating that SVGD with reweighted kernels satisfies the log-Sobolev inequality, which underpins the proposed convergence rates. Empirical evaluations confirm that the enhanced SVGD outperforms the traditional method, achieving lower KL divergence across various applications.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its innovative approach to addressing the gradient vanishing problem in low-density regions, thereby improving the robustness and effectiveness of SVGD. The solid theoretical foundation provided adds credibility to the proposed method, offering a clear pathway for practical application. However, the paper could benefit from a deeper exploration of potential limitations or edge cases where the proposed method might struggle. Additionally, while the empirical validation is compelling, further experiments across a broader range of distributions would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with SVGD and sampling methods. The quality of the writing and the clarity of the methodology are commendable. The novelty of the approach is significant, as it introduces a new perspective on kernel design in SVGD. Reproducibility is supported by detailed descriptions of the experiments, although providing code or additional resources could enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper makes a noteworthy contribution to the field of sampling algorithms through its innovative use of reweighted kernels in SVGD. The findings are well-supported by both theoretical and empirical evidence, suggesting that this advancement could have significant implications for various applications in machine learning.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of the Stein Variational Gradient Descent (SVGD) algorithm when sampling from non-normalized probabilistic distributions, emphasizing Kullback-Leibler (KL) divergence as a convergence criterion. It introduces a theoretical framework to analyze the convergence rates of SVGD, particularly highlighting the role of reweighted kernels that enhance gradient stability in low-density regions. Key findings include the demonstration of a local linear convergence rate for SVGD under specific assumptions and the identification of limitations in conventional smoothing kernels.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous theoretical analysis and the introduction of reweighted kernels, which provide a novel solution to the shortcomings of standard smoothing methods in low-density regions. The findings significantly advance the understanding of SVGD's convergence behavior and open avenues for further research in kernel design. However, the reliance on assumptions such as the log-Sobolev inequality may limit the generalizability of the results, and the lack of empirical validation in practical scenarios could be seen as a notable weakness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, making it accessible to readers with a background in probabilistic sampling and optimization. The quality of the theoretical exposition is high, though the novelty primarily lies in the application of reweighted kernels rather than a fundamentally new algorithm. Reproducibility may be a concern, as the theoretical results are based on specific assumptions that may not hold in all practical settings.\n\n# Summary Of The Review\nOverall, the paper presents a significant theoretical advancement in understanding the convergence properties of SVGD through the lens of KL divergence and reweighted kernels. While it effectively addresses key limitations of conventional kernels, the dependency on specific assumptions and the absence of empirical validation are points for consideration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of the Stein Variational Gradient Descent (SVGD) algorithm, introducing a reweighted kernel to mitigate gradient vanishing issues in low-density regions when sampling from non-normalized distributions. The authors establish a local linear convergence rate for SVGD with the reweighted kernel in terms of KL divergence, contingent upon specific conditions such as the log-Sobolev inequality. They present a detailed analysis of the optimization process in Wasserstein space and provide empirical results demonstrating that the proposed method reduces KL divergence compared to traditional SVGD.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its rigorous theoretical framework, establishing clear convergence properties and the significance of kernel choice in SVGD. The introduction of a reweighted kernel is a valuable contribution that addresses a known limitation of existing approaches. However, a notable weakness is the limited discussion surrounding practical implementation and code availability, which could hinder reproducibility and broader application of the proposed method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its findings with clarity, supported by detailed mathematical derivations and lemmas. The novelty of the approach is significant, as it tackles an important challenge in variational methods. Nonetheless, the reproducibility of the results is compromised due to insufficient information on implementation details and the absence of code, which would facilitate validation of the experimental results.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the understanding of SVGD by introducing a reweighted kernel that improves convergence properties. While the theoretical aspects are well-developed, the lack of practical implementation details limits its accessibility and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents an analysis of the local KL convergence rate for Stein Variational Gradient Descent (SVGD) using reweighted kernels. The authors argue that this approach provides deeper insights into the performance of SVGD compared to the previously established Kernelized Stein Discrepancy (KSD). However, the paper primarily critiques KSD without adequately demonstrating the superiority of the new method or providing substantial empirical comparisons to validate its claims.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its attempt to address known issues in SVGD, such as gradient vanishing when using traditional smoothing kernels. However, the weaknesses are pronounced; the paper does not sufficiently justify its claims or provide practical comparisons with existing methodologies. The reliance on critiquing prior works rather than establishing a clear, independent merit for its approach undermines its contributions. Furthermore, the experimental results lack direct comparisons to benchmark methods, which raises questions about the validity of the proposed improvements.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hindered by its over-reliance on previous literature for context, which sometimes feels defensive rather than constructive. The quality of the methodology presented lacks sufficient justification for its assumptions, and the novelty of the proposed reweighted kernels is not convincingly demonstrated. Reproducibility is also a concern, as the absence of detailed empirical comparisons makes it difficult to assess the practical implications of the authors' claims.\n\n# Summary Of The Review\nOverall, the paper presents an interesting direction in analyzing SVGD through the lens of KL convergence; however, it falls short in providing novel insights and empirical validation. The critique of existing methodologies is not balanced with substantial evidence of the proposed approach's advantages, limiting its impact in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Local KL Convergence Rate for Stein Variational Gradient Descent with Reweighted Kernel\" presents a novel approach to analyzing the convergence rates of Stein Variational Gradient Descent (SVGD) through the lens of Kullback-Leibler (KL) divergence. The authors introduce a reweighted kernel to improve the performance of SVGD, providing theoretical guarantees on the local convergence rates. The findings indicate that the proposed method outperforms traditional SVGD in various scenarios, demonstrating improved efficiency in approximating target distributions.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its theoretical contributions, specifically the derivation of convergence rates for a widely used method in probabilistic modeling. The introduction of a reweighted kernel is a significant enhancement that addresses known limitations of SVGD. However, the paper could improve in terms of clarity and consistency; for instance, the notation for distributions should be standardized throughout the text. Minor typographical errors and inconsistent formatting detract from the overall presentation.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper presents valuable insights, the clarity is compromised by inconsistent notation and typographical errors. The methodology is sound and reproducible, but the lack of systematic formatting may pose challenges for readers seeking to implement the proposed methods. The novelty of the approach is clear, but it could be better highlighted through more structured presentation and clearer explanations of assumptions and definitions.\n\n# Summary Of The Review\nOverall, the paper provides a meaningful contribution to the understanding of SVGD with a reweighted kernel, offering theoretical insights and practical implications. However, it requires improvements in clarity and consistency to enhance comprehensibility and impact. The findings are valuable, but the presentation could benefit from careful proofreading and formatting adjustments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the local KL convergence rate of Stein Variational Gradient Descent (SVGD) using reweighted kernels, presenting a theoretical framework that emphasizes the advantages of these kernels over traditional approaches. The authors utilize log-Sobolev inequalities to derive convergence guarantees and discuss the implications of their findings for high-dimensional data. However, the experimental validation relies solely on synthetic datasets, limiting the practical applicability of their results.\n\n# Strength And Weaknesses\nThe introduction of reweighted kernels is a noteworthy contribution, enhancing the SVGD algorithm's performance. However, the focus on local KL convergence is a limitation, as the paper could have benefited from a broader analysis using other convergence metrics like Wasserstein distance. Furthermore, while the theoretical guarantees are well-articulated, the lack of varied distributional assumptions and the limited empirical validation weaken the overall robustness of the findings. The paper also fails to address alternative strategies for gradient vanishing in low-density areas and does not sufficiently explore the implications of poor initializations.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its theoretical contributions clearly. The novelty of the reweighted kernel approach stands out; however, the clarity could be improved with a more extensive discussion of potential applications and limitations. The methodology is reproducible to an extent, but the reliance on synthetic data and lack of comparisons with state-of-the-art algorithms hinder a comprehensive evaluation of its practical effectiveness.\n\n# Summary Of The Review\nOverall, the paper presents a significant theoretical advancement in the field of SVGD through the introduction of reweighted kernels, but it suffers from a narrow focus and limited empirical validation. Expanding the analysis to include various convergence metrics and real-world applications would strengthen the contributions considerably.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates the convergence properties of Stein Variational Gradient Descent (SVGD) for non-normalized distributions, with a specific focus on minimizing KL divergence rather than relying solely on Kernelized Stein Discrepancy (KSD). The authors introduce reweighted kernels to address gradient vanishing issues in low-density regions, which can impede error control. They establish a local linear convergence rate under the log-Sobolev inequality (LSI) and validate their theoretical claims through experiments that demonstrate improved performance of SVGD with reweighted kernels compared to traditional smoothing kernels.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its novel approach to addressing the gradient estimation issues in SVGD by introducing reweighted kernels, which significantly enhances the algorithm's performance in low-density areas. The theoretical framework provided is robust, establishing clear convergence rates under reasonable assumptions. However, the paper could benefit from a more extensive discussion on the limitations of the proposed approach and potential implications of the assumptions made. Additionally, while the empirical results support the theoretical claims, the experiments could be expanded to include more diverse datasets and real-world applications to strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow from theoretical groundwork to empirical validation. The methodology is presented in a way that is accessible to readers familiar with the concepts of SVGD and KL divergence. The novel aspect of reweighted kernels is clearly highlighted, underscoring its significance in the field. However, while the empirical validation is promising, details regarding the reproducibility of the experiments, such as hyperparameter settings and implementation specifics, could be more thoroughly documented to facilitate replication.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the understanding of convergence properties in SVGD by introducing reweighted kernels and demonstrating local linear convergence rates. The theoretical results are compelling and supported by empirical evidence, although further exploration of the limitations and reproducibility aspects would enhance the paper’s impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to Stein Variational Gradient Descent (SVGD) by introducing a reweighted kernel that aims to improve the convergence rates of the sampling process. The authors provide theoretical convergence guarantees under certain conditions and conduct experiments on a limited set of distributions to evaluate the proposed method. Despite the promising results, the paper acknowledges limitations in the generalizability of its findings due to the specific assumptions made regarding the target distribution.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to enhance SVGD through a reweighted kernel, which may offer advantages in certain scenarios. However, several weaknesses limit the paper’s impact: the lack of comprehensive comparisons with various smoothing kernels restricts the generalizability of the results, and the empirical validation focuses on a narrow range of distributions, raising concerns about robustness. Additionally, assumptions such as the necessity for the target distribution to satisfy the log-Sobolev inequality may hinder practical applications, while local convergence guarantees could restrict effectiveness in broader contexts. Computational inefficiencies in high-dimensional spaces and the absence of discussions on bandwidth choices or interactions with existing sampling techniques further detract from the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly, with a well-structured methodology and findings. However, the novelty of the approach is somewhat diminished by the limited empirical validation and the narrow scope of theoretical guarantees. Reproducibility may be affected due to the lack of exploration of various parameter settings and the computational limitations mentioned, which could hinder practical implementations.\n\n# Summary Of The Review\nOverall, while the paper introduces an interesting enhancement to SVGD through a reweighted kernel, its contributions are undermined by a lack of empirical validation across diverse distributions and limitations in theoretical applicability. Future work is necessary to address the identified gaps and to explore the performance of the proposed method under more general conditions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a study on the convergence properties of Stein Variational Gradient Descent (SVGD) when utilizing reweighted kernels. The authors argue that traditional kernel methods struggle with low-density areas, leading to issues like gradient vanishing. They propose a theoretical framework for analyzing the local KL convergence rate of SVGD with reweighted kernels, claiming that this approach mitigates some of the shortcomings associated with conventional methods. The findings include a series of experiments comparing the performance of reweighted versus standard smoothing kernels, illustrating the purported advantages of their methodology.\n\n# Strength And Weaknesses\nThe strengths of the paper include a clear attempt to address the well-known limitations of SVGD, particularly in low-density regions, and the introduction of reweighted kernels as a potential solution. However, the weaknesses are pronounced; the theoretical analysis does not provide substantial new insights, as it largely reiterates established findings in the field. The experimental section lacks depth and fails to deliver surprising results, making the contributions appear less significant. Additionally, the assumptions made in the local KL convergence analysis seem overly simplistic and hand-wavy.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and comprehensible, but the clarity is undermined by a reliance on familiar concepts without sufficient novelty. The quality of the theoretical work does not meet the expectations for a paper aiming to contribute new knowledge to the field. Reproducibility is not sufficiently addressed, as the experiments do not provide sufficient detail or novel methodologies to encourage independent verification.\n\n# Summary Of The Review\nOverall, the paper provides a rehash of existing ideas in the context of SVGD and presents only marginal improvements through the introduction of reweighted kernels. The theoretical contributions are weak, and the empirical findings do not offer compelling evidence for the claimed advantages. The work lacks the depth and novelty expected for a conference of this caliber.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents a significant advancement in Stein Variational Gradient Descent (SVGD) through the introduction of reweighted kernels. The authors rigorously analyze the theoretical convergence rates associated with this new approach and highlight its potential advantages over traditional smoothing kernels. The findings suggest that reweighted kernels can enhance sampling efficiency and flexibility, particularly in complex distributions, although empirical validation is primarily limited to synthetic data.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its theoretical contributions, particularly the analysis of convergence rates, and the innovative introduction of reweighted kernels, which opens up new avenues for research in sampling methods. However, the paper's reliance on synthetic data for empirical validation is a notable weakness, as it limits the demonstration of robustness in real-world applications. Additionally, while the theoretical framework is solid, the exploration of potential hybrid approaches with other sampling techniques remains underdeveloped.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas clearly, making it accessible to both practitioners and researchers. The quality of the theoretical analysis is high, and the novelty of introducing reweighted kernels is significant. However, the reproducibility of results may be hindered by the lack of extensive empirical experiments, particularly on diverse, real-world datasets. Future work should address these aspects to enhance the overall impact of the findings.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of sampling methods through the introduction of reweighted kernels in SVGD, supported by a solid theoretical foundation. However, its empirical validation is limited, and further research is needed to explore practical applications and hybrid methodologies.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel enhancement to the Stein Variational Gradient Descent (SVGD) algorithm by implementing reweighted kernels to mitigate the gradient vanishing issue encountered in low-density regions. The authors conduct comprehensive experiments across various synthetic datasets, demonstrating that the reweighted kernel SVGD achieves lower Kullback-Leibler (KL) divergence compared to traditional SVGD methods. The findings indicate a significant improvement in convergence rates, showcasing the ability of the proposed method to reach any ε-neighborhood of the target distribution with a linear convergence rate. Furthermore, numerical illustrations underscore the stability and efficiency of the new approach relative to standard SVGD and Langevin dynamics.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing a well-known limitation of the SVGD algorithm, thereby enhancing its applicability in practical scenarios. The benchmark results are compelling and demonstrate the method's superiority in terms of convergence rates and performance metrics across different initial distributions. However, one potential weakness is the focus solely on synthetic datasets, which may limit the generalizability of the findings to real-world applications. Additionally, the paper could benefit from a broader discussion on the implications of using reweighted kernels in various contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The quality of the presentation is high, with numerical illustrations and quantitative metrics effectively supporting the claims made. The novelty of introducing reweighted kernels to SVGD is significant within the context of variational inference. The reproducibility of the experiments is bolstered by the detailed descriptions of the methodologies used and the performance metrics reported.\n\n# Summary Of The Review\nOverall, the paper presents a meaningful advancement in the SVGD methodology through the introduction of reweighted kernels, demonstrating significant empirical improvements. While the focus on synthetic datasets may limit broader applicability, the results are robust and well-supported, indicating a strong contribution to the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to sampling distributions using Stein Variational Gradient Descent (SVGD) and Kernel Stein Discrepancy (KSD). It proposes a refined methodology that enhances the efficiency of existing algorithms while maintaining theoretical guarantees. The authors conduct extensive experiments to demonstrate the efficacy of their approach compared to state-of-the-art methods, reporting significant improvements in convergence rates and sample quality across various datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative refinement of SVGD, which addresses key limitations in previous works related to computational efficiency and theoretical robustness. The experimental results are compelling, showcasing the advantages of the proposed method over established techniques. However, weaknesses include a dense and somewhat disorganized presentation, particularly in the introduction and mathematical formulations, which may hinder reader comprehension. Additionally, the paper could benefit from clearer definitions and consistent notation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by several factors, including complex sentence structures and insufficient transitions between sections. While the quality of the research is high, the writing style could be more accessible to a broader audience. The novelty of the proposed methodology is significant, as it advances the state of the art in sampling techniques. However, reproducibility may be affected by the lack of detailed explanations regarding the implementation of algorithms and the dataset descriptions.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of sampling methods through its innovative approach and robust empirical results. However, the presentation requires improvements in clarity and organization to enhance accessibility and comprehension for readers.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.3472009543625796,
    -1.5642462281018077,
    -1.6935638287064791,
    -1.4709716218111333,
    -1.5269905620660125,
    -1.6916264321385968,
    -1.6119096335449739,
    -1.7040880040191468,
    -1.459625327359065,
    -1.688773342033928,
    -1.7305300405634185,
    -1.3727121885783433,
    -1.6060207187277291,
    -1.6053245420850175,
    -1.6441255801533499,
    -1.6137202791323397,
    -1.7760505515852794,
    -1.6285767658191233,
    -1.5663245415720164,
    -1.7368275008287852,
    -1.8660221136978044,
    -1.642726268031622,
    -1.8433063655907416,
    -1.6842268432415708,
    -1.789604662524744,
    -1.8758080591960977,
    -1.531919128054763,
    -1.6075179966340982,
    -1.6176193377440844
  ],
  "logp_cond": [
    [
      0.0,
      -2.2627031364180445,
      -2.254602577024545,
      -2.2441352413943276,
      -2.2635569915693217,
      -2.2500212562523845,
      -2.2749733058623995,
      -2.245990121372179,
      -2.2530788995643394,
      -2.248781601150923,
      -2.2392758011172647,
      -2.291080734247629,
      -2.2379179665356186,
      -2.2577715827102462,
      -2.2368858178117303,
      -2.2418751098422867,
      -2.2650383702073036,
      -2.2512614220658937,
      -2.248768993134991,
      -2.2484177127203946,
      -2.241923185746935,
      -2.249296458690135,
      -2.255446701015403,
      -2.2372878997421575,
      -2.266436847664162,
      -2.261125923018551,
      -2.2722017016440645,
      -2.2616037584957462,
      -2.2848821158146357
    ],
    [
      -1.2157030346010655,
      0.0,
      -1.0728362094957529,
      -1.032925650747225,
      -1.1348723990785168,
      -1.1181470612018773,
      -1.2433814686177231,
      -1.0808261437276752,
      -1.0953877301953885,
      -1.1475051353696266,
      -1.0621651949576563,
      -1.3365536279678343,
      -1.0779782063936365,
      -1.1234261501169136,
      -1.099366844271303,
      -1.1253586562627853,
      -1.1479724363701562,
      -1.0709432759107091,
      -1.123834065628543,
      -1.0891799003755562,
      -1.147030258587468,
      -1.1612896599058697,
      -1.0683054408605475,
      -1.0216539724928189,
      -1.2211208594257632,
      -1.154149885621813,
      -1.2308026135169554,
      -1.1565483609913099,
      -1.2920095627205608
    ],
    [
      -1.3099055024769524,
      -1.1873752473916652,
      0.0,
      -1.2158010419774787,
      -1.2082141776841575,
      -1.148051126971781,
      -1.3466519398893295,
      -1.180171042971858,
      -1.2375171540130239,
      -1.263460400237269,
      -1.173654699128918,
      -1.4348293092379787,
      -1.216901588752303,
      -1.2017929955046491,
      -1.2164752331044228,
      -1.1637088781733398,
      -1.2640884001677453,
      -1.2638367257864005,
      -1.1461571139969475,
      -1.1764180927027466,
      -1.2689743989437496,
      -1.31636974805358,
      -1.2165654218853161,
      -1.1478894076078776,
      -1.3074898731803308,
      -1.2256167241072127,
      -1.3058479788514583,
      -1.2635709722900248,
      -1.4002522585108501
    ],
    [
      -1.153018877049453,
      -1.0426216836917208,
      -1.08869673412175,
      0.0,
      -1.1292033899995881,
      -1.0828004520324959,
      -1.2103268919552321,
      -1.0521550144041216,
      -1.0413790368969085,
      -1.142935403397369,
      -1.0300164765247646,
      -1.2522568127017653,
      -1.0394055152838333,
      -1.0760218299199327,
      -1.0453000444106597,
      -1.0736144460553938,
      -1.094388453495193,
      -1.0749494424499064,
      -1.0270950458176533,
      -1.0544094661326808,
      -1.0651621718096584,
      -1.1405709246114255,
      -1.0745731271031345,
      -1.0180741873712504,
      -1.1874620056710028,
      -1.1049213228407506,
      -1.2223979080296425,
      -1.1252674367806674,
      -1.220582728081847
    ],
    [
      -1.2188940997253215,
      -1.1644187476187378,
      -1.159963072231603,
      -1.1919605792030745,
      0.0,
      -1.1627083823798214,
      -1.224148639631058,
      -1.1760874939057664,
      -1.1997307250942237,
      -1.224003765622964,
      -1.1817885528829546,
      -1.2996087944509647,
      -1.2370303721541707,
      -1.1593961306944855,
      -1.2040849517290078,
      -1.1712600897999992,
      -1.1888817876582265,
      -1.1878624094538082,
      -1.1349889489796778,
      -1.1860520168742106,
      -1.1943717411648767,
      -1.241049532643777,
      -1.1918651934142874,
      -1.147296591330264,
      -1.1758730596090565,
      -1.1397033138094788,
      -1.2024270281565816,
      -1.1830374846255076,
      -1.2773012521239675
    ],
    [
      -1.3677491358535026,
      -1.279582369468657,
      -1.2028817742604654,
      -1.2742111659303204,
      -1.2894694145316077,
      0.0,
      -1.3588902152290143,
      -1.234909916312653,
      -1.291027453064116,
      -1.3340780061886,
      -1.253472427754001,
      -1.4410587190102062,
      -1.2667949934746074,
      -1.2719870070929502,
      -1.2772628365283978,
      -1.2022439552231434,
      -1.3189141000168842,
      -1.2946148846591727,
      -1.2623683512069914,
      -1.261623604879014,
      -1.2871003027666752,
      -1.3381157975490152,
      -1.2795084613256167,
      -1.2633090654524761,
      -1.3223941821934748,
      -1.3008739071972202,
      -1.3496622882491782,
      -1.3112035729126135,
      -1.4183851680229136
    ],
    [
      -1.4062483053088304,
      -1.3138397481940052,
      -1.3025321638732228,
      -1.3469336473543414,
      -1.3082434452884293,
      -1.284014642211636,
      0.0,
      -1.3323087970526506,
      -1.3006227528123704,
      -1.3801868890310465,
      -1.2965645165781614,
      -1.3758771600337285,
      -1.3216253704975949,
      -1.331366308330061,
      -1.330533864947674,
      -1.3200483984505957,
      -1.3145779634511428,
      -1.315569578190196,
      -1.339172472056902,
      -1.295821373246359,
      -1.3594920527756522,
      -1.3310315952079317,
      -1.3043401780463715,
      -1.2817737313864048,
      -1.3460951533608165,
      -1.3447525363335275,
      -1.3405725044448622,
      -1.3274511777744178,
      -1.4012592975568507
    ],
    [
      -1.3934648594661407,
      -1.2538260676210014,
      -1.3028530805043717,
      -1.2856401176480718,
      -1.329945511911687,
      -1.3043856818050639,
      -1.4323256399931357,
      0.0,
      -1.2957370852138121,
      -1.3545247262487814,
      -1.2348725235573708,
      -1.4639234185212018,
      -1.2290069846506295,
      -1.2938969620702743,
      -1.3499474691067095,
      -1.2848297286596162,
      -1.3663196376110414,
      -1.265629921711713,
      -1.2628013474007935,
      -1.2825278276545027,
      -1.3313254095533342,
      -1.3861233172226584,
      -1.3137363393997044,
      -1.2349402101079918,
      -1.3886588718727622,
      -1.3412178570215914,
      -1.3778856628989824,
      -1.3810506300779253,
      -1.4150570435483905
    ],
    [
      -1.1103958625550896,
      -0.9917012746459192,
      -1.0526296594098643,
      -1.0006911857621326,
      -1.1000793095908181,
      -1.0595021779773135,
      -1.1828931353453058,
      -1.0263421195639344,
      0.0,
      -1.0907077365914355,
      -0.9943441979013323,
      -1.2525368759479645,
      -1.042924875522915,
      -1.060252468363352,
      -1.0409216828989714,
      -1.0290287215827563,
      -1.0559768522961497,
      -1.0204984310636902,
      -0.9822670535192196,
      -0.9651304801443442,
      -1.1039602114639155,
      -1.1076951045765828,
      -1.0531187619276974,
      -1.0328828064530984,
      -1.153814412254601,
      -1.102144192104308,
      -1.156259540111726,
      -1.0591711327795184,
      -1.1680616283557268
    ],
    [
      -1.3353150448425082,
      -1.309603359641153,
      -1.3141390913832445,
      -1.338104011608792,
      -1.3164782960159929,
      -1.3474194688741628,
      -1.406690376027879,
      -1.2775722709294608,
      -1.3488475079741367,
      0.0,
      -1.327257771970182,
      -1.4166094233105406,
      -1.33667133296648,
      -1.3431959390982757,
      -1.3321931494006582,
      -1.3627450740048248,
      -1.3466319812941485,
      -1.3411044526775466,
      -1.3564416904116865,
      -1.3434849005300011,
      -1.3763940731674285,
      -1.3757723187787168,
      -1.3373418398551387,
      -1.3317125797864342,
      -1.2575193561282696,
      -1.3618207000225013,
      -1.3734986195997494,
      -1.3541178456021976,
      -1.3719950952296858
    ],
    [
      -1.3549567576005168,
      -1.2911994336858579,
      -1.2878772846736508,
      -1.2362288452143466,
      -1.365227510874816,
      -1.305704362241002,
      -1.459055880532685,
      -1.2822897815934913,
      -1.3026805684870646,
      -1.4000583380541178,
      0.0,
      -1.5250231240966365,
      -1.2473382218527158,
      -1.3029914825618043,
      -1.361576456437548,
      -1.1769630849050492,
      -1.3274770124970712,
      -1.328424200981801,
      -1.2191411532042202,
      -1.2579269215471423,
      -1.326127964113082,
      -1.3819774316251123,
      -1.3167692022398092,
      -1.232681618552467,
      -1.3856952334674384,
      -1.3734524260359227,
      -1.4394840851335666,
      -1.3895739226164792,
      -1.490892718489588
    ],
    [
      -1.208078316808938,
      -1.1766532571686406,
      -1.1800857758826886,
      -1.1907722583660385,
      -1.1555574382955873,
      -1.1655801732217748,
      -1.1219668928686193,
      -1.1385742418662816,
      -1.1701036724289597,
      -1.1633994781077042,
      -1.1546269586127555,
      0.0,
      -1.152810721192175,
      -1.184317486958338,
      -1.170262303243093,
      -1.186561327513416,
      -1.1694024984523002,
      -1.1681365845595328,
      -1.1769945561242547,
      -1.1631576794681224,
      -1.1717425464118667,
      -1.1729390176302221,
      -1.1747095639807896,
      -1.1466078048268316,
      -1.1855190185923001,
      -1.1587571322000427,
      -1.1884953046803775,
      -1.1652883527840976,
      -1.1366907666363706
    ],
    [
      -1.2589045358377655,
      -1.2351104964703696,
      -1.2588411956795564,
      -1.1365369877697262,
      -1.3079838453145292,
      -1.2423147441494145,
      -1.338995412892417,
      -1.1314721320225916,
      -1.2226040340751627,
      -1.251170445474917,
      -1.1112968219692225,
      -1.3653695665860603,
      0.0,
      -1.2642356948462685,
      -1.2663490046913453,
      -1.1537520183493817,
      -1.265602572223096,
      -1.2739676230677712,
      -1.1287071380869165,
      -1.2170955466745659,
      -1.2150912347979088,
      -1.2459228880751534,
      -1.2360523384190423,
      -1.2044287133379827,
      -1.3283844841393933,
      -1.2669895746884414,
      -1.3566152214412524,
      -1.2886783093759933,
      -1.3408280346127612
    ],
    [
      -1.1816026215283928,
      -1.109134116901225,
      -0.9780989713825003,
      -1.0599458895009455,
      -1.079164484067382,
      -1.0481203127683545,
      -1.2033751690522185,
      -1.0555640342835484,
      -1.089780116883828,
      -1.1759771199894118,
      -1.027237107197987,
      -1.3481455514308476,
      -1.0715153832463136,
      0.0,
      -1.0639995155849646,
      -1.0505620937537898,
      -1.1194657494807077,
      -1.045121264526433,
      -1.08955389264896,
      -1.0487696449534385,
      -1.075278008232653,
      -1.1562648531622244,
      -1.142880494090246,
      -0.9623414509135142,
      -1.203871695129513,
      -1.0821798532499296,
      -1.1940816108693462,
      -1.139963873172994,
      -1.312433798999717
    ],
    [
      -1.296064342632221,
      -1.249502356894451,
      -1.2600387204946655,
      -1.264766056830608,
      -1.3075685574319813,
      -1.2507873989144638,
      -1.363606080172483,
      -1.2323051714050837,
      -1.2844885010028126,
      -1.3211520734841997,
      -1.2251930213596365,
      -1.4017122544406442,
      -1.2495035817374343,
      -1.2795516487269527,
      0.0,
      -1.2634268883580195,
      -1.282538403757693,
      -1.2578086607888583,
      -1.2864689605099922,
      -1.226900841103429,
      -1.2425458000019514,
      -1.24185484728649,
      -1.261359564158315,
      -1.2463503694093638,
      -1.3615111224285794,
      -1.2517327675861456,
      -1.3600116066150278,
      -1.2661994089861883,
      -1.3383808247876878
    ],
    [
      -1.2066915314499937,
      -1.1893718932313067,
      -1.1069157026616916,
      -1.0772703619141775,
      -1.2310508198961918,
      -1.119624994402615,
      -1.2783652061119182,
      -1.0855786171313317,
      -1.175467896835102,
      -1.235482004027132,
      -1.035801508253929,
      -1.3827553882467045,
      -1.0662163526767503,
      -1.1849154434429343,
      -1.1834279304939255,
      0.0,
      -1.1647521391662174,
      -1.2143961701547101,
      -1.026417297973941,
      -1.1094945097507645,
      -1.1840035355913734,
      -1.2288562475034277,
      -1.1747180200457452,
      -1.1559292362013787,
      -1.2416491390788071,
      -1.1831455991768334,
      -1.2715162712616657,
      -1.2618883587415666,
      -1.3466311496721124
    ],
    [
      -1.360049229263351,
      -1.308082314309117,
      -1.3127562783417486,
      -1.2854984333855195,
      -1.318642955564233,
      -1.3336032511906382,
      -1.444184317452896,
      -1.356571177305674,
      -1.2998696802938878,
      -1.4084944158013533,
      -1.3534196804185217,
      -1.537158646548937,
      -1.3903989847957159,
      -1.3406910018309803,
      -1.365539781752937,
      -1.3556114511286006,
      0.0,
      -1.3513412996368597,
      -1.3245582750780516,
      -1.3312678873003494,
      -1.4102441124187919,
      -1.356758616659225,
      -1.3604666781174586,
      -1.321169100732533,
      -1.3953298630883952,
      -1.3491696496300911,
      -1.4472812466482627,
      -1.3525947341903157,
      -1.4631700875190372
    ],
    [
      -1.2242632293466953,
      -1.031995996973339,
      -1.0559410568344934,
      -1.0767290335792281,
      -1.1287782605850367,
      -1.0929505344452441,
      -1.2461907272236434,
      -1.046425450722055,
      -1.062814317649985,
      -1.1616425467516855,
      -1.0450258951825635,
      -1.3659465827546857,
      -1.0979278562953463,
      -1.0651427998206602,
      -1.1094665632004428,
      -1.1106349615077886,
      -1.181169831802622,
      0.0,
      -1.064945070784324,
      -1.0384957232918839,
      -1.200241664791684,
      -1.2451364744760296,
      -1.1343073364873462,
      -1.0616741987376481,
      -1.198128168510923,
      -1.1619929951596866,
      -1.2022644226513304,
      -1.1521411372404133,
      -1.3140174014807922
    ],
    [
      -1.2411871920455861,
      -1.1600943148746026,
      -1.1114371425734482,
      -1.0852357306617426,
      -1.1579208616126297,
      -1.1379326426086926,
      -1.2164431177344155,
      -1.1438220363768323,
      -1.1193080149357948,
      -1.1767816435068987,
      -1.0619118771352143,
      -1.296416869848266,
      -1.0974179018624657,
      -1.1777382738750233,
      -1.184107912297305,
      -1.0394821435577883,
      -1.1620593304636242,
      -1.1524057308312576,
      0.0,
      -1.1226310645298014,
      -1.174037915172694,
      -1.224984336259916,
      -1.151386465828056,
      -1.1237805075618452,
      -1.2151025920786844,
      -1.2165337426208913,
      -1.2051696967300887,
      -1.1909786151119657,
      -1.2239510944926637
    ],
    [
      -1.29609726267232,
      -1.1821815832291735,
      -1.1916180385401394,
      -1.200392916685545,
      -1.2464912264492893,
      -1.2276896276437128,
      -1.3730369431250937,
      -1.2053608665072633,
      -1.1423088572776083,
      -1.3288277269554072,
      -1.1345987171720602,
      -1.4643026039604197,
      -1.202539084175305,
      -1.2556042339442786,
      -1.1900827828770117,
      -1.1737642094788343,
      -1.2678774355163414,
      -1.2120536291885848,
      -1.1852599958245207,
      0.0,
      -1.3142366683407676,
      -1.2854673115401565,
      -1.270471281197698,
      -1.218438425029919,
      -1.3034301319794424,
      -1.2990178460874662,
      -1.345210890899534,
      -1.2673133685231956,
      -1.4225327911310366
    ],
    [
      -1.4740056286405518,
      -1.4295752296246294,
      -1.4410393117103881,
      -1.4245220772837355,
      -1.4277822963515552,
      -1.4472032651965456,
      -1.5138226067169278,
      -1.4731292201683779,
      -1.4956417436278582,
      -1.5356797426661475,
      -1.4131992943882403,
      -1.6158584648251069,
      -1.5077059925442775,
      -1.419599712513419,
      -1.4378457315028443,
      -1.4651016756443207,
      -1.4620273099956183,
      -1.4822176281124895,
      -1.437199072024453,
      -1.4827408223959841,
      0.0,
      -1.4326025271851996,
      -1.3806618746059964,
      -1.424964641339239,
      -1.4807604847116078,
      -1.3084763268140993,
      -1.4932436642392244,
      -1.4877739315880742,
      -1.585264052414299
    ],
    [
      -1.2855989707299298,
      -1.2586608233144145,
      -1.3060531228463608,
      -1.2249980674438206,
      -1.298182854346549,
      -1.3030683831193635,
      -1.3196174407636936,
      -1.293232120915782,
      -1.285495476580419,
      -1.2631737009391797,
      -1.2776149016373686,
      -1.371601395755664,
      -1.2868404168591157,
      -1.3103174710828975,
      -1.2054265874742358,
      -1.294482227812794,
      -1.253901762565847,
      -1.2888713061270183,
      -1.277309237306831,
      -1.291171723330019,
      -1.2161908269608652,
      0.0,
      -1.211466543881546,
      -1.287005342986034,
      -1.2684384695459638,
      -1.2495180383809041,
      -1.3200889350374785,
      -1.25731340867359,
      -1.335521576190895
    ],
    [
      -1.5028878629938935,
      -1.3828154920688467,
      -1.4048822322129804,
      -1.374927967540966,
      -1.4199011358063744,
      -1.4108925531250482,
      -1.515998341936293,
      -1.4214259191718595,
      -1.4208534878082963,
      -1.4680279351604668,
      -1.3793220237507695,
      -1.576934802230511,
      -1.4264605487589372,
      -1.428145339534411,
      -1.4313220942771558,
      -1.3813001978726394,
      -1.4244969625128185,
      -1.4051262378475817,
      -1.4185170319116815,
      -1.423810077943103,
      -1.415598652632896,
      -1.4557200667597303,
      0.0,
      -1.388574451283974,
      -1.4160672516135173,
      -1.3996267465050785,
      -1.4892493136450684,
      -1.4181818604986203,
      -1.567952694161729
    ],
    [
      -1.3424004076073008,
      -1.1623414109586319,
      -1.1671492369069376,
      -1.1593737850724903,
      -1.2532321922093612,
      -1.1875762676445245,
      -1.3855432932799858,
      -1.231016320173587,
      -1.2362165311220261,
      -1.3310148589536257,
      -1.1210221779695506,
      -1.458284090269561,
      -1.2147252572563756,
      -1.1842406766953661,
      -1.2492141544660802,
      -1.2036480936889158,
      -1.2944732501955951,
      -1.1955838688964153,
      -1.1901238966204741,
      -1.223836269901635,
      -1.2568356810038444,
      -1.304841823689979,
      -1.2500067977118248,
      0.0,
      -1.3385348847563159,
      -1.2761591523130125,
      -1.3431480244314058,
      -1.334387384607269,
      -1.4383233802480901
    ],
    [
      -1.495494156074685,
      -1.443595887019334,
      -1.420464180421078,
      -1.4477912508174362,
      -1.3875274563264592,
      -1.4715429133401214,
      -1.473433158883694,
      -1.410551669888552,
      -1.4755589143427734,
      -1.3409357307065695,
      -1.4363171107048796,
      -1.5565706869681448,
      -1.4593702142404266,
      -1.4544823086175793,
      -1.4779690116371023,
      -1.4362072512992832,
      -1.4134580409427615,
      -1.432897708871913,
      -1.4088012120733961,
      -1.4429189346618727,
      -1.43709316870555,
      -1.4251435861131962,
      -1.4362611600711097,
      -1.4619756066374674,
      0.0,
      -1.4544596340669462,
      -1.415206641782132,
      -1.4488694783280947,
      -1.5068757946993268
    ],
    [
      -1.5435405734421022,
      -1.4544829176801632,
      -1.4576320499097326,
      -1.4392869686932501,
      -1.4471880833620583,
      -1.4198030550774412,
      -1.547709085396756,
      -1.437618600900772,
      -1.4995835167161342,
      -1.5298356202182304,
      -1.41988284048904,
      -1.63154336883589,
      -1.4794530087004487,
      -1.4007740722409798,
      -1.4299235484691386,
      -1.4359951445968921,
      -1.458046529268452,
      -1.475915619574221,
      -1.4465659369050865,
      -1.4765339168610836,
      -1.3884862952413743,
      -1.4760721238713919,
      -1.3959567125251187,
      -1.4228737969862923,
      -1.554077325928845,
      0.0,
      -1.5072193901014923,
      -1.501551875767453,
      -1.5873880386547805
    ],
    [
      -1.2079660751926116,
      -1.1667590589975199,
      -1.104291937759818,
      -1.2016373810821355,
      -1.144912430402755,
      -1.1181036154717294,
      -1.1946643143164701,
      -1.133760478182549,
      -1.2083827451569424,
      -1.1182489003282474,
      -1.1267839777514377,
      -1.2545019140680838,
      -1.1962960822252822,
      -1.1744961691247828,
      -1.1595542416773847,
      -1.1563679982912605,
      -1.1472972865041018,
      -1.1886540025010552,
      -1.145571001216783,
      -1.189107613772663,
      -1.140652962228951,
      -1.16837979253166,
      -1.0651506218933016,
      -1.1897536214209374,
      -1.093151696521231,
      -1.1433319610197075,
      0.0,
      -1.1797675657341653,
      -1.2190642439010675
    ],
    [
      -1.2697356262058161,
      -1.1905708950450402,
      -1.1817546218673156,
      -1.2058587506227698,
      -1.1197811242138531,
      -1.1741327661140735,
      -1.2595899149272454,
      -1.1929364041762265,
      -1.166312784512689,
      -1.2522936762925256,
      -1.2053440448290407,
      -1.3387112540956083,
      -1.2253318952238534,
      -1.173773641164987,
      -1.1807872511137143,
      -1.1993732058539726,
      -1.1547487241450893,
      -1.192826546034155,
      -1.2042459663110334,
      -1.2258476842094665,
      -1.2262510148716037,
      -1.2371253909910114,
      -1.214792166060791,
      -1.195282364099238,
      -1.2270596334424093,
      -1.194351199986762,
      -1.2480125572692402,
      0.0,
      -1.2542866467537952
    ],
    [
      -1.258297763524464,
      -1.259641433311148,
      -1.2628745694703623,
      -1.3013712081812598,
      -1.2686458718045786,
      -1.251054500221892,
      -1.294160152458538,
      -1.2123838061034282,
      -1.269276490576537,
      -1.2001891042929347,
      -1.2519079819715404,
      -1.2382525073666377,
      -1.2317194578265518,
      -1.2961044449926906,
      -1.268556032483997,
      -1.2534604482525327,
      -1.2520748462286408,
      -1.295388896639982,
      -1.264887349413125,
      -1.2572813217525014,
      -1.2758642495294454,
      -1.234121419613326,
      -1.264267110554237,
      -1.2748329801725304,
      -1.2113969920412806,
      -1.2576724324327495,
      -1.2575023383100967,
      -1.2435078525641554,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.08449781794453504,
      0.09259837733803478,
      0.10306571296825195,
      0.08364396279325792,
      0.09717969811019511,
      0.07222764850018004,
      0.10121083299040068,
      0.0941220547982402,
      0.09841935321165662,
      0.10792515324531493,
      0.05612022011495066,
      0.109282987826961,
      0.08942937165233333,
      0.11031513655084924,
      0.10532584452029292,
      0.082162584155276,
      0.09593953229668584,
      0.0984319612275888,
      0.098783241642185,
      0.10527776861564453,
      0.09790449567244464,
      0.09175425334717646,
      0.10991305462042211,
      0.08076410669841749,
      0.08607503134402839,
      0.07499925271851504,
      0.08559719586683334,
      0.062318838547943844
    ],
    [
      0.34854319350074214,
      0.0,
      0.4914100186060548,
      0.5313205773545826,
      0.4293738290232909,
      0.44609916689993034,
      0.32086475948408455,
      0.48342008437413253,
      0.4688584979064192,
      0.4167410927321811,
      0.5020810331441514,
      0.2276926001339734,
      0.4862680217081712,
      0.4408200779848941,
      0.46487938383050476,
      0.4388875718390224,
      0.41627379173165147,
      0.49330295219109854,
      0.44041216247326465,
      0.47506632772625146,
      0.4172159695143396,
      0.40295656819593795,
      0.49594078724126023,
      0.5425922556089888,
      0.3431253686760445,
      0.4100963424799946,
      0.3334436145848523,
      0.4076978671104978,
      0.2722366653812469
    ],
    [
      0.3836583262295268,
      0.5061885813148139,
      0.0,
      0.4777627867290004,
      0.48534965102232164,
      0.5455127017346981,
      0.3469118888171496,
      0.5133927857346212,
      0.4560466746934553,
      0.43010342846921024,
      0.5199091295775611,
      0.25873451946850046,
      0.4766622399541762,
      0.49177083320183,
      0.4770885956020563,
      0.5298549505331394,
      0.4294754285387339,
      0.42972710292007865,
      0.5474067147095316,
      0.5171457360037326,
      0.4245894297627295,
      0.37719408065289906,
      0.476998406821163,
      0.5456744210986015,
      0.3860739555261483,
      0.4679471045992665,
      0.38771584985502083,
      0.42999285641645435,
      0.29331157019562903
    ],
    [
      0.31795274476168034,
      0.42834993811941247,
      0.38227488768938334,
      0.0,
      0.34176823181154514,
      0.3881711697786374,
      0.26064472985590115,
      0.41881660740701165,
      0.4295925849142248,
      0.3280362184137642,
      0.4409551452863687,
      0.21871480910936802,
      0.4315661065273,
      0.3949497918912006,
      0.42567157740047357,
      0.3973571757557395,
      0.3765831683159402,
      0.39602217936122686,
      0.44387657599347996,
      0.41656215567845245,
      0.40580945000147484,
      0.33040069719970777,
      0.3963984947079988,
      0.45289743443988284,
      0.2835096161401305,
      0.36605029897038266,
      0.24857371378149073,
      0.34570418503046585,
      0.2503888937292862
    ],
    [
      0.308096462340691,
      0.3625718144472747,
      0.36702748983440947,
      0.33502998286293795,
      0.0,
      0.36428217968619103,
      0.30284192243495456,
      0.3509030681602461,
      0.32725983697178873,
      0.3029867964430484,
      0.3452020091830579,
      0.2273817676150478,
      0.2899601899118418,
      0.3675944313715269,
      0.3229056103370047,
      0.3557304722660133,
      0.338108774407786,
      0.3391281526122043,
      0.39200161308633463,
      0.3409385451918019,
      0.3326188209011358,
      0.28594102942223554,
      0.33512536865172504,
      0.3796939707357485,
      0.35111750245695594,
      0.3872872482565337,
      0.3245635339094308,
      0.34395307744050485,
      0.24968930994204497
    ],
    [
      0.3238772962850942,
      0.41204406266993976,
      0.48874465787813137,
      0.4174152662082764,
      0.4021570176069891,
      0.0,
      0.33273621690958244,
      0.45671651582594386,
      0.40059897907448083,
      0.3575484259499968,
      0.43815400438459573,
      0.25056771312839055,
      0.4248314386639893,
      0.41963942504564655,
      0.414363595610199,
      0.48938247691545333,
      0.3727123321217125,
      0.397011547479424,
      0.42925808093160533,
      0.43000282725958283,
      0.40452612937192156,
      0.35351063458958154,
      0.4121179708129801,
      0.42831736668612064,
      0.36923224994512194,
      0.3907525249413766,
      0.34196414388941854,
      0.3804228592259833,
      0.27324126411568317
    ],
    [
      0.20566132823614347,
      0.2980698853509687,
      0.30937746967175106,
      0.26497598619063245,
      0.30366618825654457,
      0.32789499133333777,
      0.0,
      0.27960083649232326,
      0.31128688073260347,
      0.23172274451392738,
      0.31534511696681244,
      0.23603247351124534,
      0.290284263047379,
      0.28054332521491276,
      0.28137576859729996,
      0.2918612350943781,
      0.2973316700938311,
      0.2963400553547779,
      0.2727371614880718,
      0.3160882602986148,
      0.25241758076932164,
      0.28087803833704217,
      0.3075694554986024,
      0.33013590215856903,
      0.2658144801841573,
      0.2671570972114463,
      0.27133712910011165,
      0.28445845577055606,
      0.2106503359881231
    ],
    [
      0.3106231445530061,
      0.4502619363981455,
      0.4012349235147752,
      0.418447886371075,
      0.37414249210745987,
      0.399702322214083,
      0.27176236402601117,
      0.0,
      0.4083509188053347,
      0.34956327777036544,
      0.4692154804617761,
      0.24016458549794506,
      0.4750810193685173,
      0.41019104194887257,
      0.35414053491243735,
      0.41925827535953064,
      0.3377683664081055,
      0.43845808230743377,
      0.4412866566183533,
      0.42156017636464416,
      0.3727625944658126,
      0.3179646867964885,
      0.3903516646194425,
      0.469147793911155,
      0.3154291321463847,
      0.3628701469975555,
      0.32620234112016444,
      0.32303737394122156,
      0.2890309604707564
    ],
    [
      0.3492294648039753,
      0.46792405271314574,
      0.4069956679492006,
      0.45893414159693235,
      0.3595460177682468,
      0.4001231493817514,
      0.2767321920137591,
      0.4332832077951305,
      0.0,
      0.3689175907676294,
      0.4652811294577326,
      0.20708845141110044,
      0.4167004518361499,
      0.39937285899571284,
      0.41870364446009356,
      0.43059660577630865,
      0.40364847506291524,
      0.4391268962953747,
      0.4773582738398453,
      0.4944948472147207,
      0.3556651158951494,
      0.3519302227824821,
      0.40650656543136754,
      0.42674252090596654,
      0.3058109151044639,
      0.3574811352547569,
      0.303365787247339,
      0.40045419457954656,
      0.2915636990033381
    ],
    [
      0.35345829719141975,
      0.379169982392775,
      0.37463425065068345,
      0.3506693304251358,
      0.37229504601793506,
      0.3413538731597652,
      0.2820829660060489,
      0.41120107110446713,
      0.33992583405979127,
      0.0,
      0.361515570063746,
      0.2721639187233873,
      0.3521020090674478,
      0.3455774029356522,
      0.3565801926332697,
      0.3260282680291031,
      0.3421413607397794,
      0.3476688893563813,
      0.33233165162224143,
      0.3452884415039268,
      0.31237926886649947,
      0.3130010232552112,
      0.35143150217878927,
      0.3570607622474937,
      0.43125398590565833,
      0.3269526420114266,
      0.31527472243417853,
      0.3346554964317303,
      0.31677824680424216
    ],
    [
      0.3755732829629017,
      0.43933060687756065,
      0.4426527558897677,
      0.4943011953490719,
      0.36530252968860255,
      0.42482567832241647,
      0.27147416003073355,
      0.44824025896992725,
      0.42784947207635393,
      0.3304717025093007,
      0.0,
      0.20550691646678199,
      0.4831918187107027,
      0.4275385580016142,
      0.3689535841258704,
      0.5535669556583693,
      0.40305302806634735,
      0.4021058395816175,
      0.5113888873591983,
      0.4726031190162763,
      0.40440207645033643,
      0.3485526089383062,
      0.4137608383236093,
      0.4978484220109516,
      0.34483480709598013,
      0.35707761452749587,
      0.2910459554298519,
      0.34095611794693936,
      0.2396373220738306
    ],
    [
      0.16463387176940536,
      0.19605893140970276,
      0.19262641269565473,
      0.1819399302123048,
      0.217154750282756,
      0.20713201535656856,
      0.25074529570972404,
      0.23413794671206167,
      0.20260851614938358,
      0.20931271047063915,
      0.21808522996558777,
      0.0,
      0.21990146738616834,
      0.18839470162000538,
      0.20244988533525032,
      0.1861508610649274,
      0.20330969012604316,
      0.20457560401881048,
      0.19571763245408857,
      0.2095545091102209,
      0.20096964216647661,
      0.1997731709481212,
      0.1980026245975537,
      0.22610438375151176,
      0.18719316998604318,
      0.2139550563783006,
      0.18421688389796587,
      0.20742383579424573,
      0.23602142194197273
    ],
    [
      0.3471161828899636,
      0.3709102222573595,
      0.3471795230481727,
      0.4694837309580029,
      0.29803687341319995,
      0.36370597457831466,
      0.26702530583531203,
      0.47454858670513755,
      0.3834166846525664,
      0.35485027325281204,
      0.4947238967585066,
      0.24065115214166877,
      0.0,
      0.3417850238814606,
      0.33967171403638385,
      0.45226870037834743,
      0.3404181465046332,
      0.3320530956599579,
      0.47731358064081264,
      0.38892517205316324,
      0.3909294839298203,
      0.3600978306525757,
      0.36996838030868684,
      0.4015920053897464,
      0.27763623458833586,
      0.3390311440392877,
      0.24940549728647676,
      0.31734240935173585,
      0.2651926841149679
    ],
    [
      0.4237219205566247,
      0.4961904251837925,
      0.6272255707025172,
      0.545378652584072,
      0.5261600580176355,
      0.557204229316663,
      0.401949373032799,
      0.5497605078014691,
      0.5155444252011896,
      0.4293474220956057,
      0.5780874348870304,
      0.25717899065416994,
      0.533809158838704,
      0.0,
      0.5413250265000529,
      0.5547624483312277,
      0.48585879260430986,
      0.5602032775585846,
      0.5157706494360574,
      0.556554897131579,
      0.5300465338523646,
      0.44905968892279313,
      0.46244404799477157,
      0.6429830911715033,
      0.40145284695550454,
      0.5231446888350879,
      0.41124293121567135,
      0.46536066891202355,
      0.2928907430853005
    ],
    [
      0.34806123752112894,
      0.39462322325889887,
      0.3840868596586844,
      0.3793595233227418,
      0.33655702272136856,
      0.3933381812388861,
      0.2805194999808669,
      0.4118204087482662,
      0.3596370791505372,
      0.32297350666915015,
      0.4189325587937134,
      0.24241332571270569,
      0.3946219984159156,
      0.36457393142639716,
      0.0,
      0.38069869179533034,
      0.36158717639565685,
      0.38631691936449153,
      0.3576566196433577,
      0.41722473904992086,
      0.4015797801513985,
      0.40227073286685977,
      0.3827660159950348,
      0.39777521074398603,
      0.28261445772477045,
      0.39239281256720426,
      0.284113973538322,
      0.37792617116716154,
      0.30574475536566204
    ],
    [
      0.407028747682346,
      0.4243483859010331,
      0.5068045764706481,
      0.5364499172181623,
      0.3826694592361479,
      0.49409528472972464,
      0.33535507302042156,
      0.528141662001008,
      0.43825238229723773,
      0.3782382751052078,
      0.5779187708784108,
      0.2309648908856352,
      0.5475039264555894,
      0.4288048356894054,
      0.43029234863841426,
      0.0,
      0.4489681399661223,
      0.3993241089776296,
      0.5873029811583987,
      0.5042257693815753,
      0.42971674354096634,
      0.38486403162891203,
      0.4390022590865945,
      0.45779104293096107,
      0.3720711400535326,
      0.43057467995550636,
      0.34220400787067407,
      0.35183192039077316,
      0.26708912946022734
    ],
    [
      0.4160013223219283,
      0.46796823727616244,
      0.4632942732435308,
      0.49055211819975986,
      0.4574075960210464,
      0.4424473003946412,
      0.33186623413238325,
      0.41947937427960547,
      0.4761808712913915,
      0.3675561357839261,
      0.4226308711667577,
      0.23889190503634228,
      0.38565156678956347,
      0.435359549754299,
      0.41051076983234225,
      0.42043910045667876,
      0.0,
      0.4247092519484197,
      0.4514922765072278,
      0.44478266428492996,
      0.3658064391664875,
      0.41929193492605443,
      0.4155838734678208,
      0.45488145085274634,
      0.3807206884968841,
      0.42688090195518824,
      0.3287693049370166,
      0.42345581739496363,
      0.31288046406624215
    ],
    [
      0.40431353647242796,
      0.5965807688457843,
      0.5726357089846299,
      0.5518477322398951,
      0.49979850523408653,
      0.5356262313738791,
      0.3823860385954798,
      0.5821513150970683,
      0.5657624481691383,
      0.4669342190674377,
      0.5835508706365597,
      0.26263018306443753,
      0.530648909523777,
      0.5634339659984631,
      0.5191102026186805,
      0.5179418043113346,
      0.4474069340165012,
      0.0,
      0.5636316950347993,
      0.5900810425272394,
      0.4283351010274392,
      0.38344029134309365,
      0.4942694293317771,
      0.5669025670814751,
      0.4304485973082002,
      0.4665837706594367,
      0.4263123431677929,
      0.4764356285787099,
      0.3145593643383311
    ],
    [
      0.32513734952643025,
      0.4062302266974138,
      0.4548873989985682,
      0.4810888109102738,
      0.40840367995938665,
      0.42839189896332375,
      0.3498814238376009,
      0.42250250519518406,
      0.4470165266362216,
      0.38954289806511766,
      0.5044126644368021,
      0.2699076717237503,
      0.4689066397095507,
      0.3885862676969931,
      0.38221662927471134,
      0.526842398014228,
      0.4042652111083922,
      0.41391881074075876,
      0.0,
      0.44369347704221496,
      0.3922866263993223,
      0.3413402053121004,
      0.41493807574396047,
      0.4425440340101712,
      0.35122194949333196,
      0.3497907989511251,
      0.3611548448419277,
      0.3753459264600507,
      0.3423734470793527
    ],
    [
      0.4407302381564653,
      0.5546459175996117,
      0.5452094622886459,
      0.5364345841432403,
      0.4903362743794959,
      0.5091378731850724,
      0.3637905577036915,
      0.531466634321522,
      0.5945186435511769,
      0.40799977387337805,
      0.602228783656725,
      0.27252489686836556,
      0.5342884166534803,
      0.4812232668845067,
      0.5467447179517735,
      0.5630632913499509,
      0.4689500653124439,
      0.5247738716402004,
      0.5515675050042645,
      0.0,
      0.42259083248801765,
      0.45136018928862875,
      0.4663562196310873,
      0.5183890757988663,
      0.4333973688493429,
      0.43780965474131905,
      0.3916166099292513,
      0.4695141323055896,
      0.31429470969774864
    ],
    [
      0.3920164850572525,
      0.436446884073175,
      0.4249828019874162,
      0.4415000364140689,
      0.43823981734624917,
      0.4188188485012587,
      0.35219950698087654,
      0.39289289352942647,
      0.3703803700699462,
      0.33034237103165687,
      0.45282281930956403,
      0.2501636488726975,
      0.35831612115352685,
      0.44642240118438536,
      0.42817638219496,
      0.40092043805348365,
      0.4039948037021861,
      0.3838044855853149,
      0.4288230416733514,
      0.3832812913018202,
      0.0,
      0.4334195865126047,
      0.4853602390918079,
      0.4410574723585654,
      0.38526162898619654,
      0.557545786883705,
      0.37277844945857996,
      0.3782481821097301,
      0.28075806128350544
    ],
    [
      0.3571272973016921,
      0.3840654447172074,
      0.33667314518526115,
      0.41772820058780136,
      0.3445434136850729,
      0.33965788491225846,
      0.32310882726792833,
      0.3494941471158399,
      0.35723079145120296,
      0.37955256709244223,
      0.36511136639425334,
      0.27112487227595783,
      0.35588585117250626,
      0.3324087969487244,
      0.4372996805573861,
      0.3482440402188278,
      0.38882450546577485,
      0.35385496190460364,
      0.3654170307247908,
      0.35155454470160286,
      0.4265354410707567,
      0.0,
      0.43125972415007596,
      0.35572092504558794,
      0.3742877984856581,
      0.3932082296507178,
      0.32263733299414343,
      0.38541285935803193,
      0.30720469184072696
    ],
    [
      0.340418502596848,
      0.4604908735218949,
      0.4384241333777612,
      0.46837839804977555,
      0.4234052297843671,
      0.4324138124656933,
      0.32730802365444855,
      0.4218804464188821,
      0.4224528777824452,
      0.37527843043027476,
      0.46398434183997206,
      0.2663715633602306,
      0.41684581683180433,
      0.41516102605633054,
      0.4119842713135857,
      0.4620061677181022,
      0.418809403077923,
      0.43818012774315984,
      0.42478933367906,
      0.41949628764763847,
      0.4277077129578455,
      0.38758629883101126,
      0.0,
      0.4547319143067676,
      0.42723911397722425,
      0.4436796190856631,
      0.35405705194567316,
      0.4251245050921213,
      0.27535367142901257
    ],
    [
      0.34182643563426995,
      0.5218854322829389,
      0.5170776063346332,
      0.5248530581690805,
      0.4309946510322096,
      0.4966505755970463,
      0.29868354996158497,
      0.4532105230679837,
      0.4480103121195447,
      0.3532119842879451,
      0.5632046652720202,
      0.22594275297200972,
      0.4695015859851952,
      0.49998616654620465,
      0.4350126887754906,
      0.480578749552655,
      0.3897535930459757,
      0.48864297434515547,
      0.49410294662109666,
      0.4603905733399358,
      0.42739116223772644,
      0.37938501955159176,
      0.434220045529746,
      0.0,
      0.3456919584852549,
      0.4080676909285583,
      0.341078818810165,
      0.3498394586343019,
      0.24590346299348065
    ],
    [
      0.2941105064500589,
      0.34600877550541,
      0.3691404821036659,
      0.3418134117073077,
      0.4020772061982847,
      0.3180617491846225,
      0.31617150364104996,
      0.37905299263619185,
      0.3140457481819705,
      0.44866893181817447,
      0.3532875518198644,
      0.2330339755565991,
      0.33023444828431736,
      0.3351223539071646,
      0.31163565088764167,
      0.35339741122546076,
      0.3761466215819824,
      0.3567069536528309,
      0.3808034504513478,
      0.3466857278628712,
      0.352511493819194,
      0.3644610764115477,
      0.3533435024536342,
      0.32762905588727653,
      0.0,
      0.3351450284577977,
      0.37439802074261186,
      0.3407351841966493,
      0.2827288678254172
    ],
    [
      0.33226748575399556,
      0.4213251415159345,
      0.4181760092863651,
      0.4365210905028476,
      0.42861997583403944,
      0.4560050041186565,
      0.32809897379934183,
      0.43818945829532563,
      0.3762245424799635,
      0.34597243897786734,
      0.4559252187070577,
      0.2442646903602077,
      0.396355050495649,
      0.4750339869551179,
      0.4458845107269591,
      0.4398129145992056,
      0.4177615299276458,
      0.3998924396218768,
      0.4292421222910112,
      0.3992741423350141,
      0.4873217639547234,
      0.39973593532470586,
      0.47985134667097906,
      0.45293426220980537,
      0.32173073326725277,
      0.0,
      0.3685886690946054,
      0.37425618342864464,
      0.2884200205413172
    ],
    [
      0.3239530528621515,
      0.3651600690572432,
      0.4276271902949451,
      0.3302817469726276,
      0.3870066976520081,
      0.4138155125830336,
      0.3372548137382929,
      0.39815864987221405,
      0.3235363828978206,
      0.4136702277265156,
      0.40513515030332536,
      0.2774172139866793,
      0.3356230458294809,
      0.3574229589299802,
      0.3723648863773783,
      0.3755511297635026,
      0.38462184155066126,
      0.3432651255537078,
      0.38634812683798003,
      0.3428115142821,
      0.3912661658258121,
      0.36353933552310314,
      0.4667685061614615,
      0.3421655066338256,
      0.43876743153353215,
      0.3885871670350556,
      0.0,
      0.35215156232059774,
      0.31285488415369556
    ],
    [
      0.3377823704282821,
      0.416947101589058,
      0.4257633747667826,
      0.4016592460113284,
      0.4877368724202451,
      0.43338523052002476,
      0.34792808170685285,
      0.41458159245787174,
      0.4412052121214092,
      0.3552243203415726,
      0.40217395180505755,
      0.2688067425384899,
      0.3821861014102448,
      0.43374435546911116,
      0.4267307455203839,
      0.40814479078012567,
      0.4527692724890089,
      0.4146914505999433,
      0.4032720303230648,
      0.3816703124246317,
      0.3812669817624945,
      0.37039260564308685,
      0.39272583057330723,
      0.4122356325348602,
      0.3804583631916889,
      0.41316679664733624,
      0.359505439364858,
      0.0,
      0.353231349880303
    ],
    [
      0.3593215742196203,
      0.3579779044329363,
      0.35474476827372214,
      0.31624812956282455,
      0.34897346593950584,
      0.3665648375221924,
      0.3234591852855464,
      0.4052355316406562,
      0.3483428471675474,
      0.4174302334511497,
      0.365711355772544,
      0.3793668303774467,
      0.3858998799175326,
      0.3215148927513938,
      0.34906330526008733,
      0.36415888949155173,
      0.36554449151544355,
      0.32223044110410237,
      0.3527319883309594,
      0.360338015991583,
      0.34175508821463896,
      0.3834979181307583,
      0.3533522271898475,
      0.34278635757155396,
      0.4062223457028038,
      0.3599469053113349,
      0.3601169994339877,
      0.374111485179929,
      0.0
    ]
  ],
  "row_avgs": [
    0.09197448176137914,
    0.42670073505134154,
    0.45043570536364463,
    0.36848566364542606,
    0.3332121778885885,
    0.3932802508402579,
    0.2814505041236959,
    0.37707179212417336,
    0.38834204554800483,
    0.3444634287792209,
    0.3959302183021684,
    0.20493393397541051,
    0.35911712533240747,
    0.49409494647782515,
    0.36293522903530057,
    0.4307798032361166,
    0.4105532962137265,
    0.49013425730885263,
    0.403101014172438,
    0.4794629845447807,
    0.40246338766811823,
    0.3626847990098869,
    0.4086985341062684,
    0.4223249443612071,
    0.34418420294467655,
    0.4020602014670042,
    0.3698973534378118,
    0.39640664840433665,
    0.36023742481225707
  ],
  "col_avgs": [
    0.34400863064521686,
    0.4087954515483617,
    0.4129824927401356,
    0.41798004227935753,
    0.3866202326876178,
    0.4050570491129594,
    0.3091432184272359,
    0.4179818015982114,
    0.3945806569786923,
    0.35573633394006976,
    0.4354825455062454,
    0.2439936886274316,
    0.40293251898127286,
    0.3884430571408621,
    0.39233753713791203,
    0.4133154163875818,
    0.37708032850117074,
    0.38649911177770896,
    0.4197311696486136,
    0.4046099413702939,
    0.3780600427564231,
    0.3547767834878531,
    0.3970774162718663,
    0.4206517104536825,
    0.34547792667732213,
    0.3806877828098889,
    0.32323868652143206,
    0.3614802003727141,
    0.27665531554819245
  ],
  "combined_avgs": [
    0.217991556203298,
    0.41774809329985163,
    0.43170909905189014,
    0.3932328529623918,
    0.35991620528810314,
    0.39916864997660867,
    0.2952968612754659,
    0.3975267968611924,
    0.3914613512633486,
    0.3500998813596453,
    0.4157063819042069,
    0.22446381130142107,
    0.3810248221568402,
    0.4412690018093436,
    0.3776363830866063,
    0.4220476098118492,
    0.3938168123574486,
    0.43831668454328077,
    0.4114160919105258,
    0.4420364629575373,
    0.39026171521227065,
    0.35873079124887,
    0.4028879751890674,
    0.4214883274074448,
    0.34483106481099934,
    0.3913739921384466,
    0.34656801997962194,
    0.37894342438852535,
    0.31844637018022476
  ],
  "gppm": [
    545.5694259910699,
    591.5022324382113,
    589.5929520936257,
    588.2151807953189,
    598.2971336048452,
    591.0641825232024,
    630.2639535779603,
    584.048231772515,
    599.1220155960654,
    611.8430602771169,
    577.9419285926157,
    657.6782471920403,
    592.602237645785,
    600.8165311567636,
    597.0591026429371,
    590.1420955108173,
    603.814468036934,
    600.6795738575757,
    587.9634759092054,
    592.9493147510418,
    601.5547149785908,
    614.6505149182718,
    594.5450676654135,
    583.2412033595361,
    615.6326242099809,
    599.6147993964715,
    630.6028723885449,
    610.5309104313276,
    648.8642052636376
  ],
  "gppm_normalized": [
    1.2149960543818044,
    1.4557184906773963,
    1.4561222253772537,
    1.4442464809185547,
    1.4694361641120857,
    1.4512119031674235,
    1.5451919973561938,
    1.4283968081753986,
    1.4678569378600141,
    1.4951084540142736,
    1.4145818053435353,
    1.6016317944782155,
    1.4551507068578224,
    1.4706064358123552,
    1.4616595783348387,
    1.4436880359486441,
    1.4772059842554845,
    1.472276517047327,
    1.4406564379943636,
    1.4573686500074583,
    1.4667994587652242,
    1.495986559056716,
    1.4519496812157426,
    1.4291390360533478,
    1.5035720658020282,
    1.4632616136652434,
    1.5367206668317164,
    1.4933027679072557,
    1.5735255723863
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335,
    468,
    442,
    473,
    474,
    383,
    401,
    465,
    446,
    413,
    388,
    399,
    481,
    480,
    453,
    411,
    415,
    389,
    446,
    406,
    459,
    411,
    398,
    367,
    435,
    415,
    350,
    346,
    429,
    364,
    533,
    424,
    479,
    491,
    712,
    431,
    444,
    521,
    415,
    476,
    408,
    564,
    485,
    428,
    449,
    470,
    420,
    441,
    430,
    446,
    447,
    421,
    459,
    433,
    414,
    473,
    353,
    452,
    397,
    650,
    429,
    489,
    455,
    450,
    445,
    432,
    435,
    464,
    421,
    395,
    489,
    392,
    476,
    454,
    450,
    376,
    391,
    389,
    441,
    381,
    413,
    421,
    418,
    441,
    428,
    410,
    396,
    417,
    1943,
    416,
    433,
    407,
    650,
    411,
    434,
    376,
    416,
    422,
    408,
    502,
    428,
    414,
    429,
    420,
    352,
    404,
    352,
    439,
    417,
    355,
    372,
    425,
    401,
    534,
    404,
    461,
    349,
    539,
    434,
    448,
    462,
    435,
    468,
    471,
    457,
    428,
    459,
    392,
    455,
    380,
    417,
    420,
    425,
    418,
    379,
    391,
    487,
    375,
    426,
    378,
    397,
    363,
    392,
    405,
    407,
    337,
    473,
    419,
    495,
    432,
    470,
    474,
    380,
    373,
    422,
    479,
    404,
    457,
    444,
    433,
    437,
    416,
    423,
    392,
    464,
    456,
    432,
    385,
    405,
    480,
    428,
    438,
    447,
    512,
    369,
    548,
    444,
    472,
    485,
    427,
    474,
    473,
    415,
    410,
    430,
    430,
    487,
    409,
    392,
    429,
    460,
    396,
    399,
    475,
    446,
    421,
    438,
    387,
    384,
    422,
    399,
    414,
    378,
    371,
    1186,
    433,
    392,
    449,
    504,
    427,
    469,
    454,
    423,
    423,
    419,
    564,
    431,
    389,
    423,
    393,
    386,
    397,
    399,
    369,
    384,
    386,
    384,
    444,
    415,
    410,
    366,
    432,
    362
  ],
  "response_lengths": [
    5333,
    2491,
    2245,
    2555,
    2942,
    2465,
    2724,
    2582,
    2334,
    2364,
    2407,
    3128,
    2375,
    2211,
    2437,
    2228,
    2193,
    2229,
    2253,
    2090,
    2131,
    2161,
    2208,
    2541,
    2409,
    2302,
    2088,
    2467,
    2097
  ]
}