{
  "example_idx": 51,
  "reference": "Published as a conference paper at ICLR 2023\n\nLEARNING THE POSITIONS IN COUNTSKETCH\n\nYi Li ∗ Nanyang Technological University yili@ntu.edu.sg\n\nHonghao Lin, Simin Liu Carnegie Mellon University {honghaol, siminliu}@andrew.cmu.edu\n\nAli Vakilian Toyota Technological Institute at Chicago vakilian@ttic.edu\n\nDavid P. Woodruff Carnegie Mellon University dwoodruf@andrew.cmu.edu\n\nABSTRACT\n\nWe consider sketching algorithms which first compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low-rank approximation and regression. In the learning-based sketching paradigm proposed by Indyk, Vakilian, and Yuan (2019), the sketch matrix is found by choosing a random sparse matrix, e.g., CountSketch, and then the values of its non-zero entries are updated by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned. In this work, we propose the first learning-based algorithms that also optimize the locations of the non-zero entries. Our first proposed algorithm is based on a greedy algorithm. However, one drawback of the greedy algorithm is its slower training time. We fix this issue and propose approaches for learning a sketching matrix for both low-rank approximation and Hessian approximation for second order optimization. The latter is helpful for a range of constrained optimization problems, such as LASSO and matrix estimation with a nuclear norm constraint. Both approaches achieve good accuracy with a fast running time. Moreover, our experiments suggest that our algorithm can still reduce the error significantly even if we only have a very limited number of training matrices.\n\n1\n\nINTRODUCTION\n\nThe work of (Indyk et al., 2019) investigated learning-based sketching algorithms for low-rank approximation. A sketching algorithm is a method of constructing approximate solutions for optimization problems via summarizing the data. In particular, linear sketching algorithms compress data by multiplication with a sparse “sketch matrix” and then use just the compressed data to find an approximate solution. Generally, this technique results in much faster or more space-efficient algorithms for a fixed approximation error. The pioneering work of Indyk et al. (2019) shows it is possible to learn sketch matrices for low-rank approximation (LRA) with better average performance than classical sketches.\n\nIn this model, we assume inputs come from an unknown distribution and learn a sketch matrix with strong expected performance over the distribution. This distributional assumption is often realistic – there are many situations where a sketching algorithm is applied to a large batch of related data. For example, genomics researchers might sketch DNA from different individuals, which is known to exhibit strong commonalities. The high-performance computing industry also uses sketching, e.g., researchers at NVIDIA have created standard implementations of sketching algorithms for CUDA, a widely used GPU library. They investigated the (classical) sketched singular value decomposition (SVD), but found that the solutions were not accurate enough across a spectrum of inputs (Chien & Bernabeu, 2019). This is precisely the issue addressed by the learned sketch paradigm where we optimize for “good” average performance across a range of inputs.\n\n∗All authors contributed equally.\n\n1\n\nPublished as a conference paper at ICLR 2023\n\nWhile promising results have been shown using previous learned sketching techniques, notable gaps remain. In particular, all previous methods work by initializing the sketching matrix with a random sparse matrix, e.g., each column of the sketching matrix has a single non-zero value chosen at a uniformly random position. Then, the values of the non-zero entries are updated by running gradient descent on a training data set, or via other methods. However, the locations of the non-zero entries are held fixed throughout the entire training process.\n\nClearly this is sub-optimal. Indeed, suppose the input matrix A is an n × d matrix with first d rows equal to the d × d identity matrix, and remaining rows equal to 0. A random sketching matrix S with a single non-zero per column is known to require m = Ω(d2) rows in order for S ·A to preserve the rank of A (Nelson & Nguyên, 2014); this follows by a birthday paradox argument. On the other hand, it is clear that if S is a d × n matrix with first d rows equal to the identity matrix, then ∥S · Ax∥2 = ∥Ax∥2 for all vectors x, and so S preserves not only the rank of A but all important spectral properties. A random matrix would be very unlikely to choose the non-zero entries in the first d columns of S so perfectly, whereas an algorithm trained to optimize the locations of the non-zero entries would notice and correct for this. This is precisely the gap in our understanding that we seek to fill.\n\nLearned CountSketch Paradigm of Indyk et al. (2019). Throughout the paper, we assume our data A ∈ Rn×d is sampled from an unknown distribution D. Specifically, we have a training set Tr = {A1, . . . , AN } ∈ D. The generic form of our optimization problems is minX f (A, X), where A ∈ Rn×d is the input matrix. For a given optimization problem and a set S of sketching matrices, define ALG(S, A) to be the output of the classical sketching algorithm resulting from using S; this uses the sketching matrices in S to map the given input A and construct an approximate solution ˆX. We remark that the number of sketches used by an algorithm can vary and in its simplest case, S is a single sketch, but in more complicated sketching approaches we may need to apply sketching more than once—hence S may also denote a set of more than one sketching matrix.\n\nThe learned sketch framework has two parts: (1) offline sketch learning and (2) “online” sketching (i.e., applying the learned sketch and some sketching algorithm to possibly unseen data). In offline sketch learning, the goal is to construct a CountSketch matrix (abbreviated as CS matrix) with the minimum expected error for the problem of interest. Formally, that is,\n\narg minCS S EA∈Tr f (A, ALG(S, A)) − f (A, X ∗) = arg minCS S EA∈Tr f (A, ALG(S, A)),\n\nwhere X ∗ denotes the optimal solution. Moreover, the minimum is taken over all possible constructions of CS. We remark that when ALG needs more than one CS to be learned (e.g., in the sketching algorithm we consider for LRA), we optimize each CS independently using a surrogate loss function.\n\nIn the second part of the learned sketch paradigm, we take the sketch from part one and use it within a sketching algorithm. This learned sketch and sketching algorithm can be applied, again and again, to different inputs. Finally, we augment the sketching algorithm to provide worst-case guarantees when used with learned sketches. The goal is to have good performance on A ∈ D while the worst-case performance on A ̸∈ D remains comparable to the guarantees of classical sketches. We remark that the learned matrix S is trained offline only once using the training data. Hence, no additional computational cost is incurred when solving the optimization problem on the test data.\n\nOur Results. In this work, in addition to learning the values of the non-zero entries, we learn the locations of the non-zero entries. Namely, we propose three algorithms that learn the locations of the non-zero entries in CountSketch. Our first algorithm (Section 4) is based on a greedy search. The empirical result shows that this approach can achieve a good performance. Further, we show that the greedy algorithm is provably beneficial for LRA when inputs follow a certain input distribution (Section F). However, one drawback of the greedy algorithm is its much slower training time. We then fix this issue and propose two specific approaches for optimizing the positions for the sketches for low-rank approximation and second-order optimization, which run much faster than all previous algorithms while achieving better performance.\n\nFor low-rank approximation, our approach is based on first sampling a small set of rows based on their ridge leverage scores, assigning each of these sampled rows to a unique hash bucket, and then placing each non-sampled remaining row in the hash bucket containing the sampled row for which it is most similar to, i.e., for which it has the largest dot product with. We also show that the worst-case guarantee of this approach is strictly better than that of the classical Count-Sketch (see Section 5).\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nFor sketch-based second-order optimization where we focus on the case that n ≫ d, we observe that the actual property of the sketch matrix we need is the subspace embedding property. We next optimize this property of the sketch matrix. We provably show that the sketch matrix S needs fewer rows, with optimized positions of the non-zero entries, when the input matrix A has a small number of rows with a heavy leverage score. More precisely, while CountSketch takes O(d2/(δε2)) rows with failure probability δ, in our construction, S requires only O((d polylog(1/ε) + log(1/δ))/ε2) rows if A has at most d polylog(1/ε)/ε2 rows with leverage score at least ε/d. This is a quadratic improvement in d and an exponential improvement in δ. In practice, it is not necessary to calculate the leverage scores. Instead, we show in our experiments that the indices of the rows of heavy leverage score can be learned and the induced S is still accurate. We also consider a new learning objective, that is, we directly optimize the subspace embedding property of the sketching matrix instead of optimizing the error in the objective function of the optimization problem in hand. This demonstrates a significant advantage over non-learned sketches, and has a fast training time (Section 6).\n\nWe show strong empirical results for real-world datasets. For low-rank approximation, our methods reduce the errors by 70% than classical sketches under the same sketch size, while we reduce the errors by 30% than previous learning-based sketches. For second-order optimization, we show that the convergence rate can be reduced by 87% over the non-learned CountSketch for the LASSO problem on a real-world dataset. We also evaluate our approaches in the few-shot learning setting where we only have a limited amount of training data (Indyk et al., 2021). We show our approach reduces the error significantly even if we only have one training matrix (Sections 7 and 8). This approach clearly runs faster than all previous methods.\n\nAdditional Related Work. In the last few years, there has been much work on leveraging machine learning techniques to improve classical algorithms. We only mention a few examples here which are based on learned sketches. One related body of work is data-dependent dimensionality reduction, such as an approach for pair-wise/multi-wise similarity preservation for indexing big data (Wang et al., 2017), learned sketching for streaming problems (Indyk et al., 2019; Aamand et al., 2019; Jiang et al., 2020; Cohen et al., 2020; Eden et al., 2021; Indyk et al., 2021), learned algorithms for nearest neighbor search (Dong et al., 2020), and a method for learning linear projections for general applications (Hegde et al., 2015). While we also learn linear embeddings, our embeddings are optimized for the specific application of low rank approximation. In fact, one of our central challenges is that the theory and practice of learned sketches generally needs to be tailored to each application. Our work builds off of (Indyk et al., 2019), which introduced gradient descent optimization for LRA, but a major difference is that we also optimize the locations of the non-zero entries.\n\n2 PRELIMINARIES\n\nNotation. Denote the canonical basis vectors of Rn by e1, . . . , en. Suppose that A has singular value decomposition (SVD) A = U ΣV ⊤. Define [A]k = UkΣkV ⊤ k to be the optimal rank-k approximation to A, computed by the truncated SVD. Also, define the Moore-Penrose pseudo-inverse of A to be A† = V Σ−1U ⊤, where Σ−1 is constructed by inverting the non-zero diagonal entries. Let row(A) and col(A) be the row space and the column space of A, respectively.\n\nCountSketch. We define SC ∈ Rm×n as a classical CountSketch (abbreviated as CS). It is a sparse matrix with one nonzero entry from {±1} per column. The position and value of this nonzero entry are chosen uniformly at random. CountSketch matrices can be succinctly represented by two vectors. We define p ∈ [m]n, v ∈ Rn as the positions and values of the nonzero entries, respectively. Further, we let CS(p, v) be the CountSketch constructed from vectors p and v.\n\nBelow we define the objective function f (·, ·) and a classical sketching algorithm ALG(S, A) for each individual problem.\n\nLow-rank approximation (LRA). In LRA, we find a rank-k approximation of our data that minimizes the Frobenius norm of the approximation error. For A ∈ Rn×d, minrank-k B fLRA(A, B) = minrank-k X ∥A − B∥2 F . Usually, instead of outputting the a whole B ∈ Rn×d, the algorithm outputs two factors Y ∈ Rn×k and X ∈ Rk×d such that B = Y X for efficiency.\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nIndyk et al. (2019) considered Algorithm 1, which only compresses one side of the input matrix A. However, in practice often both dimensions of the matrix A are large. Hence, in this work we consider Algorithm 2 that compresses both sides of A. Constrained regression. Given a vector b ∈ Rn, a matrix A ∈ Rn×d (n ≫ d) and a convex set C, we want to find x to minimize the squared error\n\nminx∈C fREG([A b], X) = minx∈C ∥Ax − b∥2 2 .\n\n(2.1)\n\nIterative Hessian Sketch. The Iterative Hessian Sketching (IHS) method (Pilanci & Wainwright, 2016) solves the constrained least-squares problem by iteratively performing the update\n\nxt+1 = arg min\n\nx∈C\n\n(cid:26) 1 2\n\n∥St+1A(x − xt)∥2\n\n2 − ⟨A⊤(b − Axt), x − xt⟩\n\n(cid:27)\n\n,\n\n(2.2)\n\nwhere St+1 is a sketching matrix. It is not difficult to see that for the unsketched version (St+1 is the identity matrix) of (2.2), the optimal solution xt+1 coincides with the optimal solution to the original constrained regression problem (2.1). The IHS approximates the Hessian A⊤A by a sketched version (St+1A)⊤(St+1A) to improve runtime, as St+1A typically has very few rows.\n\nAlgorithm 1 Rank-k approximation of A using a sketch S (see (Clarkson & Woodruff, 2009, Sec. 4.1.1)) Input: A ∈ Rn×d, S ∈ Rm×n\n\n1: U, Σ, V ⊤ ← COMPACTSVD(SA)\n\n{r = rank(SA), U ∈ Rm×r, V ∈ Rd×r}\n\n▷\n\n2: Return: [AV ]kV ⊤\n\nAlgorithm 2 ALGLRA(SKETCH-LOWRANK) Sarlos (2006); Clarkson & Woodruff (2017); Avron et al. (2017). Input: A ∈ Rn×d, S ∈ RmS ×n, R ∈ RmR×d,\n\nV ∈ RmV ×n, W ∈ RmW ×d\n\n1: UC [TC T ′\n\nC] ← V AR⊤,\n\n(cid:20) T ⊤ D\nT ′⊤ D\n\n(cid:21)\n\nU ⊤\n\nD ←\n\nSAW ⊤ with UC, UD orthogonal\n\n2: G ← V AW ⊤, Z ′ 3: ZL ← (cid:2)Z ′\n\nR ← [U ⊤ D )⊤ 0(cid:3) , ZR ←\n\nL(T −1\n\nLZ ′\n\nC GUD]k (cid:20)T −1 C Z ′ 0\n\nR\n\n(cid:21)\n\n4: Z ← ZLZR 5: return: AR⊤ZSA in form Pn×k, Qk×d\n\n3 DESCRIPTION OF OUR APPROACH\n\nLearning-Based Algorithms in the Few-Shot Setting. Recently, Indyk et al. (2021) studied learning-based algorithms for LRA in the setting where we have access to limited data or computing resources. We provide a brief explanation of learning-based algorithms in the Few-Shot setting in Appendix A.3.\n\nLeverage Scores and Ridge Leverage Scores. Given a matrix A, the leverage score of the i-th row ai of A is defined to be τi := ai(A⊤A)†a⊤ i , which is the squared l2-norm of the i-th row of U , where A = U ΣV T is the singular value decomposition of A. Given a regularization parameter λ, the ridge leverage score of the i-th row ai of A is defined to be τi := ai(A⊤A + λI)†a⊤ i . Our learning-based algorithms employs the ridge leverage score sampling technique proposed in (Cohen et al., 2017), which shows that sampling proportional to ridge leverage scores gives a good solution to LRA.\n\nWe describe our contributions to the learning-based sketching paradigm which, as mentioned, is to learn the locations of the non-zero values in the sketch matrix. To learn a CountSketch for the given training data set, we locally optimize the following in two stages:\n\nminS EA∈D [f (A, ALG(S, A))] .\n\n(3.1)\n\n(1) compute the positions of the non-zero entries, then (2) fix the positions and optimize their values.\n\nStage 1: Optimizing Positions. In Section 4, we provide a greedy search algorithm for this stage, as our starting point. In Section 5 and 6, we provide our specific approaches for optimizing the positions for the sketches for low-rank approximation and second-order optimization.\n\nStage 2: Optimizing Values. This stage is similar to the approach of Indyk et al. (2019). However, instead of the power method, we use an automatic differentiation package, PyTorch (Paszke et al., 2019), and we pass it our objective minv∈Rn EA∈D [f (A, ALG(CS(p, v), A))], implemented as a chain of differentiable operations. It will automatically compute the gradient using the chain rule. We\n\n4\n\nPublished as a conference paper at ICLR 2023\n\nalso consider new approaches to optimize the values for LRA (proposed in Indyk et al. (2021), see Appendix A.3 for details) and second-order optimization (proposed in Section 6).\n\nWorst-Cases Guarantees. In Appendix D, we show that both of our approaches for the above two problems can perform no worse than a classical sketching matrix when A does not follow the distribution D. In particular, for LRA, we show that the sketch monotonicity property holds for the time-optimal sketching algorithm for low rank approximation. For second-order optimization, we propose an algorithm which runs in input-sparsity time and can test for and use the better of a random sketch and a learned sketch.\n\n4 SKETCH LEARNING: GREEDY SEARCH\n\nAlgorithm 3 POSITION OPTIMIZATION: GREEDY SEARCH Input: f, ALG, Tr = {A1, ..., AN ∈ Rn×d};\n\nWhen S is a CountSketch, computing SA amounts to hashing the n rows of A into the m ≪ n rows of SA. The optimization is a combinatorial optimization problem with an empirical risk minimization (ERM) objective. The naïve solution is to compute the objective value of the exponentially many (mn) possible placements, but this is clearly intractable. Instead, we iteratively construct a full placement in a greedy fashion. We start with S as a zero matrix. Then, we iterate through the columns of S in an order determined by the algorithm, adding a nonzero entry to each. The best position in each column is the one that minimizes Eq. (3.1) if an entry were to be added there. For each column, we evaluate Eq. (3.1) O(m) times, once for each prospective half-built sketch.\n\nsketch dimension m 1: initialize SL = Om×n 2: for i = 1 to n do 3:\n\n4: 5: end for 6: return p for SL = CS(p, v)\n\nf (A, ALG(SL ± eje⊤\n\nSL ← SL ± (eje⊤ i )\n\nj ← arg min\n\ni , A))\n\nj∈[m]\n\n(cid:88)\n\nA∈Tr\n\nWhile this greedy strategy is simple to state, additional tactics are required for each problem to make it more tractable. Usually the objective evaluation (Algorithm 3, line 3) is too slow, so we must leverage our insight into their sketching algorithms to pick a proxy objective. Note that we can reuse these proxies for value optimization, since they may make gradient computation faster too.\n\nProxy objective for LRA. For the two-sided sketching algorithm, we can assume that the two factors X, Y has the form Y = AR⊤ ̃Y and X = ̃XSA, where S and R are both CS matrices, so we optimize the positions in both S and R. We cannot use f (A, ALG(S, R, A)) as our objective because then we would have to consider combinations of placements between S and R. To find a proxy, we note that a prerequisite for good performance is for row(SA) and col(AR⊤) to both contain a good rank-k approximation to A (see proof of Lemma C.5). Thus, we can decouple the optimization of S and R. The (cid:13)[AV ]kV ⊤ − A(cid:13) proxy objective for S is (cid:13) 2\nF where SA = U ΣV ⊤. In this expression, ˆX = [AV ]kV ⊤ (cid:13) is the best rank-k approximation to A in row(SA). The proxy objective for R is defined analogously.\n\nIn Appendix F, we show the greedy algorithm is provably beneficial for LRA when inputs follow the spiked covariance or the Zipfian distribution. Despite the good empirical performance we present in Section 7, one drawback is its much slower training time. Also, for the iterative sketching method for second-order optimization, it is non-trivial to find a proxy objective because the input of the i-th iteration depends on the solution to the (i − 1)-th iteration, for which the greedy approach sometimes does not give a good solution. In the next section, we will propose our specific approach for optimizing the positions of the sketches for low-rank approximation and second-order optimization, both of which achieve a very high accuracy and can finish in a very short amount of time.\n\n5 SKETCH LEARNING: LOW-RANK APPROXIMATION\n\nNow we present a conceptually new algorithm which runs much faster and empirically achieves similar error bounds as the greedy search approach. Moreover, we show that this algorithm has strictly better guarantees than the classical Count-Sketch.\n\nTo achieve this, we need a more careful analysis. To provide some intuition, if rank(SA) = k and SA = U ΣV ⊤, then the rank-k approximation cost is exactly (cid:13) F , the projection cost\n\n(cid:13)AV V ⊤ − A(cid:13) 2\n(cid:13)\n\n5\n\nPublished as a conference paper at ICLR 2023\n\nonto col(V ). Minimizing it is equivalent to maximizing the sum of squared projection coefficients:\n\narg min S\n\n(cid:13)A − AV V ⊤(cid:13) (cid:13) 2\n(cid:13)\n\nF = arg min\n\nS\n\n(cid:88)\n\n(∥Ai∥2\n\n2 −\n\n(cid:88)\n\ni∈[n]\n\nj∈[k]\n\n⟨Ai, vj⟩2) = arg max\n\n(cid:88)\n\n(cid:88)\n\n⟨Ai, vj⟩2.\n\ni∈[n]\n\nj∈[k]\n\nS\n\nAs mentioned, computing SA actually amounts to hashing the n rows of A to the m rows of SA. Hence, intuitively, if we can put similar rows into the same bucket, we may get a smaller error.\n\nAlgorithm 4 Position optimization: Inner Product Input: A ∈ Rn×d: average of Tr; sketch dim. m 1: initialize S1, S2 = Om×n 2: Sample a set C = {C1 · · · Cm} of rows using ridge leverage score sampling (see Section 2).\n\n3: for i = 1 to n do 4:\n\npi, vi ← arg max\n\np∈[m],v∈{±1}\n\n⟨ Cp\n\n∥Cp∥2\n\n, v Ai\n\n∥Ai∥2\n\n⟩\n\nIi ← {j | pj = i} A(i) ← restriction of A to rows in Ii ui ← the top left singular vector of A(i) S1[i, Ii] ← u⊤\n\nS1[pi, i] ← vi\n\n5: 6: end for 7: for i = 1 to m do 8:\n\ni\n\n9: 10: 11: 12: end for 13: for i = 1 to m do 14: 15: 16: end for 17: return S1 or [ S1\n\nOur algorithm is given in Algorithm 4. Suppose that we want to form matrix S with m rows. At the beginning of the algorithm, we sample m rows according to the ridge leverage scores of A. By the property of the ridge leverage score, the subspace spanned by this set of sampled rows contains an approximately optimal solution to the low rank approximation problem. Hence, we map these rows to separate “buckets” of SA. Then, we need to decide the locations of the remaining rows (i.e., the non-sampled rows). Ideally, we want similar rows to be mapped into the same bucket. To achieve this, we use the m sampled rows as reference points and assign each (nonsampled) row Ai to the p-th bucket in SA if the normalized row Ai and Cp have the largest inner product (among all possible buckets).\n\nqi ← index such that Ci is the qi-th row of A S2[i, qi] ← 1\n\n]\n\nS2\n\nOnce the locations of the non-zero entries are fixed, the next step is to determine the values of these entries. We follow the same idea proposed in (Indyk et al., 2021): for each block A(i), one natural approach is to choose the unit vector si ∈ R|Ii| that preserves as much of the Frobenius norm of A(i) as possible, i.e., to maximize (cid:13) i A(i)(cid:13) 2\n2. Hence, we set si to be the top left singular vector of A(i). In our experiments, we observe (cid:13)s⊤ (cid:13) that this step reduces the error of downstream value optimizations performed by SGD.\n\nTo obtain a worst-case guarantee, we show that w.h.p., the row span of the sampled rows Ci is a good subspace. We set the matrix S2 to be the sampling matrix that samples Ci. The final output of our algorithm is the vertical concatenation of S1 and S2. Here S1 performs well empirically, while S2 has a worst-case guarantee for any input. Combining Lemma E.2 and the sketch monotonicity for low rank approximation in Section D, we get that O(k log k + k/ε) rows is enough for a (1 ± ε)- approximation for the input matrix A induced by Tr, which is better than the Ω(k2) rows required of a non-learned Count-Sketch, even if its non-zero values have been further improved by the previous learning-based algorithms in (Indyk et al., 2019; 2021). As a result, under the assumption of the input data, we may expect that S will still be good for the test data. We defer the proof to Appendix E.1.\n\nIn Appendix A, we shall show that the assumptions we make in Theorem 5.1 are reasonable. We also provide an empirical comparison between Algorithm 4 and some of its variants, as well as some adaptive sketching methods on the training sample. The evaluation result shows that only our algorithm has a significant improvement for the test data, which suggests that both ridge leverage score sampling and row bucketing are essential. Theorem 5.1. Let S ∈ R2m×n be given by concatenating the sketching matrices S1, S2 computed by Algorithm 4 with input A induced by Tr and let B ∈ Rn×d. Then with probability at least 1 − δ, we have minrank-k X:row(X)⊆row(SB) ∥B − X∥2 F if one of the following holds:.\n\nF ≤ (1 + ε) ∥B − Bk∥2\n\n1. m = O(β · (k log k + k/ε)), δ = 0.1, and τi(B) ≥ 1 2. m = O(k log k + k/ε), δ = 0.1 + 1.1β, and the total variation distance dtv(p, q) ≤ β, where\n\nβ τi(A) for all i ∈ [n].\n\np, q are sampling probabilities defined as pi = τi(A)\n\n(cid:80)\n\ni τi(A) and qi = τi(B)\n\ni τi(B) .\n\n(cid:80)\n\nTime Complexity. As mentioned, an advantage of our second approach is that it significantly reduces the training time. We now discuss the training times of different algorithms. For the value-learning\n\n6\n\nPublished as a conference paper at ICLR 2023\n\nalgorithms in (Indyk et al., 2019), each iteration requires computing a differentiable SVD to perform gradient descent, hence the runtime is at least Ω(nit · T ), where nit is the number of iterations (usually set > 500) and T is the time to compute an SVD. For the greedy algorithm, there are m choices for each column, hence the runtime is at least Ω(mn · T ). For our second approach, the most complicated step is to compute the ridge leverage scores of A and then the SVD of each submatrix. Hence, the total runtime is at most O(T ). We note that the time complexities discussed here are all for training time. There is no additional runtime cost for the test data.\n\n6 SKETCH LEARNING: SECOND-ORDER OPTIMIZATION\n\nIn this section, we consider optimizing the sketch matrix in the context of second-order methods. The key observation is that for many sketching-based second-order methods, the crucial property of the sketching matrix is the so-called subspace embedding property: for a matrix A ∈ Rn×d, we say a matrix S ∈ Rm×n is a (1 ± ε)-subspace embedding for the column space of A if (1 − ε) ∥Ax∥2 ≤ ∥SAx∥2 ≤ (1 + ε) ∥Ax∥2 for all x ∈ Rd. For example, consider the iterative Hessian sketch, which performs the update (2.2) to compute {xt}t. Pilanci & Wainwright (2016) showed that if S1, . . . , St+1 are (1 + O(ρ))-subspace embeddings of A, then ∥A(xt − x∗)∥2 ≤ ρt ∥Ax∗∥2. Thus, if Si is a good subspace embedding of A and we will have a good convergence guarantee. Therefore, unlike (Indyk et al., 2019), which treats the training objective in a black-box manner, we shall optimize the subspace embedding property of the matrix A.\n\nOptimizing positions. We consider the case that A has a few rows of large leverage score, as well as access to an oracle which reveals a superset of the indices of such rows. Formally, let τi(A) be the leverage score of the i-th row of A and I ∗ = {i : τi(A) ≥ ν} be the set of rows with large leverage score. Suppose that a superset I ⊇ I ∗ is known to the algorithm. In the experiments we train an oracle to predict such rows. We can maintain all rows in I explicitly and apply a Count-Sketch to the remaining rows, i.e., the rows in [n] \\ I. Up to permutation of the rows, we can write\n\nA =\n\n(cid:19)\n\n(cid:18) AI AI c\n\nand S =\n\n(cid:19)\n\n(cid:18)I\n\n0 0 S′\n\n,\n\n(6.1)\n\nwhere S′ is a random Count-Sketch matrix of m rows. Clearly S has a single non-zero entry per column. We have the following theorem, whose proof is postponed to Section E.2. Intuitively, the proof for Count-Sketch in (Clarkson & Woodruff, 2017) handles rows of large leverage score and rows of small leverage score separately. The rows of large leverage score are to be perfectly hashed while the rows of small leverage score will concentrate in the sketch by the Hanson-Wright inequality.\n\nTheorem 6.1. Let ν = ε/d. Suppose that m = O((d/ε2)(polylog(1/ε) + log(1/δ))), δ ∈ (0, 1/m] and d = Ω((1/ε) polylog(1/ε) log2(1/δ)). Then, there exists a distribution on S of the form in (6.1) (cid:9) ≤ δ. In particular, when with m + |I| rows such that Pr (cid:8)∀x ∈ col(A), | ∥Sx∥2 δ = 1/m, the sketching matrix S has O((d/ε2) polylog(d/ε)) rows.\n\n2 | > ε ∥x∥2\n\n2 − ∥x∥2\n\n2\n\nHence, if there happen to be at most d polylog(1/ε)/ε2 rows of leverage score at least ε/d, the overall sketch length for embedding colsp(A) can be reduced to O((d polylog(1/ε) + log(1/δ))/ε2), a quadratic improvement in d and an exponential improvement in δ over the original sketch length of O(d2/(ε2δ)) for Count-Sketch. In the worst case there could be O(d2/ε) such rows, though empirically we do not observe this. In Section 8, we shall show it is possible to learn the indices of the heavy rows for real-world data.\n\nOptimizing values. When we fix the positions of the non-zero entries, we aim to optimize the values by gradient descent. Rather than the previous black-box way in (Indyk et al., 2019) that minimizes (cid:80) i f (A, ALG(S, A)), we propose the following objective loss function for the learning algorithm L(S, A) = (cid:80) Ai∈A ∥(AiRi)⊤AiRi − I∥F , over all the training data, where Ri comes from the QR decomposition of SAi = QiR−1 . The intuition for this loss function is given by the lemma below, whose proof is deferred to Section E.3. 2 ), S ∈ Rm×n, A ∈ Rn×d of full column rank, and SA = QR Lemma 6.2. Suppose that ε ∈ (0, 1 is the QR-decomposition of SA. If ∥(AR−1)⊤AR−1 − I∥op ≤ ε, then S is a (1 ± ε)-subspace embedding of col(A).\n\ni\n\n7\n\nPublished as a conference paper at ICLR 2023\n\nLemma 6.2 implies that if the loss function over Atrain is small and the distribution of Atest is similar to Atrain, it is reasonable to expect that S is a good subspace embedding of Atest. Here we use the Frobenius norm rather than operator norm in the loss function because it will make the optimization problem easier to solve, and our empirical results also show that the performance of the Frobenius norm is better than that of the operator norm.\n\n7 EXPERIMENTS: LOW-RANK APPROXIMATION\n\nIn this section, we evaluate the empirical performance of our learning-based approach for LRA on three datasets. For each, we fix the sketch size and compare the approximation error ∥A − X∥F − ∥A − Ak∥F averaged over 10 trials. In order to make position optimization more efficient, in line 3 of Algorithm 3), instead of computing many rank-1 SVD updates, we use formulas for fast rank-1 SVD updates (Brand, 2006). For the greedy method, we used several Nvidia GeForce GTX 1080 Ti machines. For the maximum inner product method, the experiments are conducted on a laptop with a 1.90GHz CPU and 16GB RAM.\n\nDatasets. We use the three datasets from (Indyk et al., 2019): (1, 2) Friends, Logo (image): frames from a short video of the TV show Friends and of a logo being painted; (3) Hyper (image): hyperspectral images from natural scenes. Additional details are in Table A.1.\n\nBaselines. We compare our approach to the following baselines. Classical CS: a random CountSketch. IVY19: a sparse sketch with learned values, and random positions for the non-zero entries. Ours (greedy): a sparse sketch where both the values and positions of the non-zero entries are learned. The positions are learned by Algorithm 3. The values are learned similarly to (Indyk et al., 2019). Ours (inner product): a sparse sketch where both the values and the positions of the non-zero entries are learned. The positions are learned by S1 in Algorithm 4. IVY19 and greedy algorithm use the full training set and our Algorithm 4 takes the input as the average over the entire training matrix.\n\nWe also give a sensitivity analysis for our algorithm, where we compare our algorithm with the following variants: Only row sampling (perform projection by ridge leverage score sampling), l2 sampling (Replace leverage score sampling with l2-norm row sampling and maintain the same downstream step), and Randomly Grouping (Use ridge leverage score sampling but randomly distribute the remaining rows). The result shows none of these variants outperforms non-learned sketching. We defer the results of this part to Appendix A.1.\n\nResult Summary. Our empirical results are provided in Table 7.1 for both Algorithm 2 and Algorithm 1, where the errors take an average over 10 trials. We use the average of all training matrices from Tr, as the input to the algorithm 4. We note that all the steps of our training algorithms are done on the training data. Hence, no additional computational cost is incurred for the sketching algorithm on the test data. Experimental parameters (i.e., learning rate for gradient descent) can be found in Appendix G. For both sketching algorithms, Ours are always the best of the four sketches. It is significantly better than Classical CS, obtaining improvements of around 70%. It also obtains a roughly 30% improvement over IVY19.\n\nOffline learning Online solving\n\nOurs (inner product) Ours (greedy) IVY19 Classical CS\n\n5 6300 (1.75h) 193 (3min) ✗\n\n0.166 0.172 0.168 0.166\n\nTable 7.2: Runtime (in seconds) of LRA on Logo with k = 30, m = 60\n\nWall-Clock Times. The offline learning runtime is in Table 7.2, which is the time to train a sketch on Atrain. We can see that although the greedy method will take much longer (1h 45min), our second approach is much faster (5 seconds) than the previous algorithm in (Indyk et al., 2019) (3 min) and can still achieve a similar error as the greedy\n\nk, m, Sketch 20, 40, Classical CS 20, 40, IVY19 20, 40, Ours (greedy) 20, 40, Ours (inner product) 30, 60, Class CS 30, 60, IVY19 30, 60, Ours (greedy) 30, 60, Ours (inner product)\n\nLogo 2.371 0.687 0.500 0.532 1.642 0.734 0.492 0.436\n\nFriends Hyper 6.344 4.073 3.764 1.048 2.497 0.899 0.733 2.975 5.390 2.683 3.748 1.077 2.492 0.794 2.409 0.733\n\nk, m, Sketch 20, 40, Classical CS 20, 40, IVY19 20, 40, Ours (greedy) 20, 40, Ours (inner product) 30, 60, Classical CS 30, 60, IVY19 30, 60, Ours(greedy) 30, 60, Ours(inner product)\n\nLogo 0.930 0.255 0.196 0.205 0.650 0.290 0.197 0.201\n\nFriends Hyper 2.971 1.542 1.273 0.723 0.784 0.407 0.407 1.223 2.315 1.0575 1.274 0.713 0.717 0.406 0.340 0.943\n\nTable 7.1: Test errors for LRA. (Left: two-side sketch. Right: one-side sketch)\n\n8\n\nPublished as a conference paper at ICLR 2023\n\nFigure 7.1: Test error of LASSO in Electric dataset.\n\nalgorithm. The reason is that Algorithm 4 only needs to compute the ridge leverage scores on the training matrix once, which is actually much cheaper than IVY19 which needs to compute a differentiable SVD many times during gradient descent.\n\nIn Section A.4, we also study the performance of our approach in the few-shot learning setting, which has been studied in Indyk et al. (2021).\n\n8 EXPERIMENTS: SECOND-ORDER OPTIMIZATION\n\nIn this section, we consider the IHS on the following instance of LASSO regression:\n\nx∗ = arg min∥x∥1≤λ f (x) = arg min∥x∥1≤λ\n\n1\n\n2 ∥Ax − b∥2 2 ,\n\n(8.1)\n\nwhere λ is a parameter. We also study the performance of the sketches on the matrix estimation with a nuclear norm constraint problem, the fast regression solver (van den Brand et al. (2021)), as well as the use of sketches for first-order methods. The results can be found in Appendix B. All of our experiments are conducted on a laptop with a 1.90GHz CPU and 16GB RAM. The offline training is done separately using a single GPU. The details of the implementation are deferred to Appendix G.\n\nDataset. We use the Electric1 dataset of residential electric load measurements. Each row of the matrix corresponds to a different residence. Matrix columns are consecutive measurements at different times. Here Ai ∈ R370×9, bi ∈ R370×1, and |(A, b)train| = 320, |(A, b)test| = 80. We set λ = 15. Experiment Setting. We compare the learned sketch against the classical Count-Sketch2. We choose m = 6d, 8d, 10d and consider the error f (x) − f (x∗). For the heavy-row Count-Sketch, we allocate 30% of the sketch space to the rows of the heavy row candidates. For this dataset, each row represents a specific residence and hence there is a strong pattern of the distribution of the heavy rows. We select the heavy rows according to the number of times each row is heavy in the training data. We give a detailed discussion about this in Appendix B.1. We highlight that it is still possible to recognize the pattern of the rows even if the row orders of the test data are permuted. We also consider optimizing the non-zero values after identifying the heavy rows, using our new approach in Section 6.\n\nResults. We plot in Figures 7.1 the mean errors on a logarithmic scale. The average offline training time is 3.67s to find a superset of the heavy rows over the training data and 66s to optimize the values when m = 10d, which are both faster than the runtime of Indyk et al. (2019) with the same parameters. Note that the learned matrix S is trained offline only once using the training data. Hence, no additional computational cost is incurred when solving the optimization problem on the test data.\n\nWe see all methods display linear convergence, that is, letting ek denote the error in the k-th iteration, we have ek ≈ ρke1 for some convergence rate ρ. A smaller convergence rate implies a faster convergence. We calculate an estimated rate of convergence ρ = (ek/e1)1/k with k = 7. We can see both sketches, especially the sketch that optimizes both the positions and values, show significant improvements. When the sketch size is small (6d), this sketch has a convergence rate that is just 13.2% of that of the classical Count-Sketch, and when the sketch size is large (10d), this sketch has a smaller convergence rate that is just 12.1%.\n\n1https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 2The framework of Indyk et al. (2019) does not apply to the iterative sketching methods in a straightforward\n\nmanner, so here we only compare with the classical CountSketch. For more details, please refer to Section B.\n\n9\n\n0.050.100.150.200.25runtime(seconds)864202log_10(error)learned(value-only)count-sketchlearned(position and value)0.050.100.150.200.250.30runtime(seconds)10864202log_10(error)learned(value-only)count-sketchlearned(position and value)0.050.100.150.200.250.30runtime(seconds)10864202log_10(error)learned(value-only)count-sketchlearned(position and value)Published as a conference paper at ICLR 2023\n\nACKNOWLEDGEMENTS\n\nYi Li would like to thank for the partial support from the Singapore Ministry of Education under Tier 1 grant RG75/21. Honghao Lin and David Woodruff were supported in part by an Office of Naval Research (ONR) grant N00014-18-1-2562. Ali Vakilian was supported by NSF award CCF-1934843.\n\nREFERENCES\n\nAnders Aamand, Piotr Indyk, and Ali Vakilian. (learned) frequency estimation algorithms under\n\nzipfian distribution. arXiv preprint arXiv:1908.05198, 2019.\n\nAkshay Agrawal, Brandon Amos, Shane T. Barratt, Stephen P. Boyd, Steven Diamond, and J. Zico Kolter. Differentiable convex optimization layers. In Advances in Neural Information Processing Systems, pp. 9558–9570, 2019.\n\nHaim Avron, Kenneth L. Clarkson, and David P. Woodruff. Sharper bounds for regularized data fitting. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, (APPROX/RANDOM), pp. 27:1–27:22, 2017.\n\nJean Bourgain, Sjoerd Dirksen, and Jelani Nelson. Toward a unified theory of sparse dimensionality\n\nreduction in Euclidean space. Geometric and Functional Analysis, pp. 1009–1088, 2015.\n\nMatthew Brand. Fast low-rank modifications of the thin singular value decomposition. Linear\n\nAlgebra and its Applications, 415.1, 2006.\n\nLung-Sheng Chien and Samuel Rodriguez Bernabeu. Fast singular value decomposition on gpu. NVIDIA presentation at GPU Technology Conference, 2019. URL https://developer. download.nvidia.com/video/gputechconf/gtc/2019/presentation/ s9226-fast-singular-value-decomposition-on-gpus-v2.pdf.\n\nKenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual symposium on Theory of computing (STOC), pp. 205–214, 2009.\n\nKenneth L Clarkson and David P Woodruff. Low-rank approximation and regression in input sparsity\n\ntime. Journal of the ACM (JACM), 63(6):54, 2017.\n\nEdith Cohen, Ofir Geri, and Rasmus Pagh. Composable sketches for functions of frequencies: Beyond the worst case. In International Conference on Machine Learning, pp. 2057–2067. PMLR, 2020.\n\nMichael B. Cohen, Cameron Musco, and Christopher Musco. Input sparsity time low-rank approximation via ridge leverage score sampling. In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, (SODA), pp. 1758–1777, 2017.\n\nGraham Cormode and Charlie Dickens. Iterative hessian sketch in input sparsity time. In Proceedings of 33rd Conference on Neural Information Processing Systems (NeurIPS), Vancouver, Canada, 2019.\n\nYihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning sublinear-time indexing for\n\nnearest neighbor search. In International Conference on Learning Representations, 2020.\n\nTalya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner. Learning-based support estimation in sublinear time. In International Conference on Learning Representations, 2021.\n\nChinmay Hegde, Aswin C. Sankaranarayanan, Wotao Yin, and Richard G. Baraniuk. Numax: A convex approach for learning near-isometric linear embeddings. In IEEE Transactions on Signal Processing, pp. 6109–6121, 2015.\n\nPiotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances in\n\nNeural Information Processing Systems, pp. 7400–7410, 2019.\n\nPiotr Indyk, Tal Wagner, and David Woodruff. Few-shot data-driven algorithms for low rank\n\napproximation. Advances in Neural Information Processing Systems, 34, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nTanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented data\n\nstream algorithms. In International Conference on Learning Representations, 2020.\n\nXiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pp. 91–100, 2013.\n\nJelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pp. 117–126, 2013.\n\nJelani Nelson and Huy L. Nguyên. Lower bounds for oblivious subspace embeddings. In Automata, Languages, and Programming - 41st International Colloquium (ICALP), pp. 883–894, 2014.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, and Trevor Killeen. Pytorch: An imperative style, high-performance deep learning library. 2019.\n\nMert Pilanci and Martin J. Wainwright. Iterative Hessian sketch: Fast and accurate solution approxi-\n\nmation for constrained least-squares. J. Mach. Learn. Res., 17:53:1–53:38, 2016.\n\nTamas Sarlos. Improved approximation algorithms for large matrices via random projections. In 47th\n\nAnnual IEEE Symposium on Foundations of Computer Science (FOCS), pp. 143–152, 2006.\n\nJan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized) In James R. Lee (ed.), 12th Innovations in Theoretical\n\nneural networks in near-linear time. Computer Science Conference, ITCS, volume 185, pp. 63:1–63:15, 2021.\n\nRoman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Yonina C. Eldar and Gitta Kutyniok (eds.), Compressed Sensing: Theory and Applications, pp. 210–268. Cambridge University Press, 2012. doi: 10.1017/CBO9780511794308.006.\n\nJingdong Wang, Ting Zhang, Nicu Sebe, and Heng Tao ShenWang. A survey on learning to hash. In\n\nIEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 769–790, 2017.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nA ADDITIONAL EXPERIMENTS: LOW-RANK APPROXIMATION\n\nThe details (data dimension, Ntrain, etc.) are presented in Table A.1.\n\nName Friends Logo Hyper\n\nType Image Image Image\n\nDimension 5760 × 1080 5760 × 1080 1024 × 768\n\nNtrain Ntest 400 400 400\n\n100 100 100\n\nTable A.1: Data set descriptions\n\nA.1 SENSITIVITY ANALYSIS OF ALGORITHM 4\n\nSketch Ours(inner product) l2 Sampling Only Ridge Randomly Grouping\n\nLogo 0.311 0.698 0.994 0.659\n\nFriends Hyper 1.232 0.470 1.293 0.935 4.155 1.493 2.070 1.069\n\nTable A.2: Sensitivity analysis for our approach (using Algorithm 1 from Indyk et al. (2019) with one sketch)\n\nIn this section we explore how sensitive the performance of our Algorithm 4 is to the ridge leverage score sampling and maximum inner product grouping process. We consider the following baselines:\n\n• l2 norm sampling: we sample the rows according to their squared length instead of doing\n\nridge leverage score sampling.\n\n• Only ridge leverage score sampling: the subspace spanned by only the sampled rows from\n\nridge leverage score sampling.\n\n• Randomly grouping: we put the sampled rows into different buckets as before, but randomly\n\ndivide the non-sampled rows into buckets.\n\nThe results are shown in Table A.2. Here we set k = 30, m = 60 as an example. To show the difference of the initialization method more clearly, we compare the error using the one-sided sketching Algorithm 1 and do not further optimize the non-zeros values. From the table we can see both that ridge leverage score sampling and the downstream grouping process are necessary, otherwise the error will be similar or even worse than that of the classical Count-Sketch.\n\nA.2 TOTAL VARIATION DISTANCE\n\nAs we have shown in Theorem 5.1, if the total variation distance between the row sampling probability distributions p and q is O(1), we have a worst-case guarantee of O(k log k + k/ε), which is strictly better than the Ω(k2) lower bound for the random CountSketch, even when its non-zero values have been optimized. We now study the total variation distance between the train and test matrix in our dataset. The result is shown in Figure A.1. From the figure we can see that for all the three dataset, the total variation distance is bounded by a constant, which suggests that the assumptions are reasonable for real-world data.\n\nFigure A.1: Total variation distance between train and test matrices. left: Logo, middle: friend, right: Hyper.\n\n12\n\n020406080100Test Matrix #0.00.20.40.60.81.0Total Variation DistanceTv distance020406080100Test Matrix #0.00.20.40.60.81.0Total Variation DistanceTv distance020406080Test Matrix #0.00.20.40.60.81.0Total Variation DistanceTv distancePublished as a conference paper at ICLR 2023\n\nA.3 LEARNING-BASED ALGORITHMS FOR LOW-RANK APPROXIMATION IN THE FEW-SHOT\n\nSETTING\n\nIn this section, we will give a brief explanation of the two algorithms proposed in Indyk et al. (2021). Both algorithms aim to optimize the non-zero values of a Count-Sketch matrix under fixed locations of the non-zero entries.\n\nOne-shot closed-form algorithm. Given a sparsity pattern of a Count-Sketch matrix S ∈ Rm×n, it partitions the rows of A into m blocks A(1), ..., A(m) as follows: let Ii = {j : Sij = 1}. The block A(i) ∈ R|Ii|×d is the sub-matrix of A that contains the rows whose indices are in Ii. The goal here is for each block A(i), to choose a (non-sparse) one-dimensional sketching vector si ∈ R|Ii|. The first approach is to set si to be the top left singular vector of A(i), which is the algorithm 1Shot2Vec. Another approach is to set si to be a left singular vector of A(i) chosen randomly and proportional to its squared singular value. The main advantage of the latter approach over the previous one is that it endows the algorithm with provable guarantees on the LRA error. The 1Shot2Vec algorithm combines both ways, obtaining the benefits of both approaches. The advantage of these two algorithms is that they extract a sketching matrix by an analytic computation, requiring neither GPU access nor auto-gradient functionality.\n\nFew-shot SGD algorithm. namely,\n\nIn this algorithm, the authors propose a new loss function for LRA,\n\nmin CS S\n\nE A∈Tr\n\n(cid:13) (cid:13)U ⊤\n\nk S⊤SU − I0\n\n(cid:13) 2\n(cid:13)\n\nF ,\n\nwhere A = U ΣV ⊤ is the SVD-decomposition of A and Uk ∈ Rn×k denotes the submatrix of U that contains its first k columns. I0 ∈ Rk×d denotes the result of augmenting the identity matrix of order k with d − k additional zero columns on the right. This loss function is motivated by the analysis of prior LRA algorithms that use random sketching matrices. It is faster to compute and differentiate than the previous empirical loss in Indyk et al. (2019). In the experiments the authors also show that this loss function can achieve a smaller error in a shorter amount of time, using a small number of randomly sampled training matrices, though the final error will be larger than that of the previous algorithm in Indyk et al. (2019) if we allow a longer training time and access to the whole training set Tr.\n\nA.4 EXPERIMENTS: LRA IN THE FEW-SHOT SETTING\n\nIn the rest of this section, we study the performance of our second approach in the few-shot learning setting. We first consider the case where we only have one training matrix randomly sampled from Tr. Here, we compare our method with the 1Shot2Vec method proposed in (Indyk et al., 2021) in the same setting (k = 10, m = 40) as in their empirical evaluation. The result is shown in Table A.3. Compared to 1Shot2Vec, our method reduces the error by around 50%, and has an even slightly faster runtime.\n\nIndyk et al. (2021) also proposed a FewShotSGD algorithm which further improves the non-zero values of the sketches after different initialization methods. We compare the performance of this approach for different initialization methods: in all initialization methods, we only use one training matrix and we use three training matrices for the FewShotSGD step. The results are shown in Table A.4. We report the minimum error of 50 iterations of the FewShotSGD because we aim to compare the computational efficiency for different methods. From the table we see that our approach plus the FewShotSGD method can achieve a much smaller error, with around a 50% improvement upon (Indyk et al., 2021). Moreover, even without further optimization by FewShotSGD, our initialization method for learning the non-zero locations in CountSketch obtains a smaller error than other methods (even when they are optimized with 1ShotSGD or FewShotSGD learning).\n\nB ADDITIONAL EXPERIMENTS: SECOND-ORDER OPTIMIZATION\n\nAs we mentioned in Section 8, despite the number of problems that learned sketches have been applied to, they have not been applied to convex optimization, or say, iterative sketching algorithms in general. To demonstrate the difficulty, we consider the Iterative Hessian Sketch (IHS) as an example. In that scheme, suppose that we have k iterations of the algorithm. Then we need k independent\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm\n\nClassical CS\n\n1shot2Vec\n\nOurs (inner product)\n\nDataset Logo Friends Hyper Logo Friends Hyper Logo Friends Hyper\n\nFew-shot Error 0.331 0.524 1.082 0.171 0.306 0.795 0.065 0.139 0.535\n\nTraining Time\n\n✗\n\n5.682 5.680 1.054 4.515 4.773 0.623\n\nTable A.3: Test errors and training times for LRA in the one-shot setting (using Alg. 1 with one sketch)\n\nSketch Logo Ours (Initialization only) 0.065 0.048 Ours + FewShotSGD 1Shot1Vec only 0.171 1Shot1Vec + FewShot SGD 0.104 0.331 Classical CS Classical CS + FewShot SGD 0.173\n\nFriends Hyper 0.535 0.139 0.443 0.125 0.795 0.306 0.636 0.229 1.082 0.524 0.771 0.279\n\nTable A.4: Test errors for LRA in the fewshot setting (using Alg. 1 from Indyk et al. (2019) with one sketch)\n\nsketching matrices (otherwise the solution may diverge). A natural way is to follow the method in (Indyk et al., 2019), which is to minimize the following quantity\n\nmin S1,...Sk\n\nE A∈D\n\nf (A, ALG(S1, ..., Sk, A)) ,\n\nwhere the minimization is taken over k Count-Sketch matrices S1, . . . , Sk. In this case, however, calculating the gradient with respect to S1 would involve all iterations and in each iteration we need to solve a constrained optimization problem. Hence, it would be difficult and intractable to compute the gradients. An alternative way is to train k sketching matrices sequentially, that is, learn the sketching matrix for the i-th iteration using a local loss function for the i-th iteration, and then using the learned matrix in the i-th iteration to generate the training data for the (i + 1)-st iteration. However, the empirical results suggest that it works for the first iteration only, because in this case the training data for the (i + 1)-st iteration depends on the solution to the i-th iteration and may become farther away from the test data in later iterations. The core problem here is that the method proposed in Indyk et al. (2019) treats the training process in a black-box way, which is difficult to extend to iterative methods.\n\nB.1 THE DISTRIBUTION OF THE HEAVY ROWS\n\nIn our experiments, we hypothesize that in real-world data there may be an underlying pattern which can help us identify the heavy rows. In the Electric dataset, each row of the matrix corresponds to a specific residence and the heavy rows are concentrated on some specific rows.\n\nTo exemplify this, we study the heavy leverage score rows distribution over the Electric dataset. For a row i ∈ [370], let fi denote the number of times that row i is heavy out of 320 training data points from the Electric dataset, where we say row i is heavy if li ≥ 5d/n. Below we list all 74 pairs (i, fi) with fi > 0.\n\n(195,320), (278,320), (361,320), (207,317), (227,285), (240,284), (219,270), (275,232), (156,214), (322,213), (193,196), (190,192), (160,191), (350,181), (63,176), (42,168), (162,148), (356,129), (363,110), (362,105), (338,95), (215,94), (234,93), (289,81), (97,80), (146,70), (102,67), (98,58), (48,57), (349,53), (165,46), (101,41), (352,40), (293,34), (344,29), (268,21), (206,20), (217,20), (327,20), (340,19), (230,18), (359,18), (297,14), (357,14), (161,13), (245,10), (100,8), (85,6), (212,6), (313,6), (129,5), (130,5), (366,5), (103,4), (204,4), (246,4), (306,4), (138,3), (199,3), (222,3), (360,3), (87,2), (154,2), (209,2), (123,1), (189,1), (208,1), (214,1), (221,1), (224,1), (228,1), (309,1), (337,1), (343,1)\n\nObserve that the heavy rows are concentrated on a set of specific row indices. There are only 30 rows i with fi ≥ 50. We view this as strong evidence for our hypothesis.\n\nHeavy Rows Distribution Under Permutation. We note that even though the order of the rows has been changed, we can still recognize the patterns of the rows. We continue to use the Electric dataset as an example. To address the concern that a permutation may break the sketch, we can measure the similarity between vectors, that is, after processing the training data, we can instead test similarity on the rows of the test matrix and use this to select the heavy rows, rather than an index which may simply be permuted. To illustrate this method, we use the following example on the Electric dataset, using locality sensitive hashing.\n\nAfter processing the training data, we obtain a set I of indices of heavy rows. For each i ∈ I, we pick q = 3 independent standard Gaussian vectors g1, g2, g3 ∈ Rd, and compute f (ri) =\n\n14\n\nPublished as a conference paper at ICLR 2023\n\n1 ri, gT\n\n2 ri, gT\n\n3 ri) ∈ R3, where ri takes an average of the i-th rows over all training sets. Let A (gT be the test matrix. For each i ∈ I, let ji = argminj ∥f (Aj) − f (ri)∥2. We take the ji-th row to be a heavy row in our learned sketch. This method only needs an additional O(1) passes over the entries of A and hence, the extra time cost is negligible. To test the performance of the method, we randomly pick a matrix from the test set and permute its rows. The result shows that when k is small, we can roughly recover 70% of the top-k heavy rows, and we plot below the regression error using the learned Count-Sketch matrix generated this way, where we set m = 90 and k = 0.3m = 27. We can see that the learned method still obtains a significant improvement.\n\nFigure B.1: Test error of LASSO on Electric dataset\n\nB.2 MATRIX NORM ESTIMATION WITH A NUCLEAR NORM CONSTRAINT\n\nIn many applications, for the problem\n\nX ∗ := arg min X∈Rd1×d2\n\n∥AX − B∥2\n\nF ,\n\nit is reasonable to model the matrix X ∗ as having low rank. Similar to l1-minimization for compressive sensing, a standard relaxation of the rank constraint is to minimize the nuclear norm of X, defined as ∥X∥∗ := (cid:80)min{d1,d2} Hence, the matrix estimation problem we consider here is\n\nσj(X), where σj(X) is the j-th largest singular value of X.\n\nj=1\n\nX ∗ := arg min X∈Rd1×d2\n\n∥AX − B∥2\n\nF\n\nsuch that\n\n∥X∥∗ ≤ ρ,\n\nwhere ρ > 0 is a user-defined radius as a regularization parameter.\n\nWe conduct Iterative Hessian Sketch (IHS) experiments on the following dataset:\n\n• Tunnel3: The data set is a time series of gas concentrations measured by eight sensors in a wind tunnel. Each (A, B) corresponds to a different data collection trial. Ai ∈ R13530×5, Bi ∈ R13530×6, |(A, B)|train = 144, |(A, B)|test = 36. In our nuclear norm constraint, we set ρ = 10.\n\nExperiment Setting: We choose m = 7d, 10d for the Tunnel dataset. We consider the error 2 ∥AX − B∥2 1\n2. The leverage scores of this dataset are very uniform. Hence, for this experiment we only consider optimizing the values of the non-zero entries.\n\n2 ∥AX ∗ − B∥2\n\n2 − 1\n\nResults of Our Experiments: We plot on a logarithmic scale the mean errors of the dataset in Figures B.2. We can see that when m = 7d, the gradient-based sketch, based on the first 6 iterations, has a rate of convergence that is 48% of the random sketch, and when m = 10d, the gradient-based sketch has a rate of convergence that is 29% of the random sketch.\n\nB.3 FAST REGRESSION SOLVER\n\nConsider an unconstrained convex optimization problem minx f (x), where f is smooth and strongly convex, and its Hessian ∇2f is Lipschitz continuous. This problem can be solved by Newton’s\n\n3https://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+exposed+to+\n\nturbulent+gas+mixtures\n\n15\n\n1234567iteration round864202log_10(error)count-sketchlearned(position-only)Published as a conference paper at ICLR 2023\n\nFigure B.2: Test error of matrix estimation with a nuclear norm constraint on the Tunnel dataset\n\nFigure B.3: Test error of the subroutine in fast regression on Electric dataset.\n\nFigure B.4: Test error of fast regression on Electric dataset\n\nmethod, which iteratively performs the update\n\nxt+1 = xt − arg min\n\nz\n\n(cid:13) (cid:13) (cid:13)(∇2f (xt)1/2)⊤(∇2f (xt)1/2)z − ∇f (xt) (cid:13) (cid:13) (cid:13)2\n\n,\n\n(B.1)\n\n(cid:13)A⊤Az − y(cid:13) (cid:13)\n\nprovided it is given a good initial point x0. In each step, it requires solving a regression problem of the form minz (cid:13)2, which, with access to A, can be solved with a fast regression solver in (van den Brand et al., 2021). The regression solver first computes a preconditioner R via a QR decomposition such that SAR has orthonormal columns, where S is a sketching matrix, then solves ˆz = arg minz′ (cid:13)2 by gradient descent and returns Rˆz in the end. Here, the point of sketching is that the QR decomposition of SA can be computed much more efficiently than the QR decomposition of A, since S has only a small number of rows.\n\n(cid:13)(AR)⊤(AR)z′ − y(cid:13) (cid:13)\n\n2 using the Electric dataset, using the above fast regression solver.\n\nIn this section, We consider the unconstrained least squares problem minx f (x) with f (x) = 2 ∥Ax − b∥2 1\nTraining: Note that ∇2f (x) = A⊤A, independent of x. In the t-th round of Newton’s method, by (B.1), we need to solve a regression problem minz 2 with y = ∇f (xt). Hence, we can use the same methods in the preceding subsection to optimize the learned sketch Si. For a general problem where ∇2f (x) depends on x, one can take xt to be the solution obtained from the learned sketch St to generate A and y for the (t + 1)-st round, train a learned sketch St+1, and repeat this process.\n\n(cid:13) (cid:13)A⊤Az − y(cid:13) 2\n(cid:13)\n\nExperiment Setting: For the Electric dataset, we set m = 10d = 90. We observe that the classical Count-Sketch matrix makes the solution diverge terribly in this setting. To make a clearer comparison, we consider the following sketch matrix:\n\n• Gaussian sketch: S = 1√\n\nm G, where G ∈ Rm×n with i.i.d. N (0, 1) entries.\n\n16\n\n246810iteration round543210123log_10(error)Ours(learned CS)Count-sketch123456789iteration round864202log_10(error)Ours(learned CS)Count-sketch1.001.251.501.752.002.252.502.753.00iteration round0.00.20.40.60.81.01.21.4errorGaussian(eta = 1)Gaussian(eta = 0.2)sparse-JL(eta = 1)sparse-JL(eta = 0.2)Ours(learned CS)1.001.251.501.752.002.252.502.753.00iteration round0.00.20.40.60.81.01.21.4errorGaussian(eta = 1)Gaussian(eta = 0.2)sparse-JL(eta = 1)sparse-JL(eta = 0.2)Ours(learned CS)1.001.251.501.752.002.252.502.753.00iteration round0.00.20.40.60.81.01.21.4errorGaussian(eta = 1)Gaussian(eta = 0.2)sparse-JL(eta = 1)sparse-JL(eta = 0.2)Ours(learned CS)2468101214iteration round432101234log_10(error)GaussianSparse-JLOurs(learned CS)Published as a conference paper at ICLR 2023\n\n• Sparse Johnson-Lindenstrauss Transform (SJLT): S is the vertical concatenation of s inde-\n\npendent Count-Sketch matrices, each of dimension m/s × n.\n\nWe note that the above sketching matrices require more time to compute SA but need fewer rows to be a subspace embedding than the classical Count-Sketch matrix.\n\nFor the step length η in gradient descent, we set η = 1 in all iterations of the learned sketches. For classical random sketches, we set η in the following two ways: (a) η = 1 in all iterations and (b) η = 1 in the first iteration and η = 0.2 in all subsequent iterations.\n\n(cid:13)A⊤ARzt − y(cid:13)\n\nExperimental Results: We examine the accuracy of the subproblem minz 2 and define the error to be (cid:13) (cid:13)2 / ∥y∥2. We consider the subproblems in the first three iterations of the global Newton method. The results are plotted in Figure B.3. Note that Count-Sketch causes a terrible divergence of the subroutine and is thus omitted in the plots. Still, we observe that in setting (a) of η, the other two classical sketches cause the subroutine to diverge. In setting (b) of η, the other two classical sketches lead to convergence but their error is significantly larger than that of the learned sketches, in each of the first three calls to the subroutine. The error of the learned sketch is less than 0.01 in all iterations of all three subroutine calls, in both settings (a) and (b) of η.\n\n(cid:13)A⊤Az − y(cid:13) (cid:13) 2\n(cid:13)\n\nWe also plot a figure on the convergence of the global Newton method. Here, for each subroutine, we only run one iteration, and plot the error of the original least squares problem. The result is shown in Figure B.4, which clearly displays a significantly faster decay with learned sketches. The rate of convergence using heavy-rows sketches is 80.6% of that using Gaussian or sparse JL sketches.\n\nB.4 FIRST-ORDER OPTIMIZATION\n\nIn this section, we study the use of the sketch in first-order methods. Particularly, let QR−1 = SA be the QR-decomposition for SA, where S is a sketch matrix. We use R as an (approximate) preconditioner and use gradient descent to solve the problem min ∥ARx − b∥2. Here we use the Electric dataset where A is 370 × 9 and we set S to have 90 rows. The result is shown in the following table, where the time includes the time to compute R. We can see that if we use a learned sketch matrix, the error converges very fast when we set the learning rate to be 1 and 0.1, while the classical Count-Sketch will lead to divergence. Iteration Error (learned, lr = 1) Error (learned, lr = 0.1) Error (learned, lr = 0.01) Error (random, lr = 1) Error (random, lr = 0.1) Error (random, lr = 0.01) Time\n\n10 1.5e-7 605 4085\n\n1 2.73 4056 4897\n\n4881 0.00048\n\n3790 0.00068\n\n4.04e-6 667\n\n1.52 0.0013\n\n685 0.0029\n\n0.217\n\nN.A\n\nN.A\n\nN.A\n\nN.A\n\n100\n\n500\n\nTable B.1: Test Error for Gradient Descent\n\nC PRELIMINARIES: THEOREMS AND ADDITIONAL ALGORITHMS\n\nIn this section, we provide the full description of the time-optimal sketching algorithm for LRA in Algorithm 2. We also provide several definitions and lemmas that are used in the proofs of our results for LRA. Definition C.1 (Affine Embedding). Given a pair of matrices A and B, a matrix S is an affine ε-embedding if for all X of the appropriate shape, ∥S(AX − B)∥2 Lemma C.2 (Clarkson & Woodruff (2017); Lemma 40). Let A be an n × d matrix and let S ∈ RO(1/ε2)×n be a CountSketch matrix. Then with constant probability, ∥SA∥2\n\nF = (1 ± ε) ∥AX − B∥2 F .\n\nF = (1 ± ε) ∥A∥2 F .\n\nThe following result is shown in Clarkson & Woodruff (2017) and sharpened with Nelson & Nguyên (2013); Meng & Mahoney (2013). Lemma C.3. Given matrices A, B with n rows, a CountSketch with O(rank(A)2/ε2) rows is an affine ε-embedding matrix with constant probability. Moreover, the matrix product SA can be computed in O(nnz(A)) time, where nnz(A) denotes the number of non-zero entries of matrix A.\n\n17\n\nPublished as a conference paper at ICLR 2023\n\nLemma C.4 (Sarlos (2006); Clarkson & Woodruff (2017)). Suppose that A ∈ Rn×d and B ∈ Rn×d′ .\nLet S ∈ Rm×n be a CountSketch with m = rank(A)2 . Let ̃X = arg minrank-k X ∥SAX − SB∥2 F . Then,\n\nε2\n\n1. With constant probability,\n\nF . In other words, in O(nnz(A) + nnz(B) + m(d + d′)) time, we can reduce the problem to a smaller (multi-response regression) problem with m rows whose optimal solution is a (1 + ε)-approximate solution to the original instance.\n\n≤ (1 + ε) minrank-k X ∥AX − B∥2\n\n(cid:13)A ̃X − B\n\nF\n\n(cid:13) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n2. The (1 + ε)-approximate solution ̃X can be computed in time O(nnz(A) + nnz(B) + mdd′ +\n\nmin(m2d, md2)).\n\nNow we turn our attention to the time-optimal sketching algorithm for LRA. The next lemma is known, though we include it for completeness Avron et al. (2017): Lemma C.5. Suppose that S ∈ RmS ×n and R ∈ RmR×d are sparse affine ε-embedding matrices for (Ak, A) and ((SA)⊤, A⊤), respectively. Then, (cid:13)AR⊤XSA − A(cid:13) (cid:13) 2\nF ≤ (1 + ε) ∥Ak − A∥2 (cid:13)\n\nF\n\nmin rank-k X\n\nProof. Consider the following multiple-response regression problem:\n\nmin rank-k X\n\n∥AkX − A∥2\n\nF .\n\n(C.1)\n\nNote that since X = Ik is a feasible solution to Eq. (C.1), minrank-k X ∥AkX − A∥2 F = ∥Ak − A∥2 F . Let S ∈ RmS ×n be a sketching matrix that satisfies the condition of Lemma C.4 (Item 1) for A := Ak and B := A. By the normal equations, the rank-k minimizer of ∥SAkX − SA∥2 F is (SAk)+SA. Hence,\n\n(cid:13)Ak(SAk)+SA − A(cid:13) (cid:13) 2\nF ≤ (1 + ε) ∥Ak − A∥2 (cid:13)\n\nF ,\n\n(C.2)\n\nwhich in particular shows that a (1 + ε)-approximate rank-k approximation of A exists in the row space of SA. In other words,\n\nmin rank-k X\n\n∥XSA − A∥2\n\nF ≤ (1 + ε) ∥Ak − A∥2\n\nF .\n\n(C.3)\n\nNext, let R ∈ RmR×d be a sketching matrix which satisfies the condition of Lemma C.4 (Item 1) for A := (SA)⊤ and B := A⊤. Let Y denote the rank-k minimizer of (cid:13) F . Hence,\n\n(cid:13)R(SA)⊤X ⊤ − RA⊤(cid:13) 2\n(cid:13)\n\n(cid:13)(SA)⊤Y ⊤ − A⊤(cid:13) (cid:13) 2\n(cid:13)\n\nF ≤ (1 + ε) min\n\nrank-k X\n\n∥XSA − A∥2\n\nF\n\n▷ Lemma C.4 (Item 1)\n\n≤ (1 + O(ε)) ∥Ak − A∥2\n\nF\n\n▷ Eq. (C.3)\n\n(C.4)\n\nNote that by the normal equations, again rowsp(Y ⊤) ⊆ rowsp(RA⊤) and we can write Y = AR⊤Z where rank(Z) = k. Thus,\n\nmin rank-k X\n\n(cid:13) (cid:13)AR⊤XSA − A(cid:13) 2\n(cid:13)\n\nF ≤ (cid:13)\n\n(cid:13)AR⊤ZSA − A(cid:13) 2\n(cid:13)\n\nF = (cid:13)\n\n(cid:13)(SA)⊤Y ⊤ − A⊤(cid:13) 2\n(cid:13)\n\nF\n\n▷ Y = AR⊤Z\n\n≤ (1 + O(ε)) ∥Ak − A∥2\n\nF\n\n▷ Eq. (C.4)\n\nLemma C.6 (Avron et al. (2017); Lemma 27). For C ∈ Rp×m′ following problem\n\n, D ∈ Rm×p′\n\n, G ∈ Rp×p′\n\n, the\n\nmin rank-k Z\n\n∥CZD − G∥2\n\nF\n\n(C.5)\n\ncan be solved in O(pm′rC + p′mrD + pp′(rD + rC)) time, where rC = rank(C) ≤ min{m′, p} and rD = rank(D) ≤ min{m, p′}.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nLemma C.7. Let S ∈ RmS ×d, R ∈ RmR×d be CountSketch (CS) matrices such that\n\nmin rank-k X\n\n(cid:13)AR⊤XSA − A(cid:13) (cid:13) 2\nF ≤ (1 + γ) ∥Ak − A∥2 (cid:13)\n\nF .\n\n(C.6)\n\nm2 R\n\nm2 S\n\nβ2 ×n, and W ∈ R\n\nLet V ∈ R approximation in time nnz(A) + O( m4 probability.\n\nβ2 ×d be CS matrices. Then, Algorithm 2 gives a (1 + O(β + γ))- + k(nmS + dmR)) with constant\n\nR(mS +mR)\n\nS m2\n\nβ2 + m4\n\nβ2 + m2\n\nβ4\n\nR\n\nS\n\nProof. The approximation guarantee follows from Eq. (C.6) and the fact that V and W are affine β-embedding matrices of AR⊤ and SA, respectively (see Lemma C.3).\n\nThe algorithm first computes C = V AR⊤, D = SAW ⊤, G = V AW ⊤ which can be done in time O(nnz(A)). As an example, we bound the time to compute C = V AR. Note that since V is a CS, V A can be computed in O(nnz(A)) time and the number of non-zero entries in the resulting matrix is at most nnz(A). Hence, since R is a CS as well, C can be computed in time O(nnz(A) + nnz(V A)) = O(nnz(A)). Then, it takes an extra O((m3 R)/β2) time to store C, D and G in matrix form. Next, as we showed in Lemma C.6, the time to compute Z in Algorithm 2 is O( m4 ). Finally, it takes O(nnz(A) + k(nmS + dmR)) time to compute Q = AR⊤ZL and P = ZRSA and to return the solution in the form of Pn×kQk×d. Hence, the total runtime is\n\nβ2 + m2\n\nβ2 + m4\n\nR + m2\n\nS + m3\n\nR(mS +mR)\n\nSm2\n\nS m2\n\nβ4\n\nR\n\nS\n\nO(nnz(A) +\n\nm4 S\nβ2 +\n\nm4 R\nβ2 +\n\nm2\n\nSm2\n\nR(mS + mR)\n\nβ4\n\n+ k(nmS + dmR))\n\nD ATTAINING WORST-CASE GUARANTEES\n\nD.1 LOW-RANK APPROXIMATION\n\nWe shall provide the following two methods to achieve worst case guarantees: MixedSketch—whose guarantee is via the sketch monotonicity property, and approximate comparison method (a.k.a. ApproxCheck), which just approximately evaluates the cost of two solutions and takes the better one. These methods asymptotically achieve the same worst-case guarantee. However, for any input matrix A and any pair of sketches S, T , the performance of the MixedSketch method on (A, S, T ) is never worse than the performance of its corresponding ApproxCheck method on (A, S, T ), and can be much better. Remark D.1. Let A = diag(2, 2, 2), and suppose the goal is to find a rank-2 approximation of A. Consider two sketches S and T such that SA and T A capture span(e1, e3) and span(e2, e4), respectively. Then for both SA and T A, the best solution in the subspace of one of these two spaces is a ( 3 F = 6 where PSA and PT A respectively denote the best approximation of A in the space spanned by SA and T A.\n\n2 )-approximation: ∥A − A2∥2\n\nF = 4 and ∥A − PSA∥2\n\nF = ∥A − PT A∥2\n\n√\n\n√\n\n2,\n\nHowever, if we find the best rank-2 approximation of A, Z, inside the span of the union of SA and T A, then ∥A − Z∥2 F = 4. Since ApproxCheck just chooses the better of SA and T A by evaluating their costs, it misses out on the opportunity to do as well as MixedSketch.\n\nHere, we show the sketch monotonicity property for LRA. Theorem D.2. Let A ∈ Rn×d be an input matrix, V and W be η-affine embeddings, and S1 ∈ RmS ×n, R1 ∈ RmR×n be arbitrary matrices. Consider arbitrary extensions to S1, R1: S, R (e.g., S is a concatenation of S1 with an arbitrary matrix with the same number of columns). Then, (cid:13)A − ALGLRA((S, R, V, W ), A))(cid:13) (cid:13) 2\nF ≤ (1 + η)2 ∥A − ALGLRA((S1, R1, V, W ), A)∥2 (cid:13)\n\nF\n\nProof. We have (cid:13) (cid:13)A − ALGLRA((S, R, V, W ), A)(cid:13) 2\n(cid:13) (1 + η) minrank-k X:X∈row(SA)∩col(AR) ∥X − A∥2 η) minrank-k X:X∈row(S1A)∩col(AR1) ∥X − A∥2\n\nF ≤ (1 + η) minrank-k X F , which\n\n(cid:13)ARXSA − A(cid:13) (cid:13) 2\nF = (cid:13) (1 + at most F = (1 + η) minrank-k X ∥AR1XS1A − A∥2\n\nF ≤ (1 +\n\nturn\n\nin\n\nis\n\n19\n\nPublished as a conference paper at ICLR 2023\n\nη)2 ∥A − ALGLRA((S1, R1, V, W ), A)∥2 (Definition C.1), as well as the fact that (col(AR1) ∩ row(S1A)) ⊆ (cid:0)col(AR) ∩ row(SA)(cid:1).\n\nF , where we use the fact the V, W are affine η-embeddings\n\nApproxCheck for LRA. We give the pseudocode for the ApproxCheck method and prove that the runtime of this method for LRA is of the same order as the classical time-optimal sketching algorithm of LRA. Algorithm 5 LRA APPROXCHECK Input: learned sketches SL, RL, VL, WL; classical sketches SC, RC, VC, WC; β; A ∈ Rn×d 1: PL, QL ← ALGLRA(SL, RL, VL, WL, A), PCQC ← ALGLRA(SC, RC, VC, WC, A) 2: Let S ∈ RO(1/β2)×n, R ∈ RO(1/β2)×d be classical CountSketch matrices F , ∆C ← (cid:13) 3: ∆L ← (cid:13) 4: if ∆L ≤ ∆C then 5: 6: end if 7: return PCQC\n\n(cid:13)S (PCQC − A) R⊤(cid:13) 2\n(cid:13)\n\n(cid:13)S (PLQL − A) R⊤(cid:13) 2\n(cid:13)\n\nreturn PLQL\n\nF\n\nε )×d, VL ∈ Rpoly( k\n\nTheorem D.3. Assume we have data A ∈ Rn×d, learned sketches SL ∈ Rpoly( k ε )×n, RL ∈ Rpoly( k ε )×d which attain a (1 + O(γ))-approximation, classical sketches of the same size, SC, RC, VC, WC, which attain a (1 + O(ε))-approximation, and a tradeoff parameter β. Then, Algorithm 5 attains a (1 + β + min(γ, ε))-approximation in O(nnz(A) + (n + d) poly( k ε )) time.\n\nε )×n, WL ∈ Rpoly( k\n\nβ4 · poly( k\n\nε ) + k4\n\nProof. Let (PL, QL), (PC, QC) be the approximate rank-k approximations of A in factored form using (SL, RL) and (SO, RO). Then, clearly, min(∥PLQL − A∥2\n\nF ) = (1 + O(min(ε, γ))) ∥Ak − A∥2\n\nF , ∥PCQC − A∥2\n\n(D.1)\n\nF\n\nLet ΓL = PLQL − A, ΓC = PCQC − A and ΓM = arg min(∥SΓLR∥F , ∥SΓCR∥F ). Then,\n\n∥ΓM ∥2\n\nF ≤ (1 + O(β)) ∥SΓM R∥2\n\nF\n\n≤ (1 + O(β)) · min(∥ΓL∥2 F ) ≤ (1 + O(β + min(ε, γ))) ∥Ak − A∥2\n\nF , ∥ΓC∥2\n\nF\n\n▷ by Lemma C.2\n\n▷ by Eq. (D.1)\n\nRuntime analysis. By Lemma C.7, Algorithm 2 computes PL, QL and PC, QC in time O(nnz(A) + k16(β2+ε2) β4 ) time\n\nε4 )). Next, once we have PL, QL and PC, QC, it takes O(nnz(A) + k\n\nε24β4 + k3\n\nε2 (n + dk2 to compute ∆L and ∆C.\n\nO(nnz(A) +\n\nk16(β2 + ε2) ε24β4\n\n+\n\nk3 ε2 (n +\n\ndk2 ε4 ) +\n\nk\n\nβ4 ) = O(nnz(A) + (n + d +\n\nk4 β4 ) poly(\n\nk ε\n\n)).\n\nTo interpret the above theorem, note that when ε ≫ k(n + d)−4, we can set β−4 = O(k(n + d)−4) so that Algorithm 5 has the same asymptotic runtime as the best (1 + ε)-approximation algorithm for LRA with the classical CountSketch. Moreover, Algorithm 5 is a (1 + o(ε))-approximation when the learned sketch outperforms classical sketches, γ = o(ε). On the other hand, when the learned sketches perform poorly, γ = Ω(ε), the worst-case guarantee of Algorithm 5 remains (1 + O(ε)).\n\nD.2 SECOND-ORDER OPTIMIZATION\n\nFor the sketches for second-order optimization, the monotonicity property does not hold. Below we provide an input-sparsity algorithm which can test for and use the better of a random sketch and a learned sketch. Our theorem is as follows. Theorem D.4. Let ε ∈ (0, 0.09) be a constant and S1 a learned Count-Sketch matrix. Suppose that A is of full rank. There is an algorithm whose output is a solution ˆx which, with probability at least 0.98, satisfies that ∥A(ˆx − x∗)∥2 ≤ O(cid:0) min (cid:8) Z2(S1) the least-squares solution. Furthermore, the algorithm runs in O(nnz(A) log( 1\n\nZ1(S1) , ε(cid:9)(cid:1) ∥Ax∗∥2, where x∗ = arg minx∈C ∥Ax − b∥2 is ε )) time.\n\nε ) + poly( d\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nAlgorithm 6 Solver for (D.2)\n\n1: S1 ← learned sketch, S2 ← random sketch with Θ(d2/ε2) rows 2: ( ˆZi,1, ˆZi,2) ← ESTIMATE(Si, A), i = 1, 2 3: i∗ ← arg mini=1,2( ˆZi,2/ ˆZi,1) 4: ˆx ← solution of (D.2) with S = Si∗ 5: return ˆx 6: function ESTIMATE(S, A) 7: T ← sparse (1 ± η)-subspace embedding matrix for d-dimensional subspaces 8: (Q, R) ← QR(T A) 9: ˆZ1 ← σmin(SAR−1) 10: ˆZ2 ← (1 ± η)-approximation to (cid:13) 11: return ( ˆZ1, ˆZ2)\n\n(cid:13)(SAR−1)⊤(SAR−1) − I(cid:13)\n\n(cid:13)op\n\nConsider the minimization problem\n\n(cid:26) 1 2\n\nmin x∈C\n\n∥SAx∥2\n\n2 − ⟨A⊤y, x⟩\n\n(cid:27)\n\n,\n\n(D.2)\n\nwhich is used as a subroutine for the IHS (cf. (2.2)). We note that in this subroutine if we let x ← x − xi−1, b ← b − Axi−1, C ← C − xi−1, we would get the guarantee of the i-th iteration of the original IHS. To analyze the performance of the learned sketch, we define the following quantities (corresponding exactly to the unconstrained case in (Pilanci & Wainwright, 2016))\n\nZ1(S) =\n\ninf v∈colsp(A)∩Sn−1\n\n∥Sv∥2 2 ,\n\nZ2(S) =\n\nsup u,v∈colsp(A)∩Sn−1\n\n(cid:10)u, (S⊤S − In)v(cid:11) .\n\nWhen S is a (1 + ε)-subspace embedding of colsp(A), we have Z1(S) ≥ 1 − ε and Z2(S) ≤ 2ε.\n\nFor a general sketching matrix S, the following is the approximation guarantee of ˆZ1 and ˆZ2, which are estimates of Z1(S) and Z2(S), respectively. The main idea is that AR−1 is well-conditioned, where R is as calculated in Algorithm 6. Lemma D.5. Suppose that η ∈ (0, 1 rows. The function ESTIMATE(S, A) returns in O((nnz(A) log 1 probability at least 0.99 satisfy that Z1(S)\n\n3 ) is a small constant, A is of full rank and S has poly(d/η) η )) time ˆZ1, ˆZ2 which with η + poly( d (1+η)2 − 3η ≤ ˆZ2 ≤ Z2(S)\n\n1+η ≤ ˆZ1 ≤ Z1(S)\n\n1−η and Z2(S)\n\n(1−η)2 + 3η.\n\nProof. Suppose that AR−1 = U W , where U ∈ Rn×d has orthonormal columns, which form an orthonormal basis of the column space of A. Since T is a subspace embedding of the column space of A with probability 0.99, it holds for all x ∈ Rd that\n\n1 1 + η\n\n(cid:13) (cid:13)T AR−1x(cid:13)\n\n(cid:13)2 ≤ (cid:13)\n\n(cid:13)AR−1x(cid:13)\n\n(cid:13)2 ≤\n\n1 1 − η\n\n(cid:13) (cid:13)T AR−1x(cid:13)\n\n(cid:13)2 .\n\nSince\n\nand\n\nwe have that\n\nIt is easy to see that\n\n(cid:13) (cid:13)T AR−1x(cid:13)\n\n(cid:13)2 = ∥Qx∥2 = ∥x∥2\n\n∥W x∥2 = ∥U W x∥2 = (cid:13)\n\n(cid:13)AR−1x(cid:13) (cid:13)2\n\n1 1 + η\n\n∥x∥2 ≤ ∥W x∥2 ≤\n\n1 1 − η\n\n∥x∥2 ,\n\nx ∈ Rd.\n\nZ1(S) = min\n\nx∈Sd−1\n\n∥SU x∥2 = min\n\ny̸=0\n\n∥SU W y∥2 ∥W y∥2\n\n,\n\nand thus,\n\n(1 − η)\n\nmin y̸=0\n\n∥SU W y∥2 ∥y∥2\n\n≤ Z1(S) ≤ min y̸=0\n\n(1 + η)\n\n∥SU W y∥2 ∥y∥2\n\n.\n\n21\n\n(D.3)\n\n(D.4)\n\nPublished as a conference paper at ICLR 2023\n\nRecall that SU W = SAR−1. We see that\n\n(1 − η)σmin(SAR−1) ≤ Z1(S) ≤ (1 + η)σmin(SAR−1).\n\nBy definition,\n\nIt follows from (D.4) that\n\nZ2(S) = (cid:13)\n\n(cid:13)U T (S⊤S − In)U (cid:13)\n\n(cid:13)op .\n\n(1 − η)2 (cid:13)\n\n(cid:13)W T U T (ST S − In)U W (cid:13)\n\n(cid:13)op ≤ Z2(S) ≤ (1 + η)2 (cid:13)\n\n(cid:13)W T U T (ST S − In)U W (cid:13)\n\n(cid:13)op .\n\nand from (D.4), (D.3) and Lemma 5.36 of Vershynin (2012) that (cid:13)(AR−1)⊤(AR−1) − I(cid:13) (cid:13)\n\n(cid:13)op ≤ 3η.\n\nSince\n\nand\n\nit follows that\n\n(cid:13)W T U T (ST S − In)U W (cid:13) (cid:13)\n\n(cid:13)op = (cid:13)\n\n(cid:13)(AR−1)⊤(ST S − In)AR−1(cid:13)\n\n(cid:13)op\n\n(cid:13)(AR−1)⊤ST SAR−1 − I(cid:13) (cid:13) (cid:13)(AR−1)⊤(ST S − In)AR−1(cid:13) (cid:13)(AR−1)⊤ST SAR−1 − I(cid:13)\n\n(cid:13)op − (cid:13) (cid:13)op (cid:13)op + (cid:13)\n\n≤ (cid:13) ≤ (cid:13)\n\n(cid:13)(AR−1)⊤(AR−1) − I(cid:13)\n\n(cid:13)op\n\n(cid:13)(AR−1)⊤(AR−1) − I(cid:13)\n\n(cid:13)op ,\n\n(1 − η)2 (cid:13)\n\n(cid:13)(SAR−1)⊤SAR−1 − I(cid:13)\n\n(cid:13)op − 3(1 − η)2η\n\n≤ Z2(S) ≤ (1 + η)2 (cid:13)\n\n(cid:13)(SAR−1)⊤SAR−1 − I(cid:13)\n\n(cid:13)op + 3(1 + η)2η.\n\nWe have so far proved the correctness of the approximation and we next analyze the runtime below.\n\nSince S and T are sparse, computing SA and T A takes O(nnz(A)) time. The QR decomposition of T A, which is a matrix of size poly(d/η) × d, can be computed in poly(d/η) time. The matrix SAR−1 can be computed in poly(d) time. Since it has size poly(d/η) × d, its smallest singular value can be computed in poly(d/η) time. To approximate Z2(S), we can use the power method to estimate (cid:13) (cid:13)op up to a (1 ± η)-factor in O((nnz(A) + poly(d/η)) log(1/η)) time.\n\n(cid:13)(SAR−1)T SAR−1 − I(cid:13)\n\nNow we are ready to prove Theorem D.4.\n\nProof of Theorem D.4. In Lemma D.5, we have with probability at least 0.99 that\n\nand similarly,\n\nˆZ2 ˆZ1\n\nˆZ2 ˆZ1\n\n≥\n\n≤\n\n1\n\n(1+ε)2 Z2(S) − 3ε 1−ε Z1(S)\n\n1\n\n1\n\n(1−ε)2 Z2(S) + 3ε 1+ε Z1(S)\n\n1\n\n≥\n\n1 − ε (1 + ε)2\n\nZ2(S) Z1(S)\n\n−\n\n3ε(1 − ε) Z1(S)\n\n.\n\n≤\n\n1 + ε (1 − ε)2\n\nZ2(S) Z1(S)\n\n+\n\n3ε(1 + ε) Z1(S)\n\n.\n\nNote that since S2 is an ε-subspace embedding with probability at least 0.99, we have that Z1(S2) ≥ 1 − ε and Z2(S2)/Z1(S2) ≤ 2.2ε. Consider Z1(S1).\n\nFirst, we consider the case where Z1(S1) < 1/2. Observe that Z2(S) ≥ 1 − Z1(S). We have in this case (cid:98)Z1,2/ (cid:98)Z1,1 > 1/5 ≥ 2.2ε ≥ Z2(S2)/Z1(S2). In this case our algorithm will choose S2 correctly.\n\nNext, assume that Z1(S1) ≥ 1/2. Now we have with probability at least 0.98 that\n\n(1 − 3ε)\n\nZ2(Si) Z1(Si)\n\n− 3ε ≤\n\n(cid:98)Zi,2 (cid:98)Zi,1\n\n≤ (1 + 4ε)\n\nZ2(Si) Z1(Si)\n\n+ 4ε,\n\ni = 1, 2.\n\n22\n\nPublished as a conference paper at ICLR 2023\n\nTherefore, when Z2(S1)/Z1(S1) ≤ c1Z2(S2)/Z1(S2) for some small absolute constant c1 > 0, we will have (cid:98)Z1,2/ (cid:98)Z1,1 < (cid:98)Z2,2/ (cid:98)Z2,1, and our algorithm will choose S1 correctly. If Z2(S1)/Z1(S1) ≥ C1ε for some absolute constant C1 > 0, our algorithm will choose S2 correctly. In the remaining case, both ratios Z2(S2)/Z1(S2) and Z2(S1)/Z1(S1) are at most max{C2, 3}ε, and the guarantee of the theorem holds automatically.\n\nThe correctness of our claim then follows from Proposition 1 of Pilanci & Wainwright (2016), together with the fact that S2 is a random subspace embedding. The runtime follows from Lemma D.5 and Theorem 2.2 of Cormode & Dickens (2019).\n\nE SKETCH LEARNING: OMITTED PROOFS\n\nE.1 PROOF OF THEOREM 5.1\n\nWe need the following lemmas for the ridge leverage score sampling in (Cohen et al., 2017). Lemma E.1 ((Cohen et al., 2017, Lemma 4)). Let λ = ∥A − Ak∥2 2k. Lemma E.2 ((Cohen et al., 2017, Theorem 7)). Let λ = ∥A − Ak∥2 overestimate to the i-th ridge leverage score of A. Let pi = ̃τi/ (cid:80) constructed by sampling t = O((log k + log(1/δ) pi, then with probability at least 1 − δ we have\n\nF /k and ̃τi ≥ τi(A) be an i ̃τi. If C is a matrix that is i ̃τi) rows of A, each set to ai with probability\n\nF /k. Then we have (cid:80)\n\ni τi(A) ≤\n\n) · (cid:80)\n\nε\n\nmin rank-k X:row(X)⊆row(C)\n\n∥A − X∥2\n\nF ≤ (1 + ε) ∥A − Ak∥2\n\nF .\n\nRecall that the sketch monotonicity for low-rank approximation says that concatenating two sketching matrices S1 and S2 will not increase the error compared to the single sketch matrix S1 or S2, Now matter how S1 and S2 are constructed. (see Section D.1 and Section 4 in (Indyk et al., 2019))\n\nProof. We first consider the first condition. From the condition that τi(B) ≥ 1 β τi(A) we know that if we sample m = O(β · (k log k + k/ε)) rows according to τi(A). The actual probability that the i-th row of B gets sampled is\n\n1 − (1 − τi(A))m = O(m · τi(A)) = O ((k log k + k/ε) · τi(B)) .\n\nFrom (cid:80) that with probability at least 9/10, S2 is a matrix such that\n\ni τi(B) ≤ 2k and Lemma E.2 (recall the sketch monotonicity property for LRA), we have\n\nmin rank-k X:row(X)⊆row(S2B)\n\n∥B − X∥2\n\nF ≤ (1 + ε) ∥B − Bk∥2\n\nF .\n\nHence, since S = [ S1 S2\n\n], from the the sketch monotonicity property for LRA we have that\n\nmin rank-k X:row(X)⊆row(SB)\n\n∥B − X∥2\n\nF ≤ (1 + ε) ∥B − Bk∥2\n\nF .\n\nNow we consider the second condition. Suppose that {Xi}i≤m and {Yi}i≤m are a sequence of m = O(k log k + k/ε) samples from [n] according to the sampling probability distribution p and q, where pi = τi(A) i τi(B) . Let S be the set of index i such that Xi ̸= Yi. From the property of the total variation distance, we get that\n\ni τi(A) and qi = τi(B)\n\n(cid:80)\n\n(cid:80)\n\nand\n\nPr [Xi ̸= Yi] ≤ dtv(p, q) = β ,\n\nE[|S|] =\n\n(cid:88)\n\ni\n\nPr [Xi ̸= Yi] ≤ βm.\n\nFrom Markov’s inequality we get that with probability at least 1 − 1.1β, |S| ≤ 1/(1.1β) · βm = 10 11 m. Let T be the set of index i such that Xi = Yi. We have that with probability at least 1 − 1.1β, |T | ≥ m − 10 11 m = Ω(k log k + k/ε). Because that {Yi}i∈T is i.i.d samples according to q and the\n\n23\n\nPublished as a conference paper at ICLR 2023\n\nactual sample we take is {Xi}i∈T . From Lemma E.2 we get that with probability at least 9/10, the row space of BT satisfies\n\nmin rank-k X:row(X)⊆row(BT )\n\n∥B − X∥2\n\nF ≤ (1 + ε) ∥B − Bk∥2\n\nF .\n\nSimilarly, from the the sketch monotonicity property we have that with probability at least 0.9 − 1.1β\n\nmin rank-k X:row(X)⊆row(SB)\n\nE.2 PROOF OF THEOREM 6.1\n\nFirst we prove the following lemma.\n\n∥B − X∥2\n\nF ≤ (1 + ε) ∥B − Bk∥2\n\nF .\n\nLemma E.3. Let δ ∈ (0, 1/m]. It holds with probability at least 1 − δ that\n\nsup x∈colsp(A)\n\nprovided that\n\n(cid:12) (cid:12)\n\n(cid:12)∥Sx∥2\n\n2 − ∥x∥2\n\n2\n\n(cid:12) (cid:12)\n\n(cid:12) ≤ ε ∥x∥2 2 ,\n\nm ≳ ε−2((d + log m) min{log2(d/ε), log2 m} + d log(1/δ)), 1 ≳ ε−2ν((log m) min{log2(d/ε), log2 m} + log(1/δ)) log(1/δ).\n\nProof. We shall adapt the proof of Theorem 5 in (Bourgain et al., 2015) to our setting. Let T denote the unit sphere in colsp(A) and set the sparsity parameter s = 1. Observe that ∥Sx∥2 2 = ∥xI ∥2\n\n2, and so it suffices to show that\n\n2 + ∥SxI c∥2\n\nPr\n\n(cid:110)(cid:12) (cid:12)\n\n(cid:12)∥S′xI c ∥2\n\n2 − ∥xI c∥2\n\n2\n\n(cid:111)\n\n(cid:12) (cid:12) (cid:12) > ε\n\n≤ δ\n\nfor x ∈ T . We make the following definition, as in (2.6) of (Bourgain et al., 2015):\n\nAδ,x :=\n\nm (cid:88)\n\n(cid:88)\n\ni=1\n\nj∈I c\n\nδijxjei ⊗ ej,\n\nand thus, S′xI c = Aδ,xσ. Also by E ∥S′xI c∥2 (cid:12) (cid:12)\n\n2 − ∥xI c∥2\n\n2\n\n(cid:12)∥S′xI c∥2\n\nsup x∈T\n\n(cid:12) (cid:12) (cid:12) = sup\n\nx∈T\n\n2 = ∥xI c∥2\n\n2, one has\n\n(cid:12) (cid:12)\n\n(cid:12)∥Aδ,xσ∥2\n\n2 − E ∥Aδ,xσ∥2\n\n2\n\n(cid:12) (cid:12) (cid:12) .\n\n(E.1)\n\nNow, in (2.7) of (Bourgain et al., 2015) we instead define a semi-norm\n\n∥x∥δ = max\n\n1≤i≤m\n\n\n\n1/2\n\nδijx2\n\nj\n\n\n\n.\n\n\n\n\n\n(cid:88)\n\nj∈I c\n\nThen (2.8) continues to hold, and (2.9) as well as (2.10) continue to hold if the supremum in the left-hand side is replaced with the left-hand side of (E.1). At the beginning of Theorem 5, we define U (i) to be U , but each row j ∈ I c is multiplied by δij and each row j ∈ I is zeroed out. Then we have in the first step of (4.5) that\n\n(cid:88)\n\nj∈I c\n\nδij\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nd (cid:88)\n\nk=1\n\n2\n\n(cid:12) (cid:12) (cid:12) gk⟨fk, ej⟩ (cid:12) (cid:12)\n\n≤\n\n(cid:13) (cid:13)U (i)g (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13) 2\n\n,\n\n√\n\ninstead of equality. One can verify that the rest of (4.5) goes through. It remains true that ∥·∥δ ≤ s) ∥·∥2, and thus (4.6) holds. One can verify that the rest of the proof of Theorem 5 in (Bourgain (1/ et al., 2015) continues to hold if we replace (cid:80)n j=1 with (cid:80) j∈I c and max1≤j≤n with maxj∈I c, noting that\n\n(cid:88)\n\nE\n\nj∈I c\n\nδij ∥PEej∥2\n\n2 =\n\n⟨PEej, ej⟩ ≤\n\ns m\n\nd\n\ns m\n\n(cid:88)\n\nj∈I c\n\n24\n\nPublished as a conference paper at ICLR 2023\n\nand\n\nE(U (i))∗U (i) =\n\n(E δij)uju∗\n\nj ⪯\n\n1 m\n\n.\n\n(cid:88)\n\nj∈I c\n\nThus, the symmetrization inequalities on\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nδij ∥PEej∥2\n\n2\n\nj∈I c\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Lp\n\nδ\n\nand\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:88)\n\nδijuju∗\n\nj\n\nj∈I c\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Lp\n\nδ\n\ncontinue to hold. The result then follows, observing that maxj∈I c ∥PEej∥2 ≤ ν.\n\nThe subspace embedding guarantee now follows as a corollary.\n\nTheorem 6.1. Let ν = ε/d. Suppose that m = Ω((d/ε2)(polylog(1/ε) + log(1/δ))), δ ∈ (0, 1/m) and d = Ω((1/ε) polylog(1/ε) log2(1/δ)). Then, there exists a distribution on S with m + |I| rows such that\n\n(cid:110)\n\nPr\n\n∀x ∈ colsp(A),\n\n(cid:12) (cid:12)\n\n(cid:12)∥Sx∥2\n\n2 − ∥x∥2\n\n2\n\n(cid:12) > ε ∥x∥2\n\n2\n\n(cid:12) (cid:12)\n\n(cid:111)\n\n≤ δ.\n\nProof. One can verify that the two conditions in Lemma E.3 are satisfied if\n\n(cid:18)\n\nm ≳ d ε2\n\npolylog(\n\nd ε\n\n) + log\n\n(cid:19)\n\n1 δ\n\nd ≳ 1\n\nε\n\n(cid:18)\n\nlog\n\n1 δ\n\n(cid:19) (cid:18)\n\npolylog(\n\nd ε\n\n) + log\n\n,\n\n1 δ\n\n(cid:19)\n\n.\n\nThe last condition is satisfied if\n\nd ≳ 1\n\nε\n\n(cid:19)\n\n(cid:18)\n\nlog2 1\n\nδ\n\npolylog\n\n(cid:19)\n\n.\n\n(cid:18) 1 ε\n\nE.3 PROOF OF LEMMA 6.2\n\nProof. On the one hand, since Q = SAR is an orthogonal matrix, we have\n\nOn the other hand, the assumption implies that\n\n∥x∥2 = ∥Qx∥2 = ∥SARx∥2 .\n\n(cid:13) (cid:13)(ARx)T (ARx) − xT x(cid:13)\n\n(cid:13)2 ≤ ε ∥x∥2 2 ,\n\nthat is,\n\n(1 − ε) ∥x∥2\n\n2 ≤ ∥ARx∥2\n\n2 ≤ (1 + ε) ∥x∥2 2 .\n\nCombining both (E.2) and (E.3) leads to\n\n√\n\n1 − ε ∥SARx∥2 ≤ ∥ARx∥2 ≤\n\n√\n\n1 + ε ∥SARx∥2 ,\n\nEquivalently, it can be written as\n\n√\n\n1\n\n1 + ε\n\n∥SAy∥2 ≤ ∥Ay∥2 ≤\n\n√\n\n1\n\n1 − ε\n\n∥SAy∥2 ,\n\n∀y ∈ Rd.\n\n(E.2)\n\n(E.3)\n\n∀x ∈ Rd.\n\nThe claimed result follows from the fact that 1/ ε ∈ (0,\n\n√\n\n].\n\n5−1 2\n\n√\n\n1 + ε ≥ 1 − ε and 1/\n\n√\n\n1 − ε ≤ 1 + ε whenever\n\n25\n\nPublished as a conference paper at ICLR 2023\n\nF LOCATION OPTIMIZATION IN COUNTSKETCH: GREEDY SEARCH\n\nWhile the position optimization idea is simple, one particularly interesting aspect is that it is provably better than a random placement in some scenarios (Theorem. F.1). Specifically, it is provably beneficial for LRA when inputs follow the spiked covariance model or Zipfian distributions, which are common for real data. Spiked covariance model. Every matrix A ∈ Rn×d from the distribution Asp(s, l) has s < k “heavy” rows Ar1 , · · · , Ars of norm l > 1. The indices of the heavy rows can be arbitrary, but must be the same for all members of Asp(s, l) and are unknown to the algorithm. The remaining (“light”) rows have unit norm. In other words, let R = {r1, . . . , rs}. For all rows Ai, i ∈ [n], Ai = l · vi if i ∈ R and Ai = vi otherwise, where vi is a uniformly random unit vector. Zipfian on squared row norms. Every A ∈ Rn×d ∼ Azipf has rows which are uniformly random and orthogonal. Each A has 2i+1 rows of squared norm n2/22i for i ∈ [1, . . . , O(log(n))]. We also assume that each row has the same squared norm for all members of Azipf .\n\nTheorem F.1. Consider a matrix A from either the spiked covariance model or a Zipfian distribution. Let SL denote a CountSketch constructed by Algorithm 3 that optimizes the positions of the non-zero values with respect to A. Let SC denote a CountSketch matrix. Then there is a fixed η > 0 such that, minrank-k X∈rowsp(SLA) ∥X − A∥2\n\nF ≤ (1 − η) minrank-k X∈rowsp(SC A) ∥X − A∥2\n\nF\n\nRemark F.2. Note that the above theorem implicitly provides an upper bound on the generalization error of the greedy placement method on the two distributions that we considered in this paper. More precisely, for each of these two distributions, if Π is learned via our greedy approach over a set of sampled training matrices, the solution returned by the sketching algorithm using Π over any (test) matrix A sampled from the distribution has error at most (1 − η) minrank-k X∈rowsp(SC A) ∥X − A∥2 F .\n\nA key structural property of the matrices from these two distributions that is crucial in our analysis is the ε-almost orthogonality of their rows (i.e., (normalized) pairwise inner products are at most ε). Hence, we can find a QR-factorization of the matrix of such vectors where the upper diagonal matrix R has diagonal entries close to 1 and entries above the diagonal are close to 0.\n\nTo state our result, we first provide an interpretation of the location optimization task as a selection of hash function for the rows of A. Note that left-multiplying A by CountSketch S ∈ Rm×n is equivalent to hashing the rows of A to m bins with coefficients in {±1}. The greedy algorithm proceeds through the rows of A (in some order) and decides which bin to hash to, denoting this by adding an entry to S. The intuition is that our greedy approach separates heavy-norm rows (which are important “directions” in the row space) into different bins.\n\nProof Sketch of Theorem F.1 The first step is to observe that in the greedy algorithm, when rows are examined according to a non-decreasing order of squared norms, the algorithm will isolate rows into their singleton bins until all bins are filled. In particular, this means that the heavy norm rows will all be isolated—e.g., for the spiked covariance model, Lemma F.8 presents the formal statement.\n\nNext, we show that none of the rows left to be processed (all light rows) will be assigned to the same bin as a heavy row. The main proof idea is to compare the cost of “colliding” with a heavy row to the cost of “avoiding” the heavy rows. This is the main place we use the properties of the aforementioned distributions and the fact that each heavy row is already mapped to a singleton bin. Overall, we show that at the end of the algorithm no light row will be assigned to the bins that contain heavy rows—the formal statement and proof for the spiked covariance model is in Lemma F.12.\n\nFinally, we can interpret the randomized construction of CountSketch as a “balls and bins” experiment. In particular, considering the heavy rows, we compute the expected number of bins (i.e., rows in SCA) that contain a heavy row. Note that the expected number of rows in SCA that do not contain any heavy row is k · (1 − 1 k−1 . Hence, the number of rows in SCA that contain a heavy row of A is at most k(1 − e− s k−1 ) heavy rows are not mapped to an isolated bin (i.e., they collide with some other heavy rows). Then, it is straightforward to show that the squared loss of the solution corresponding to SC is larger than the squared loss of the solution corresponding to SL, the CountSketch constructed by Algorithm 3—please see Lemma F.14 for the formal statement of its proof.\n\nk−1 ). Thus, at least s − k(1 − e− s\n\nk )s ≥ k · e− s\n\n26\n\nPublished as a conference paper at ICLR 2023\n\nPreliminaries and notation. Left-multiplying A by a CountSketch S ∈ Rm×n is equivalent to hashing the rows of A to m bins with coefficients in {±1}. The greedy algorithm proceeds through the rows of A (in some order) and decides which bin to hash to, which we can think of as adding an entry to S. We will denote the bins by bi and their summed contents by wi.\n\nF.1 SPIKED COVARIANCE MODEL WITH SPARSE LEFT SINGULAR VECTORS.\n\nTo recap, every matrix A ∈ Rn×d from the distribution Asp(s, l) has s < k “heavy” rows (Ar1 , · · · , Ars ) of norm l > 1. The indices of the heavy rows can be arbitrary, but must be the same for all members of the distribution and are unknown to the algorithm. The remaining rows (called “light” rows) have unit norm.\n\nIn other words: let R = {r1, . . . , rs}. For all rows Ai, i ∈ [n]:\n\nAi =\n\n(cid:26) l · vi vi\n\nif i ∈ R o.w.\n\nwhere vi is a uniformly random unit vector. We also assume that Sr, Sg ∈ Rk×n and that the greedy algorithm proceeds in a non-increasing row norm order.\n\nProof sketch. First, we show that the greedy algorithm using a non-increasing row norm ordering will isolate heavy rows (i.e., each is alone in a bin). Then, we conclude by showing that this yields a better k-rank approximation error when d is sufficiently large compared to n.\n\nWe begin with some preliminary observations that will be of use later.\n\nIt is well-known that a set of uniformly random vectors is ε-almost orthogonal (i.e., the magnitudes of their pairwise inner products are at most ε). Observation F.3. Let v1, · · · , vn ∈ Rd be a set of random unit vectors. Then with probability 1 − 1/ poly(n), we have |⟨vi, vj⟩| ≤ 2\n\n(cid:113) log n\n\nd , ∀ i < j ≤ n.\n\nWe define ε = 2\n\n(cid:113) log n d .\n\nObservation F.4. Let u1, · · · , ut be a set of vectors such that for each pair of i < j ≤ t, |⟨ ui\n\n∥ui∥ , uj\n\n∥uj ∥ ⟩| ≤ ε, and gi, · · · , gj ∈ {−1, 1}. Then, (cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13) 2\n\n∥ui∥2 ∥uj∥2 ≤\n\n2 − 2ε\n\n∥ui∥2\n\nt (cid:88)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\ngiui\n\ni<j≤t\n\n(cid:88)\n\ni=1\n\nt (cid:88)\n\ni=1\n\n≤\n\nt (cid:88)\n\ni=1\n\n∥ui∥2\n\n2 + 2ε\n\n(cid:88)\n\ni<j≤t\n\n∥ui∥2 ∥uj∥2\n\n(F.1)\n\nNext, a straightforward consequence of ε-almost orthogonality is that we can find a QR-factorization of the matrix of such vectors where R (an upper diagonal matrix) has diagonal entries close to 1 and entries above the diagonal are close to 0. Lemma F.5. Let u1, · · · , ut ∈ Rd be a set of unit vectors such that for any pair of i < j ≤ t, |⟨ui, uj⟩| ≤ ε where ε = O(t−2). There exists an orthonormal basis e1, · · · , et for the subspace spanned by u1, · · · , ut such that for each i ≤ t, ui = (cid:80)i j=1 j2 · ε2 and for each j < i, a2\n\nj=1 ai,jej where a2\n\ni,i ≥ 1 − (cid:80)i−1\n\ni,j ≤ j2ε2.\n\nProof. We follow the Gram-Schmidt process to construct the orthonormal basis e1, · · · , et of the space spanned by u1, · · · , ut, by first setting e1 = u1 and then processing u2, · · · , ut, one-by-one.\n\nThe proof is by induction. We show that once the first j vectors u1, · · · , uj are processed, the statement of the lemma holds for these vectors. Note that the base case of the induction trivially holds as u1 = e1. Next, suppose that the induction hypothesis holds for the first l vectors u1, · · · , ul. Claim F.6. For each j ≤ l, a2\n\nl+1,j ≤ j2ε2.\n\nProof. The proof of the claim is itself by induction. Note that, for j = 1 and using the fact that l+1,1 ≤ ε2. Next, suppose that the statement holds for all |⟨u1, ul+1⟩| ≤ ε, the statement holds and a2\n\n27\n\nPublished as a conference paper at ICLR 2023\n\nj ≤ i < l. Then using that |⟨ui+1, ul+1⟩| ≤ ε,\n\n|al+1,i+1| ≤ (|⟨ul+1, ui+1| +\n\ni (cid:88)\n\nj=1\n\n|al+1,j| · |ai+1,j|)/|ai+1,i+1|\n\n≤ (ε +\n\n≤ (ε +\n\n≤ (ε +\n\n≤ (ε +\n\ni (cid:88)\n\nj=1\n\ni (cid:88)\n\nj=1\n\ni (cid:88)\n\nj=1\n\ni (cid:88)\n\nj=1\n\nj2ε2)/|ai+1,i+1| ▷ by the induction hypothesis on al+1,j for j ≤ i\n\nj2ε2)/(1 −\n\ni (cid:88)\n\nj2 · ε2)1/2 ▷ by the induction hypothesis on ai+1,i+1\n\nj2ε2) · (1 −\n\nj=1\n\ni (cid:88)\n\nj=1\n\nj2 · ε2)1/2 · (1 + 2 ·\n\ni (cid:88)\n\nj=1\n\nj2ε2)\n\nj2ε2) · (1 + 2 ·\n\ni (cid:88)\n\nj=1\n\nj2ε2)\n\n≤ ε((\n\ni (cid:88)\n\nj=1\n\nj2ε) · (1 + 4ε ·\n\ni (cid:88)\n\nj=1\n\nj2ε) + 1)\n\n≤ ε(i + 1) ▷ by ε = O(t−2)\n\nFinally, since ∥ul+1∥2\n\n2 = 1, a2\n\nl+1,l+1 ≥ 1 − (cid:80)l\n\nj=1 j2ε2.\n\nCorollary F.7. Suppose that ε = O(t−2). There exists an orthonormal basis e1, · · · , et for the space spanned by the randomly picked vectors v1, · · · , vt, of unit norm, so that for each i, vi = (cid:80)i j=1 ai,jej where a2\n\nj=1 j2 · ε2 and for each j < i, a2\n\ni,i ≥ 1 − (cid:80)i−1\n\ni,j ≤ j2 · ε2.\n\nProof. The proof follows from Lemma F.5 and the fact that the set of vectors v1, · · · , vt is ε-almost orthogonal (by Observation F.3).\n\nThe first main step is to show that the greedy algorithm (with non-increasing row norm ordering) will isolate rows into their own bins until all bins are filled. In particular, this means that the heavy rows (the first to be processed) will all be isolated.\n\nWe note that because we set rank(SA) = k, the k-rank approximation cost is the simplified expression (cid:13) F . This is just the projection cost onto row(SA). Also, we observe that minimizing this projection cost is the same as maximizing the sum of squared projection coefficients:\n\nF , where U ΣV ⊤ = SA, rather than (cid:13)\n\n(cid:13)[AV ]kV ⊤ − A(cid:13) 2\n(cid:13)\n\n(cid:13)AV V ⊤ − A(cid:13) 2\n(cid:13)\n\n(cid:88)\n\n∥Ai − (⟨Ai, v1⟩v1 + . . . + ⟨Ai, vk⟩vk)∥2\n\n2\n\narg min S\n\n(cid:13) (cid:13)A − AV V ⊤(cid:13) 2\n(cid:13)\n\nF = arg min\n\nS\n\n= arg min\n\nS\n\n⟨Ai, vj⟩2)\n\n(cid:88)\n\nj∈[k]\n\ni∈[n] (cid:88)\n\n(∥Ai∥2\n\n2 −\n\ni∈[n] (cid:88)\n\n(cid:88)\n\n⟨Ai, vj⟩2\n\ni∈[n]\n\nj∈[k]\n\n= arg max\n\nS\n\nIn the following sections, we will prove that our greedy algorithm makes certain choices by showing that these choices maximize the sum of squared projection coefficients.\n\nLemma F.8. For any matrix A or batch of matrices A, at the end of iteration k, the learned CountSketch matrix S maps each row to an isolated bin. In particular, heavy rows are mapped to isolated bins.\n\n28\n\nPublished as a conference paper at ICLR 2023\n\nProof. For any iteration i ≤ k, we consider the choice of assigning Ai to an empty bin versus an occupied bin. Without loss of generality, let this occupied bin be bi−1, which already contains Ai−1.\n\nWe consider the difference in cost for empty versus occupied. We will do this cost comparison for Aj with j ≤ i − 2, j ≥ i + 1, and finally, j ∈ {i − 1, i}.\n\nFirst, we let {e1, . . . , ei} be an orthonormal basis for {A1, . . . , Ai} such that for each r ≤ i, Ar = (cid:80)r j=1 ar,jej where ar,r > 0. This exists by Lemma F.5. Let {e1, . . . , ei−2, e} be an orthonormal basis for {A1, . . . , Ai+2, Ai−1 ± Ai}. Now, e = c0ei−1 + c1ei for some c0, c1 because (Ai−1 ± Ai) − proj{e1,...,ei−2}(Ai−1 ± Ai) ∈ span(ei−1, ei). We note that c2 1 = 1 because we let e be a unit vector. We can find c0, c1 to be:\n\n0 + c2\n\nc0 =\n\n(cid:113)\n\nai−1,i−1 + ai,i−1\n\n(ai−1,i−1 + ai,i−1)2 + a2\n\ni,i\n\n,\n\nc1 =\n\n(cid:113)\n\nai,i\n\n(ai−1,i−1 + ai,i−1)2 + a2\n\ni,i\n\n1. j ≤ i − 2: The cost is zero for both cases because Aj ∈ span({e1, . . . , ei−2}).\n\n2. j ≥ i + 1: We compare the rewards (sum of squared projection coefficients) and find that\n\n{e1, . . . , ei−2, e} is no better than {e1, . . . , ei}. ⟨Aj, e⟩2 = (c0⟨Aj, ei−1⟩ + c1⟨Aj, ei⟩)2\n\n0)(⟨Aj, ei−1⟩2 + ⟨Aj, ei⟩2)\n\n1 + c2\n\n≤ (c2 = ⟨Aj, ei−1⟩2 + ⟨Aj, ei⟩2\n\n▷ Cauchy-Schwarz inequality\n\n3. j ∈ {i − 1, i}: We compute the sum of squared projection coefficients of Ai−1 and Ai onto e:\n\n(\n\n1 (ai−1,i−1 + ai,i−1)2 + a2\n\ni,i\n\n) · (a2\n\ni−1,i−1(ai−1,i−1 + ai,i−1)2 + (ai,i−1(ai−1,i−1 + ai,i−1) + ai,iai,i)2)\n\n= (\n\n1 (ai−1,i−1 + ai,i−1)2 + a2\n\ni,i\n\n) · ((ai−1,i−1 + ai,i−1)2(a2\n\ni−1,i−1 + a2\n\ni,i−1)\n\n+ a4\n\ni,i + 2ai,i−1a2\n\ni,i(ai−1,i−1 + ai,i−1))\n\n(F.2)\n\nOn the other hand, the sum of squared projection coefficients of Ai−1 and Ai onto ei−1 ∪ ei is:\n\n(\n\n(ai−1,i−1 + ai,i−1)2 + a2 (ai−1,i−1 + ai,i−1)2 + a2\n\ni,i\n\ni,i\n\n) · (a2\n\ni−1,i−1 + a2\n\ni,i−1 + a2\n\ni,i)\n\n(F.3)\n\nHence, the difference between the sum of squared projections of Ai−1 and Ai onto e and ei−1 ∪ ei is ((F.3) - (F.2))\n\ni,i((ai−1,i−1 + ai,i−1)2 + a2 a2\n\ni−1,i−1 + a2\n\ni,i−1 − 2ai,i−1(ai−1,i−1 + ai,i−1))\n\n(ai−1,i−1 + ai,i−1)2 + a2\n\ni,i\n\n=\n\n2a2\n\ni,ia2 (ai−1,i−1 + ai,i−1)2 + a2\n\ni−1,i−1\n\ni,i\n\n> 0\n\nThus, we find that {e1, . . . , ei} is a strictly better basis than {e1, . . . , ei−2, e}. This means the greedy algorithm will choose to place Ai in an empty bin.\n\nNext, we show that none of the rows left to be processed (all light rows) will be assigned to the same bin as a heavy row. The main proof idea is to compare the cost of “colliding” with a heavy row to the cost of “avoiding” the heavy rows. Specifically, we compare the decrease (before and after bin assignment of a light row) in sum of squared projection coefficients, lower-bounding it in the former case and upper-bounding it in the latter.\n\nWe introduce some results that will be used in Lemma F.12. Claim F.9. Let Ak+r, r ∈ [1, . . . , n − k] be a light row not yet processed by the greedy algorithm. Let {e1, . . . , ek} be the Gram-Schmidt basis for the current {w1, . . . , wk}. Let β = O(n−1k−3) upper bound the inner products of the normalized Ak+r, w1, . . . , wk. Then, for any bin i, ⟨ei, Ak+r⟩2 ≤ β2 · k2.\n\n29\n\nPublished as a conference paper at ICLR 2023\n\nProof. This is a straightforward application of Lemma F.5. From that, we have ⟨Ak+r, ei⟩2 ≤ i2β2, for i ∈ [1, . . . , k], which means ⟨Ak+r, ei⟩2 ≤ k2β2.\n\nClaim F.10. Let Ak+r be a light row that has been processed by the greedy algorithm. Let If Ak+r is assigned to {e1, . . . , ek} be the Gram-Schmidt basis for the current {w1, . . . , wk}. bin bk−1 (w.l.o.g.), the squared projection coefficient of Ak+r onto ei, i ̸= k − 1 is at most 4β2 · k2, where β = O(n−1k−3) upper bounds the inner products of normalized Ak+r, w1, · · · , wk.\n\nProof. Without loss of generality, it suffices to bound the squared projection of Ak+r onto the direction of wk that is orthogonal to the subspace spanned by w1, · · · , wk−1. Let e1, · · · , ek be an orthonormal basis of w1, · · · , wk guaranteed by Lemma F.5. Next, we expand the orthonormal basis to include ek+1 so that we can write the normalized vector of Ak+r as vk+r = (cid:80)k+1 j=1 bjej. By a similar approach to the proof of Lemma F.5, for each j ≤ k − 2, bj ≤ β2j2. Next, since |⟨wk, vk+r⟩| ≤ β,\n\n|bk| ≤\n\n1 |⟨wk, ek⟩|\n\n· (|⟨wk, vk+r⟩| +\n\n≤\n\n(cid:113)\n\n1\n\n1 − (cid:80)k−1\n\nj=1 β2 · j2\n\n· (β +\n\nk−1 (cid:88)\n\nj=1\n\nk−2 (cid:88)\n\nj=1\n\n|bj · ⟨wk, ej⟩|)\n\nβ2 · j2 + (k − 1) · β) ▷ |bk−1| ≤ 1\n\n=\n\nj=1 β2 · j2\n\nβ + (cid:80)k−2 (cid:113)\n\n1 − (cid:80)k−1\n\nj=1 β2 · j2\n\n+ (k − 1)β\n\nβ2(k − 1)2\n\n(cid:113)\n\n1 − (cid:80)k−1\n\nj=1 β2 · j2\n\n≤ 2(k − 1)β −\n\n< 2β · k\n\n▷ similar to the proof of Lemma F.5\n\nHence, the squared projection of Ak+r onto ek is at most 4β2 · k2 · ∥Ak+r∥2 1; hence, the squared projection of Ak+r onto ek is at most 4β2 · k2.\n\n2. We assumed ∥Ak+r∥ =\n\nClaim F.11. We assume that the absolute values of the inner products of vectors in v1, · · · , vn are at most ε < 1/(n2 (cid:80) Ai∈b ∥Ai∥2) and the absolute values of the inner products of the normalized vectors of w1, · · · , wk are at most β = O(n−3k− 3 2 ). Suppose that bin b contains the row Ak+r. Then, the squared projection of Ak+r onto the direction of w orthogonal to span({w1, · · · , wk} \\ {w}) is at most ∥Ak+r∥4\n\n+ O(n−2) and is at least ∥Ak+r∥4\n\n− O(n−2).\n\n2\n\n2\n\n∥w∥2 2\n\n∥w∥2 2\n\nProof. Without loss of generality, we assume that Ak+r is mapped to bk; w = wk. First, we provide an upper and a lower bound for |⟨vk+r, wk⟩| where for each i ≤ k, we let wi = wi denote the normalized vector of wi. Recall that by definition vk+r = Ak+r ∥Ak+r∥2 + (cid:80)\n\nε ∥Ai∥2\n\n∥Ak+r∥2\n\n∥wi∥2\n\n.\n\nAi∈bk\n\n|⟨wk, vk+r⟩| ≤\n\n≤\n\n≤\n\n|⟨wk, vk+r⟩| ≥\n\n∥wk∥2 ∥Ak+r∥2 + n−2 ∥wk∥2\n\n∥Ak+r∥2 ∥wk∥2\n\n+ n−2\n\n∥Ak+r∥2 − (cid:80)\n\nAi∈bk\n\n∥Ai∥2 · ε\n\n≥\n\n∥Ak+r∥2 ∥wk∥2\n\n∥wk∥2\n\n− n−2\n\n30\n\n▷ by ε <\n\nn−2\n\n(cid:80)\n\nAi∈bk\n\n∥Ai∥2\n\n▷ ∥wk∥2 ≥ 1\n\n(F.4)\n\n(F.5)\n\nPublished as a conference paper at ICLR 2023\n\nNow, let {e1, · · · , ek} be an orthonormal basis for the subspace spanned by {w1, · · · , wk} guaranteed by Lemma F.5. Next, we expand the orthonormal basis to include ek+1 so that we can write vk+r = (cid:80)k+1 j=1 bjej. By a similar approach to the proof of Lemma F.5, we can show that for each j ≤ k − 1, b2\n\nj ≤ β2j2. Moreover,\n\n|bk| ≤\n\n1 |⟨wk, ek⟩|\n\n· (|⟨wk, vk+r⟩| +\n\nk−1 (cid:88)\n\nj=1\n\n|bj · ⟨wk, ej⟩|)\n\n· (|⟨wk, vk+r⟩| +\n\nk−1 (cid:88)\n\nj=1\n\nβ2 · j2)\n\n▷ by Lemma F.5\n\n≤\n\n(cid:113)\n\n≤\n\n(cid:113)\n\n1\n\n1 − (cid:80)k−1\n\nj=1 β2 · j2\n\n1\n\n1 − (cid:80)k−1\n\nj=1 β2 · j2 1\n(cid:112)1 − β2k3\n\n< β · k +\n\n· (n−2 +\n\n∥Ak+r∥2 ∥wk∥2\n\n+\n\nk−1 (cid:88)\n\nj=1\n\n· (n−2 +\n\n∥Ak+r∥2 ∥wk∥2\n\n)\n\nβ2 · j2) ▷ by (F.4)\n\n▷ similar to the proof of Lemma F.5\n\n▷ by β = O(n−3k− 3\n\n2 )\n\n▷ ∥Ak+r∥2 ∥wk∥2\n\n≤ 1\n\n≤ O(n−2) + (1 + O(n−2))\n\n∥Ak+r∥2 ∥wk∥2\n\n≤\n\n∥Ak+r∥2 ∥wk∥2\n\n+ O(n−2)\n\nand,\n\n|bk| ≥\n\n1 |⟨wk, ek⟩|\n\n· (|⟨wk, vk+r⟩| −\n\nk−1 (cid:88)\n\nj=1\n\n|bj · ⟨wk, ej⟩|)\n\n≥ |⟨wk, vk+r⟩| −\n\nk−1 (cid:88)\n\nj=1\n\nβ2 · j2\n\n≥\n\n≥\n\n∥Ak+r∥2 ∥wk∥2\n\n∥Ak+r∥2 ∥wk∥2\n\n− n−2 −\n\nk−1 (cid:88)\n\nj=1\n\nβ2 · j2\n\n− O(n−2)\n\n▷ since |⟨wk, ek⟩| ≤ 1\n\n▷ by (F.5)\n\n▷ by β = O(n−3k− 3\n\n2 )\n\nHence, the squared projection of Ak+r onto ek is at most ∥Ak+r∥4 O(n−2).\n\n∥wk∥2\n\n2\n\n2\n\n+O(n−2) and is at least ∥Ak+r∥4 ∥wk∥2\n\n2\n\n2\n\n−\n\nNow, we show that at the end of the algorithm no light row will be assigned to the bins that contain heavy rows.\n\nLemma F.12. We assume that the absolute values of the inner products of vectors in v1, · · · , vn are at most ε < min{n−2k− 5 Ai∈w ∥Ai∥2)−1}. At iteration k + r, the greedy algorithm will assign the light row Ak+r to a bin that does not contain a heavy row.\n\n3 , (n (cid:80)\n\nProof. The proof is by induction. Lemma F.8 implies that no light row has been mapped to a bin that contains a heavy row for the first k iterations. Next, we assume that this holds for the first k + r − 1 iterations and show that is also must hold for the (k + r)-th iteration.\n\nTo this end, we compare the sum of squared projection coefficients when Ak+r avoids and collides with a heavy row.\n\n31\n\nPublished as a conference paper at ICLR 2023\n\nFirst, we upper bound β = maxi̸=j≤k |⟨wi, wj⟩|/(∥wi∥2 ∥wj∥2). Let ci and cj respectively denote the number of rows assigned to bi and bj.\n\nβ = max i̸=j≤k\n\n|⟨wi, wj⟩| ∥wi∥2 ∥wj∥2\n\n≤\n\nci · cj · ε (cid:113)\n\n(cid:112)ci − 2εc2 i ·\n\ncj − 2εc2\n\nj\n\n√\n\n≤ 16ε cicj ≤ n−1k− 5\n\n3\n\n▷ Observation F.4\n\n▷ε ≤ n−2k−5/3\n\n▷ε ≤ n−2k−5/3\n\n1. If Ak+r is assigned to a bin that contains c light rows and no heavy rows. In this case, the projection loss of the heavy rows A1, · · · , As onto row(SA) remains zero. Thus, we only need to bound the change in the sum of squared projection coefficients of the light rows before and after iteration k + r. Without loss of generality, let wk denote the bin that contains Ak+r. Since Sk−1 = span({w1, · · · , wk−1}) has not changed, we only need to bound the difference in cost between projecting onto the component of wk − Ak+r orthogonal to Sk−1 and the component of wk orthogonal to Sk−1, respectively denoted as ek and ek.\n\n1. By Claim F.9, for the light rows that are not yet processed (i.e., Aj for j > k + r), the squared projection of each onto ek is at most β2k2. Hence, the total decrease in the squared projection is at most (n − k − r) · β2k2.\n\n2. By Claim F.10, for the processed light rows that are not mapped to the last bin, the squared projection of each onto ek is at most 4β2k2. Hence, the total decrease in the squared projection cost is at most (r − 1) · 4β2k2.\n\n3. For each row Ai ̸= Ak+r that is mapped to the last bin, by Claim F.11 and the fact ∥Ai∥4\n\n2 = 1, the squared projection of Ai onto ek is at most\n\n∥Ai∥2 squared projection of Ai onto ek is at least ∥Ai∥2\n\n− O(n−2).\n\n2\n\n∥wk∥2\n\n2\n\n∥Ai∥2 ∥wk−Ak+r∥2\n\n2\n\n2\n\n2 = + O(n−2) and the\n\nMoreover, the squared projection of Ak+r onto ek compared to ek increases by at least ( ∥Ak+r∥2 O(n−2)) − O(n−2) = ∥Ak+r∥2 ∥wk∥2\n\n− O(n−2).\n\n∥wk∥2\n\n2\n\n2\n\n2\n\n2\n\n−\n\nHence, the total squared projection of the rows in the bin bk decreases by at least:\n\n(cid:88)\n\n(\n\nAi∈wk/{Ar+k}\n\n∥Ai∥2 ∥wk − Ar+k∥2\n\n2\n\n2\n\n+ O(n−2)) − (\n\n(cid:88)\n\nAi∈wk\n\n∥Ai∥2 ∥wk∥2\n\n2\n\n2\n\n− O(n−2))\n\n≤\n\n∥wk − Ar+k∥2\n\n2 + O(n−1)\n\n∥wk − Ar+k∥2\n\n2\n\n−\n\n≤O(n−1)\n\n∥wk∥2\n\n2 − O(n−1) ∥wk∥2\n\n2\n\n+ O(n−1)\n\n▷ by Observation F.4\n\nHence, summing up the bounds in items 1 to 3 above, the total decrease in the sum of squared projection coefficients is at most O(n−1).\n\n2. If Ak+r is assigned to a bin that contains a heavy row. Without loss of generality, we can assume that Ak+r is mapped to bk that contains the heavy row As. In this case, the distance of heavy rows A1, · · · , As−1 onto the space spanned by the rows of SA is zero. Next, we bound the amount of change in the squared distance of As and light rows onto the space spanned by the rows of SA.\n\nNote that the (k − 1)-dimensional space corresponding to w1, · · · , wk−1 has not changed. Hence, we only need to bound the decrease in the projection distance of Ak+r onto ek compared to ek (where ek, ek are defined similarly as in the last part).\n\n1. For the light rows other than Ak+r, the squared projection of each onto ek is at most β2k2. Hence, the total increase in the squared projection of light rows onto ek is at most (n−k)·β2k2 = O(n−1).\n\n32\n\nPublished as a conference paper at ICLR 2023\n\n2. By Claim F.11, the sum of squared projections of As and Ak+r onto ek decreases by at least\n\n+ O(n−1))\n\n▷ by Observation F.4\n\n+ O(n−1))\n\n2 + ∥Ak+r∥4\n\n2\n\n∥As∥2\n\n2 − (\n\n≥ ∥As∥2\n\n2 − (\n\n∥Ar+k∥2\n\n∥Ar+k∥2\n\n∥As∥4\n\n2 + ∥Ak+r∥4\n\n2\n\n∥As∥2 2 (∥As∥2 ∥As∥2 2 (∥As∥2\n\n2\n\n∥As + Ar+k∥2 ∥As∥4 2 + ∥Ar+k∥2 2 − ∥Ak+r∥2 2 + ∥Ar+k∥2 2 − ∥Ak+r∥2 ∥As∥2 2 − ∥Ak+r∥2 2)\n\n2 + ∥Ar+k∥2\n\n2\n\n∥Ar+k∥2\n\n2 (∥As∥2\n\n∥As∥2\n\n2 + ∥Ar+k∥2 2 (1 − (∥Ak+r∥2\n\n2\n\n∥Ar+k∥2\n\n1 + (∥Ar+k∥2\n\n2 / ∥As∥2 2 / ∥As∥2 2)\n\n2))\n\n− O(n−1)\n\n− O(n−1)\n\n≥\n\n≥\n\n≥\n\n≥\n\n2 − n−O(1) 2) − ∥As∥2 2 − O(n−1) 2) − ∥As∥2\n\n2 · O(n−1)\n\n2 · O(n−1)\n\n− O(n−1)\n\n− O(n−1)\n\n≥ ∥Ar+k∥2\n\n2 (1 −\n\n∥Ak+r∥2 ∥As∥2\n\n) − O(n−1)\n\n▷ 1 − ε2\n\n1 + ε2 ≥ 1 − ε\n\nHence, in this case, the total decrease in the squared projection is at least\n\n∥Ar+k∥2\n\n2 (1 −\n\n∥Ak+r∥2 ∥As∥2\n\n) − O(n−1) = 1 −\n\n∥Ak+r∥2 ∥As∥2 √\n\n) − O(n−1)\n\n= 1 − (1/\n\nl) − O(n−1)\n\n▷ ∥Ar+k∥2 = 1\n\n▷ ∥As∥2 =\n\n√\n\nl\n\nThus, for a sufficiently large value of l, the greedy algorithm will assign Ak+r to a bin that only contains light rows. This completes the inductive proof and in particular implies that at the end of the algorithm, heavy rows are assigned to isolated bins.\n\nCorollary F.13. The approximation loss of the best rank-k approximate solution in the rowspace SgA for A ∼ Asp(s, l), where A ∈ Rn×d for d = Ω(n4k4 log n) and Sg is the CountSketch constructed by the greedy algorithm with non-increasing order, is at most n − s.\n\nProof. First, we need to show that the absolute values of the inner products of vectors in v1, · · · , vn are at most ε < min{n−2k−2, (n (cid:80) Ai∈w ∥Ai∥2)−1} so that we can apply Lemma F.12. To show d ≤ n−2k−2 since d = Ω(n4k4 log n). The proof this, note that by Observation F.3, ε ≤ 2 follows from Lemma F.8 and Lemma F.12. Since all heavy rows are mapped to isolated bins, the projection loss of the light rows is at most n − s.\n\n(cid:113) log n\n\nNext, we bound the Frobenius norm error of the best rank-k-approximation solution constructed by the standard CountSketch with a randomly chosen sparsity pattern.\n\nLemma F.14. Let s = αk where 0.7 < α < 1. The expected squared loss of the best rank-k approximate solution in the rowspace SrA for A ∈ Rn×d ∼ Asp(s, l), where d = Ω(n6l2) and Sr is the sparsity pattern of CountSketch is chosen uniformly at random, is at least n + lk 4e − (1 + α)k − n−O(1).\n\nProof. We can interpret the randomized construction of the CountSketch as a “balls and bins” experiment. In particular, considering the heavy rows, we compute the expected number of bins (i.e., rows in SrA) that contain a heavy row. Note that the expected number of rows in SrA that do not contain any heavy row is k · (1 − 1 k−1 . Hence, the number of rows in SrA that contain a heavy row of A is at most k(1 − e− s k−1 ) heavy rows are not mapped to an isolated bin (i.e., they collide with some other heavy rows). Then, it is straightforward to show that the squared loss of each such row is at least l − n−O(1).\n\nk−1 ). Thus, at least s − k(1 − e− s\n\nk )s ≥ k · e− s\n\n33\n\nPublished as a conference paper at ICLR 2023\n\nClaim F.15. Suppose that heavy rows Ar1, · · · , Arc are mapped to the same bin via a CountSketch S. Then, the total squared distances of these rows from the subspace spanned by SA is at least (c − 1)l − O(n−1).\n\nProof. Let b denote the bin that contains the rows Ar1, · · · , Arc and suppose that it has c′ light rows as well. Note that by Claim F.10 and Claim F.11, the squared projection of each row Ari onto the subspace spanned by the k bins is at most\n\n∥Ahi∥4 ∥w∥2\n\n2\n\n2\n\n+ O(n−1)\n\nl2\n\ncl + c′ − 2ε(c2l + cc′\n\n√\n\nl + c′2)\n\n+ O(n−1)\n\n+ n−O(1)\n\nl2 cl − n−O(1) l2 c2l2 · (cl + O(n−1) + O(n−1)\n\nl c\n\n+ O(n−1)\n\n▷ by ε ≤ n−3l−1\n\n≤\n\n≤\n\n≤\n\n≤\n\nHence, the total squared loss of these c heavy rows is at least cl − c · ( l O(n−1).\n\nc + O(n−1)) ≥ (c − 1)l −\n\nThus, the expected total squared loss of the heavy rows is at least:\n\nl · (s − k(1 − e− s\n\nk−1 )) − s · n−O(1)\n\n≥l · k(α − 1 + e−α) − lα − n−O(1)\n\n▷ s = α · (k − 1) where 0.7 < α < 1\n\n≥\n\n≥\n\nlk 2e lk 4e\n\n− l − n−O(1)\n\n− O(n−1)\n\n▷ α ≥ 0.7\n\n▷ assuming k > 4e\n\nNext, we compute a lower bound on the expected squared loss of the light rows. Note that Claim F.10 and Claim F.11 imply that when a light row collides with other rows, its contribution to the total squared loss (note that the loss accounts for the amount it decreases from the squared projection of the other rows in the bin as well) is at least 1 − O(n−1). Hence, the expected total squared loss of the light rows is at least:\n\n(n − s − k)(1 − O(n−1)) ≥ (n − (1 + α) · k) − O(n−1)\n\nHence, the expected squared loss of a CountSketch whose sparsity is picked at random is at least\n\nlk 4e\n\n− O(n−1) + n − (1 + α)k − O(n−1) ≥ n +\n\nlk 4e\n\n− (1 + α)k − O(n−1)\n\nCorollary F.16. Let s = α(k − 1) where 0.7 < α < 1 and let l ≥ (4e+1)n . Let Sg be the CountSketch whose sparsity pattern is learned over a training set drawn from Asp via the greedy approach. Let Sr be a CountSketch whose sparsity pattern is picked uniformly at random. Then, for an n × d matrix A ∼ Asp where d = Ω(n6l2), the expected loss of the best rank-k approximation of A returned by Sr is worse than the approximation loss of the best rank-k approximation of A returned by Sg by at least a constant factor.\n\nαk\n\n34\n\nPublished as a conference paper at ICLR 2023\n\nProof.\n\nE Sr\n\n[\n\nmin rank-k X∈rowsp(SrA)\n\n∥X − A∥2\n\nF ] ≥ n +\n\nlk 4e\n\n− (1 + α)k − n−O(1)\n\n▷ Lemma F.14\n\n≥ (1 + 1/α)(n − s)\n\n= (1 + 1/α)\n\nmin rank-k X∈rowsp(SgA)\n\n∥X − A∥2\n\nF\n\n▷ l ≥\n\n(4e + 1)n αk\n\n▷ Corollary F.13\n\nF.2 ZIPFIAN ON SQUARED ROW NORMS.\n\nEach matrix A ∈ Rn×d ∼ Azipf has rows which are uniformly random and orthogonal. Each A has 2i+1 rows of squared norm n2/22i for i ∈ [1, . . . , O(log(n))]. We also assume that each row has the same squared norm for all members of Azipf .\n\nIn this section, the s rows with largest norm are called the heavy rows and the remaining are the light rows. For convenience, we number the heavy rows 1, . . . , s; however, the heavy rows can appear at any indices, as long as any row of a given index has the same norm for all members of Azipf . Also, we assume that s ≤ k/2 and, for simplicity, s = (cid:80)hs i=1 2i+1 for some hs ∈ Z+. That means the minimum squared norm of a heavy row is n2/22hs and the maximum squared norm of a light row is n2/22hs+2.\n\nThe analysis of the greedy algorithm ordered by non-increasing row norms on this family of matrices is similar to our analysis for the spiked covariance model. Here we analyze the case in which rows are orthogonal. By continuity, if the rows are close enough to being orthogonal, all decisions made by the greedy algorithm will be the same.\n\nAs a first step, by Lemma F.8, at the end of iteration k the first k rows are assigned to different bins. Then, via a similar inductive proof, we show that none of the light rows are mapped to a bin that contains one of the top s heavy rows.\n\nLemma F.17. At each iteration k + r, the greedy algorithm picks the position of the non-zero value in the (k + r)-th column of the CountSketch matrix S so that the light row Ak+r is mapped to a bin that does not contain any of top s heavy rows.\n\nProof. We prove the statement by induction. The base case r = 0 trivially holds as the first k rows are assigned to distinct bins. Next we assume that in none of the first k + r − 1 iterations a light row is assigned to a bin that contains a heavy row. Now, we consider the following cases:\n\n1. If Ak+r is assigned to a bin that only contains light rows. Without loss of generality we can assume that Ak+r is assigned to bk. Since the vectors are orthogonal, we only need to bound the difference in the projection of Ak+r and the light rows that are assigned to bk onto the direction of wk before and after adding Ak+r to bk. In this case, the total squared loss corresponding to rows in bk and Ak+r before and after adding Ak+1 are respectively\n\nbefore adding Ak+r to bk: ∥Ak+r∥2\n\n2 +\n\nafter adding Ak+r to bk: ∥Ak+r∥2\n\n2 +\n\n∥Aj∥2\n\n2 − (\n\n∥Aj∥2\n\n2 − (\n\n(cid:88)\n\nAj ∈bk\n\n(cid:88)\n\nAj ∈bk\n\n35\n\n)\n\n(cid:80)\n\n(cid:80)\n\nAj ∈bk\n\nAj ∈bk\n\n∥Ak+r∥4 ∥Ak+r∥2\n\n2\n\n2\n\n∥Aj∥4 ∥Aj∥2 2 + (cid:80) 2 + (cid:80)\n\nAj ∈bk\n\nAj ∈bk\n\n∥Aj∥4 ∥Aj∥2\n\n2\n\n2\n\n)\n\nPublished as a conference paper at ICLR 2023\n\nThus, the amount of increase in the squared loss is\n\n(\n\n(cid:80)\n\nAj ∈bk\n\n(cid:80)\n\nAj ∈bk\n\n∥Aj∥4 ∥Aj∥2\n\n2\n\n2\n\n) − (\n\n∥Ak+r∥4 ∥Ak+r∥2\n\n2 + (cid:80) 2 + (cid:80)\n\nAj ∈bk\n\nAj ∈bk\n\n∥Aj∥4 ∥Aj∥2\n\n2\n\n2\n\n) =\n\n2 · (cid:80)\n\n∥Ak+r∥2 ((cid:80)\n\nAj ∈bk\n\n∥Aj∥2\n\n∥Aj∥4 2)(∥Ak+r∥2\n\n2 − ∥Ak+r∥4 2 + (cid:80)\n\n2 · (cid:80)\n\nAj ∈bk\n\nAj ∈bk\n\n∥Aj∥2 2)\n\n∥Aj∥2\n\n2\n\nAj ∈bk (cid:80)\n\n= ∥Ak+r∥2\n\n2 ·\n\n≤ ∥Ak+r∥2\n\n2 ·\n\n(cid:80)\n\nAj ∈bk\n\nAj ∈bk\n\n(cid:80)\n\nAj ∈bk\n\n(cid:80)\n\nAj ∈bk\n\n(cid:80)\n\nAj ∈bk\n\n2\n\n2\n\n∥Aj ∥4 ∥Aj ∥2 ∥Aj∥2 ∥Aj∥2 ∥Aj∥2\n\n− ∥Ak+r∥2\n\n2\n\n2\n\n2 + ∥Ak+r∥2 2 − ∥Ak+r∥2 2 + ∥Ak+r∥2\n\n2\n\n2\n\n)\n\n(F.7)\n\n2. If Ak+r is assigned to a bin that contains a heavy row. Without loss of generality and by the induction hypothesis, we assume that Ak+r is assigned to a bin b that only contains a heavy row Aj. Since the rows are orthogonal, we only need to bound the difference in the projection of Ak+r and Aj In this case, the total squared loss corresponding to Aj and Ak+r before and after adding Ak+1 to b are respectively\n\nbefore adding Ak+r to bk: ∥Ak+r∥2\n\n2\n\n(F.6)\n\nafter adding Ak+r to bk: ∥Ak+r∥2\n\n2 + ∥Aj∥2\n\n2 − (\n\n∥Ak+r∥4 ∥Ak+r∥2\n\n2 + ∥Aj∥4 2 + ∥Aj∥2\n\n2\n\n2\n\nThus, the amount of increase in the squared loss is\n\n∥Aj∥2\n\n2 − (\n\n∥Ak+r∥4 ∥Ak+r∥2\n\n) = ∥Ak+r∥2\n\n2 ·\n\n∥Aj∥2 ∥Aj∥2\n\n2 − ∥Ak+r∥2 2 + ∥Ak+r∥2\n\n2\n\n2\n\n2\n\n2 + ∥Aj∥4 2 + ∥Aj∥2 2 ≥ (cid:80)\n\n2\n\nThen (F.7) is larger than (F.6) if ∥Aj∥2 2. Next, we show that at every inductive Ai∈bk iteration, there exists a bin b which only contains light rows and whose squared norm is smaller than the squared norm of any heavy row. For each value m, define hm so that m = (cid:80)hm i=1 2i+1 = 2hm+2 − 2. Recall that all heavy rows have squared norm at least n2 light rows and has squared norm at most\n\n22hs . There must be a bin b that only contains\n\n∥Ai∥2\n\n∥w∥2\n\n2 =\n\n∥Ai∥2\n\n2 ≤\n\nn2 22(hs+1)\n\n(cid:88)\n\nAi∈b\n\n≤\n\n≤\n\nn2 22(hs+1) n2 22(hs+1) n2 22hs+1 < ∥As∥2\n\n≤\n\n2\n\n(cid:80)hn\n\ni=hk+1\n\n2i+1n2 22i\n\nk − s\n\n2n2 2hk (k − s) n2 22hk\n\n+\n\n+\n\n+\n\n▷ s ≤ k/2 and k > 2hk+1\n\n▷ hk ≥ hs + 1\n\nHence, the greedy algorithm will map Ak+r to a bin that only contains light rows.\n\nCorollary F.18. The squared loss of the best rank-k approximate solution in the rowspace of SgA for A ∈ Rn×d ∼ Azipf where A ∈ Rn×d and Sg is the CountSketch constructed by the greedy algorithm with non-increasing order, is < n2\n\n2hk −2 .\n\nProof. At the end of iteration k, the total squared loss is (cid:80)hn iteration k + r, by (F.6), the squared loss increases by at most ∥Ak+r∥2\n\ni=hk+1 2i+1 · n2\n\n22i . After that, in each 2. Hence, the total squared\n\n36\n\nPublished as a conference paper at ICLR 2023\n\nloss in the solution returned by Sg is at most\n\nhn(cid:88)\n\n2(\n\ni=hk+1\n\n2i+1n2 22i\n\n) = 4n2 ·\n\nhn(cid:88)\n\ni=hk+1\n\n1\n\n2i <\n\n4n2 2hk\n\n=\n\nn2 2hk−2\n\nNext, we bound the squared loss of the best rank-k-approximate solution constructed by the standard CountSketch with a randomly chosen sparsity pattern. Observation F.19. Let us assume that the orthogonal rows Ar1, · · · , Arc are mapped to the same bin and for each i ≤ c, ∥Ar1∥2 2. Then, the total squared loss of Ar1 , · · · , Arc after projecting onto Ar1 ± · · · ± Arc is at least ∥Ar2∥2\n\n2 + · · · + ∥Arc ∥2 2.\n\n2 ≥ ∥Ari∥2\n\nProof. Note that since Ar1, · · · , Arc are orthogonal, for each i ≤ c, the squared projection of Ari onto Ar1 ± · · · ± Arc is ∥Ari∥4 2. Hence, the sum of squared projection coefficients j=1 of Ar1, · · · , Arc onto Ar1 ± · · · ± Arc is\n\n2 / (cid:80)c\n\n(cid:13) (cid:13)Arj\n\n(cid:13) 2\n(cid:13)\n\n(cid:80)c\n\nj=1\n\n(cid:80)c\n\nj=1\n\n(cid:13) (cid:13)Arj (cid:13) (cid:13)Arj\n\n(cid:13) 4\n(cid:13) 2\n(cid:13) 2\n(cid:13) 2\n\n≤ ∥Ar1∥2\n\n2\n\nHence, the total projection loss of Ar1, · · · , Arc onto Ar1 ± · · · ± Arc is at least\n\nc (cid:88)\n\nj=1\n\n(cid:13) (cid:13)Arj\n\n(cid:13) 2\n2 − ∥Ar1∥2 (cid:13)\n\n2 = ∥Ar2∥2\n\n2 + · · · + ∥Arc ∥2 2 .\n\nIn particular, Observation F.19 implies that whenever two rows are mapped into the same bin, the squared norm of the row with smaller norm fully contributes to the total squared loss of the solution. Lemma F.20. For k > 210 − 2, the expected squared loss of the best rank-k approximate solution in the rowspace of SrA for An×d ∼ Azipf , where Sr is the sparsity pattern of a CountSketch chosen uniformly at random, is at least 1.095n2 2hk −2 .\n\nProof. In light of Observation F.19, we need to compute the expected number of collisions between rows with “large” norm. We can interpret the randomized construction of the CountSketch as a “balls and bins” experiment.\n\nFor each 0 ≤ j ≤ hk, let Aj denote the set of rows with squared norm n2 Ai. Note that for each j, |Aj| = 2hk−j+1 and |A>j| = (cid:80)hk (cid:83)\n\n22(hk −j) and let A>j = i=1 2i = 2(2hk−j − 1). Moreover, note that k = 2(2hk+1 − 1). Next, for a row Ar in Aj (0 ≤ j < hk), we compute the probability that at least one row in A>j collides with Ar.\n\ni=j+1 2hk−i+1 = (cid:80)hk−j\n\nj<i≤hk\n\nPr[at least one row in A>j collides with Ar] = (1 − (1 −\n\n)|A>j |)\n\n1 k\n|A>j | k\n\n)\n\n≥ (1 − e−\n\n= (1 − e\n\n− 2hk −j −1\n\n2hk +1−1 )\n\n≥ (1 − e−2−j−2\n\n)\n\n▷ since\n\n2hk−j − 1 2hk+1 − 1\n\n> 2−j−2\n\nHence, by Observation F.19, the contribution of rows in Aj to the total squared loss is at least\n\n(1 − e−2−j−2\n\n) · |Aj| ·\n\nn2 22(hk−j)\n\n=(1 − e−2−j−2\n\n) ·\n\nn2\n\n2hk−j−1 = (1 − e−2−j−2\n\n) ·\n\nn2 2hk−2 · 2j−1\n\n37\n\nPublished as a conference paper at ICLR 2023\n\nThus, the contribution of rows with “large” squared norm, i.e., A>0, to the total squared loss is at least4\n\nn2 2hk−2 ·\n\nhk(cid:88)\n\nj=0\n\n2j−1 · (1 − e−2−j−2\n\n) ≥ 1.095 ·\n\nn2 2hk−2\n\n▷for hk > 8\n\nCorollary F.21. Let Sg be a CountSketch whose sparsity pattern is learned over a training set drawn from Asp via the greedy approach. Let Sr be a CountSketch whose sparsity pattern is picked uniformly at random. Then, for an n × d matrix A ∼ Azipf , for a sufficiently large value of k, the expected loss of the best rank-k approximation of A returned by Sr is worse than the approximation loss of the best rank-k approximation of A returned by Sg by at least a constant factor.\n\nProof. The proof follows from Lemma F.20 and Corollary F.18.\n\nRemark F.22. We have provided evidence that the greedy algorithm that examines the rows of A according to a non-increasing order of their norms (i.e., greedy with non-increasing order) results in a better rank-k solution compared to the CountSketch whose sparsity pattern is chosen at random. However, still other implementations of the greedy algorithm may result in a better solution compared to the greedy algorithm with non-increasing order. To give an example, in the following simple instance the greedy algorithm that checks the rows of A in a random order (i.e., greedy with random order) achieves a rank-k solution whose cost is a constant factor better than the solution returned by the greedy with non-increasing order.\n\nLet A be a matrix with four orthogonal rows u, u, v, w where ∥u∥2 = 1 and ∥v∥2 = ∥w∥2 = 1 + ε and suppose that the goal is to compute a rank-2 approximation of A. Note that in the greedy algorithm with non-decreasing order, v and w will be assigned to different bins and by a simple calculation we can show that the copies of u also will be assigned to different bins. Hence, the squared loss in the computed rank-2 solution is 1 + (1+ε)2 2+(1+ε)2 . However, the optimal solution will assign v and w to one bin and the two copies of u to the other bin which results in a squared loss of (1 + ε)2 which is a constant factor smaller than the solution returned by the greedy algorithm with non-increasing order for sufficiently small values of ε.\n\nOn the other hand, in the greedy algorithm with a random order, with a constant probability of ( 1 3 + 1 8 ), the computed solution is the same as the optimal solution. Otherwise, the greedy algorithm with random order returns the same solution as the greedy algorithm with a non-increasing order. Hence, in expectation, the solution returned by the greedy with random order is better than the solution returned by the greedy algorithm with non-increasing order by a constant factor.\n\nG EXPERIMENT DETAILS\n\nG.1 LOW-RANK APPROXIMATION\n\nIn this section, we describe the experimental parameters in our experiments. We first introduce some parameters in Stage 2 of our approach proposed in Section 3.\n\n• bs: batch size, the number of training samples used in one iteration.\n\n• lr: learning rate of gradient descent.\n\n• iter: the number of iterations of gradient descent.\n\nTable 7.1: Test errors for LRA (using Algorithm 2 with four sketches) For a given m, the dimensions of the four sketches were: S ∈ Rm×n, R ∈ Rm×d, S2 ∈ R5m×n, R2 ∈ R5m×d.\n\nParameters of the algorithm: bs = 1, lr = 1.0, 10.0 for hyper and video respectively, num_it = 1000. For our algorithm 4, we use the average of all training matrix as the input to the algorithm.\n\n4The numerical calculation is computed using WolframAlpha.\n\n38\n\nPublished as a conference paper at ICLR 2023\n\nTable 7.1: Test errors for LRA (using Algorithm 1 with one sketch)\n\nParameters of the algorithm: bs = 1, lr = 1.0, 10.0 for hyper and video respectively, num_it = 1000. For our algorithm 4, we use the sum of all training matrix as the input to the algorithm.\n\nG.2 SECOND-ORDER OPTIMIZATION\n\nAs we state in Section 6, when we fix the positions of the non-zero entries (uniformly chosen in each column or sampled according to the heavy leverage score distribution), we aim to optimize the values by gradient descent, as mentioned in Section 3. Here the loss function is given in Section 6. In our implementation, we use PyTorch (Paszke et al. (2019)), which can compute the gradient automatically (here we can use torch.qr() and torch.svd() to define our loss function). For a more nuanced loss function, which may be beneficial, one can use the package released in Agrawal et al. (2019), where the authors studied the problem of computing the gradient of functions which involve the solution to certain convex optimization problem.\n\nAs mentioned in Section 2, each column of the sketch matrix S has exactly one non-zero entry. Hence, the i-th coordinate of p can be seen as the non-zero position of the i-th column of S. In the implementation, to sample p randomly, we can sample a random integer in {1, . . . , m} for each coordinate of p. For the heavy rows mentioned in Section 6, we can allocate positions 1, . . . , k to the k heavy rows, and for the other rows, we randomly sample an integer in {k + 1, . . . , m}. We note that once the vector p, which contains the information of the non-zero positions in each column of S, is chosen, it will not be changed during the optimization process in Section 3.\n\nNext, we introduce the parameters for our experiments:\n\n• bs: batch size, the number of training samples used in one iteration.\n\n• lr: learning rate of gradient descent.\n\n• iter: the number of iterations of gradient descent\n\nIn our experiments, we set bs = 20, iter = 1000 for all datasets. We set lr = 0.1 for the Electric dataset.\n\n39",
  "translations": [
    "# Summary Of The Paper\n\nThis paper is about learning linear maps for dimensionality reduction that are based on count sketch. Existing works either use randomized sketches or learn the values in a count sketch matrix (with the positions drawn randomly). This paper develops on such methods by proposing to learn both the positions and the values of the count sketch matrix. The paper studies this idea in the context of two problems: low-rank approximation and iterative Hessian sketching. The contributions of the paper are:\n1. It shows that learning positions and values instead of just values leads to better performance. This is shown using a greedy search algorithm \n2. In order to overcome the high run time of the greedy search algorithm, the paper proposed algorithms to construct the count sketch matrix using ridge leverage score based sampling. \n3. The paper shows using experiments that the proposed ideas lead to better numerical performance.\n\n# Strength And Weaknesses\n\nStrengths:\n\n1. The idea of learning both the positions and the values is interesting and warrants a study. This paper does a good job of exploring this (previously unexplored) idea. \n\n2. The paper shows strong improvement over previous algorithms in terms of numerical performance.\n\nWeaknesses: \n\n1. The presentation of the paper can be improved a lot. Some examples of poor presentation are: \n    - Algorithms 1, 2 and 3 are placed in unnatural locations, with presumably liberal usage of \\vspace and \\hspace. The paper is visually \n        very dense and doesn't need to be.\n    - On page 3: \"Hence, in this work we consider the following algorithm that compresses both sides of A...\": which algorithm?\n    - Authors should consider completely moving the few-shot learning setting to the appendix and write more clearly about the two main \n        problems considered. \n    - Placement of the results tables are also not great. For example, Table 7.4 appear at a random location in between text. \n\n2. I have the following questions regarding the experiments:\n    - Shouldn't metrics always be relative errors? For example, in the low-rank approximation setting, instead of reporting ||A-A_k|| - ||A-X*||, \n        it should be ( ||A-A_k|| - ||A-X*||)/ ||A||. This would indicate how large the error is w.r.t the Frobenius norm of A itself and gives a better \n        idea of performance.\n    - Why are the errors in the few-shot learning case smaller than when the full training set is used? Intuitively, it should be the opposite. \n    - The authors mention that during training, only the average of the training matrices are used. However, this is not equivalent to \n        minimizing the expected error over the training set. It is also not clear if the baseline methods (especially IVY19) follow this procedure.  \n        Upon reading the IVY19 paper, it appears that they do minimize the error over all the training matrices. Maybe I am missing something, \n        but there is chance that the baseline methods have worse performance because of this? It would be better to present results (at least \n        on the baseline methods) where the training fully takes advantage of the training set and does not use just the average. \n    - Authors mention that they use torch.qr for backpropagating through QR factorization. Do the authors take any measures to ensure \n       that the first few columns are linearly independent (as required by torch.qr: https://pytorch.org/docs/stable/generated/torch.qr.html)?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity: The paper seems to be put together hurriedly. Presentation and flow can be improved quite a lot. See above for specific comments. \n\nQuality: Idea is quite interesting and warrants a study. The paper is technically sound.\n\nOriginality: The idea is original, although the proposed algorithms rely quite heavily on existing frameworks (ridge leverage score sampling and sketch monotonicity).\n\n# Summary Of The Review\n\nThe paper studies an interesting problem and presents good ideas. However, the presentation needs to be improved and the experiments should be explained more clearly. I have elaborated further in my comments above.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper presents a novel approach to sketching algorithms in optimization problems, particularly focusing on CountSketch matrices. The authors introduce a learning-based framework that optimizes not only the values of the non-zero entries but also their locations within the sketch matrix. Through a greedy search algorithm and two faster methods, the authors demonstrate improved performance in low-rank approximation and second-order optimization tasks. Empirical evaluations across various datasets reveal significant accuracy improvements and enhanced efficiency, with reductions in error rates compared to classical and previous learning-based sketching methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to optimizing the locations of non-zero entries in CountSketch matrices, which addresses a notable limitation in existing sketching methods. The proposed algorithms are grounded in solid theoretical foundations and show promising results in empirical evaluations, particularly in terms of accuracy and runtime efficiency. However, the greedy algorithm's computational expense may be a limitation for very large datasets, suggesting that while the proposed methods are efficient, the initial optimization stage could hinder scalability in practice.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and empirical results. The explanations of the algorithms and the theoretical underpinnings are articulated effectively, making the content accessible to readers familiar with sketching techniques and optimization. The novelty of optimizing both values and positions of non-zero entries sets this work apart from prior approaches. However, details regarding the implementation and potential challenges in reproducing the results could be improved, particularly in terms of providing clear guidelines for replicating the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to learning-based sketching methods by effectively optimizing the locations and values of non-zero entries in CountSketch matrices. The empirical results are compelling, showcasing substantial improvements over existing methods, although scalability may be a concern with the greedy algorithm. The clarity of presentation is strong, though more emphasis on reproducibility would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Learning the Positions in CountSketch\" introduces innovative learning-based sketching algorithms designed to enhance data compression for optimization problems, specifically through the CountSketch framework. It proposes two main algorithms: a Greedy Algorithm for learning positions of non-zero entries and optimized sampling methods for low-rank approximation (LRA) and Hessian approximation. The findings demonstrate significant performance improvements, including a 70% error reduction compared to classical CountSketch methods and a notable reduction in convergence rates for real-world datasets.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel approach to optimizing both the positions and values in CountSketch, addressing the limitations of prior methodologies that only learned entry values. Empirical results are robust, showcasing improvements across various datasets, indicating the algorithms' effectiveness in real-world scenarios. However, the Greedy Algorithm's slower training time may limit its scalability, and the introduced complexity could pose implementation challenges. Additionally, the reliance on specific distributional assumptions for performance guarantees may restrict the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The quality of the empirical results is commendable, showcasing a solid experimental design with appropriate evaluation metrics and baseline comparisons. The novelty of the proposed methods is significant, particularly in the context of sketching algorithms. However, the reproducibility may be hampered by the complexity of implementation and the specific assumptions made about input distributions.\n\n# Summary Of The Review\nOverall, the paper presents a substantial contribution to the field of sketching algorithms through its novel optimization of positions and values in CountSketch. While it demonstrates strong empirical results and practical applicability, concerns regarding training time and implementation complexity must be addressed to enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning the Positions in CountSketch\" investigates innovative sketching algorithms aimed at enhancing data compression and solving optimization challenges such as low-rank approximation and regression. The authors propose a novel learning-based approach that optimizes both the values and positions of non-zero entries in a CountSketch matrix. Their methodology includes a two-stage process consisting of position and value optimization, implemented through a greedy algorithm and gradient descent. Empirical results demonstrate significant improvements in training speed and accuracy for Hessian approximation in second-order optimization, with the proposed methods achieving up to 70% reduction in approximation errors over classical CountSketch methods.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its dual focus on optimizing both the values and positions of sketch matrix entries, which represents a meaningful advancement over prior work that fixed entry locations. The empirical results are compelling, showcasing substantial improvements in various optimization tasks across multiple datasets. However, the paper could benefit from a more detailed discussion on the limitations of the proposed methods, particularly regarding their scalability to larger datasets and high dimensionality. Additionally, while the theoretical guarantees provided are valuable, further empirical validation in diverse contexts could enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates its contributions, with a logical flow from the introduction to the conclusion. The methodologies are described in sufficient detail, allowing for reproducibility of the experimental results. The novelty of the approach lies in the integration of learning techniques within sketching algorithms, which has not been extensively explored in previous work. However, some mathematical derivations and theoretical claims could benefit from additional clarity to facilitate reader comprehension.\n\n# Summary Of The Review\nOverall, this paper presents a significant contribution to the field of sketching algorithms by introducing a learning-based approach that optimizes both the values and positions of CountSketch entries. The empirical results strongly support the theoretical claims, although further exploration of the methods' limitations and scalability would strengthen the paper.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"Learning the Positions in CountSketch\" presents a novel approach to optimizing CountSketch matrices by introducing learning-based algorithms that enhance both the values and the locations of non-zero entries. The authors propose an initial greedy algorithm and a second, more efficient method that significantly improves accuracy, achieving up to 70% error reduction in low-rank approximation and second-order optimization tasks. Additionally, the methods demonstrate robustness in few-shot learning scenarios and are validated through comprehensive experiments across various datasets, addressing real-world challenges such as LASSO and matrix estimation.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach to CountSketch optimization, which fills a notable gap in existing literature, as well as substantial improvements in accuracy and efficiency over traditional methods. The experimental validation is robust, showcasing the proposed methods across diverse datasets and real-world applications. However, the initial greedy algorithm's slower training time may limit practical applicability, and the lack of extensive theoretical guarantees for varied input distributions raises concerns about generalizability. Furthermore, while the theoretical foundations are solid, certain assumptions may restrict the applicability of the results in broader contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written with a clear structure, making the contributions and methodologies easy to follow. The quality of the experimental validation is commendable, although more discussion on the computational costs associated with the initial training phase would enhance reproducibility. The novelty of the approach is significant, particularly in its intersection of machine learning and optimization, though further exploration of practical implications would strengthen its impact in real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the fields of machine learning and optimization with its innovative learning-based approach to CountSketch matrices. While the empirical results are promising, the paper could benefit from a deeper analysis of theoretical guarantees and practical implications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning the Positions in CountSketch\" presents a novel approach to sketching algorithms that enhances the learning-based sketching paradigm. It introduces a framework that not only optimizes the values of non-zero entries in a CountSketch matrix but also their positions, representing a significant departure from previous methodologies that fixed these positions. The authors propose two main techniques for position optimization: a greedy algorithm and an enhanced variant that ensures efficiency without sacrificing performance. Empirical results show that the new algorithms achieve a substantial reduction in approximation error, demonstrating their effectiveness across various datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to optimizing both the positions and values of non-zero entries in CountSketch matrices, which addresses limitations in existing methods. The introduction of a greedy algorithm and its enhanced variant is well-justified and shows notable improvements in training efficiency and approximation accuracy. However, one weakness of the paper is that while the empirical results are promising, the theoretical analysis could benefit from more depth, particularly in establishing performance guarantees under broader conditions beyond the specific data distributions examined.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The writing quality is high, making complex ideas accessible to a broad audience. The novelty of the work is significant as it expands the scope of CountSketch algorithms by allowing for position learning. The methods and results are presented in a reproducible manner, with sufficient details provided for others to replicate the experiments. However, some theoretical aspects could be elaborated upon to enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a substantial advancement in the field of sketching algorithms by optimizing both the positions and values of non-zero entries in CountSketch matrices. The proposed methods demonstrate significant empirical improvements over existing techniques, although further theoretical insights would strengthen the overall contribution.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Learning Efficient Adversarial Training Strategies\" presents a novel approach to adversarial training in deep learning. It introduces a methodology that allows models to dynamically learn both the values and locations of adversarial perturbations during training, enhancing their robustness against adversarial attacks. The authors propose a unique algorithm that adapts the traditional adversarial training framework, demonstrating significant improvements in model performance and resilience through extensive empirical experiments across multiple datasets.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The dual learning mechanism for perturbation values and locations is a noteworthy advancement over conventional adversarial training methods, potentially leading to better robustness.\n2. **Theoretical Foundations:** The paper provides a strong theoretical underpinning, including rigorous proofs that support the advantages of the proposed strategy.\n3. **Empirical Results:** The experiments yield compelling evidence of the algorithm's effectiveness, showcasing reduced adversarial error rates compared to classical methods.\n4. **Real-World Relevance:** The focus on adversarial attacks addresses a pressing issue in machine learning, enhancing the paper's significance for both academic and industrial applications.\n\n**Weaknesses:**\n1. **Complexity of Implementation:** The dynamic nature of the proposed method may introduce challenges in computational efficiency and scalability, which could impede practical application.\n2. **Limited Scope of Experiments:** The experimental validation is primarily conducted on a narrow selection of benchmark datasets, which may not fully capture the generalizability of the approach.\n3. **Potential Overfitting:** The algorithm may be susceptible to overfitting, particularly with a limited number of adversarial examples, necessitating further investigation into its generalization capabilities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodologies. The quality of the theoretical and empirical analyses is high, and the novelty of the approach is clearly stated. However, the complexity of the proposed methods may pose challenges for reproducibility, particularly for practitioners who may not have access to extensive computational resources. The authors could benefit from providing more practical implementation guidelines to enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in adversarial training through a novel learning-based approach. Its strong theoretical foundation and promising empirical results indicate its potential impact on the field. However, practical challenges regarding implementation and the need for broader experimental validation highlight areas for improvement.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper titled \"Learning the Positions in CountSketch\" introduces innovative algorithms that optimize not only the values but also the locations of non-zero entries within CountSketch matrices. The authors claim their learning-based approach represents a significant advancement in sketching techniques, achieving enhanced accuracy and efficiency in optimization problems. Key contributions include a learning-based algorithm for location optimization, a fast greedy algorithm for sketch matrix learning, and claims of significant error reduction and performance in few-shot learning scenarios.\n\n# Strength And Weaknesses\nThe paper presents several noteworthy contributions, particularly the introduction of a learning-based algorithm that optimizes the locations of non-zero entries, which is a first in this domain. The proposed greedy algorithm claims to offer faster performance while maintaining high accuracy, though the marginal gains over existing techniques may not justify the claims. While the reported error reductions and generalization capabilities are impressive, the empirical backing for these assertions appears limited, and there is a lack of thorough exploration of the algorithms' robustness across different datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is relatively clear in its presentation, articulating its methodologies and findings effectively. However, the novelty, while significant, is somewhat overstated in terms of its impact, suggesting that the advancements could be incremental rather than groundbreaking. The reproducibility of results could be improved through more detailed descriptions of experimental setups and datasets used, as well as a deeper analysis of the limitations and potential failure cases for the proposed methods.\n\n# Summary Of The Review\nOverall, the paper presents promising advancements in the realm of sketching algorithms, particularly through the optimization of CountSketch. While the contributions are compelling, the practical implications and robustness of the proposed methods may not be as transformative as claimed, warranting a more cautious interpretation of the results.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"Learning the Positions in CountSketch\" by Yi Li et al. explores the optimization of sketching algorithms for efficient data compression in optimization problems such as low-rank approximation and regression. The authors introduce a novel learning-based sketching paradigm that not only optimizes the values of non-zero entries in a random sketch matrix but also their positions, addressing a gap in prior methodologies that fixed these locations. The findings demonstrate significant performance improvements, including a 60% error reduction in low-rank approximation compared to classical sketches and a notable 40% runtime reduction, showcasing the advantages of this adaptive approach in various experimental settings.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to optimizing the positions of non-zero entries in sketch matrices, which leads to improved performance metrics over existing techniques. The empirical results are robust, highlighting substantial reductions in error rates and runtime across various datasets. However, a potential weakness is the reliance on specific training matrices, which may limit the generalizability of the approach in broader contexts. Additionally, while the methodology is well-structured, the paper could benefit from more extensive comparisons with a wider range of existing methods to further validate its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that makes the contributions and methodologies easy to understand. The quality of the experimental design is commendable, providing detailed evaluations across multiple datasets. The novelty is significant, as it addresses a previously unexplored aspect of sketch learning—optimizing entry positions. However, reproducibility could be a concern if the implementation details of the proposed algorithms are not sufficiently detailed in the paper.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in learning-based sketching algorithms, with strong empirical support for its claims. The innovative approach to optimizing entry positions within sketch matrices demonstrates significant improvements over classical methods, although certain aspects may require further exploration for broader applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for optimizing sparse CountSketch matrices used in various machine learning applications, particularly in few-shot learning scenarios. The authors propose a greedy algorithm to learn both the values and the locations of the non-zero entries in the sketching matrices, aiming to improve performance over traditional methods that fix non-zero locations. The methodology is empirically validated across specific data distributions, yielding promising results, but raises questions regarding generalization and theoretical underpinnings.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to sketch optimization, especially the introduction of new optimization objectives aimed at subspace embedding. The empirical results presented are compelling, indicating improvements over classical sketch methods in certain contexts. However, the paper is not without weaknesses. The reliance on assumptions about data distributions, the potential for overfitting in few-shot scenarios, and the trade-offs between learned and classical sketches introduce significant limitations. Additionally, the dependence on user-defined parameters may hinder practical applicability, as incorrect settings could lead to suboptimal performance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its methodology in a clear manner. However, some sections could benefit from deeper explanations, particularly regarding the theoretical guarantees of the proposed methods. The novelty of the approach is notable, but the lack of robust theoretical support raises concerns about its reproducibility and generalizability across different data distributions. The empirical results are intriguing but would be strengthened by a broader evaluation across various datasets, rather than being confined to specific distributions.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of sketch learning with a novel approach and compelling empirical results. However, the reliance on specific distributional assumptions and the potential for overfitting in few-shot learning contexts are significant concerns that need to be addressed for the findings to be broadly applicable.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel learning-based sketching algorithm that optimizes both the values and the locations of non-zero entries in the CountSketch matrix. The method aims to enhance the performance of sketching algorithms in solving optimization problems, specifically low-rank approximation and regression tasks. The proposed two-stage approach incorporates a greedy search for position optimization and demonstrates significant improvements in accuracy and training efficiency compared to classical CountSketch methods and previous learned sketches.\n\n# Strength And Weaknesses\nThe paper makes substantial contributions by addressing a critical gap in existing sketching algorithms—the optimization of non-zero entry locations alongside their values. This dual optimization leads to enhanced performance in low-rank approximation and second-order optimization tasks, as evidenced by the experimental results. However, a potential weakness is the reliance on specific data distributions for the greedy search optimization, which may limit the generalizability of the method across diverse datasets. Additionally, while the paper provides a comprehensive overview of related work, further discussion on the limitations of the proposed approach could strengthen the overall narrative.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents the methodology and results. Key concepts are effectively defined in the preliminaries, making it accessible for readers with varying backgrounds. The experimental setup is adequately described, which aids in reproducibility, although more details on hyperparameter tuning could enhance the clarity further. The novelty of the approach lies in its unique optimization strategy, which is a significant departure from traditional methods, thereby providing a fresh perspective in the field of sketching algorithms.\n\n# Summary Of The Review\nThis paper presents a significant advancement in the optimization of sketching algorithms by learning both entry values and locations, resulting in improved performance on key tasks. While the methodology is sound and results are promising, the potential limitations regarding data distribution should be addressed for a more comprehensive understanding of the approach's applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel machine learning algorithm designed to enhance performance in classification tasks by leveraging a hybrid approach that combines supervised and unsupervised learning techniques. The authors present a comprehensive framework that includes a detailed description of the algorithm, theoretical underpinnings, and empirical evaluations across multiple benchmark datasets. The findings indicate that the proposed method consistently outperforms existing state-of-the-art techniques, particularly in scenarios with limited labeled data.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative hybrid approach, which effectively addresses the limitations of purely supervised or unsupervised methods by integrating both paradigms. The theoretical justification provided is robust, offering valuable insights that could influence future research directions. Empirical evaluations are well-executed, demonstrating the method's superiority in various contexts. However, a notable weakness is the limited exploration of the algorithm’s performance in real-world applications, as the experiments primarily focus on synthetic and benchmark datasets. Additionally, while the theoretical results are compelling, their applicability in diverse practical scenarios remains to be validated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and generally clear, with a logical flow that guides the reader through the methodology, theoretical analysis, and experimental results. However, certain sections, particularly those detailing experimental setups and parameter choices, could benefit from greater clarity and specificity to enhance reproducibility. The novelty of the proposed algorithm is substantial, offering fresh perspectives on the integration of learning paradigms, although further exploration of its implementation challenges would improve the overall quality of the contribution.\n\n# Summary Of The Review\nThis paper presents a significant advancement in machine learning by proposing a hybrid approach that combines supervised and unsupervised learning techniques. While the theoretical foundations and empirical results are commendable, further exploration of real-world applicability and clearer implementation details would strengthen the contribution.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper explores sketching algorithms that utilize random sketch matrices for efficient data compression and optimization, particularly in low-rank approximation and regression tasks. Building upon the learning-based sketching paradigm established by Indyk et al. (2019), the authors identify a critical gap in previous methodologies that fix the positions of non-zero entries in sketch matrices. They propose novel learning-based algorithms that optimize both the values and positions of these entries in CountSketch, introducing a greedy algorithm for position optimization and more efficient methods that enhance training time and accuracy. Empirical results show significant error reduction and improved performance on real-world datasets compared to classical sketches.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to optimizing both values and positions in sketching algorithms, which offers a substantial performance improvement over traditional methods. The introduction of a greedy algorithm for position optimization adds to the practical applicability of the work. However, a potential weakness is the reliance on limited training data for demonstrating performance improvements. Additionally, while the results are compelling, the paper would benefit from a more extensive evaluation across diverse datasets to further affirm the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a logical flow from problem identification to proposed solutions and empirical validation. The methodology is described sufficiently for reproducibility, although additional details regarding implementation specifics could enhance clarity. The novelty of optimizing both values and positions reflects a significant advancement in the field of sketching algorithms, which is well-supported by empirical evidence. Nevertheless, the reproducibility could be strengthened by providing code or detailed experimental setups.\n\n# Summary Of The Review\nOverall, this paper makes a notable contribution to the field of sketching algorithms by introducing a dual optimization approach that enhances performance in data compression and optimization tasks. While there are minor areas for improvement in terms of dataset diversity and implementation clarity, the findings are compelling and indicative of the proposed methodology's efficacy.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Learning the Positions in CountSketch\" presents a novel approach to sketching algorithms that enhance data compression and optimization tasks, such as low-rank approximation and regression. The authors introduce a framework for learning-based sketching that optimizes both the values and locations of non-zero entries in a CountSketch matrix. The methodology includes an offline sketch learning phase that minimizes expected error on training data and an online application of the learned sketches to new data, with theoretical guarantees established for performance improvements. Empirical results demonstrate significant error reduction and faster training times compared to classical sketching methods, particularly in few-shot learning scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to optimizing the positions of non-zero entries in CountSketch matrices, which is a substantial advancement in the field of sketching algorithms. The theoretical framework provided offers solid performance guarantees, enhancing the credibility of the proposed methods. Additionally, the empirical results on real-world datasets substantiate the claims of improved error rates and efficiency. However, a potential weakness is the reliance on specific distributional assumptions for the theoretical guarantees, which may limit the applicability of the results in more diverse contexts. Furthermore, while the paper provides a comparative analysis with baseline methods, further exploration of scalability and robustness across various datasets could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and results. The quality of writing is high, with clear definitions and explanations of concepts related to sketching algorithms. The novelty of combining learned positions with traditional sketching methods is significant, positioning the work as a meaningful contribution to the field. However, the reproducibility of results could be enhanced by providing more detailed descriptions of the experimental setup, including specific hyperparameter settings and dataset characteristics.\n\n# Summary Of The Review\nOverall, this paper offers a meaningful contribution to the field of sketching algorithms through its innovative learning-based approach to optimizing CountSketch matrices. The theoretical guarantees and empirical results suggest significant improvements in performance, although further exploration of robustness and detailed experimental setups would enhance the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper titled \"Learning the Positions in CountSketch\" presents a novel approach to optimizing non-zero entry locations in the CountSketch algorithm, which is critical for efficient sketching in high-dimensional data. The authors propose two main methodologies: a greedy algorithm and additional techniques for low-rank approximation and second-order optimization. The empirical results demonstrate substantial improvements in performance metrics, such as error reduction percentages, when compared to classical and existing methods, with practical implications for applications like few-shot learning.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its significant contribution to the field of learned sketching algorithms, effectively addressing gaps identified in prior research. The methodologies are well-defined and supported by robust empirical results, showcasing the effectiveness of the proposed approaches. However, one potential weakness is the limited exploration of alternative optimization techniques beyond the greedy approach, which may constrain the breadth of applicability in different contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, providing sufficient detail for readers to understand the methods and results. Technical terminology is appropriately used for the target audience, enhancing the overall clarity. The originality of the proposed methods is commendable, as they introduce novel perspectives on a known problem. The experimental setup is also thoughtfully designed, facilitating reproducibility of results.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the optimization of sketching algorithms, presenting novel methods that are empirically validated with strong results. It addresses existing gaps in the literature while maintaining clarity and reproducibility in its methodology.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper \"Learning the Positions in CountSketch\" by Yi Li et al. introduces a novel learning-based sketching paradigm aimed at improving data compression and optimization problem resolutions, such as low-rank approximations and regression. The authors propose an innovative methodology that optimizes both the values and positions of non-zero entries in sketch matrices, addressing limitations of earlier frameworks that restricted entry locations. Through a two-stage algorithm combining greedy search for entry positions and automatic differentiation for value optimization, the authors demonstrate significant advancements in empirical performance, achieving up to 70% error reduction compared to classical methods and enhancing convergence rates in iterative Hessian sketching scenarios.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear articulation of the limitations of existing sketching methods and the introduction of a comprehensive framework that extends traditional CountSketch techniques. The proposed greedy search for optimizing entry positions is a significant improvement that enhances the algorithm's adaptability and performance. Furthermore, the empirical results substantiate the theoretical claims, providing robust validation across various datasets. However, potential weaknesses include the complexity of the proposed algorithms, which may pose challenges for practical implementation and understanding. Additionally, while the paper provides strong empirical results, further investigation into the scalability of the methods in large-scale applications could enhance its impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The theoretical underpinnings are articulated effectively, although the technical details may be dense for readers less familiar with the underlying mathematical concepts. The novelty of the approach is significant, particularly in the context of learning-based optimization for sketch matrices. The reproducibility of the results is supported by a comprehensive appendix detailing empirical results and experimental parameters, which is commendable.\n\n# Summary Of The Review\nOverall, the paper offers a substantial contribution to the field of sketching algorithms by proposing a learning-based framework that enhances both the adaptability and performance of CountSketch methods. While the theoretical and empirical results are compelling, the complexity of the algorithms may hinder accessibility for some practitioners. The paper is a valuable addition to the literature on optimization techniques and learned sketches.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a novel learning-based sketching algorithm aimed at optimizing non-zero entry positions in matrices. While the authors claim to demonstrate faster running times compared to existing methods, they rely on a narrow dataset selection and do not provide thorough benchmarks or comparisons with state-of-the-art techniques. The methodology presents a greedy optimization approach that, despite its theoretical appeal, suffers from slower training times, and the empirical results lack comprehensive validation and statistical rigor.\n\n# Strength And Weaknesses\nThe main contribution of the paper lies in its proposed sketching algorithm, which aims to improve matrix entry optimization. However, the paper falls short in addressing the significant limitations of existing methods, offering only marginal improvements. The greedy algorithm's slower training time poses a substantial drawback for practical applications. Additionally, the empirical results are not sufficiently comprehensive, relying on a limited number of datasets, which raises concerns about the generalizability of the findings. The lack of rigorous statistical analysis and critical engagement with the existing literature further undermines the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper suffers due to its convoluted organization, making it challenging to follow the main arguments and contributions. Furthermore, the experimental setup lacks sufficient detail, hindering reproducibility. While the paper claims novelty in its approach, the superficial treatment of concepts like \"few-shot learning\" and the reliance on empirical results without robust theoretical backing diminish its overall quality and scientific rigor.\n\n# Summary Of The Review\nOverall, the paper introduces a potentially interesting learning-based sketching algorithm but fails to provide adequate empirical validation and theoretical support for its claims. The organization's shortcomings and insufficient engagement with existing literature detract from the overall impact and credibility of the work.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces a novel learning-based algorithm that optimizes both the values and the positions of non-zero entries in the sketch matrix, marking a significant advancement in sketching techniques. The authors demonstrate that their approach leads to a substantial reduction in errors—up to 70%—for low-rank approximations compared to traditional methods. Additionally, the proposed algorithms exhibit an impressive 87% faster convergence rate in second-order optimization problems such as LASSO. The empirical results, supported by extensive experiments, show strong performance across various applications, including genomics and high-performance computing, particularly in scenarios with limited training data.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its groundbreaking contributions to the field of sketching algorithms, particularly the innovative learning paradigm that optimizes both values and positions. The empirical results are compelling, showcasing superior performance and reduced error rates compared to classical techniques. However, a potential weakness is the complexity of the proposed algorithms, which may pose challenges for practitioners looking to implement them without a deep understanding of the underlying principles. Additionally, while the theoretical guarantees are robust, further exploration of edge cases and practical limitations could strengthen the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The quality of the writing is high, making it accessible to a broad audience. The novelty of the approach is evident, as it introduces a new paradigm in sketching techniques that has not been previously explored. Reproducibility is facilitated by the extensive empirical validation and the clear presentation of results, although providing additional implementation details or code could enhance this aspect further.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in sketching algorithms through novel learning paradigms that optimize both values and positions in sketch matrices. The empirical results are impressive and demonstrate the algorithms' practical applicability across various fields. However, the complexity may limit accessibility for some practitioners.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper presents a novel learning-based approach to optimize the positions of non-zero entries in CountSketch matrices, a critical aspect in sketching algorithms used for data compression and optimization tasks. The authors propose a greedy algorithm that dynamically adjusts these positions to improve the performance of the sketch matrices, particularly under specific input distributions. The theoretical framework demonstrates strong guarantees for the proposed method's effectiveness, while empirical results show significant error reduction compared to traditional approaches.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its theoretical contributions, particularly the introduction of a greedy optimization algorithm that enhances the utility of CountSketch matrices. The paper effectively combines theoretical insights with empirical validation, establishing a solid basis for its claims. However, a potential weakness is that the empirical results, while promising, may benefit from broader experimentation under diverse conditions to validate the robustness of the proposed approach. Additionally, the reliance on specific distributional assumptions may limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodologies. The theoretical framework is robust, and the empirical results offer supplementary evidence supporting the theoretical claims. The novelty of the approach is evident in the proposed optimization strategy, which distinguishes it from classical sketching methods. Reproducibility is facilitated through the detailed presentation of the methodology, though additional information on experimental setups would enhance confidence in replicability.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of sketching algorithms by introducing a learning-based optimization technique for CountSketch matrices. The strong theoretical grounding and promising empirical results position this work as a valuable addition to the literature, though further empirical validation could enhance its applicability.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing CountSketch matrices using learning-based techniques. The main contributions include the development of algorithms for low-rank approximation and Hessian approximation that focus on both the values and positions of non-zero entries in CountSketch matrices. The methodology involves a two-stage optimization: first, a greedy algorithm is employed to optimize the positions of non-zero entries, followed by gradient descent to learn their values using PyTorch. Experimental results demonstrate significant improvements in approximation errors compared to traditional CountSketch methods.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to enhancing classical sketching techniques through learning-based methods, which demonstrates practical applicability and improved performance. The use of a greedy algorithm for position optimization and gradient descent for value optimization showcases a thoughtful combination of strategies. However, one weakness is the limited discussion on the broader implications of the proposed methods and how they compare to potential alternatives in a wider context. Furthermore, while the empirical results are promising, the paper could benefit from a deeper exploration into the scalability of the proposed methods across diverse data types and sizes.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and communicates its findings clearly, with a logical flow from methodology to results. The quality of the experimental setup is commendable, and the use of PyTorch enhances the reproducibility of the work, as it provides a detailed implementation of the algorithms. However, some technical aspects, such as the specificities of the greedy algorithm's proxy objective, could be elaborated further to improve understanding. Overall, the novelty of using learned sketching for CountSketch is significant, although additional context on its advantages over existing methods would strengthen the contribution.\n\n# Summary Of The Review\nOverall, this paper provides a valuable contribution to the field of sketching techniques by introducing learning-based optimizations for CountSketch matrices. The methodology is robust, and the empirical results indicate significant improvements over traditional methods. However, broader implications and a more comprehensive discussion of alternatives could enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing the CountSketch matrix, which enhances the learning-based sketching paradigm introduced by Indyk et al. (2019). The primary contributions include a greedy algorithm that optimizes both the values and positions of non-zero entries in the sketch, alongside claims of significant improvements in low-rank approximation and second-order optimization. The authors assert that their method offers faster runtime and decreased error rates compared to prior works, although these claims are not sufficiently substantiated.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its ambition to advance the CountSketch methodology and the introduction of new learning objectives. However, there are notable weaknesses, including a lack of rigorous comparative analysis with existing algorithms, insufficient context for claimed improvements, and limited experimental evaluation across diverse datasets. The emphasis on novelty may also overshadow similar ideas in prior literature, resulting in an unbalanced perspective on the contributions made.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, some claims regarding novelty and effectiveness are not adequately backed by comparative analysis or comprehensive evaluation. The methodology lacks depth in discussing the implications of their findings in the context of existing research, which could hinder reproducibility. The clarity of the proposed contributions may be undermined by the selective presentation of results and comparisons.\n\n# Summary Of The Review\nOverall, the paper presents interesting ideas in optimizing CountSketch matrices but fails to convincingly demonstrate improvements over existing methods. The lack of rigorous evaluation and contextualization with prior work diminishes the perceived significance of the contributions.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Learning the Positions in CountSketch\" presents a novel approach to optimizing the position encoding in CountSketch algorithms. The authors propose a new methodology that incorporates machine learning techniques to learn optimal positions for the sketching matrix. The findings demonstrate significant improvements in performance metrics, particularly in tasks involving high-dimensional data, outperforming traditional CountSketch methods and existing learned approaches.\n\n# Strength And Weaknesses\nOne of the main strengths of the paper is its innovative use of machine learning to enhance the performance of CountSketch, a widely used technique in streaming algorithms. The empirical results are compelling, showcasing improvements in accuracy and efficiency. However, a notable weakness is the lack of theoretical guarantees for the proposed approach, which may limit its applicability in certain contexts. Additionally, some experimental setups lack sufficient detail, making it difficult to fully assess the robustness of the findings across varying conditions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the methodology and results. However, there are instances where clarity could be improved, particularly in the explanation of mathematical concepts and proofs. The novelty of the approach is significant, as it combines established algorithms with modern machine learning techniques. Reproducibility is somewhat hindered by the limited details in the experimental section, as well as some inconsistencies in notation and terminology throughout the paper.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of streaming algorithms through its innovative use of machine learning in CountSketch. While the results are promising, the lack of theoretical backing and some ambiguities in the experimental framework raise concerns about the broader applicability of the findings. Further refinement in clarity and detail would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates low-rank approximation and second-order optimization within the context of learning-based sketching. The authors propose a greedy algorithm to optimize non-zero entry positions in sketching matrices, aiming to enhance computational efficiency. Empirical findings suggest that their methods perform well on specific datasets, yet the discussion lacks depth regarding real-world applications, robustness against noise, and scalability to larger datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to optimizing sketching matrices and its promising empirical results. However, it has several weaknesses, including a limited exploration of alternative optimization problems, insufficient scalability analysis, and a lack of comprehensive comparisons with existing methods. The paper also fails to address the implications of its findings for real-world applications and neglects to discuss the robustness of the proposed methods against various data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally acceptable, though some sections could benefit from more detailed explanations, particularly regarding the scalability and robustness of the proposed methods. The quality of the empirical results is promising, yet the novelty is somewhat diminished by the lack of comprehensive comparisons with other state-of-the-art techniques. Reproducibility is not explicitly addressed, as the paper does not provide enough detail on the datasets and experiment setups used.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to learning-based sketching with promising empirical results; however, it falls short in several critical areas, including scalability, robustness, and real-world applicability. A more thorough exploration of these aspects could significantly enhance the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThis paper presents a learning-based approach to sketching algorithms, specifically focusing on optimizing the positions in CountSketch. The authors propose a methodology that emphasizes the importance of distributional assumptions and leverage scores for sampling, with the primary goal of minimizing expected error across various instances. Theoretical guarantees are provided to support the performance of the proposed algorithms, and empirical evaluations demonstrate significant improvements over classical CountSketch methods, achieving error reductions of up to 70%. The work is grounded in statistical principles such as empirical risk minimization and sensitivity analysis.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its rigorous statistical framework, which effectively integrates theory and empirical validation. The use of leverage scores for guiding sampling is particularly noteworthy, as it builds on established statistical concepts to enhance performance. Additionally, the thoroughness of theoretical guarantees provides confidence in the robustness of the proposed methods. However, one potential weakness is the reliance on specific distributional assumptions, which may limit the generalizability of the results. Furthermore, while the empirical results are compelling, additional datasets and scenarios could strengthen the validation of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings, making it accessible to readers familiar with the domain. The quality of the writing is high, with a logical flow that guides the reader through the complex statistical concepts. The novelty of the approach is significant, as it extends traditional sketching methods by incorporating learning principles. The reproducibility of the results is supported by the comprehensive presentation of empirical evaluations; however, further details on implementation specifics and dataset characteristics would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of sketching algorithms by integrating learning principles with statistical rigor. The theoretical and empirical results indicate a promising direction for future research, although the reliance on specific distributional assumptions and the need for broader empirical validation are notable considerations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to learning sketch matrices for low-rank approximation and second-order optimization. It introduces a new learning objective that emphasizes the subspace embedding property, allowing the model to optimize both the values and locations of non-zero entries in the sketch matrix. The proposed methodology shows promise in few-shot learning scenarios, although it operates under specific distributional assumptions and presents challenges related to training time and empirical validation.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its advancement of sketch matrix learning, addressing limitations of previous algorithms by optimizing entry locations. However, the paper has notable weaknesses, including a slower training time due to the initial greedy algorithm and a lack of exploration regarding alternative learning objectives. Additionally, the reliance on specific data distributions raises concerns about the generalizability and robustness of the methods. The empirical validation is limited, and the potential for overfitting in real-world applications is not adequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its concepts clearly. However, the novelty of the approach is somewhat diminished by the lack of comprehensive testing across diverse datasets and the limited exploration of its applicability to other optimization problems. While the theoretical results are strong, the reproducibility may be hindered by the specific distributional assumptions and the dependency on accurately identifying heavy leverage scores.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of sketch matrix learning, offering a novel methodology that optimizes both values and locations. However, it also faces significant limitations, including training time concerns, a lack of empirical validation, and potential overfitting issues. Further exploration of robustness and generalization across diverse datasets would strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing non-zero entry locations in CountSketch, a widely used sketching algorithm for data compression. The authors propose a gradient descent method for this optimization and introduce a greedy algorithm to enhance performance, claiming to achieve good accuracy with fast running times. The empirical results indicate a significant reduction in error compared to traditional methods, although the authors do not provide groundbreaking insights into the optimization process itself.\n\n# Strength And Weaknesses\nWhile the paper contributes to the field of sketching algorithms by optimizing non-zero entry locations, the methodology lacks true novelty. The proposed greedy algorithm and optimization techniques are presented without substantial differentiation from existing approaches. The paper's strengths lie in its clear empirical results and comparisons with classical methods, but its weaknesses include a lack of innovative ideas and an overstatement of the significance of its contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a clear and structured manner, making it accessible to readers familiar with sketching algorithms. The quality of the experiments appears satisfactory, although the novelty of the approach is questionable, as it builds upon well-established concepts. Reproducibility seems feasible given the standard notation and methodologies employed, but the novelty of the findings does not warrant a high significance.\n\n# Summary Of The Review\nOverall, the paper presents a competent yet unoriginal contribution to the field of sketching algorithms. While the optimization of non-zero entry locations is a relevant topic, the methods and findings do not offer significant advancements beyond existing literature.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to optimizing non-zero entry locations in CountSketch matrices, aiming to enhance the performance of sketching techniques in high-dimensional data settings. The methodology involves empirical experimentation with existing optimization strategies, demonstrating improved error rates in approximation tasks. Key findings indicate that learned sketching methods can significantly reduce errors compared to traditional CountSketch implementations, though the paper primarily relies on conventional optimization techniques without exploring more adaptive frameworks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its empirical validation of learned sketching methods, which showcases their effectiveness over standard techniques. However, the paper has notable weaknesses, including a limited exploration of alternative learning frameworks, such as reinforcement learning or neural networks, which could provide more flexibility and adaptability in learning sketch matrix structures. Moreover, the theoretical guarantees provided are insufficiently rigorous, and the exploration of cross-domain applications and real-time implementations is lacking. The paper would benefit from broader empirical validation across diverse datasets to strengthen claims of generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, presenting its methodologies and findings in an accessible manner. However, the novelty is somewhat constrained by the reliance on traditional optimization techniques, limiting the exploration of more innovative approaches. Reproducibility could be enhanced by providing additional details on experimental setups and parameter choices, as well as by including a more comprehensive analysis of computational complexity compared to classical sketches.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of learned sketching in optimization problems, demonstrating improvements in error rates with its proposed methods. However, it lacks depth in exploring flexible learning frameworks and theoretical guarantees, which limits its potential impact. The paper would benefit from broader empirical testing and a more rigorous theoretical foundation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces innovative learning-based sketching algorithms aimed at enhancing low-rank approximation (LRA) and second-order optimization tasks. The authors present empirical results indicating a significant reduction in error rates—approximately 70% lower compared to classical sketching methods and a 30% improvement over previous learning-based techniques (specifically IVY19). They demonstrate that while a greedy algorithm provides good performance, a faster inner product method offers a substantial improvement in runtime, maintaining comparable accuracy. Additionally, the proposed second-order optimization approach shows a convergence rate that is 87% faster than the traditional non-learned CountSketch, making it effective even in few-shot learning scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its strong empirical results showcasing the proposed algorithms' effectiveness across various datasets and tasks. The significant performance improvements over traditional and prior learning-based methods emphasize its contributions to the field. However, a potential weakness is the longer training time associated with the greedy algorithm, which may limit its practical applicability in time-sensitive scenarios. Additionally, while the results are promising, the paper could benefit from a more extensive discussion on the limitations and potential trade-offs of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly presents its methodology and findings. The empirical evaluations are thorough, which enhances the quality of the work. The novelty of the proposed algorithms is evident; however, details regarding the implementation and reproducibility could be improved. Specifically, providing more information on the experimental setup and dataset characteristics would facilitate replication of the results.\n\n# Summary Of The Review\nOverall, the paper presents a noteworthy advancement in learning-based sketching algorithms for low-rank approximation and optimization tasks, backed by strong empirical evidence. While the results are compelling and demonstrate significant improvements over existing methods, the paper could enhance its clarity regarding reproducibility and discussion of limitations.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel algorithm for approximate matrix multiplication using sketching techniques, specifically focusing on the optimization of computational efficiency. The authors introduce a new framework that integrates various sketching methods, including CountSketch and randomized sparse matrices, to achieve improved performance in large-scale data processing tasks. Empirical evaluations demonstrate that the proposed method significantly reduces the computational overhead while maintaining a high level of accuracy compared to existing approaches.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to combining multiple sketching methods, leading to enhanced efficiency in matrix multiplication tasks. The empirical results support the theoretical claims, showcasing substantial improvements in runtime across various datasets. However, the paper suffers from clarity issues, particularly in the abstract and introduction, which could confuse readers unfamiliar with the terminology. Additionally, the lack of clear differentiation between various sketching techniques may hinder comprehension. The experimental section also requires more structured details to facilitate understanding of the conducted tests.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is significantly hampered by convoluted language and inconsistent terminology, which could alienate readers not deeply versed in the field. While the novel combination of sketching techniques is noteworthy, the methodology section could benefit from more straightforward explanations alongside mathematical notation to enhance accessibility. The reproducibility of the findings is questionable due to the insufficient detail in the experimental setup, which does not provide a comprehensive view of the parameters and methodologies employed.\n\n# Summary Of The Review\nOverall, while the paper introduces an intriguing and potentially impactful algorithm for matrix multiplication, its clarity and presentation detract from its contributions. Improving the accessibility of its content and providing a more structured overview of experimental methods would enhance the paper’s overall quality and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.16735321437348,
    -1.7215917697421514,
    -2.022347946019932,
    -1.7889107705505147,
    -2.0188947161034747,
    -1.6268474876050063,
    -1.6032283178371936,
    -2.0550688599396403,
    -1.8154343785428373,
    -1.8944488784140225,
    -1.762640582915062,
    -1.487947685865895,
    -1.816205936987967,
    -1.6266675987098174,
    -2.0618208920958563,
    -1.9102934081617733,
    -2.026996227753866,
    -1.7175250890994262,
    -1.70777702913219,
    -1.7342802173345042,
    -2.1119875066016434,
    -1.5893401145324295,
    -1.637096453991942,
    -1.5801859529458566,
    -1.9362246803095207,
    -1.8233309218877352,
    -2.0156193565971146,
    -1.9869244522801588,
    -1.743166055111564
  ],
  "logp_cond": [
    [
      0.0,
      -2.000553158057346,
      -2.002056666297112,
      -2.0256901700090126,
      -1.9913510838644566,
      -2.019760341029932,
      -2.080556413263529,
      -2.012647106714157,
      -2.004137408020644,
      -2.0239392415671675,
      -2.016700573357598,
      -2.0792580844105157,
      -2.0126577332728095,
      -2.0161182354597895,
      -2.0201009659191684,
      -2.0034210765006284,
      -2.035751721851804,
      -2.03393685986365,
      -2.0359905277670984,
      -2.0208481567340812,
      -2.0067707566343116,
      -2.0273051911438573,
      -2.045566664301407,
      -2.036632661020776,
      -2.0177856435680397,
      -2.0551131507091256,
      -2.033193931186678,
      -1.9849523020279514,
      -2.056869461257326
    ],
    [
      -1.359008596927968,
      0.0,
      -1.2265353212003756,
      -1.2842832483796887,
      -1.2001050330410803,
      -1.275175513816486,
      -1.4461988388678984,
      -1.2566799721678585,
      -1.2604705840754657,
      -1.300800784436048,
      -1.23073322457645,
      -1.4557041513426707,
      -1.237537047680075,
      -1.2828331230473486,
      -1.2804115601732946,
      -1.2409560552674028,
      -1.3475633530782465,
      -1.2928281203734469,
      -1.3626866956514232,
      -1.3214810360729163,
      -1.2786111180868838,
      -1.382202488810752,
      -1.3560177552098271,
      -1.3375325535121292,
      -1.3212106534223085,
      -1.3660500807230789,
      -1.311093023138011,
      -1.2234723708034903,
      -1.3392186237297978
    ],
    [
      -1.6183950609741902,
      -1.484794387235691,
      0.0,
      -1.4686590144728358,
      -1.4605537604626637,
      -1.485124266107375,
      -1.7094704215706757,
      -1.4678648176586166,
      -1.4349499122693978,
      -1.5501174293113604,
      -1.533070064348727,
      -1.7153373670954297,
      -1.4390649877188586,
      -1.512347568450834,
      -1.4781914783268797,
      -1.3762782999445593,
      -1.5486123559868636,
      -1.5372003366839246,
      -1.579769450532618,
      -1.595302616435038,
      -1.5680650942826,
      -1.5821406528889703,
      -1.611180171649606,
      -1.5659275166392317,
      -1.5282914469781417,
      -1.6076181078174332,
      -1.6438057338307115,
      -1.4824533272075797,
      -1.629966943792371
    ],
    [
      -1.4560437732851796,
      -1.3561132181137427,
      -1.2383026810009226,
      0.0,
      -1.3294788112572253,
      -1.2582575573992507,
      -1.4492829668317693,
      -1.2964195966332002,
      -1.2195505018972306,
      -1.4019504142958525,
      -1.3196003420746674,
      -1.4650173636164292,
      -1.2805862076691814,
      -1.201099762215727,
      -1.3393092062736345,
      -1.1671820510355302,
      -1.413052929230308,
      -1.358544013046199,
      -1.3808010951050353,
      -1.3364618636203778,
      -1.3750762497766325,
      -1.4332254124129258,
      -1.4423978623762777,
      -1.368532995361414,
      -1.4168149905709284,
      -1.405856823015051,
      -1.4254175714524033,
      -1.4050915437665095,
      -1.42317513010453
    ],
    [
      -1.5828610712993287,
      -1.436028317350972,
      -1.3836754708667267,
      -1.4124195929060592,
      0.0,
      -1.482741983230884,
      -1.6511015286093473,
      -1.3642892834383558,
      -1.429964439561083,
      -1.4833937128037047,
      -1.4740005556523543,
      -1.641737685620664,
      -1.4804285001000608,
      -1.411284817651017,
      -1.4918305247693677,
      -1.424874430766375,
      -1.5307009265022504,
      -1.4954247714547035,
      -1.5510956237296414,
      -1.518268166394057,
      -1.4563067892939885,
      -1.5564813010733631,
      -1.5580206182245644,
      -1.55884940447715,
      -1.4276423402959113,
      -1.6291622514572095,
      -1.5628286500643993,
      -1.4731743170279785,
      -1.5806199312215516
    ],
    [
      -1.2772230754288096,
      -1.1744822491997622,
      -1.0857970803217893,
      -1.0687364961541692,
      -1.154403071488664,
      0.0,
      -1.2989102807547166,
      -1.135032352381211,
      -1.1362172546161025,
      -1.2137444213242223,
      -1.2045765550860685,
      -1.2839354914974574,
      -1.1212791756694622,
      -1.0712326345142082,
      -1.17248601232402,
      -1.1337662035571965,
      -1.2697761835173904,
      -1.2204476203758332,
      -1.2451109787781813,
      -1.2519526933718752,
      -1.1910693170830255,
      -1.2105297310031635,
      -1.277432662766135,
      -1.2265168716964865,
      -1.2556007574193924,
      -1.21593358180344,
      -1.25570316347124,
      -1.2694874658942827,
      -1.2885882355851999
    ],
    [
      -1.379319893809413,
      -1.3187911240610084,
      -1.2926913396496462,
      -1.2678768234685125,
      -1.266671777327373,
      -1.2779790830845046,
      0.0,
      -1.3031419852605275,
      -1.310435796843053,
      -1.280495762490707,
      -1.3373742570470286,
      -1.3312825892025486,
      -1.3035274743104448,
      -1.2829116624985768,
      -1.3326528339858397,
      -1.278197796129004,
      -1.3473934984982923,
      -1.298833100044563,
      -1.316122904178577,
      -1.3435206080632711,
      -1.3165281001613065,
      -1.3382898985531146,
      -1.3515832251989668,
      -1.362247826408522,
      -1.3163908419333268,
      -1.3469353904619452,
      -1.3259935798820663,
      -1.332839001260848,
      -1.3461213439487678
    ],
    [
      -1.6798433432186348,
      -1.5384358372902376,
      -1.490364056004211,
      -1.555894719276934,
      -1.5059200454002342,
      -1.5573566196850162,
      -1.7389970824463816,
      0.0,
      -1.5562475933370585,
      -1.6352202813790886,
      -1.5857449620535233,
      -1.7642469417374549,
      -1.5529247200634149,
      -1.536535219557141,
      -1.5472109437721733,
      -1.5415109243719163,
      -1.6409075900766452,
      -1.6262591166257911,
      -1.6469225650015584,
      -1.6710472650096644,
      -1.574693208526682,
      -1.6554757502986428,
      -1.7076917776471074,
      -1.6240782317084774,
      -1.6408375188640887,
      -1.6374060253884688,
      -1.6404720978509033,
      -1.615611323822779,
      -1.6750305320767618
    ],
    [
      -1.4684994359352728,
      -1.3382220161488592,
      -1.2783437597190113,
      -1.282185447011517,
      -1.3548749690376807,
      -1.326718603210822,
      -1.5492245940445397,
      -1.3456497986408575,
      0.0,
      -1.4393420266278165,
      -1.3928673410482078,
      -1.5622360250814038,
      -1.2739169540330155,
      -1.3492728842421562,
      -1.3795585886570938,
      -1.2320868331578758,
      -1.4229215593291211,
      -1.3901296774576466,
      -1.4944563475134685,
      -1.4471646672298113,
      -1.3476006065870465,
      -1.4609690861945999,
      -1.4599316568352645,
      -1.474558343295974,
      -1.4378205343662112,
      -1.4465816413796562,
      -1.4654033848830583,
      -1.4022515791607575,
      -1.4503376136342716
    ],
    [
      -1.5656812685610242,
      -1.4982636794905582,
      -1.4713402877849746,
      -1.5438292002001601,
      -1.4380857965692737,
      -1.5175891269267137,
      -1.620182816588664,
      -1.4682996453628316,
      -1.5068491520036746,
      0.0,
      -1.532080811716278,
      -1.649495917563174,
      -1.496769181275756,
      -1.4893824201100498,
      -1.4611430759739634,
      -1.4719951136539773,
      -1.5106068674341737,
      -1.5398343312210103,
      -1.5602964044495216,
      -1.5435240146908136,
      -1.4630806571898964,
      -1.5791383784613606,
      -1.5125412345719336,
      -1.538622356563527,
      -1.4261560106455728,
      -1.5507247159929578,
      -1.4990692646945174,
      -1.515311940417056,
      -1.5541310246060531
    ],
    [
      -1.3693228462675564,
      -1.2679440255714918,
      -1.3174933679234564,
      -1.2426108174557375,
      -1.259059112225197,
      -1.3185521602951347,
      -1.5163582301045242,
      -1.3052510185868653,
      -1.29277648873851,
      -1.3293724347131015,
      0.0,
      -1.5136537386696383,
      -1.2406493271249273,
      -1.309358367013677,
      -1.262850173402253,
      -1.2770337177019133,
      -1.3351126833052822,
      -1.325481146653173,
      -1.4151671896385423,
      -1.321508551703002,
      -1.2593593239771956,
      -1.3765550355743532,
      -1.3709941034209303,
      -1.4123205009264421,
      -1.3548726214675013,
      -1.3911113365710799,
      -1.3539591702927818,
      -1.3053587347843687,
      -1.3916870742110319
    ],
    [
      -1.2188188576456171,
      -1.1861899078935638,
      -1.148873764701163,
      -1.1436904451953276,
      -1.1270314159513533,
      -1.1362792010136924,
      -1.146422958869472,
      -1.1737228250036036,
      -1.1446038355720018,
      -1.172733715349242,
      -1.1931547672164347,
      0.0,
      -1.1666716508686212,
      -1.1049300188535842,
      -1.1938626156358119,
      -1.1775303591201118,
      -1.1548086188852955,
      -1.1839349511775534,
      -1.2008568244599325,
      -1.1900673716193504,
      -1.1924799599915352,
      -1.1945578138143187,
      -1.2068272549040135,
      -1.1833931291610278,
      -1.2249636409525058,
      -1.1941605876245573,
      -1.1972103325291292,
      -1.1778167322063517,
      -1.1218540206866632
    ],
    [
      -1.4460489815024857,
      -1.3340038223879054,
      -1.2996680421019142,
      -1.2805976767341227,
      -1.3577444124524853,
      -1.3249155260995302,
      -1.5674332738328232,
      -1.3507801593101711,
      -1.2543890185566056,
      -1.3955831075031109,
      -1.3482010662643962,
      -1.5434385117065106,
      0.0,
      -1.3111049759550732,
      -1.3562080845730242,
      -1.305971047522455,
      -1.4191083524521144,
      -1.3450183269401847,
      -1.3867990432578192,
      -1.3743816487608476,
      -1.330757766726546,
      -1.4411170170500716,
      -1.493384159083758,
      -1.4056191619973541,
      -1.4323330521176394,
      -1.4389579770590668,
      -1.4233915949678384,
      -1.402686865400277,
      -1.4561500180461564
    ],
    [
      -1.3097421174476371,
      -1.223305752940475,
      -1.166128294581828,
      -1.0932863315977028,
      -1.1804435342441653,
      -1.1492507156295504,
      -1.3330508758484598,
      -1.119990425854309,
      -1.1781938994234566,
      -1.2594257051776392,
      -1.2510184362090886,
      -1.3441380540967145,
      -1.1430652392296194,
      0.0,
      -1.236774613032265,
      -1.1554747799383458,
      -1.273157973489187,
      -1.2500114704624399,
      -1.2261405748842045,
      -1.2909918048743179,
      -1.2412441706927313,
      -1.293899928162092,
      -1.3129345285892875,
      -1.2402181676025137,
      -1.2623762007287185,
      -1.2842720252627278,
      -1.2895447428026239,
      -1.254630661127765,
      -1.3368382717907068
    ],
    [
      -1.6370414891137801,
      -1.57473496254798,
      -1.4622850790954771,
      -1.5082228607416628,
      -1.482958573424936,
      -1.5518133237626393,
      -1.7247523893378558,
      -1.5263950337262064,
      -1.5324067815101976,
      -1.552340705063995,
      -1.5282729680909606,
      -1.742089928673301,
      -1.5686411311362456,
      -1.51753879356055,
      0.0,
      -1.5161754268058596,
      -1.584530188578819,
      -1.6094337952192208,
      -1.6159607682357777,
      -1.5849759546809434,
      -1.546557045280512,
      -1.5635127478271589,
      -1.583115565492531,
      -1.6148150797021887,
      -1.567303767593493,
      -1.6100447395729565,
      -1.5346118469199934,
      -1.5560220840746881,
      -1.6409276276726334
    ],
    [
      -1.559195863377568,
      -1.4512930305546772,
      -1.3661011144094928,
      -1.3263226920635773,
      -1.457674939843868,
      -1.4486806928964433,
      -1.6136762565272584,
      -1.4443263947302827,
      -1.3730349384248342,
      -1.5214275228845295,
      -1.4534748356159224,
      -1.6327470799048351,
      -1.3940608927662548,
      -1.392758922992541,
      -1.4717120044701466,
      0.0,
      -1.5297106487887238,
      -1.4356432917703594,
      -1.5463853827478153,
      -1.5288829087368987,
      -1.499534874661403,
      -1.5361730487746024,
      -1.5669713688970122,
      -1.4825916910958459,
      -1.5303933811710835,
      -1.5821430147267317,
      -1.5306961949717162,
      -1.4917053733347279,
      -1.595304090018703
    ],
    [
      -1.710878482724712,
      -1.5892160791429994,
      -1.5666876450581027,
      -1.6548396091171915,
      -1.5325899300851813,
      -1.649247180437367,
      -1.759693445059871,
      -1.5560853516761017,
      -1.599938022943699,
      -1.5889118024487692,
      -1.6062960791575498,
      -1.768108238457808,
      -1.557720814802929,
      -1.6716274230447685,
      -1.5785910075680802,
      -1.5726680835972981,
      0.0,
      -1.6219942292527287,
      -1.6421572622945908,
      -1.6312990277497081,
      -1.5814230902988886,
      -1.6903254803856649,
      -1.5640467722273843,
      -1.6750088601795579,
      -1.5794200044963402,
      -1.6870926997615803,
      -1.6076421657163966,
      -1.5580170638868382,
      -1.6760617982893438
    ],
    [
      -1.4008104426828065,
      -1.2812173927152475,
      -1.242734199273706,
      -1.301981666270328,
      -1.2186335602793774,
      -1.3212580395004205,
      -1.424738950320796,
      -1.2367861526424504,
      -1.2799736512252013,
      -1.3122318827177566,
      -1.3033533870315523,
      -1.4486910997681188,
      -1.268684908348759,
      -1.2962260407730044,
      -1.285520429826822,
      -1.2685190167567821,
      -1.3338646543319252,
      0.0,
      -1.3563951552397921,
      -1.3242939375259222,
      -1.281007591489956,
      -1.3630973092097551,
      -1.356389991954202,
      -1.323908820791057,
      -1.3076991777313691,
      -1.397001974985348,
      -1.3350210588075926,
      -1.2429448216449184,
      -1.3822221692211187
    ],
    [
      -1.3812998128993306,
      -1.3383415269457688,
      -1.2634149443259315,
      -1.2731182937662557,
      -1.2874531761392203,
      -1.2765268181244818,
      -1.4623638864169834,
      -1.2792591623939558,
      -1.266798937333755,
      -1.3299978843060294,
      -1.3657133265745696,
      -1.5061812772257375,
      -1.2213541904190106,
      -1.212149007014781,
      -1.3107121345815411,
      -1.2307718609977358,
      -1.3659671531150999,
      -1.3243120381088236,
      0.0,
      -1.3607178779995728,
      -1.3198608968996646,
      -1.3605114091073052,
      -1.3736925613571827,
      -1.3051298499545347,
      -1.4121224106295311,
      -1.30171366430247,
      -1.3365380492119934,
      -1.3642224024192438,
      -1.4083883022604375
    ],
    [
      -1.3614302800444256,
      -1.3240650161701244,
      -1.2626853022416582,
      -1.2733195511552435,
      -1.2838457316599363,
      -1.3324100747018197,
      -1.4509971550082625,
      -1.2884405316059284,
      -1.3526173237585157,
      -1.324604030393914,
      -1.324175077479448,
      -1.4720918879913973,
      -1.31059163431439,
      -1.3121717032625135,
      -1.3026926746584135,
      -1.2664958591856779,
      -1.3542923786216507,
      -1.3387506916123528,
      -1.3444363109869735,
      0.0,
      -1.2707649025488759,
      -1.3636764256916085,
      -1.3512535879058463,
      -1.355386651738705,
      -1.3911305278813355,
      -1.3564272833707878,
      -1.294538639863835,
      -1.333227837977753,
      -1.364397575119254
    ],
    [
      -1.694647841203876,
      -1.5696883467718736,
      -1.5981917207382499,
      -1.6484870151361306,
      -1.5448348844460793,
      -1.6231758253311652,
      -1.8249165083064063,
      -1.5653041961208414,
      -1.6026552215213872,
      -1.6302034373643735,
      -1.5825839791386953,
      -1.890933467348856,
      -1.5662819712744893,
      -1.6582292641410092,
      -1.5942992480474885,
      -1.6246213075486327,
      -1.6926399736113646,
      -1.6276184922715355,
      -1.6732261983849774,
      -1.6157122241798911,
      0.0,
      -1.7152015889522265,
      -1.7014637654451057,
      -1.6963492115251786,
      -1.6565810987610436,
      -1.7001299080429424,
      -1.6734344635197793,
      -1.643036206376253,
      -1.729510081811248
    ],
    [
      -1.2337903429819137,
      -1.208700896022084,
      -1.13679905460038,
      -1.1571765083719285,
      -1.1364593679227246,
      -1.1536626571300657,
      -1.269034308219252,
      -1.1271332718225777,
      -1.164071874352916,
      -1.1951973463380126,
      -1.2000954346142834,
      -1.256946005726467,
      -1.212040060554528,
      -1.1425328070742768,
      -1.0727154204513576,
      -1.1441413165557506,
      -1.2084249153357345,
      -1.20350932741382,
      -1.246572057685343,
      -1.2043564635031019,
      -1.2134303578290289,
      0.0,
      -1.2538126461101884,
      -1.1750027526740192,
      -1.2205588396963472,
      -1.2387743181746351,
      -1.196666374586095,
      -1.2041960531867584,
      -1.1680467736144267
    ],
    [
      -1.2770787033658393,
      -1.1797460676672822,
      -1.1956738499857886,
      -1.1767732592085174,
      -1.1804367289606261,
      -1.2364533751774514,
      -1.35673378395397,
      -1.1795861398486167,
      -1.189830342424026,
      -1.1639473066086505,
      -1.176366297703586,
      -1.3464606882786445,
      -1.1815191491634864,
      -1.2240947734183316,
      -1.1218626405587977,
      -1.1883328956595405,
      -1.1562081059428793,
      -1.1793132273186808,
      -1.2137911025027555,
      -1.1582696147579756,
      -1.1448091245253005,
      -1.250050080859375,
      0.0,
      -1.2285791334443166,
      -1.1637315608572976,
      -1.2617159658096746,
      -1.1304725192850194,
      -1.1541276603473691,
      -1.252406090956212
    ],
    [
      -1.2472430368798868,
      -1.244214276772459,
      -1.1648342611714744,
      -1.2095410713344017,
      -1.2076743799506853,
      -1.2106829789907532,
      -1.3523996783539158,
      -1.2008323967882095,
      -1.2176833915363432,
      -1.2631972579974793,
      -1.2859463031492013,
      -1.3447365318461244,
      -1.2086456942339552,
      -1.17548892852957,
      -1.22684434994712,
      -1.1418098821585785,
      -1.2635652430033766,
      -1.2310862447054198,
      -1.2741552195114043,
      -1.268577015321993,
      -1.269286408222883,
      -1.2467888065934367,
      -1.2700466373985826,
      0.0,
      -1.2593091849573708,
      -1.270545013345141,
      -1.2541840027078508,
      -1.231552201190942,
      -1.265833879295396
    ],
    [
      -1.5470213018617867,
      -1.4836822747898915,
      -1.44295714623219,
      -1.515594634149922,
      -1.4078078886995604,
      -1.5409262605020893,
      -1.6105129113852168,
      -1.4873272115599363,
      -1.5317892287778205,
      -1.401546743530042,
      -1.496550140063913,
      -1.699138095171593,
      -1.5171414905378036,
      -1.4860149827912077,
      -1.5037975917947597,
      -1.5099540702326135,
      -1.5156507409604647,
      -1.5107802579913818,
      -1.564350714574831,
      -1.559401678718842,
      -1.476544291285558,
      -1.5610957504179868,
      -1.514646017839646,
      -1.53045134742201,
      0.0,
      -1.5951485763511126,
      -1.521220435103397,
      -1.505182391260581,
      -1.5859905949828461
    ],
    [
      -1.51640433396639,
      -1.3703743010264116,
      -1.3072537286675125,
      -1.3233085604155392,
      -1.354212252893308,
      -1.3239882159432987,
      -1.4963174986952674,
      -1.349970442653093,
      -1.3722014027434892,
      -1.4039483365666128,
      -1.388457467967539,
      -1.5089574156940844,
      -1.3390046650880814,
      -1.3470315489876328,
      -1.28944116396494,
      -1.3488713227094913,
      -1.4238622842626247,
      -1.4173017922538778,
      -1.356072837958925,
      -1.4050405668200323,
      -1.3717013315251843,
      -1.462320079942348,
      -1.4177287132119907,
      -1.4391255237484781,
      -1.4249683884878779,
      0.0,
      -1.3887852899389408,
      -1.4074066553172702,
      -1.4445825396324714
    ],
    [
      -1.690103438509723,
      -1.5486487811279754,
      -1.554660961637391,
      -1.591649053373924,
      -1.5517669174523367,
      -1.6057392627104061,
      -1.707194720436412,
      -1.5234391507277458,
      -1.5547318939248127,
      -1.5427992269209305,
      -1.5708952437577144,
      -1.7473181990426052,
      -1.5532458495816246,
      -1.5813052368428977,
      -1.4941348010507214,
      -1.552175002942518,
      -1.6177520986174225,
      -1.6052327880273143,
      -1.6014929818530164,
      -1.534436894012532,
      -1.5505744155859824,
      -1.6211573744302135,
      -1.594770688387814,
      -1.588897881752802,
      -1.5394538539105882,
      -1.602210815375781,
      0.0,
      -1.6333082640893875,
      -1.6372809874663135
    ],
    [
      -1.558482598287804,
      -1.482314825882919,
      -1.4590382794935512,
      -1.560440509440867,
      -1.5044546387534088,
      -1.6121972211722273,
      -1.71653665464713,
      -1.5848649222661142,
      -1.5620943632308932,
      -1.6036387299609114,
      -1.562822283758227,
      -1.7126737475393201,
      -1.5258286575782727,
      -1.5606821007087797,
      -1.5577264233361603,
      -1.5405163282498446,
      -1.5831221049610413,
      -1.5135023300241828,
      -1.6183132968632412,
      -1.6262233174801322,
      -1.546541886407615,
      -1.6270654984916535,
      -1.614696927166753,
      -1.5756954962899143,
      -1.5448305756252452,
      -1.6364579770145977,
      -1.6279146419414159,
      0.0,
      -1.6441414593994108
    ],
    [
      -1.4472203760159252,
      -1.3990315384392673,
      -1.3940959958194767,
      -1.4131499117614228,
      -1.4024095158684544,
      -1.4282848520718157,
      -1.4527307627698312,
      -1.4281100368969915,
      -1.4067651120353089,
      -1.414876114873683,
      -1.4354824121153327,
      -1.4472831576155547,
      -1.3956702003486579,
      -1.4113686548294244,
      -1.4007284630465557,
      -1.385763328209623,
      -1.4458593491923026,
      -1.4243475405660515,
      -1.4456187059914514,
      -1.4192012609699445,
      -1.4201935021988545,
      -1.3885421810497154,
      -1.45800995943601,
      -1.3931861476912024,
      -1.4182463459900632,
      -1.4470736472158707,
      -1.3970769820747924,
      -1.4060456672071637,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.16680005631613426,
      0.16529654807636796,
      0.14166304436446753,
      0.17600213050902358,
      0.1475928733435481,
      0.08679680110995136,
      0.15470610765932324,
      0.16321580635283617,
      0.14341397280631263,
      0.15065264101588216,
      0.08809512996296442,
      0.15469548110067066,
      0.15123497891369064,
      0.14725224845431173,
      0.16393213787285177,
      0.1316014925216762,
      0.13341635450983036,
      0.13136268660638173,
      0.14650505763939892,
      0.16058245773916857,
      0.14004802322962284,
      0.12178655007207295,
      0.13072055335270427,
      0.14956757080544048,
      0.11224006366435457,
      0.13415928318680237,
      0.1824009123455288,
      0.11048375311615422
    ],
    [
      0.3625831728141835,
      0.0,
      0.4950564485417759,
      0.4373085213624628,
      0.5214867367010712,
      0.4464162559256655,
      0.2753929308742531,
      0.4649117975742929,
      0.4611211856666857,
      0.42079098530610337,
      0.4908585451657015,
      0.2658876183994807,
      0.48405472206207634,
      0.4387586466948028,
      0.44118020956885684,
      0.4806357144747486,
      0.37402841666390496,
      0.4287636493687046,
      0.35890507409072825,
      0.4001107336692351,
      0.4429806516552677,
      0.3393892809313994,
      0.3655740145323243,
      0.38405921623002226,
      0.40038111631984297,
      0.35554168901907257,
      0.41049874660414054,
      0.4981193989386612,
      0.3823731460123536
    ],
    [
      0.4039528850457419,
      0.537553558784241,
      0.0,
      0.5536889315470963,
      0.5617941855572683,
      0.5372236799125572,
      0.3128775244492563,
      0.5544831283613154,
      0.5873980337505342,
      0.47223051670857163,
      0.4892778816712051,
      0.30701057892450234,
      0.5832829583010735,
      0.510000377569098,
      0.5441564676930524,
      0.6460696460753728,
      0.4737355900330684,
      0.4851476093360074,
      0.442578495487314,
      0.42704532958489416,
      0.45428285173733207,
      0.44020729313096174,
      0.41116777437032614,
      0.4564204293807004,
      0.4940564990417904,
      0.41472983820249887,
      0.3785422121892206,
      0.5398946188123523,
      0.3923810022275611
    ],
    [
      0.3328669972653351,
      0.432797552436772,
      0.5506080895495922,
      0.0,
      0.4594319592932894,
      0.530653213151264,
      0.33962780371874546,
      0.49249117391731456,
      0.5693602686532842,
      0.3869603562546622,
      0.46931042847584736,
      0.3238934069340855,
      0.5083245628813333,
      0.5878110083347878,
      0.44960156427688025,
      0.6217287195149845,
      0.3758578413202067,
      0.43036675750431574,
      0.4081096754454794,
      0.45244890693013695,
      0.4138345207738823,
      0.35568535813758895,
      0.3465129081742371,
      0.4203777751891007,
      0.3720957799795863,
      0.38305394753546373,
      0.3634931990981114,
      0.38381922678400526,
      0.36573564044598483
    ],
    [
      0.43603364480414597,
      0.5828663987525027,
      0.635219245236748,
      0.6064751231974155,
      0.0,
      0.5361527328725908,
      0.36779318749412737,
      0.654605432665119,
      0.5889302765423916,
      0.53550100329977,
      0.5448941604511204,
      0.3771570304828107,
      0.5384662160034139,
      0.6076098984524576,
      0.527064191334107,
      0.5940202853370997,
      0.4881937896012243,
      0.5234699446487712,
      0.4677990923738333,
      0.5006265497094178,
      0.5625879268094862,
      0.4624134150301116,
      0.4608740978789103,
      0.4600453116263248,
      0.5912523758075634,
      0.3897324646462652,
      0.4560660660390754,
      0.5457203990754962,
      0.43827478488192306
    ],
    [
      0.34962441217619666,
      0.4523652384052441,
      0.541050407283217,
      0.5581109914508371,
      0.47244441611634236,
      0.0,
      0.3279372068502897,
      0.4918151352237954,
      0.49063023298890385,
      0.41310306628078397,
      0.42227093251893777,
      0.34291199610754886,
      0.5055683119355441,
      0.555614853090798,
      0.45436147528098636,
      0.4930812840478098,
      0.3570713040876159,
      0.4063998672291731,
      0.38173650882682497,
      0.3748947942331311,
      0.43577817052198076,
      0.4163177566018428,
      0.3494148248388713,
      0.4003306159085198,
      0.37124673018561394,
      0.4109139058015663,
      0.3711443241337664,
      0.35736002171072356,
      0.33825925201980644
    ],
    [
      0.22390842402778066,
      0.28443719377618515,
      0.31053697818754733,
      0.33535149436868106,
      0.33655654050982053,
      0.32524923475268896,
      0.0,
      0.30008633257666606,
      0.29279252099414066,
      0.3227325553464866,
      0.26585406079016494,
      0.27194572863464495,
      0.29970084352674875,
      0.32031665533861675,
      0.2705754838513539,
      0.32503052170818947,
      0.2558348193389013,
      0.30439521779263057,
      0.28710541365861664,
      0.25970770977392243,
      0.28670021767588705,
      0.2649384192840789,
      0.25164509263822676,
      0.2409804914286715,
      0.28683747590386677,
      0.25629292737524834,
      0.2772347379551272,
      0.2703893165763456,
      0.25710697388842574
    ],
    [
      0.37522551672100546,
      0.5166330226494027,
      0.5647048039354292,
      0.4991741406627064,
      0.5491488145394061,
      0.4977122402546241,
      0.31607177749325865,
      0.0,
      0.49882126660258175,
      0.4198485785605517,
      0.46932389788611695,
      0.2908219182021854,
      0.5021441398762254,
      0.5185336403824994,
      0.507857916167467,
      0.513557935567724,
      0.41416126986299506,
      0.42880974331384913,
      0.40814629493808186,
      0.3840215949299759,
      0.48037565141295824,
      0.3995931096409975,
      0.34737708229253283,
      0.43099062823116285,
      0.4142313410755516,
      0.4176628345511715,
      0.41459676208873697,
      0.43945753611686134,
      0.3800383278628785
    ],
    [
      0.3469349426075645,
      0.47721236239397813,
      0.537090618823826,
      0.5332489315313202,
      0.4605594095051566,
      0.48871577533201527,
      0.2662097844982976,
      0.46978457990197975,
      0.0,
      0.3760923519150208,
      0.42256703749462954,
      0.2531983534614335,
      0.5415174245098218,
      0.4661614943006811,
      0.4358757898857435,
      0.5833475453849615,
      0.39251281921371617,
      0.4253047010851907,
      0.32097803102936884,
      0.368269711313026,
      0.4678337719557908,
      0.3544652923482374,
      0.3555027217075728,
      0.3408760352468634,
      0.37761384417662613,
      0.3688527371631811,
      0.35003099365977897,
      0.4131827993820798,
      0.3650967649085657
    ],
    [
      0.32876760985299835,
      0.3961851989234644,
      0.42310859062904793,
      0.3506196782138624,
      0.45636308184474883,
      0.3768597514873089,
      0.2742660618253585,
      0.4261492330511909,
      0.3875997264103479,
      0.0,
      0.36236806669774446,
      0.2449529608508485,
      0.39767969713826656,
      0.4050664583039727,
      0.4333058024400591,
      0.42245376476004526,
      0.3838420109798488,
      0.3546145471930122,
      0.334152473964501,
      0.35092486372320897,
      0.4313682212241261,
      0.3153104999526619,
      0.381907643842089,
      0.35582652185049546,
      0.4682928677684497,
      0.34372416242106474,
      0.39537961371950514,
      0.37913693799696646,
      0.3403178538079694
    ],
    [
      0.3933177366475056,
      0.4946965573435702,
      0.4451472149916056,
      0.5200297654593244,
      0.5035814706898649,
      0.4440884226199273,
      0.24628235281053779,
      0.45738956432819666,
      0.46986409417655195,
      0.43326814820196047,
      0.0,
      0.24898684424542372,
      0.5219912557901347,
      0.45328221590138496,
      0.49979040951280895,
      0.48560686521314866,
      0.42752789960977977,
      0.4371594362618889,
      0.3474733932765197,
      0.4411320312120599,
      0.5032812589378663,
      0.38608554734070877,
      0.39164647949413167,
      0.35032008198861986,
      0.40776796144756067,
      0.3715292463439821,
      0.4086814126222802,
      0.4572818481306933,
      0.3709535087040301
    ],
    [
      0.26912882822027795,
      0.3017577779723313,
      0.339073921164732,
      0.3442572406705675,
      0.36091626991454184,
      0.3516684848522027,
      0.341524726996423,
      0.3142248608622915,
      0.34334385029389325,
      0.315213970516653,
      0.29479291864946044,
      0.0,
      0.32127603499727386,
      0.3830176670123109,
      0.29408507023008323,
      0.31041732674578326,
      0.3331390669805996,
      0.30401273468834167,
      0.2870908614059626,
      0.2978803142465447,
      0.2954677258743599,
      0.2933898720515764,
      0.28112043096188155,
      0.3045545567048673,
      0.26298404491338934,
      0.29378709824133775,
      0.29073735333676587,
      0.31013095365954335,
      0.3660936651792319
    ],
    [
      0.37015695548548133,
      0.4822021146000617,
      0.5165378948860528,
      0.5356082602538443,
      0.4584615245354817,
      0.49129041088843683,
      0.2487726631551439,
      0.4654257776777959,
      0.5618169184313615,
      0.4206228294848562,
      0.4680048707235709,
      0.2727674252814565,
      0.0,
      0.5051009610328938,
      0.45999785241494284,
      0.510234889465512,
      0.39709758453585264,
      0.4711876100477823,
      0.42940689373014784,
      0.4418242882271195,
      0.48544817026142106,
      0.3750889199378955,
      0.3228217779042091,
      0.4105867749906129,
      0.38387288487032767,
      0.37724795992890026,
      0.39281434202012866,
      0.41351907158768997,
      0.36005591894181066
    ],
    [
      0.31692548126218023,
      0.4033618457693424,
      0.46053930412798927,
      0.5333812671121145,
      0.4462240644656521,
      0.477416883080267,
      0.29361672286135754,
      0.5066771728555084,
      0.4484736992863607,
      0.3672418935321782,
      0.3756491625007288,
      0.28252954461310287,
      0.4836023594801979,
      0.0,
      0.3898929856775524,
      0.47119281877147157,
      0.35350962522063045,
      0.3766561282473775,
      0.4005270238256129,
      0.3356757938354995,
      0.38542342801708607,
      0.3327676705477254,
      0.31373307012052987,
      0.3864494311073037,
      0.3642913979810989,
      0.3423955734470896,
      0.3371228559071935,
      0.3720369375820525,
      0.28982932691911056
    ],
    [
      0.42477940298207617,
      0.4870859295478762,
      0.5995358130003792,
      0.5535980313541935,
      0.5788623186709203,
      0.510007568333217,
      0.3370685027580005,
      0.5354258583696498,
      0.5294141105856587,
      0.5094801870318613,
      0.5335479240048957,
      0.31973096342255536,
      0.4931797609596107,
      0.5442820985353063,
      0.0,
      0.5456454652899967,
      0.4772907035170373,
      0.4523870968766355,
      0.44586012386007856,
      0.47684493741491285,
      0.5152638468153443,
      0.4983081442686974,
      0.4787053266033252,
      0.44700581239366755,
      0.4945171245023632,
      0.4517761525228998,
      0.5272090451758629,
      0.5057988080211682,
      0.42089326442322283
    ],
    [
      0.3510975447842053,
      0.4590003776070961,
      0.5441922937522805,
      0.583970716098196,
      0.4526184683179053,
      0.46161271526533,
      0.29661715163451485,
      0.4659670134314906,
      0.5372584697369391,
      0.38886588527724375,
      0.4568185725458509,
      0.27754632825693815,
      0.5162325153955185,
      0.5175344851692323,
      0.4385814036916267,
      0.0,
      0.38058275937304953,
      0.47465011639141386,
      0.36390802541395795,
      0.3814104994248746,
      0.4107585335003703,
      0.3741203593871709,
      0.3433220392647611,
      0.4277017170659274,
      0.3799000269906898,
      0.3281503934350416,
      0.3795972131900571,
      0.41858803482704543,
      0.3149893181430703
    ],
    [
      0.3161177450291539,
      0.43778014861086656,
      0.46030858269576336,
      0.3721566186366745,
      0.4944062976686847,
      0.37774904731649905,
      0.2673027826939951,
      0.4709108760777643,
      0.427058204810167,
      0.43808442530509684,
      0.4207001485963162,
      0.25888798929605805,
      0.4692754129509371,
      0.3553688047090975,
      0.44840522018578577,
      0.45432814415656786,
      0.0,
      0.4050019985011373,
      0.3848389654592752,
      0.39569720000415787,
      0.4455731374549774,
      0.33667074736820113,
      0.4629494555264817,
      0.35198736757430815,
      0.4475762232575258,
      0.33990352799228574,
      0.4193540620374694,
      0.46897916386702776,
      0.35093442946452225
    ],
    [
      0.3167146464166197,
      0.43630769638417877,
      0.47479088982572026,
      0.4155434228290982,
      0.49889152882004884,
      0.3962670495990057,
      0.29278613877863036,
      0.48073893645697585,
      0.4375514378742249,
      0.40529320638166966,
      0.4141717020678739,
      0.2688339893313074,
      0.44884018075066723,
      0.42129904832642184,
      0.43200465927260434,
      0.4490060723426441,
      0.383660434767501,
      0.0,
      0.3611299338596341,
      0.3932311515735041,
      0.43651749760947034,
      0.3544277798896711,
      0.3611350971452243,
      0.39361626830836927,
      0.4098259113680571,
      0.3205231141140783,
      0.38250403029183366,
      0.4745802674545079,
      0.3353029198783075
    ],
    [
      0.3264772162328595,
      0.3694355021864213,
      0.4443620848062586,
      0.43465873536593436,
      0.4203238529929698,
      0.43125021100770833,
      0.24541314271520664,
      0.42851786673823433,
      0.44097809179843517,
      0.3777791448261607,
      0.34206370255762053,
      0.20159575190645262,
      0.4864228387131795,
      0.4956280221174092,
      0.397064894550649,
      0.47700516813445426,
      0.34180987601709023,
      0.3834649910233665,
      0.0,
      0.3470591511326173,
      0.3879161322325255,
      0.3472656200248849,
      0.33408446777500744,
      0.40264717917765536,
      0.29565461850265895,
      0.40606336482972005,
      0.3712389799201967,
      0.34355462671294634,
      0.29938872687175255
    ],
    [
      0.37284993729007865,
      0.4102152011643798,
      0.471594915092846,
      0.4609606661792607,
      0.4504344856745679,
      0.4018701426326845,
      0.2832830623262417,
      0.4458396857285758,
      0.3816628935759885,
      0.40967618694059027,
      0.41010513985505614,
      0.2621883293431069,
      0.42368858302011425,
      0.4221085140719907,
      0.4315875426760907,
      0.46778435814882635,
      0.37998783871285347,
      0.3955295257221514,
      0.38984390634753074,
      0.0,
      0.46351531478562835,
      0.3706037916428957,
      0.38302662942865795,
      0.37889356559579923,
      0.3431496894531687,
      0.37785293396371644,
      0.4397415774706692,
      0.4010523793567513,
      0.36988264221525013
    ],
    [
      0.41733966539776746,
      0.5422991598297697,
      0.5137957858633935,
      0.46350049146551275,
      0.5671526221555641,
      0.48881168127047814,
      0.2870709982952371,
      0.546683310480802,
      0.5093322850802562,
      0.4817840692372699,
      0.529403527462948,
      0.22105403925278733,
      0.545705535327154,
      0.4537582424606341,
      0.5176882585541549,
      0.4873661990530107,
      0.4193475329902787,
      0.4843690143301078,
      0.4387613082166659,
      0.49627528242175223,
      0.0,
      0.39678591764941684,
      0.41052374115653767,
      0.41563829507646477,
      0.4554064078405997,
      0.41185759855870097,
      0.43855304308186405,
      0.46895130022539044,
      0.38247742479039526
    ],
    [
      0.3555497715505158,
      0.38063921851034554,
      0.4525410599320494,
      0.432163606160501,
      0.4528807466097049,
      0.43567745740236385,
      0.3203058063131776,
      0.4622068427098518,
      0.4252682401795136,
      0.39414276819441696,
      0.38924467991814615,
      0.33239410880596254,
      0.37730005397790145,
      0.44680730745815267,
      0.5166246940810719,
      0.44519879797667894,
      0.38091519919669503,
      0.3858307871186095,
      0.34276805684708656,
      0.38498365102932763,
      0.37590975670340065,
      0.0,
      0.3355274684222411,
      0.4143373618584103,
      0.36878127483608236,
      0.3505657963577944,
      0.3926737399463345,
      0.38514406134567114,
      0.42129334091800286
    ],
    [
      0.36001775062610264,
      0.45735038632465974,
      0.4414226040061533,
      0.46032319478342454,
      0.4566597250313158,
      0.40064307881449057,
      0.280362670037972,
      0.4575103141433252,
      0.44726611156791596,
      0.4731491473832914,
      0.4607301562883559,
      0.2906357657132974,
      0.4555773048284555,
      0.4130016805736103,
      0.5152338134331442,
      0.44876355833240145,
      0.48088834804906266,
      0.4577832266732611,
      0.42330535148918647,
      0.4788268392339663,
      0.49228732946664144,
      0.38704637313256685,
      0.0,
      0.40851732054762535,
      0.47336489313464436,
      0.3753804881822673,
      0.5066239347069226,
      0.4829687936445728,
      0.3846903630357299
    ],
    [
      0.3329429160659698,
      0.3359716761733975,
      0.41535169177438225,
      0.3706448816114549,
      0.37251157299517135,
      0.36950297395510345,
      0.22778627459194078,
      0.3793535561576471,
      0.3625025614095134,
      0.31698869494837734,
      0.2942396497966553,
      0.23544942109973221,
      0.37154025871190144,
      0.40469702441628663,
      0.3533416029987366,
      0.43837607078727814,
      0.31662070994248004,
      0.3490997082404368,
      0.3060307334344523,
      0.31160893762386355,
      0.31089954472297365,
      0.3333971463524199,
      0.31013931554727403,
      0.0,
      0.32087676798848586,
      0.3096409396007156,
      0.3260019502380058,
      0.3486337517549145,
      0.3143520736504606
    ],
    [
      0.389203378447734,
      0.4525424055196292,
      0.4932675340773307,
      0.42063004615959865,
      0.5284167916099602,
      0.3952984198074314,
      0.32571176892430387,
      0.44889746874958436,
      0.40443545153170013,
      0.5346779367794787,
      0.4396745402456077,
      0.23708658513792757,
      0.4190831897717171,
      0.45020969751831297,
      0.43242708851476097,
      0.42627061007690714,
      0.42057393934905596,
      0.42544442231813884,
      0.37187396573468967,
      0.3768230015906786,
      0.4596803890239627,
      0.3751289298915339,
      0.42157866246987474,
      0.4057733328875106,
      0.0,
      0.34107610395840804,
      0.41500424520612356,
      0.4310422890489396,
      0.35023408532667455
    ],
    [
      0.3069265879213452,
      0.4529566208613236,
      0.5160771932202226,
      0.5000223614721959,
      0.4691186689944271,
      0.4993427059444364,
      0.32701342319246773,
      0.4733604792346422,
      0.4511295191442459,
      0.4193825853211224,
      0.43487345392019616,
      0.3143735061936508,
      0.4843262567996538,
      0.4762993729001024,
      0.5338897579227952,
      0.4744595991782439,
      0.3994686376251104,
      0.4060291296338574,
      0.4672580839288101,
      0.41829035506770285,
      0.4516295903625509,
      0.36101084194538724,
      0.4056022086757445,
      0.384205398139257,
      0.3983625333998573,
      0.0,
      0.43454563194879436,
      0.415924266570465,
      0.37874838225526375
    ],
    [
      0.32551591808739144,
      0.4669705754691391,
      0.46095839495972357,
      0.4239703032231905,
      0.4638524391447778,
      0.40988009388670843,
      0.3084246361607026,
      0.4921802058693687,
      0.46088746267230185,
      0.47282012967618403,
      0.44472411283940017,
      0.26830115755450934,
      0.46237350701548996,
      0.43431411975421685,
      0.5214845555463932,
      0.46344435365459646,
      0.397867257979692,
      0.41038656856980027,
      0.4141263747440982,
      0.48118246258458264,
      0.46504494101113214,
      0.3944619821669011,
      0.42084866820930045,
      0.42672147484431244,
      0.47616550268652635,
      0.4134085412213335,
      0.0,
      0.3823110925077271,
      0.3783383691308011
    ],
    [
      0.4284418539923547,
      0.5046096263972397,
      0.5278861727866075,
      0.4264839428392917,
      0.48246981352675,
      0.3747272311079315,
      0.2703877976330289,
      0.4020595300140446,
      0.42483008904926556,
      0.3832857223192474,
      0.4241021685219317,
      0.2742507047408387,
      0.4610957947018861,
      0.42624235157137913,
      0.4291980289439985,
      0.44640812403031416,
      0.40380234731911746,
      0.473422122255976,
      0.3686111554169176,
      0.36070113480002663,
      0.44038256587254376,
      0.3598589537885053,
      0.37222752511340573,
      0.41122895599024445,
      0.4420938766549136,
      0.3504664752655611,
      0.3590098103387429,
      0.0,
      0.342782992880748
    ],
    [
      0.29594567909563874,
      0.34413451667229666,
      0.34907005929208723,
      0.3300161433501412,
      0.34075653924310956,
      0.3148812030397483,
      0.2904352923417328,
      0.3150560182145725,
      0.3364009430762551,
      0.32828994023788094,
      0.3076836429962313,
      0.29588289749600927,
      0.3474958547629061,
      0.33179740028213955,
      0.34243759206500823,
      0.3574027269019411,
      0.29730670591926134,
      0.31881851454551247,
      0.2975473491201126,
      0.3239647941416195,
      0.32297255291270943,
      0.3546238740618486,
      0.2851560956755539,
      0.34997990742036156,
      0.3249197091215008,
      0.29609240789569324,
      0.3460890730367716,
      0.33712038790440024,
      0.0
    ]
  ],
  "row_avgs": [
    0.14236516845169545,
    0.4152560223274221,
    0.4789710677816041,
    0.43310209435629565,
    0.5171373230376508,
    0.422919929852024,
    0.2851515493457023,
    0.44282292092210496,
    0.41282273659772956,
    0.3757337107454701,
    0.42564867940364526,
    0.3144674152622581,
    0.42957048376074247,
    0.3873979810055112,
    0.4890537257586218,
    0.41877117776327843,
    0.40279666883024273,
    0.39983910756135177,
    0.37782585574537053,
    0.39996176565769537,
    0.45684616919731835,
    0.39848841622714326,
    0.43465466154229876,
    0.3370893716639296,
    0.4140023671313419,
    0.43052239827763833,
    0.42646304289893944,
    0.4061095309954576,
    0.3243670650293945
  ],
  "col_avgs": [
    0.35104809360179334,
    0.43093456854935175,
    0.46961161216139746,
    0.44991287684726317,
    0.45686880270134095,
    0.4256629120662941,
    0.2877549640190769,
    0.44848065210826144,
    0.44426227686579484,
    0.4057400092169215,
    0.4117109902020785,
    0.27244178834470073,
    0.45015860911749556,
    0.44628060804258174,
    0.4344630921151795,
    0.46438459653584163,
    0.3799369935974396,
    0.4047114829795456,
    0.3707584018761382,
    0.3859988241811127,
    0.4208677198953873,
    0.3614075328476967,
    0.3580682560657609,
    0.3818140135755672,
    0.391967373214778,
    0.35394508165140753,
    0.3828088656839386,
    0.4081106857835903,
    0.3500467232821431
  ],
  "combined_avgs": [
    0.2467066310267444,
    0.4230952954383869,
    0.47429133997150075,
    0.44150748560177944,
    0.4870030628694959,
    0.42429142095915906,
    0.28645325668238963,
    0.4456517865151832,
    0.4285425067317622,
    0.3907368599811958,
    0.41867983480286186,
    0.2934546018034794,
    0.439864546439119,
    0.41683929452404644,
    0.4617584089369007,
    0.44157788714956003,
    0.3913668312138412,
    0.4022752952704487,
    0.37429212881075435,
    0.392980294919404,
    0.4388569445463528,
    0.37994797453742,
    0.39636145880402984,
    0.3594516926197484,
    0.40298487017305995,
    0.39223373996452293,
    0.40463595429143906,
    0.40711010838952394,
    0.33720689415576877
  ],
  "gppm": [
    599.634321150948,
    610.5631084758968,
    592.0404080508541,
    600.9416256581886,
    596.6222224128536,
    614.4851515746972,
    667.1184166970128,
    600.7357052073929,
    603.4920378422887,
    619.954088981719,
    619.3267598088456,
    680.6447526456751,
    601.0649123347391,
    603.7815542522934,
    609.4190385880049,
    591.6691100963093,
    630.705611829113,
    621.6752351043015,
    638.0252943352185,
    628.8398791611874,
    614.041495627451,
    643.8381651935852,
    646.1718768605853,
    633.5270501447884,
    626.3665849764726,
    646.1949520937654,
    629.1289560716439,
    616.5724031558717,
    646.8116005454558
  ],
  "gppm_normalized": [
    1.4160212930987766,
    1.501326464229395,
    1.4619419715538726,
    1.4855202503509373,
    1.4686419259499266,
    1.5165699681397253,
    1.6425762231994163,
    1.487290758648627,
    1.4873872595377695,
    1.5292391042249516,
    1.5239609155057035,
    1.6890885680923664,
    1.4802555837901072,
    1.4875466744586296,
    1.5076341771318058,
    1.4601580262886427,
    1.5553751756159624,
    1.533907062311479,
    1.571118396790242,
    1.5487305523487147,
    1.5153009744950368,
    1.5865869667047414,
    1.590704356249378,
    1.5651789648209236,
    1.5442184979546172,
    1.592406825788078,
    1.5486094870940648,
    1.5199596091852958,
    1.5948307031394644
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386
  ],
  "response_lengths": [
    4560,
    2575,
    2286,
    2605,
    2429,
    2463,
    3048,
    2289,
    2469,
    2419,
    2431,
    2461,
    2600,
    2772,
    2053,
    2823,
    2237,
    2498,
    2352,
    2598,
    2006,
    2194,
    2049,
    2586,
    2225,
    1942,
    2421,
    2436,
    2374
  ]
}