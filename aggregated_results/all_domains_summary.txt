BINARY CATEGORY ANALYSIS - ALL DOMAINS SUMMARY
======================================================================
Generated: 2025-07-11 20:07:32
Total domains analyzed: 10

DOMAINS BY COMPRESSION RATIO:
------------------------------------------------------------

TRANSLATION TASKS:
  WMT14 Translation            | Compression:  1.1:1   | GF: 21 | Prob:  8
  Opus Books Translation       | Compression:  1.3:1   | GF: 22 | Prob:  8

SUMMARIZATION TASKS:
  SamSum                       | Compression:  4.8:1   | GF: 14 | Prob: 15
  PubMed                       | Compression:  6.7:1   | GF: 14 | Prob: 15
  Multi-News                   | Compression:  9.0:1   | GF: 14 | Prob: 15
  BillSum                      | Compression:  9.3:1   | GF: 14 | Prob: 15
  CNN/DailyMail                | Compression: 13.8:1   | GF: 14 | Prob: 15
  Reddit TIFU                  | Compression: 16.1:1   | GF: 14 | Prob: 15
  XSum                         | Compression: 18.5:1   | GF: 14 | Prob: 15

PEER REVIEW TASKS:
  ICLR 2023 Peer Review        | Compression: 20.2:1   | GF: 15 | Prob: 13

EFFECT SIZES BY MECHANISM:
------------------------------------------------------------

BASELINE:
  WMT14 Translation              | d= 0.930 **
  Opus Books Translation         | d= 1.144 *
  SamSum                         | d= 0.109 ns
  PubMed                         | d= 0.856 *
  Multi-News                     | d= 0.876 *
  BillSum                        | d= 0.908 *
  CNN/DailyMail                  | d= 0.607 ns
  Reddit TIFU                    | d= 0.133 ns
  XSum                           | d= 0.293 ns
  ICLR 2023 Peer Review          | d=-0.120 ns

MI (DoE):
  WMT14 Translation              | d= 1.610 [ 1.462,  1.789] ***
  Opus Books Translation         | d= 1.632 [ 1.067,  2.898] ***
  SamSum                         | d= 2.521 [ 2.280,  2.804] ***
  PubMed                         | d= 2.008 [ 1.716,  2.378] ***
  Multi-News                     | d= 1.530 [ 1.376,  1.744] ***
  BillSum                        | d= 2.239 [ 2.041,  2.499] ***
  CNN/DailyMail                  | d= 2.063 [ 1.884,  2.289] ***
  Reddit TIFU                    | d= 2.517 [ 2.287,  2.816] ***
  XSum                           | d= 1.894 [ 1.721,  2.122] ***
  ICLR 2023 Peer Review          | d= 0.679 [ 0.477,  0.917] ***

GPPM:
  WMT14 Translation              | d= 0.698 [ 0.635,  0.783] ***
  Opus Books Translation         | d= 2.192 [ 1.472,  4.440] ***
  SamSum                         | d= 2.518 [ 2.306,  2.767] ***
  PubMed                         | d= 3.185 [ 2.733,  3.743] ***
  Multi-News                     | d= 2.702 [ 2.437,  3.054] ***
  BillSum                        | d= 3.589 [ 3.273,  4.014] ***
  CNN/DailyMail                  | d= 3.417 [ 3.073,  3.801] ***
  Reddit TIFU                    | d= 3.758 [ 3.457,  4.169] ***
  XSum                           | d= 2.849 [ 2.616,  3.119] ***
  ICLR 2023 Peer Review          | d= 0.732 [ 0.537,  0.956] ***

TVD-MI:
  WMT14 Translation              | d= 3.318 [ 3.093,  3.596] ***
  Opus Books Translation         | d= 1.971 [ 1.391,  3.171] ***
  SamSum                         | d= 6.137 [ 5.596,  6.833] ***
  PubMed                         | d= 6.525 [ 5.899,  7.339] ***
  Multi-News                     | d= 6.549 [ 5.819,  7.520] ***
  BillSum                        | d= 5.912 [ 5.277,  6.756] ***
  CNN/DailyMail                  | d= 5.872 [ 5.171,  6.688] ***
  Reddit TIFU                    | d= 7.235 [ 6.498,  8.200] ***
  XSum                           | d= 6.688 [ 6.087,  7.455] ***
  ICLR 2023 Peer Review          | d= 1.816 [ 1.518,  2.247] ***

LLM JUDGE (WITH CONTEXT):
  Opus Books Translation         | d= 2.617 [ 1.963,  3.899] ***
  SamSum                         | d= 2.698 [ 2.452,  3.016] ***
  PubMed                         | d= 8.136 [ 7.349,  9.200] ***
  Multi-News                     | d= 4.059 [ 3.561,  4.732] ***
  BillSum                        | d= 4.235 [ 3.773,  4.792] ***
  CNN/DailyMail                  | d= 3.552 [ 3.224,  3.953] ***
  Reddit TIFU                    | d= 2.703 [ 2.379,  3.108] ***
  XSum                           | d= 3.392 [ 3.042,  3.820] ***
  ICLR 2023 Peer Review          | d= 0.259 [ 0.065,  0.473] *

LLM JUDGE (WITHOUT CONTEXT):
  Opus Books Translation         | d= 0.278 [-0.090,  0.647] ns
  SamSum                         | d= 0.539 [ 0.389,  0.697] ***
  PubMed                         | d= 3.250 [ 2.845,  3.787] ***
  Multi-News                     | d= 0.538 [ 0.382,  0.710] ***
  BillSum                        | d= 0.163 [ 0.025,  0.307] *
  CNN/DailyMail                  | d= 0.725 [ 0.626,  0.831] ***
  Reddit TIFU                    | d= 0.046 [-0.093,  0.192] ns
  XSum                           | d=-0.282 [-0.429, -0.135] ***
  ICLR 2023 Peer Review          | d=-1.689 [-2.000, -1.454] ***

H1a TEST PREVIEW: All mechanisms achieve d > 0.5?
------------------------------------------------------------
MI (DoE)                  | ✓ PASS | Min d: 0.679
GPPM                      | ✓ PASS | Min d: 0.698
TVD-MI                    | ✓ PASS | Min d: 1.816
LLM JUDGE (WITH CONTEXT)  | ✗ FAIL | Min d: 0.259
LLM JUDGE (WITHOUT CONTEXT) | ✗ FAIL | Min d: 0.046