{
  "example_idx": 71,
  "reference": "Under review as a conference paper at ICLR 2023\n\nMIXED FEDERATED LEARNING: JOINT DECENTRALIZED AND CENTRALIZED LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFederated learning (FL) enables learning from decentralized privacy-sensitive data, with computations on raw data confined to take place at edge clients. This paper introduces mixed FL, which incorporates an additional loss term calculated at the coordinating server (while maintaining FL’s private data restrictions). For example, additional datacenter data can be leveraged to jointly learn from centralized (datacenter) and decentralized (federated) training data and better match an expected inference data distribution. Mixed FL also enables offloading some intensive computations (e.g., embedding regularization) to the server, greatly reducing communication and client computation load. For these and other mixed FL use cases, we present three algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER. We perform extensive experiments of the algorithms on three tasks, demonstrating that mixed FL can blend training data to achieve an oracle’s accuracy on an inference distribution, and can reduce communication and computation overhead by more than 90%. Finally, we state convergence bounds for all algorithms, and give intuition on the mixed FL problems best suited to each. The theory confirms our empirical observations of how the algorithms perform under different mixed FL problem settings.\n\n1\n\nINTRODUCTION\n\nFederated learning (FL) (McMahan et al., 2017) is a machine learning setting where multiple ‘clients’ (e.g., mobile phones) collaborate to train a model under coordination of a central server. Clients’ raw data are never transferred. Instead, focused updates intended for immediate aggregation are used to achieve the learning objective (Kairouz et al., 2019). FL typically delivers model quality improvements because training examples gathered in situ by clients reflect actual inference serving requests. For example, a mobile keyboard next-word prediction model can be trained from actual SMS messages, yielding higher accuracy than a model trained on a proxy document corpus. Because of the benefits, FL has been used to train production models for many applications (Hard et al., 2018; Ramaswamy et al., 2019; Apple, 2019; Ramaswamy et al., 2020; Hartmann, 2021; Hard et al., 2022).\n\nBuilding on FL, we can gain significant benefits from ‘mixed FL’: jointly1 training with an additional centralized objective in conjunction with the decentralized objective of FL. Let x be model parameters to be optimized. Let f denote a mixed loss, a sum2 of a federated loss ff and a centralized loss fc:\n\nf (x) = ff(x) + fc(x)\n\n(1)\n\nMixed loss f might be a more useful training objective than ff for many reasons, including:\n\nMitigating Distribution Shift by Adding Centralized Data to FL While FL helps with reducing train vs. inference distribution skew, it may not remove it completely. Examples include: training device populations that are subsets of inference device populations (e.g., training on high-end phones, for eventual use also on low-end phones), label-biased example retention on edge clients (e.g., only retaining positive examples of a binary classification task), and infrequent safety-critical example\n\n1We use ‘joint’ to distinguish our work from sequential ‘central-then-FL’ use cases, e.g. transfer learning. 2To simplify we subsume any relative weights into loss terms, i.e. this can be f (x) = (wf ̃ff(x))+(wc ̃fc(x)).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\nevents with outsized importance (e.g., automotive hard-braking events needed to train a self-driving AI) (Anonymous, a). The benefits of FL can be achieved while overcoming remaining distribution skew by incorporating data from an additional datacenter dataset, via mixed FL. This affords a composite set of training data that better matches the inference distribution.\n\nReducing Client Computation and Communication In representation learning, negative examples are used to push dissimilar items apart in a latent embedding space while keeping positive examples closer together (Oord et al., 2018). In federated settings, clients’ caches may have limited local negative examples, and recent work (Anonymous, b) showed this significantly degrades performance compared to centralized learning. This work also showed that using an additional loss (a regularization) to push representations apart, instead of negative examples, can resolve this performance gap. However, if done naively this requires communicating and computing over a large embedding table, introducing massive overhead for large-scale tasks. Applying mixed FL, where federated loss ff is the primary ‘affinity’ loss and centralized loss fc is the ‘spreadout’ regularization, avoids communicating the entire embedding table to clients and greatly reduces client computation.\n\nThough mixed FL can clearly be useful, an actual process to minimize f is not trivial. FL requires that clients’ data stay on device, as they contain private information that possibly reveals personal identity. Moreover, centralized loss/data is expected to differ significantly3 from client loss/data.\n\nContributions\n\n• We motivate the mixed FL problem and present three algorithms for addressing it: PARALLEL TRAINING (PT), 1-WAY GRADIENT TRANSFER (1-W GT), and 2-WAY GRADIENT TRANSFER (2-W GT). These algorithms maintain the data privacy protections inherent in FL. [Section 2] • We experiment with facial attribute classification and language modeling, demonstrating that our algorithms overcome distribution shift. We match the accuracy of hypothetical ‘oracle’ scenarios where the entire inference distribution was colocated for training. [Section 4]\n\n• We experiment with user-embedding based movie recommendation, reducing communication overhead by 93.9% and client computation by 99.9% with no degradation in quality. [Section 4] • We state convergence bounds for each algorithm (in strongly, general, and non-convex settings), providing theoretical explanations for convergence behaviors we observe in the experiments. This indicates how the algorithms will perform on new mixed FL tasks. [Section 5]\n\n2 ALGORITHMS\n\nIn FL, the loss function ff is an average of client loss functions fi. The client loss fi is an expectation over batches of data examples Bi on client i.\n\nff(x) =\n\n1 N\n\nN (cid:88)\n\ni=1\n\nfi(x),\n\nfi(x) = EBi [fi(x; Bi)]\n\n(2)\n\nFEDAVG (McMahan et al., 2017) is a ubiquitous, heuristic FL method designed to minimize Equation 2 w.r.t. model x in a manner that allows all client data (Bi) to remain at respective clients i. Providing strong privacy protection is a major motivation for FL. Storing raw data locally on clients rather than replicating it on servers decreases the attack surface of the system. Also, using focused ephemeral updates and early aggregation follows principles of data minimization (White House Report, 2013).4\n\nWhile training with loss ff via FEDAVG can yield an effective model x, this paper shows there are scenarios where ‘mixing’ in an additional ‘centralized’ loss fc proves beneficial to the training of x. Such a loss term can make use of batches of centralized data examples Bc, from a datacenter dataset:\n\nfc(x) = EBc [fc(x; Bc)]\n\n(3)\n\n3Were they not to differ, one could treat a centralized compute node as an additional client in standard FL,\n\nand simply make use of an established FL algorithm like FEDAVG for training x.\n\n4Even stronger privacy properties are possible when FL is combined with technologies such as differential\n\nprivacy (DP) and secure multiparty computation (SMPC) (Wang et al., 2021).\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nThe centralized loss fc will differ from the federated loss ff (else it would not be useful). This can be a difference in the distributions that Bc and Bi are drawn from, and/or in the functional forms of fc and fi. We will present an expression that quantifies the difference between fc and ff in Section 5.\n\nWe now state our mixed FL algorithms (Algorithms 1 and 2). Appendix A has a few practical details.\n\n• PARALLEL TRAINING performs a round of FEDAVG (minimizing ff) in parallel with steps of centralized training (minimizing fc), merges (e.g., averages) the respective updates, and repeats. Green in Algorithm 1 indicates added steps beyond FEDAVG for PARALLEL TRAINING.\n\n• 1-WAY GRADIENT TRANSFER starts a round by calculating a gradient of fc. It is sent to participating clients and summed with clients’ gradients of fi during client optimization. Blue in Algorithm 2 indicates added steps beyond FEDAVG for 1-WAY GRADIENT TRANSFER. • 2-WAY GRADIENT TRANSFER is PARALLEL TRAINING with gradient sharing. Two gradients are now used, one based on fc and sent to clients (like 1-W GT), one based on ff and applied centrally. Purple in Algorithm 1 is added steps beyond PT for 2-WAY GRADIENT TRANSFER.\n\nAlgorithm 1: PARALLEL TRAINING and 2-WAY GRADIENT TRANSFER (FEDAVG with added steps for PARALLEL TRAINING and further steps for 2-WAY GRADIENT TRANSFER)\n\nInput: Initial model x(0); CLIENTOPTIMIZER, SERVEROPTIMIZER, CENTRALOPTIMIZER, MERGEOPTIMIZER with respective\n\nlearning rates η, ηs, ηc, ηm; central loss function fc (3); initial augmenting centralized/federated gradients, ̃g(0)\n\n, ̃g(0)\n\nf\n\n(zeroed)\n\nc\n\nfor t ∈ {0, 1, . . . , T − 1} do\n\nInitialize central model x(t,0) for central step k = 0, . . . , Kc − 1 do Sample centralized batch B(k) Perform central update x(t,k+1)\n\n= x(t)\n\nc\n\nc\n\nc\n\n; compute stochastic gradient gc(x(t,k) = CENTRALOPTIMIZER(x(t,k)\n\n; B(k) , gc(x(t,k)\n\nc\n\nc\n\nc\n\nc\n\n) of fc(x(t,k)\n\n)\n\nc\n\n; B(k)\n\nc\n\n) + ̃g(t)\n\nf\n\n, ηc, t)\n\nCompute central model delta ∆(t) Sample a subset S (t) of clients; for client i ∈ S (t) in parallel do\n\nc = x(t,Kc )\n\n− x(t)\n\nc\n\n∆(t)\n\ni\n\n, pi = CLIENTUPDATE (x(t), ̃g(t)\n\nc\n\n, CLIENTOPTIMIZER, η, t)\n\ni / (cid:80)\n\ni∈S(t) pi\n\ni∈S(t) pi∆(t)\n\nf = SERVEROPTIMIZER(x(t), −∆(t), ηs, t)\n\nAggregate client changes ∆(t) = (cid:80) Compute federated model x(t) Compute federated model delta ∆(t) c + ∆(t) Aggregate central model and federated model deltas ∆(t) = ∆(t) Update global model x(t+1) = MERGEOPTIMIZER(x(t), −∆(t), ηm, t) c /(ηcKc) − ̃g(t) Update augmenting centralized gradient ̃g(t+1) i /(η (cid:80) i∈S(t) ∆(t) Update augmenting federated gradient ̃g(t+1)\n\nf = x(t)\n\nf − x(t)\n\n= −∆(t)\n\n= − (cid:80)\n\nc\n\nf\n\nf\n\nf\n\ni∈S(t) Ki) − ̃g(t)\n\nc\n\n(see Appendix A)\n\n; (possible) augmenting gradient ̃g(t)\n\n; CLIENTOPTIMIZER, learning rate η; round t; client loss fi (2)\n\nCLIENTUPDATE: Input: Initial client model x(t,0) i\nInitialize client weight pi = 0 for client step k = 0, . . . , Ki − 1 do\n\nSample batch B(k) Perform client update x(t,k+1)\n\ni\n\ni\n\nc\n\ni\n\n; compute stochastic gradient gi(x(t,k)\n\n; B(k)\n\n) of fi(x(t,k)\n\n); update client weight pi = pi + |B(k)\n\n|\n\ni\n\ni\n\ni\n\n= CLIENTOPTIMIZER(x(t,k)\n\ni\n\n, gi(x(t,k)\n\ni\n\n; B(k)\n\ni\n\n) + ̃g(t)\n\nc\n\n, η, t)\n\nCompute client model changes ∆(t)\n\ni = x(t,Ki)\n\ni\n\n− x(t,0)\n\ni\n\nand return ∆(t)\n\ni\n\n, pi\n\nAlgorithm 2: 1-WAY GRADIENT TRANSFER (FEDAVG (McMahan et al., 2017) with added steps)\n\nInput: Initial model x(0); CLIENTOPTIMIZER, SERVEROPTIMIZER with respective learning rates η, ηs; central loss function fc (3) for t ∈ {0, 1, . . . , T − 1} do Sample batch B(t) Sample a subset S (t) of clients; for client i ∈ S (t) in parallel do\n\n; compute stochastic gradient gc(x(t); B(t)\n\n) of fc(x(t)); set augmenting gradient ̃g(t)\n\nc = gc(x(t); B(t)\n\nc\n\nc\n\nc\n\n)\n\n∆(t)\n\ni\n\n, pi = CLIENTUPDATE (x(t), ̃g(t)\n\nc\n\n, CLIENTOPTIMIZER, η)\n\n(CLIENTUPDATE function defined in Algorithm 1)\n\nAggregate client changes ∆(t) = (cid:80) Update global model x(t+1) = SERVEROPTIMIZER(x(t), −∆(t), ηs, t)\n\ni∈S(t) pi∆(t)\n\ni∈S(t) pi\n\ni / (cid:80)\n\n3 RELATED WORK\n\nThere are parallels between GRADIENT TRANSFER and algorithms addressing inter-client data heterogeneity in FL, like SCAFFOLD (Karimireddy et al., 2020b) or Mime (Karimireddy et al., 2020a). Those algorithms calculate a gradient reflective of the entire federated population and transmit it to clients to reduce update variance, improving optimization on non-IID client data. GRADIENT TRANSFER calculates a gradient reflective of centralized data/loss, to augment client computations\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nof decentralized data/loss (in 2-W GT, also the converse). However, SCAFFOLD requires keeping state at the server (control variates) for each participating client, impractical in real large-scale FL systems (Kairouz et al., 2019). 2-W GT only requires state (augmenting gradients) for two entities, and so is easily implemented.\n\nAnother instance of a non-IID client approach (Zhao et al., 2018) influencing mixed FL is the EXAMPLE TRANSFER algorithm (Anonymous, a). Here, centralized examples are sent to federated clients, instead of gradients. This is typically precluded in real FL applications. The data volume needed to transfer may be excessive, and in general EXAMPLE TRANSFER does not offer solutions for one of the main motivations of this work, reducing client computation and communication.\n\n‘Transfer learning’ also involves two different training datasets, but has a different purpose. Transfer learning pretrains a model on a distribution (e.g., centralized data in a datacenter), then fine-tunes it on the actual distribution of interest (e.g., decentralized data via FL). It is desirable as a way to quickly train on the latter distribution (e.g., as in Ro et al. (2022)). But its sequential approach results in catastrophic forgetting (McCloskey & Cohen, 1989; Ratcliff, 1990; French, 1999); accuracy on pretraining data is lost as the model learns to fit fine-tuning data instead. In mixed FL, we desire good inference performance on all data distributions trained on. See Appendix B.4.5 for more.\n\nIn differentially private (DP) optimization, a line of work has aimed to improve privacy/utility tradeoffs by utilizing additional non-private data. One way is to use non-private data to pretrain (Abadi et al., 2016). Another avenue is to use non-private data to learn the gradient geometry (Zhou et al., 2020; Amid et al., 2021; Asi et al., 2021; Kairouz et al., 2021a; Li et al., 2022), improving accuracy by enabling tighter, non-isotropic gradient noise during DP optimization. Amid et al. (2021) and Li et al. (2022) consider the FL use case5. As in transfer learning, additional data is used only to improve performance on a single distribution, and retaining accuracy on other distributions is a nongoal (in contrast to mixed FL). Also, the non-private data used is generally matching (in distribution) to the private data, whereas in mixed FL we typically explicitly leverage distinct distributions.\n\nA few recent works (Yu et al., 2020; Elbir et al., 2021; Jeong et al., 2021) present instances of mixed FL problems; they shift computations to the server that are intensive or impossible at the clients. These works do not address distribution shift or present more general mixed FL algorithms.\n\n4 EXPERIMENTS\n\nWe now present experiments on three tasks (Table 1), showing a range of problems for which mixed FL is useful. They also show the comparative performance of each algorithm described in Section 2.\n\nTable 1: Experiments summary. For training hyperparameters and further details, see Appendix B.\n\nMODEL ARCH. TYPE\n\nFULLY-CONNECTED\n\nRNN\n\nDUAL ENCODER\n\nSMILE CLASSIFICATION\n\nLANGUAGE MODELING MOVIE RECOMMEND.\n\nFEDERATED DATA FED. CLIENT LOSS (fi) BINARY C.E. FED. WEIGHT (wf)\n\n0.5\n\nCELEBA (SMILING)\n\nSTACK OVERFLOW CATEGORICAL C.E. 0.75\n\nMOVIELENS HINGE 0.5\n\nCENTRALIZED DATA CENT. LOSS (fc) CENT. WEIGHT (wc)\n\nCELEBA (NON-SMILING) WIKIPEDIA BINARY C.E. 0.5\n\nCATEGORICAL C.E. 0.25\n\n- SPREADOUT (REG.) 0.5\n\n4.1 ADDRESS LABEL IMBALANCE IN TRAIN DATA (SMILE CLASSIFIER; CELEBA)\n\nEarlier work (Anonymous, a) motivated mixed FL with the example problem of training a ‘smiling’- vs.-‘unsmiling’ classifier via FL with mobile phones, with the challenge that the phones’ camera application (by the nature of its usage) tends to only persist images of smiling faces. A solution\n\n5An interesting similarity between PDA-DPMD (Amid et al., 2021) and our work: in PDA-DPMD for FL, a first order approximation of mirror descent is used, where the server model update is calculated as weighted sum of private (federated) and public loss terms, just as in PARALLEL TRAINING or 2-WAY GRADIENT TRANSFER.\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Smile Classifier: Eval. AUC (ROC) vs. Round.\n\n(b) Language Model: Evaluation Accuracy vs. Round.\n\nFigure 1: Mixed FL resolves distribution shift, enabling accuracy equal to if data were colocated and centrally trained (‘oracle’). The smile classifier reaches an oracle’s evaluation AUC of ROC of over 0.95. The language model reaches an oracle’s evaluation accuracy of over 0.66. Evaluation is over all data (i.e., smiling and non-smiling faces; Stack Overflow and Wikipedia). Plots show 95% conf.\n\n(a) Smile Classifier: Eval. Loss vs. Round.\n\n(b) Language Model: Eval. Loss vs. Round.\n\nFigure 2: Comparative convergence differs; (a) shows PT converging worse than 1-W GT or 2-W GT, while (b) shows all algorithms converging the same. See Section 5.3 for theoretical explanation.\n\nfor this severe label imbalance is to apply mixed FL, utilizing an additional datacenter dataset of unsmiling faces to train a capable classifier. To experiment, CelebA data6 (Liu et al., 2015; Caldas et al., 2018) is split into a federated ‘smiling’ dataset and centralized ‘unsmiling’ dataset.\n\nFigures 1a and 2a show the AUC and loss convergence of our three algorithms when applied to this problem. Empirically, we observe GRADIENT TRANSFER converge faster than PARALLEL TRAINING. Section 5 will provide a theoretical explanation for this behavior.\n\n4.2 MITIGATE BIAS IN TRAIN DATA (LANGUAGE MODEL; STACK OVERFLOW, WIKIPEDIA)\n\nConsider the problem of learning a language model like a RNN-based next character prediction model, used to make typing suggestions to a user in a mobile keyboard application. Because the ultimate inference application is on mobile phones, it is natural to train this model via FL, leveraging cached SMS text content highly reflective of inference time usage (at least for some users).\n\nHowever, the mobile phones participating in the federated learning of the model might be only a subset of the mobile phones for which we desire to deploy for inference. Higher-end mobile phones can disproportionately participate in FL, as their larger memory and faster processors allow them to complete client training faster. But to do well at inference, a model should make accurate predictions for users of lower-end phones as well. A purely FL approach can do an inadequate job of learning these users’ usage patterns. (See Kairouz et al. (2019) for more on aspects of fairness and bias in FL.)\n\nMixed FL overcomes this problem, by training a model jointly on federated data (representative of users of higher-end phones) and a datacenter dataset (representative of users of lower-end phones). We simulate this scenario using two large public datasets: the Stack Overflow dataset7 (Kaggle) for federated data, and the Wikipedia dataset8 (Wikimedia Foundation) for datacenter data. Figure 1b shows results. The ‘only FL’ scenario learns Stack Overflow (but not Wikipedia) patterns of character usage, and so has reduced accuracy (∼ 0.60) when evaluated on examples from both datasets. The\n\n6CelebA federated data available via open source FL software (TFF CelebA documentation, 2022). 7Stack Overflow federated data available via open source (TFF StackOverflow documentation, 2022). 8Wikipedia data available via open source (TFDS Wikipedia (20201201.en) documentation, 2022).\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Movie recommendation: computation (COMP.) and communication (COMM.) overhead per client. The baseline scenario computes everything on clients. See Appendix B.3 for analysis.\n\nCOMP. (MFLOP)\n\nCOMM. (KB)\n\nBASELINE PT 1-W GT 2-W GT\n\n125.16 0.025 0.025 0.025\n\n494 20 30 30\n\nFigure 3: Movie prediction, Eval. Loss vs. Round. We see that mixed FL results in similar loss as the more expensive baseline scenario (see Table 2).\n\nmixed FL algorithms demonstrate learning both: they all achieve an evaluation accuracy (∼ 0.66) comparable to an imagined ‘oracle’ that could centrally train on the combination of datasets.\n\nUnlike the smile classification experiment, here PARALLEL TRAINING converges about as fast as GRADIENT TRANSFER. Section 5 will provide a theoretical explanation for this behavior.\n\n4.3 REGULARIZE EMBEDDINGS AT SERVER (MOVIE RECOMMENDATION; MOVIELENS)\n\nThe third task we study, movie recommendation, is one with an embedding regularization term as described in Section 1. As Table 1 shows, a key difference from the previous two mixed FL experimental scenarios is that here we are mixing losses with different functional forms (instead of mixing different datacenter and federated data distributions). We study this scenario by training a dual encoder representation learning model (Covington et al., 2016) for next movie prediction on the MovieLens dataset (Harper & Konstan, 2015; GroupLens).\n\nAs described in Section 1, limited negative examples can degrade representation learning performance. Previous work (Anonymous, b) proposed using losses insensitive to local client negatives to improve federated model performance. They observed significantly improved performance by using a two-part loss: (1) a hinge loss to pull embeddings for similar movies together, and (2) a spreadout regularization (Zhang et al., 2017) to push embeddings for unrelated movies apart. For clients to calculate (2), the server must communicate all movie embeddings to each client, and clients must perform a matrix multiplication over the entire embedding table. This introduces enormous computation and communication overhead when the number of movies is large.\n\nMixed FL can alleviate this communication and computation overhead. Instead of computing both loss terms on clients, clients calculate only the hinge loss and the server calculates the expensive regularization, avoiding costly computation on each client. Also, computing the hinge loss only requires the embeddings of movies in a client’s local dataset. Federated select (Charles et al., 2022) enables only those embeddings to be sent to that client, saving communication and on-client memory.\n\nExperiments show all mixed FL algorithms achieve model performance (around 0.1 for recall@10) comparable to the baseline scenario where everything is computed on the clients. Moreover, mixed FL eliminates more than 99.9% of client computation and more than 93.9% of communication (see Table 2). For computation and communication analysis, see Appendix B. Note that real-world models can be much larger than this movie recommendation model9. Without mixed FL, communicating such large models to clients and computing regularization would be impractical in large-scale settings. PARALLEL TRAINING converges slightly slower than either GRADIENT TRANSFER algorithm (Figure 3) but reaches the same evaluation loss at around 1500 rounds.\n\n5 CONVERGENCE\n\n5.1 PRELIMINARIES\n\nWe now describe convergence properties for each mixed FL algorithm from Section 2. We assume mixed loss f has a finite minimizer (i.e., ∃ x∗ s.t. f (x) ≥ f (x∗) ∀ x). We assume client losses fi\n\n9E.g., for a next URL prediction task with millions of URLs the embedding table size can reach gigabytes.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nand centralized loss fc are β-smooth. If fi are β-smooth, federated loss ff is also10. For some results, we assume fi and fc are μ-convex (possibly strongly convex, μ > 0). If fi are μ-convex, ff is also11.\n\nFor a parameter vector x, we use ∇fi(x) to denote the full gradient of fi (i.e., over all data on client i). Similarly, ∇ff(x) and ∇fc(x) denote full gradients12 of ff and fc at x. We use gi(x) to denote an unbiased stochastic gradient of fi, calculated on a random batch Bi of examples on client i.\n\nWe focus on the impact to convergence when differences exist between the federated and centralized losses/data. As such, we make the following homogeneity assumption about the federated data, which simplifies the analysis and brings out the key differences. Our analysis can be easily extended to heterogeneous clients by assuming a bound on variance of the client gradients.\n\nAssumption 5.1. The federated clients have homogeneous data distributions (i.e., with examples that are drawn IID from a common data distribution), and their stochastic gradients have bounded variance. Specifically, for some σ > 0, we have for all clients i and parameter vectors x:\n\nE [gi(x)] = ∇ff(x), E (cid:107)gi(x) − ∇ff(x)(cid:107)2 ≤ σ2.\n\n(4)\n\nUnder such conditions, FEDAVG convergence can match SGD (Table 2, Karimireddy et al. (2020b)).\n\nLet gf denote an unbiased stochastic gradient of federated loss ff, formed by randomly sampling a cohort of S (out of N total) clients, randomly sampling a batch Bi of client data examples, and averaging the respective client stochastic gradients. Given Assumption 5.1 we bound variance of gf:\n\ngf(x) =\n\n1 S\n\n(cid:88)\n\ni∈S\n\ngi(x), E (cid:107)gf(x) − ∇ff(x)(cid:107)2 = E\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 S\n\n(cid:88)\n\ni∈S\n\ngi(x) − ∇ff(x)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n≤\n\n1 S\n\nσ2\n\n(5)\n\nLet gc(x) denote a stochastic gradient of the centralized loss fc at x, calculated on a randomly sampled batch Bc of centralized examples (from a datacenter dataset), with variance bounded by σ2 c :\n\nE (cid:107)gc(x) − ∇fc(x)(cid:107)2 ≤ σ2 c .\n\n(6)\n\nSummarizing Equations 4-6, a client’s stochastic gradient gi(x) has variance bounded by σ2, the federated cohort’s stochastic gradient gf(x) has variance bounded by σ2/S, and the centralized stochastic gradient gc(x) has variance bounded by σ2 c . Increasing client batch size |Bi| reduces variance of gi(x) and gf(x), increasing cohort size S reduces variance of gf(x), and increasing central batch size |Bc| reduces variance of gc(x).\n\nNote that σ2/S only bounds variance within the federated data distribution, and σ2 c only bounds variance within the central data distribution. To say something about variance across the two data distributions, we adapt the notion of ‘bounded gradient dissimilarity’ (or ‘BGD‘) introduced in Karimireddy et al. (2020b) (Definition A1), and apply it to the mixed FL scenario here.\n\nDefinition 5.2 (mixed FL (G, B)-BGD). There exist constants G ≥ 0 and B ≥ 1 such that ∀x:\n\nwf\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇ff(x) wf\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n+ wc\n\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n∇fc(x) wc\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\n≤ G2 + B2 (cid:107)∇f (x)(cid:107)2\n\nIn the definition, wf and wc are proportions of influence (wf +wc = 1) of the federated and centralized objectives on the overall mixed optimization. (The simplest setting is wf = wc = 1/2.)\n\n5.2 BOUNDS\n\nWe can now state upper bounds on convergence (to an error in mixed loss smaller than (cid:15)) for the respective mixed FL algorithms. For ease of comparison, the convergence bounds are summarized in Table 3. The Theorems and Proofs of these convergence bounds are given in Appendix C. As mentioned\n\n10By its definition in Equation 2 combined with the triangle inequality. 11By its definition in Equation 2, it is convex combination of fi. 12Note: ∇ff(x) is useful for theoretical convergence analysis, but cannot be practically computed in a real\n\ncross-device FL setting. In contrast, ∇fc(x) can be computed.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Order of number of rounds required to be within (cid:15) of optimal mixed loss, for different mixing strategies. See Appendix C. σ2 as in (4), σ2 c as in (6), G and B as in Def. 5.2 with wf = wc = 1/2. β is smoothness (Def. D.1), μ is convexity bound (Def. D.4). K is client local steps taken (≥ 2), S is client cohort size (per round). D and F are initial distances/loss errors, described in Appendix C.\n\nPARALLEL TRAINING\n\n1-W GT\n\n2-W GT\n\nμ-CONVEX\n\nCONVEX\n\nNONCONVEX\n\n√ √\n\nβ\n\n(cid:15) + B2β\n\n(σ2+Sσ2\n\nc )\n\nKSμ(cid:15) + G c )D2\n\nμ\n\n(σ2+Sσ2 KS(cid:15)2 (σ2+Sσ2 KS(cid:15)2\n\nc )βF\n\n+ G\n\n+ G\n\n√\n\nβ\n\n3 (cid:15) 2\n√\n\n3 2\n\n(cid:15)\n\nβ\n\nμ log( 1 (cid:15) ) + B2βD2 + B2βF\n\n(cid:15)\n\n(cid:15)\n\n(σ2+KSσ2\n\nKSμ(cid:15) + β\n\nc )\n\n(σ2+KSσ2 KS(cid:15)2 (σ2+KSσ2 KS(cid:15)2\n\nc )D2\n\nc )βF\n\nμ log( 1 (cid:15) ) + βD2\n\n(cid:15)\n\n+ βF\n\n(cid:15)\n\n(σ2+Sσ2\n\nc )\n\nKSμ(cid:15) + β c )D2\n\n(σ2+Sσ2 KS(cid:15)2 (σ2+Sσ2 KS(cid:15)2\n\nc )βF\n\n+ βF\n\n(cid:15)\n\nμ log( 1 (cid:15) ) + βD2\n\n(cid:15)\n\npreviously, the analysis extends in a straightforward manner to the setting of heterogeneous clients assuming a bound on the variance of client gradients: for all x, 1 i=1 (cid:107)∇fi(x) − ∇ff(x)(cid:107)2 ≤ σ2 for some σf ≥ 0. Under this assumption, the bounds in Table 3 change by an additional Kσ2 f term in the expression involving σ2 and σ2 c in the parenthesis on the numerator of the leading term. The derivation of these more general bounds follows on the same lines, so we omit it for brevity.\n\n(cid:80)N\n\nN\n\nf\n\nAnalyzing Table 3, there are several implications to be drawn.\n\nSignificant (G,B)-BGD impedes PARALLEL TRAINING The convergence bounds for PARALLEL TRAINING show a dependence on the G and B parameters from Definition 5.2. If a mixed FL problem involves a large amount of dissimilarity between the federated and centralized gradients (i.e., if G (cid:29) 0 or B (cid:29) 1), then PARALLEL TRAINING will be slower to converge than alternatives.\n\nSignificant σ2 c impedes 1-WAY GRADIENT TRANSFER 1-WAY GRADIENT TRANSFER is more sensitive to central variance σ2 c on convergence scales with the number of steps K. 1-WAY GRADIENT TRANSFER requires a central batch size |Bc| that is K times larger to achieve the same impact on convergence. Intuitively, this makes sense; in a round, PARALLEL TRAINING and 2-WAY GRADIENT TRANSFER sample K fresh batches during centralized optimization, while 1-WAY GRADIENT TRANSFER only samples a single central batch.\n\nc . Unlike the other algorithms, the impact of σ2\n\n2-WAY GRADIENT TRANSFER should always converge at least as well as others The convergence bound for 2-WAY GRADIENT TRANSFER is unaffected by gradient dissimilarity (i.e., G (cid:29) 0 or B (cid:29) 1), unlike PARALLEL TRAINING. Also, the bound for 2-WAY GRADIENT TRANSFER is less sensitive to σ2 c than the bound for 1-WAY GRADIENT TRANSFER (as described above).\n\n5.3 ANALYSIS OF EXPERIMENTS\n\nPARALLEL TRAINING has a convergence bound substantially different than the GRADIENT TRANSFER algorithms; the dependence on the BGD parameters G and B indicates there are mixed FL problems where PARALLEL TRAINING is slower to converge than GRADIENT TRANSFER (in either form). How can we know if a particular problem is one where PARALLEL TRAINING will have slower convergence? It would be useful to know G and B, but they cannot be exactly measured. G and B (Definition 5.2) are upper bounds holding ∀x, and the entire space of x cannot realistically be checked. Instead, we introduce sampled approximations to empirically estimate these upper bounds. Let x(t) be the global model at start of round t. Let ̃∇fft, ̃∇fct, ̃∇f t be approximations of federated, centralized, total gradients at round t. Considering Definition 5.2, we define ̃Gt as a sampled approximation of G assuming B = 1, and ̃Bt as a sampled approximation of B assuming G = 0:\n\n ̃∇fft =\n\n1 S\n\n ̃G2\n\nt =\n\n1 wf\n\n(cid:13) (cid:13) (cid:13)\n\n ̃∇fft\n\n2\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:88)\n\n(cid:16)\n\ngi(x(t))\n\n(cid:17)\n\n,\n\n ̃∇fct = gc(x(t)),\n\n ̃∇f t = ̃∇fft + ̃∇fct\n\ni∈S (cid:13) ̃∇fct (cid:13) (cid:13)\n\n1 wc\n\n2\n\n(cid:13) (cid:13) (cid:13)\n\n−\n\n(cid:13) (cid:13) (cid:13)\n\n ̃∇f t\n\n2\n\n(cid:13) (cid:13) (cid:13)\n\n,\n\n ̃B2\n\nt =\n\n(cid:18) 1 wf\n\n(cid:13) ̃∇fft (cid:13) (cid:13)\n\n2\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\n1 wc\n\n(cid:13) (cid:13) (cid:13)\n\n ̃∇fct\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13)\n\n/\n\n ̃∇f t\n\n2\n\n(cid:13) (cid:13) (cid:13)\n\n(7)\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 4: Sampled approximations of (G,B)-BGD. (See Appendix B.1 for details.)\n\nmax10≤t≤100 ̃G2 max10≤t≤100 ̃B2\n\nt\n\nt\n\nSMILE CLASSIFICATION LANGUAGE MODELING MOVIE RECOMMEND.\n\n49.04 1.26\n\n0.03 1.34\n\n0.03 1.50\n\nTable 4 states maximums of ̃G2 theoretical explanation for empirical observations made during the experiments.\n\nt for the experiments of Section 4. This provides some\n\nt and ̃B2\n\n• Smile Classification (4.1) As discussed, PARALLEL TRAINING is at a disadvantage when G (cid:29) 0 or B (cid:29) 1, and Table 4 (left column) shows that ̃Gt is significantly large in smile classification. This aligns with our empirical observation in Figure 2a of PARALLEL TRAINING converging slower than either GRADIENT TRANSFER algorithm.\n\n• Language Modeling (4.2) Table 4 (center column) shows language modeling with more trivial gradient dissimilarity: ̃Gt ≈ 0 and ̃Bt ≈ 1. PARALLEL TRAINING should be competitive. Figure 2b empirically confirms this to be true; all algorithms converge roughly equivalently.\n\n6 CONCLUSION\n\nThis paper has introduced mixed FL, including motivation, algorithms and their convergence properties, and intuition for when a given algorithm will be useful for a given problem. Our experiments indicate mixed FL can improve accuracy and reduce communication and computation across tasks.\n\nThis work focused on jointly learning from a single decentralized client population and a centralized entity, as it illuminates the key aspects of the mixed FL problem. Note that mixed FL and the associated properties we define in this paper (like mixed FL (G, B)-BGD) are easily expanded to work with multiple (> 1) distinct client populations participating. E.g., a population of mobile phones and a separate population of smart speakers, or mobile phones separated into populations with distinct capabilities/usage (high-end vs. low-end, or by country/language). Also, there need not be a centralized entity; mixing can be solely between distinct federated datasets.\n\nIt is interesting to reflect on the bounds of Table 3, and what they indicate about the benefits of separating a single decentralized client population into multiple populations for mixed FL purposes. The bounds are in terms of σ2 (representing within population ‘variability’) and G and B (representing cross-population ‘variability’). Splitting a population based on traits will likely decrease σ2 (each population is now more homogeneous) but introduce or increase G and B (populations are now distinctive). This might indicate scenarios where GRADIENT TRANSFER methods (only bounded by σ2) become more useful and PARALLEL TRAINING (also bound by G and B) becomes less useful.\n\nThe limits of our convergence bounds should be noted. First, they are ‘loose’; practical performance in particular algorithmic scenarios could be better, and thus comparisons between algorithms could differ. Second, our bounds assume IID federated data, which is invalid in practice; convergence properties differ on non-IID data. While our analysis, extended to handle non-IID data, shows that the bounds do not materially change, it is still a place where theory and practice slightly diverge.\n\nIt is important to note that mixed FL is orthogonal to the choice of algorithm for federated training. For simplicity, this paper described mixed FL with FEDAVG used for federated training. However, one could use DP-FedAvg (McMahan et al. (2018)) or DP-FTRL (Kairouz et al. (2021b)) with mixed FL to ensure that DP protections are applied to the federated data. Similarly, an adaptive SERVEROPTIMIZER like FEDADAM (Reddi et al., 2020) could be used in the federated portion of training (preliminary mixed FL results with FEDADAM are given in Appendix B.4.4).\n\nIn principle, mixed FL techniques are expected to have positive societal impacts insofar as they further develop the toolkit for FL (which has security and privacy benefits to users) and improve accuracy on final inference distributions. Also, we’ve shown (Section 4.2) how mixed FL can address participation biases that arise in FL. However, the addition of server-based data to federated optimization raises the possibility that biases in large public corpora find their way into more applications of FL.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nMartin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In 23rd ACM Conference on Computer and Communications Security (ACM CCS), 2016.\n\nEhsan Amid, Arun Ganesh, Rajiv Mathews, Swaroop Ramaswamy, Shuang Song, Thomas Steinke, Vinith M Suriyakumar, Om Thakkar, and Abhradeep Thakurta. Public data-assisted mirror descent for private model training. arXiv preprint arXiv:2112.00193, 2021.\n\nGalen Andrew, Om Thakkar, Brendan McMahan, and Swaroop Ramaswamy. Differentially private learning with adaptive clipping. Advances in Neural Information Processing Systems, 34:17455– 17466, 2021.\n\nAnonymous. Anonymized for iclr submission review., a.\n\nAnonymous. Anonymized for iclr submission review., b.\n\nApple. Designing for privacy (video and slide deck). Apple WWDC, https://developer.\n\napple.com/videos/play/wwdc2019/708, 2019.\n\nHilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient methods for convex optimization. In International Conference on Machine Learning, pp. 383–392. PMLR, 2021.\n\nSebastian Caldas, Peter Wu, Tian Li, Jakub Koneˇcný, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nZachary Charles, Kallista Bonawitz, Stanislav Chiknavaryan, Brendan McMahan, et al. Federated select: A primitive for communication-and memory-efficient federated learning. arXiv preprint arXiv:2208.09432, 2022.\n\nPaul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations.\n\nIn Proceedings of the 10th ACM conference on recommender systems, pp. 191–198, 2016.\n\nAhmet M Elbir, Sinem Coleri, and Kumar Vijay Mishra. Hybrid federated and centralized learning. In 2021 29th European Signal Processing Conference (EUSIPCO), pp. 1541–1545. IEEE, 2021.\n\nRobert French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3:\n\n128–135, 05 1999. doi: 10.1016/S1364-6613(99)01294-2.\n\nGroupLens. Movielens 1m dataset.\n\nURL https://grouplens.org/datasets/\n\nmovielens/1m/.\n\nAndrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.\n\nAndrew Hard, Kurt Partridge, Neng Chen, Sean Augenstein, Aishanee Shah, Hyun Jin Park, Alex Park, Sara Ng, Jessica Nguyen, Ignacio Lopez Moreno, et al. Production federated keyword spotting via distillation, filtering, and joint federated-centralized training. arXiv preprint arXiv:2204.06322, 2022.\n\nF Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm\n\ntransactions on interactive intelligent systems (tiis), 5(4):1–19, 2015.\n\nFlorian Hartmann. Predicting text selections with federated learning. Google AI Blog, https://ai. googleblog.com/2021/11/predicting-text-selections-with.html, 2021.\n\nWonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised learning with inter-client consistency & disjoint learning. In International Conference on Learning Representations (ICLR) 2021. International Conference on Learning Representations (ICLR), 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nKaggle.\n\nStack overflow data.\n\nURL https://www.kaggle.com/datasets/\n\nstackoverflow/stackoverflow.\n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.\n\nPeter Kairouz, Monica Ribero Diaz, Keith Rush, and Abhradeep Thakurta. (nearly) dimension\n\nindependent private erm with adagrad rates via publicly estimated subspaces. In Mikhail Belkin and Samory Kpotufe (eds.), Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pp. 2717–2746. PMLR, 15–19 Aug 2021a. URL https://proceedings.mlr. press/v134/kairouz21a.html.\n\nPeter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shuffling. In International Conference on Machine Learning, pp. 5213–5225. PMLR, 2021b.\n\nSai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in federated learning. arXiv preprint arXiv:2008.03606, 2020a.\n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated learning. International Conference on Machine Learning (ICML), 2020b.\n\nTian Li, Manzil Zaheer, Sashank J Reddi, and Virginia Smith. Private adaptive optimization with\n\nside information. arXiv preprint arXiv:2202.05963, 2022.\n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\n\nProceedings of International Conference on Computer Vision (ICCV), December 2015.\n\nMichael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The\n\nsequential learning problem. Psychology of Learning and Motivation, 24:109–165, 1989.\n\nH Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1273–1282, 2017. Initial version posted on arXiv in February 2016.\n\nH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. In International Conference on Learning Representations (ICLR), 2018.\n\nNicole Mitchell, Johannes Ballé, Zachary Charles, and Jakub Koneˇcn`y. Optimizing the communication-accuracy trade-off in federated learning with rate-distortion theory. arXiv preprint arXiv:2201.02664, 2022.\n\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\n\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n\nSwaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Françoise Beaufays. Federated learning\n\nfor emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019.\n\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H. Brendan McMahan, and Françoise Beaufays. Training production language models without memorizing user data. arXiv preprint arXiv:2009.10031, 2020.\n\nRoger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and\n\nforgetting functions. Psychological review, 97 2:285–308, 1990.\n\nSashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nJae Hun Ro, Theresa Breiner, Lara McConnaughey, Mingqing Chen, Ananda Theertha Suresh, Shankar Kumar, and Rajiv Mathews. Scaling language model size in cross-device federated learning. In ACL 2022 Workshop on Federated Learning for Natural Language Processing, 2022. URL https://openreview.net/forum?id=ShNG29KGF-c.\n\nSebastian U Stich. Unified optimal analysis of the (stochastic) gradient method. arXiv preprint\n\narXiv:1907.04232, 2019.\n\nTFDS Wikipedia (20201201.en) documentation. Tensorflow datasets (tfds) wikipedia documentation, 2022. URL https://www.tensorflow.org/datasets/catalog/wikipedia# wikipedia20201201en.\n\nTFF CelebA documentation.\n\ntff.simulation.datasets.celeba.load_data documentation, 2022. URL https://www.tensorflow.org/federated/api_docs/ python/tff/simulation/datasets/celeba/load_data.\n\nTFF StackOverflow documentation. tff.simulation.datasets.stackoverflow.load_data\n\ndocumentation, 2022. URL https://www.tensorflow.org/federated/api_docs/ python/tff/simulation/datasets/stackoverflow/load_data.\n\nJianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan McMahan, Blaise Agüera y Arcas, Maruan Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.\n\nWhite House Report. Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy. Journal of Privacy and Confidentiality, 2013.\n\nWikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.\n\nFelix Yu, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. Federated learning with\n\nonly positive labels. 2020.\n\nXu Zhang, Felix X Yu, Sanjiv Kumar, and Shih-Fu Chang. Learning spread-out local feature descriptors. In Proceedings of the IEEE international conference on computer vision, pp. 4595– 4603, 2017.\n\nYue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated\n\nlearning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.\n\nYingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private\n\nsgd with gradient subspace identification. arXiv preprint arXiv:2007.03813, 2020.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA PRACTICAL IMPLEMENTATION DETAILS\n\nA.1 DOWNLOAD SIZE\n\nGRADIENT TRANSFER (either 1-WAY or 2-WAY) requires sending additional data as part of the communication from server to clients at the start of a federated round. Apart from the usual model checkpoint weights, with GRADIENT TRANSFER we must now also transmit ‘augmenting’ gradients of the model weights w.r.t centralized data as well. Naively, this doubles the download size as the gradient is the same size as the model. However, the augmenting gradients should be amenable to compression, e.g. using an approach such as Mitchell et al. (2022).\n\nA.2 UPLOAD SIZE\n\nWith PARALLEL TRAINING and 1-WAY GRADIENT TRANSFER, no client gradient information is used outside of the clients themselves, so there is no additional information (apart from the model deltas and aggregation weights) to upload to the server. With 2-WAY GRADIENT TRANSFER, client gradient information is used in centralized training, so needs to be conveyed to the server somehow.\n\nWhen the FL client optimization is SGD, the average client gradient in a round (over all clients participating, over all steps) can be determined from the model deltas and aggregation weights that are already being sent back to the server, meaning no additional upload bandwidth is necessary. Apart from bandwidth considerations, this also means there is no additional vector for private data leakage. The algorithm to do this is as follows.\n\nEach client i transmits to the server a local model change ∆(t) and an aggregation weight pi that is related to number of steps taken Ki. The average total gradient applied at client i during round t is:\n\ni\n\nThe average client gradient (i.e., w.r.t. just client data) at client i is:\n\n ̄g(t) = −\n\n1 ηKi\n\n∆(t)\n\ni\n\n ̄g(t) i = −\n\n1 ηKi\n\n∆(t)\n\ni − ̃g(t)\n\nc\n\n(8)\n\n(9)\n\nwhere ̃g(t) in round t. The average (across the cohort) of average client gradients, weighted by Ki, is:\n\nis the augmenting centralized gradient that was calculated from centralized data and used\n\nc\n\n ̄g(t) f = −\n\n1 η (cid:80) i Ki\n\n(cid:88)\n\ni\n\n∆(t)\n\ni − ̃g(t)\n\nc\n\n(10)\n\nThis average client gradient ̄g(t) 4, Option II. It will be used as the augmenting federated gradient ̃g(t+1) t + 1, to augment centralized optimization. See Algorithm 1.\n\nf\n\nf\n\nis in the spirit of SCAFFOLD (Karimireddy et al., 2020b) Equation\n\nin the subsequent round\n\nA.3 DEBUGGING AND HYPERPARAMETER INTUITION VIA K = 1\n\nAs these algorithms each involve different hyperparameters, validating that software implementations13 are behaving as expected is non-trivial. Something that proved useful for debugging purposes, as well as provided practical experience in understanding equivalences between the algorithms, was to perform test cases with the number of local steps K set to 1. In this setting, the three mixed FL algorithms are effectively identical and should make equivalent progress during training.\n\nNote that the convergence bounds of Table 3 hold for K ≥ 2, so this takes us outside the operating regime where the bounds predict performance. It also takes us outside an operating regime that is typically useful (FL use cases generally find multiple steps per round to be beneficial). But it does serve a purpose when debugging.\n\n13An open-source mixed FL code repository will be shared with the public version of the paper.\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\n(a) ̃G2\n\nt vs. Round t.\n\n(b) ̃B2\n\nt vs. Round t.\n\nFigure 4: Sampled approximations of mixed FL (G, B)-BGD, for the three experiments in Section 4.\n\nB EXPERIMENTS: ADDITIONAL INFORMATION AND RESULTS\n\nB.1\n\n ̃Gt AND ̃Bt METRICS PLOTS\n\nTable 4 is an informative comparison of the mixed FL optimization landscape of the three respective experiments conducted in Section 4. It includes maximum values for the metrics ̃Gt and ̃Bt (the sampled approximations of the parameters defined in (G, B)-BGD (Definition 5.2). Here we provide some additional information.\n\nFigure 4 plots these sampled approximation metrics over the first 100 rounds of training. We ran 5 simulations per experiment and took the maximum at each round across simulations. We used the same hyperparameters as described below (in Subsection B.2), except taking only a single step per round (K = 1).\n\nB.2 ADDITIONAL DETAILS FOR EXPERIMENTS IN SECTION 4\n\nGeneral Notes on Hyperparameter Selection For the various experiments in Section 4, we empirically determined good hyperparameter settings (as documented in Tables 5-10). Our general approach for each task was to leave server learning rate ηs at 1, select a number of steps K that made the most use of the examples in each client’s cache, and then do a sweep of client learning rates η to determine a setting that was fast but didn’t diverge. For PARALLEL TRAINING and 2-WAY GRADIENT TRANSFER, which involve central optimization and merging, we set the merging learning rate ηm to be 1, and set the central learning rate ηc as the product of client and server learning rates: ηc = ηηs (and since ηs = 1, this meant client and central learning rates were equal).\n\nGeneral Notes on Comparing Algorithms We generally kept hyperparameters equivalent when comparing the algorithms. For example, we aimed to set batch sizes for all algorithms such that central and client gradient variances σ2 and σ2 c have equivalent impact on convergence (meaning |Bc| = S|Bi| for PT and 2-W GT, and |Bc| = KS|Bi| for 1-W GT). In the case of language model training with 1-WAY GRADIENT TRANSFER, following this rubric would have meant a central batch size |Bc| of 12800; we reduced this in half for practical computation reasons. For a given task, we also generally kept learning rates the same for all algorithms. Interestingly, we observed that as η (and ηc, if applicable) is increased for a given task, the 2-WAY GRADIENT TRANSFER algorithm is the first of the three to diverge, and so we had to adjust, e.g., in the language modeling experiment we used a lower η for 2-W GT than for PT and 1-W GT.\n\nB.2.1 CELEBA SMILE CLASSIFICATION\n\nDatasets The CelebA federated dataset consists of 9,343 raw clients, which can be broken into train/evaluation splits of 8,408/935 clients, respectively (TFF CelebA documentation, 2022). The raw clients have average cache size of ∼ 21 face images. The images are about equally split between smiling and unsmiling faces. In order to enlarge cache size, we group three raw clients together into one composite client, so our federated training data involves 2,802 clients with caches of (on average) ∼ 63 face images (and about half that when we limit the clients to only have smiling faces).\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nTable 5: Smile classifier training, federated hyperparameters.\n\nTable 6: Smile classifier training, centralized and overall hyperparameters.\n\n(all)\n\nS\n\n|Bi| K\n\nη\n\n100\n\n5\n\n2\n\n0.01\n\nηs\n\n1.0\n\n(1-W GT) |Bc|\n\n|Bc| K ηc\n\n(PT and 2-W GT)\n\n(all)\n\nηm\n\n1.0\n\nwf\n\n0.5\n\nwc\n\n0.5\n\n1000\n\n500\n\n2 = η\n\nTable 7: Language model training, federated hyperparameters.\n\nTable 8: Language model training, centralized and overall hyperparameters.\n\n(all)\n\nS\n\n|Bi| K\n\nη\n\nηs\n\n(1-W GT) |Bc|\n\n(PT and 2-W GT)\n\n|Bc| K ηc\n\n100\n\n8\n\n16 2.0 (2-W GT: 1.0) 1.0\n\n6400\n\n800\n\n16 = η\n\n(all)\n\nwf\n\nwc\n\n0.75\n\n0.25\n\nηm\n\n1.0\n\nTable 9: Movie recommender training, federated hyperparameters.\n\nTable 10: Movie recommender training, centralized and overall hyperparameters.\n\n(all) |Bi| K η\n\nS\n\n100\n\n16\n\n10\n\n0.5\n\nηs\n\n1.0\n\n(1-W GT) |Bc|\n\n|Bc| K ηc\n\n(PT and 2-W GT)\n\n(all)\n\nηm\n\n1.0\n\nwf\n\n0.5\n\nwc\n\n0.5\n\n−\n\n−\n\n10 = η\n\nOur evaluation data consists of both smiling and unsmiling faces, and is meant to stand in for the inference distribution (where accurate classification of both smiling and unsmiling inputs is necessary). Note that as CelebA contains smiling and unsmiling faces in nearly equal amounts, a high evaluation accuracy cannot come at the expense of one particular label being poorly classified.\n\nModel Architecture The architecture used is a very basic fully-connected neural network14 with a single hidden layer of 64 neurons with ReLU activations.\n\nHyperparameter Settings The settings used in mixed FL training are shown in Tables 5 and 6. SGD was used for all optimizers: CLIENTOPTIMIZER and SERVEROPTIMIZER, and (if PT or 2-W GT) CENTRALOPTIMIZER and MERGEOPTIMIZER.\n\nB.2.2 STACK OVERFLOW/WIKIPEDIA LANGUAGE MODELING\n\nDatasets The Stack Overflow dataset is a large-scale federated dataset, consisting of 342,477 training clients and 204,088 evaluation clients (TFF StackOverflow documentation, 2022). The training clients have average cache size of ∼ 400 examples, and evaluation clients have average cache size of ∼ 80 examples. The Wikipedia dataset (wikipedia/20201201.en) consists of 6,210,110 examples (TFDS Wikipedia (20201201.en) documentation, 2022), which we partition by designating the first 5,000,000 examples for training and the remainder for evaluation. All raw text data is processed into sequences of 100 characters.\n\nOur evaluation data is a combined dataset consisting of randomly shuffled examples drawn from the Stack Overflow evaluation clients and the Wikipedia dataset. After accounting for raw examples discarded during processing, the evaluation dataset is comprised of approximately 75% Stack Overflow and 25% Wikipedia. Hence, we used this weighting proportion when performing mixed FL training in the language modeling problem (Table 8).\n\n14Adapted from an online tutorial involving CelebA binary attribute classification: “TensorFlow Constrained\n\nOptimization Example Using CelebA Dataset”.\n\n15\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Evaluation Accuracy on Stack Overflow vs. Round.\n\n(b) Evaluation Accuracy on Wikipedia vs. Round.\n\nFigure 5: Language model accuracy, evaluated on only decentralized (left) or centralized (right) data.\n\nModel Architecture The architecture used is a recurrent neural network (RNN)15 with an embedding dimension of 256 and 1024 GRU units.\n\nHyperparameter Settings The settings used in mixed FL training are shown in Tables 7 and 8. Note that in this experiment we also found clipping the norm of gradients (both federated and centralized) to be beneficial, for all mixed FL algorithms. We used adaptive clipping for the federated gradients (via the method described in Andrew et al. (2021), without noise addition) and a fixed clip value of 0.2 for the centralized gradients. SGD was used for all optimizers: CLIENTOPTIMIZER and SERVEROPTIMIZER, and (if PT or 2-W GT) CENTRALOPTIMIZER and MERGEOPTIMIZER.\n\nEvaluation Accuracy on Individual Data Splits Figure 5 shows the accuracy of models (trained either via mixed FL or pure FL) when evaluated on only federated data (Stack Overflow) or only centralized data (Wikipedia) individually. Confirming what we expect, the various mixed FL algorithms do a good job of achieving accuracy on both datasets. But if we train only via FL (without mixing), then we do a good job of learning the federated data (Stack Overflow) character sequences, but aren’t nearly as accurate at predicting the next character in centralized data (Wikipedia) sequences.\n\nB.2.3 MOVIELENS MOVIE RECOMMENDATION\n\nDataset The MovieLens 1M dataset (GroupLens) contains approximately 1 million ratings from 6,040 users on 3,952 movies. Examples are grouped by user, forming a natural data partitioning across clients. For all mixed FL algorithms we study, we keep examples from 20% of users (randomly selecting 20% of users and shuffling the examples of these users) as the datacenter data, and use examples from the remaining 80% users as client data. With this data splitting strategy, the server data won’t include the same individual client distributions but it will still be sampled from the same meta distribution of clients. We then split the clients data into the train and test sets, resulting in 3,865 train, 483 validation, and 483 test users. The average cache size of each client is ∼ 160 examples.\n\nModel Architecture The architecture used is the same as Anonymous (b), a dual encoder representation learning model with a bag-of-word encoder for the left tower (which takes in a list of movies a user has seen) and a simple embedding lookup encoder for the right tower (which takes in the next movie a user sees).\n\nHyperparameter Settings The settings used in mixed FL training are shown in Tables 9 and 10. In Subsection 4.3, SGD was used for all optimizers: CLIENTOPTIMIZER and SERVEROPTIMIZER, and (if PT or 2-W GT) CENTRALOPTIMIZER and MERGEOPTIMIZER. See Appendix B.4.4 for an additional experiment where ADAM is used as the SERVEROPTIMIZER in 1-W GT.\n\nRecall@10 As mentioned in Subsection 4.3, all mixed FL algorithms achieved similar global recall@10 compared to the baseline. Figure 6 shows evaluation recall@10 over 2000 training rounds.\n\n15Adapted from an online tutorial involving next character prediction: “Text generation with an RNN”.\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 6: Next movie prediction performance (recall@10).\n\nTable 11: Computation and communication overheads for Movie Recommendation task.\n\nbaseline\n\nPARALLEL TRAINING\n\n1-W GT\n\n2-W GT\n\nComp. Comm.\n\n(K · N 2 · d)/2 2 · N · d\n\n(N 2 · d)/2 2 · n · d\n\n(N 2 · d)/2 3 · n · d\n\n(N 2 · d)/2 3 · n · d\n\nB.3 COMPUTATION AND COMMUNICATION SAVINGS FOR MOVIE RECOMMENDATION\n\nThis section provides a detailed analysis of the computation and communication savings brought by mixed FL in the movie recommendation task.\n\nFor movie recommendation, both the input feature and the label are movie IDs with a vocabulary size of N . They share the same embedding table, with an embedding dimension of d. The input features and label embedding layers account for most of the model parameters in a dual encoder. Therefore, we use the total size of the feature and label embeddings to approximate the model size: M = (N + N ) · d. Batch size is Bi and local steps per round is K. Let the averaged number of movies in each client’s local dataset for each training round be n, smaller than Bi · K.\n\nComputation As shown in the second row of Table 11, the amount of computation for regularization term is (K · N 2 · d)/2 if calculating on-device (baseline). When computing the regularization term on the server (mixed FL), the complexity is (N 2 · d)/2. The total computation saving with mixed FL is ((K − 1) · N 2 · d)/2. We use (N 2 · d)/2 instead of N 2 · d for regularization term computation which is more accurate for an optimized implementation.\n\nThe total computation complexity of the forward pass is O(Bid + Bid2 + B2 i d), where the three items are for the bag-of-word encoder, the context hidden layer, and similarity calculation. The hinge loss and spreadout computation is O(Bi) + O(0.5N 2d). The gradient computation is O(2Bid2 + 2B2 i d) for network backward pass and O(Bi) + O(N d) for hinge and spreadout. Therefore, when computing the regularization term on the server with mixed FL, the computation savings for each client is 1 − (Bid + 3Bid2 + 3B2 i d + 2Bi + 0.5N 2d + N d), which is 99.98% for all mixed FL algorithms.\n\ni d + 2Bi)/(Bid + 3Bid2 + 3B2\n\nCommunication The communication overheads of each algorithm are presented in the last row of Table 11. For the baseline, the server and each client need to communicate the full embedding table and the gradients, so the communication overhead is 2 · N · d or 494KB. With PARALLEL TRAINING, the server and each client only communicate movie embeddings and the gradients corresponding to movies in that client’s local datasets. Thus the communication traffic is reduced to 2 · n · d or 20KB. GRADIENT TRANSFER requires the server to send both the movie embeddings and gradients to each client. The communication overhead then becomes 3 · n · d or 30KB. Overall, mixed FL can save more than 93.9% communication overhead than the baseline.\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Eval. Loss vs. Round.\n\n(b) Eval. AUC (ROC) vs. Round.\n\nFigure 7: Smile classifier training, 1-W GT with various central batch sizes |Bc|.\n\n(a) Eval. Loss vs. Round.\n\n(b) Eval. AUC (ROC) vs. Round.\n\nFigure 8: Smile classifier training, 2-W GT with various central batch sizes |Bc|.\n\nB.4 ADDITIONAL OBSERVATIONS AND EXPERIMENTS\n\nB.4.1 EFFECT OF σ2\n\nC ON CONVERGENCE\n\nTable 3 shows that the theoretical bounds on rounds to convergence are directly proportional to the client variance bound σ2 and central variance bound σ2 c . Also, as discussed in Subsection 5.2, 1-WAY GRADIENT TRANSFER is more sensitive to high central variance than the other two algorithms. Whereas in the other algorithms the impact of σ2 c on convergence scales with cohort size S, in 1-WAY GRADIENT TRANSFER it scales with cohort size S and steps taken per round K.\n\nTo observe the effect of σ2 c in practice, and compare its effect on 1-WAY GRADIENT TRANSFER vs. 2-WAY GRADIENT TRANSFER, we ran sweeps of CelebA smile classification training, varying the central batch size |Bc|. The plots of evaluation loss and evaluation AUC of ROC are shown in Figures 7 (1-W GT) and 8 (2-W GT). For each central batch size setting, we ran 10 trials; the plots show the means of each setting’s trials, with corresponding 95% confidence bounds.\n\nFigure 7 confirms the sensitivity of 1-WAY GRADIENT TRANSFER to central variance, with experiments using larger central batches Bc converging faster than experiments using smaller central batches. However, at least in the case of this task, the benefits of lower variance disappear quickly. The convergence of AUC of ROC did not appreciably improve for central batch sizes larger than 25. Presumably there is little effect at these larger central batch sizes because in these cases the convergence is now dominated by client variance (i.e., further convergence improvements would come from increasing client batch size |Bi|).\n\nComparing Figure 8 with Figure 7, we empirically observe that 2-WAY GRADIENT TRANSFER has lower sensitivity than 1-WAY GRADIENT TRANSFER to central batch size/central variance.\n\nB.4.2 TRADING η FOR K\n\nThe convergence bounds of Table 3 have an additional implication, in regards to the trade off between client learning rate η (and central learning rate ηc) and number of local steps taken K.\n\nIt’s better to reduce η and ηc and increase K, but there are limits The convergence bounds are not related to client or central learning rate (η or ηc), but are inversely related to local steps K. In general, it’s best to take as many steps as possible, and if necessary reduce learning rates accordingly. But there are limits to how large K can be. First, clients have finite caches of data, and K will always be limited by cache size divided by batch size. Second, in the case of 1-WAY GRADIENT TRANSFER,\n\n18\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Eval. Loss vs. Round (PT).\n\n(b) Eval. AUC (ROC) vs. Round (PT).\n\n(c) Eval. Loss vs. Round (1-W GT).\n\n(d) Eval. AUC (ROC) vs. Round (1-W GT).\n\n(e) Eval. Loss vs. Round (2-W GT).\n\n(f) Eval. AUC (ROC) vs. Round (2-W GT).\n\nFigure 9: Smile classifier training with different η and K settings (for each mixed FL algorithm).\n\nany increase in K means that central variance σ2 above), necessitating even larger central batch sizes (which at some point is infeasible).\n\nc must be proportionally reduced (as mentioned\n\nWe empirically observed this relationship by running smile classification (Figure 9) and language modeling (Figure 10) experiments where client learning rate η (and central learning rate ηc) are inversely proportionally varied with K. For each hyperparameter configuration we ran 5 trials; the figures include 95% confidence intervals. The results confirm that reducing these learning rates, and making a corresponding increase in the number of steps, is beneficial. It never hurts convergence, and often helps.\n\nB.4.3 DIFFERENCES IN EFFECTIVE STEP SIZE\n\nTable 12 in Appendix C shows that in order to yield the convergence bounds stated in this paper, each algorithm makes different assumptions of maximum effective step size. From this we draw one final implication in regards to comparing the mixed FL algorithms.\n\nFor given η, maximum K varies by algorithm, or, for given K, maximum η varies by algorithm Consider just effective federated step size ̃η = ηηsK for the moment. Assume that server learning rate ηs is held constant. Then each mixed FL algorithm has a different theoretical upper bound on the product of client learning rate η and local steps per round K. If using a common η, the theoretical upper limit on K varies by mixed FL algorithm. Alternatively if using a common K, the theoretical upper limit on η varies by mixed FL algorithm.\n\nThe maximum effective step sizes of Table 12 imply that 2-WAY GRADIENT TRANSFER has narrower limits than 1-WAY GRADIENT TRANSFER on the allowable ranges of η and K. It also indicates that for PARALLEL TRAINING the allowable range of η, ηc, and K depends on the B parameter from mixed FL (G, B)-BGD (Definition 5.2).\n\nSome of this behavior has been observed empirically, when hyperparameter tuning our experiments (discussed in Subsection B.2). For example, for the language modeling experiment, assuming a\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Eval. Loss vs. Round (PT).\n\n(b) Eval. Accuracy vs. Round (PT).\n\n(c) Eval. Loss vs. Round (1-W GT).\n\n(d) Eval. Accuracy vs. Round (1-W GT).\n\n(e) Eval. Loss vs. Round (2-W GT).\n\n(f) Eval. Accuracy vs. Round (2-W GT).\n\nFigure 10: Language model training with different η and K settings (for each mixed FL algorithm).\n\n(a) Evaluation Loss vs. Round.\n\n(b) Evaluation Accuracy vs. Round.\n\nFigure 11: Language model training, with higher η for PT and 1-W GT (K = 16 for all). The increased learning rate boosts progress on evaluation loss and accuracy early in optimization, but does not change the number of rounds ultimately required for convergence. All three algorithms have reached similar loss and accuracy by round 3000, and are still converging.\n\nconstant number of steps of K = 16, 2-WAY GRADIENT TRANSFER tends to diverge when learning rate η was increased beyond 1.0, whereas 1-WAY GRADIENT TRANSFER is observed to converge even with learning rate η of 5.0. (PARALLEL TRAINING is in-between; it still converges with learning rate η of 3.0, but diverges when learning rate η is 5.0.) An interesting characteristic to note is that using different η in different algorithms does not really impact comparative convergence. Figure 11 shows convergence in the language modeling experiment, when 2-WAY GRADIENT TRANSFER uses η = 1.0 and 1-WAY GRADIENT TRANSFER and PARALLEL TRAINING both use η = 3.0 (in all cases, with K = 16). The higher learning rate of 1-W GT and PT helps a little early, but does not impact the number of rounds to convergence. This holds with the theoretical convergence bounds of Table 3, which show a relationship with steps K but not learning rates (as also discussed above).\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 12: Next movie prediction performance when training with adaptive optimizer (client optimizer: SGD, server optimizer: ADAM).\n\nB.4.4\n\n1-W GT WITH ADAPTIVE OPTIMIZATION\n\nWe briefly studied the performance of 1-WAY GRADIENT TRANSFER when using ADAM in place of SGD as the server optimizer, i.e., FEDADAM (Reddi et al., 2020). Note that the server adaptive optimizer requires a smaller learning rate to perform well. Figure 12 reports the results of using ADAM as the server optimizer with a server learning rate of 0.01. All the other hyperparameters are the same as in Tables 9 and 10. We observe that (1) ADAM works better than SGD, leading to better convergence, and (2) 1-WAY GRADIENT TRANSFER performs almost the same as the baseline when using ADAM. We will extend our investigation of mixed FL with adaptive optimization in the future. This will include studying methods for applying adaptive optimization to PARALLEL TRAINING and 2-WAY GRADIENT TRANSFER; these algorithms are more complicated since they involve additional optimizers (CENTRALOPTIMIZER and MERGEOPTIMIZER).\n\nB.4.5 COMPARISON WITH TRANSFER LEARNING\n\nSection 3 discussed how transfer learning is ill-suited for mixed FL. Here we show why.\n\nRecall the bias mitigation mixing task of Section 4.2. The end goal is a single trained language model that performs well both for high-end and low-end mobile phone users. We use Stack Overflow and Wikipedia datasets as respective experimental stand-ins, as they model the problem well. They are in the same domain (Latin alphabet) and same language (English) but with distinct usage distributions.\n\nTransfer learning sequentially trains the language model, first ‘pretraining’ on one dataset and then switching to ‘fine-tune’ on the other. One could first centrally pretrain (with Wikipedia), then switch to federated fine-tuning (with Stack Overflow), or do the converse. Figure 13 shows what happens. The switches take place at 250 rounds. We also show PARALLEL TRAINING as a point of comparison.\n\nFigure 13 shows that in transfer learning, catastrophic forgetting of the pretraining task occurs after switching to the fine-tuning task. The centralized pretraining case precipitously drops its evaluation accuracy on datacenter data (Wikipedia). The federated pretraining case does the same with federated data (Stack Overflow). Figure 13a shows that PARALLEL TRAINING achieves superior evaluation accuracy on the stand-in for the desired inference distribution (i.e. the combined Stack Overflow and Wikipedia datasets).\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Eval. Accuracy on All Data vs. Round.\n\n(b) Eval. Accuracy on Stack Overflow vs. Round.\n\n(c) Eval. Accuracy on Wikipedia vs. Round.\n\nFigure 13: Mixed FL endeavors to learn both data distributions, but transfer learning fails to retain knowledge of the pretraining task. When the pretraining-to-fine-tuning switch happens at round 250, the transfer learning cases fit to the new distribution and ‘forget’ the previous distribution they were pretrained on. PARALLEL TRAINING does not suffer from this problem as it sees data from both distributions throughout training.\n\nTo simplify Figure 13 we don’t show GRADIENT TRANSFER results, but they would match PARALLEL TRAINING (e.g., as shown in Figures 1b and 2b). Because the PARALLEL TRAINING and GRADIENT TRANSFER algorithms presented in this paper jointly train on both data distributions in parallel (as opposed to sequentially), they avoid catastrophic forgetting.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nTable 12: Maximum effective federated step size, ̃η = ηηsK, for convergence bounds in Appendix C and Table 3. When applicable (PT, 2-W GT) the effective centralized step size, ηcK, shares the same maximum (and assume that merging learning rate ηm is 1). β is the smoothness bound (Def. D.1).\n\nPT\n\n1-W GT\n\nμ-CONVEX\n\nCONVEX\n\nNONCONVEX\n\n1 6(1+B2)β 1\n6(1+B2)β 1\n6(1+B2)β\n\n1 8β 1\n8β 1\n18β\n\n2-W GT (cid:16) 1\n\n81β , 1\n\n15μ\n\nmin\n\n(cid:17)\n\n1 81β 1\n24β\n\nTable 13: Assumptions on merging or server learning rates, for convergence bounds in Appendix C and Table 3.\n\n(ASSUMES)\n\nηm ≥ 1\n\nηs ≥\n\nC CONVERGENCE THEOREMS\n\nPT\n\n1-W GT\n\n2-W GT\n\n√\n\nS\n\nηm ≥ 1\n\nThe three subsections that follow state theorems for convergence (to an error smaller than (cid:15)) for the respective mixed FL algorithms. The convergence bounds are summarized in Table 3 in Section 5. Tables 12 and 13 convey some supporting aspects of the convergence bounds, about limits on effective step size ( ̃η = ηηsK) and assumptions on learning rates.\n\nC.1 PARALLEL TRAINING\n\nGiven Assumption 5.1, one can view PARALLEL TRAINING as a ‘meta-FEDAVG’ involving two ‘meta-clients’. One meta-client is the population of IID federated clients (collectively having loss ff), and the other meta-client is the centralized data at the datacenter (having loss fc). As such, we can take the convergence theorem for FEDAVG derived in Karimireddy et al. (2020b) (Section 3, Theorem I) and observe that it applies to the number of rounds T to reach convergence in the PARALLEL TRAINING scenario.\n\nTheorem C.1. For PARALLEL TRAINING, where the federated data is IID (Assumption 5.1), for β-smooth functions ff and fc which satisfy Definition 5.2, the number of rounds T to reach an expected error smaller than (cid:15) is:\n\nμ-Strongly convex:\n\nT = ̃O\n\nGeneral convex:\n\nT = O\n\nNon-convex:\n\nT = O\n\n(cid:18)\n\n(cid:18)\n\n(cid:16) (σ2+Sσ2 c )\n\nKSμ(cid:15) + G\n\nμ\n\n√ √\n\nβ\n\n(cid:15) + B2β\n\n(σ2+Sσ2 KS(cid:15)2\n\nc )D2\n\n(σ2+Sσ2 KS(cid:15)2\n\nc )βF\n\n√\n\nβ\n\n+ G\n\n+ G\n\nβ\n\n3 (cid:15) 2\n√\n\n3 2\n\n(cid:15)\n\n(cid:17)\n\nμ log( 1 (cid:15) ) (cid:19) + B2βD2 (cid:19)\n\n(cid:15)\n\n+ B2βF\n\n(cid:15)\n\nwhere F = f (x(0)) − f (x∗), D2 =\n\n(cid:13) (cid:13)\n\n(cid:13)x(0) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2\n\n. Conditions for above: ηm ≥ 1; ηc, ηηs ≤\n\n1 6(1+B2)βKηm\n\n.\n\nProof. The analysis is exactly along the lines of the analysis in Karimireddy et al. (2020b), Appendix D.2, in the context of FEDAVG. Effectively, the analysis applies to the ‘meta-FEDAVG’ problem of PARALLEL TRAINING, with two ‘meta-clients’, one being the central loss/data (with stochastic gradients with variance of σ2 c ) and the other being the federated loss/data. The homogeneity of the clients and the averaging over the sampled clients effectively reduces the variance of the stochastic gradients to σ2/S. The analysis follows in a straightforward manner by accounting for the variance in appropriate places. We omit the details for brevity.\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nC.2\n\n1-WAY GRADIENT TRANSFER\n\nWe now provide convergence bounds for the 1-WAY GRADIENT TRANSFER scenario. Unlike PARALLEL TRAINING, which could be thought of as a ‘meta’ version of an existing FL algorithm (FEDAVG), 1-WAY GRADIENT TRANSFER is an entirely new FL algorithm. As such, we must formulate a novel proof (Appendix D) of its convergence bounds.\n\nGiven Assumption 5.1, the following Theorem gives the number of rounds to reach a given expected error.\n\nTheorem C.2. For 1-WAY GRADIENT TRANSFER, where the federated data is IID (Assumption 5.1), for β-smooth functions fi and fc, the number of rounds T to reach an expected error smaller than (cid:15) is:\n\nμ-Strongly convex:\n\nT = ̃O\n\nGeneral convex:\n\nT = O\n\nNon-convex:\n\nT = O\n\n(cid:16) (σ2+KSσ2 c )\n\nKSμ(cid:15) + β\n\n(cid:16) (σ2+KSσ2 KS(cid:15)2 (cid:16) (σ2+KSσ2 KS(cid:15)2\n\nc )D2\n\nc )βF\n\nμ log( 1 (cid:15) ) (cid:17) + βD2 (cid:17)\n\n(cid:15)\n\n+ βF\n\n(cid:15)\n\n(cid:17)\n\nwhen ηs >\n\nwhen ηs >\n\nwhen ηs ≥\n\n(cid:113) 5\n\n8 S, η ≤ 1 8 S, η ≤ 1 S, η ≤ 1\n\n(cid:113) 5 √\n\n8βKηs\n\n8βKηs\n\n18βKηs\n\nwhere F = f (x(0)) − f (x∗), D2 =\n\n(cid:13) (cid:13)\n\n(cid:13)x(0) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2\n\n.\n\nProof. Detailed proof given in Appendix D.\n\nC.3\n\n2-WAY GRADIENT TRANSFER\n\nGiven Assumption 5.1, one can view 2-WAY GRADIENT TRANSFER as a ‘meta-SCAFFOLD’ involving two ‘meta-clients’ (analogous to the view of PARALLEL TRAINING as ‘meta-FEDAVG’ in Subsection C.1). As such, we can take the convergence theorem for SCAFFOLD derived in Karimireddy et al. (2020b) (Section 5, Theorem III) and observe that it applies to the number of rounds T to reach convergence in the 2-WAY GRADIENT TRANSFER scenario.\n\nTheorem C.3. For 2-WAY GRADIENT TRANSFER, where the federated data is IID (Assumption 5.1), for β-smooth functions ff and fc, the number of rounds T to reach an expected error smaller than (cid:15) is:\n\nμ-Strongly convex:\n\nT = ̃O\n\nGeneral convex:\n\nT = O\n\nNon-convex:\n\nT = O\n\n(cid:16) (σ2+Sσ2 c )\n\nKSμ(cid:15) + β\n\n(cid:16) (σ2+Sσ2 KS(cid:15)2 (cid:16) (σ2+Sσ2 KS(cid:15)2\n\nc )D2\n\nc )βF\n\nμ log( 1 (cid:15) ) (cid:17) + βD2 (cid:17)\n\n(cid:15)\n\n+ βF\n\n(cid:15)\n\n(cid:17)\n\nwhen ηm ≥ 1; ηc, ηηs ≤ min\n\n(cid:16)\n\n1 81βKηm\n\n,\n\n1 15μKηm\n\n(cid:17)\n\nwhen ηm ≥ 1; ηc, ηηs ≤\n\nwhen ηm ≥ 1; ηc, ηηs ≤\n\n1 81βKηm\n\n1 24βKηm\n\nwhere F = f (x(0)) − f (x∗), D2 =\n\n(cid:13) (cid:13)\n\n(cid:13)x(0) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2\n\n.\n\nProof. The analysis is exactly along the lines of the analysis in Karimireddy et al. (2020b), Appendix E, in the context of SCAFFOLD. Effectively, the analysis applies to the ‘meta-SCAFFOLD’ problem of 2-WAY GRADIENT TRANSFER, with two ‘meta-clients’, one being the central loss/data (with stochastic gradients with variance of σ2 c ) and the other being the federated loss/data. The homogeneity of the clients and the averaging over the sampled clients effectively reduces the variance of the stochastic gradients to σ2/S. The analysis follows in a straightforward manner by accounting for the variance in appropriate places. We omit the details for brevity.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nD CONVERGENCE PROOFS FOR 1-WAY GRADIENT TRANSFER\n\nWe will prove the convergence rate of 1-WAY GRADIENT TRANSFER for 3 different cases: strongly convex, general convex, and non-convex. While these proofs are influenced by those for SCAFFOLD in Karimireddy et al. (2020b), we note some additional technical challenges we face with 1-WAY GRADIENT TRANSFER. First, as the name indicates, gradient information only flows one way (in SCAFFOLD, gradient information flows from and to all clients). Second, we only sample a batch of centralized data once per round (whereas SCAFFOLD can draw multiple batches during a round). The result is that (as described in Subsection 5.2), 1-WAY GRADIENT TRANSFER works out to be more sensitive to central variance σ2\n\nc and number of steps K than 2-WAY GRADIENT TRANSFER.\n\nWe will first state a number of definitions and lemmas in Subsection D.1 that are needed in proving convergence rate of 1-WAY GRADIENT TRANSFER, before proceeding to the actual proofs in Subsection D.2.\n\nD.1 ADDITIONAL DEFINITIONS AND LEMMAS\n\nNote that some of the lemmas below are restatements of lemmas given in Karimireddy et al. (2020b). We opt to restate here (versus referencing the relevant lemma in Karimireddy et al. (2020b) each time) due to the volume of usage of the lemmas, to ease the burden on the reader.\n\nWe will first present the subset of definitions and lemmas which don’t make any assumptions of convexity (Subsection D.1.1), followed by the subset that assume convexity (Subsection D.1.2)\n\nD.1.1 GENERAL DEFINITIONS AND LEMMAS\n\nDefinition D.1 (β-Smoothness). A function h is β-smooth if it satisfies:\n\n(cid:107)∇h(x) − ∇h(y)(cid:107) ≤ β (cid:107)x − y(cid:107) , for any x, y\n\nThis implies the following quadratic upper bound on h:\n\n(cid:104)∇h(x), y − x(cid:105) ≥ −\n\nh(x) − h(y) +\n\n(cid:18)\n\n(cid:19)\n\n(cid:107)x − y(cid:107)2\n\nβ 2\n\n, for any x, y\n\nLemma D.2 (Relaxed triangle inequality). Let {v1, . . . , vτ } be τ vectors in Rd. Then for any a > 0:\n\n(cid:107)vi + vj(cid:107)2 ≤ (1 + a) (cid:107)vi(cid:107)2 +\n\n(cid:18)\n\n1 +\n\n(cid:19)\n\n1 a\n\n(cid:107)vj(cid:107)2\n\nAlso:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nvi\n\n≤ τ\n\nτ (cid:88)\n\ni=1\n\n(cid:107)vi(cid:107)2\n\nProof. The first statement for any a > 0 follows from the identity: (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:107)vi + vj(cid:107)2 = (1 + a) (cid:107)vi(cid:107)2 +\n\n(cid:107)vj(cid:107)2 −\n\n1 +\n\n1 a\n\n(cid:18)\n\n(cid:19)\n\n√\n\navi +\n\n1 √\n\na\n\nvj\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13)\n\nThe second statement follows from the convexity of v → (cid:107)v(cid:107)2 and Jensen’s inequality:\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 τ\n\nτ (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nvi\n\n≤\n\n1 τ\n\nτ (cid:88)\n\ni=1\n\n(cid:107)vi(cid:107)2\n\nLemma D.3 (Separating mean and variance). Let {Ξ1, . . . , Ξτ } be τ random variables in Rd which are not necessarily independent. First suppose that their mean is E[Ξi] = ξi and variance is bounded as E[(cid:107)Ξi − ξi(cid:107)2] ≤ σ2. Then:\n\nE\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nΞi\n\n ≤\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nξi\n\n+ τ 2σ2\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nNow instead suppose that their conditional mean is E[Ξi|Ξi−1, . . . , Ξ1] = ξi, i.e. the variables {Ξi − ξi} form a martingale difference sequence, and the variance is bounded same as above. Then we can show the tighter bound: \n\n\n\nE\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nΞi\n\n2  ≤ 2E\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nξi\n\n2  + 2τ σ2\n\nProof. For any random variable X, E[X 2] = (E[X − E[X]])2 + (E[X])2 implying:\n\nE\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nΞi\n\n =\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nξi\n\n+ E\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(Ξi − ξi)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nExpanding the last term of the above expression using relaxed triangle inequality (Lemma D.2) proves the first claim:\n\nE\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(Ξi − ξi)\n\n ≤ τ\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nE\n\n(cid:104)\n\n(cid:107)Ξi − ξi(cid:107)2(cid:105)\n\n≤ τ 2σ2\n\nτ (cid:88)\n\ni=1\n\nFor the second statement, ξi is not deterministic and depends on Ξi−1, . . . , Ξ1. Hence we have to resort to the cruder relaxed triangle inequality to claim:\n\nE\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n2  ≤ 2E\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nΞi\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n2  + 2E\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nξi\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(Ξi − ξi)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nThen we use the tighter expansion of the second term:\n\nE\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nτ (cid:88)\n\ni=1\n\n(Ξi − ξi)\n\n =\n\n(cid:88)\n\ni,j\n\nE [(cid:104)Ξi − ξi, Ξj − ξj(cid:105)] =\n\n(cid:88)\n\nE\n\n(cid:104)\n\n(cid:107)Ξi − ξi(cid:107)2(cid:105)\n\n≤ τ σ2\n\ni\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nThe cross terms in the above expression have zero mean since {Ξi − ξi} form a martingale difference sequence.\n\nD.1.2 DEFINITIONS AND LEMMAS ASSUMING CONVEXITY\n\nDefinition D.4 (μ-Convexity). A function h is μ-convex for μ ≥ 0 if it satisfies:\n\n(cid:104)∇h(x), y − x(cid:105) ≤ −\n\n(cid:16)\n\nh(x) − h(y) +\n\n(cid:107)x − y(cid:107)2(cid:17)\n\nμ 2\n\n, for any x, y\n\nWhen μ > 0, we have strong convexity, a quadratic lower bound on h. Proposition D.5 (Convexity and smoothness). If client losses fi and centralized loss fc are each β-smooth (Definition D.1), and x∗ is an optimum of the overall loss f (as defined in Equation 1), then the following holds true:\n\n1 2β\n\n(cid:32)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:107)∇fi(x) − ∇fi(x∗)(cid:107)2 + (cid:107)∇fc(x) − ∇fc(x∗)(cid:107)2\n\n≤ f (x) − f (x∗)\n\n(cid:33)\n\nProof. Define the functions ̃fi(x) := fi(x) − (cid:104)∇fi(x∗), x(cid:105), for all clients i, and the function ̃fc(x) := fc(x) − (cid:104)∇fc(x∗), x(cid:105). Since fi and fc are convex and β-smooth, so are ̃fi and ̃fc, and furthermore their gradients vanish at x∗; hence, x∗ is a common minimizer for ̃fi, ̃fc and f . Using the β-smoothness of ̃fi and fc, we have\n\n1 2β\n\n(cid:107)∇ ̃fi(x)(cid:107)2 ≤ ̃fi(x) − ̃fi(x)\n\nand\n\n1 2β\n\n(cid:107)∇ ̃fc(x)(cid:107)2 ≤ ̃fc(x) − ̃fc(x).\n\nNote that 1 bound then follows from the above two facts.\n\n ̃fi + ̃fc = f since 1\n\ni=1\n\nN\n\nN\n\n(cid:80)N\n\n(cid:80)N\n\ni=1 ∇fi(x∗) + ∇fc(x∗) = ∇f (x∗) = 0. The claimed\n\n26\n\nUnder review as a conference paper at ICLR 2023\n\nProposition D.6 (Convex bound on gradient of overall loss). If client losses fi and centralized loss fc are each μ-convex (Definition D.4) and β-smooth (Definition D.1), and x∗ is an optimum of the overall loss f (as defined in Equation 1), then the expected norm of the gradient of overall loss is bounded as:\n\nE (cid:107)∇f (x)(cid:107)2 ≤ 4βE [f (x) − f (x∗)]\n\nProof.\n\nE (cid:107)∇f (x)(cid:107)2 = E (cid:107)∇f (x) − ∇f (x∗)(cid:107)2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nN (cid:88)\n\n= E\n\n1 N\n\ni=1\n\n(∇fi(x) − ∇fi(x∗)) + (∇fc(x) − ∇fc(x∗))\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\nApplying the relaxed triangle inequality (Lemma D.2) twice:\n\nE (cid:107)∇f (x)(cid:107)2 ≤ 2E\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(∇fi(x) − ∇fi(x∗))\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+ 2E (cid:107)∇fc(x) − ∇fc(x∗)(cid:107)2\n\n≤ 2E 1\n\nN\n\nN (cid:88)\n\ni=1\n\n(cid:107)(∇fi(x) − ∇fi(x∗))(cid:107)2 + 2E (cid:107)∇fc(x) − ∇fc(x∗)(cid:107)2\n\n≤ 2E\n\n(cid:34)\n\n1 N\n\nN (cid:88)\n\ni=1\n\n(cid:107)(∇fi(x) − ∇fi(x∗))(cid:107)2 + (cid:107)∇fc(x) − ∇fc(x∗)(cid:107)2\n\n(cid:35)\n\nApplying Proposition D.5:\n\nE (cid:107)∇f (x)(cid:107)2 ≤ 2E [2β (f (x) − f (x∗))]\n\n≤ 4βE [f (x) − f (x∗)]\n\nLemma D.7 (Perturbed strong convexity). The following holds for any β-smooth and μ-stronglyconvex function h, and any x, y, z in the domain of h:\n\n(cid:104)∇h(x), z − y(cid:105) ≥ h(z) − h(y) +\n\nμ 4\n\n(cid:107)y − z(cid:107)2 − β (cid:107)z − x(cid:107)2\n\nProof. Given any x, y, and z, we get the following two inequalities using smoothness (Definition D.1) and strong convexity (Definition D.4) of h:\n\n(cid:104)∇h(x), z − x(cid:105) ≥h(z) − h(x) −\n\n≥h(x) − h(y) +\n\nβ 2\nμ 2\n\n(cid:107)z − x(cid:107)2\n\n(cid:107)y − x(cid:107)2\n\nFurther, applying the relaxed triangle inequality (Lemma D.2) gives:\n\nμ 2\n\n(cid:107)y − x(cid:107)2 ≥\n\nμ 4\n\n(cid:107)y − z(cid:107)2 −\n\nμ 2\n\n(cid:107)x − z(cid:107)2\n\nCombining all the inequalities together we have:\n\n(cid:104)∇h(x), z − y(cid:105) ≥ h(z) − h(y) +\n\nμ 4\n\n(cid:107)y − z(cid:107)2 −\n\nβ + μ 2\n\n(cid:107)z − x(cid:107)2\n\nThe lemma follows since β ≥ μ.\n\nLemma D.8 (Contractive mapping). For any β-smooth and μ-strongly convex function h, values x and y in the domain of h, and step-size (learning rate) η ≤ 1\n\nβ , the following holds true:\n\n(cid:107)x − η∇h(x) − y + η∇h(y)(cid:107)2 ≤ (1 − μη) (cid:107)x − y(cid:107)2\n\n27\n\nUnder review as a conference paper at ICLR 2023\n\nProof. Expanding terms, and applying smoothness (Definition D.1):\n\n(cid:107)x − η∇h(x) − y + η∇h(y)(cid:107)2 = (cid:107)x − y(cid:107)2 + η2 (cid:107)∇h(x) − ∇h(y)(cid:107)2\n\n− 2η(cid:104)∇h(x) − ∇h(y), x − y(cid:105)\n\n≤ (cid:107)x − y(cid:107)2 + (cid:0)η2β − 2η(cid:1) (cid:104)∇h(x) − ∇h(y), x − y(cid:105)\n\nIf step-size is such that η ≤ 1\n\nβ , then:\n\n(cid:0)η2β − 2η(cid:1) (cid:104)∇h(x) − ∇h(y), x − y(cid:105) ≤ −η(cid:104)∇h(x) − ∇h(y), x − y(cid:105)\n\nFinally, for μ-strong convexity (Definition D.4) of h we have:\n\n−η(cid:104)∇h(x) − ∇h(y), x − y(cid:105) ≤ −μη (cid:107)x − y(cid:107)2\n\nD.2 PROOFS OF THEOREM C.2\n\nWe will now prove the rates of convergence stated in Theorem C.2 for 1-WAY GRADIENT TRANSFER. Subsection D.2.1 proves the convergence rates for strongly convex and general convex cases, and Subsection D.2.2 proves the convergence rates for the non-convex case.\n\nLet S be the cardinality of the cohort of clients S participating in a round of training. Let the server and client optimizers be SGD. Let the clients all take an equal number of steps K, and let ̃η be the ‘effective step-size’, equal to Kηsη. With 1-WAY GRADIENT TRANSFER, the server update of the global model at round t can be written as:\n\nx(t+1) − x(t) = −\n\n ̃η KS\n\n(cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni∈S\n\nk=1\n\ngi(x(t,k)\n\ni\n\n) + gc(x(t))\n\n(cid:17)\n\nx(t+1) − x(t) = − ̃ηgc(x(t)) −\n\n ̃η KS\n\n(cid:88)\n\nK (cid:88)\n\n(cid:16)\n\n(cid:17)\n\ngi(x(t,k)\n\ni\n\n)\n\ni∈S\n\nk=1\n\n(11)\n\nHenceforth, let E|t[·] denote expectation conditioned on x(t). As in Karimireddy et al. (2020b), we’ll define a client local ‘drift’ term in round t as:\n\nE (t) =\n\n1 KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n(12)\n\nLemma D.9 (Bound on variance of server update). The variance of the server update is bounded as:\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nE|t\n\n≤ 4 ̃η2β2E (t) + 2 ̃η2\n\n(cid:18) 2 KS\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n+ 2 ̃η2E|t\n\n(cid:20)(cid:13) (cid:13) (cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n2(cid:21)\n\nProof. Let S denote the set of clients sampled in round t. For brevity, we will use ∆x to refer to x(t+1) − x(t).\n\nE|t (cid:107)∆x(cid:107)2 = E|t\n\n= E|t\n\n≤ E|t\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\n\n\n\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n ̃η KS\n\n ̃η KS\n\n(cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni∈S\n\nk=1\n\n(cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni∈S\n\nk=1\n\n2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n\n\n+\n\n(cid:16)\n\ngi(x(t,k)\n\ni\n\n) + gc(x(t))\n\n(cid:17)\n\ngi(x(t,k)\n\ni\n\n) − ∇ff(x(t))\n\n(cid:17)\n\n28\n\n∇ff(x(t)) + gc(x(t))\n\n(cid:17)\n\n2\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\nUnder review as a conference paper at ICLR 2023\n\nWe separate terms by applying the relaxed triangle inequality (Lemma D.2):\n\nE|t (cid:107)∆x(cid:107)2 ≤ 2 ̃η2E|t\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 KS\n\n(cid:88)\n\nK (cid:88)\n\ni∈S\n\nk=1\n\n(cid:16)\n\ngi(x(t,k)\n\ni\n\n) − ∇ff(x(t))\n\n(cid:17)\n\n(cid:124)\n\n(cid:123)(cid:122) A\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2\n\n+ 2 ̃η2E|t\n\n\n\n(cid:124)\n\n(cid:125)\n\n(cid:20)(cid:13) (cid:13)∇ff(x(t)) + gc(x(t)) (cid:13)\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n(cid:123)(cid:122) B\n\n(cid:125)\n\nIn term A, we separate mean and variance for the client stochastic gradients gi, using Lemma D.3 and Equation 4:\n\nA ≤ 4 ̃η2E|t\n\n\n\n\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 KS\n\n(cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni∈S\n\nk=1\n\n∇ff(x(t,k)\n\ni\n\n) − ∇ff(x(t))\n\n(cid:17)\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n2\n\n +\n\n4 ̃η2σ2 KS\n\nWe apply the relaxed triangle inequality (Lemma D.2) followed by smoothness (Definition D.1), to convert it to an expression in terms of drift E (t):\n\nA ≤\n\n4 ̃η2 KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n(cid:20)(cid:13) (cid:13)∇ff(x(t,k) (cid:13)\n\ni\n\nE|t\n\n) − ∇ff(x(t))\n\n2(cid:21)\n\n(cid:13) (cid:13) (cid:13)\n\n+\n\n4 ̃η2σ2 KS\n\n≤\n\n4 ̃η2β2 KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n(cid:20)(cid:13) (cid:13)x(t,k) (cid:13)\n\ni\n\nE|t\n\n≤ 4 ̃η2β2E (t) +\n\n4 ̃η2σ2 KS\n\n2(cid:21)\n\n− x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n+\n\n4 ̃η2σ2 KS\n\nIn term B we have a full gradient of the federated loss ∇ff and a stochastic gradient of the centralized loss gc. We use Lemma D.3 to separate the stochastic gradient into a full gradient of the centralized loss ∇fc and a variance term, allowing us to express in terms of full gradient of the overall loss ∇f .\n\nB = 2 ̃η2E|t\n\n≤ 2 ̃η2E|t\n\n≤ 2 ̃η2E|t\n\n2(cid:21)\n\n(cid:20)(cid:13) (cid:13) (cid:13)∇ff(x(t)) + gc(x(t)) (cid:13) (cid:13) (cid:13) (cid:20)(cid:13) (cid:13) (cid:13)∇ff(x(t)) + ∇fc(x(t)) (cid:13) (cid:13) (cid:13) (cid:20)(cid:13) 2(cid:21) (cid:13) (cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+ 2 ̃η2σ2\n\nc\n\n2(cid:21)\n\n+ 2 ̃η2σ2\n\nc\n\nCombining A and B back together:\n\nE|t (cid:107)∆x(cid:107)2 ≤ 4 ̃η2β2E (t) + 2 ̃η2\n\n(cid:18) 2 KS\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n+ 2 ̃η2E|t\n\n(cid:20)(cid:13) (cid:13) (cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n2(cid:21)\n\nD.2.1 CONVEX CASES\n\nWe will state two lemmas, one (Lemma D.10) related to the progress in round t towards reaching x∗, and the other (Lemma D.11) bounding the federated clients ‘drift’ in round t, E (t). We then combine the two lemmas together to give the proofs of convergence rate for the strongly convex (μ > 0) and general convex (μ = 0) cases.\n\n29\n\nUnder review as a conference paper at ICLR 2023\n\nLemma D.10 (One round progress). Suppose our functions satisfy bounded variance σ2, μ-convexity (Definition D.4), and β-smoothness (Definition D.1). If ̃η < 1 8β , the updates of 1-WAY GRADIENT TRANSFER satisfy:\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nE|t\n\n(cid:18)\n\n≤\n\n1 −\n\n3μ ̃η 2\n\n+ 2 ̃η2\n\n(cid:16)\n\n− ̃η\n\nf (x(t)) − f (x∗)\n\n(cid:17)\n\n+\n\n5 16\n\nE (t)\n\n(cid:19) (cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13) (cid:18) 2 KS\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\nProof. The expected server update, with N total clients in the federated population, is:\n\n(cid:104)\n\nx(t+1) − x(t)(cid:105)\n\nE\n\n= − ̃ηE\n\n(cid:104) gc(x(t))\n\n(cid:105)\n\n−\n\n ̃η KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n(cid:104)\n\nE\n\ngi(x(t,k)\n\ni\n\n(cid:105) )\n\nThe distance from optimal x∗ in parameter space at round t is (cid:13) from optimal at round t + 1, conditioned on x(t) and earlier rounds, is:\n\n(cid:13)x(t) − x∗(cid:13) 2\n(cid:13)\n\n. The expected distance\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nE|t\n\n= E|t\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t) + x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\n=\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:68)\n\nE|t\n\n+ 2\n\n(cid:104)\n\nx(t+1) − x(t)(cid:105)\n\n, x(t) − x∗(cid:69)\n\n+ E|t\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nFor clarity, we now focus on individual terms, beginning with C:\n\n(cid:124)\n\n(cid:123)(cid:122) C\n\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) D\n\n(cid:125)\n\n(cid:104)\n\nx(t+1) − x(t)(cid:105)\n\n, x(t) − x∗(cid:69)\n\nC = 2\n\n(cid:68)\n\nE|t (cid:42)(cid:32)\n\n= 2\n\n− ̃ηE\n\n(cid:104) gc(x(t))\n\n(cid:105)\n\n−\n\n ̃η KN\n\nN (cid:88)\n\nK (cid:88)\n\n(cid:104)\n\nE\n\ni=1\n\nk=1\n\n(cid:33)\n\n(cid:105) )\n\ngi(x(t,k)\n\ni\n\n(cid:43)\n\n, x(t) − x∗\n\n= 2 ̃η\n\n(cid:68)\n\n∇fc(x(t)), x∗ − x(t)(cid:69)\n\n+\n\n(cid:124)\n\n(cid:123)(cid:122) C1\n\n(cid:125)\n\n(cid:42) N\n\n(cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n2 ̃η KN\n\n(cid:124)\n\n∇ff(x(t,k)\n\ni\n\n), x∗ − x(t)\n\n(cid:123)(cid:122) C2\n\n(cid:43)\n\n(cid:125)\n\nWe can use convexity (Definition D.4) to bound C1, with x = x(t), and y = x∗:\n\n(cid:18)\n\nC1 ≤ −2 ̃η\n\nfc(x(t)) − fc(x∗) +\n\nμ 2\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:19)\n\nWe apply perturbed convexity (Lemma D.7) to bound C2, with x = x(t,k)\n\ni\n\n, y = x∗, and z = x(t):\n\nC2 ≤\n\n2 ̃η KN\n\nN (cid:88)\n\nK (cid:88)\n\n(cid:18)\n\ni=1\n\nk=1\n\nff(x∗) − ff(x(t)) + β\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n−\n\nμ 4\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:18)\n\n(cid:18)\n\n≤ −2 ̃η\n\n≤ −2 ̃η\n\nff(x(t)) − ff(x∗) +\n\nff(x(t)) − ff(x∗) +\n\nμ 4\n\nμ 4\n\n+\n\n2β ̃η KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\n+ 2β ̃ηE (t)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:19)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:19)\n\n30\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\nUnder review as a conference paper at ICLR 2023\n\nCombining C1 and C2 back together:\n\n(cid:18)\n\nC ≤ −2 ̃η\n\nf (x(t)) − f (x∗) +\n\n3μ 4\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:19)\n\n+ 2β ̃ηE (t)\n\nNow we turn to term D, which is the variance of the server update (from Lemma D.9):\n\nD = E|t\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\n≤ 4 ̃η2β2E (t) + 2 ̃η2\n\n(cid:18) 2 KS\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n+ 2 ̃η2E|t\n\n(cid:20)(cid:13) (cid:13) (cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n2(cid:21)\n\nWe can leverage Proposition D.6 to replace the norm squared of the gradient of the overall loss:\n\nD = E|t\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\n≤ 4 ̃η2β2E (t) + 2 ̃η2\n\n(cid:18) 2 KS\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\n+ 8 ̃η2βE|t\n\n(cid:104)\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\nReturning to our equation for the expected distance from optimal x∗ in parameter space, and making use of the bounds we established for C and D:\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nE|t\n\n=\n\n≤\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:68)\n\nE|t\n\n+ 2\n\n(cid:104)\n\nx(t+1) − x(t)(cid:105)\n\n, x(t) − x∗(cid:69)\n\n+ E|t\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\n(cid:124)\n\n− 2 ̃η\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:123)(cid:122) C\n\n(cid:125)\n\n(cid:124)\n\n(cid:123)(cid:122) D\n\n(cid:125)\n\n(cid:18)\n\nf (x(t)) − f (x∗) +\n\n3μ 4\n\n(cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:19)\n\n+ 2β ̃ηE (t) + 4 ̃η2β2E (t) + 2 ̃η2\n\n(cid:18) 2 KS\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n(cid:104)\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\n+ 8 ̃η2βE|t (cid:19) (cid:13) (cid:13)\n\n3μ ̃η 2\n\n1 −\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ (cid:0)8 ̃η2β − 2 ̃η(cid:1) (cid:16)\n\n+ 2 ̃ηβ (1 + 2 ̃ηβ) E (t) + 2 ̃η2\n\n(cid:17)\n\nf (x(t)) − f (x∗) (cid:18) 2 KS\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n(cid:18)\n\n≤\n\nAssuming that ̃η ≤ 1\n\n8β :\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nE|t\n\n(cid:18)\n\n≤\n\n1 −\n\n3μ ̃η 2\n\n+ 2 ̃η2\n\n(cid:16)\n\n− ̃η\n\nf (x(t)) − f (x∗)\n\n(cid:17)\n\n+\n\n5 16\n\nE (t)\n\n(cid:19) (cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13) (cid:18) 2 KS\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\nLemma D.11 (Bounded drift). Suppose our functions satisfy bounded variance, μ-convexity (Definition D.4), and β-smoothness (Definition D.1). Then the drift is bounded as:\n\nE (t) ≤ 12K 2η2βE\n\n(cid:104)\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\n+ 3K 2η2\n\n(cid:18) 1 K\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n31\n\nUnder review as a conference paper at ICLR 2023\n\nProof. We begin with the summand of the drift term, looking at the drift of a particular client i at local step k. Expanding this summand out:\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n= E|t\n\n= E|t\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)x(t,k−1) (cid:13)x(t,k−1)\n\n(cid:13) (cid:13)\n\ni\n\n− η\n\n(cid:16)\n\ngi(x(t,k−1)\n\ni\n\n) + gc(x(t))\n\n(cid:17)\n\n− x(t)(cid:13)\n\n− x(t) − ηgi(x(t,k−1)\n\ni\n\n) − ηgc(x(t))\n\n2 (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n.\n\nSeparating mean and variance of the client gradient, then using the relaxed triangle inequality (Lemma D.2) to further separate out terms:\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤ E|t (cid:18)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t) − η∇ff(x(t,k−1)\n\ni\n\n) − ηgc(x(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ η2σ2\n\n≤\n\n1 +\n\n(cid:19)\n\n1 a\n\nE|t (cid:124)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t) − η\n\n(cid:16)\n\n∇ff(x(t,k−1) (cid:123)(cid:122) F\n\ni\n\n) − ∇ff(x(t))\n\n(cid:17)(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:125)\n\n+ (1 + a) η2 (cid:13)\n\n(cid:13) 2\n(cid:13)∇ff(x(t)) + gc(x(t)) (cid:13) (cid:13) (cid:13)\n\n+ η2σ2.\n\nTerm F is bounded via the contractive mapping lemma (Lemma D.8), provided that η ≤ 1 β :\n\nF ≤ (1 − μη)E|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤ E|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n.\n\nPutting back into the bound on drift on client i at local step k, and letting a = K:\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤ K+1\n\nK\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ 2Kη2 (cid:13)\n\n(cid:13) 2\n(cid:13)∇ff(x(t)) + gc(x(t)) (cid:13) (cid:13) (cid:13)\n\n+ η2σ2.\n\nUnrolling the recursion:\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤\n\n≤\n\n(cid:18)\n\n2Kη2 (cid:13)\n\n(cid:13)∇ff(x(t)) + gc(x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ η2σ2\n\n(cid:18)\n\n2Kη2 (cid:13)\n\n≤ 4K 2η2 (cid:13)\n\n(cid:13) 2\n(cid:13)∇ff(x(t)) + gc(x(t)) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13)∇ff(x(t)) + gc(x(t)) (cid:13)\n\n+ η2σ2\n\n+ 2Kη2σ2.\n\n(cid:19) k−1 (cid:88)\n\n(cid:0) K+1 K\n\n(cid:1)j\n\nj=0\n\n(cid:19)\n\n(2K)\n\nThe second inequality above uses the following bound:\n\nk−1 (cid:88)\n\nj=0\n\n(cid:0) K+1 K\n\n(cid:1)j\n\n((1 + 1\n\nK )k − 1) = K ≤ (e − 1)K ≤ 2K.\n\nNow separating mean and variance of the central gradient:\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤ 4K 2η2 (cid:13)\n\n(cid:13)∇ff(x(t)) + ∇fc(x(t)) (cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+ 2K 2η2\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:18) 1 K\n\n≤ 4K 2η2 (cid:13)\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\n.\n\n+ 2K 2η2σ2\n\nc + 2Kη2σ2\n\nFinally, we apply Proposition D.6:\n\n32\n\nUnder review as a conference paper at ICLR 2023\n\nE (t) ≤ 4K 2η2 (cid:13)\n\n(cid:13)∇f (x(t)) (cid:13) (cid:104)\n\n≤ 16K 2η2βE|t\n\nf (x(t)) − f (x∗)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ 2K 2η2\n\n(cid:18) 1 K\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n(cid:105)\n\n+ 2K 2η2\n\n(cid:18) 1 K\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\n.\n\nAssuming that ̃η ≤ 1\n\n8β :\n\nE (t) ≤ 2\n\n(cid:104)\n\nE|t\n\n ̃η η2 s\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\n+ 2\n\n ̃η2 η2 s\n\n(cid:18) 1 K\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\nProofs of Theorem C.2 for Convex Cases Adding the statements of Lemmas D.10 and D.11, and\n\nassuming that ηs >\n\n8 S, η = 1\n\n8βKηs\n\nso that ̃η = 1\n\n8β , we get:\n\n(cid:113) 5\n\n(cid:20)(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x∗(cid:13)\n\n(cid:13) (cid:13)\n\n2(cid:21)\n\nE|t\n\n(cid:18)\n\n≤\n\n1 −\n\n3μ ̃η 2\n\n+\n\n5 16\n\n(cid:19) (cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:16)\n\n− ̃η\n\nf (x(t)) − f (x∗)\n\n(cid:17)\n\n(cid:104)\n\nE|t\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\n+ 2\n\n ̃η2 η2 s\n\n(cid:18) 1 K\n\n(cid:19)(cid:19)\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n(cid:18)\n\n=\n\n1 −\n\n(cid:19) (cid:13) (cid:13)\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:16)\n\n− ̃η\n\nf (x(t)) − f (x∗)\n\n(cid:17)\n\n(cid:104)\n\nE|t\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\n+ 4\n\n(cid:19)\n\n ̃η2 KS\n\nσ2 +\n\n(cid:18) 5 8\n\n ̃η2 η2 s\n\n+ 2 ̃η2\n\n(cid:19)\n\nσ2\n\nc\n\n(cid:18)\n\n≤\n\n1 −\n\n(cid:13)x(t) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:18)\n\n2\n\n ̃η η2 s\n(cid:18) 2 KS\n\n+ 2 ̃η2\n\n3μ ̃η 2\n5 ̃η η2 8\ns (cid:18) 5 8\n\n+\n\n+\n\n ̃η2 Kη2 s\n(cid:19) (cid:13) (cid:13)\n\n3μ ̃η 2\n(cid:18) S − 1 S\n\n−\n\n(cid:19)\n\n(cid:104)\n\n ̃ηE|t\n\nf (x(t)) − f (x∗)\n\n(cid:105)\n\n+\n\n(cid:18) 5σ2 KS\n\n(cid:19)\n\n ̃η2.\n\n+ 3σ2\n\nc\n\nWe can now remove the conditioning over x(t) by taking an expectation on both sides over x(t), to get a recurrence relation of the same form.\n\nFor the case of strong convexity (μ > 0), we can use lemmas (e.g., Lemma 1 in Karimireddy et al. (2020b), Lemma 2 in Stich (2019)) which establish a linear convergence rate for such recursions. This results in the following bound16 for T ≥ 8β 3μ :\n\n(cid:104)\n\nE\n\n(cid:105)\n\nf ( ̄x(T ))\n\n− f (x∗) = ̃O\n\n(cid:18) σ2 + KSσ2 μKST\n\nc\n\n+ μ\n\n(cid:13) (cid:13)\n\n(cid:13)x(0) − x∗(cid:13)\n\n2 (cid:13) (cid:13)\n\nexp\n\n(cid:18) −3μT 16β\n\n(cid:19)(cid:19)\n\n,\n\nwhere ̄x(T ) is a weighted average of x(1), x(2), . . . , x(T +1) with geometrically decreasing weights (1 − 3μ ̃η\n\n2 )1−r for x(r), r = 1, 2, . . . , T + 1.\n\nThis yields an expression for the number of rounds T to reach an error (cid:15):\n\nT = ̃O\n\n(cid:18) σ2 + KSσ2 KSμ(cid:15)\n\nc\n\n+\n\nβ μ\n\nlog\n\n(cid:18) 1 (cid:15)\n\n(cid:19)(cid:19)\n\n16The ̃O notation hides dependence on logarithmic terms which can be removed by using varying step-sizes.\n\n33\n\nUnder review as a conference paper at ICLR 2023\n\nFor the case of general convexity (μ = 0), we can use lemmas (e.g., Lemma 2 in Karimireddy et al. (2020b), Lemma 4 in Stich (2019)) which establish a sublinear convergence rate for such recursions. In this case we get the following bound:\n\n(cid:104)\n\nE\n\n(cid:105)\n\nf ( ̄x(T ))\n\n− f (x∗) ≤\n\n(cid:18) S\n\n(cid:19) (cid:32)\n\nS − 1\n\n8β (cid:13)\n\n(cid:13)x(0) − x∗(cid:13) 2\n(cid:13) T + 1\n\n+\n\n(cid:112)20σ2 + 12KSσ2\n\n(cid:13)x(0) − x∗(cid:13) (cid:13) (cid:13)\n\nc\n\n(cid:112)KS (T + 1)\n\n(cid:33)\n\n,\n\nwhere ̄x(T ) = 1\n\nT +1\n\n(cid:80)T +1\n\nt=1 x(t).\n\nThis yields an expression for the number of rounds T to reach an error (cid:15):\n\nT = O\n\n(cid:32) (cid:0)σ2 + KSσ2 KS(cid:15)2\n\nc\n\n(cid:1) D2\n\n(cid:33)\n\n.\n\n+\n\nβD2 (cid:15)\n\nIn the above expression, D2 is a distance in parameter space at initialization, (cid:13)\n\n(cid:13)x(0) − x∗(cid:13) 2\n(cid:13)\n\n.\n\nD.2.2 NON-CONVEX CASE\n\nWe will now prove the rate of convergence stated in Theorem C.2 for the non-convex case for 1-WAY GRADIENT TRANSFER. We will state two lemmas, one (Lemma D.12) establishing the progress made in each round, and one (Lemma D.13) bounding how much the federated clients ‘drift’ in a round during the course of local training. We then combine the two lemmas together give the proof of convergence rate for the non-convex case.\n\nLemma D.12 (Non-convex one round progress). The progress made in a round can be bounded as:\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n(cid:105)\n\n≤ f (x(t)) −\n\n4 ̃η 9\n\n(cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+\n\nβ 27\n\nE (t) +\n\n(cid:18) 2 KS\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\nβ ̃η2\n\nProof. We begin by using the smoothness of f to get the following bound on the expectation of f (x(t+1)) conditioned on x(t):\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n(cid:105)\n\n≤ E|t\n\n(cid:20)\n\nf (x(t)) +\n\n(cid:68)\n\n∇f (x(t)), x(t+1) − x(t)(cid:69)\n\n+\n\nβ 2\n\n≤ f (x(t)) + E|t\n\n(cid:68)\n\n∇f (x(t)), x(t+1) − x(t)(cid:69)\n\n+\n\nβ 2\n\nE|t\n\n2(cid:21)\n\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13) (cid:13)x(t+1) − x(t)(cid:13)\n\n(cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n.\n\nSubstituting in the definition of the 1-WAY GRADIENT TRANSFER server update (Equation 11), and using Assumption 5.1 for the expectation of the client stochastic gradient:\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n(cid:105)\n\n(cid:42)\n\n≤ f (x(t)) + E|t\n\n∇f (x(t)), −\n\n ̃η KS\n\n(cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni∈S\n\nk=1\n\ngi(x(t,k)\n\ni\n\n) + gc(x(t))\n\n(cid:17)\n\n(cid:43)\n\n+\n\nβ 2\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:42)\n\n≤ f (x(t)) − ̃η\n\n∇f (x(t)),\n\n1 KN\n\nN (cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni=1\n\nk=1\n\n∇ff(x(t,k)\n\ni\n\n) + ∇fc(x(t))\n\n(cid:43)\n\n(cid:17)\n\n+\n\nβ 2\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n.\n\nNext, we make use of the fact that −ab = 1\n\n2 ((b − a)2 − a2 − b2) ≤ − 1\n\n2 a2 + 1\n\n2 (b − a)2:\n\n34\n\nUnder review as a conference paper at ICLR 2023\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n(cid:105)\n\n≤ f (x(t)) −\n\n(cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n ̃η 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 KN\n\nN (cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni=1\n\nk=1\n\n∇ff(x(t,k)\n\ni\n\n) + ∇fc(x(t))\n\n(cid:17)\n\n− ∇f (x(t))\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n+\n\n ̃η 2\n\nβ 2\n\n+\n\n+\n\n ̃η 2\n\nβ 2\n\nE|t\n\n2 (cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13) (cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n≤ f (x(t)) −\n\n ̃η 2\n\nE|t\n\n2 (cid:13) (cid:13)\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13) (cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n≤ f (x(t)) −\n\n ̃η 2\n\n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)\n\n1 KN\n\nN (cid:88)\n\nK (cid:88)\n\n(cid:16)\n\ni=1\n\nk=1\n\n∇ff(x(t,k)\n\ni\n\n) − ∇ff(x(t))\n\n(cid:17)\n\n(cid:13) 2\n(cid:13) (cid:13) (cid:13) (cid:13)\n\n+\n\n+\n\n ̃η 2\n\nβ 2\n\n1 KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)∇ff(x(t,k)\n\ni\n\n) − ∇ff(x(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n.\n\nNext, we use smoothness (Definition D.1), and the definition of client drift (Equation 12):\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n≤ f (x(t)) −\n\n(cid:105)\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n ̃η 2\n ̃ηβ2 2\n\n+\n\n+\n\n1 KN\n\nN (cid:88)\n\nK (cid:88)\n\ni=1\n\nk=1\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\nβ 2\n\nE|t\n\n(cid:13) (cid:13)\n\n(cid:13)x(t+1) − x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤ f (x(t)) −\n\n ̃η 2\n\n+\n\nβ 2\n\nE|t\n\n ̃ηβ2 2\n\nE (t)\n\n+\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13) (cid:13)x(t+1) − x(t)(cid:13) (cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n.\n\nThe last term is the variance of the server update, for which we can substitute the bound from Lemma D.9:\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n(cid:105)\n\n≤ f (x(t)) −\n\n(cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+\n\n ̃η 2\n(cid:18)\n\n+\n\nβ 2\n\n4 ̃η2β2E (t) + 2 ̃η2\n\nE (t)\n\n ̃ηβ2 2\n(cid:18) 2 KS\n\n(cid:19)\n\nσ2 + σ2\n\nc\n\n+ 2 ̃η2E|t\n\n≤ f (x(t)) −\n\n− β ̃η2\n\n(cid:19) (cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:18) ̃ηβ2 2\n\n+ 2 ̃η2β3\n\n(cid:18) ̃η 2\n(cid:18) 2 KS\n\n+ ̃η2β\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\n.\n\n2(cid:19)\n\n(cid:13) (cid:13) (cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13) (cid:19)\n\nE (t)\n\nAssuming a bound on effective step-size ̃η ≤ 1\n\n18β :\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n(cid:105)\n\n≤ f (x(t)) −\n\n4 ̃η 9\n\n(cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+\n\nβ 27\n\nE (t) +\n\n(cid:18) 2 KS\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\nβ ̃η2.\n\n35\n\nUnder review as a conference paper at ICLR 2023\n\nLemma D.13 (Non-convex bounded drift). Suppose our functions satisfy bounded variance and β-smoothness (Definition D.1). Then the drift is bounded as:\n\nE (t) ≤\n\n4 ̃η 9βη2 s\n\nE\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n2 ̃η2 η2 s\n\n(cid:18) 1 K\n\nσ2 + 4σ2\n\nc\n\n(cid:19)\n\n.\n\nProof. We begin with the summand of the drift term, looking at the drift of a particular client i at local step k. Expanding this summand out:\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n= E\n\n= E\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)x(t,k−1) (cid:13)x(t,k−1)\n\n(cid:13) (cid:13)\n\ni\n\n(cid:16)\n\n− η\n\ngi(x(t,k−1)\n\ni\n\n) + gc(x(t))\n\n(cid:17)\n\n− x(t)(cid:13)\n\n− x(t) − ηgi(x(t,k−1)\n\ni\n\n) − ηgc(x(t))\n\n2 (cid:13) (cid:13) (cid:13) 2\n(cid:13) (cid:13)\n\n.\n\nSeparating mean and variance of the client gradient:\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤ E\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t) − η∇ff(x(t,k−1)\n\ni\n\n) − ηgc(x(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ η2σ2.\n\nNext we use relaxed triangle inequality (Lemma D.2) to further separate terms:\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n(cid:18)\n\n≤\n\n1 +\n\n(cid:19)\n\nE\n\n1 a\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ (1 + a) η2E\n\n(cid:18)\n\n≤\n\n1 +\n\n(cid:19)\n\nE\n\n1 a\n\n(cid:13) (cid:13)\n\ni\n\n(cid:13)∇ff(x(t,k−1) − x(t)(cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n(cid:13) (cid:13)\n\n2 (cid:13) (cid:13)\n\n) + gc(x(t))\n\n+ η2σ2\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ (1 + a) η2E\n\n(cid:16)\n\n(cid:13) (cid:13) (cid:13)\n\n∇ff(x(t,k−1)\n\ni\n\n) − ∇ff(x(t))\n\n(cid:17)\n\n+\n\n(cid:16)\n\ngc(x(t)) − ∇fc(x(t))\n\n(cid:17)\n\n+ ∇f (x(t))\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ η2σ2 (cid:18)\n\n≤\n\n1 +\n\n(cid:19)\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n1 a\n\n+ (1 + a) 2η2 E (cid:124)\n\n(cid:13) (cid:13)\n\n(cid:13)∇ff(x(t,k−1)\n\ni\n\n) − ∇ff(x(t)) (cid:123)(cid:122) H\n\n(cid:125)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ (1 + a) 4η2E\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+ (1 + a) 4η2 E (cid:124)\n\n(cid:13) (cid:13)gc(x(t)) − ∇fc(x(t)) (cid:13) (cid:123)(cid:122) J\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n(cid:125)\n\n+η2σ2.\n\nIn the above inequality, term H can be converted via smoothness (Definition D.1), and term J is the variance of the centralized stochastic gradient (Equation 6). Letting a = K, we have:\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤\n\n(cid:18) K + 1 K\n+ 4Kη2σ2\n\n+ 2Kη2β2\n\nc + η2σ2.\n\n(cid:19)\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k−1)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n+ 4Kη2E\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\nUnrolling the above recurrence, we get:\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤\n\n(cid:18)\n\n4Kη2E\n\n(cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+ 4Kη2σ2\n\nc + η2σ2\n\n(cid:19) k−1 (cid:88)\n\nj=0\n\n(cid:18) K + 1 K\n\n(cid:19)j\n\n+ 2Kη2β2\n\n≤\n\n(cid:18) 4 ̃η2 Kη2 s\n\nE\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n ̃η2 Kη2 s\n\n(cid:18) 1 K\n\nσ2 + 4σ2\n\nc\n\n(cid:19)(cid:19) k−1 (cid:88)\n\nj=0\n\n(cid:18) K + 1 K\n\n+\n\n2 ̃η2β2 Kη2 s\n\n(cid:19)j\n\n36\n\nUnder review as a conference paper at ICLR 2023\n\nAssuming ηs ≥ 1, and ̃η ≤ 1\n\n18β , we have K+1\n\nK + 2 ̃η2β2\n\nKη2 s\n\n≤ 1 + 163\n\n162K , and hence\n\nk−1 (cid:88)\n\nj=0\n\n(cid:16) K+1\n\nK + 2 ̃η2β2\n\nKη2 s\n\n(cid:17)j\n\n≤\n\nK−1 (cid:88)\n\nj=0\n\n(cid:0)1 + 163\n\n162K\n\n(cid:1)j\n\n= (cid:0)1 + ( 163\n\n162K\n\n(cid:1)K\n\n− 1) 162K\n\n163 ≤ (e\n\n163\n\n162 − 1)K ≤ 2K.\n\nE\n\n(cid:13) (cid:13)\n\n(cid:13)x(t,k)\n\ni\n\n− x(t)(cid:13)\n\n2 (cid:13) (cid:13)\n\n≤\n\n(cid:18) 2 ̃η\n\n9βKη2 s\n\nE\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n ̃η2 Kη2 s\n\n(cid:18) 1 K\n\nσ2 + 4σ2\n\nc\n\n(cid:19)(cid:19)\n\n2K\n\nAdding back the summation terms over i and k, the bound on client drift is:\n\nE (t) ≤\n\n4 ̃η 9βη2 s\n\nE\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n+\n\n2 ̃η2 η2 s\n\n(cid:18) 1 K\n\nσ2 + 4σ2\n\nc\n\n(cid:19)\n\n.\n\nProofs of Theorem C.2 for Non-Convex Case Adding the statements of Lemmas D.12 and D.13, and assuming ηs ≥\n\nS, we get:\n\n√\n\n(cid:104)\n\nE|t\n\nf (x(t+1))\n\n≤ f (x(t)) −\n\n(cid:105)\n\n+\n\n4 ̃η 9\n(cid:18) 4 ̃η 9βη2 s\n1 3\n\n ̃η\n\nE\n\n(cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2\n(cid:13)∇f (x(t)) (cid:13) (cid:13) (cid:13)\n\n+\n\n(cid:18) 2 KS 2 ̃η2 η2 s\n(cid:18) 3 KS\n\n+\n\nσ2 + σ2\n\nc\n\n(cid:19)\n\nβ ̃η2\n\n(cid:18) 1 K\n\n(cid:19)(cid:19)\n\nσ2 + 4σ2\n\nc\n\nσ2 + 2σ2\n\nc\n\n(cid:19)\n\nβ ̃η2\n\n+\n\nβ 27\n\n≤ f (x(t)) −\n\nWith the above, we have a recursive bound on the loss after round t + 1. We can use lemmas (e.g., Lemma 2 in Karimireddy et al. (2020b), Lemma 4 in Stich (2019)) which establish a sub-linear convergence rate for such recursions. Assuming ̃η ≤ 1\n\nS, we get:\n\n√\n\nmin t∈{1,2,...,T +1}\n\n(cid:13) (cid:13)∇f (x(t)) (cid:13)\n\n(cid:13) 2\n(cid:13) (cid:13)\n\n≤\n\n54βF T + 1\n\n+\n\n18β and ηs ≥ (cid:113)(cid:0) 3 6\n\nKS σ2 + 2σ2 T + 1\n\n√\n\nc\n\n(cid:1) βF\n\n.\n\nIn the above expressions, F is the error at initialization, f (x(0)) − f (x∗).\n\nThis yields an expression for the number of rounds T to reach an error (cid:15):\n\nT = O\n\n(cid:32) (cid:0)σ2 + KSσ2 KS(cid:15)2\n\nc\n\n(cid:1) βF\n\n(cid:33)\n\n.\n\n+\n\nβF (cid:15)\n\n37",
  "translations": [
    "# Summary Of The Paper\n\nThe paper focuses on a federated learning setting where there is a form of potentially high bias on the client-side (I use the work bias in the basic sense (i.e., favored towards to) instead of the pure statistical sense). These biases are exemplified by the datasets used by the paper, which come in the form of unbalanced labeling on client devices, and differences in the specification of client devices (i.e., high-end phones vs low-end phones). For these biases, the paper proposes to mix the datasets between the server (or datacenter) and the clients. Specifically, the paper uses two datasets:\n\n* CelebA: the dataset is partitioned into people who are \"smiling\" and who are \"not smiling.\" The claim is that the majority of pictures on people's phone are smiling. Here, the server contains the non-smiling pictures and the clients contain smiling pictures and the task is to predict smiling or not smiling.\n\n* StackOverflow and Wikipedia: the StackOverflow dataset is used for clients, and the Wikipedia dataset is used for the server.\n\nThe paper also proposes a mixed loss function, which is the sum of the loss on server data and the loss on client data.\n\n# Strength And Weaknesses\n\nIn Section 2 Algorithms, it would be more helpful to explain the notation used and the various components in the two algorithms in this section rather than relegating details to the appendix. Additionally, it is not clear to me what the components ClientOptimizer, ServerOptimizer, CentralOptimizer, and MergerOptimizer in this context are, what each performs (they appear to be blackbox functions), how they relate to equation (1), and how they connect to Section 5.\n\nThe rationale on the StackOverflow and Wikipedia (and the allocation of 25% server, 75% client) dataset as it pertains to client devices is not thoroughly explained in the experimental section.\n\nSection 4.3 as it pertains to \"mixing different loss\" and to the focus of the paper is not clear to me.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nClarity and quality require improvement. The novelty is low.\n\n# Summary Of The Review\n\nThe proposed idea and presentation require more work.\n\nUpdate: Thank you to the authors for their reply. I will maintain my review.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n1: The contributions are neither significant nor novel.\n\n# Empirical Novelty And Significance\n\n1: The contributions are neither significant nor novel.",
    "# Summary Of The Paper\nThe paper introduces **Mixed Federated Learning (FL)**, a novel approach that integrates both decentralized and centralized learning paradigms while adhering to privacy constraints. The authors present three algorithms—**PARALLEL TRAINING**, **1-WAY GRADIENT TRANSFER**, and **2-WAY GRADIENT TRANSFER**—that enhance model performance by leveraging centralized data for additional training while minimizing communication and computational burdens on clients. Through extensive experiments across multiple tasks, including smile classification, language modeling, and movie recommendation, the authors demonstrate that mixed FL achieves oracle-level accuracy and significantly reduces overheads, alongside establishing theoretical convergence bounds for each algorithm.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear motivation for mixed FL, which addresses critical challenges in traditional FL, such as distribution shifts and resource limitations. The proposed algorithms are well-defined, and the experimental results robustly validate the claims of improved performance and reduced overhead. However, the paper could benefit from a deeper exploration of potential limitations and failure modes of the proposed methods, particularly in scenarios with highly heterogeneous client distributions. Additionally, while the convergence analysis is presented, further discussion on the implications of these results in practical applications would enhance the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions and findings. The methodology is thoroughly described, making it relatively easy for other researchers to reproduce the results. The novelty of combining centralized and decentralized approaches in FL is significant, as it offers a fresh perspective on a well-established area of research. However, while the algorithms and experimental setups are detailed, further clarification on hyperparameter choices and their impact on performance could improve reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of federated learning by introducing mixed FL, which effectively combines centralized and decentralized learning strategies. The empirical results are compelling, demonstrating both improved model accuracy and reduced client burdens, although a more thorough discussion of limitations and practical implications would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to Federated Learning (FL) termed Mixed Federated Learning (Mixed FL), which combines decentralized and centralized objectives to address data distribution shifts that can hinder model training. The authors introduce three algorithms: Parallel Training (PT), 1-Way Gradient Transfer (1-W GT), and 2-Way Gradient Transfer (2-W GT), each designed to integrate centralized and federated training processes. The paper establishes theoretical convergence bounds for these algorithms and validates their effectiveness through extensive experiments across three tasks: smile classification, language modeling, and movie recommendation. Results demonstrate that Mixed FL algorithms achieve oracle-level accuracy while significantly reducing communication and computation costs.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to leveraging centralized data within a federated framework, addressing a critical gap in the existing literature on FL. The extensive experimental validation across diverse tasks adds robustness to the findings, showcasing practical applicability. Additionally, the theoretical insights provide a foundation for understanding the convergence behavior of the proposed algorithms. However, the paper also has limitations, such as the assumption of homogeneity among clients, which may not be valid in non-IID scenarios commonly encountered in practice. The added complexity of implementing Mixed FL algorithms may hinder their adoption, and the generalizability of the results beyond the tested domains remains uncertain. Furthermore, the theoretical convergence bounds are described as \"loose,\" suggesting the need for refinement.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the significance of the proposed Mixed FL approach. The methodology is explained in detail, and the experimental design is thorough, allowing for reproducibility. However, the complexity of the algorithms might pose challenges for practitioners looking to implement them. The novelty of the approach is significant, as it presents a fresh perspective on integrating centralized data within the constraints of federated learning while maintaining privacy.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning through its introduction of Mixed Federated Learning, supported by both theoretical and empirical evidence. While the findings are promising, the assumptions regarding data distribution and the complexity of implementation may limit practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces **Mixed Federated Learning (FL)**, a novel framework that integrates decentralized client training with centralized data to enhance model performance while preserving privacy. The authors propose three algorithms: **PARALLEL TRAINING (PT)**, **1-WAY GRADIENT TRANSFER (1-W GT)**, and **2-WAY GRADIENT TRANSFER (2-W GT)**, all designed to balance the trade-offs between centralized and decentralized data usage. Experimental results demonstrate that mixed FL achieves oracle-level accuracy and significantly reduces communication and computational overhead, with reductions of over 90%. Theoretical convergence bounds for each algorithm are also established, indicating the robust performance of the proposed methods across varying conditions.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear motivation for mixed FL and the introduction of multiple innovative algorithms that address the challenges of federated learning, particularly in handling data heterogeneity and privacy concerns. The empirical validation across diverse tasks, including facial attribute classification and language modeling, effectively showcases the practical applicability of the proposed methods. However, a potential weakness is the reliance on assumptions regarding client loss smoothness and bounded variance, which may limit the generalizability of the convergence results under real-world conditions. Furthermore, while the experimental results are compelling, additional comparisons with more baseline methods could strengthen the evaluation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings, making it accessible to readers with a background in federated learning. The quality of writing is high, with thorough explanations and justifications for the proposed algorithms and their theoretical underpinnings. The novelty of introducing mixed FL is significant, as it combines elements of both centralized and decentralized approaches effectively. Reproducibility is supported by detailed descriptions of the algorithms and experiments, although the paper could benefit from providing access to code or data to enhance transparency.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the field of federated learning by introducing mixed FL, which effectively leverages both centralized and decentralized data while maintaining privacy. The empirical results are strong, and the theoretical contributions provide valuable insights into the convergence properties of the proposed algorithms. However, some assumptions may limit general applicability, and additional comparisons could enhance the robustness of the findings.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces a novel framework for mixed federated learning (FL), which merges centralized and decentralized learning strategies to improve model performance on varying inference distributions. The methodology includes the development of three algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER, all designed to reduce computational and communication overhead for client devices. The findings demonstrate significant improvements in accuracy across multiple tasks, alongside a reduction in resource requirements, while also establishing theoretical convergence bounds for the proposed algorithms.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to mixed FL, extensive experimental validation, and solid theoretical foundations. However, it has notable weaknesses, including a lack of exploration of scenarios where mixed FL might underperform compared to traditional methods, limited diversity in datasets for experiments, and assumptions of IID data distributions in theoretical analysis. Furthermore, while the practical implementation details are user-friendly, the paper lacks a thorough discussion of the computational complexity of algorithms as they scale and does not adequately address the trade-offs of server-side computations or the mechanisms for mitigating distribution shifts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections and accessible descriptions of algorithms. The quality of the experiments is commendable, though the generalizability of results could be improved with additional diverse datasets. The novelty of introducing mixed FL is significant, but certain aspects, such as the assumptions made in theoretical analysis, may hinder reproducibility in real-world applications. Overall, while the paper is well-written, it would benefit from addressing the highlighted limitations to enhance understanding and applicability.\n\n# Summary Of The Review\nThis paper presents a valuable contribution to federated learning through the introduction of mixed FL, supported by extensive experimental validation and theoretical foundations. However, several limitations regarding assumptions, dataset diversity, and practical implications must be addressed for the contribution to be fully realized in real-world applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces \"mixed federated learning\" (MFL), a novel framework that integrates decentralized and centralized learning objectives, leveraging both federated and centralized datasets to enhance model training while preserving data privacy. The authors present three algorithms: PARALLEL TRAINING (PT), 1-WAY GRADIENT TRANSFER (1-W GT), and 2-WAY GRADIENT TRANSFER (2-W GT). Empirical results demonstrate that MFL achieves significant reductions in communication and computation costs—over 90% reduction in communication and up to 99.9% in client computation—while matching oracle-level accuracy across various tasks, including facial attribute classification, language modeling, and movie recommendation.\n\n# Strength And Weaknesses\n**Strengths:**\n- The innovative framework of MFL provides a promising direction for federated learning, balancing model performance with data privacy, which is paramount in today's data-sensitive environment.\n- The robustness of experimental results across diverse tasks showcases the practical applicability of the proposed methods, establishing a strong case for MFL's effectiveness in real-world scenarios.\n- The theoretical insights, including convergence bounds for the algorithms, bolster the understanding of their performance and reliability, guiding future research.\n\n**Weaknesses:**\n- The complexity introduced by integrating centralized data into the federated learning framework may pose challenges for implementation, particularly in ensuring seamless interoperability.\n- The scalability of the proposed algorithms in the face of highly heterogeneous client distributions remains unexamined, potentially limiting their effectiveness in diverse deployment scenarios.\n- There is a risk of bias being introduced into the federated learning process due to the influence of centralized data, which requires careful consideration and management.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodologies, and findings. The quality of writing is high, facilitating comprehension of complex concepts. The novelty of the mixed federated learning approach is significant, offering a fresh perspective on federated learning frameworks. The reproducibility of the results is supported by detailed experimental setups, although additional guidance on implementation would enhance clarity and practical application.\n\n# Summary Of The Review\nOverall, the paper represents a noteworthy contribution to the field of federated learning through its introduction of mixed federated learning. The proposed algorithms are theoretically sound and empirically validated, presenting a compelling case for their application in privacy-sensitive contexts. Future work should address implementation complexities and the implications of centralized data to further solidify the framework's applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "## ICLR Review Notes: Adversarial Training with Mixed Learning Techniques\n\n### Summary Of The Paper\nThe paper presents a novel framework for adversarial training called \"Mixed Adversarial Training\" (MAT), which combines decentralized and centralized learning methodologies to enhance the robustness of machine learning models against adversarial attacks. The authors introduce three algorithms: Parallel Adversarial Training, One-Way Gradient Transfer, and Two-Way Gradient Transfer, each designed to optimize the training process while preserving data privacy. Empirical results demonstrate that MAT effectively reduces vulnerability to adversarial attacks across various tasks, such as image classification and text generation, while maintaining or improving model accuracy. Additionally, the authors provide theoretical convergence bounds for their proposed methods.\n\n### Strength And Weaknesses\n**Strengths:**\n- The integration of decentralized and centralized approaches in adversarial training is innovative and has the potential to significantly improve model robustness.\n- Extensive empirical validation across diverse datasets and tasks showcases the effectiveness of MAT, offering strong evidence for its claims.\n- The paper is well-structured, presenting clear distinctions among the three algorithms, which aids in understanding and potential future application.\n\n**Weaknesses:**\n- The complexity introduced by multiple algorithms may hinder real-world implementation, where simplicity and efficiency are often critical.\n- The theoretical guarantees provided may rely on specific assumptions about data distribution and client heterogeneity, which could limit applicability in practical scenarios.\n\n### Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical flow that facilitates understanding of the proposed methods and their implications. The quality of the empirical experiments is high, with results presented in a clear and interpretable manner. The novelty of combining decentralized and centralized learning for adversarial training is significant. However, the reproducibility may be challenged by the complexity of the algorithms and the assumptions underlying the theoretical claims.\n\n### Summary Of The Review\nThe paper makes a notable contribution to adversarial training by introducing Mixed Adversarial Training, which effectively leverages both decentralized and centralized approaches to enhance model robustness. While the empirical results are promising, the complexity of implementation and the reliance on specific assumptions may pose challenges for practical application. Overall, the work is a valuable addition to the field, meriting further exploration.\n\n### Correctness\n4\n\n### Technical Novelty And Significance\n5\n\n### Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Mixed Federated Learning (MFL), a novel approach that integrates both decentralized and centralized data to enhance model accuracy while preserving privacy. MFL claims to significantly reduce communication overhead and client computation, achieving reductions of over 90% in both areas. The authors present three innovative algorithms—PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER—designed to optimize federated learning, with extensive experimental validation demonstrating MFL's ability to achieve oracle-level accuracy across diverse tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of MFL as a paradigm shift in federated learning, showcasing the potential for substantial improvements in both model performance and efficiency. The three algorithms presented are well-conceived and address critical limitations of existing federated optimization methods. However, the paper's claims of unprecedented accuracy and efficiency may require more comprehensive empirical validation to substantiate the assertions made, particularly regarding the communication and computation overhead reductions. Additionally, while the theoretical contributions are compelling, more detail on practical implementations and potential limitations could enhance the work's robustness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its objectives and findings. The novelty of MFL and its associated algorithms is evident, representing a significant advancement in the field of federated learning. However, the reproducibility of results may be a concern, as the paper does not provide enough detail on experimental setups or datasets used, which could hinder other researchers from replicating the findings effectively. Overall, the quality of writing is high, but the depth of methodological transparency could be improved.\n\n# Summary Of The Review\nThis paper presents a groundbreaking approach to federated learning through Mixed Federated Learning (MFL), combining decentralized and centralized methods to achieve remarkable performance improvements. While the theoretical contributions are significant, the empirical validation and methodological transparency require further enhancement to fully establish the claims made.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces mixed federated learning (FL), a novel approach that integrates decentralized and centralized learning to enhance model performance while reducing computational load. The authors propose three algorithms: PARALLEL TRAINING (PT), 1-WAY GRADIENT TRANSFER (1-W GT), and 2-WAY GRADIENT TRANSFER (2-W GT). The findings indicate that mixed FL can achieve oracle-level accuracy and significantly decrease communication and computation overhead, with reported reductions of over 80%. Experimental results demonstrate that mixed FL outperforms traditional FL in various tasks, including a smile classifier, language model, and movie recommendation systems, while also establishing convergence bounds for the proposed algorithms.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative combination of decentralized and centralized training, potentially offering substantial improvements in efficiency and accuracy. The introduction of three distinct algorithms provides a comprehensive approach to mixed FL, allowing for flexibility depending on the application. However, the paper does face weaknesses, particularly in the overstatement of communication overhead reductions and accuracy claims, which were found to be exaggerated. Furthermore, while the convergence analysis is informative, the benefits in certain scenarios are less pronounced than initially claimed, which could mislead readers regarding the efficacy of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with well-structured sections that outline the methodologies and findings. However, some claims made in the abstract and conclusions may lead to confusion due to discrepancies in the results presented. The quality of the experiments appears solid, but the reproducibility could be hampered by the altered claims regarding performance metrics, which should be more accurately represented. The novelty of the mixed FL approach is significant, as it addresses real-world challenges in federated learning, but the accuracy of the presented findings is a concern.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to mixed federated learning with three innovative algorithms that show potential for real-world applications. However, the overstatement of certain claims and discrepancies in experimental results raise concerns about the reliability of the findings. The contributions are noteworthy, but the paper would benefit from more careful validation of its claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to mixed federated learning (FL) that incorporates centralized data to enhance model performance while maintaining client data privacy. The authors propose a framework that leverages both federated and centralized loss functions to achieve efficient learning in scenarios where clients have heterogeneous data distributions. The findings demonstrate reduced communication and computation overhead, although the methodology relies on several assumptions, including the IID nature of client data and the effectiveness of centralized data.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative integration of centralized data into the federated learning paradigm, which could potentially address some challenges associated with data heterogeneity. However, the paper's reliance on strong assumptions, such as the IID distribution of client data and the effectiveness of centralized data, raises concerns about the practical applicability of the proposed methods. The lack of exploration regarding the potential risks of integrating centralized data and the limited scope of empirical evaluations further weaken the overall contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure and logical flow of ideas. However, the clarity of certain assumptions, particularly those regarding data distribution and convergence properties, could be improved to enhance understanding. The novelty of the proposed mixed FL approach is notable, yet it requires more rigorous comparisons with existing methods to establish its significance. The reproducibility of results may be hindered by the limited description of experimental setups and evaluation metrics.\n\n# Summary Of The Review\nOverall, while the paper introduces a promising approach to mixed federated learning, it is constrained by strong assumptions and a limited empirical scope. The potential benefits of integrating centralized data are not thoroughly explored, leading to concerns about the practical applicability of the proposed framework.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel approach to federated learning (FL) called mixed federated learning, which integrates centralized and decentralized learning objectives while ensuring data privacy. The authors propose three algorithms—PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER—that optimize a mixed loss function combining federated and centralized losses. Through empirical experiments on facial attribute classification, language modeling, and movie recommendation tasks, the authors demonstrate that mixed FL significantly improves model accuracy while reducing communication and computation overhead for clients. The paper also establishes theoretical convergence bounds for the algorithms under various conditions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to enhancing federated learning by effectively integrating centralized data without compromising privacy. The proposed algorithms are well-defined and address practical issues faced in traditional FL, such as client data heterogeneity and resource constraints. However, the paper could benefit from a more detailed analysis of the computational complexity of the algorithms, as well as a clearer discussion of the limitations and potential challenges in real-world applications of mixed FL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly presents its methodology and findings. The organization of the content is logical, making it easy for the reader to follow the development of the proposed algorithms. The experimental results are clearly presented and support the claims made by the authors. However, the novelty of the proposed mixed FL approach could be further emphasized by more explicitly distinguishing it from existing techniques. Reproducibility is facilitated by the thorough description of algorithms and experiments, though the availability of code and datasets would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a promising advancement in federated learning through the introduction of mixed federated learning and its associated algorithms. While the contributions are significant and the findings are well-supported, the paper could improve by addressing computational complexity and providing clearer distinctions from existing methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel framework for enhancing the interpretability of deep learning models in the context of image classification. The authors propose a two-step approach: first, generating a set of interpretable features through unsupervised learning techniques, and second, utilizing these features to improve the model's decision-making transparency. The methodology involves both qualitative and quantitative evaluations, demonstrating that the proposed approach not only maintains the accuracy of the baseline models but also significantly enhances the interpretability of their predictions. Experimental results on several benchmark datasets highlight the effectiveness of the proposed framework in making complex models more understandable without sacrificing performance.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Novelty**: The paper presents an innovative approach to interpretability, addressing a critical issue in deep learning that has been gaining traction in recent years.\n2. **Comprehensive Evaluation**: The authors provide a thorough evaluation of their method across multiple datasets, showcasing its versatility and robustness.\n3. **Clear Motivation**: The motivation for the research is clearly articulated, emphasizing the importance of interpretability in real-world applications of AI.\n\n**Weaknesses:**\n1. **Lack of Comparative Baselines**: The proposed method could benefit from more extensive comparisons against existing state-of-the-art interpretability techniques to validate its effectiveness.\n2. **Methodological Clarity**: While the framework is promising, certain aspects of the methodology lack clarity, particularly in terms of the feature extraction process.\n3. **Theoretical Underpinnings**: The theoretical justification for why the proposed features lead to better interpretability could be strengthened, as current discussions are somewhat superficial.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, making it accessible for readers. The novelty of the approach is evident, and the authors provide a logical flow of ideas. However, the reproducibility of the results may be compromised due to insufficient methodological details, particularly in the feature extraction phase. Including more specific information about the implementation and dataset characteristics would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper offers a promising contribution to the field of interpretable machine learning, showcasing an innovative framework that balances model performance and interpretability. However, it would greatly benefit from clearer methodological details and stronger empirical validation against existing methods to reinforce its claims.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces Mixed Federated Learning (Mixed FL), a novel approach that integrates decentralized and centralized learning, thereby enhancing the capabilities of traditional federated learning frameworks. It proposes three algorithms—PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER—that utilize a coordinating server to compute additional loss terms while preserving data privacy. The methodology enables the integration of centralized data alongside federated data, which aids in addressing distribution shifts and reduces the computational burden on clients. Empirical results demonstrate that Mixed FL achieves high accuracy that rivals oracle performance while significantly decreasing overhead in terms of communication and computation.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative blending of decentralized and centralized learning paradigms, which can potentially lead to improved model performance, especially in scenarios where client data alone is insufficient for effective training. The algorithms are well-defined and their empirical validation supports the claims made regarding accuracy and efficiency. However, a potential weakness is the reliance on the coordinating server, which could introduce a single point of failure and may raise concerns regarding the robustness and scalability of the approach in practice. Additionally, the paper could benefit from a more thorough discussion of the limitations and scenarios where Mixed FL might not be applicable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its ideas clearly, with a logical flow from introduction to conclusion. The methodology is described in sufficient detail to allow for reproducibility, though additional empirical comparisons with existing federated learning approaches could enhance the understanding of its impact. The novelty of combining centralized and decentralized learning is commendable, and the experiments provide good insights into its effectiveness. However, further clarification on the theoretical underpinnings of the algorithms could strengthen the overall clarity and depth of the contribution.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in federated learning through the introduction of Mixed FL, showcasing promising results via empirical validation. While the approach is innovative and addresses important challenges, further exploration of its limitations and robustness would strengthen the contribution.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to federated learning (FL) termed Mixed Federated Learning (MFL), which integrates both decentralized and centralized learning methods to enhance model performance and efficiency. The authors introduce three new algorithms: Parallel Training, 1-Way Gradient Transfer, and 2-Way Gradient Transfer, specifically designed to tackle prevalent challenges in FL, such as distribution shifts and computation overhead. Empirical evaluations on tasks including facial attribute classification, language modeling, and movie recommendation demonstrate significant improvements in accuracy and reductions in communication and computation overhead, with convergence bounds established theoretically to support the efficacy of the proposed methods.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative combination of decentralized and centralized learning strategies, which addresses important challenges in the field of federated learning. The introduction of three distinct algorithms provides a comprehensive framework for tackling various aspects of FL, demonstrating a clear understanding of the underlying issues. The empirical results are robust, showcasing significant performance improvements across different tasks. However, one potential weakness is the lack of extensive real-world testing beyond the selected tasks, which may limit the generalizability of the findings. Additionally, while the theoretical convergence analysis is solid, further exploration of edge cases and more diverse datasets could strengthen the paper's contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology, making it accessible to readers with varying levels of familiarity with federated learning. The quality of writing is high, with comprehensive explanations of the algorithms and their implications. The novelty of the proposed mixed FL approach is significant, as it bridges a gap in existing research on federated learning. Reproducibility is supported by a detailed description of the algorithms and experimental setup, although the paper could benefit from sharing code or datasets to facilitate replication of results by other researchers.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of federated learning by introducing Mixed Federated Learning, which effectively combines decentralized and centralized learning methods. The proposed algorithms show promise in improving model performance while addressing critical challenges such as data privacy and computation overhead. While the theoretical insights are robust, further empirical validation in more diverse settings would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the concept of Mixed Federated Learning (MFL), which synergizes the benefits of decentralized and centralized learning approaches. It proposes three algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER, aimed at enhancing learning efficiency while reducing communication and computational overhead. Empirical results demonstrate that these algorithms significantly improve accuracy across various tasks, including smile classification, language modeling, and movie recommendation, while also providing theoretical convergence bounds for their performance.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to federated learning, combining centralized and decentralized frameworks, which addresses existing limitations in the field. The empirical validation across multiple tasks provides strong support for the proposed methods, highlighting their practical relevance. However, the paper could benefit from a more comprehensive explanation of the algorithms to improve clarity for readers unfamiliar with the subject. Additionally, while the theoretical analysis is robust, the potential biases introduced by centralized data are not fully addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clear, making it accessible to a broad audience. The novelty of the proposed mixed FL approach is significant and contributes to ongoing discussions in federated learning research. The empirical results are presented with sufficient detail to allow for reproducibility, although additional visual aids could enhance understanding of the more complex concepts discussed.\n\n# Summary Of The Review\nThis paper presents a meaningful advancement in federated learning methodologies by introducing Mixed Federated Learning, along with practical algorithms and empirical validation. While the contributions are compelling and well-supported, further clarification and exploration of potential biases would strengthen its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel paradigm called mixed Federated Learning (FL), which integrates a supplementary loss term computed at a coordinating server while preserving inherent privacy constraints. It presents three innovative algorithms: PARALLEL TRAINING (PT), 1-WAY GRADIENT TRANSFER (1-W GT), and 2-WAY GRADIENT TRANSFER (2-W GT). The methodology allows for the concurrent optimization of centralized and decentralized datasets, effectively addressing distribution shifts and achieving oracle-level accuracy while significantly reducing communication overhead (by over 90%) and client computational load (by up to 99.9%). The authors also provide theoretical convergence bounds for each algorithm across various conditions.\n\n# Strength And Weaknesses\nThe main strengths of this paper lie in its innovative approach to addressing the limitations of traditional FL by incorporating centralized data without compromising privacy. The empirical results demonstrate substantial improvements in accuracy and efficiency across multiple tasks, showcasing the practical applicability of the proposed algorithms. However, the paper may benefit from a more detailed exploration of the assumptions underlying the convergence analysis, as well as clearer explanations of the implications of the theoretical bounds in practical scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-defined sections and a structured approach to presenting the algorithms and their evaluations. The quality of the writing is commendable, although some mathematical formulations could be elaborated for better accessibility. The novelty of the mixed FL paradigm is significant, as it effectively combines decentralized and centralized learning while adhering to privacy constraints. The reproducibility of the results appears feasible, provided that the implementation details of the algorithms are made publicly available.\n\n# Summary Of The Review\nOverall, this paper presents a compelling advancement in the field of Federated Learning by introducing mixed FL, which successfully integrates centralized data into decentralized frameworks while maintaining privacy. The empirical results are impressive, demonstrating the practical impact of the proposed algorithms on accuracy and computational efficiency.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper introduces a new approach termed mixed federated learning (FL), which aims to optimize communication and computation overhead in federated settings. It presents three algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER. The authors claim significant improvements in efficiency through empirical experiments, asserting reductions in overhead by up to 90%. However, the paper lacks a robust justification for the necessity of this new approach, especially in real-world applications where traditional FL might suffice.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to address communication and computation challenges in federated learning. However, this strength is undermined by a weak motivation for mixed FL and a lack of novel contributions. The algorithms presented do not exhibit substantial innovation compared to existing methods, and the distinctions between them are not compelling. Additionally, the empirical evaluation is limited in scope, raising concerns about the reproducibility and generalizability of the findings. The theoretical underpinnings of the convergence bounds are overly optimistic and do not adequately consider non-IID data settings or potential biases introduced by centralized data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by vague concluding remarks that fail to outline future research directions or the potential impact of the findings. The overall quality suffers from an insufficient critical evaluation of the proposed approach's limitations and a lack of discussion on ethical implications. The novelty of the algorithms is not compelling, as they largely depend on existing methods without significant advancements. Reproducibility is a concern due to the narrow experimental focus and the absence of diverse scenarios or data distributions.\n\n# Summary Of The Review\nOverall, the paper presents an interesting yet poorly justified approach to mixed federated learning. While it attempts to address communication and computation challenges, it lacks rigorous theoretical and empirical contributions, making it difficult to ascertain its significance in advancing the field of federated learning.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach termed Mixed Federated Learning (FL), which integrates decentralized learning from privacy-sensitive data with centralized objectives to improve model performance and accuracy. The methodology involves innovative algorithms such as PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER, which significantly reduce communication overhead by over 90% and computation load for clients by up to 99.9%. Experimental results demonstrate that models trained using mixed FL can achieve oracle-level accuracy across various tasks, including facial attribute classification, language modeling, and user embedding-based movie recommendations, while maintaining user privacy.\n\n# Strength And Weaknesses\nThe primary strengths of this paper include its groundbreaking approach to federated learning, which balances privacy and performance, and its substantial empirical results showing improved model accuracy and efficiency. The theoretical foundations provide confidence in the proposed methods, making them a valuable contribution to the field. However, potential weaknesses could include the need for more comprehensive evaluations across different datasets and real-world scenarios to fully establish the robustness of the proposed methodologies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the motivations and contributions of mixed FL. The quality of the presentation is high, with detailed explanations of the algorithms and their advantages. The novelty of combining centralized objectives with decentralized learning is significant and presents a fresh perspective on federated learning. However, reproducibility may be a concern if the experiments are not adequately detailed in terms of implementation specifics and dataset configurations.\n\n# Summary Of The Review\nOverall, this paper introduces a transformative approach in the realm of federated learning with Mixed FL, effectively addressing key challenges related to privacy and performance. The empirical results are promising, though further validation in diverse settings would strengthen the claims.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach to Mixed Federated Learning (MFL), which combines decentralized federated learning with centralized learning objectives to enhance model training while preserving data privacy. The authors propose a mixed loss function that incorporates both federated and centralized losses and provide theoretical convergence bounds for three algorithms: Parallel Training, 1-Way Gradient Transfer, and 2-Way Gradient Transfer. The findings indicate that MFL effectively mitigates distribution shift and reduces variance in gradient estimates, thus improving model generalization, particularly in non-IID settings.\n\n# Strength And Weaknesses\nThe paper's primary strengths lie in its robust theoretical framework and the introduction of bounded gradient dissimilarity (BGD) to characterize the convergence behavior in mixed FL scenarios. The convergence bounds derived for different algorithms and the insights on mitigating distribution shifts are notably valuable contributions. However, a significant weakness is the acknowledgment of loose theoretical bounds, which may not accurately reflect practical performance. Additionally, the assumptions regarding the smoothness and convexity of loss functions could limit the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its theoretical contributions clearly, making it accessible to readers familiar with federated learning concepts. The quality of the theoretical analysis is high, with a thorough exploration of convergence properties. The novelty of the approach, particularly in the context of mixed loss functions and the implications for client heterogeneity, is significant. However, reproducibility may be hampered due to the reliance on specific assumptions that may not hold in all practical scenarios.\n\n# Summary Of The Review\nOverall, the paper presents a compelling theoretical advancement in Mixed Federated Learning, highlighting its potential to improve model performance while addressing privacy concerns. While the contributions are substantial, the loose bounds and assumptions present limitations that may impact practical applications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel approach to Mixed Federated Learning (Mixed FL) by introducing three distinct algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER. Each algorithm is designed to enhance the efficiency of federated learning while maintaining data privacy, as client data remains on-device. The findings indicate that the proposed methods effectively optimize communication overhead and improve model performance by managing gradients from both centralized and federated sources, with a focus on specific hyperparameter settings that influence convergence rates.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its detailed algorithmic contributions, which address critical issues in federated learning such as privacy and communication efficiency. The implementation details are well-articulated, ensuring that the algorithms can be reproduced and tested by other researchers. However, a notable weakness is the limited exploration of broader theoretical implications or comparisons with existing federated learning methods. The focus on algorithm specifics may detract from a comprehensive understanding of how these methods fit into the larger landscape of federated learning research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clearly written, with a logical structure that guides the reader through the methodology and findings. The quality of the implementation details provided is commendable, particularly regarding hyperparameter settings and debugging methodologies. The novelty of the proposed algorithms is evident, particularly in their approach to managing gradients and communication overhead. The commitment to reproducibility, highlighted by the planned open-source code repository, further strengthens the paper's contributions.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of federated learning through the introduction of innovative algorithms that effectively address privacy and communication challenges. While the clarity and reproducibility are strong points, the paper could benefit from a broader discussion of its implications and a comparative analysis with existing methods.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces a concept called mixed federated learning (FL), which aims to integrate decentralized learning approaches with centralized data objectives. The authors propose three algorithms—PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER—claiming to achieve oracle-level accuracy while minimizing communication and computation overheads. However, the methodology and empirical findings seem to echo prior research without significant innovation or depth, particularly compared to established frameworks such as SCAFFOLD and Mime.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its structured presentation of mixed FL and a clear outline of the proposed algorithms. However, it suffers from several weaknesses, including a lack of depth in theoretical contributions and an overemphasis on the novelty of its claims, particularly regarding convergence analysis and communication savings. The complexity of the proposed algorithms may deter practical application, especially when simpler and more effective methods exist. The paper also fails to adequately address ethical implications associated with centralized data, which is vital in the context of FL.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly. However, the novelty of the proposed methods is questionable, as they do not significantly advance the existing body of knowledge in federated learning. Additionally, the lack of practical implementation details hinders reproducibility, which is a critical aspect for advancing research in this domain. The authors could improve clarity by contextualizing their contributions more effectively within the existing literature.\n\n# Summary Of The Review\nOverall, while the paper introduces the concept of mixed federated learning and proposes new algorithms, it does not provide substantial advancements over existing methods. The contributions are overshadowed by the complexity of the proposed solutions and their resemblance to prior works, leaving significant room for improvement in both theoretical contributions and practical applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"MIXED FEDERATED LEARNING: JOINT DECENTRALIZED AND CENTRALIZED LEARNING\" introduces a novel framework for federated learning that integrates both decentralized and centralized learning approaches. The authors propose a mixed loss function that balances these two methodologies, enabling improved performance on distributed datasets. The methodology includes the derivation of convergence bounds for various algorithmic settings, alongside empirical validation through extensive experiments. Key findings indicate that the mixed approach enhances model accuracy while reducing communication and computational overhead.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to combining decentralized and centralized learning, which addresses significant challenges in federated learning, such as distribution shift and communication efficiency. The theoretical contributions, including convergence bounds, are a notable addition to the literature. However, weaknesses include inconsistencies in notation and formatting, which detract from the clarity of the presentation. Additionally, while empirical results are promising, they would benefit from greater specificity regarding the performance metrics and a more comprehensive analysis of the findings in the context of existing literature.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is hampered by several formatting issues and inconsistent notation, particularly in equations and citations, which may confuse readers. The quality of the research is high, with well-founded theoretical contributions and relevant empirical experiments. In terms of novelty, the mixed approach to federated learning presents a significant advancement. Reproducibility is somewhat affected by the aforementioned clarity issues, as algorithm descriptions and experimental setups could be more explicitly detailed to facilitate replication.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to federated learning by merging decentralized and centralized techniques, yielding promising results. However, the effectiveness of the communication and computation reductions could be better substantiated, and the clarity of the presentation requires improvement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a study on mixed federated learning (FL), introducing three novel algorithms aimed at reducing communication and computation overhead in decentralized learning environments. The authors analyze convergence bounds for these algorithms and provide initial empirical evaluations on a limited set of tasks. However, the application of mixed FL is not extensively explored in diverse domains, which may limit the generalizability of their findings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in the introduction of new algorithms and their theoretical analysis, specifically regarding convergence bounds. However, the paper has notable weaknesses, including a lack of comparative analysis with existing FL methods, limited experimental scope, and insufficient exploration of practical implications related to scalability, biases, and ethical concerns. Additionally, the authors' discussions on potential extensions and future applications lack depth and actionable insights.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly, with a logical structure that conveys the authors' contributions. However, the novelty of the work is somewhat diminished by the absence of a comprehensive literature review and the limited empirical evaluation of the proposed algorithms. Reproducibility could be improved by providing more detailed methodologies and discussions on implementation challenges, particularly concerning computational costs and real-world applicability.\n\n# Summary Of The Review\nOverall, the paper presents interesting contributions to the field of mixed federated learning, but it falls short in terms of depth and breadth of analysis. It would benefit from a more extensive exploration of real-world applications and a clearer discussion of the implications of its findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces \"Mixed Federated Learning\" (Mixed FL), a novel framework that combines centralized and decentralized learning by incorporating a centralized loss term alongside the federated loss function. The authors propose three algorithms: PARALLEL TRAINING (PT), 1-WAY GRADIENT TRANSFER (1-W GT), and 2-WAY GRADIENT TRANSFER (2-W GT), and provide a thorough convergence analysis under various conditions (strongly convex, general convex, and non-convex settings). Empirical validation is conducted across diverse tasks, including facial attribute classification, language modeling, and movie recommendation, demonstrating significant performance improvements and resource efficiency.\n\n# Strength And Weaknesses\nThe paper's key strengths lie in its rigorous statistical framework and comprehensive experimental validation, which underscore the effectiveness of the proposed Mixed FL approach. The detailed convergence analysis and the significance testing of performance metrics enhance the robustness of the claims. However, a potential weakness is the limited exploration of client heterogeneity and the assumption of certain convexity conditions, which may not generalize across all federated learning scenarios. Additionally, while the paper suggests future research directions, it lacks a thorough investigation of the practical implications of implementing Mixed FL in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodologies, and findings. The statistical rigor in the loss function composition and the convergence bounds is commendable, making the results reproducible for practitioners. The novelty of integrating centralized and decentralized learning in federated contexts is significant, although the paper could benefit from clearer discussions on potential limitations and assumptions behind the proposed methods.\n\n# Summary Of The Review\nOverall, the paper provides a significant contribution to the field of federated learning by introducing Mixed FL, supported by a solid statistical foundation and empirical evidence. While it lays the groundwork for future research directions, the applicability of the findings to diverse federated learning environments requires further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a mixed federated learning (FL) framework that aims to combine decentralized client data with centralized datasets for improved model performance across various tasks such as facial attribute classification, language modeling, and movie recommendation. The methodology focuses on specific algorithms tailored to these tasks, claiming to enhance the learning process by leveraging both federated and centralized data. However, the findings suggest that while the proposed approach shows some improvement in performance, it raises concerns regarding the scalability, generalization, and privacy implications of integrating centralized data.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to combining federated and centralized data, which offers a potential avenue for improving model accuracy. However, several weaknesses limit the paper's contributions: it does not address the scalability of the proposed FL algorithms to larger datasets, nor does it explore the impact of heterogeneous client data distributions adequately. The assumptions of IID data may not reflect real-world scenarios, and the stated convergence bounds are considered loose, potentially misrepresenting practical performance. Furthermore, the paper overlooks significant aspects such as the computational resources needed for centralized training and the robustness against adversarial attacks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is moderate, with a structured presentation of its contributions, yet it lacks depth in discussing various limitations and implications. The quality of the methodology is acceptable, though the novel aspects are constrained by the narrow scope of experiments conducted. Reproducibility is hindered due to insufficient details regarding the experimental setup, particularly concerning the computational resources and specific implementation of the algorithms.\n\n# Summary Of The Review\nOverall, the paper offers a novel exploration into mixed federated learning, but its contributions are tempered by significant limitations in scalability, generalization, and privacy considerations. Future work should address these gaps to enhance the framework's applicability and robustness in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a framework for Mixed Federated Learning (FL), which aims to enhance model performance by integrating both decentralized and centralized data sources. The authors introduce three algorithms—PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER—and provide empirical results demonstrating their effectiveness in matching oracle accuracy across tasks such as smile classification and language modeling. However, the novelty of the proposed methods and their real-world applicability are called into question.\n\n# Strength And Weaknesses\nThe primary strengths of the paper lie in its attempt to combine centralized and decentralized data in the context of federated learning, which could theoretically lead to improved model outcomes. However, the weaknesses are pronounced: the contributions feel derivative rather than innovative, with the algorithms lacking depth and practical significance. Additionally, the experimental design relies on well-known tasks that do not push the boundaries of the field, and the comparisons with existing methods lack critical analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by excessive detail in familiar concepts and algorithms that do not add significant value. The quality of the writing suffers from a lack of focus on novel contributions and practical implications. Although the paper provides a step-by-step breakdown of its methodologies, the novelty is minimal, as it presents enhancements to existing methods without substantial innovation. The reproducibility of the experiments is not thoroughly addressed, as the practical applications and real-world challenges are not explored.\n\n# Summary Of The Review\nOverall, the paper fails to provide significant new insights into Mixed Federated Learning, instead reiterating established concepts and algorithms in a convoluted manner. While there are some empirical results presented, their relevance to real-world applications is questionable, leading to an impression of a lack of depth and critical analysis.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n2/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces a novel framework for mixed federated learning (FL), which integrates both decentralized and centralized learning paradigms to enhance model robustness and reduce biases in machine learning applications. The authors propose three key algorithms: PARALLEL TRAINING, 1-WAY GRADIENT TRANSFER, and 2-WAY GRADIENT TRANSFER, aimed at optimizing model performance while minimizing communication overhead. Key findings indicate significant reductions in communication costs and improvements in bias mitigation, with practical applications demonstrated in facial attribute classification and language modeling.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to combining centralized and decentralized learning, which aligns well with current research needs in hybrid data environments. The proposed algorithms show promise in reducing communication costs and addressing distribution shift and bias. However, there are notable weaknesses, including a reliance on IID data assumptions for theoretical contributions, which limits the applicability of the findings in non-IID contexts. Additionally, the methodological enhancements could benefit from exploring alternative optimization techniques and loss function dynamics.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clearly articulates its contributions, although some sections could benefit from additional explanations regarding algorithmic details and theoretical assumptions. The quality of the experiments appears robust, yet the reproducibility could be enhanced by providing more comprehensive descriptions of the implementation and datasets used. The novelty of integrating mixed FL is significant, but more exploration of non-IID scenarios and advanced optimization strategies would strengthen the paper's contributions.\n\n# Summary Of The Review\nOverall, the paper presents a promising framework for mixed federated learning that addresses important issues in machine learning. While it offers valuable insights and methodologies, there are opportunities for further refinement, especially in accommodating non-IID data distributions and optimizing algorithm performance.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel approach termed \"mixed federated learning\" (FL), which integrates both decentralized and centralized learning methodologies to improve model performance across various tasks. The authors demonstrate that mixed FL can achieve oracle-level accuracy in specific scenarios, effectively addressing distribution shifts. Experimental evaluations on three tasks—smile classification, language modeling, and movie recommendation—show significant improvements, including over 0.95 AUC in smile classification and reductions in communication overhead by 93.9% and computation by 99.9% in movie recommendation. The paper also analyzes the convergence characteristics of different algorithms used within mixed FL, providing insights into their performance and applicability.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its strong empirical results, which highlight the effectiveness of mixed FL in enhancing model accuracy and efficiency. The claim of achieving oracle-level performance on specific tasks is particularly noteworthy, as it suggests a high level of applicability in real-world scenarios. Furthermore, the substantial reductions in communication and computation overhead present a compelling case for the practical adoption of mixed FL. However, a potential weakness is the variability in performance across different tasks, particularly the lower accuracy in language modeling. This inconsistency may limit the generalizability of the findings and raise questions about the method's robustness in diverse applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The use of visual aids, such as figures depicting training progress and performance metrics, enhances the clarity of the results. The reporting of hyperparameters and dataset specifics contributes positively to reproducibility, allowing others to validate the performance claims. In terms of novelty, the integration of centralized and decentralized learning strategies is a fresh approach that could have significant implications for the field. However, the paper could benefit from a more comprehensive discussion on the limitations and potential challenges of implementing mixed FL in practice.\n\n# Summary Of The Review\nOverall, the paper presents a compelling argument for the efficacy of mixed federated learning, supported by thorough experimental evaluations that demonstrate significant improvements in accuracy and efficiency across various tasks. While the findings are impressive, the variability in performance across different tasks warrants further exploration to enhance generalizability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to mixed federated learning (FL), aiming to address the limitations of traditional FL frameworks. The authors introduce a new methodology that integrates multiple learning paradigms to enhance model accuracy while maintaining data privacy. Key findings indicate that this mixed approach significantly outperforms traditional FL techniques, particularly in scenarios with heterogeneous data distributions.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to federated learning, which combines various learning strategies to improve performance. The empirical results demonstrate a clear advantage over existing methods, contributing valuable insights to the field. However, the paper suffers from several weaknesses, including a lack of clarity in the abstract, inconsistent section headings, and complex sentence structures that may hinder comprehension. Additionally, some content is repetitive, which could be streamlined for better flow.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by dense language and complex explanations, particularly in the abstract and methodology sections. While the novel contributions are valuable, the presentation could benefit from clearer definitions of key terms and better integration of figures and tables with explanatory text. The reproducibility of the findings may be challenging due to the lack of detailed explanations surrounding the equations and proofs presented in the appendix.\n\n# Summary Of The Review\nOverall, the paper introduces a significant advancement in mixed federated learning, showcasing promising results. However, the writing quality and presentation need improvement to enhance clarity and accessibility for a broader audience.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4681269114795037,
    -1.7811788085108808,
    -1.7710168894702276,
    -1.6388144419908472,
    -1.9587436555290405,
    -1.8175647637229067,
    -1.5863059867322946,
    -1.7553793022978064,
    -1.8466804140626862,
    -1.6412253919154556,
    -1.6487884473296885,
    -1.3920702883182783,
    -1.7944622226484543,
    -1.5922508353241966,
    -1.7780159057877714,
    -1.781573953203213,
    -1.8417227071180815,
    -1.8872506297284348,
    -1.7595356825735957,
    -1.766411080700532,
    -1.881746620600864,
    -1.716533869291802,
    -1.7879784612554488,
    -1.8687226863789996,
    -1.8950694461089508,
    -1.9469917429594084,
    -1.8442735438292823,
    -1.7595371470356693,
    -1.7505533221082765
  ],
  "logp_cond": [
    [
      0.0,
      -2.2584476382189704,
      -2.256533989139989,
      -2.254556218589481,
      -2.263201027361274,
      -2.2503941362827957,
      -2.2700308658304658,
      -2.2862306629429647,
      -2.2850963869260905,
      -2.2548550155426836,
      -2.2463002684486764,
      -2.313586428063602,
      -2.2552762415765746,
      -2.2721352250536273,
      -2.2578591629477436,
      -2.2649120637172593,
      -2.2654268611568678,
      -2.288491839171669,
      -2.2613152996723813,
      -2.2634130062855604,
      -2.282993445362191,
      -2.2567408104659874,
      -2.2741848992743026,
      -2.269134059561045,
      -2.2434775223314802,
      -2.2590816170911014,
      -2.2626253778478533,
      -2.278299579066371,
      -2.298092752099493
    ],
    [
      -1.4785044511578425,
      0.0,
      -1.1661884231271964,
      -1.130249339981401,
      -1.2404796328998062,
      -1.2070139656258947,
      -1.3986147710978774,
      -1.2043138669922875,
      -1.15760892975626,
      -1.38439056065158,
      -1.173524857875472,
      -1.5457276478096944,
      -1.19795237867763,
      -1.223214656292302,
      -1.2173572903994294,
      -1.1749952992779482,
      -1.28698621147398,
      -1.1995626166088522,
      -1.3286736283816447,
      -1.3314960862919005,
      -1.217150917056816,
      -1.3905375645286773,
      -1.4453056622197915,
      -1.2695138997805677,
      -1.3852633614708183,
      -1.244099549844047,
      -1.3097983588766384,
      -1.2375306877976477,
      -1.4808548420867484
    ],
    [
      -1.5152625797770436,
      -1.2244912266820818,
      0.0,
      -1.241609009198879,
      -1.3399986833763227,
      -1.1903097116080485,
      -1.3876229108089304,
      -1.3339651489654003,
      -1.2431829783175394,
      -1.407334424915774,
      -1.2888765978459233,
      -1.5309320293682556,
      -1.342360347233502,
      -1.3127058704949188,
      -1.2893513011794495,
      -1.2845621048827167,
      -1.349994569434674,
      -1.2680265600395932,
      -1.3304320372876546,
      -1.3640695605240525,
      -1.316013490792243,
      -1.4729710441810495,
      -1.4406265625179715,
      -1.2667518078286781,
      -1.3457472215696618,
      -1.3446338071619568,
      -1.3712876336939503,
      -1.3093617813218075,
      -1.4758272741711986
    ],
    [
      -1.366817978425689,
      -1.05874729485482,
      -1.1183531351877511,
      0.0,
      -1.2014970090552084,
      -1.0812945364901656,
      -1.2663246527321588,
      -1.1336806037046532,
      -1.1088618533280894,
      -1.2870492197647452,
      -1.1130784894508798,
      -1.3944276344655917,
      -1.1636563802010473,
      -1.1448558043076513,
      -1.1768270369490177,
      -1.0531585095082368,
      -1.185443106743667,
      -1.1296806525691203,
      -1.2041370789397285,
      -1.2140177009798165,
      -1.1781279502084396,
      -1.3018648570405993,
      -1.326396106173115,
      -1.1231124704211883,
      -1.2986475373396427,
      -1.169382741684263,
      -1.1976280108269042,
      -1.2212028576621283,
      -1.355980773753545
    ],
    [
      -1.6559319815912918,
      -1.4068266865433816,
      -1.4427413473147686,
      -1.4404818712765193,
      0.0,
      -1.5014059201148304,
      -1.5381234903633592,
      -1.4963868553913557,
      -1.4602010370446272,
      -1.5865769954115694,
      -1.441167629556027,
      -1.6984268573428227,
      -1.4612774672635696,
      -1.4784677642891109,
      -1.4593760042518078,
      -1.455088198457199,
      -1.4364942705046042,
      -1.5022848546914096,
      -1.5071150407054779,
      -1.5218348982313974,
      -1.4953220232204036,
      -1.6073731627569745,
      -1.5840091092005395,
      -1.4589416957479922,
      -1.5844402735634173,
      -1.5392053633586038,
      -1.4757935096903099,
      -1.6053242042862164,
      -1.6750121708932908
    ],
    [
      -1.5454303072795474,
      -1.2883087385175864,
      -1.2525825985837709,
      -1.2351916603379711,
      -1.3998592731472883,
      0.0,
      -1.4321493306028312,
      -1.3530137245218377,
      -1.2928443906931866,
      -1.4755155187920936,
      -1.3129519906149316,
      -1.5760905572792692,
      -1.36104977131101,
      -1.3352248296541245,
      -1.3354695879401863,
      -1.284699677405127,
      -1.3893185179601721,
      -1.2473215107543756,
      -1.402164644765627,
      -1.4358149151865331,
      -1.348408598505868,
      -1.521045829232934,
      -1.5146373693380701,
      -1.3133699010301745,
      -1.437109105697264,
      -1.3873141636812347,
      -1.3952610906898495,
      -1.4214331391553656,
      -1.5551170577794642
    ],
    [
      -1.3681526572457563,
      -1.1878301851849267,
      -1.1544086547473302,
      -1.1861403571005908,
      -1.1845676751512202,
      -1.1125506189500032,
      0.0,
      -1.263825741869848,
      -1.2500602845389932,
      -1.3118873817160073,
      -1.2345130922569896,
      -1.3695336491635477,
      -1.223742559181321,
      -1.226041162343568,
      -1.2467270176030156,
      -1.2632412457671947,
      -1.2685023054116664,
      -1.2689199612707422,
      -1.2358708157530336,
      -1.2810054887257203,
      -1.2850545996871816,
      -1.3055460739456453,
      -1.33968402093342,
      -1.187041042473078,
      -1.300901656644495,
      -1.3136726984591216,
      -1.2975303980776938,
      -1.3088063110513197,
      -1.3973963014617186
    ],
    [
      -1.5141646349072007,
      -1.2664860193346084,
      -1.3021623263037467,
      -1.2182878409544042,
      -1.3231364632769946,
      -1.2482332499694488,
      -1.4050601976293662,
      0.0,
      -1.190711382404067,
      -1.4388704810967505,
      -1.3434542813872226,
      -1.5068971178601736,
      -1.2938633805626651,
      -1.2979071471109798,
      -1.3452653481526342,
      -1.2784148594571967,
      -1.318389565129951,
      -1.1706759177449888,
      -1.3155539301910648,
      -1.297825297106221,
      -1.2515058585322643,
      -1.4674843492677339,
      -1.4676572033918471,
      -1.3432451422592868,
      -1.477564806715707,
      -1.2701052814656182,
      -1.302827305813217,
      -1.397314297235436,
      -1.4325460993273618
    ],
    [
      -1.5869654254170065,
      -1.3028238978724067,
      -1.3218904682150578,
      -1.3026419986885989,
      -1.4009479729492966,
      -1.3461393634571008,
      -1.5129758338044428,
      -1.33752780768583,
      0.0,
      -1.543153686791724,
      -1.4091347960036986,
      -1.6434298687972724,
      -1.4035950906326045,
      -1.4066785983185264,
      -1.3901766443104004,
      -1.350422300903108,
      -1.420780291013733,
      -1.3084237647216879,
      -1.4085579799377466,
      -1.414656026954865,
      -1.3857494905918488,
      -1.575253395297069,
      -1.5402830158800704,
      -1.3957146815884731,
      -1.534923598552999,
      -1.3729893925572216,
      -1.38304271155535,
      -1.4102355487360703,
      -1.5695249970486798
    ],
    [
      -1.3221607980154444,
      -1.219983593465293,
      -1.1979263355556762,
      -1.201483764231696,
      -1.2216919579284864,
      -1.1988590614757657,
      -1.295133227125489,
      -1.2396186971085914,
      -1.238622390523612,
      0.0,
      -1.1745833413764968,
      -1.364994269193039,
      -1.2173029755081655,
      -1.2072058553665599,
      -1.2635372727610812,
      -1.2105938433400145,
      -1.162194074708577,
      -1.2937249406038371,
      -1.2642624867737273,
      -1.2480732494704139,
      -1.187857179161571,
      -1.250556101420717,
      -1.1949418016302653,
      -1.1845502146412281,
      -1.1855191821770488,
      -1.2701599059622917,
      -1.2528030225596631,
      -1.2369346436188327,
      -1.3056010295395197
    ],
    [
      -1.3440893354930945,
      -1.031109039585223,
      -1.0827910310555247,
      -1.046646919143575,
      -1.1460615311488644,
      -1.0408121791560498,
      -1.237677425015367,
      -1.1546876911578114,
      -1.0855715016961254,
      -1.2512299827025506,
      0.0,
      -1.4170170286918704,
      -1.1089246496893879,
      -1.0809177328903916,
      -1.0762366983884581,
      -1.1034821088108284,
      -1.107251293888734,
      -1.1149384134870612,
      -1.1522791096833596,
      -1.1715014925281144,
      -1.1141573149030888,
      -1.292200854919456,
      -1.3008855986379668,
      -1.058681197716249,
      -1.2085908404878611,
      -1.167690983857558,
      -1.1813156233796616,
      -1.2414036314660868,
      -1.3344958801699576
    ],
    [
      -1.1496519258261484,
      -1.1408190099315054,
      -1.142767978230311,
      -1.1521423099388028,
      -1.1252039513344871,
      -1.1189076699195177,
      -1.1406496441884297,
      -1.145266184341155,
      -1.142917005912511,
      -1.1467928174345907,
      -1.1490722896486039,
      0.0,
      -1.1447512976079377,
      -1.165379206606495,
      -1.148427222735625,
      -1.1691752665332515,
      -1.135256020282342,
      -1.1615874830685533,
      -1.1441102524815705,
      -1.1689534085145796,
      -1.1380623418935132,
      -1.1418609941303342,
      -1.1349552287320372,
      -1.1454449285991168,
      -1.1458858010965458,
      -1.193888799062698,
      -1.1583745091436137,
      -1.163796383758734,
      -1.1517534085362104
    ],
    [
      -1.5182824303281823,
      -1.2066218317082533,
      -1.2500386911178925,
      -1.2446650033485558,
      -1.3063197362048706,
      -1.2713052344050642,
      -1.415097943782859,
      -1.2835564481515644,
      -1.278004460759694,
      -1.4270215764082133,
      -1.2849843126778222,
      -1.529629348425119,
      0.0,
      -1.3439010286262931,
      -1.2782384097266373,
      -1.1927851841211348,
      -1.3130589761708815,
      -1.2876315605778048,
      -1.3703111960529926,
      -1.3128521267212552,
      -1.2506585633342553,
      -1.4177049416851264,
      -1.475124677612798,
      -1.2780339350415038,
      -1.4398934933926344,
      -1.305683268020577,
      -1.346632334997164,
      -1.3778989410977651,
      -1.5032106976949275
    ],
    [
      -1.3147427470478268,
      -1.0501203106341312,
      -1.0894450805273492,
      -1.0179389230822358,
      -1.1191823933510368,
      -1.0937644558951185,
      -1.219787586532106,
      -1.170491122913904,
      -1.1088207510339931,
      -1.2299272276055253,
      -1.0283687424159045,
      -1.3756380442168663,
      -1.1451839419796215,
      0.0,
      -1.0654581383077129,
      -1.136360115729,
      -1.1390131787484072,
      -1.086048188956206,
      -1.1727620570906732,
      -1.189450799240359,
      -1.163207228605579,
      -1.2398171578576875,
      -1.2193463060635887,
      -1.0828153468227533,
      -1.179116224867657,
      -1.1803234033213312,
      -1.1477095855195438,
      -1.149660862939555,
      -1.3209837553610648
    ],
    [
      -1.4533549812606765,
      -1.1193519174652038,
      -1.1031662377078908,
      -1.1887664232439399,
      -1.1621769707152554,
      -1.172168436371606,
      -1.307677304243038,
      -1.2914168723376145,
      -1.1081747875263193,
      -1.4045703632976831,
      -1.1725916967068086,
      -1.4904894501545414,
      -1.2690951132454205,
      -1.1652012474188238,
      0.0,
      -1.2333789361847092,
      -1.2327982068384027,
      -1.2247469017590944,
      -1.2755377428149928,
      -1.283808439772589,
      -1.249361860289058,
      -1.4166712216828088,
      -1.3601875568298947,
      -1.1563095124252267,
      -1.3577947493139868,
      -1.2362865408603303,
      -1.244358166729498,
      -1.3066832354407312,
      -1.4416191145574766
    ],
    [
      -1.4568824733394259,
      -1.2112960285454684,
      -1.2100256427480998,
      -1.1483896263736477,
      -1.3006486102834554,
      -1.2188531317590694,
      -1.3905778455396867,
      -1.258720425585398,
      -1.209299579752665,
      -1.4183545055600701,
      -1.2483910417930664,
      -1.5175245434233522,
      -1.1829777107031716,
      -1.299501814571715,
      -1.2722973171449012,
      0.0,
      -1.2945955306771122,
      -1.2174434698956225,
      -1.3650060383479115,
      -1.345723252384873,
      -1.2520031098686513,
      -1.4236059963520085,
      -1.4560580931260338,
      -1.2716241228437999,
      -1.4455348355964353,
      -1.297915210360013,
      -1.3377249687882031,
      -1.3572784752263996,
      -1.4690173608300887
    ],
    [
      -1.5680252693720096,
      -1.33106769461748,
      -1.339356413742951,
      -1.3150084604950245,
      -1.3370379309128293,
      -1.3261581511479281,
      -1.4676277708488237,
      -1.3989977872427433,
      -1.382633168941657,
      -1.4534398580442718,
      -1.325433690200308,
      -1.6077644935238038,
      -1.3670997185280749,
      -1.3869070874301668,
      -1.342243901147773,
      -1.352022252271214,
      0.0,
      -1.4320458727107999,
      -1.4265567265287447,
      -1.4378164095423747,
      -1.3610233305809298,
      -1.514785639501379,
      -1.4241634898448559,
      -1.3687536386031343,
      -1.4583330278796447,
      -1.4406110044880898,
      -1.4250717170653233,
      -1.4700183167421526,
      -1.583037958011896
    ],
    [
      -1.6028631026585136,
      -1.2885139583732128,
      -1.28440016405545,
      -1.2747544013327587,
      -1.402444206555036,
      -1.2227324051737083,
      -1.4587774573448709,
      -1.241330637739576,
      -1.2607555877904877,
      -1.5352326996836763,
      -1.3439016303746987,
      -1.612729483750806,
      -1.366476773717571,
      -1.328036175432893,
      -1.3506322829308015,
      -1.2608838868831307,
      -1.3889737450091721,
      0.0,
      -1.3475240407510776,
      -1.4171106240234908,
      -1.338526026512776,
      -1.5284387228986833,
      -1.5241358504710825,
      -1.3495036210537301,
      -1.4849541166544535,
      -1.306490196110332,
      -1.3491256293754532,
      -1.40547968670364,
      -1.5246311701059554
    ],
    [
      -1.4640840869799399,
      -1.3214906007779628,
      -1.2935605632086693,
      -1.2925732591052654,
      -1.3053501361960869,
      -1.2949065394652632,
      -1.369824107366972,
      -1.3043151998913982,
      -1.2671284151367672,
      -1.3851951629806978,
      -1.2580053981836359,
      -1.5246136669854697,
      -1.3143984767604808,
      -1.34487099175478,
      -1.3352444162449082,
      -1.3194718161133892,
      -1.3358044555154303,
      -1.2778198294391299,
      0.0,
      -1.2500308458369762,
      -1.3204606251676092,
      -1.4159943446974776,
      -1.4224768639643788,
      -1.2896111277700437,
      -1.3734709113509413,
      -1.2859259824135798,
      -1.2821139655446132,
      -1.415871958013122,
      -1.4075607982510234
    ],
    [
      -1.4925350673162883,
      -1.298638965769873,
      -1.3217004920055924,
      -1.28185766044306,
      -1.3149953165476722,
      -1.3374009777972389,
      -1.3541122178446705,
      -1.2856598196912155,
      -1.28460902692744,
      -1.3985935404983796,
      -1.3026326462135212,
      -1.5148920865871247,
      -1.2864426578598027,
      -1.3237504936983557,
      -1.3087773965781921,
      -1.3055167082391248,
      -1.3159329251516259,
      -1.269192623223551,
      -1.2476920603923127,
      0.0,
      -1.292228769121399,
      -1.4360897539827266,
      -1.4374202808381136,
      -1.3061063866061837,
      -1.447738409662402,
      -1.3006620395048407,
      -1.2909696615567112,
      -1.425151747213859,
      -1.4216646363848824
    ],
    [
      -1.6173632927408903,
      -1.3187410731005962,
      -1.3235171093610236,
      -1.3217523996403722,
      -1.4233286570065773,
      -1.3286079111550226,
      -1.5048592881875973,
      -1.3586224748346056,
      -1.3450105528682346,
      -1.5238513456537641,
      -1.370062459406517,
      -1.641251647794597,
      -1.3898167489918458,
      -1.421011833414523,
      -1.3761371550837291,
      -1.3285484649983537,
      -1.4068369738774316,
      -1.3479637503775443,
      -1.4544097253848878,
      -1.4268301739884457,
      0.0,
      -1.5549800471099258,
      -1.505705828907167,
      -1.4005084103016072,
      -1.5298182207786943,
      -1.4177334463331641,
      -1.4210504203157681,
      -1.4693022224491887,
      -1.5894089176558095
    ],
    [
      -1.37164798671176,
      -1.299827444283377,
      -1.2770308086710296,
      -1.2914277161338326,
      -1.271918779182871,
      -1.3362255925418138,
      -1.3299984843702273,
      -1.3062229895743398,
      -1.2616427056057427,
      -1.275130382466082,
      -1.2872479938988317,
      -1.4012307515298437,
      -1.2712724822959869,
      -1.2731836175608424,
      -1.2903993977972505,
      -1.3047434659469146,
      -1.2635866363330068,
      -1.3379888705313365,
      -1.2942991625968108,
      -1.3533711688039516,
      -1.2695387024701135,
      0.0,
      -1.3089821152025416,
      -1.2777273025212164,
      -1.3206914062117354,
      -1.3348037407214417,
      -1.2954582962621042,
      -1.298356299468989,
      -1.3520138215689605
    ],
    [
      -1.437626116196305,
      -1.3287641044569019,
      -1.3307859304479033,
      -1.3222469519073154,
      -1.2967444921232658,
      -1.3347625443470181,
      -1.422528803081253,
      -1.3918737868702267,
      -1.3389756238784567,
      -1.3518715679242446,
      -1.3411341281354705,
      -1.4952515376209918,
      -1.3983563541083583,
      -1.3577238559875457,
      -1.3375358008328748,
      -1.3396623793458236,
      -1.3021072947426275,
      -1.4062849729041211,
      -1.395194183317377,
      -1.3661804703762928,
      -1.2946893357441063,
      -1.3914259962249433,
      0.0,
      -1.3556836493980755,
      -1.328707935978219,
      -1.3769354478401936,
      -1.343532595849755,
      -1.3373585804619221,
      -1.4323829906274674
    ],
    [
      -1.5631723967007525,
      -1.2888343622315284,
      -1.2471130604062624,
      -1.2730865334135453,
      -1.3722931817089232,
      -1.252537737270147,
      -1.4718716377963363,
      -1.4202234275633752,
      -1.3313699403436003,
      -1.4910006031396184,
      -1.2512036380158325,
      -1.6286935219179544,
      -1.33883555798405,
      -1.338904084203039,
      -1.3380468873806848,
      -1.371758793007221,
      -1.4176043391553885,
      -1.360326215169681,
      -1.4089520204088655,
      -1.3957255282897774,
      -1.4153465656957274,
      -1.496976072354029,
      -1.5268237603520678,
      0.0,
      -1.4487211499522668,
      -1.43408111898847,
      -1.387109461154484,
      -1.441368043500101,
      -1.5825441987691151
    ],
    [
      -1.5977200735157469,
      -1.4832070807233693,
      -1.4403131841811787,
      -1.4831935898198843,
      -1.4964379054402526,
      -1.4507073173090028,
      -1.578694334763232,
      -1.6073325310958224,
      -1.552714307706452,
      -1.4571118998923274,
      -1.447977979984864,
      -1.6463764118125361,
      -1.546613292368099,
      -1.4693780430747847,
      -1.5099480678443395,
      -1.5227823098083921,
      -1.533075068500266,
      -1.5305339142197838,
      -1.5190873372065015,
      -1.5844504224405762,
      -1.5250894983339076,
      -1.5807404639704927,
      -1.5153922595228615,
      -1.455322017180886,
      0.0,
      -1.5519558797476403,
      -1.5235634335808261,
      -1.512532379961722,
      -1.606167663699687
    ],
    [
      -1.6756120901117295,
      -1.4017488038590542,
      -1.4251202867308337,
      -1.4246952896967073,
      -1.499452208131014,
      -1.4451496399204764,
      -1.539015803730207,
      -1.4337767418139784,
      -1.3876437667580956,
      -1.615634919769665,
      -1.4690357515088364,
      -1.7287943975299696,
      -1.4506750293800896,
      -1.4806651622842877,
      -1.4466465085829794,
      -1.4692689853386083,
      -1.505246168517444,
      -1.4223592945359373,
      -1.4650882060012822,
      -1.4372088949740314,
      -1.4333644892873516,
      -1.6150775642541368,
      -1.5914484647938465,
      -1.462745496140694,
      -1.5779868357177074,
      0.0,
      -1.4251250114695153,
      -1.5279029991698911,
      -1.5996492299468554
    ],
    [
      -1.5392778883023124,
      -1.3714312359353458,
      -1.3717297423629309,
      -1.3544642921936465,
      -1.3187098590023008,
      -1.3333163255530134,
      -1.4577467026441135,
      -1.3670872108177148,
      -1.313186586425389,
      -1.486364124766842,
      -1.3797800496439425,
      -1.5966415987381035,
      -1.3891098102790174,
      -1.355354848038657,
      -1.392205404169942,
      -1.3917810093376393,
      -1.3704859169537875,
      -1.2877457118731352,
      -1.3342377417443543,
      -1.38312599792792,
      -1.3931945271582686,
      -1.5353799029558257,
      -1.5084692072875592,
      -1.3544327578621196,
      -1.4416618627030833,
      -1.3512623748884158,
      0.0,
      -1.4874782548924252,
      -1.513425737744181
    ],
    [
      -1.4543805065993105,
      -1.239333921093286,
      -1.2793192440066854,
      -1.3240579839392557,
      -1.3877038531286785,
      -1.3191631945825728,
      -1.458895202560133,
      -1.3570030321133963,
      -1.2722660896932199,
      -1.4044966042462363,
      -1.3698299706426316,
      -1.5241420706827307,
      -1.3431151510224844,
      -1.3477109949934452,
      -1.3598642622414314,
      -1.333005572283276,
      -1.3951139625411082,
      -1.3079831396003365,
      -1.4286629273241365,
      -1.4586546468363975,
      -1.354754480279219,
      -1.430424645175342,
      -1.421734037576193,
      -1.3739965294994996,
      -1.3791071546772244,
      -1.3953499747086013,
      -1.4256315478628492,
      0.0,
      -1.4699928326426266
    ],
    [
      -1.3975391069303378,
      -1.3252687592900847,
      -1.3037112097726593,
      -1.312562543552719,
      -1.3211077400692293,
      -1.3323131799294716,
      -1.3201219199257663,
      -1.2810714469924283,
      -1.2603622162730763,
      -1.2925215173852929,
      -1.3286283291393588,
      -1.4088143680885192,
      -1.3057826631766312,
      -1.324609693027756,
      -1.3242480863730526,
      -1.3302722651984389,
      -1.2994715336741487,
      -1.2723569406602862,
      -1.2581691973654474,
      -1.2621177245788808,
      -1.3204002934004293,
      -1.270149964066319,
      -1.3004721579156127,
      -1.3235733018698967,
      -1.3044871141777097,
      -1.2598662512348684,
      -1.238350913765191,
      -1.313474545337992,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.20967927326053326,
      0.21159292233951454,
      0.2135706928900225,
      0.20492588411822954,
      0.21773277519670797,
      0.19809604564903793,
      0.18189624853653896,
      0.18303052455341318,
      0.21327189593682006,
      0.22182664303082733,
      0.15454048341590187,
      0.2128506699029291,
      0.1959916864258764,
      0.2102677485317601,
      0.2032148477622444,
      0.20270005032263594,
      0.1796350723078346,
      0.20681161180712238,
      0.20471390519394328,
      0.18513346611731274,
      0.2113861010135163,
      0.19394201220520113,
      0.1989928519184585,
      0.22464938914802346,
      0.20904529438840225,
      0.20550153363165036,
      0.18982733241313277,
      0.17003415938001076
    ],
    [
      0.3026743573530384,
      0.0,
      0.6149903853836844,
      0.6509294685294797,
      0.5406991756110746,
      0.5741648428849861,
      0.3825640374130035,
      0.5768649415185934,
      0.6235698787546209,
      0.3967882478593008,
      0.6076539506354088,
      0.23545116070118643,
      0.5832264298332508,
      0.5579641522185788,
      0.5638215181114514,
      0.6061835092329326,
      0.49419259703690077,
      0.5816161919020286,
      0.45250518012923613,
      0.4496827222189803,
      0.5640278914540648,
      0.39064124398220357,
      0.33587314629108933,
      0.5116649087303131,
      0.39591544704006254,
      0.5370792586668338,
      0.4713804496342424,
      0.5436481207132331,
      0.3003239664241324
    ],
    [
      0.255754309693184,
      0.5465256627881458,
      0.0,
      0.5294078802713487,
      0.43101820609390495,
      0.5807071778621791,
      0.3833939786612972,
      0.4370517405048273,
      0.5278339111526882,
      0.36368246455445363,
      0.4821402916243043,
      0.24008486010197205,
      0.42865654223672567,
      0.4583110189753088,
      0.4816655882907781,
      0.4864547845875109,
      0.4210223200355536,
      0.5029903294306344,
      0.44058485218257304,
      0.40694732894617514,
      0.45500339867798467,
      0.2980458452891781,
      0.3303903269522561,
      0.5042650816415495,
      0.42526966790056586,
      0.4263830823082708,
      0.39972925577627727,
      0.46165510814842015,
      0.295189615299029
    ],
    [
      0.27199646356515816,
      0.5800671471360273,
      0.5204613068030961,
      0.0,
      0.43731743293563885,
      0.5575199055006816,
      0.3724897892586885,
      0.5051338382861941,
      0.5299525886627579,
      0.35176522222610207,
      0.5257359525399674,
      0.24438680752525554,
      0.4751580617897999,
      0.4939586376831959,
      0.46198740504182956,
      0.5856559324826105,
      0.4533713352471802,
      0.5091337894217269,
      0.4346773630511187,
      0.4247967410110307,
      0.4606864917824076,
      0.33694958495024796,
      0.31241833581773215,
      0.5157019715696589,
      0.3401669046512046,
      0.4694317003065842,
      0.44118643116394307,
      0.4176115843287189,
      0.28283366823730227
    ],
    [
      0.30281167393774866,
      0.5519169689856589,
      0.516002308214272,
      0.5182617842525212,
      0.0,
      0.4573377354142101,
      0.42062016516568135,
      0.46235680013768476,
      0.4985426184844133,
      0.3721666601174711,
      0.5175760259730136,
      0.2603167981862178,
      0.49746618826547095,
      0.48027589123992964,
      0.4993676512772327,
      0.5036554570718415,
      0.5222493850244363,
      0.4564588008376309,
      0.45162861482356265,
      0.4369087572976431,
      0.46342163230863687,
      0.351370492772066,
      0.374734546328501,
      0.49980195978104835,
      0.3743033819656232,
      0.4195382921704367,
      0.48295014583873064,
      0.35341945124282415,
      0.2837314846357497
    ],
    [
      0.2721344564433592,
      0.5292560252053202,
      0.5649821651391358,
      0.5823731033849355,
      0.41770549057561834,
      0.0,
      0.3854154331200754,
      0.464551039201069,
      0.5247203730297201,
      0.342049244930813,
      0.504612773107975,
      0.24147420644363748,
      0.45651499241189675,
      0.4823399340687822,
      0.4820951757827203,
      0.5328650863177797,
      0.4282462457627345,
      0.5702432529685311,
      0.4154001189572796,
      0.3817498485363735,
      0.4691561652170386,
      0.2965189344899726,
      0.3029273943848365,
      0.5041948626927322,
      0.38045565802564263,
      0.4302506000416719,
      0.42230367303305716,
      0.39613162456754103,
      0.26244770594344247
    ],
    [
      0.21815332948653832,
      0.39847580154736795,
      0.4318973319849644,
      0.40016562963170377,
      0.4017383115810744,
      0.47375536778229144,
      0.0,
      0.32248024486244664,
      0.3362457021933014,
      0.27441860501628734,
      0.35179289447530504,
      0.21677233756874692,
      0.3625634275509737,
      0.36026482438872653,
      0.339578969129279,
      0.32306474096509996,
      0.3178036813206282,
      0.31738602546155237,
      0.350435170979261,
      0.30530049800657433,
      0.301251387045113,
      0.28075991278664936,
      0.24662196579887463,
      0.39926494425921666,
      0.28540433008779953,
      0.272633288273173,
      0.28877558865460085,
      0.2774996756809749,
      0.18890968527057606
    ],
    [
      0.2412146673906057,
      0.488893282963198,
      0.45321697599405963,
      0.5370914613434021,
      0.43224283902081173,
      0.5071460523283575,
      0.3503191046684402,
      0.0,
      0.5646679198937394,
      0.31650882120105583,
      0.4119250209105838,
      0.2484821844376328,
      0.4615159217351412,
      0.4574721551868266,
      0.41011395414517215,
      0.4769644428406097,
      0.4369897371678553,
      0.5847033845528176,
      0.43982537210674155,
      0.4575540051915854,
      0.5038734437655421,
      0.2878949530300725,
      0.28772209890595923,
      0.41213416003851955,
      0.27781449558209936,
      0.4852740208321882,
      0.45255199648458944,
      0.3580650050623704,
      0.3228332029704446
    ],
    [
      0.25971498864567977,
      0.5438565161902795,
      0.5247899458476284,
      0.5440384153740874,
      0.4457324411133896,
      0.5005410506055854,
      0.33370458025824346,
      0.5091526063768563,
      0.0,
      0.3035267272709623,
      0.4375456180589876,
      0.20325054526541386,
      0.4430853234300818,
      0.44000181574415986,
      0.4565037697522858,
      0.49625811315957824,
      0.4259001230489532,
      0.5382566493409984,
      0.4381224341249397,
      0.4320243871078213,
      0.4609309234708374,
      0.27142701876561715,
      0.3063973981826158,
      0.4509657324742131,
      0.31175681550968726,
      0.4736910215054646,
      0.46363770250733616,
      0.43644486532661597,
      0.2771554170140065
    ],
    [
      0.3190645939000112,
      0.42124179845016263,
      0.4432990563597794,
      0.4397416276837596,
      0.4195334339869692,
      0.44236633043968987,
      0.3460921647899666,
      0.4016066948068642,
      0.4026030013918436,
      0.0,
      0.4666420505389588,
      0.27623112272241657,
      0.4239224164072901,
      0.4340195365488957,
      0.3776881191543744,
      0.43063154857544106,
      0.4790313172068785,
      0.34750045131161844,
      0.37696290514172826,
      0.3931521424450417,
      0.4533682127538845,
      0.39066929049473864,
      0.44628359028519027,
      0.45667517727422746,
      0.4557062097384068,
      0.3710654859531639,
      0.38842236935579244,
      0.4042907482966229,
      0.33562436237593585
    ],
    [
      0.30469911183659404,
      0.6176794077444656,
      0.5659974162741639,
      0.6021415281861136,
      0.5027269161808241,
      0.6079762681736387,
      0.4111110223143215,
      0.4941007561718771,
      0.5632169456335632,
      0.39755846462713795,
      0.0,
      0.23177141863781814,
      0.5398637976403007,
      0.5678707144392969,
      0.5725517489412304,
      0.5453063385188601,
      0.5415371534409545,
      0.5338500338426273,
      0.496509337646329,
      0.4772869548015741,
      0.5346311324265998,
      0.35658759241023263,
      0.3479028486917217,
      0.5901072496134396,
      0.4401976068418274,
      0.48109746347213056,
      0.467472823950027,
      0.40738481586360176,
      0.314292567159731
    ],
    [
      0.2424183624921299,
      0.25125127838677286,
      0.24930231008796722,
      0.23992797837947544,
      0.26686633698379114,
      0.27316261839876055,
      0.25142064412984855,
      0.24680410397712338,
      0.24915328240576717,
      0.24527747088368757,
      0.24299799866967442,
      0.0,
      0.24731899071034058,
      0.22669108171178332,
      0.24364306558265336,
      0.22289502178502674,
      0.2568142680359362,
      0.23048280524972498,
      0.24796003583670778,
      0.22311687980369865,
      0.2540079464247651,
      0.25020929418794413,
      0.25711505958624103,
      0.24662535971916144,
      0.24618448722173247,
      0.1981814892555802,
      0.23369577917466455,
      0.22827390455954433,
      0.24031687978206784
    ],
    [
      0.276179792320272,
      0.587840390940201,
      0.5444235315305619,
      0.5497972192998986,
      0.4881424864435837,
      0.5231569882433902,
      0.3793642788655953,
      0.5109057744968899,
      0.5164577618887602,
      0.3674406462402411,
      0.5094779099706321,
      0.26483287422333524,
      0.0,
      0.4505611940221612,
      0.5162238129218171,
      0.6016770385273196,
      0.4814032464775728,
      0.5068306620706495,
      0.4241510265954618,
      0.48161009592719917,
      0.543803659314199,
      0.37675728096332795,
      0.31933754503565637,
      0.5164282876069506,
      0.35456872925581995,
      0.48877895462787735,
      0.4478298876512903,
      0.4165632815506892,
      0.2912515249535268
    ],
    [
      0.27750808827636986,
      0.5421305246900654,
      0.5028057547968474,
      0.5743119122419609,
      0.4730684419731599,
      0.49848637942907814,
      0.3724632487920907,
      0.42175971241029275,
      0.4834300842902035,
      0.36232360771867134,
      0.5638820929082922,
      0.21661279110733034,
      0.44706689334457517,
      0.0,
      0.5267926970164838,
      0.45589071959519667,
      0.4532376565757894,
      0.5062026463679907,
      0.4194887782335235,
      0.40280003608383774,
      0.42904360671861763,
      0.3524336774665091,
      0.3729045292606079,
      0.5094354885014434,
      0.41313461045653965,
      0.4119274320028654,
      0.4445412498046528,
      0.4425899723846416,
      0.2712670799631318
    ],
    [
      0.32466092452709483,
      0.6586639883225676,
      0.6748496680798806,
      0.5892494825438315,
      0.615838935072516,
      0.6058474694161653,
      0.4703386015447333,
      0.48659903345015687,
      0.6698411182614521,
      0.3734455424900882,
      0.6054242090809627,
      0.28752645563323,
      0.5089207925423509,
      0.6128146583689476,
      0.0,
      0.5446369696030622,
      0.5452176989493687,
      0.553269004028677,
      0.5024781629727786,
      0.4942074660151823,
      0.5286540454987134,
      0.36134468410496257,
      0.4178283489578767,
      0.6217063933625446,
      0.42022115647378455,
      0.5417293649274411,
      0.5336577390582733,
      0.4713326703470402,
      0.33639679123029476
    ],
    [
      0.3246914798637872,
      0.5702779246577447,
      0.5715483104551133,
      0.6331843268295654,
      0.48092534291975775,
      0.5627208214441437,
      0.39099610766352644,
      0.522853527617815,
      0.572274373450548,
      0.36321944764314296,
      0.5331829114101467,
      0.2640494097798609,
      0.5985962425000415,
      0.4820721386314981,
      0.5092766360583119,
      0.0,
      0.4869784225261009,
      0.5641304833075906,
      0.41656791485530165,
      0.4358507008183401,
      0.5295708433345618,
      0.35796795685120464,
      0.32551586007717925,
      0.5099498303594132,
      0.33603911760677785,
      0.4836587428432002,
      0.44384898441501,
      0.42429547797681355,
      0.3125565923731244
    ],
    [
      0.2736974377460719,
      0.5106550125006015,
      0.5023662933751305,
      0.526714246623057,
      0.5046847762052522,
      0.5155645559701534,
      0.3740949362692578,
      0.44272491987533824,
      0.4590895381764244,
      0.3882828490738097,
      0.5162890169177734,
      0.2339582135942777,
      0.4746229885900066,
      0.45481561968791473,
      0.4994788059703086,
      0.48970045484686753,
      0.0,
      0.40967683440728164,
      0.41516598058933685,
      0.40390629757570684,
      0.48069937653715167,
      0.3269370676167025,
      0.41755921727322565,
      0.47296906851494724,
      0.3833896792384368,
      0.40111170262999174,
      0.4166509900527582,
      0.37170439037592895,
      0.25868474910618544
    ],
    [
      0.2843875270699212,
      0.598736671355222,
      0.6028504656729847,
      0.6124962283956761,
      0.48480642317339884,
      0.6645182245547265,
      0.42847317238356397,
      0.6459199919888587,
      0.6264950419379471,
      0.35201793004475856,
      0.5433489993537362,
      0.2745211459776289,
      0.5207738560108639,
      0.5592144542955417,
      0.5366183467976333,
      0.6263667428453041,
      0.4982768847192627,
      0.0,
      0.5397265889773573,
      0.4701400057049441,
      0.5487246032156587,
      0.3588119068297515,
      0.36311477925735236,
      0.5377470086747047,
      0.4022965130739813,
      0.5807604336181029,
      0.5381250003529816,
      0.48177094302479473,
      0.36261945962247943
    ],
    [
      0.29545159559365586,
      0.43804508179563295,
      0.4659751193649264,
      0.4669624234683303,
      0.45418554637750885,
      0.4646291431083325,
      0.3897115752066238,
      0.4552204826821975,
      0.4924072674368285,
      0.37434051959289794,
      0.5015302843899598,
      0.23492201558812598,
      0.4451372058131149,
      0.41466469081881563,
      0.4242912663286875,
      0.44006386646020657,
      0.42373122705816546,
      0.48171585313446585,
      0.0,
      0.5095048367366195,
      0.4390750574059865,
      0.34354133787611807,
      0.33705881860921694,
      0.469924554803552,
      0.38606477122265437,
      0.4736097001600159,
      0.4774217170289825,
      0.34366372456047367,
      0.3519748843225723
    ],
    [
      0.2738760133842437,
      0.46777211493065907,
      0.4447105886949396,
      0.48455342025747195,
      0.4514157641528598,
      0.4290101029032931,
      0.4122988628558615,
      0.48075126100931653,
      0.48180205377309204,
      0.3678175402021524,
      0.46377843448701084,
      0.2515189941134073,
      0.47996842284072927,
      0.44266058700217625,
      0.45763368412233985,
      0.4608943724614072,
      0.45047815554890613,
      0.4972184574769809,
      0.5187190203082193,
      0.0,
      0.47418231157913304,
      0.33032132671780534,
      0.3289907998624184,
      0.46030469409434827,
      0.31867267103813,
      0.46574904119569127,
      0.47544141914382076,
      0.3412593334866729,
      0.3447464443156496
    ],
    [
      0.26438332785997365,
      0.5630055475002678,
      0.5582295112398403,
      0.5599942209604918,
      0.45841796359428666,
      0.5531387094458413,
      0.37688733241326666,
      0.5231241457662583,
      0.5367360677326294,
      0.35789527494709983,
      0.511684161194347,
      0.24049497280626686,
      0.49192987160901813,
      0.460734787186341,
      0.5056094655171348,
      0.5531981556025103,
      0.4749096467234324,
      0.5337828702233196,
      0.4273368952159762,
      0.45491644661241826,
      0.0,
      0.3267665734909382,
      0.376040791693697,
      0.48123821029925673,
      0.3519283998221696,
      0.46401317426769983,
      0.46069620028509584,
      0.4124443981516752,
      0.29233770294505446
    ],
    [
      0.34488588258004205,
      0.4167064250084249,
      0.4395030606207724,
      0.4251061531579694,
      0.44461509010893097,
      0.38030827674998813,
      0.3865353849215747,
      0.41031087971746216,
      0.4548911636860593,
      0.44140348682571995,
      0.42928587539297025,
      0.3153031177619583,
      0.4452613869958151,
      0.44335025173095954,
      0.4261344714945514,
      0.41179040334488737,
      0.45294723295879513,
      0.37854499876046543,
      0.4222347066949912,
      0.3631627004878504,
      0.4469951668216885,
      0.0,
      0.40755175408926037,
      0.4388065667705856,
      0.39584246308006654,
      0.38173012857036026,
      0.4210755730296978,
      0.41817756982281296,
      0.3645200477228414
    ],
    [
      0.3503523450591437,
      0.45921435679854694,
      0.4571925308075455,
      0.46573150934813334,
      0.491233969132183,
      0.45321591690843066,
      0.3654496581741957,
      0.3961046743852221,
      0.4490028373769921,
      0.4361068933312042,
      0.4468443331199783,
      0.29272692363445696,
      0.38962210714709045,
      0.43025460526790305,
      0.45044266042257397,
      0.44831608190962524,
      0.48587116651282125,
      0.38169348835132766,
      0.3927842779380717,
      0.42179799087915604,
      0.4932891255113425,
      0.39655246503050545,
      0.0,
      0.4322948118573733,
      0.4592705252772298,
      0.4110430134152552,
      0.44444586540569375,
      0.45061988079352666,
      0.35559547062798136
    ],
    [
      0.30555028967824716,
      0.5798883241474713,
      0.6216096259727373,
      0.5956361529654544,
      0.4964295046700764,
      0.6161849491088527,
      0.3968510485826633,
      0.44849925881562447,
      0.5373527460353993,
      0.3777220832393813,
      0.6175190483631672,
      0.24002916446104527,
      0.5298871283949496,
      0.5298186021759606,
      0.5306757989983149,
      0.4969638933717786,
      0.4511183472236111,
      0.5083964712093185,
      0.45977066597013416,
      0.47299715808922227,
      0.45337612068327227,
      0.37174661402497056,
      0.34189892602693184,
      0.0,
      0.4200015364267329,
      0.4346415673905297,
      0.48161322522451555,
      0.42735464287889857,
      0.2861784876098845
    ],
    [
      0.29734937259320393,
      0.4118623653855815,
      0.4547562619277721,
      0.41187585628906653,
      0.3986315406686982,
      0.44436212879994796,
      0.3163751113457187,
      0.28773691501312837,
      0.34235513840249876,
      0.4379575462166234,
      0.4470914661240868,
      0.24869303429641465,
      0.3484561537408517,
      0.4256914030341661,
      0.38512137826461124,
      0.37228713630055865,
      0.3619943776086847,
      0.364535531889167,
      0.3759821089024493,
      0.31061902366837457,
      0.36997994777504317,
      0.31432898213845806,
      0.3796771865860893,
      0.43974742892806473,
      0.0,
      0.3431135663613105,
      0.37150601252812465,
      0.3825370661472287,
      0.2889017824092637
    ],
    [
      0.27137965284767884,
      0.5452429391003542,
      0.5218714562285747,
      0.522296453262701,
      0.44753953482839437,
      0.501842103038932,
      0.4079759392292013,
      0.51321500114543,
      0.5593479762013127,
      0.33135682318974347,
      0.477955991450572,
      0.2181973454294388,
      0.4963167135793187,
      0.46632658067512067,
      0.500345234376429,
      0.4777227576208001,
      0.4417455744419643,
      0.524632448423471,
      0.48190353695812616,
      0.509782847985377,
      0.5136272536720567,
      0.3319141787052715,
      0.35554327816556186,
      0.4842462468187143,
      0.369004907241701,
      0.0,
      0.521866731489893,
      0.4190887437895172,
      0.3473425130125529
    ],
    [
      0.30499565552696994,
      0.47284230789393655,
      0.47254380146635144,
      0.4898092516356358,
      0.5255636848269816,
      0.5109572182762689,
      0.3865268411851688,
      0.47718633301156754,
      0.5310869574038932,
      0.35790941906244034,
      0.4644934941853398,
      0.2476319450911788,
      0.45516373355026496,
      0.4889186957906253,
      0.45206813965934023,
      0.45249253449164306,
      0.4737876268754948,
      0.5565278319561471,
      0.510035802084928,
      0.4611475459013623,
      0.4510790166710137,
      0.3088936408734566,
      0.33580433654172315,
      0.48984078596716274,
      0.40261168112619905,
      0.4930111689408665,
      0.0,
      0.3567952889368571,
      0.3308478060851012
    ],
    [
      0.3051566404363588,
      0.5202032259423832,
      0.48021790302898393,
      0.43547916309641366,
      0.3718332939069908,
      0.4403739524530965,
      0.3006419444755364,
      0.402534114922273,
      0.48727105734244947,
      0.355040542789433,
      0.38970717639303776,
      0.23539507635293866,
      0.41642199601318497,
      0.41182615204222417,
      0.3996728847942379,
      0.4265315747523932,
      0.3644231844945611,
      0.4515540074353328,
      0.33087421971153286,
      0.3008825001992719,
      0.40478266675645025,
      0.32911250186032737,
      0.3378031094594762,
      0.38554061753616975,
      0.3804299923584449,
      0.364187172327068,
      0.33390559917282014,
      0.0,
      0.28954431439304273
    ],
    [
      0.35301421517793874,
      0.4252845628181918,
      0.4468421123356172,
      0.43799077855555746,
      0.42944558203904726,
      0.4182401421788049,
      0.4304314021825102,
      0.46948187511584827,
      0.4901911058352002,
      0.45803180472298366,
      0.4219249929689177,
      0.3417389540197573,
      0.44477065893164536,
      0.4259436290805205,
      0.42630523573522394,
      0.42028105690983764,
      0.45108178843412783,
      0.47819638144799037,
      0.4923841247428291,
      0.4884355975293957,
      0.4301530287078472,
      0.48040335804195755,
      0.45008116419266386,
      0.4269800202383798,
      0.44606620793056684,
      0.4906870708734081,
      0.5122024083430856,
      0.4370787767702846,
      0.0
    ]
  ],
  "row_avgs": [
    0.20053075433562859,
    0.4945034707237111,
    0.4285773082138249,
    0.4397340140348522,
    0.4403282739910806,
    0.43011127084941764,
    0.32297905970675356,
    0.41660752427680076,
    0.4188718909097259,
    0.40512270565677333,
    0.48262247983860723,
    0.24328981190794893,
    0.45484985292731706,
    0.43384070401467023,
    0.5127393348165706,
    0.4645285688667721,
    0.4258998221192821,
    0.5015592624617549,
    0.42124373453373815,
    0.42344806756995484,
    0.44899552946808247,
    0.4109635792466965,
    0.425252481586554,
    0.46534683506211233,
    0.36905449369089954,
    0.44855824153243606,
    0.43787759089349704,
    0.38040523515880115,
    0.4437024298521478
  ],
  "col_avgs": [
    0.2899341626887507,
    0.49668624737306377,
    0.49495814714381586,
    0.5013870846020737,
    0.44704586958196263,
    0.4919631145220191,
    0.3721658004114176,
    0.44631881842156274,
    0.4890560369781256,
    0.35997592078408874,
    0.4719239149027121,
    0.24860408424560182,
    0.4508949611970722,
    0.451958196372937,
    0.4516419725792417,
    0.46757012792653335,
    0.43846644467068946,
    0.4660415984684964,
    0.4243223859834863,
    0.4133926935990964,
    0.451518854344533,
    0.33536763631302524,
    0.3431799703042556,
    0.46312693835879104,
    0.3713345484050682,
    0.42869368683305653,
    0.4300870125782717,
    0.3954117284714806,
    0.2985163701851827
  ],
  "combined_avgs": [
    0.24523245851218967,
    0.4955948590483874,
    0.46176772767882035,
    0.4705605493184629,
    0.4436870717865216,
    0.46103719268571836,
    0.34757243005908556,
    0.4314631713491818,
    0.45396396394392574,
    0.38254931322043106,
    0.47727319737065965,
    0.24594694807677536,
    0.4528724070621946,
    0.4428994501938036,
    0.48219065369790615,
    0.4660493483966527,
    0.4321831333949858,
    0.48380043046512566,
    0.42278306025861223,
    0.41842038058452563,
    0.45025719190630775,
    0.37316560777986085,
    0.38421622594540483,
    0.4642368867104517,
    0.3701945210479839,
    0.4386259641827463,
    0.43398230173588437,
    0.38790848181514087,
    0.3711094000186652
  ],
  "gppm": [
    637.6613906562046,
    565.3666996078933,
    565.1175700737792,
    563.3371843958176,
    585.9877674386408,
    562.762212244236,
    621.4635940620219,
    588.4479264214688,
    566.7825571470456,
    630.9162402777202,
    579.4521768609977,
    677.3221394104569,
    584.9178612870727,
    586.9592752140234,
    588.6318014783183,
    579.1057333621272,
    591.7163556021821,
    579.1282648417008,
    599.825796231789,
    604.4269423154049,
    586.3171396592517,
    639.44364299797,
    638.5005955229743,
    579.9361717230197,
    620.7015263392439,
    595.4403474358476,
    596.7819991075796,
    608.6324434791779,
    658.3289684976316
  ],
  "gppm_normalized": [
    1.5043534981119924,
    1.3686836913335523,
    1.371925301236163,
    1.369473455258248,
    1.4171599016122225,
    1.3678621329343275,
    1.5054429831637675,
    1.4324185828010054,
    1.3749497513494038,
    1.528924132801608,
    1.4031546902177503,
    1.6570035412519553,
    1.4158803741758725,
    1.420728566161116,
    1.4307684628971682,
    1.4044643621241355,
    1.4354542759658613,
    1.4067051636032524,
    1.4537964088276083,
    1.4645546201392587,
    1.4245851443505348,
    1.5495463084152814,
    1.5462307195014267,
    1.4108942212095634,
    1.5049865782514924,
    1.4434921262340779,
    1.445241133973078,
    1.4763067951573188,
    1.595136811431609
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534,
    448,
    422,
    431,
    410,
    445,
    437,
    448,
    356,
    418,
    400,
    408,
    567,
    423,
    428,
    389,
    424,
    419,
    417,
    416,
    470,
    368,
    384,
    403,
    423,
    394,
    366,
    382,
    442,
    412,
    532,
    444,
    478,
    502,
    430,
    533,
    483,
    440,
    480,
    379,
    427,
    503,
    458,
    484,
    371,
    423,
    429,
    400,
    409,
    416,
    406,
    403,
    340,
    427,
    412,
    405,
    403,
    478,
    337
  ],
  "response_lengths": [
    2528,
    2631,
    2773,
    2894,
    2509,
    3022,
    2830,
    2480,
    2680,
    2242,
    2490,
    2890,
    2684,
    2893,
    2169,
    2461,
    2374,
    2290,
    2331,
    2354,
    2278,
    2416,
    1967,
    2435,
    2425,
    2211,
    2358,
    2819,
    1936
  ]
}