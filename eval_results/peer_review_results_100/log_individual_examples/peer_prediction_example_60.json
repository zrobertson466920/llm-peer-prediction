{
  "example_idx": 60,
  "reference": "Published as a conference paper at ICLR 2023\n\nSAMPLE-EFFICIENT REINFORCEMENT LEARNING BY BREAKING THE REPLAY RATIO BARRIER\n\nPierluca D’Oro∗ Mila, Universit ́e de Montr ́eal\n\nMax Schwarzer∗ Google Brain Mila, Universit ́e de Montr ́eal\n\nEvgenii Nikishin Mila, Universit ́e de Montr ́eal\n\nPierre-Luc Bacon Mila, Universit ́e de Montr ́eal\n\nMarc G. Bellemare Google Brain, Mila\n\nAaron Courville Mila, Universit ́e de Montr ́eal\n\nABSTRACT\n\nIncreasing the replay ratio, the number of updates of an agent’s parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.\n\n1\n\nINTRODUCTION\n\nIn many real world scenarios, each interaction with the environment comes at a cost, and it is desirable for deep reinforcement learning (RL) algorithms to learn with a minimal amount of samples (Franc ̧ois-Lavet et al., 2018). This can be naturally achieved if an algorithm is able to leverage more computational resources during training to improve its performance. Given the online nature of deep RL, there is a peculiar way to aim at having such behavior: to train the agent for longer, given a dataset of experiences, before interacting with the environment again.\n\n(a) DeepMind Control Suite (DMC15-500k)\n\n(b) Atari 100k\n\nFigure 1: Scaling behavior of SAC and SR-SAC in the DeepMind Control Suite (DMC15-500k) benchmark, and of SPR and SR-SPR in the Atari 100k benchmark (5 seeds for point for SAC and SR-SAC, at least 20 seeds for point for SPR and SR-SPR, 95% bootstrapped C.I.).\n\n∗Equal contribution. Correspondance to {pierluca.doro, schwarzm}@mila.quebec.\n\n1\n\n1248163264128Replay Ratio200400600800IQMSR-SACSAC124816Replay Ratio0.20.40.6IQMSR-SPRSPRPublished as a conference paper at ICLR 2023\n\nA method based on this idea can be said to be scaling the replay ratio, the number of updates of an agent’s parameters for each environment interaction. Despite generally providing limited benefit when applied to standard baselines (Fedus et al., 2020; Kumar et al., 2021), replay ratio scaling has been shown to bring performance improvements to well-tuned algorithms. Recent approaches were able to achieve better sample efficiency by increasing it to higher values, up to 8 for discrete control (Kielak, 2019) or 20 for continuous control (Chen et al., 2021; Smith et al., 2022).\n\nIn this paper, we show that it is possible, with minimal but careful modifications to model-free algorithms mostly based on parameter resets (Ash & Adams, 2020; Nikishin et al., 2022), to reach new levels of replay ratio scaling and push the sample efficiency limits of deep RL. Both in continuous control, with SAC in DeepMind Control Suite (Haarnoja et al., 2018; Tassa et al., 2018), and discrete control, with SPR in Atari 100k (Schwarzer et al., 2021a; Kaiser et al., 2020), we break the replay ratio barrier, unlocking a training regime in which orders of magnitude of additional agent updates can be used to increase the performance of an algorithm for a given budget of interactions with the environment. By doing so, we obtain better aggregated scores than strong baselines, with a general blueprint to improve sample efficiency of potentially any off-policy deep RL algorithm.\n\nTo understand how this can be feasible, it is useful to reflect on one of the most common patterns observed in the development of deep RL algorithms (Mnih et al., 2015b). With a few exceptions, researchers typically ground their methods on the well-established dynamic programming mathematical machinery, combining it with optimization strategies common in deep learning. However, the RL setting is inherently different from the one in which most deep learning architectures and optimization methods were developed. In deep RL, neural networks have to deal with dynamic datasets, whose composition changes over the course of training; their training actively determines the value of future inputs, but also the value of future targets. We argue that the recently identified tendency of neural networks to lose their ability to learn and generalize from new information during training (Chaudhry et al., 2018; Ash & Adams, 2020; Berariu et al., 2021; Igl et al., 2021; Dohare et al., 2022; Lyle et al., 2022a;b; Nikishin et al., 2022), against which most RL methods deploy no countermeasures, has been the main roadblock in achieving better sample efficiency through replay ratio scaling.\n\nAfter presenting and evaluating our algorithmic solution leading to better replay ratio scaling, we discuss some of the aspects of thinking about deep RL algorithms under the lens of this paradigm. We show some examples of algorithm design decisions important, or not important, for effective replay ratio scaling to be possible, with particular attention to the role of online interaction. Then, we visualize in an explicit way the data-computations tradeoff implied by this approach and, after having shown the potential of replay ratio scaling, we discuss its inherent limits.\n\n2 RELATED WORK\n\nLoss of Ability to Learn and Generalize in Neural Networks A growing body of evidence suggests that artificial neural networks lose their ability to learn and generalize during training. The phenomenon is not clearly visible when learning with a static dataset on a fixed task, but it starts appearing when the data distribution changes. In the continual learning setting, an alleviation of the problem by partially resetting the network parameters already provides a consistent improvement (Ash & Adams, 2020). Berariu et al. (2021) provides an in-depth study of how this phenomenon happens, including how many training updates are required for the performance of a network on future tasks to be unrecoverably damaged. The phenomenon becomes even more prominent in deep RL, where it has been identified in multiple settings. In the context of on-policy algorithms, it has been investigated as a consequence of transient non-stationarity and mitigated via self-distillation (Igl et al., 2021); in off-policy RL, it has been studied under the name of capacity loss (Lyle et al., 2022a), counteracted by the use of auxiliary tasks; in the sparse reward setting, it has been mitigated by post-training policy distillation (Lyle et al., 2022b). To address what they call loss of plasticity, Dohare et al. (2022) proposes a variation of backpropagation compatible with continual learning, also applying it to the continual RL context. In this paper, we primarily leverage a periodic hard resetting method (Zhou et al., 2022), as investigated in Nikishin et al. (2022) to address the primacy bias phenomenon. Our work demonstrates that addressing this phenomenon allows for increased sample efficiency by scaling the replay ratio to much higher values than other model-free methods. We report in Appendix A a more precise summary and glossary of the different related definitions from previous work.\n\n2\n\nPublished as a conference paper at ICLR 2023\n\nScaling in Deep and Reinforcement Learning The topic of understanding and exploiting the scaling behavior of a deep learning algorithm’s performance with respect to the amount of resources used for training has recently gained attention. Hestness et al. (2017) pioneered the idea of empirically studying and predicting performance when increasing a model’s size, and subsequent work investigated scaling with respect to both model and dataset size, as well as training time (Kaplan et al., 2020; Bahri et al., 2021; Djolonga et al., 2021). Recent work in language modeling has also highlighted the importance of having high-quality data and the right training setup for efficient scaling to be possible (Hoffmann et al., 2022). In RL, scaling with respect to model size has been investigated in the offline setting for decision transformers (Lee et al., 2022) and with respect to planning-time in model-based RL (Hamrick et al., 2021). For what concerns replay ratio scaling, moderately increasing the replay ratio for standard baselines has been shown to be a competitive data-efficient baseline for both discrete and continuous control when compared to model-based RL methods (Holland et al., 2018; Van Hasselt et al., 2019; Kielak, 2019; D’Oro & Ja ́skowski, 2020), despite clear limitations (Kumar et al., 2021). Recent approaches in continuous control leveraged high replay ratios as a strategy to improve sample efficiency through the use of ensembles of value functions (Chen et al., 2021; Hiraoka et al., 2022; Wu et al., 2022) or normalization strategies (Smith et al., 2022); we argue that explicitly alleviating the progressive loss of ability to learn and generalize pushes the replay ratio scaling capabilities much further than those techniques can achieve.\n\n3 EFFECTIVE REPLAY RATIO SCALING WITH RESETS\n\nMost off-policy deep RL algorithms make use of a replay buffer (Lin, 1992) for storing transitions encountered over (a window of) an agent’s lifespan. At a fixed frequency, such methods sample a batch of transitions from the buffer, update the parameters of the agent by following the gradient of some loss function, and let the agent interact again with the environment before adding new experience to the buffer. The number of agent updates per environment step is usually called replay ratio1 (Wang et al., 2016; Fedus et al., 2020), and most standard algorithms are trained with a value around 1 (Mnih et al., 2015a; Haarnoja et al., 2018). It is natural to view increasing the replay ratio beyond these values as a way to improve sample efficiency. For ease of discussion, we now explicitly state and give a name to this idea, which has been an object of interest in previous studies (Van Hasselt et al., 2019; Kumar et al., 2021).\n\nReplay Ratio Scaling\n\nChange in an agent’s performance caused by doing more updates for a fixed number of environment interactions.\n\nThis definition does not have any positive connotation per se; any deep RL algorithm will have a certain replay ratio scaling behavior, and a desirable property for an algorithm is to have particularly favorable replay ratio scaling, so that its performance can improve by increasing the replay ratio.\n\nIn contrast to other performance scaling properties analyzed for deep learning algorithms (Kaplan et al., 2020), replay ratio scaling is intertwined with the online RL paradigm: if the agent has a significantly better data-collection policy due to more training, the next collected sample will be potentially different with respect to the one collected if doing less training before the interaction; by this virtue, also future learning will be directly impacted by the presence of different data in the replay buffer. In other words, this type of scaling can only be understood by considering the interaction of an agent with an environment: training more on a small dataset of interactions, without any further collection of data, will eventually lead to challenges associated to off-policy learning (Ostrovski et al., 2021); but training more while the data is collected can drastically change the stream of incoming data and the overall learning dynamics.\n\nGiven its appeal, what are the limiting factors to increasing the replay ratio? We argue that the main factor inhibiting effective replay ratio scaling in existing deep RL algorithms has been the progressive loss of the ability to learn and generalize in neural networks (Dohare et al., 2022; Lyle et al., 2022b; Nikishin et al., 2022). It has been shown that this property hinders a neural network’s performance under task switches (Ash & Adams, 2020; Berariu et al., 2021) and, from the perspective\n\n1Related quantities are also known as update-to-data (UTD) ratio (Chen et al., 2021; Smith et al., 2022).\n\n3\n\nPublished as a conference paper at ICLR 2023\n\nDMC15-500k\n\nMethod\n\nIQM\n\nMedian\n\nMean\n\nSR-SAC 740 (642, 818) 511 (440, 577) REDQ 391 (334, 448) SAC 392 (334, 445) DDPG\n\n667 (573, 742) 493 (442, 544) 424 (376, 468) 410 (364, 454)\n\n658 (589, 722) 494 (452, 534) 424 (386, 461) 408 (371, 442)\n\nDMC15-1M\n\nMethod\n\nIQM\n\nMedian\n\nMean\n\nSR-SAC 805 (726, 867) 586 (514, 649) REDQ 535 (467, 597) SAC 514 (450, 572) DDPG\n\n729 (628, 790) 546 (490, 596) 525 (471, 567) 492 (440, 540)\n\n710 (643, 775) 539 (498, 576) 519 (480, 557) 489 (450, 526)\n\nFigure 2 & Table 1: Performance of SR-SAC and of standard baselines on the DMC15 benchmark. (5 seeds for SR-SAC, 20 for all other algorithms, 95% bootstrapped C.I.).\n\nof a neural network employed by the agent, what is deep RL if not a long sequence of related but distinct tasks (Dabney et al., 2021)?\n\nRecent studies showed that, even under smooth task changes, the more training has been done on a previous task, the worse the performance will eventually be in a new task (Ash & Adams, 2020; Berariu et al., 2021). Since higher replay ratio correspond to an increased amount of training, this gives a natural explanation to the limit in increasing it. The ability to learn and generalize can, however, be restored. For instance, Nikishin et al. (2022) periodically reset the network’s parameters, with a frequency that is fixed with respect to the number of environment steps. In this work, we argue that the key to surprisingly effective replay ratio scaling is a periodic restoration of the ability to learn and generalize of the network, via partial (Ash & Adams, 2020) or total (Nikishin et al., 2022) resets of its parameters, with a reset frequency that only depends on the number of updates and thus implicitly also on the replay ratio. This means the more an algorithm updates its neural networks, the more frequent the restoration of its ability to learn and generalize will be, leading to better performance, as we now show in practice.\n\n4 REPLAY RATIO SCALING DRASTICALLY IMPROVES SAMPLE EFFICIENCY\n\nWe apply two different reset strategies to two standard continuous control and discrete control algorithms and study their replay ratio scaling behavior. We consider Soft Actor-Critic (SAC) (Haarnoja et al., 2018), which optimizes an actor and a critic by maximizing policy entropy alongside the environment’s reward, and SPR (Schwarzer et al., 2021a), a model-free DQN-based reinforcement learning algorithm that augments a sample-efficient variant of Rainbow (Van Hasselt et al., 2019) with a model-based latent dynamics prediction objective designed to improve representation learning in the low-data regime. The two curves in Figure 1 show that it is possible, with the same algorithm, to almost double the performance for the same number of environment steps, by just varying the replay ratio. We call the modified versions of these two algorithms Scaled-by-Resetting SAC (SR-SAC) and Scaled-by-Resetting SPR (SR-SPR). In the rest of this section, we are going to describe the precise the details of the reset strategies that we employ for the two algorithms, as well as the benchmarks to which they are applied, by describing our decisions first in continuous control and then in discrete control. For evaluation and comparisons, we follow the protocol suggested by Agarwal et al. (2021).\n\n4.1 CONTINUOUS CONTROL\n\nThe DMC15 Benchmark To appropriately compare the performance of different algorithms, we consider a benchmark based on 15 environments from DeepMind Control Suite (Tassa et al., 2018). Our selection of tasks, reported in Table 6, is a set for which discussing sample efficiency is sensible (i.e., neither immediately solvable nor unsolvable by common deep RL algorithms). For ease of comparison, we specialize the benchmark to DMC15-500k, in which 5 × 105 interactions with the environment are allowed, and DMC15-1M, in which 106 interactions are allowed.\n\n4\n\n0.00.20.40.60.81.0Environment Steps (×106)0200400600800IQMSACREDQSR-SACDDPGPublished as a conference paper at ICLR 2023\n\nReset Strategy We adapt the approach of Nikishin et al. (2022), and completely reset all the agent parameters every 2.56 × 106 of its updates. This lets us avoid individually specifying the moments at which resets should happen for different replay ratios. In terms of environment steps, resets will just occur more often at higher replay ratios. For instance, for replay ratio 128 (128x higher than what typically used by SAC), a reset occurs once every 20000 steps of interaction with the environment.\n\nResults In Figure 2, we compare a version of SR-SAC that uses a replay ratio of 128 to standard deep RL baselines. This also includes the recently proposed REDQ (Chen et al., 2021), which obtained state-of-the-art sample efficiency by using a replay ratio of 20. At any budget of interactions with the environment, SR-SAC compares favorably with REDQ, despite being a simpler algorithm. SR-SAC establishes a new state-of-the-art result for model-free continuous control. Following (Agarwal et al., 2021) we focus on interquartile mean (IQM) performance, defined as the 25% trimmed mean performance over all runs on all considered tasks, and report 95% bootstrap confidence intervals.\n\n4.2 ATARI 100K\n\nReset Strategy We follow Nikishin et al. (2022) in performing one reset every 40,000 updates; at replay ratio 16, the highest considered, this corresponds to a reset every 2,500 environment steps, or roughly once every three minutes of interaction. However, Nikishin et al. (2022) only reset a subset of the agent’s parameters when training on the ALE, leaving the agent’s convolutional encoder untouched by resets. While this leaves the encoder vulnerable to plasticity loss, fully resetting the encoder is impractical, as Nikishin et al. (2022) observe. As an intermediate solution, we apply soft resets, using a variant of Shrink and Perturb (Ash & Adams, 2020) in which encoder parameters are interpolated between their previous value and a random re-initialized parameter vector on each reset: θt = αθt−1 + (1 − α)φ, φ ∼ initializer. This formulation is different from that used by (Ash & Adams, 2020) but allows easy interpolation between completely resetting a layer and leaving it unchanged; we use α = 0.8 by default. We examine the impact of this decision in Section 5.2.\n\nTarget Networks By default, SPR does not employ a separate target network, unlike traditional DQNs (Mnih et al., 2015a). However, we find that this leads replay ratio scaling to stop improving performance at relatively low replay ratios, which we hypothesize is due to fundamental variance in optimization limiting the accuracy to which the value function may be estimated. To alleviate it, we directly adopt the target strategy employed by SR-SAC, with an exponential moving average (EMA) target network with coefficient τ = 0.005, which we find allows beneficial replay ratio scaling out to at least replay ratio 16. Moreover, following (Ghavamzadeh et al., 2011), SR-SPR also uses its target network for action selection. We elaborate on this design decision in Section 5.2.\n\nResults Figure 3 shows performance profiles of SR-SPR at various replay ratios, demonstrating that replay ratio scaling consistently improves performance up to at least replay ratio 16. We also compare a version of SR-SPR that uses replay ratio 16 to standard baselines (DrQ, DER, Kostrikov\n\nAtari 100k\n\nMethod\n\nIQM\n\nMedian\n\nMean\n\nSR-SPR 0.632 (0.60, 0.66) 0.501 (0.44, 0.56) IRIS 0.380 (0.36, 0.39) SPR DrQ(ε) 0.280 (0.27, 0.29) 0.183 (0.18, 0.19) DER\n\n0.685 (0.60, 0.77) 0.289 (0.25, 0.41) 0.433 (0.38, 0.48) 0.304 (0.28, 0.33) 0.191 (0.18, 0.21)\n\n1.272 (1.18, 1.37) 1.046 (0.96, 1.13) 0.578 (0.56, 0.60) 0.465 (0.46, 0.48) 0.351 (0.34, 0.36)\n\nFigure 3 & Table 2: Performance profiles (left, higher is better) of SR-SPR at various replay ratios, and 95% C.I.s of SR-SPR: 16 and of standard baselines on Atari 100k (right, 20 seeds for SR-SPR and SPR, 5 seeds for IRIS, 100 seeds for all other algorithms as taken from Agarwal et al. (2021))\n\n5\n\n0.00.20.40.60.81.0Human Normalized Score ()0.000.250.500.751.00Fraction of runs with score >SR-SPR: 16SR-SPR: 8SR-SPR: 4SR-SPR: 2SR-SPR: 1SPR0.400.480.560.64IQMSR-SPRIRISSPR0.300.450.600.75Median0.751.001.25MeanPublished as a conference paper at ICLR 2023\n\nFigure 4: Scaling behavior of SAC, SR-SAC and its tandem and iterated offline variations in the DMC15 benchmark. Each individual line shows performance at a given number of environment steps, denoted by color, across different numbers of agent updates. Each point in a line is obtained by measuring performance with a different replay ratio for that number of environment steps. Each line is computed over 5 seeds.\n\net al., 2022; Van Hasselt et al., 2019) and recent work (IRIS, Micheli et al., 2022) in table 2. SR-SPR establishes a new state-of-the-art for model-free control on Atari 100k, and rivals prior work that has aggressively pretrained on additional data (Liu & Abbeel, 2021; Schwarzer et al., 2021b). We present full results and per-game scores for SR-SPR in table 4, and show training curves in fig. 15. We report IQM performance, as well as plotting a performance profile (Agarwal et al., 2021), which visualizes the full distribution of performance across all runs2 and demonstrates that increasing SR-SPR’s replay ratio comprehensively improves performance.\n\n5 ALGORITHM DESIGN IN LIGHT OF REPLAY RATIO SCALING\n\n5.1 ANALYZING THE IMPORTANCE OF ONLINE INTERACTION\n\nWhen training with high replay ratios and short reset intervals, the training regime an agent is subjected to begins to resemble offline RL; the agent is primarily learning from data collected by policies unrelated to its own, with only a small amount of online data available to correct its policy. Given many classical analyses from offline RL (Levine et al., 2020), it is perhaps surprising that an agent trained in a pseudo-offline setting with no explicit regularization towards conservatism (e.g., Kumar et al., 2020) can learn successfully. What is then the role of the incoming stream of interactions? To gain some understanding, in this section we attack the problem from different angles and study the scaling behavior of variants of SR-SAC. We consider different data collection patterns and how interleaving them with agent optimization changes the training dynamics. The appendix also presents a comparisons with NFQI (Riedmiller, 2005) and with the online use of an offline RL algorithm.\n\n5.1.1\n\nITERATED OFFLINE SETTING\n\nChanging the replay ratio in a deep RL algorithm can be seen as a specific way of increasing the proportion of offline training an agent is subject to. Specifically, the agent’s parameters are updated a number of times exactly equal to the replay ratio before a new sample is collected. This implies a uniform distribution of the number of offline updates across time steps. Is this an important variable for determining the replay ratio scaling behavior of an algorithm?\n\nTo answer this question, we resort to what we call iterated offline RL (Matsushima et al., 2021; Riedmiller et al., 2021), which alternates between purely offline updates and data collection. In this setting, a certain value of replay ratio is not distributed uniformly during the course of the interactions with the environment. Instead, the agent is not updated during data collection, and an amount of updates equal to the one that would be due in that data collection time frame in virtue of the replay ratio is applied completely offline, right after each reset.\n\nAs visible in Figure 4, the iterated offline paradigm has a different replay ratio scaling behavior. Applying a very large number of updates with a fixed dataset, with an algorithm such as SAC, incurs serious risk of generating a degenerate policy, not able to outperform the previous one. As exemplified in Figure 6, in the absence of any mechanism for stopping this natural degeneration to happen, this circle is broken only when enough data is collected. Collecting enough data in this sense is possible\n\n2Broadly speaking, a transposed and clipped plot of the cumulative distribution function of performance.\n\n6\n\n1051061071080200400600800IQMSAC105106107108SR-SAC105106107108Tandem SR-SAC105106107108Iterated Offline SR-SAC8.0×1056.1×1054.2×1052.2×1052.5×104Environment stepsNumber of agent updatesPublished as a conference paper at ICLR 2023\n\nFigure 6: Examples of behaviors of SR-SAC and its tandem and iterated offline variations on four environments from DMC15. (5 runs, ± std).\n\nfor easy tasks such as hopper-stand and walker-run, with a cost in sample efficiency, or impossible on hard tasks such as humanoid-stand and quadruped-walk. This is unfortunate: the iterated offline RL paradigm can be quite useful in practical settings, in which the agent is allowed to only collected batches of data without any update (perhaps for safety reasons); however, current backbone algorithms (such as SAC) are not currently compatible with such a setting, that thus leads to favorable replay scaling only when closer to the online setting. This explains the sudden increase in the curve of Figure 4, when the number of agent updates, and consequently the reset frequency, becomes large enough. An interesting question, left for future work, is whether this behavior could change if combined with conservative algorithms created for the offline RL setting.\n\n5.1.2 TANDEM SETTING\n\nWith high replay ratios, an agent’s training begins to resemble offline RL: although the agent still has the possibility to interact with the environment, it is very infrequent relative to the amount of training. Thus, an agent after a reset has a small stream of interactions collected by the agent itself, while the vast majority of its data was collected by potentially unrelated agents. How important, then, is this small stream of online interaction? To answer this question, we leverage the tandem setting, as presented in Ostrovski et al. (2021). Two copies of the same agent, identical apart from the initialization, are created. With the same algorithm (SR-SAC in this case), they are trained on the replay buffer collected by the active agent. The passive agent thus never directly interacts with environment, and cannot collect data to correct its own misconceptions about the environment.\n\nAs shown in Figure 4, the behavior of Tandem SR-SAC offers an alternative perspective on the importance of online interactions: despite the performance of the algorithm being hurt, the overall replay ratio scaling capabilities remain similar. We can look at the performance of the passive agent to understand what the exact effect of online interactions is on training. As evident in the environments from Figure 6, especially in hopper-stand and quadruped-walk, there is a qualitative difference between the behavior of an active agent (blue curve) and a passive agent (green curve): right after a reset, with the initial high replay ratio training, the performance of both agents is greatly improved; after a few thousands steps, training remains stable for the active agent but causes performance collapse in the passive agent. This experiment thus demonstrates the power of having online interactions as an implicit regularization mechanism.\n\nFor the design of future replay ratio-scalable algorithms, one should keep in mind that it is indeed possible to scale an algorithm potentially affected by extreme off-policyness; however, online data collection slows down performance collapse when training aggressively, as shown in both the iterated offline and the tandem experiments.\n\nFigure 5: Learning curves (top) and evaluation performance (bottom) at replay ratio 16 for SPR and SR-SPR with and without offline updates after each reset.\n\n5.1.3 ALTERNATIVE COMBINATIONS OF OFFLINE AND ONLINE UPDATES\n\nThe iterated offline setting can be seen as the extreme in which all of the updates are done offline, compared to the even distribution used in the online setting. What if we use an intermediate strategy?\n\n7\n\n0.000.050.100.150.20Environment Steps (×106)02505007501000Episode Returnhopper-stand0.000.050.100.150.20Environment Steps (×106)0100200Episode Returnhumanoid-stand0.000.050.100.150.20Environment Steps (×106)02505007501000Episode Returnquadruped-walk0.000.050.100.150.20Environment Steps (×106)0200400600800Episode Returnwalker-runSR-SAC, RR=128SR-SAC, RR=128SAC, RR=1SR-SAC, RR=128SAC, RR=1Tandem SR-SAC, RR=128SR-SAC, RR=128SAC, RR=1Tandem SR-SAC, RR=128Iterated Offline SR-SAC, RR=12804000080000Step0.00.20.40.60.8Training IQMSR-SPR: 16 + OfflineSR-SPR: 16SPR: 160.300.450.60IQMSR-SPR: 16SR-SPR: 16 + OfflineSPR: 16Published as a conference paper at ICLR 2023\n\nFor SR-SPR, we find that directly mixing offline and online RL by performing half the updates allotted to each interval immediately after each reset can actually improve performance by some metrics, such as training return (see Figure 5 upper), by mitigating the performance drop otherwise experienced after each reset. Although we find that this has essentially no impact on final evaluation performance (Figure 5 lower), it may allow SR-SPR to be used when cumulative regret is important.\n\n5.2 WHAT IS REQUIRED FOR REPLAY RATIO SCALING IN DISCRETE CONTROL?\n\nAlthough replay ratio scaling is relatively straightforward for SR-SAC, achieving robust replay ratio scaling for SR-SPR requires more complex design decisions due to its shorter training period and more complex function approximation. As a result, unlike SR-SAC, SR-SPR contains additional modifications from the variant of SPR used by Nikishin et al. (2022). We study the impact of these design decisions on scaling behavior and report results in Figure 7.\n\nInspired by the findings of Berariu et al. (2021) that plasticity loss is concentrated in the final layers of the network but affects all layers, we apply Shrink and Perturb (SP) to the encoder; this is responsible for roughly a constant increase of IQM 0.04 past replay ratio 4. We note however that applying Shrink and Perturb alone to all the parameters of the network is not sufficient to enable beneficial scaling; it is important that at least the network’s final layers be completely reset. We explain this using the observations from (Berariu et al., 2021) that the last layers are more responsible for the loss of plasticity.\n\nThat said, the most important factor in allowing SR-SPR to continue scaling well is its use of a target network. This effect is primarily due to better action selection through the target network; we found that the stabilizing effect on optimization was a less important factor. This is reminiscent of speedy Q-learning (Ghavamzadeh et al., 2011), where the use of an exponential moving average policy was shown to improve convergence speed, and can also be understood in relationship to the policy churn phenomenon (Schaul et al., 2022) (see Figure 10 in the appendix).\n\nFigure 7: The replay ratio scaling behavior of SRSPR with various components ablated.\n\nMeanwhile, removing both Shrink and Perturb and the target network is roughly equivalent to taking the method of Nikishin et al. (2022) but setting reset intervals as in SR-SPR. As Figure 7 suggests, this alone suffices to yield some replay ratio scaling but not as efficient compared to SR-SPR. However, maintaining a fixed reset interval (in terms of environment steps) when varying replay ratio, as done by Nikishin et al. (2022), leads to poor performance at replay ratios above 4.\n\nIntriguingly, we note that these modifications are beneficial specifically for replay ratio scaling; at replay ratios 1 or 2 they do not improve performance (although for the most part they do not significantly harm performance either). We thus hypothesize that there may be other modifications to complex algorithms such as SPR that could be made to further improve replay ratio scaling properties, but that are today not in widespread use because they do not improve performance in standard low replay ratio settings.\n\n5.3 VISUALIZING THE DATA/COMPUTE TRADEOFF\n\nIf an order of magnitude more of updates can be used for improving the performance of an algorithm, additional tradeoffs start to emerge. The type of computations that replay ratio scaling implies are fundamentally different than other concepts of scaling, (e.g., about larger models): scaling here is inherently sequential. Thus, obtaining more hardware does not help faster execution of the algorithm.\n\n8\n\n124816Replay Ratio0.250.300.350.400.450.500.550.600.65IQMSR-SPRNo SPNo Target ExplorationOnline TargetsNo SP, Online targetsConstant intervalSP OnlyPublished as a conference paper at ICLR 2023\n\nWhen collecting new transitions is not very expensive, the choice between collecting new samples in the environment and spending more time updating an agent could become nontrivial.\n\nWe visualize this tradeoff in Figure 8. The plot is obtained by combining runs of SR-SAC with doubling replay ratio from 0.25 to 128, and considering, for a fixed data budget (in terms of environment steps), the total computational budget (in terms of total number of agent updates at that point), as well as the achieved performance. There exists multiple ways to achieve the same level of performance, as denoted by the color. This plot shows that resets provide a knob on replay ratio scaling and allows to tradeoff data for computation. If, for a given problem, sample efficiency is more important than computational considerations, one can spend about two orders of magnitude of additional agent updates to obtain the same performance that can be obtained by waiting for 800000 additional samples to be collected from the environment. The peculiar feature of the approach we advocate for in this paper is that it allows to act on this tradeoff with an algorithm basically as simple as the employed backbone.\n\n6 THE LIMITS OF REPLAY RATIO SCALING\n\nFigure 8: Performance of SR-SAC in DMC15 as a function of the number of interactions and of the number of agent updates, determined by the replay ratio.\n\nWe have seen what becomes possible when higher level of replay ratio scaling are unlocked by resets. What are the limits of this paradigm? First of all, replay ratio scaling is always possible up to a finite value, at which there is simply not enough information left to be extracted from the existing dataset of experience. Current methods, including the one proposed in this paper, are not able to automatically identify when this limit is reached, and they are therefore still subject to performance collapse when increasing the replay ratio too much. Second, replay ratio scaling cannot go beyond the intrinsic limitations of the given deep RL algorithm: for example, if the task is simply impossible to solve because of hard credit assignment or exploration, then replay ratio scaling is only of limited help. Third, the strategy we proposed for replay ratio scaling is based on keeping the entire history of interactions with the environment in the replay buffer. While this is feasible for the kind of sampleefficiency benchmarks that we have used in this paper, it might also require special consideration to be applied to larger problems; for instance, it is possible to keep a large replay buffer on permanent storage, albeit at the cost of slower batch retrieval. Lastly, replay ratio scaling can inherently become time-consuming for a training agent, which can limit the applicability of methodologies like ours to settings requiring high-frequency interactions with an environment.\n\n7 CONCLUSIONS\n\nIn this paper, we have shown that, by leveraging partial or full resets of an agent’s parameters, it is possible to unlock new levels of favorable replay ratio scaling and, consequently, of sampleefficiency for model-free deep RL algorithms. We demonstrated this by a careful evaluation on the DeepMind Control Suite and Atari 100k benchmarks, where our approach (SR-SAC and SRSPR) demonstrated far superior performance compared to strong baselines, with minimal amounts of additional algorithmic complexity. Then, we discussed which algorithmic design choices are important for achieving such levels of replay ratio scaling with a deep RL algorithm, as well as the tradeoffs implied by this paradigm. Through our empirical analysis, we showed the value of online data collection, offering a perspective on its relationship with offline RL (Levine et al., 2020).\n\nMore generally, this paper is about how to leverage a discovery for the design of future deep RL algorithms. We believe this work to be an example of how the development of effective deep RL methods should be achieved not only through extending existing algorithms or creating new ones, but also through the discovery of new phenomena related to deep RL systems, and of techniques for exploiting them to increase performance. It is natural to wonder whether deeper understanding or exploitation of surprising empirical properties (Ostrovski et al., 2021; Schaul et al., 2022) beyond the one behind this work could lead to the emergence of new capabilities in deep RL algorithms.\n\n9\n\n0.20.40.60.81.0Data budget (Environment steps)1e6106107Computational budget (Number of agent updates)300400500600700IQMPublished as a conference paper at ICLR 2023\n\nACKNOWLEDGMENTS\n\nThe authors thank Zhixuan Lin for adapting the REDQ baseline, Nathan U. Rahn, Rishabh Agarwal, David Yu-Tung Hui, Jesse Farebrother for insightful discussions and useful suggestions on the early draft, the Mila community for creating a stimulating research environment, Digital Research Alliance of Canada and Nvidia for computational resources. This work was partially supported by CIFAR, Samsung, Hitachi, Facebook AI Chair, Borealis, IVADO, and Gruppo Ermenegildo Zegna.\n\nWe acknowledge the Python community (Van Rossum & Drake Jr, 1995; Oliphant, 2007) for developing the core set of tools that enabled this work, including JAX (Bradbury et al., 2018; Babuschkin et al., 2020), Jupyter (Kluyver et al., 2016), Matplotlib (Hunter, 2007), numpy (Oliphant, 2006; Van Der Walt et al., 2011), pandas (McKinney, 2012), and SciPy (Jones et al., 2014).\n\nREFERENCES\n\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deep reinforcement learning at the edge of the statistical precipice. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\n\nJordan Ash and Ryan P Adams. On warm-starting neural network training. Advances in Neural\n\nInformation Processing Systems, 33:3884–3894, 2020.\n\nIgor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin, Chris Jones, Tom Hennigan, Matteo Hessel, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Lena Martens, Vladimir Mikulik, Tamara Norman, John Quan, George Papamakarios, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Wojciech Stokowiec, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind.\n\nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural\n\nscaling laws. arXiv preprint arXiv:2102.06701, 2021.\n\nTudor Berariu, Wojciech Czarnecki, Soham De, Jorg Bornschein, Samuel Smith, Razvan Pascanu, and Claudia Clopath. A study on the plasticity of neural networks. arXiv preprint arXiv:2106.00042, 2021.\n\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n\nPablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A research framework for deep reinforcement learning. CoRR, abs/1812.06110, 2018. URL http://arxiv.org/abs/1812.06110.\n\nArslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 532–547, 2018.\n\nXinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning:\n\nLearning fast without a model. arXiv preprint arXiv:2101.05982, 2021.\n\nWill Dabney, Andr ́e Barreto, Mark Rowland, Robert Dadashi, John Quan, Marc G Bellemare, and David Silver. The value-improvement path: Towards better representations for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 7160– 7168, 2021.\n\n10\n\nPublished as a conference paper at ICLR 2023\n\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, et al. On robustness and transferability of convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16458–16468, 2021.\n\nShibhansh Dohare, Richard S. Sutton, and A. Rupam Mahmood. Continual backprop: Stochastic gradient descent with persistent randomness, 2022. URL https://openreview.net/forum? id=86sEVRfeGYS.\n\nPierluca D’Oro and Wojciech Ja ́skowski. How to learn a useful critic? model-based action-gradientestimator policy optimization. Advances in Neural Information Processing Systems, 33:313–324, 2020.\n\nWilliam Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark In International\n\nRowland, and Will Dabney. Revisiting fundamentals of experience replay. Conference on Machine Learning, pp. 3061–3071. PMLR, 2020.\n\nVincent Franc ̧ois-Lavet, Peter Henderson, Riashat Islam, Marc G Bellemare, Joelle Pineau, et al. An introduction to deep reinforcement learning. Foundations and Trends® in Machine Learning, 11 (3-4):219–354, 2018.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n\nMohammad Ghavamzadeh, Hilbert Kappen, Mohammad Azar, and R ́emi Munos.\n\nSpeedy q-learning. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. URL https://proceedings.neurips.cc/paper/2011/file/ ab1a4d0dd4d48a2ba1077c4494791306-Paper.pdf.\n\nTuomas Haarnoja, Aurick Zhou, P. Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum\n\nentropy deep reinforcement learning with a stochastic actor. In ICML, 2018.\n\nJessica B Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez, Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Holger Buesing, Petar Veliˇckovi ́c, and Theophane Weber. On the role of planning in model-based deep reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=IrM64DGB21.\n\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n\nTakuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka. Dropout q-functions for doubly efficient reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= xCVJMsPv3RT.\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. Training compute-optimal large language models. ArXiv, abs/2203.15556, 2022.\n\nG Zacharias Holland, Erin J Talvitie, and Michael Bowling. The effect of planning shape on dyna-style\n\nplanning in high-dimensional state spaces. arXiv preprint arXiv:1806.01825, 2018.\n\nJohn D Hunter. Matplotlib: A 2d graphics environment. IEEE Annals of the History of Computing, 9\n\n(03):90–95, 2007.\n\nMaximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson.\n\nTransient non-stationarity and generalisation in deep reinforcement learning. In ICLR, 2021.\n\nEric Jones, Travis Oliphant, and Pearu Peterson. SciPy: Open source scientific tools for Python.\n\n2014.\n\n11\n\nPublished as a conference paper at ICLR 2023\n\nLukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, K. Czechowski, D. Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Ryan Sepassi, G. Tucker, and Henryk Michalewski. Model-based reinforcement learning for atari. ArXiv, abs/1903.00374, 2020.\n\nJared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.\n\nKacper Piotr Kielak. Do recent advancements in model-based deep reinforcement learning really\n\nimprove data efficiency? 2019.\n\nThomas Kluyver, Benjamin Ragan-Kelley, Fernando P ́erez, Brian E Granger, Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, Jessica B Hamrick, Jason Grout, Sylvain Corlay, et al. Jupyter Notebooks-a publishing format for reproducible computational workflows., volume 2016. 2016.\n\nIlya Kostrikov. JAXRL: Implementations of Reinforcement Learning algorithms in JAX, 10 2021.\n\nURL https://github.com/ikostrikov/jaxrl.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\n\nq-learning. ArXiv, abs/2110.06169, 2022.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.\n\nAviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization\n\ninhibits data-efficient deep reinforcement learning. ArXiv, abs/2010.14498, 2021.\n\nKuang-Huei Lee, Ofir Nachum, Mengjiao Yang, L. Y. Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian S. Fischer, Eric Jang, Henryk Michalewski, and Igor Mordatch. Multi-game decision transformers. ArXiv, abs/2205.15241, 2022.\n\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. URL https://arxiv.org/abs/2005. 01643.\n\nLong-Ji Lin. Reinforcement learning for robots using neural networks. Carnegie Mellon University,\n\n1992.\n\nHao Liu and Pieter Abbeel. Aps: Active pretraining with successor features.\n\nIn International\n\nConference on Machine Learning, pp. 6736–6747. PMLR, 2021.\n\nClare Lyle, Mark Rowland, and Will Dabney. Understanding and preventing capacity loss in\n\nreinforcement learning. ArXiv, abs/2204.09560, 2022a.\n\nClare Lyle, Mark Rowland, Will Dabney, Marta Z. Kwiatkowska, and Yarin Gal. Learning dynamics\n\nand generalization in reinforcement learning. ArXiv, abs/2206.02126, 2022b.\n\nTatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deploymentefficient reinforcement learning via model-based offline optimization. In International Conference on Learning Representations, 2021.\n\nWes McKinney. Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. ”\n\nO’Reilly Media, Inc.”, 2012.\n\nVincent Micheli, Eloi Alonso, and Franc ̧ois Fleuret. Transformers are sample efficient world models,\n\n2022. URL https://arxiv.org/abs/2209.00588.\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015a.\n\n12\n\nPublished as a conference paper at ICLR 2023\n\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charlie Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529–533, 2015b.\n\nEvgenii Nikishin, Max Schwarzer, Pierluca D’Oro, Pierre-Luc Bacon, and Aaron C. Courville. The\n\nprimacy bias in deep reinforcement learning. In ICML, 2022.\n\nTravis E Oliphant. A guide to NumPy, volume 1. Trelgol Publishing USA, 2006.\n\nTravis E Oliphant. Python for scientific computing. Computing in Science & Engineering, 9(3):\n\n10–20, 2007.\n\nGeorg Ostrovski, Pablo Samuel Castro, and Will Dabney. The difficulty of passive learning in deep\n\nreinforcement learning. Advances in Neural Information Processing Systems, 2021.\n\nMartin Riedmiller. Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method. In European conference on machine learning, pp. 317–328. Springer, 2005.\n\nMartin A. Riedmiller, Jost Tobias Springenberg, Roland Hafner, and Nicolas Manfred Otto Heess.\n\nCollect & infer - a fresh look at data-efficient reinforcement learning. In CoRL, 2021.\n\nTom Schaul, Andr ́e Barreto, John Quan, and Georg Ostrovski. The phenomenon of policy churn.\n\narXiv preprint arXiv:2206.00730, 2022.\n\nMax Schwarzer, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron C. Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In ICLR, 2021a.\n\nMax Schwarzer, Nitarshan Rajkumar, Michael Noukhovitch, Ankesh Anand, Laurent Charlin, R Devon Hjelm, Philip Bachman, and Aaron C Courville. Pretraining representations for data-efficient reinforcement learning. Advances in Neural Information Processing Systems, 34:12686–12699, 2021b.\n\nLaura Smith, Ilya Kostrikov, and Sergey Levine. A walk in the park: Learning to walk in 20 minutes\n\nwith model-free reinforcement learning. ArXiv, abs/2208.07860, 2022.\n\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller. Deepmind control suite. ArXiv, abs/1801.00690, 2018.\n\nStefan Van Der Walt, S Chris Colbert, and Gael Varoquaux. The numpy array: a structure for efficient\n\nnumerical computation. Computing in science & engineering, 13(2):22–30, 2011.\n\nHado P Van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in\n\nreinforcement learning? Advances in Neural Information Processing Systems, 32, 2019.\n\nGuido Van Rossum and Fred L Drake Jr. Python tutorial, volume 620. Centrum voor Wiskunde en\n\nInformatica Amsterdam, 1995.\n\nZiyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, and Nando de Freitas. Sample efficient actor-critic with experience replay. arXiv preprint arXiv:1611.01224, 2016.\n\nYanqiu Wu, Xinyue Chen, Che Wang, Yiming Zhang, Zijian Zhou, and Keith W. Ross. Aggressive qlearning with ensembles: Achieving both high sample efficiency and high asymptotic performance, 2022. URL https://openreview.net/forum?id=NOApNZTiTNU.\n\nHattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron Courville. Fortuitous forgetting in connectionist networks. In International Conference on Learning Representations, 2022. URL https:// openreview.net/forum?id=ei3SY1_zYsE.\n\n13\n\nPublished as a conference paper at ICLR 2023\n\nExpression\n\nDamage Warm-Starting\n\nfrom\n\nDamage from Non-Stationarity\n\nDefinition\n\n[Phenomenon for which] “a warm-started network performs worse on test samples than a network trained on the same data but with a new random initialization” “A memory effect where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance”\n\nUsed In\n\nAsh & Adams (2020)\n\nIgl et al. (2021)\n\nCapacity Loss\n\n“Reduced ability to fit new targets in deep neural networks”\n\nLoss of Plasticity\n\n“Loss of the ability of the model to keep learning”\n\nPrimacy Bias\n\n“A tendency to overfit initial experiences that damages the rest of the learning process”\n\net\n\nLyle et al. (2022b), Lyle et al. (2022a) al. Berariu (2021), Dohare et al. (2022) Nikishin (2022)\n\nal.\n\net\n\nTable 3: Definitions of coinciding and related phenomena from previous work justifying the effectiveness of our strategy for replay ratio scaling.\n\n(a) DeepMind Control Suite (DMC15-500k)\n\n(b) Atari 100k\n\nFigure 9: Sensitivity of the IQM to varying reset intervals (in terms of gradient updates) of SR-SAC on the DeepMind Control Suite (DMC15-500k) benchmark, and of SR-SPR on the Atari 100k benchmark. (10 seeds, 95% bootstrapped C.I.).\n\nA DEFINITIONS FROM RELATED WORKS\n\nTo further clarify our description of previous work from the related work section, we report in Table 3 definitions of the different terms used to refer to the loss of the ability to learn and generalize in neural networks. Each definition is directly taken from one of the papers corresponding to it. Note that, despite their overlap, they reflect slightly different perspectives on the nature of this phenomenon, and it can be worth for future investigations to pin down which one of these is more relevant for replay ratio scaling or reinforcement learning as a whole.\n\nB ADDITIONAL EXPERIMENTAL RESULTS\n\nB.1 ADDITIONAL STUDIES\n\nReset Interval An important hyperparameter for both SR-SAC and SR-SPR is the interval at which resets are performed, as denominated in terms of number of agent updates. In Figure 9, we study how performance is impacted by this choice, at different replay ratios. Overall, both SR-SAC and SR-SPR perform well for a vast range of reset intervals, with favorable replay ratio scaling and generally smooth performance degradation. Note that, for large intervals (e.g., the last point on the right for SR-SAC with RR = 16, and last two points in the bottom right for SR-SPR), this is equivalent to actually performing no resets, just running the unmodified baseline algorithms. Thus, performance experiences non-smooth drops only in these easily avoidable cases.\n\n14\n\n12345678Reset Interval1e6400500600700IQMRR=16RR=32200004000080000160000Reset Interval0.250.300.350.400.450.500.550.600.65IQMRR=16RR=8RR=4RR=2RR=1Published as a conference paper at ICLR 2023\n\nFigure 10: Churn-related diagnostics (based on the policy churn definition from Schaul et al. (2022)) for the online and target networks. Different colors and RR denote different values of replay ratio.\n\nCounteracting Policy Churn by Acting with the Target Network Schaul et al. (2022) defined the policy churn as the change in the agent’s policy due to optimization. It was shown that a certain amount of policy churn can be beneficial for exploration in the absence of external noise (e.g., coming from ε-greedy exploration); however, it is intuitive that excessive churn can actually hurt performance, for instance by breaking the the consistency of trajectories. We hypothesize that mitigating excessive policy churn is a major reason why SR-SPR performs better when actions are selected with the target network rather than the online network. Figure 10 shows that, if we measure churn before each interaction with the environment, increasing the replay ratio will naturally increase it, while acting with the target network will decrease it. When the replay ratio is too high, it is likely that the benefits coming from additional offline computations might be nullified by the inconsistency in the exploration data; acting with the target network reduces these inconsistencies without giving up on the more efficient optimization, at the cost of introducing minimal delays in the improvement of the data-collecting policy.\n\nUsing an offline RL algorithm online In Section 5.1, we conducted a set of experiments with the goal of highlighting the importance of online interactions, concluding that a consistent stream of online interaction data and neural networks able to learn and generalize from a dataset of experiences are key factors behind effective replay ratio scaling. Online RL algorithms such as SAC are naturally reliant on the online stream of interactions; but it is natural to ask whether algorithms created for offline RL setting, where no interaction with the environment is assumed to be possible during training, can make the most out of the computational budget granted through high replay ratios in the online setting. To test this hypothesis, we run the Implicit Q-learning (IQL) (Kostrikov et al., 2022) algorithm as an online RL algorithm on DMC15-500k with a replay ratio of 32. Results in Figure 11 show that, despite being by design more robust to more aggressive training, the conservative\n\n15\n\n020000400006000080000100000Step0.000.010.020.030.040.050.06Target Churn020000400006000080000100000Step0.000.050.100.150.200.250.300.35Online Churn020000400006000080000100000Step0.30.40.50.60.70.8Online-Target Agreement020000400006000080000100000Step0.650.700.750.800.850.90Off-Policy RateRR 16.0 RR 8.0 RR 4.0 RR 2.0 RR 1.0 Published as a conference paper at ICLR 2023\n\nFigure 11: Performance on DMC15-500k of running IQL (RR=32) online, with and without resets.\n\nFigure 12: The performance of SR-SPR and SPR from scratch and when fine-tuning a pre-trained encoder on Atari 100k (10 seeds. 95% bootstrapped C.I.\n\nnature of offline RL algorithms makes them not amenable to effective online learning, regardless of the presence of resets. This shows the effectiveness of online interactions as a strong supervision mechanism, able, when supported by resets, to make online RL algorithms robust to high replay ratios without the need of overly conservative behaviors.\n\nB.2 FINETUNING PRETRAINED REPRESENTATIONS\n\nFinetuning pretrained representations has become increasing common in reinforcement learning and elsewhere (Schwarzer et al., 2021b; Liu & Abbeel, 2021; Bommasani et al., 2021). One obvious question to ask is whether or not replay scaling as demonstrated in the tabula rasa setting here can also be used to make this finetuning more sample efficient. In Figure 12 we answer this question in the affirmative. We initialize SPR and SR-SPR with pretrained encoders (taken for experimental convenience from SPR agents trained at replay ratio 1 for one million steps), and initialize all other parameters randomly. We then train at a range of replay ratios for 100k steps. For SR-SPR, we apply shrink and perturb towards the pretrained encoder weights rather than random parameters, but otherwise train as normal.\n\nWe find that while both SPR and SR-SPR benefit from the pretrained representations at low replay ratios, only SR-SPR is able to improve fine-tuning performance by replay scaling. Standard SPR with pretrained representations rapidly degrades in performance as the replay ratio is increased, while the performance of SR-SPR steadily increases at higher replay ratios. Although the gap between SR-SPR with and without pretraining closes somewhat at higher replay ratios, this is to be expected, as higher replay-ratio agents have more opportunities to improve their own representations even without pretraining.\n\nB.3 FINETUNING AFTER OFFLINE TRAINING\n\nThe efficiency of SR-SAC and SR-SPR makes the general approach behind their design potentially appealing for the setting of offline RL with an additional fine tuning phase. In Figure 13, we provide\n\n16\n\n250500750IQMSACREDQSR-SACDDPGIQL + resetsIQL200400600Median200400600Mean1248Replay Ratio0.20.30.40.50.60.7IQMPretrained SR-SPRSR-SPRPretrained SPRSPRPublished as a conference paper at ICLR 2023\n\nFigure 13: Performance of SR-IQL and IQL in two tasks from D4RL. Negative steps denotes the pretraining phase (10 seeds, ± std).\n\nFigure 14: Pareto fronts for SR-SAC and Tandem SR-SAC on DMC15 (5 seeds).\n\npreliminary evidence that the paradigm we advocated for in this paper may indeed be particularly beneficial in this setting. We test IQL with the same pretraining scheme presented in Kostrikov et al. (2022), consisting in a million offline training steps followed by a million interactions with the environment for fine tuning. We implement SR-IQL by using a replay ratio of 10 and resetting, every two million updates, all of the parameters of its neural networks during the fine tuning phase. We compare SR-IQL to IQL on two tasks from the D4RL benchmark (Fu et al., 2020). As shown in Figure 13, SR-IQL is roughly on par with IQL in the antmaze-umaze-v0 task, but reaches superior performance during fine tuning in antmaze-umaze-diverse-v0. We believe this sets the stage to experimenting with our replay ratio scaling paradigm in this setting as a promising research direction for future work.\n\nB.4 PARETO FRONTS COMPARISON\n\nWith the same approach used for studying the data/computation tradeoffs of SR-SAC, it also becomes possible to directly compare the performance of different replay ratio-scalable algorithms. As a simple example, we compare SR-SAC, its tandem version and SAC in Figure 14. The different lines are Pareto curves, obtained by retaining the points that are dominating the other ones in terms of either data or computational budget, to reach an IQM of at least 600. On this plot, SAC simply appears as a point because, not allowing for effective replay ratio scaling, it can only reach the prescribed performance by using more data and a relatively small amount of computational resources.\n\n17\n\n0.50.00.51.0Step1e6405060708090100Normalized Returnantmaze-umaze-v00.50.00.51.0Step1e6020406080100Normalized Returnantmaze-umaze-diverse-v0IQLSR-IQL468Data budget (Environment steps)1e50123Computational budget (Number of agent updates)1e7IQM > 600SR-SACTandem SR-SACSACPublished as a conference paper at ICLR 2023\n\nB.5 COMPARISON WITH NEURAL FITTED Q-ITERATION\n\nThe approach we demonstrated for replay ratio scaling, for its relationship with offline RL and its use of resets, could resemble the classic NFQI algorithm (Riedmiller, 2005), which train from scratch, after each large batch of transitions, a Q-function. Our approach propagates information across resets mainly through the use of the replay buffer, having a fast target network updated alongside the regular agent training; NFQI instead propagates information primarily through a target network, which is updated once per reset. We implement an-friendly variant of NFQI on SR-SPR at replay ratio 16, performing one target network update upon each reset. However, we find that this leads to very poor performance (IQM 0.350), achieving barely half that of standard SR-SPR. Although we hypothesized that 2,500 environment steps (the standard reset interval for SR-SPR at replay ratio 16) might be too infrequent for target network updates, making this interval shorter did not improve performance. Although we cannot rule out the possibility that NFQI might be competitive at dramatically higher replay ratios, its inherent slowness in propagating information is likely to lead it to lag in data-efficient settings; any reset interval that is sufficiently long to allow for accurate estimation of the value function may lead to insufficiently rapid value propagation via target updates, and vice versa.\n\nC COMPUTATIONAL CONSIDERATIONS\n\nFor DMC, the running time depends on the individual environment, due to differences in dimensionality of the observation as well as physics simulation time. On an NVIDIA V100 GPU, at this highest replay ratio of RR=128, our code takes about 10.5 hours on acrobot-swingup and about 15 hours on humanoid-run to complete 500k environment steps. For a replay ratio of RR=32, which yields remarkable, even if not best, performance, the time goes down to just about 3 hours and about 4 hours respectively. which is well-below the typical demands of modern model-based RL methods. With careful seed parallelization, running SR-SAC with RR=32 for 5 seeds for all tasks in the DMC15-500k benchmark takes less than 4 GPU/days on an NVIDIA V100. For Atari 100k, running time depends primarily on the replay ratio chosen. At the highest replay ratio used (16) and with five seeds running in parallel, our code takes roughly 25 hours to complete 100k steps on an NVIDIA A100, yielding a cost of roughly 5 GPU/hours per training run.\n\nD EXPERIMENTAL DETAILS\n\nWe report in Table 6 the full list of tasks for the DMC15 benchmark. Our implementation of continuous control algorithms is based on the jaxrl codebase (Kostrikov, 2021). For REDQ, we use the best hyperparameters, as recommended by Chen et al. (2021), as well as a replay ratio of 20. For discrete control, we use a version of SPR implemented in Jax (Bradbury et al., 2018) in Dopamine (Castro et al., 2018). See Table 5 for a full list of the employed hyperparameters.\n\nD.1 FULL EXPERIMENTAL RESULTS\n\nIn Figure 16, we show the scaling curve for DMC15-1M.\n\nWe report in Table 4 the full per-game results for SR-SPR and in Figure 17,18,19,20,21 full experimental results for SR-SAC. For completeness, we also report the performance of the modified settings and of REDQ at the same replay ratios.\n\n18\n\nPublished as a conference paper at ICLR 2023\n\nGame\n\nRandom Human\n\nIRIS\n\nSR-SPR:2\n\nSR-SPR:4\n\nSR-SPR:8\n\nSR-SPR:16\n\nAlien Amidar Assault Asterix Bank Heist Battle Zone Boxing Breakout Chopper Command Crazy Climber Demon Attack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull Kung Fu Master Ms Pacman Pong Private Eye Qbert Road Runner Seaquest Up N Down\n\nGames > Human IQM (↑) Optimality Gap (↓) Median (↑) Mean (↑)\n\n227.8 5.8 222.4 210.0 14.2 2360.0 0.1 1.7 811.0 10780.5 152.1 0.0 65.2 257.6 1027.0 29.0 52.0 1598.0 258.5 307.3 -20.7 24.9 163.9 11.5 68.4 533.4\n\n0 0.000 1.000 0.000 0.000\n\n7127.7 1719.5 742.0 8503.3 753.1 37187.5 12.1 30.5 7387.8 35829.4 1971.0 29.6 4334.7 2412.5 30826.4 302.8 3035.0 2665.5 22736.3 6951.6 14.6 69571.3 13455.0 7845.0 42054.7 11693.2\n\n0 1.000 0.000 1.000 1.000\n\n420.0 143.0 1524.4 853.6 53.1 13074.0 70.1 83.7 1565.0 59324.2 2034.4 31.1 259.1 2236.1 7037.4 462.7 838.2 6616.4 21759.8 999.1 14.6 100.0 745.7 9614.6 661.3 3546.2\n\n9 0.501 0.512 0.289 1.046\n\n877.9 189.2 891.9 836.7 253.6 14493.5 36.1 24.5 1609.4 28004.7 2969.0 24.1 1450.4 735.3 6832.1 412.9 1651.2 5206.4 14165.6 1472.6 -10.5 98.8 3431.7 12199.0 714.7 61851.2\n\n7 0.444 0.516 0.336 0.910\n\n964.4 211.8 987.3 894.2 460.0 17800.6 42.0 26.1 1933.7 38341.7 3016.2 24.5 1809.9 717.5 7195.7 408.8 2024.1 5364.3 17656.5 1544.7 -5.5 95.8 3699.8 14287.3 766.6 91435.2\n\n8 0.544 0.470 0.523 1.111\n\n1015.5 203.1 1069.5 916.5 472.3 19398.4 46.7 28.8 2201.0 43122.3 2898.1 24.9 1752.8 711.2 7679.6 392.8 3254.9 5824.8 17095.6 1522.6 -3.0 95.8 3850.6 13623.5 800.5 95501.1\n\n9 0.589 0.452 0.560 1.188\n\n1107.8 203.4 1088.9 903.1 531.7 17671.0 45.8 25.5 2362.1 45544.1 2814.4 25.4 2584.8 712.4 8524.0 389.1 3631.7 5914.4 18649.4 1574.1 2.9 97.9 4044.1 13463.4 819.0 112450.3\n\n9 0.632 0.433 0.685 1.272\n\nTable 4: Full results for individual games in Atari 100k for SR-SPR at various replay ratios.\n\nFigure 15: Learning curves for SR-SPR (solid) and SPR (dashed) at various replay ratios. Note that all SR-SPR runs converge to similar TD errors, gradient norms and parameter norms, while these metrics greatly differ for SPR at different replay ratios. IQM training performance does not match evaluation performance, as ongoing training episodes are often disrupted by the reset procedure.\n\n19\n\n020000400006000080000100000Step406080100120Parameter Norm020000400006000080000100000Step0.500.751.001.251.501.752.002.25Gradient Norm020000400006000080000100000Step0.060.080.100.120.140.160.18TD Error020000400006000080000100000Step0.00.20.40.6Training IQMSR-SPR:2SPR:2SR-SPR:4SPR:4SR-SPR:8SPR:8SR-SPR:16SPR:16Published as a conference paper at ICLR 2023\n\nParameter\n\nGray-scaling Observation down-sampling Frames stacked Action repetitions Reward clipping Terminal on loss of life Max frames per episode Update Dueling Support of Q-distribution Discount factor Minibatch size Optimizer Optimizer: learning rate Optimizer: β1 Optimizer: β2 Optimizer: ε Max gradient norm Priority exponent Priority correction Exploration Noisy nets parameter Training steps Evaluation trajectories Min replay size for sampling Replay period every Updates per step Multi-step return length Q network: channels Q network: filter size Q network: stride Q network: hidden units Non-linearity Target network update period λ (SPR loss coefficient) K (SPR prediction depth) Data Augmentation\n\nτ (EMA coefficient)\n\nReset Interval (gradient steps) Layers getting hard reset Shrink and Perturb α Action selection\n\nSetting\n\nTrue 84x84 4\n4 [-1, 1] True 108K Distributional Q True 51 0.99 32 Adam 0.0001 0.9 0.999 0.00015 10 0.5 0.4 → 1 Noisy nets 0.5 100K 100 2000 1 step Variable (1, 2, 4, 8, 16) 10 32, 64, 64 8 × 8, 4 × 4, 3 × 3 4, 2, 1 512 ReLU 1\n2 5\nShifts (±4 pixels) Intensity(scale=0.05) 0.995\n\n40,000 Final 2 0.8 Target network\n\nParameter\n\nDiscount factor Minibatch size Optimizer (all) Optimizer (all): learning rate Optimizer (all): β1 Optimizer (all): β2 Optimizer (all): ε Networks (all): activation Networks (all): n. hidden layers Networks (all): hidden units Initial Temperature Replay Buffer Size Updates per step Target network update period τ (EMA coefficient)\n\nReset Interval (gradient steps) Layers getting hard reset\n\nSetting\n\n0.99 256 Adam 0.0003 0.9 0.999 0.00015 ReLU 2\n256 1\n106 Variable (1 to 128) 1\n0.995\n\n2560000 All\n\nTable 5: Hyperparameters for SR-SPR and SR-SAC. The ones introduced by this work are at the bottom of the respective tables.\n\n20\n\nPublished as a conference paper at ICLR 2023\n\nEnvironment\n\nwalker quadruped reacher humanoid swimmer cheetah hopper acrobot pendulum finger fish\n\nTasks\n\nrun run, walk hard run, walk, stand swimmer6 run hop, stand swingup swingup turn hard swim\n\nTable 6: The tasks from the DMC15 benchmark. We chose commonly-employed DMC tasks for which the optimal policy is not immediately found by SAC according to https://github.com/ denisyarats/pytorch_sac#results.\n\nFigure 16: Scaling curve for SR-SAC and SAC on DMC15-1M.\n\nFigure 17: Evaluation Returns on individual DMC15 environments for replay ratio 8.\n\n21\n\n1248163264128Replay Ratio200400600800IQMSACSR-SAC0.000.250.500.751.00Environment Steps (×106)0100200Episode Returnacrobot-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returncheetah-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnfinger-turn_hard0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnfish-swim0.000.250.500.751.00Environment Steps (×106)0100200300Episode Returnhopper-hop0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhopper-stand0.000.250.500.751.00Environment Steps (×106)050100150200Episode Returnhumanoid-run0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnhumanoid-stand0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnhumanoid-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnpendulum-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-run0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnreacher-hard0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnswimmer-swimmer60.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnwalker-runSR-SAC, RR=8SR-SAC, RR=8SAC, RR=8SR-SAC, RR=8SAC, RR=8Tandem SR-SAC, RR=8SR-SAC, RR=8SAC, RR=8Tandem SR-SAC, RR=8Iterated Offline SR-SAC, RR=8SR-SAC, RR=8SAC, RR=8Tandem SR-SAC, RR=8Iterated Offline SR-SAC, RR=8REDQ, RR=8Published as a conference paper at ICLR 2023\n\nFigure 18: Evaluation Returns on individual DMC15 environments for replay ratio 16.\n\nFigure 19: Evaluation Returns on individual DMC15 environments for replay ratio 32.\n\nFigure 20: Evaluation Returns on individual DMC15 environments for replay ratio 64.\n\n22\n\n0.000.250.500.751.00Environment Steps (×106)0100200300Episode Returnacrobot-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returncheetah-run0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnfinger-turn_hard0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnfish-swim0.000.250.500.751.00Environment Steps (×106)0100200300Episode Returnhopper-hop0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhopper-stand0.000.250.500.751.00Environment Steps (×106)0100200Episode Returnhumanoid-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhumanoid-stand0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnhumanoid-walk0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnpendulum-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnquadruped-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnreacher-hard0.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnswimmer-swimmer60.000.250.500.751.00Environment Steps (×106)0250500750Episode Returnwalker-runSR-SAC, RR=16SR-SAC, RR=16SAC, RR=16SR-SAC, RR=16SAC, RR=16Tandem SR-SAC, RR=16SR-SAC, RR=16SAC, RR=16Tandem SR-SAC, RR=16Iterated Offline SR-SAC, RR=16SR-SAC, RR=16SAC, RR=16Tandem SR-SAC, RR=16Iterated Offline SR-SAC, RR=16REDQ, RR=160.000.250.500.751.00Environment Steps (×106)0100200Episode Returnacrobot-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returncheetah-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnfinger-turn_hard0.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnfish-swim0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnhopper-hop0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhopper-stand0.000.250.500.751.00Environment Steps (×106)0100200Episode Returnhumanoid-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhumanoid-stand0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnhumanoid-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnpendulum-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-run0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnreacher-hard0.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnswimmer-swimmer60.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnwalker-runSR-SAC, RR=32SR-SAC, RR=32SAC, RR=32SR-SAC, RR=32SAC, RR=32Tandem SR-SAC, RR=32SR-SAC, RR=32SAC, RR=32Tandem SR-SAC, RR=32Iterated Offline SR-SAC, RR=32SR-SAC, RR=32SAC, RR=32Tandem SR-SAC, RR=32Iterated Offline SR-SAC, RR=32REDQ, RR=320.000.250.500.751.00Environment Steps (×106)0100200300Episode Returnacrobot-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returncheetah-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnfinger-turn_hard0.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnfish-swim0.000.250.500.751.00Environment Steps (×106)0200400Episode Returnhopper-hop0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhopper-stand0.000.250.500.751.00Environment Steps (×106)0100200Episode Returnhumanoid-run0.000.250.500.751.00Environment Steps (×106)0250500750Episode Returnhumanoid-stand0.000.250.500.751.00Environment Steps (×106)0250500750Episode Returnhumanoid-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnpendulum-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnquadruped-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnreacher-hard0.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnswimmer-swimmer60.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnwalker-runSR-SAC, RR=64SR-SAC, RR=64SAC, RR=64SR-SAC, RR=64SAC, RR=64Tandem SR-SAC, RR=64SR-SAC, RR=64SAC, RR=64Tandem SR-SAC, RR=64Iterated Offline SR-SAC, RR=64SR-SAC, RR=64SAC, RR=64Tandem SR-SAC, RR=64Iterated Offline SR-SAC, RR=64REDQ, RR=64Published as a conference paper at ICLR 2023\n\nFigure 21: Evaluation Returns on individual DMC15 environments for replay ratio 128.\n\n23\n\n0.000.250.500.751.00Environment Steps (×106)0100200300Episode Returnacrobot-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returncheetah-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnfinger-turn_hard0.000.250.500.751.00Environment Steps (×106)0200400600800Episode Returnfish-swim0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnhopper-hop0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhopper-stand0.000.250.500.751.00Environment Steps (×106)0100200300Episode Returnhumanoid-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnhumanoid-stand0.000.250.500.751.00Environment Steps (×106)0250500750Episode Returnhumanoid-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnpendulum-swingup0.000.250.500.751.00Environment Steps (×106)02505007501000Episode Returnquadruped-run0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnquadruped-walk0.000.250.500.751.00Environment Steps (×106)05001000Episode Returnreacher-hard0.000.250.500.751.00Environment Steps (×106)0200400600Episode Returnswimmer-swimmer60.000.250.500.751.00Environment Steps (×106)0250500750Episode Returnwalker-runSR-SAC, RR=128SR-SAC, RR=128SAC, RR=128SR-SAC, RR=128SAC, RR=128Tandem SR-SAC, RR=128SR-SAC, RR=128SAC, RR=128Tandem SR-SAC, RR=128Iterated Offline SR-SAC, RR=128SR-SAC, RR=128SAC, RR=128Tandem SR-SAC, RR=128Iterated Offline SR-SAC, RR=128REDQ, RR=128",
  "translations": [
    "# Summary Of The Paper\n\nThis paper presents that we can significantly improve the sample-efficient of prior deep RL approaches by increasing the number of updates per environment steps, and shows that resetting all parameters or part of parameters is critical. The paper shows improved performance on a variety of benchmark tasks, and provides interesting analysis and observations that can be related to the offline RL literature.\n\n# Strength And Weaknesses\n\nStrengths\n- Well written, intuitive approach\n- Strong performance with a very simple idea\n- Exhaustive experiments and interesting analysis and discussions\n\nWeaknesses\n- Given the recent surge of leveraging pre-trained representations for vision-based RL, it would be an interesting investigation to see what would happen when considering such a pre-trained perception module.\n- Investigating the role of replay ratio scaling for model-based approaches, especially the effect on the world models and corresponding policies obtained from the models, would be an interesting addition to the paper, but I don't think this is a necessary and could be a future work.\n\n# Clarity, Quality, Novelty And Reproducibility\n\n**Clarity and Quality.**\n- There's a typo in Section 5.2: ` achieving robust replay ratio scaling for SR-SPR is requires due to its shorter training period and more complex function approximation.`\n- In Figure 3 and Table 2, it could be more helpful for clarity if baselines are introduced with acronyms along with their references.\n- For self-containedness, it would be nice to include the formulation or more detailed explanation on SPR\n\n**Novelty.**\n- Interesting and novel empirical observations\n\n**Reproducibility.**\n- It seems that the approach is not difficult to reproduce; but it would be nice to include source codes for this\n\n# Summary Of The Review\n\nThe paper consists of an intuitive approach, nice explanation, strong performance, well supported claims, and interesting discussions. I recommend the paper to be accepted and has no major concerns.\n\n# Correctness\n\n4: All of the claims and statements are well-supported and correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n4: The contributions are significant, and do not exist in prior works.",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" presents a framework for enhancing sample efficiency in deep reinforcement learning (RL) through increased replay ratios—defined as the number of updates per environment interaction. The authors introduce innovative algorithmic strategies, specifically the Scaled-by-Resetting versions of Soft Actor-Critic (SR-SAC) and SPR (SR-SPR), which leverage parameter resets to improve learning. Empirical evaluations on well-known benchmarks, including the Atari 100k and DeepMind Control Suite, demonstrate that these modified algorithms significantly outperform existing strong baselines in terms of sample efficiency and learning performance.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its novel approach to overcoming the limitations associated with replay ratio scaling, presenting clear improvements in empirical performance. The introduction of parameter resets offers a fresh perspective on enhancing RL algorithms. However, a notable weakness is the potential computational tradeoff discussed, which could limit practical applications depending on the context of use. Additionally, while the results are compelling, the authors acknowledge inherent tradeoffs and limits to replay ratio scaling that may affect generalization to other tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, making it accessible to readers familiar with RL concepts. The methodology is described in sufficient detail, allowing for reproducibility, particularly regarding the implementation of SR-SAC and SR-SPR. The results are presented with appropriate metrics and comparisons to baseline methods, enhancing the clarity and quality of the findings. The novelty of the approach, particularly concerning the use of parameter resets, is a significant contribution to the field.\n\n# Summary Of The Review\nThis paper presents a significant advancement in sample efficiency for deep reinforcement learning through innovative replay ratio scaling techniques. While the empirical results are strong, the computational tradeoffs may limit the applicability of the proposed methods in certain scenarios. Overall, the contributions are both novel and impactful, warranting further exploration in future research.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhance the sample efficiency of deep reinforcement learning (RL) algorithms through the strategic use of parameter resets, termed as \"resetting.\" The authors introduce two modified algorithms: Scaled-by-Resetting Soft Actor-Critic (SR-SAC) for continuous tasks and Scaled-by-Resetting Policy Representation (SR-SPR) for discrete tasks. The methodology involves varying the replay ratios and implementing periodic resets to improve learning performance. Experimental results demonstrate that both SR-SAC and SR-SPR achieve significant performance gains over traditional methods across multiple benchmarks, including the Atari 100k and the DeepMind Control Suite, validating the hypothesis that higher replay ratios can enhance sample efficiency.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to increasing sample efficiency through parameter resets, which is a fresh contribution to the field of deep RL. The robust experimental validation, supported by comprehensive evaluations across diverse benchmarks, further underscores the effectiveness of the proposed methods. However, the reliance on optimal reset intervals poses a limitation, as these settings may vary significantly by task, complicating practical implementation. Additionally, the generalizability of the approach to a wider range of RL tasks remains uncertain, and potential computational costs present a challenge for environments requiring high-frequency interactions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The use of clear metrics such as interquartile mean (IQM), median, and mean scores enhances the quality of reporting. The novelty of introducing parameter resets as a mechanism to improve sample efficiency is significant, contributing fresh insights to existing literature. However, the reproducibility may be hampered by the dependence on specific hyperparameters, particularly the reset intervals, which were not exhaustively explored across a broader range of tasks.\n\n# Summary Of The Review\nOverall, this paper represents a substantial advancement in the area of sample-efficient deep reinforcement learning through the introduction of parameter resets. While the findings are promising, particularly for specific benchmarks, further investigation is needed to address the challenges of hyperparameter sensitivity and generalization to a wider array of RL environments.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" introduces a novel approach to enhance sample efficiency in deep reinforcement learning (RL) by increasing the replay ratio, which refers to the number of updates per environment interaction. The authors propose a method of periodic parameter resets, which they argue restores the learning capabilities of neural networks, thereby allowing for higher replay ratios without performance degradation. The methodology is tested through modified algorithms, Scaled-by-Resetting Soft Actor-Critic (SR-SAC) and Scaled-by-Resetting Sample-efficient Policy Reinforcement (SR-SPR), which demonstrate state-of-the-art performance improvements on the Atari 100k and DeepMind Control Suite benchmarks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to addressing the limitations of replay ratio scaling in RL, providing a clear solution to enhance sample efficiency. The empirical results are compelling; both SR-SAC and SR-SPR outperformed existing algorithms across several metrics and benchmarks. However, a potential weakness is that the paper could benefit from a more detailed exploration of the limitations of the proposed method, particularly regarding its applicability to larger or more complex problems. Furthermore, while the experiments are comprehensive, further analysis on the robustness of the results in varying environments could strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with reinforcement learning. The quality of the writing and figures is high, aiding in the understanding of complex concepts. The novelty of the approach, particularly the idea of parameter resets to enhance replay ratio scaling, is significant and adds a fresh perspective to the field. The reproducibility of the results is supported by detailed descriptions of the experimental setups, hyperparameters, and performance metrics, allowing for replication of the study.\n\n# Summary Of The Review\nOverall, this paper makes a notable contribution to the field of reinforcement learning by demonstrating how parameter resets can effectively enhance sample efficiency through increased replay ratios. The empirical results show significant improvements over existing algorithms, although the paper could further explore the limitations and robustness of the proposed methods.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces two novel algorithms, SR-SAC and SR-SPR, aimed at improving sample efficiency in reinforcement learning (RL) through enhanced replay ratio scaling. The authors propose a unique mechanism of periodic parameter resets to optimize the replay process, demonstrating their approach's effectiveness on benchmarks such as Atari 100k and the DeepMind Control Suite. Comprehensive empirical evaluations highlight the robustness of the proposed methods against strong baselines, while theoretical insights provide a deeper understanding of the challenges inherent in RL.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including improved sample efficiency through higher replay ratios and a thorough evaluation across various environments, which underscores the robustness of the proposed algorithms. However, it also has limitations, such as a heavy reliance on parameter resets, which may not be applicable in all contexts. The scope of tested environments is somewhat limited, raising questions about the generalizability of the findings. While the theoretical insights offered are valuable, the lack of formal proofs diminishes their impact. Additionally, the complexity introduced by the new approach may deter its adoption in real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presented, which enhances its accessibility for readers. The novel approach to replay ratio scaling is a significant contribution to the field, although the practical implications of the proposed methods are not fully addressed. The reproducibility of the experiments seems feasible given the detailed empirical evaluations, though the implementation complexity may pose challenges for practitioners.\n\n# Summary Of The Review\nOverall, the paper makes meaningful contributions to the field of reinforcement learning with innovative algorithms and thorough evaluations. However, its reliance on specific techniques and the lack of formal proofs may limit its broader applicability and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" introduces a novel technique called Dynamic Parameter Realignment (DPR) aimed at enhancing sample efficiency in deep reinforcement learning (RL). Unlike traditional methods that increase the replay ratio, DPR focuses on periodically adjusting model parameters based on current learning dynamics to optimize training efficiency. The authors present empirical results demonstrating that their approach significantly outperforms standard RL algorithms across various benchmarks, including Atari 100k and the DeepMind Control Suite, achieving higher sample efficiency and improved performance metrics.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to improving RL training dynamics through DPR, which offers a promising alternative to conventional methods that rely heavily on replay ratios. The empirical results are compelling, showcasing substantial performance gains and enhanced sample efficiency, thereby demonstrating the practicality of the approach. However, a potential weakness is the need for more extensive exploration of the parameters that influence the effectiveness of DPR, which could limit the generalizability of the method across diverse RL environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly articulates the motivation behind DPR, its implementation, and its outcomes. The methodology is described in detail, allowing for reproducibility, although the authors could provide additional insights into the parameter tuning process for DPR. The novelty of the approach is significant, as it challenges existing paradigms in RL training. Overall, the quality of writing and presentation is high, contributing to the paper's accessibility.\n\n# Summary Of The Review\nThis paper presents a noteworthy advancement in sample-efficient reinforcement learning through the introduction of Dynamic Parameter Realignment. The empirical results validate the effectiveness of the method, though further exploration of its parameter dynamics could enhance its applicability in broader contexts.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces a novel approach to adversarial training by employing periodic parameter resetting to improve the robustness of deep neural networks against adversarial attacks. The authors highlight that conventional adversarial training methods often experience diminishing returns over time, leading to compromised generalization. By integrating a parameter resetting strategy during training, the authors demonstrate significant enhancements in adversarial robustness across several benchmark datasets, suggesting that their method can mitigate the common pitfalls associated with traditional adversarial training techniques.\n\n# Strength And Weaknesses\nThe paper's strengths include its innovative approach to adversarial training, providing a new perspective that emphasizes parameter resetting as a means to boost robustness. Extensive empirical results demonstrate the effectiveness of the proposed method, yielding better performance than traditional adversarial training methods. The simplicity of the algorithm is also a notable advantage, allowing for straightforward integration into existing frameworks. However, the paper's weaknesses lie in its limited experimental scope, focusing primarily on a narrow set of datasets which may limit the generalizability of the findings. Additionally, the absence of comprehensive ablation studies regarding hyperparameters related to the resetting strategy restricts the depth of analysis. There is also a concern regarding potential overfitting to the training distribution, particularly if the resetting frequency is not well-calibrated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, facilitating comprehension of the proposed methodology and its implications. Figures and tables illustrate performance improvements effectively, though some could benefit from clearer labeling for better interpretability. The terminology is consistent throughout, but the inclusion of a glossary for key adversarial training terms would enhance accessibility. In terms of reproducibility, while the methodology is straightforward, the lack of detailed ablation studies raises questions about the robustness of the findings under various conditions.\n\n# Summary Of The Review\nThis paper offers a compelling new approach to adversarial training through parameter resetting, yielding promising results in enhancing neural network robustness. While the contributions are noteworthy, the limited experimental scope and lack of detailed analysis on hyperparameters detract from the overall impact. The findings present valuable insights for future research in building resilient AI systems.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" presents a novel method aimed at enhancing sample efficiency in reinforcement learning by introducing parameter resets. The authors claim that their approach allows for significantly more updates than traditional methods, potentially revolutionizing the field. They report improved performance on benchmarks such as Atari 100k and the DeepMind Control Suite. However, the paper also suggests that the method's implications for generalization across various contexts and its complexity may be overstated.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious claim to improve sample efficiency in reinforcement learning, which is a critical area of research. The proposed methodology, particularly the idea of parameter resets, could hold some merit; however, it appears to lack true novelty given prior work in the field. The reported improvements on standard benchmarks are noteworthy, but they may not be as substantial as claimed, especially in light of recent advancements. Furthermore, while the authors suggest minimal complexity in implementation, the reality may involve more intricate modifications. The acknowledgment of limitations is present, but it is framed in a way that downplays potential challenges, which could mislead readers regarding the practicality of the method.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that outlines its contributions and methodology. However, claims regarding novelty and impact are often exaggerated, which detracts from the paper's overall quality. While the methodology is presented in a reproducible manner, the actual implementation details may require further elaboration to ensure that other researchers can replicate the results effectively. Additionally, the generalization of findings across different contexts is inadequately supported, which raises concerns about the reproducibility of the claims made.\n\n# Summary Of The Review\nWhile the paper presents an interesting approach to improving sample efficiency in reinforcement learning, many of the claims regarding its novelty and impact are overstated. The contributions, though potentially valuable, may not represent a significant departure from existing methods, limiting their relevance for the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhance sample efficiency in deep reinforcement learning (RL) by increasing the replay ratio, which refers to the number of updates to the agent's parameters per interaction with the environment. The authors demonstrate that implementing parameter resets, either full or partial, can significantly improve the scalability of the replay ratio. Their experiments reveal that the proposed methods, SR-SAC and SR-SPR, achieve superior performance on benchmarks such as Atari 100k and the DeepMind Control Suite, indicating a notable advancement in sample efficiency.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to tackling the sample efficiency challenge in RL through a careful manipulation of the replay ratio and parameter resets. The empirical results are compelling, showcasing significant performance improvements over existing methods, which reinforces the validity of the proposed techniques. However, a potential weakness is the reliance on specific benchmarks, which may limit the generalizability of the findings. Additionally, the discussion on the theoretical underpinnings of why parameter resets enhance performance could be more robust.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers familiar with RL concepts. The methodology is described in sufficient detail, enabling reproducibility of the experiments. The novelty of the approach is evident in the introduction of parameter resets as a mechanism to scale replay ratios effectively. However, some aspects of the algorithmic design could benefit from further elaboration, particularly regarding the trade-offs between online and offline updates.\n\n# Summary Of The Review\nOverall, this paper provides a valuable contribution to the field of reinforcement learning by introducing a method to improve sample efficiency through increased replay ratios and parameter resets. The empirical results demonstrate significant advancements over existing benchmarks, although further generalization and theoretical insights would enhance the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper proposes a method for improving sample efficiency in reinforcement learning through the use of replay ratio scaling and parameter resets. The methodology involves adjusting the number of updates per environment interaction to enhance learning efficiency and employing resets to restore the neural network's learning capacity. The findings indicate that these techniques can lead to improved performance on benchmark tasks such as Atari 100k and the DeepMind Control Suite, suggesting potential benefits in terms of learning speed and efficiency.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to enhancing sample efficiency, which addresses a critical challenge in reinforcement learning. The experimental results are promising, indicating that the proposed methods can yield better performance in controlled environments. However, several weaknesses are evident: the assumptions about linear relationships between replay ratios and performance may not hold universally, and the reliance on specific benchmarks raises questions about the generalizability of the findings. Furthermore, the potential negative effects of frequent resets and the implications of memory constraints are not sufficiently explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, with a logical flow of ideas. However, certain assumptions made within the methodology could benefit from more detailed explanations and justifications. The novelty of the proposed techniques is commendable, although their reproducibility may be hindered by the specific experimental setups used. The authors could enhance reproducibility by providing more comprehensive details on the implementation and tuning of their methods across various architectures and tasks.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to improving sample efficiency in reinforcement learning, with promising results in specific benchmarks. However, the assumptions and limitations of the proposed techniques warrant further investigation to ensure their applicability in broader contexts.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the effect of increasing the replay ratio in deep reinforcement learning (RL) by implementing parameter resets, which leads to improved sample efficiency across various benchmarks. The authors introduce two modified algorithms, SR-SAC and SR-SPR, demonstrating that these adjustments significantly enhance performance through extensive updates while addressing the challenge of minimizing sample interactions in RL. The findings suggest that parameter resets can restore neural networks' learning capabilities, thus facilitating better performance in dynamic environments.\n\n# Strength And Weaknesses\nThe paper presents a compelling argument for the benefits of replay ratio scaling combined with parameter resets, which is a novel approach to enhancing sample efficiency in deep RL. The empirical results across continuous and discrete control tasks are well-documented, illustrating clear performance improvements. However, the authors acknowledge limitations, such as performance collapse at high replay ratios, which could restrict applicability in larger or more complex tasks. Additionally, while the methodology is sound, further exploration of the specific conditions under which these techniques are effective would strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with a clear progression from motivation to methodology and results. The writing is concise and accessible, effectively communicating the key concepts and findings. The novelty of the approach lies in the integration of parameter resets with replay ratio scaling, a combination that has not been extensively explored previously. The reproducibility of the results is supported by detailed descriptions of the algorithm modifications and experimental setups, although providing access to code and datasets would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in deep reinforcement learning by introducing parameter resets to improve sample efficiency through replay ratio scaling. While there are limitations regarding the applicability of high replay ratios, the findings are well-supported and provide a valuable contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces an innovative approach to enhancing the effectiveness of neural networks in classification tasks. The authors propose a new architecture that integrates attention mechanisms with a self-supervised learning framework. They conduct extensive experiments across several benchmark datasets, demonstrating that their method not only improves accuracy but also reduces training time compared to existing models. The findings suggest that self-supervised learning can significantly enhance the performance of attention-based architectures.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The integration of attention mechanisms with self-supervised learning is a fresh perspective that addresses prevalent issues in model performance.\n2. **Comprehensive Evaluation:** The authors provide thorough empirical evaluations across multiple datasets, lending credibility to their claims.\n3. **Strong Theoretical Support:** The paper is grounded in a solid theoretical foundation, which strengthens the rationale for the proposed method.\n4. **Clear Presentation:** The writing is clear and well-structured, making the complex ideas accessible to a wider audience.\n\n**Weaknesses:**\n1. **Limited Scope of Experiments:** Despite the comprehensive evaluations, the experimental scope could be broadened to include more diverse tasks and datasets.\n2. **Hyperparameter Sensitivity:** There is a concern regarding the sensitivity of the proposed method to hyperparameter tuning, which may affect replicability.\n3. **Comparative Analysis:** While comparisons with existing methods are made, a deeper analysis could enhance the understanding of the proposed method's relative strengths and weaknesses.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and presents its ideas in a coherent and logical manner. The clarity of the explanations and the quality of the figures contribute to the overall understanding of the proposed method. However, the reproducibility could be improved by providing more details on hyperparameter settings and experimental configurations. The novelty of the approach is high, as it combines established techniques in a novel way that could lead to significant advancements in the field.\n\n# Summary Of The Review\nOverall, this paper offers a valuable contribution to the machine learning community by presenting an innovative approach that combines attention mechanisms with self-supervised learning. While the findings are promising, addressing the limitations in experimental scope and hyperparameter sensitivity could further strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" presents a novel approach to enhancing sample efficiency in deep reinforcement learning (RL) by increasing the replay ratio, which refers to the number of updates per environment interaction. The authors propose methods that incorporate parameter resets to significantly improve the effectiveness of this replay ratio scaling. Their experiments demonstrate substantial performance gains in benchmark environments, specifically Atari 100k and the DeepMind Control Suite, while discussing the key design choices that facilitate this improvement. The paper advocates for a deeper understanding of replay ratio dynamics and suggests that future RL algorithm development should leverage these empirical findings.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to a well-known limitation in deep RL regarding sample efficiency. By effectively utilizing parameter resets, the authors provide a practical solution that yields significant performance enhancements with minimal added complexity. The paper also offers a thorough analysis of design choices that contribute to effective replay ratio scaling, which adds depth to the findings. However, a potential weakness is the reliance on specific benchmark environments, which may limit the generalizability of the results. Additionally, the discussion on trade-offs associated with increased replay ratios could benefit from more detailed quantitative analysis.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and communicates its findings clearly, with a logical progression from motivation to methodology and results. The quality of the writing is high, making it accessible to readers familiar with deep RL concepts. In terms of novelty, the approach of using parameter resets to enhance replay ratio scaling is both original and significant. Reproducibility appears feasible, as the methods are described in sufficient detail and the authors provide adequate information regarding the experimental setup. However, providing code or additional resources could further enhance reproducibility efforts.\n\n# Summary Of The Review\nOverall, the paper offers a compelling contribution to the field of deep reinforcement learning by addressing the critical issue of sample efficiency through innovative methods. The findings demonstrate significant performance improvements in benchmark tasks and provide valuable insights into the interplay between replay strategies and algorithm design. However, further exploration of the generalizability of these results and a more detailed discussion on trade-offs would strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" presents significant advancements in improving sample efficiency in reinforcement learning (RL) through the novel introduction of parameter resets. The authors propose two modified algorithms, Scaled-by-Resetting Soft Actor-Critic (SR-SAC) and SR-SPR, which leverage increased replay ratios and targeted parameter resets to enhance learning capabilities. Experimental results demonstrate that these approaches achieve substantially improved performance across benchmark tasks such as Atari 100k and the DeepMind Control Suite, establishing new state-of-the-art results and providing insights into optimizing replay ratio scaling.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the challenges associated with sample efficiency in deep RL, effectively demonstrating that parameter resets can significantly enhance learning in dynamic environments. The authors provide a thorough analysis of the design choices involved in their methodology and present robust empirical results that validate their claims. However, a notable weakness is the inherent limitation of replay ratio scaling, which the authors acknowledge may lead to performance collapse if pushed too far. Additionally, the potential complexity and time demands of training with these methods could limit their practical applicability in high-frequency settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The methodology is described in sufficient detail to allow for reproducibility, with standardized benchmarks employed for performance evaluation. The novelty of the approach is significant, as it challenges existing paradigms in RL regarding replay ratio and learning efficiency. However, the discussion on the limitations of the approach could be expanded to provide a more comprehensive understanding of the conditions under which the proposed methods may falter.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of reinforcement learning by presenting an innovative approach to improving sample efficiency through parameter resets. While the methodology is sound and results are compelling, there are inherent limitations that must be addressed to enhance the practicality and robustness of the proposed methods.\n\n# Correctness\nRating: 5\n\n# Technical Novelty And Significance\nRating: 4\n\n# Empirical Novelty And Significance\nRating: 4",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" addresses the challenge of sample efficiency in reinforcement learning (RL). The authors propose a novel methodology involving parameter resets to enhance sample efficiency, as detailed in their algorithmic approach. They conduct experiments on benchmark environments, including Atari 100k and the DeepMind Control Suite, demonstrating significant performance improvements over baseline algorithms. The findings indicate that their proposed methods (SR-SAC and SR-SPR) effectively break the replay ratio barrier, leading to more effective learning in RL tasks.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear identification of a critical issue in RL—sample efficiency—and its innovative solution through parameter resets. The authors provide a thorough comparison of their methods against established baselines, showcasing substantial empirical improvements. However, the paper could benefit from a more extensive discussion on the limitations of the proposed methods, particularly in scenarios where the replay ratio may not be as impactful. Additionally, while the empirical results are promising, the applicability of the methods beyond the tested environments remains somewhat unclear.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable; it is well-structured and easy to follow, with technical terms adequately defined. The methodology is presented with sufficient detail to allow for reproducibility, and the results are effectively illustrated through clear figures and tables. The novelty of the approach is significant, as it introduces a fresh perspective on enhancing sample efficiency in RL. However, the paper could improve its discussions around potential challenges in reproducing results across different RL settings.\n\n# Summary Of The Review\nOverall, this paper presents a meaningful contribution to the field of reinforcement learning by addressing sample efficiency through innovative parameter resets. The empirical results support the proposed methods, but further exploration of their limitations and broader applicability would enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates the concept of \"replay ratio,\" defined as the ratio of updates to agent parameters per interaction with the environment, and its implications on the sample efficiency of deep reinforcement learning (RL). The authors propose a methodology involving systematic parameter resets to enhance this replay ratio, demonstrating its effectiveness through empirical results in the Atari 100k and DeepMind Control Suite environments. Key findings indicate that these techniques significantly improve sample efficiency for model-free RL algorithms, while also providing a theoretical framework for understanding the limitations and trade-offs associated with replay ratio scaling.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to enhancing sample efficiency through replay ratio scaling and systematic parameter resets, which introduces a new perspective on the training dynamics in RL. The empirical results provide compelling evidence of performance improvements, establishing a new state-of-the-art in both continuous and discrete control tasks. However, the paper could benefit from a more extensive discussion on the practical implications of the identified limitations, particularly the diminishing returns at high replay ratios and the challenges posed by complex credit assignment problems.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings, making it accessible to readers familiar with deep RL concepts. The quality of the empirical evaluations is commendable, supported by comprehensive performance metrics and graphical analyses. The novelty of the approach is significant, as it pushes the boundaries of existing methodologies in RL. However, reproducibility could be enhanced by providing detailed descriptions of hyperparameter settings and experimental conditions, which would allow for easier replication of results.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in the area of sample-efficient reinforcement learning through innovative replay ratio scaling and parameter resetting strategies. While the contributions are noteworthy and well-supported by empirical evidence, addressing the practical limitations and enhancing reproducibility would strengthen the impact of the work.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper proposes a method aimed at enhancing sample efficiency in deep reinforcement learning (RL) by increasing the replay ratio. The authors claim that their modifications lead to improved performance compared to existing baselines, although they do not present statistically significant or meaningful results. The methodology hinges on periodic parameter resets, which introduces additional complexity and raises concerns about practical feasibility in continuous learning scenarios.\n\n# Strength And Weaknesses\nThe paper's main strength lies in its attempt to address sample efficiency in deep RL, a critical area of research. However, the reliance on benchmarks such as Atari 100k and the DeepMind Control Suite diminishes the ecological validity of the findings, as these environments may not accurately represent real-world challenges. Moreover, the lack of a comprehensive ablation study to isolate the effects of various design decisions undermines the robustness of the claims. The authors' assertions about improved performance are not sufficiently backed by rigorous empirical support, making it challenging to assess the true impact of their contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by vague discussions regarding the inherent limits of replay ratio scaling and the conditions under which their method may succeed or fail. While the authors emphasize the novelty of periodic resets, this concept has been explored in previous works, reducing the originality of their contribution. Additionally, the paper does not provide detailed comparisons or specific examples to illustrate the claimed improvements, which further affects reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to enhancing sample efficiency in deep RL, but it suffers from several significant weaknesses, including a lack of generalizability and insufficient empirical support for its claims. The methodological complexities and the reliance on potentially unrepresentative benchmarks raise concerns about its practical applicability.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" presents a novel approach to enhance sample efficiency in deep reinforcement learning (RL) by increasing the replay ratio during training. The authors introduce algorithms Scaled-by-Resetting SAC (SR-SAC) and Scaled-by-Resetting SPR (SR-SPR) that either fully or partially reset neural network parameters, resulting in a significant increase in updates per environment interaction. The results demonstrate state-of-the-art performance improvements on benchmarks such as Atari 100k and DeepMind Control Suite, suggesting that these techniques could serve as a robust framework for future off-policy deep RL algorithms.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative contribution to sample efficiency, which has been a critical challenge in deep reinforcement learning. By effectively increasing the replay ratio, the proposed methods yield impressive performance boosts, establishing new benchmarks and opening avenues for future research. However, the paper could benefit from a more detailed discussion on the implications of increased computational requirements, as leveraging more computational power may not always be feasible in real-world applications. Additionally, while the experimental results are compelling, further exploration of the theoretical underpinnings of the proposed methods would bolster the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates the methodology and findings. The structure is logical, allowing readers to follow the authors’ thought process easily. The novelty of the approach is significant, as it proposes a new way to enhance sample efficiency that has not been extensively explored in prior literature. The reproducibility of the results appears strong, with clearly defined algorithms and benchmarks, although additional details regarding hyperparameter tuning and experimental setup would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, this paper presents a significant advancement in sample efficiency within deep reinforcement learning by innovatively increasing the replay ratio. The results are impressive and set new benchmarks across standard evaluations, promising to influence both academic research and practical applications in the field.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper explores the theoretical aspects of sample-efficient reinforcement learning through the lens of replay ratio scaling and parameter reset mechanisms. It posits that a higher replay ratio, which defines the number of updates per environment interaction, can improve sample efficiency but also presents challenges such as overfitting to outdated data. The authors introduce the concept of dynamic datasets in reinforcement learning, emphasizing the importance of maintaining an agent's learning capacity through mechanisms like parameter resets. Key findings suggest that while increasing the replay ratio can enhance learning, it is critical to balance this with the potential decay in an agent's ability to generalize over time.\n\n# Strength And Weaknesses\nStrengths of the paper include its rigorous theoretical framework and the comprehensive exploration of the interplay between replay ratios and parameter resets. The discussion on dynamic datasets is particularly insightful, highlighting the complexities of training agents in non-stationary environments. However, a notable weakness is the paper's limited empirical validation of the proposed theories; while the theoretical insights are robust, the lack of experimental results may hinder the practical applicability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-structured arguments and a logical flow that facilitates understanding of complex concepts. The quality of writing is high, and the theoretical contributions are novel, offering a fresh perspective on reinforcement learning. However, reproducibility may be a concern due to the absence of empirical experiments that could validate the theoretical claims made in the paper. The reliance on theoretical constructs without accompanying experimental data may limit the ability to replicate findings in practice.\n\n# Summary Of The Review\nOverall, the paper presents a valuable theoretical exploration of sample-efficient reinforcement learning through replay ratio scaling and parameter resets. While the theoretical insights are significant and well-articulated, the lack of empirical validation may limit the practical impact of the research.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" presents novel approaches for enhancing sample efficiency in reinforcement learning (RL) through increased replay ratios. The authors introduce two algorithms, SR-SAC for continuous control and SR-SPR for discrete control, which utilize parameter resetting strategies to scale the replay ratio effectively. Experimental results demonstrate that SR-SAC achieves state-of-the-art performance in the DeepMind Control Suite, while SR-SPR sets new benchmarks in the Atari 100k environment. The paper emphasizes the importance of systematic parameter resets and optimal tuning of hyperparameters to improve learning efficiency and performance.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to tackling the replay ratio challenge, which is a critical aspect of sample efficiency in RL. The methodologies employed, including the use of parameter resets and different reset strategies for the two algorithms, are well-articulated and contribute to the overall effectiveness of the proposed methods. The extensive experimental validation, including ablation studies and comparisons with baseline methods, adds credibility to the findings. However, a notable weakness is the discussion of computational efficiency, where the paper could provide more insights into the trade-offs between increased computational demands and the benefits of higher replay ratios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodologies, and experimental results. The figures and tables effectively illustrate the scaling behavior and performance outcomes, aiding comprehension. The descriptions of the algorithms and their implementations are detailed, enhancing reproducibility. The novel aspect of parameter resetting is clearly highlighted, showcasing the paper's significant contribution to the field. However, more thorough explanations of the implications of hyperparameter tuning could enhance clarity for practitioners looking to apply the findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of sample-efficient reinforcement learning by introducing innovative methods to break the replay ratio barrier. Its experimental results establish new performance benchmarks, making it a valuable contribution to the literature. However, the paper could benefit from a more in-depth discussion of computational efficiency trade-offs.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper proposes a novel approach to improving sample efficiency in deep reinforcement learning (RL) by introducing a method that purportedly breaks the replay ratio barrier. The authors claim that their method, referred to as SR-SAC, achieves state-of-the-art results on benchmarks such as Atari 100k and the DeepMind Control Suite while maintaining algorithmic simplicity. However, the paper fails to sufficiently contextualize its contributions within the existing literature and does not provide a comprehensive comparison with state-of-the-art methods.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its presentation of intriguing findings related to sample efficiency and its exploration of periodic resets. However, the weaknesses are significant. The authors do not adequately address limitations posed by previous works, such as Fedus et al. (2020) and Kumar et al. (2021), which explored similar concepts. Moreover, the lack of rigorous comparative analysis with relevant state-of-the-art methods, including REDQ and earlier versions of SPR, hinders the assessment of the proposed method's efficacy. The authors also fail to critically analyze the implications of their findings against foundational research, which raises questions about the novelty and significance of their contributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-structured, its clarity is undermined by a lack of depth in discussing the implications of existing literature. The quality of the methodology is acceptable, but the novelty of the proposed approach is questionable due to insufficient engagement with prior work. Reproducibility is hindered by the absence of detailed information regarding the comparative performance of their method against established benchmarks.\n\n# Summary Of The Review\nOverall, the paper presents interesting findings but lacks sufficient contextualization and rigorous comparison with existing methods, leading to an overstated claim of novelty. The contributions could be valuable if further substantiated against the backdrop of established literature in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"SAMPLE-EFFICIENT REINFORCEMENT LEARNING BY BREAKING THE REPLAY RATIO BARRIER\" presents a novel approach to improving sample efficiency in reinforcement learning (RL) by addressing the limitations of traditional replay mechanisms. The authors introduce a new framework that optimizes the replay ratio, allowing for more effective learning from past experiences with less reliance on extensive data. Through rigorous experimentation on various benchmark tasks, the findings demonstrate substantial improvements in sample efficiency and overall performance compared to existing methods.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to the replay ratio, which is a critical factor in reinforcement learning. The methodology is well-articulated, with a clear justification for the proposed modifications and extensive empirical validation. However, a notable weakness is the lack of exploration into the long-term implications of the proposed method in more complex environments. Additionally, while the results are promising, the paper could benefit from a more comprehensive discussion of the limitations and potential drawbacks of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear, but some complex sentences could be simplified for better readability. The quality of the writing is high, with a logical flow of ideas. The novelty of the approach is significant, as it challenges conventional methods in RL and provides a fresh perspective on replay mechanisms. Reproducibility is adequately addressed with sufficient details on the experimental setup; however, minor issues with formatting and clarity in figures and tables could hinder replication efforts.\n\n# Summary Of The Review\nOverall, this paper presents a compelling contribution to the field of reinforcement learning by introducing a novel approach to improving sample efficiency through replay ratio optimization. While the methodology and findings are robust, the paper could improve clarity in some sections and provide a deeper exploration of its limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to replay ratio scaling in reinforcement learning, primarily evaluated within the contexts of Atari games and the DeepMind Control Suite benchmarks. The authors propose a method aimed at improving sample efficiency by adjusting the replay ratio used during training. The experimental results indicate significant performance improvements, underscoring the potential of their approach in enhancing learning efficiency in standard environments.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear presentation of the replay ratio scaling method and the empirical validations demonstrating its effectiveness. However, it has notable weaknesses, including a limited exploration of diverse environments beyond the primary benchmarks used, which raises questions about the generalizability of the findings. Additionally, the paper lacks comparisons with other relevant techniques in continual learning and meta-learning, which could provide a more comprehensive understanding of the method's positioning within the field. The absence of a detailed discussion on practical applications, computational costs, and robustness against noisy data further diminishes the impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its findings in a clear and organized manner. However, the novelty is somewhat diminished by the lack of exploration into existing methods that could have provided a richer context for the proposed approach. The reproducibility of the results could be improved with more detailed methodologies, particularly regarding the experimental setups and parameter choices.\n\n# Summary Of The Review\nOverall, the paper contributes a valuable method for improving sample efficiency in reinforcement learning, yet it falls short in its exploration of broader implications and comparisons with existing techniques. A deeper analysis of limitations and practical applications could strengthen its relevance and utility in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier\" addresses the challenge of improving sample efficiency in reinforcement learning (RL) algorithms. The authors introduce a novel approach centered on the concept of **Replay Ratio Scaling**, which quantitatively examines the relationship between performance and the number of updates. They employ a rigorous statistical methodology, including the use of Interquartile Mean (IQM) and bootstrapped confidence intervals, to validate their findings. The results indicate significant improvements in sample efficiency, particularly when comparing new algorithms such as SR-SAC and SR-SPR against standard baselines like REDQ and SAC.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its comprehensive empirical analysis and the robust statistical framework used to evaluate performance. The use of multiple seeds to ensure replicability and the incorporation of confidence intervals add credibility to the findings. However, a notable weakness is the limited discussion of formal hypothesis testing for the performance comparisons, which may leave some of the statistical claims less rigorously supported. Additionally, while the paper identifies limitations in current methods for determining optimal replay ratios, it could benefit from more detailed exploration of these challenges and suggested solutions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of methodologies and results. The quality of empirical evaluation is high, with careful attention to statistical rigor. The novelty of the proposed Replay Ratio Scaling is significant, as it provides a new lens through which to evaluate RL algorithms. Reproducibility is also a strong point, given the extensive use of multiple seeds and the clear documentation of experimental setups aligned with existing protocols in the field.\n\n# Summary Of The Review\nOverall, the paper presents a substantial advancement in the evaluation of sample efficiency in reinforcement learning through a novel statistical approach. While it demonstrates strong empirical results and clarity in methodology, it could enhance its rigor through more formal hypothesis testing and a deeper exploration of limitations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to reinforcement learning focusing on the use of replay buffers and reset strategies to improve learning efficiency in environments such as Atari 100k and the DeepMind Control Suite. The authors propose methods to enhance the replay ratio while maintaining performance, with the aim of addressing issues related to performance degradation due to excessive replay. However, the exploration is limited to the aforementioned benchmarks, and the paper does not adequately address the scalability of these methods to more complex environments or applications.\n\n# Strength And Weaknesses\nWhile the paper offers interesting contributions regarding replay strategies, it has several weaknesses that limit its overall impact. Notably, it fails to address the automatic identification of limits concerning replay ratio scaling, which poses a risk of performance collapse when replay ratios become excessive. Additionally, the methodology relies on maintaining a complete replay buffer, which may not be feasible in larger or real-time scenarios. The authors also overlook the implications for safety-critical applications and do not explore complementary strategies that could enhance performance further. The lack of hyperparameter analysis and exploration of online-offline data collection interplay detracts from the overall robustness of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally acceptable, though some sections could benefit from more detailed explanations, particularly regarding the implications of the proposed methods in various contexts. The quality of the experiments is satisfactory, but the reproducibility is hindered by the limited exploration of hyperparameters and environments. The novelty lies primarily in the proposed strategies for replay and resets; however, without further validation across diverse scenarios, its significance remains limited.\n\n# Summary Of The Review\nOverall, the paper introduces some valuable ideas regarding replay strategies in reinforcement learning, yet it falls short in exploring the broader implications and potential applications of these methods. The lack of thorough investigation into scalability, safety, and alternative strategies limits its contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a method aimed at improving sample efficiency in reinforcement learning (RL) by manipulating the replay ratio and employing parameter resetting strategies. The authors claim that these approaches lead to significant performance enhancements across standard benchmarks, including Atari games and the DMC15 suite. They also discuss the importance of online interaction and provide a visualization of the data/compute tradeoffs involved in their method.\n\n# Strength And Weaknesses\nWhile the paper attempts to contribute to the RL field by advocating for increased replay ratios and parameter resetting, many of the insights presented are not novel and have been previously discussed within the community. The methodology is based on well-established concepts, which diminishes the originality of the approach. However, the empirical results demonstrating improved performance are a strength, albeit expected given the modifications to existing algorithms.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is reasonably clear in its presentation, though it relies on concepts that are familiar to most RL researchers, which undermines its novelty. The quality of the empirical results is solid, but the reproducibility may be impacted by the lack of detailed descriptions of the modifications made to the algorithms. The authors should provide clearer guidance for replicating their experiments to enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a set of well-known strategies framed as new contributions, resulting in a lack of genuine novelty. While the empirical results show improvements, they do not sufficiently substantiate the claims of groundbreaking findings. The work may be useful for practitioners, but it does not advance the theoretical understanding of RL significantly.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper investigates replay ratio scaling in reinforcement learning, emphasizing the role of online interactions and their impact on sample efficiency. The authors propose a methodology that leverages parameter resets to enhance learning stability and mitigate plasticity loss. The findings demonstrate improved performance in various tasks, suggesting that the proposed techniques can effectively optimize the learning process in dynamic environments.\n\n# Strength And Weaknesses\nThe paper makes significant contributions by addressing the challenges of sample efficiency and stability in reinforcement learning. However, while the authors provide a solid foundation, there is room for further exploration in areas such as advanced online learning techniques, hierarchical reinforcement learning (HRL), and transfer learning. Incorporating these aspects could enhance the adaptability and generalization capabilities of the proposed methods. Additionally, the reliance on periodic resets may limit the flexibility of the agents, and alternative strategies could be more beneficial.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. However, the novelty is somewhat tempered by the lack of exploration into alternative learning techniques and a limited discussion on the implications of computational efficiency. Reproducibility could be improved by providing open-source implementations of the proposed algorithms, which would facilitate further research and validation of the results.\n\n# Summary Of The Review\nOverall, the paper presents valuable insights into replay ratio scaling in reinforcement learning, with practical implications for improving sample efficiency and learning stability. While the contributions are noteworthy, there is potential for deeper exploration into advanced techniques and a broader comparison with state-of-the-art methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces two novel reinforcement learning algorithms: Scaled-by-Resetting Soft Actor-Critic (SR-SAC) and Scaled-by-Resetting Soft Policy Reinforcement (SR-SPR), which achieve significant improvements in sample efficiency on benchmark tasks. The methodology centers on leveraging higher replay ratios through parameter resets, resulting in enhanced performance across both continuous and discrete control environments. The findings reveal that SR-SAC and SR-SPR outperform standard baselines on the Atari 100k and DeepMind Control Suite benchmarks, achieving interquartile mean (IQM) scores of 740 and 0.632, respectively, thereby establishing new state-of-the-art results.\n\n# Strength And Weaknesses\nThe paper’s main strengths lie in its empirical validation of the proposed methods, showcasing substantial performance enhancements over existing algorithms such as SAC and REDQ. The consistency of performance improvements with increasing replay ratios also highlights the scalability of the approach. However, the paper could benefit from a more detailed discussion of the underlying mechanisms that contribute to the observed improvements, as well as a broader range of environments to further validate the generalizability of the results.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe presentation of the paper is generally clear, with well-organized sections detailing methodology, results, and discussions. The quality of the empirical results is high, as they are based on established benchmarks. The novelty of the proposed methods is significant, given their introduction of a new technique for improving sample efficiency. While the reproducibility of results appears feasible due to the clear reporting of experimental setups, additional details on hyperparameters and implementation specifics would enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents an impactful contribution to the field of reinforcement learning by introducing methods that significantly improve sample efficiency in both continuous and discrete control tasks. The empirical results are compelling and establish new benchmarks, though further exploration of the underlying principles and broader validation could strengthen the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to enhancing sample efficiency in reinforcement learning (RL) by introducing a method termed \"replay ratio.\" The authors employ a combination of theoretical analysis and empirical evaluation to demonstrate the effectiveness of their approach across various benchmark tasks. Key findings indicate that their method significantly improves performance compared to existing algorithms, particularly in environments with limited data availability.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its innovative approach to improving sample efficiency, which addresses a critical challenge in RL. The authors provide a solid theoretical foundation for their method, bolstered by comprehensive empirical results that validate their claims. However, the paper suffers from issues related to clarity and readability, including long sentences and dense jargon that could alienate some readers. Additionally, the presentation of figures and results could be improved for better comprehension.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept, the clarity of the writing detracts from its overall quality. The dense language and technical jargon may hinder understanding, particularly for those not well-versed in the field. Improvements in the organization and presentation of results, as well as a more accessible writing style, would enhance reproducibility and engagement with the research. Transition sentences and clearer definitions of key terms are needed to improve the flow of ideas.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of reinforcement learning, with a novel methodology that shows promise in improving sample efficiency. However, clarity issues and the need for improved presentation detract from its impact. Addressing these concerns would greatly enhance the paper's accessibility and reader engagement.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.592615694212014,
    -1.7641196672199142,
    -1.7995939913218375,
    -1.5828688458236837,
    -1.9061478665990441,
    -1.6064792798095413,
    -1.70095826073665,
    -1.9086885484207567,
    -1.5180278190022205,
    -1.651377585662946,
    -1.8834989535767372,
    -1.3303310761441696,
    -1.6329517104958606,
    -1.778452771192229,
    -1.6742717848870003,
    -1.7806754952416137,
    -1.8585466123783612,
    -1.7082286956185027,
    -1.5905654474587825,
    -1.651407871568733,
    -1.8249147927389167,
    -1.534397552455959,
    -1.8208170445875402,
    -1.8282568494436195,
    -2.052810420836715,
    -1.9764457541337976,
    -1.884982668816987,
    -1.6702436262475795,
    -1.5540326415928913
  ],
  "logp_cond": [
    [
      0.0,
      -2.270168734691511,
      -2.298206505779884,
      -2.2822224945764695,
      -2.2998618199075893,
      -2.3850071320049495,
      -2.415107874880887,
      -2.3397077111553077,
      -2.259587844562854,
      -2.344686747659595,
      -2.2689112144429644,
      -2.4090655173091697,
      -2.287263815657427,
      -2.27692285973099,
      -2.300161221361849,
      -2.3110522005204337,
      -2.347528045876307,
      -2.282318980532705,
      -2.3252389322437303,
      -2.2723233306215795,
      -2.3554474018636244,
      -2.35697601277888,
      -2.3352615685346696,
      -2.2795152135811954,
      -2.369730977112734,
      -2.3343866708449257,
      -2.3042859071475075,
      -2.3220371666219153,
      -2.3592730079834245
    ],
    [
      -1.3893887315901166,
      0.0,
      -1.2756648463410447,
      -1.2039664006370518,
      -1.3340951798520015,
      -1.405795417793398,
      -1.4395889089480816,
      -1.3457511720081095,
      -1.2511414964267513,
      -1.3809907367743024,
      -1.325489079631939,
      -1.48492932007635,
      -1.2864310679485116,
      -1.172451043117758,
      -1.2774709177324175,
      -1.3548355147674713,
      -1.3934570540257971,
      -1.2324720703077452,
      -1.412546437386631,
      -1.257152182992831,
      -1.3841558467614945,
      -1.4331314941763478,
      -1.4453324938181624,
      -1.334434805602823,
      -1.428318235919002,
      -1.3999688775668284,
      -1.3884764576588269,
      -1.30824311400872,
      -1.4857295982524
    ],
    [
      -1.4576504516378646,
      -1.279838681157957,
      0.0,
      -1.3050038574888227,
      -1.361495090732944,
      -1.513403486868333,
      -1.5351431149245607,
      -1.3853585056973972,
      -1.3435974967761395,
      -1.4108534088496503,
      -1.3544943194025831,
      -1.5723826342086542,
      -1.4074447669945525,
      -1.2740230079804529,
      -1.3529681559834237,
      -1.453278075802029,
      -1.3867913998651173,
      -1.3487753367488846,
      -1.4692363805492905,
      -1.376767017023702,
      -1.436839091552206,
      -1.5318242446665389,
      -1.508122501171448,
      -1.472029486109588,
      -1.4596299430388053,
      -1.4538331370861444,
      -1.4584393047248283,
      -1.3385769879893925,
      -1.554726922533029
    ],
    [
      -1.2645241684752688,
      -1.0270145707515952,
      -1.1220860266986943,
      0.0,
      -1.2068762208293973,
      -1.2616994203629948,
      -1.347327680928684,
      -1.1929706544349254,
      -1.1220028448876664,
      -1.2146720516370995,
      -1.0961551160239467,
      -1.3356527948734436,
      -1.1534902733640355,
      -1.0449226066525996,
      -1.1180685988364503,
      -1.2059619033116964,
      -1.2207081765539676,
      -1.0487435993359635,
      -1.2423029732277016,
      -1.1205091613118459,
      -1.235941382926215,
      -1.2658129376282217,
      -1.3387412712858535,
      -1.187287539940861,
      -1.2813944008260922,
      -1.2815093786568024,
      -1.2610488795480312,
      -1.1219595750802849,
      -1.3362852917430073
    ],
    [
      -1.4113570585788124,
      -1.3293803835157278,
      -1.4194230874892908,
      -1.324358341287901,
      0.0,
      -1.462444457258135,
      -1.5402121126843484,
      -1.3696716871342622,
      -1.3677417675794958,
      -1.414868465147769,
      -1.3958488925527819,
      -1.5823180834054495,
      -1.365885364550099,
      -1.3618662381539206,
      -1.3792989800293598,
      -1.407946478704084,
      -1.337867174814374,
      -1.3801284999199284,
      -1.466368429863101,
      -1.3685320642782575,
      -1.3859858284748727,
      -1.5437829399973535,
      -1.457062659252087,
      -1.4212066038176319,
      -1.4247227525766917,
      -1.4821457085133998,
      -1.4390306683688239,
      -1.3586313585959098,
      -1.52677361767221
    ],
    [
      -1.295766323711088,
      -1.1311278283322668,
      -1.2052012617234495,
      -1.1036411549256782,
      -1.2025966695740042,
      0.0,
      -1.2409698763544543,
      -1.1255631922695912,
      -1.1113803645963143,
      -1.1944679338170034,
      -1.2139454450085103,
      -1.272771729835551,
      -1.1012900947949003,
      -1.103047360535989,
      -1.1096301708996512,
      -1.1962521478352357,
      -1.2071005314267937,
      -1.157350848805346,
      -1.247227104033298,
      -1.1326063351406597,
      -1.187628887634423,
      -1.1920320052391555,
      -1.277240043868291,
      -1.2163300774927646,
      -1.2377430577025563,
      -1.2347730198641202,
      -1.2664003717019567,
      -1.2337944126324156,
      -1.2927438342282025
    ],
    [
      -1.4332233147534386,
      -1.4524606906993738,
      -1.4240872008157017,
      -1.444648972982983,
      -1.4403691787058597,
      -1.4516497436967202,
      0.0,
      -1.4067924227966884,
      -1.4716828082100055,
      -1.467140488341031,
      -1.4580205858859172,
      -1.4163221717257457,
      -1.4674998382003974,
      -1.4517564738198485,
      -1.440829439405691,
      -1.4588762515338747,
      -1.4192674522223732,
      -1.476203883205817,
      -1.451184733839822,
      -1.3960307104889655,
      -1.4589225862936661,
      -1.478032369284705,
      -1.4650722476577691,
      -1.4741592344870416,
      -1.436189187320921,
      -1.4441389600225274,
      -1.4339420503571747,
      -1.4858872800240024,
      -1.4194486018926566
    ],
    [
      -1.5735571431069084,
      -1.458999069636358,
      -1.5303390138493207,
      -1.476419142494705,
      -1.5255584772803812,
      -1.553047582530282,
      -1.6254038958238164,
      0.0,
      -1.5095556165848234,
      -1.5358794596703078,
      -1.5440618807018671,
      -1.645432833370495,
      -1.4783189452748566,
      -1.4872254996479584,
      -1.508518258768964,
      -1.538627629919119,
      -1.5570982201964352,
      -1.48264547391081,
      -1.5692327126971974,
      -1.4912191898451215,
      -1.5425839422267702,
      -1.5740795705581154,
      -1.628106539749633,
      -1.5494917891156381,
      -1.5803218864393267,
      -1.5560999815419094,
      -1.5688791374354831,
      -1.5786248166367705,
      -1.6139019271062613
    ],
    [
      -1.0832182857102708,
      -0.9734130969568934,
      -1.1155132871603575,
      -0.9963258147178232,
      -1.0804541965949235,
      -1.1162707300271872,
      -1.2367519144529229,
      -1.0800466221500618,
      0.0,
      -1.0671429676821222,
      -1.0307906717266606,
      -1.265700986891291,
      -1.014103391025991,
      -1.0040404794303595,
      -1.045597011425972,
      -1.0602817089145824,
      -1.063458157700049,
      -1.0300411672843905,
      -1.131615649882109,
      -1.0095100205794663,
      -1.101101179988639,
      -1.2198910227494983,
      -1.171145322781094,
      -1.0950036337429652,
      -1.1584622161390992,
      -1.1205701987746097,
      -1.1226372876952773,
      -1.053096845975896,
      -1.210408474729855
    ],
    [
      -1.2936008655176232,
      -1.2110074011039544,
      -1.2105311946266624,
      -1.181371925437918,
      -1.2123618248465666,
      -1.209865562415957,
      -1.3663009753550426,
      -1.1603270407597632,
      -1.1505050467773343,
      0.0,
      -1.2258192993083874,
      -1.3754195716253794,
      -1.1367242163447224,
      -1.1845644687951038,
      -1.1945267974877776,
      -1.2165189915538972,
      -1.137422498549359,
      -1.2106438823422692,
      -1.2339405756243595,
      -1.2320503899907285,
      -1.2132843787335634,
      -1.3112389058458305,
      -1.2479629720813343,
      -1.2721659195087818,
      -1.1690952609619585,
      -1.2739641453847785,
      -1.1979945723894245,
      -1.2206291120261903,
      -1.3244471520366266
    ],
    [
      -1.4382571592505133,
      -1.3869100423834613,
      -1.4663673004824522,
      -1.368050849140424,
      -1.478172372200136,
      -1.587856778009899,
      -1.5881534791917857,
      -1.482453830085135,
      -1.4447327463548454,
      -1.444543767465795,
      0.0,
      -1.5942299339542803,
      -1.4267264718531207,
      -1.3458156737655689,
      -1.4957113755732319,
      -1.4316863299032654,
      -1.4508718341598195,
      -1.4213457646792793,
      -1.4701589717711054,
      -1.4624544373622244,
      -1.5033252489500881,
      -1.5507269749205979,
      -1.5285350995799232,
      -1.4543183774300246,
      -1.4709456140666868,
      -1.5221367238363928,
      -1.42213331832193,
      -1.4458872462215084,
      -1.575313196108641
    ],
    [
      -1.0730452970825657,
      -1.0322454650643837,
      -0.9925493818824681,
      -1.0125617812643664,
      -1.012507553802916,
      -1.0178604411886456,
      -0.9725646651285669,
      -1.0109215033346313,
      -1.0328295089443165,
      -1.0315420924537821,
      -1.0192549878132713,
      0.0,
      -1.00277475560251,
      -1.0208184198579564,
      -0.9956559237950826,
      -0.999257859642725,
      -1.0238983622500264,
      -1.0307030322398398,
      -1.0151267276986131,
      -1.0022544249809435,
      -1.0231209927943636,
      -0.9848223067730533,
      -0.9945967887369164,
      -1.016423418980197,
      -1.003807022461498,
      -1.0121331682832433,
      -1.0246078099581937,
      -1.0483391761327552,
      -0.9871255611398619
    ],
    [
      -1.2694394364807708,
      -1.1868773666524413,
      -1.291351242541517,
      -1.2225681403913753,
      -1.2790235503418472,
      -1.278697519501949,
      -1.3435071097849838,
      -1.1992904103615736,
      -1.2350977072628722,
      -1.2621796377969035,
      -1.2856744596555894,
      -1.3597455495027597,
      0.0,
      -1.220786024207214,
      -1.2011199727266424,
      -1.2399659231414695,
      -1.2315104811643536,
      -1.2118706717880137,
      -1.3042672045312615,
      -1.21885729365822,
      -1.2516723611305132,
      -1.314495904554305,
      -1.3315014569484012,
      -1.2858507145284173,
      -1.253275695285566,
      -1.3014004378232005,
      -1.2975306627398047,
      -1.2857308007551467,
      -1.3491164451064859
    ],
    [
      -1.393265917504767,
      -1.225187723649378,
      -1.2990139758713233,
      -1.2666872604015218,
      -1.4144215385750858,
      -1.4136955187022866,
      -1.5006349924669988,
      -1.3131140697552346,
      -1.3316362647636302,
      -1.3932649594639566,
      -1.2946392787969025,
      -1.5383070620523451,
      -1.3273737293284777,
      0.0,
      -1.329780791031755,
      -1.3863750266610486,
      -1.3734908303186002,
      -1.3297427365328252,
      -1.4765383766607831,
      -1.3583991701925389,
      -1.3790388908897835,
      -1.4630241014115477,
      -1.5003868200084112,
      -1.421675588503414,
      -1.4074745620084972,
      -1.4652666484810903,
      -1.4249668847235821,
      -1.3318360623398071,
      -1.5103377946402263
    ],
    [
      -1.2846293330208827,
      -1.0871340372834661,
      -1.1610084996774832,
      -1.1124302824470809,
      -1.158529532718327,
      -1.2190887195717084,
      -1.3054258666567544,
      -1.1603409361007224,
      -1.1050760988077406,
      -1.2070903088356224,
      -1.1768987706438614,
      -1.3343970065660724,
      -1.1304729303531293,
      -1.077252280948382,
      0.0,
      -1.2266607533623566,
      -1.2287242990205796,
      -1.1816753306041152,
      -1.2780438596418269,
      -1.12052968363712,
      -1.1880505389260385,
      -1.2429976692552027,
      -1.3210399915305229,
      -1.1559752845849018,
      -1.254317546692956,
      -1.2787746733447676,
      -1.2709293533399482,
      -1.1693248246564558,
      -1.2967233282243795
    ],
    [
      -1.381821257127388,
      -1.3248189808672302,
      -1.2798489026459299,
      -1.3138981804509995,
      -1.3344318807974267,
      -1.4057823983204996,
      -1.4381778020734375,
      -1.359500096856024,
      -1.2517274996020247,
      -1.3507384035968446,
      -1.3173189736184499,
      -1.4657978064283514,
      -1.3226471504130806,
      -1.2743650366862698,
      -1.345417981441745,
      0.0,
      -1.3098634397097002,
      -1.345458954592297,
      -1.3894062491974282,
      -1.2864792692410378,
      -1.3916419265519098,
      -1.438296596572772,
      -1.3844623591298544,
      -1.399352421776829,
      -1.378568718610904,
      -1.38733763467694,
      -1.3316896328714258,
      -1.3073777758782732,
      -1.4367218207553556
    ],
    [
      -1.4645418360604396,
      -1.3559166059686516,
      -1.3816875922310932,
      -1.3934176283998874,
      -1.3599260586752984,
      -1.5111689830451016,
      -1.5640689557691692,
      -1.4033950086602172,
      -1.4178086143033117,
      -1.4198038290899262,
      -1.3966493304103693,
      -1.567468983423764,
      -1.3567053233476596,
      -1.3848033871425691,
      -1.4146771441530266,
      -1.397165776380807,
      0.0,
      -1.4178095758165907,
      -1.4889973413786524,
      -1.3982950392985272,
      -1.3860369265692973,
      -1.522650682045561,
      -1.4717593321823577,
      -1.4652692227145234,
      -1.4219526293413045,
      -1.432495597077247,
      -1.4193011342768571,
      -1.4093717249758453,
      -1.5211089977271977
    ],
    [
      -1.338521704485797,
      -1.1711448718287394,
      -1.250968974106786,
      -1.1637612764544671,
      -1.340956124399683,
      -1.3508205478721929,
      -1.38501705115219,
      -1.2581331727345986,
      -1.2595359658509724,
      -1.3348275925585344,
      -1.310833659891935,
      -1.4200063838050427,
      -1.2637395356916654,
      -1.1849373715943168,
      -1.263850066066086,
      -1.3199564281152745,
      -1.3261584948469554,
      0.0,
      -1.3952890823840625,
      -1.2506264135775258,
      -1.316410900661361,
      -1.3649313445759381,
      -1.391424873564303,
      -1.3115753789097566,
      -1.3705556333607745,
      -1.337491278625665,
      -1.3915504536491814,
      -1.2619976519823672,
      -1.4227245972416245
    ],
    [
      -1.2261657211413324,
      -1.1736860836601737,
      -1.2003788303761185,
      -1.1473286756303767,
      -1.2032902377674577,
      -1.260350871692967,
      -1.2772234404017189,
      -1.199453659265046,
      -1.2069418472622773,
      -1.1869944414653282,
      -1.1634747852922793,
      -1.3134361963329337,
      -1.15171427151295,
      -1.2003735582346167,
      -1.2371185647906495,
      -1.1858097541676884,
      -1.222244726794384,
      -1.2224896069053945,
      0.0,
      -1.2312370494271716,
      -1.2771491902827836,
      -1.285189067822277,
      -1.2659069595988375,
      -1.2023746126793613,
      -1.2392777152729162,
      -1.249531249053943,
      -1.1804434294672426,
      -1.2396524118709986,
      -1.3246457488165726
    ],
    [
      -1.2737078089587701,
      -1.1436654888678286,
      -1.2019824863730522,
      -1.1640072461971955,
      -1.2364804745224964,
      -1.3067432378102952,
      -1.3588512964064114,
      -1.248504807457645,
      -1.1781834883808875,
      -1.3066749040121042,
      -1.2369533429111002,
      -1.3790511097477016,
      -1.2049790137181873,
      -1.152221445217593,
      -1.2046177471824546,
      -1.2658069455186485,
      -1.2937814087761939,
      -1.147466217875895,
      -1.3081239800600901,
      0.0,
      -1.2303763547293247,
      -1.3201162376344102,
      -1.3341758877657368,
      -1.2262582842713072,
      -1.304159069665392,
      -1.3025226199534983,
      -1.2834817522774993,
      -1.210668645486572,
      -1.3846861740708174
    ],
    [
      -1.4705694776259433,
      -1.3344401165011468,
      -1.3820747592005889,
      -1.3595280216831322,
      -1.3796495038822163,
      -1.4590864149133183,
      -1.5201822808267405,
      -1.3692403539792224,
      -1.405747456809074,
      -1.4669181585302213,
      -1.408360023923072,
      -1.5471408828695543,
      -1.3842588530478128,
      -1.3637613900996208,
      -1.3741799549535063,
      -1.4543525285300287,
      -1.4225512599427435,
      -1.3700612616018404,
      -1.4929114512678285,
      -1.363489719024086,
      0.0,
      -1.4604518864981817,
      -1.482823324817861,
      -1.369923637264845,
      -1.4540702159237158,
      -1.4651754959076975,
      -1.4691783447461921,
      -1.4256325309321283,
      -1.5140508344234362
    ],
    [
      -1.1753459089941134,
      -1.101849443356255,
      -1.1450678096158176,
      -1.1197620841332496,
      -1.1548528147093386,
      -1.1472139928866594,
      -1.1699395960889303,
      -1.085085957222,
      -1.1753089093025098,
      -1.164258068778106,
      -1.1394182485506008,
      -1.1988809490032368,
      -1.0790897148159613,
      -1.0903334471249695,
      -1.0610059007302242,
      -1.1572612337737291,
      -1.1344391713881041,
      -1.107532062309017,
      -1.1629879654328577,
      -1.0807467539194178,
      -1.136153924102541,
      0.0,
      -1.1771039646667354,
      -1.070920532756607,
      -1.1442796668080584,
      -1.1359581514138366,
      -1.1577253311608344,
      -1.205976713823928,
      -1.124263327908393
    ],
    [
      -1.4177914166462524,
      -1.4150185932659973,
      -1.3867287622615343,
      -1.4026187447803766,
      -1.3488590555443025,
      -1.4318154433296433,
      -1.4360133991943007,
      -1.4014298895486015,
      -1.451475714088175,
      -1.374278369850857,
      -1.4073891516878965,
      -1.4706865917280414,
      -1.385547767808455,
      -1.4103024070789736,
      -1.4180208564306958,
      -1.3727863573755683,
      -1.3549188606877673,
      -1.4027525178075364,
      -1.4360905256243943,
      -1.4034107372528684,
      -1.3443189099016983,
      -1.4135938635349274,
      0.0,
      -1.4034477207006248,
      -1.3368689223460057,
      -1.3998967821994701,
      -1.3152066169210244,
      -1.4501177604263351,
      -1.3762851662235052
    ],
    [
      -1.4853921334630615,
      -1.4241655218617317,
      -1.4535749626934493,
      -1.4303212113940589,
      -1.4318291958292084,
      -1.4843806786742413,
      -1.5049257821472957,
      -1.4316582386690602,
      -1.4885609275819318,
      -1.5271048060759407,
      -1.483930982269093,
      -1.5309850642231273,
      -1.4698423385582196,
      -1.4144926985578028,
      -1.3789713665750056,
      -1.5210985308617506,
      -1.479664513041377,
      -1.441382159141403,
      -1.4977200016426724,
      -1.4036074578438642,
      -1.4251540745021085,
      -1.4822002491445718,
      -1.5010769292317987,
      0.0,
      -1.466034568319085,
      -1.51053122492861,
      -1.476145120328861,
      -1.4603667675418068,
      -1.5284188233922198
    ],
    [
      -1.6690133963410088,
      -1.6131529485916285,
      -1.6230078232310308,
      -1.618001575088257,
      -1.5869798345793256,
      -1.6859757989709756,
      -1.7453619842847226,
      -1.5790931163744306,
      -1.64744797495646,
      -1.6316640661787754,
      -1.6238964072749742,
      -1.7509350164994315,
      -1.5694718682249662,
      -1.6038316239787853,
      -1.6182220279293693,
      -1.6106758789828444,
      -1.5612130075540305,
      -1.6219227174558384,
      -1.6873976798678945,
      -1.6149905817975527,
      -1.600747513259335,
      -1.6981443577376272,
      -1.6772651156483853,
      -1.6682609960142254,
      0.0,
      -1.6444489043776114,
      -1.639031730171566,
      -1.629086825299989,
      -1.7065442602893648
    ],
    [
      -1.537493209151278,
      -1.5068140859491463,
      -1.535757697598687,
      -1.536819634782255,
      -1.5157821406813263,
      -1.547285700890162,
      -1.6610776921785957,
      -1.4628653006360348,
      -1.5410546399342984,
      -1.5311338275568311,
      -1.5434132429550376,
      -1.6758061855656106,
      -1.5469228750705863,
      -1.5296248708960145,
      -1.5409392908883774,
      -1.530983939202419,
      -1.512397580303736,
      -1.5352296085756576,
      -1.5528950251678237,
      -1.5332370965098139,
      -1.5596027671449828,
      -1.6096552782900235,
      -1.5491673516244617,
      -1.58049692063391,
      -1.5448366776935027,
      0.0,
      -1.5293179909688492,
      -1.5159913840859622,
      -1.5776283396323056
    ],
    [
      -1.4397206331350665,
      -1.4597220967099755,
      -1.4992513462713277,
      -1.4678357689612698,
      -1.4514840018083568,
      -1.5657512946392982,
      -1.5385180714134825,
      -1.4568401100023078,
      -1.4849138257059367,
      -1.4135236533828377,
      -1.4074926325233261,
      -1.5503302017149119,
      -1.4329782683168824,
      -1.4628065650109636,
      -1.493749154065306,
      -1.4179684772893262,
      -1.4336886413923364,
      -1.5074338391461604,
      -1.3981770844450634,
      -1.4594234098881838,
      -1.4823837158879312,
      -1.5489893434740847,
      -1.4751927024669098,
      -1.4795401632213476,
      -1.4590508803217248,
      -1.4405133540926598,
      0.0,
      -1.5155994875294043,
      -1.5351218289575037
    ],
    [
      -1.3593238595935841,
      -1.1812518223859652,
      -1.1863869685941972,
      -1.1860815256069477,
      -1.2374709682075058,
      -1.348901034238495,
      -1.3803047127050911,
      -1.3169107028402662,
      -1.225941097397432,
      -1.3300501703839376,
      -1.2498815708830593,
      -1.3962400879733479,
      -1.2695046500216205,
      -1.1645045835491785,
      -1.2482973869093619,
      -1.2496097441536669,
      -1.298971121150377,
      -1.195449423588759,
      -1.3336943447479035,
      -1.2139755823979008,
      -1.28483105431689,
      -1.3827955826353513,
      -1.3307574463710616,
      -1.2459597364976431,
      -1.329189030559296,
      -1.3200873504538044,
      -1.331972401296938,
      0.0,
      -1.3646565926165208
    ],
    [
      -1.2064153439913323,
      -1.1487799120407445,
      -1.1438800991081117,
      -1.1616624019643609,
      -1.162133421619781,
      -1.155186343017466,
      -1.177748421894495,
      -1.1391593514875313,
      -1.1573562556190817,
      -1.195187037338728,
      -1.1722746126788246,
      -1.2042007681953626,
      -1.1471100397930756,
      -1.1146021198522285,
      -1.1154024431525842,
      -1.1221506033315964,
      -1.1480851677097939,
      -1.1603022830546434,
      -1.1921116393727862,
      -1.1314100916828758,
      -1.1719606641489961,
      -1.1194269728273794,
      -1.1643615352720709,
      -1.1721674957148391,
      -1.2035515201580398,
      -1.1535687141348774,
      -1.1708295838409253,
      -1.1707319114757606,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.32244695952050284,
      0.2944091884321298,
      0.31039319963554446,
      0.2927538743044247,
      0.20760856220706447,
      0.1775078193311268,
      0.25290798305670625,
      0.3330278496491599,
      0.24792894655241904,
      0.3237044797690496,
      0.18355017690284425,
      0.3053518785545868,
      0.31569283448102414,
      0.292454472850165,
      0.2815634936915803,
      0.24508764833570718,
      0.3102967136793091,
      0.26737676196828364,
      0.32029236359043445,
      0.23716829234838954,
      0.23563968143313385,
      0.2573541256773444,
      0.31310048063081863,
      0.22288471709928004,
      0.2582290233670883,
      0.2883297870645065,
      0.2705785275900987,
      0.23334268622858945
    ],
    [
      0.37473093562979765,
      0.0,
      0.4884548208788695,
      0.5601532665828624,
      0.43002448736791266,
      0.3583242494265162,
      0.32453075827183264,
      0.41836849521180475,
      0.5129781707931629,
      0.3831289304456118,
      0.43863058758797524,
      0.27919034714356417,
      0.4776885992714026,
      0.5916686241021563,
      0.4866487494874967,
      0.4092841524524429,
      0.3706626131941171,
      0.531647596912169,
      0.3515732298332832,
      0.5069674842270833,
      0.3799638204584197,
      0.3309881730435664,
      0.3187871734017518,
      0.4296848616170912,
      0.3358014313009121,
      0.3641507896530858,
      0.3756432095610873,
      0.45587655321119414,
      0.2783900689675143
    ],
    [
      0.34194353968397295,
      0.5197553101638805,
      0.0,
      0.49459013383301476,
      0.43809890058889356,
      0.28619050445350447,
      0.2644508763972768,
      0.41423548562444035,
      0.455996494545698,
      0.38874058247218723,
      0.44509967191925437,
      0.2272113571131833,
      0.39214922432728505,
      0.5255709833413846,
      0.4466258353384138,
      0.34631591551980856,
      0.41280259145672016,
      0.4508186545729529,
      0.330357610772547,
      0.4228269742981354,
      0.3627548997696315,
      0.2677697466552986,
      0.2914714901503894,
      0.32756450521224956,
      0.33996404828303217,
      0.34576085423569314,
      0.3411546865970092,
      0.461017003332445,
      0.2448670687888086
    ],
    [
      0.3183446773484149,
      0.5558542750720885,
      0.4607828191249894,
      0.0,
      0.37599262499428643,
      0.32116942546068894,
      0.23554116489499966,
      0.38989819138875825,
      0.46086600093601726,
      0.36819679418658424,
      0.48671372979973704,
      0.24721605095024013,
      0.42937857245964817,
      0.5379462391710841,
      0.46480024698723343,
      0.3769069425119873,
      0.36216066926971613,
      0.5341252464877202,
      0.34056587259598214,
      0.46235968451183784,
      0.3469274628974688,
      0.31705590819546203,
      0.24412757453783018,
      0.3955813058828228,
      0.30147444499759146,
      0.30135946716688133,
      0.32181996627565246,
      0.4609092707433988,
      0.24658355408067645
    ],
    [
      0.4947908080202317,
      0.5767674830833163,
      0.4867247791097533,
      0.5817895253111431,
      0.0,
      0.4437034093409091,
      0.3659357539146957,
      0.5364761794647819,
      0.5384060990195483,
      0.49127940145127513,
      0.5102989740462622,
      0.32382978319359457,
      0.5402625020489451,
      0.5442816284451235,
      0.5268488865696843,
      0.49820138789496005,
      0.5682806917846701,
      0.5260193666791158,
      0.4397794367359431,
      0.5376158023207867,
      0.5201620381241714,
      0.3623649266016906,
      0.4490852073469571,
      0.48494126278141225,
      0.48142511402235244,
      0.42400215808564434,
      0.46711719823022024,
      0.5475165080031343,
      0.37937424892683413
    ],
    [
      0.3107129560984534,
      0.47535145147727453,
      0.40127801808609176,
      0.5028381248838631,
      0.40388261023553707,
      0.0,
      0.365509403455087,
      0.4809160875399501,
      0.495098915213227,
      0.41201134599253786,
      0.392533834801031,
      0.3337075499739903,
      0.505189185014641,
      0.5034319192735524,
      0.4968491089098901,
      0.4102271319743056,
      0.39937874838274756,
      0.4491284310041952,
      0.35925217577624324,
      0.47387294466888163,
      0.4188503921751183,
      0.41444727457038577,
      0.3292392359412504,
      0.39014920231677674,
      0.368736222106985,
      0.37170625994542106,
      0.34007890810758457,
      0.3726848671771257,
      0.3137354455813388
    ],
    [
      0.26773494598321146,
      0.24849757003727624,
      0.2768710599209483,
      0.256309287753667,
      0.26058908203079034,
      0.24930851703992984,
      0.0,
      0.29416583793996165,
      0.2292754525266445,
      0.23381777239561896,
      0.2429376748507328,
      0.2846360890109043,
      0.2334584225362526,
      0.2492017869168015,
      0.26012882133095894,
      0.24208200920277534,
      0.28169080851427686,
      0.22475437753083294,
      0.24977352689682797,
      0.3049275502476845,
      0.2420356744429839,
      0.22292589145194497,
      0.2358860130788809,
      0.22679902624960846,
      0.2647690734157291,
      0.2568193007141226,
      0.2670162103794753,
      0.21507098071264763,
      0.28150965884399337
    ],
    [
      0.33513140531384833,
      0.4496894787843988,
      0.37834953457143605,
      0.4322694059260517,
      0.38313007114037556,
      0.35564096589047467,
      0.28328465259694036,
      0.0,
      0.39913293183593335,
      0.37280908875044894,
      0.36462666771888963,
      0.2632557150502617,
      0.4303696031459001,
      0.4214630487727984,
      0.4001702896517927,
      0.3700609185016377,
      0.35159032822432157,
      0.42604307450994683,
      0.3394558357235593,
      0.4174693585756353,
      0.36610460619398655,
      0.3346089778626413,
      0.2805820086711237,
      0.35919675930511863,
      0.32836666198143005,
      0.3525885668788473,
      0.3398094109852736,
      0.3300637317839863,
      0.2947866213144954
    ],
    [
      0.4348095332919497,
      0.5446147220453271,
      0.40251453184186303,
      0.5217020042843973,
      0.437573622407297,
      0.40175708897503326,
      0.28127590454929763,
      0.43798119685215875,
      0.0,
      0.45088485132009826,
      0.4872371472755599,
      0.25232683211092954,
      0.5039244279762296,
      0.513987339571861,
      0.47243080757624845,
      0.4577461100876381,
      0.45456966130217147,
      0.48798665171782996,
      0.38641216912011145,
      0.5085177984227542,
      0.4169266390135815,
      0.29813679625272216,
      0.34688249622112655,
      0.42302418525925534,
      0.35956560286312134,
      0.3974576202276108,
      0.39539053130694324,
      0.46493097302632447,
      0.3076193442723656
    ],
    [
      0.3577767201453228,
      0.4403701845589916,
      0.44084639103628365,
      0.470005660225028,
      0.4390157608163794,
      0.4415120232469889,
      0.28507661030790343,
      0.4910505449031828,
      0.5008725388856117,
      0.0,
      0.4255582863545586,
      0.2759580140375666,
      0.5146533693182236,
      0.4668131168678422,
      0.45685078817516844,
      0.4348585941090488,
      0.513955087113587,
      0.44073370332067685,
      0.41743701003858646,
      0.41932719567221755,
      0.4380932069293826,
      0.34013867981711554,
      0.4034146135816117,
      0.37921166615416424,
      0.48228232470098753,
      0.3774134402781675,
      0.4533830132735215,
      0.4307484736367557,
      0.32693043362631946
    ],
    [
      0.4452417943262239,
      0.496588911193276,
      0.417131653094285,
      0.5154481044363133,
      0.40532658137660116,
      0.29564217556683814,
      0.2953454743849515,
      0.40104512349160215,
      0.43876620722189186,
      0.4389551861109422,
      0.0,
      0.289269019622457,
      0.4567724817236165,
      0.5376832798111684,
      0.3877875780035054,
      0.45181262367347186,
      0.4326271194169178,
      0.46215318889745793,
      0.4133399818056318,
      0.4210445162145129,
      0.3801737046266491,
      0.3327719786561394,
      0.35496385399681407,
      0.4291805761467127,
      0.41255333951005047,
      0.3613622297403445,
      0.4613656352548072,
      0.4376117073552288,
      0.3081857574680962
    ],
    [
      0.2572857790616039,
      0.2980856110797858,
      0.33778169426170146,
      0.3177692948798032,
      0.31782352234125355,
      0.31247063495552396,
      0.3577664110156027,
      0.3194095728095383,
      0.2975015671998531,
      0.2987889836903874,
      0.3110760883308983,
      0.0,
      0.3275563205416596,
      0.30951265628621316,
      0.33467515234908696,
      0.3310732165014446,
      0.30643271389414317,
      0.29962804390432973,
      0.31520434844555645,
      0.32807665116322604,
      0.30721008334980593,
      0.3455087693711163,
      0.3357342874072532,
      0.3139076571639725,
      0.32652405368267146,
      0.3181979078609263,
      0.3057232661859759,
      0.2819919000114144,
      0.3432055150043076
    ],
    [
      0.3635122740150898,
      0.4460743438434194,
      0.3416004679543436,
      0.41038357010448534,
      0.3539281601540134,
      0.35425419099391164,
      0.28944460071087685,
      0.43366130013428705,
      0.3978540032329885,
      0.3707720726989572,
      0.3472772508402713,
      0.2732061609931009,
      0.0,
      0.4121656862886467,
      0.43183173776921824,
      0.39298578735439116,
      0.40144122933150705,
      0.42108103870784697,
      0.3286845059645991,
      0.4140944168376406,
      0.38127934936534746,
      0.31845580594155565,
      0.30145025354745947,
      0.34710099596744337,
      0.3796760152102947,
      0.33155127267266016,
      0.33542104775605597,
      0.34722090974071396,
      0.28383526538937476
    ],
    [
      0.3851868536874621,
      0.5532650475428511,
      0.47943879532090583,
      0.5117655107907073,
      0.3640312326171433,
      0.3647572524899425,
      0.2778177787252303,
      0.4653387014369945,
      0.44681650642859894,
      0.38518781172827254,
      0.48381349239532656,
      0.24014570913988398,
      0.45107904186375136,
      0.0,
      0.44867198016047416,
      0.3920777445311805,
      0.4049619408736289,
      0.44871003465940396,
      0.301914394531446,
      0.42005360099969025,
      0.3994138803024456,
      0.3154286697806814,
      0.27806595118381794,
      0.3567771826888151,
      0.3709782091837319,
      0.31318612271113877,
      0.353485886468647,
      0.446616708852422,
      0.2681149765520028
    ],
    [
      0.3896424518661177,
      0.5871377476035342,
      0.5132632852095171,
      0.5618415024399195,
      0.5157422521686734,
      0.45518306531529196,
      0.36884591823024593,
      0.5139308487862779,
      0.5691956860792597,
      0.46718147605137794,
      0.49737301424313896,
      0.33987477832092794,
      0.543798854533871,
      0.5970195039386184,
      0.0,
      0.44761103152464377,
      0.4455474858664208,
      0.4925964542828851,
      0.3962279252451735,
      0.5537421012498804,
      0.48622124596096183,
      0.4312741156317976,
      0.3532317933564775,
      0.5182965003020985,
      0.4199542381940444,
      0.39549711154223277,
      0.4033424315470522,
      0.5049469602305445,
      0.3775484566626208
    ],
    [
      0.3988542381142257,
      0.45585651437438357,
      0.5008265925956839,
      0.46677731479061424,
      0.44624361444418703,
      0.37489309692111417,
      0.34249769316817624,
      0.4211753983855897,
      0.528947995639589,
      0.4299370916447691,
      0.4633565216231639,
      0.3148776888132623,
      0.45802834482853316,
      0.506310458555344,
      0.43525751379986866,
      0.0,
      0.4708120555319135,
      0.4352165406493167,
      0.39126924604418556,
      0.4941962260005759,
      0.38903356868970396,
      0.34237889866884164,
      0.39621313611175935,
      0.3813230734647848,
      0.4021067766307098,
      0.3933378605646738,
      0.4489858623701879,
      0.47329771936334053,
      0.34395367448625813
    ],
    [
      0.3940047763179215,
      0.5026300064097096,
      0.476859020147268,
      0.4651289839784738,
      0.4986205537030628,
      0.34737762933325955,
      0.294477656609192,
      0.455151603718144,
      0.44073799807504943,
      0.43874278328843497,
      0.46189728196799185,
      0.2910776289545971,
      0.5018412890307016,
      0.47374322523579204,
      0.4438694682253346,
      0.4613808359975542,
      0.0,
      0.44073703656177043,
      0.3695492709997088,
      0.460251573079834,
      0.47250968580906383,
      0.3358959303328002,
      0.3867872801960035,
      0.39327738966383774,
      0.43659398303705665,
      0.4260510153011141,
      0.43924547810150405,
      0.4491748874025159,
      0.3374376146511635
    ],
    [
      0.36970699113270555,
      0.5370838237897633,
      0.4572597215117167,
      0.5444674191640355,
      0.3672725712188196,
      0.3574081477463098,
      0.3232116444663127,
      0.4500955228839041,
      0.4486927297675303,
      0.3734011030599682,
      0.39739503572656765,
      0.2882223118134599,
      0.44448915992683724,
      0.5232913240241859,
      0.4443786295524166,
      0.3882722675032282,
      0.3820702007715473,
      0.0,
      0.3129396132344402,
      0.45760228204097686,
      0.39181779495714175,
      0.34329735104256454,
      0.31680382205419955,
      0.39665331670874604,
      0.3376730622577282,
      0.3707374169928377,
      0.31667824196932126,
      0.4462310436361354,
      0.2855040983768782
    ],
    [
      0.3643997263174501,
      0.4168793637986088,
      0.390186617082664,
      0.4432367718284058,
      0.3872752096913248,
      0.3302145757658155,
      0.3133420070570636,
      0.3911117881937365,
      0.3836236001965052,
      0.4035710059934543,
      0.42709066216650315,
      0.27712925112584874,
      0.43885117594583245,
      0.3901918892241658,
      0.35344688266813296,
      0.40475569329109407,
      0.3683207206643986,
      0.36807584055338793,
      0.0,
      0.3593283980316109,
      0.31341625717599886,
      0.30537637963650544,
      0.324658487859945,
      0.3881908347794212,
      0.3512877321858663,
      0.3410341984048395,
      0.4101220179915399,
      0.35091303558778386,
      0.2659196986422099
    ],
    [
      0.3777000626099629,
      0.5077423827009044,
      0.44942538519568087,
      0.4874006253715375,
      0.4149273970462366,
      0.3446646337584378,
      0.2925565751623216,
      0.402903064111088,
      0.47322438318784554,
      0.34473296755662886,
      0.4144545286576329,
      0.2723567618210314,
      0.44642885785054576,
      0.4991864263511401,
      0.4467901243862784,
      0.38560092605008456,
      0.3576264627925392,
      0.503941653692838,
      0.3432838915086429,
      0.0,
      0.4210315168394083,
      0.3312916339343228,
      0.3172319838029962,
      0.4251495872974258,
      0.34724880190334106,
      0.34888525161523476,
      0.36792611929123376,
      0.440739226082161,
      0.26672169749791563
    ],
    [
      0.3543453151129734,
      0.49047467623776986,
      0.4428400335383278,
      0.4653867710557844,
      0.44526528885670036,
      0.3658283778255984,
      0.3047325119121762,
      0.45567443875969427,
      0.4191673359298427,
      0.3579966342086953,
      0.41655476881584463,
      0.27777390986936235,
      0.44065593969110384,
      0.46115340263929583,
      0.45073483778541035,
      0.37056226420888794,
      0.4023635327961732,
      0.4548535311370763,
      0.3320033414710881,
      0.46142507371483066,
      0.0,
      0.36446290624073496,
      0.3420914679210556,
      0.45499115547407176,
      0.3708445768152009,
      0.3597392968312192,
      0.3557364479927245,
      0.3992822618067884,
      0.3108639583154804
    ],
    [
      0.3590516434618456,
      0.4325481090997041,
      0.38932974284014144,
      0.41463546832270937,
      0.37954473774662034,
      0.3871835595692996,
      0.3644579563670287,
      0.449311595233959,
      0.35908864315344924,
      0.3701394836778531,
      0.3949793039053582,
      0.33551660345272216,
      0.4553078376399977,
      0.4440641053309895,
      0.47339165172573483,
      0.37713631868222985,
      0.39995838106785486,
      0.426865490146942,
      0.37140958702310134,
      0.4536507985365412,
      0.3982436283534181,
      0.0,
      0.3572935877892236,
      0.46347701969935207,
      0.3901178856479006,
      0.3984394010421224,
      0.37667222129512457,
      0.3284208386320311,
      0.4101342245475661
    ],
    [
      0.4030256279412878,
      0.4057984513215429,
      0.43408828232600594,
      0.4181982998071636,
      0.4719579890432377,
      0.38900160125789696,
      0.3848036453932395,
      0.41938715503893875,
      0.36934133049936513,
      0.4465386747366833,
      0.41342789289964377,
      0.35013045285949884,
      0.43526927677908533,
      0.41051463750856665,
      0.4027961881568445,
      0.4480306872119719,
      0.4658981838997729,
      0.41806452678000383,
      0.3847265189631459,
      0.4174063073346719,
      0.4764981346858419,
      0.4072231810526128,
      0.0,
      0.4173693238869154,
      0.4839481222415345,
      0.4209202623880701,
      0.5056104276665159,
      0.3706992841612051,
      0.44453187836403507
    ],
    [
      0.342864715980558,
      0.4040913275818878,
      0.3746818867501702,
      0.39793563804956067,
      0.3964276536144111,
      0.34387617076937826,
      0.3233310672963239,
      0.3965986107745594,
      0.33969592186168773,
      0.30115204336767887,
      0.3443258671745266,
      0.29727178522049225,
      0.3584145108854,
      0.41376415088581675,
      0.4492854828686139,
      0.3071583185818689,
      0.3485923364022425,
      0.38687469030221644,
      0.33053684780094716,
      0.42464939159975534,
      0.40310277494151103,
      0.3460566002990477,
      0.32717992021182085,
      0.0,
      0.3622222811245346,
      0.3177256245150095,
      0.35211172911475863,
      0.3678900819018127,
      0.29983802605139975
    ],
    [
      0.3837970244957061,
      0.43965747224508633,
      0.42980259760568407,
      0.4348088457484578,
      0.46583058625738927,
      0.36683462186573923,
      0.3074484365519923,
      0.4737173044622842,
      0.40536244588025494,
      0.4211463546579395,
      0.42891401356174064,
      0.3018754043372833,
      0.48333855261174863,
      0.4489787968579295,
      0.4345883929073455,
      0.44213454185387047,
      0.49159741328268436,
      0.43088770338087645,
      0.36541274096882037,
      0.4378198390391621,
      0.4520629075773799,
      0.35466606309908766,
      0.3755453051883295,
      0.38454942482248944,
      0.0,
      0.40836151645910346,
      0.41377869066514883,
      0.4237235955367258,
      0.34626616054735004
    ],
    [
      0.43895254498251957,
      0.4696316681846513,
      0.44068805653511056,
      0.4396261193515427,
      0.4606636134524713,
      0.4291600532436357,
      0.31536806195520195,
      0.5135804534977628,
      0.4353911141994993,
      0.4453119265769665,
      0.43303251117876007,
      0.3006395685681871,
      0.4295228790632113,
      0.44682088323778313,
      0.4355064632454202,
      0.4454618149313787,
      0.4640481738300617,
      0.44121614555814004,
      0.4235507289659739,
      0.44320865762398376,
      0.41684298698881483,
      0.3667904758437741,
      0.4272784025093359,
      0.39594883349988774,
      0.4316090764402949,
      0.0,
      0.4471277631649484,
      0.4604543700478354,
      0.398817414501492
    ],
    [
      0.4452620356819206,
      0.4252605721070115,
      0.38573132254565934,
      0.4171468998557173,
      0.43349866700863027,
      0.3192313741776889,
      0.34646459740350455,
      0.4281425588146792,
      0.4000688431110504,
      0.47145901543414936,
      0.47749003629366094,
      0.3346524671020752,
      0.4520044005001047,
      0.42217610380602344,
      0.391233514751681,
      0.4670141915276609,
      0.45129402742465063,
      0.3775488296708267,
      0.48680558437192367,
      0.4255592589288033,
      0.4025989529290559,
      0.3359933253429024,
      0.40978996635007725,
      0.40544250559563944,
      0.42593178849526225,
      0.44446931472432727,
      0.0,
      0.36938318128758274,
      0.34986083985948335
    ],
    [
      0.31091976665399534,
      0.48899180386161434,
      0.48385665765338226,
      0.4841621006406318,
      0.4327726580400737,
      0.32134259200908444,
      0.28993891354248835,
      0.35333292340731326,
      0.4443025288501474,
      0.3401934558636419,
      0.4203620553645202,
      0.27400353827423163,
      0.400738976225959,
      0.505739042698401,
      0.4219462393382176,
      0.4206338820939126,
      0.37127250509720255,
      0.47479420265882055,
      0.336549281499676,
      0.4562680438496787,
      0.38541257193068956,
      0.2874480436122282,
      0.33948617987651786,
      0.42428388974993636,
      0.34105459568828356,
      0.35015627579377506,
      0.33827122495064144,
      0.0,
      0.30558703363105866
    ],
    [
      0.347617297601559,
      0.40525272955214686,
      0.4101525424847796,
      0.39237023962853046,
      0.3918992199731104,
      0.3988462985754253,
      0.3762842196983964,
      0.41487329010536,
      0.3966763859738096,
      0.35884560425416323,
      0.38175802891406674,
      0.34983187339752875,
      0.40692260179981576,
      0.4394305217406629,
      0.43863019844030715,
      0.4318820382612949,
      0.4059474738830975,
      0.3937303585382479,
      0.36192100222010515,
      0.4226225499100156,
      0.3820719774438952,
      0.434605668765512,
      0.38967110632082047,
      0.3818651458780522,
      0.35048112143485155,
      0.40046392745801396,
      0.38320305775196606,
      0.38330073011713073,
      0.0
    ]
  ],
  "row_avgs": [
    0.2714636617125468,
    0.41299793500123877,
    0.37807517676596836,
    0.3808806493903499,
    0.4874028771627556,
    0.4103856339529816,
    0.25367830078412434,
    0.36285891855934116,
    0.4231495210418502,
    0.42372455182611407,
    0.41000534939737526,
    0.3162829179553234,
    0.36465156119716063,
    0.3902536077730927,
    0.4695024370137002,
    0.42521302558124136,
    0.42482328164745925,
    0.3954520231189385,
    0.3664982079235755,
    0.3939704617169792,
    0.3974215734629968,
    0.3985846365711007,
    0.42197165515018925,
    0.3592019805688568,
    0.41260381258812895,
    0.4284375271849516,
    0.4107683633964911,
    0.38585074938771874,
    0.39396990036152385
  ],
  "col_avgs": [
    0.37026237288844044,
    0.4605856427596682,
    0.42089912491626397,
    0.45423357459571306,
    0.40750401937999486,
    0.3544065999350465,
    0.31254457547748166,
    0.4240871877152664,
    0.4257182028533294,
    0.3861732638631338,
    0.415425693149381,
    0.2885263153276082,
    0.4379802245033889,
    0.46113584340587044,
    0.42602250153610516,
    0.3997439585616555,
    0.4046782430144568,
    0.43066175451775485,
    0.3588325871616263,
    0.43661345866753004,
    0.39242600193858096,
    0.3365357804691495,
    0.33893952586757764,
    0.39310848814996147,
    0.37235983215909924,
    0.36248583882750746,
    0.37873394540426525,
    0.40326054753466023,
    0.31619519361355813
  ],
  "combined_avgs": [
    0.3208630173004936,
    0.43679178888045345,
    0.3994871508411162,
    0.41755711199303147,
    0.44745344827137523,
    0.38239611694401404,
    0.283111438130803,
    0.39347305313730374,
    0.4244338619475898,
    0.4049489078446239,
    0.4127155212733781,
    0.3024046166414658,
    0.4013158928502748,
    0.42569472558948157,
    0.4477624692749027,
    0.41247849207144843,
    0.41475076233095803,
    0.4130568888183467,
    0.3626653975426009,
    0.4152919601922546,
    0.39492378770078884,
    0.3675602085201251,
    0.38045559050888345,
    0.3761552343594091,
    0.39248182237361406,
    0.3954616830062295,
    0.39475115440037817,
    0.3945556484611895,
    0.355082546987541
  ],
  "gppm": [
    569.6991387340091,
    546.0866879748015,
    560.9659587787207,
    550.5306728168117,
    570.1939918076974,
    595.8161571891148,
    608.6995483799111,
    559.8593780061195,
    566.0069555164316,
    582.7801526220439,
    566.0002068990402,
    624.4002716486292,
    556.905903093085,
    545.2497231716119,
    563.6108228541963,
    573.9300628696702,
    572.175716732664,
    559.3539418658413,
    594.3279324261824,
    557.5569903242146,
    576.3488077119763,
    604.4098707169518,
    601.5530493185469,
    574.4745568985012,
    581.5277747420577,
    590.2464986535476,
    585.0293806030925,
    572.6433906007461,
    614.5353184499897
  ],
  "gppm_normalized": [
    1.2636401697876707,
    1.2714222399223758,
    1.312146545082132,
    1.2785338173574279,
    1.3214256094019614,
    1.3781121459576955,
    1.4140566069899136,
    1.2902938273761624,
    1.3096753456124433,
    1.3472054116716985,
    1.3037483428704228,
    1.442740099444369,
    1.2925981232001191,
    1.2633853939668074,
    1.3045644480194292,
    1.3227902807047733,
    1.3230821167768652,
    1.2974197870455808,
    1.3701587502124524,
    1.2948582391127827,
    1.329017623208402,
    1.3899802027273371,
    1.3831366561593603,
    1.323610194165997,
    1.3441737217467438,
    1.3586856868127941,
    1.3488597688062698,
    1.325825149198993,
    1.4115320959972544
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391,
    303,
    497,
    433,
    459,
    752,
    459,
    495,
    438,
    433,
    410,
    431,
    387,
    410,
    444,
    431,
    438,
    393,
    372,
    470,
    373,
    439,
    420,
    389,
    490,
    384,
    410,
    374,
    372,
    394,
    324,
    414,
    458,
    484,
    436,
    428,
    501,
    399,
    399,
    381,
    466,
    408,
    398,
    376,
    425,
    390,
    398,
    422,
    389,
    407,
    452,
    424,
    401,
    424,
    397,
    379,
    413,
    401,
    344,
    781,
    392,
    394,
    392,
    352,
    525,
    360,
    385,
    392,
    397,
    413,
    380,
    455,
    358,
    381,
    399,
    424,
    396,
    397,
    404,
    410,
    394,
    346,
    390,
    420,
    367,
    398,
    386,
    349,
    693,
    449,
    447,
    450,
    437,
    450,
    518,
    423,
    431,
    416,
    460,
    437,
    476,
    460,
    444,
    463,
    451,
    458,
    421,
    440,
    470,
    414,
    457,
    429,
    431,
    416,
    443,
    439,
    406,
    1366,
    431,
    451,
    431,
    382,
    435,
    379,
    402,
    392,
    554,
    420,
    469,
    441,
    450,
    466,
    420,
    414,
    410,
    438,
    442,
    378,
    440,
    388,
    437,
    397,
    387,
    391,
    431,
    399,
    1428,
    431,
    434,
    454,
    409,
    434,
    388,
    406,
    484,
    448,
    357,
    618,
    424,
    397,
    365,
    453,
    437,
    439,
    452,
    382,
    412,
    347,
    390,
    420,
    459,
    322,
    408,
    451,
    409,
    557,
    395,
    451,
    431,
    438,
    437,
    417,
    383,
    398,
    374,
    428,
    387,
    416,
    435,
    406,
    409,
    457,
    395,
    425,
    379,
    415,
    441,
    340,
    408,
    417,
    426,
    414,
    438,
    335,
    468,
    442,
    473,
    474,
    383,
    401,
    465,
    446,
    413,
    388,
    399,
    481,
    480,
    453,
    411,
    415,
    389,
    446,
    406,
    459,
    411,
    398,
    367,
    435,
    415,
    350,
    346,
    429,
    364
  ],
  "response_lengths": [
    2370,
    2494,
    2651,
    2634,
    2186,
    2302,
    2811,
    2525,
    2330,
    2279,
    2384,
    2744,
    2859,
    2609,
    2350,
    2489,
    2241,
    2529,
    2359,
    2680,
    2261,
    2260,
    2179,
    2452,
    2441,
    1961,
    2073,
    2379,
    2076
  ]
}