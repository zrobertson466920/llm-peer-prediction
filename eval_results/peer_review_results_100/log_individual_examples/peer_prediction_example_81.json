{
  "example_idx": 81,
  "reference": "Under review as a conference paper at ICLR 2023\n\nQUASICONVEX SHALLOW NEURAL NETWORK\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nDeep neural networks generally have highly non-convex structures, resulting in multiple local optima of network weights. The non-convex network is likely to fail, i.e., being trapped in bad local optima with large errors, especially when the task involves convexity (e.g., linearly separable classification). While convexity is essential in training neural networks, designing a convex network structure without strong assumptions (e.g., linearity) of activation or loss function is challenging. To extract and utilize convexity, this paper presents the QuasiConvex shallow Neural Network (QCNN) architecture with mild assumptions. We first decompose the network into building blocks where quasiconvexity is thoroughly studied. Then, we design additional layers to preserve quasiconvexity where such building blocks are integrated into general networks. The proposed QCNN, interpreted as a quasiconvex optimization problem, allows for efficient training with theoretical guarantees. Specifically, we construct equivalent convex feasibility problems to solve the quasiconvex optimization problem. Our theoretical results are verified via extensive experiments on common machine learning tasks. The quasiconvex structure in QCNN demonstrates even better learning ability than non-convex deep networks in some tasks.\n\n1\n\nINTRODUCTION\n\nNeural networks have been at the heart of machine learning algorithms, covering a variety of applications. In neural networks, the optimal network weights are generally found by minimizing a supervised loss function using some form of stochastic gradient descent (SGD) (Saad (1998)), in which the gradient is evaluated using the backpropagation procedure (LeCun et al. (1998)). However, the loss function is generally highly non-convex, especially in deep neural networks, since the multiplication of weights between hidden layers and non-linear activation functions tend to break the convexity of the loss function. Therefore, there are many local optima solutions of network weights (Choromanska et al. (2015)). While some experiments show that certain local optima are equivalent and yield similar learning performance, the network is likely to be trapped in bad local optima with a large loss.\n\nIssue 1: Is non-convex deep neural networks always better?\n\nDeep neural networks have shown success in many machine learning applications, such as image classification, speech recognition, and natural language processing (Hinton & Salakhutdinov (2006); Ciregan et al. (2012), Hinton et al. (2012), and Kingma et al. (2014)). Many people believe that the multiple layers in deep neural networks allow models to learn more complex features and perform more intensive computational tasks. However, deep neural networks are generally highly non-convex in the loss function, which makes the training burdensome. Since the loss function has many critical points, which include spurious local optima and saddle points (Choromanska et al. (2015)), it hinders the network from finding the global optima and makes the training sensitive to the initial guess. In fact, (Sun et al. (2016)) pointed out that increasing depth in neural networks is not always good since there is a trade-off between non-convex structure and representation power. In some engineering tasks requiring additional physical modeling, simply applying deep neural networks is likely to fail. Even worse, we usually don’t know how to improve the deep neural networks during a failure since it is a black box procedure without many theoretical guarantees.\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n(Left) The motivation and challenge of this study.\n\nFigure 1: Proposed Method. (Middle) We design a quasiconvex neural network structure to efficiently train for optimal network weights in a quasiconvex optimization problem. The quasiconvexity is studied and preserved via special pooling layers. (Right) Unlike non-convex loss function, the quasiconvex loss function of our design allows for finding the global optima.\n\nIssue 2: Solution to non-convexity is not practical.\n\nTo overcome non-convexity in neural networks, new designs of network structure were proposed. The first line of research focused on specific activation functions (e.g., linear or quadratic) and specific target functions (e.g., polynomials) (Andoni et al. (2014)) where the network structure can be convexity. However, such methods were limited in practical applications (Janzamin et al. (2015)). Another line of research aimed at deriving the dual problem of the optimization problem formulated by neural network training. Unlike the non-convex neural network, its dual problem is usually convex. Then, conditions ensuring strong duality (zero duality gap and dual problem solvable) were discussed to find the optimal solution to the neural network. For example, Ergen & Pilanci (2020) derived the dual problem for neural networks with ReLU activation, and Wang et al. (2021) showed that parallel deep neural networks have zero duality gap. However, the derivation of strong duality in the literature requires the planted model assumption, which is impractical in many real-world datasets. Aside from studying the convexity in network weights, some work explored the convexity in data input and label. For instance, an input convex structure with given weights Amos et al. (2017) altered the neural network output to be a convex function of (some of) the inputs. Nevertheless, such a function is only an inference procedure with given network weights.\n\nIn this work, we introduce QCNN, the first QuasiConvex shallow Neural Network structure that learns the optimal weights in a quasiconvex optimization problem. We first decompose a general neural network (shown in the middle of Figure 1) into building blocks (denoted by distinct colors). In each building block, the multiplication of two weights, as well as the non-linear activation function in the forward propagation, makes the building block non-convex. Nevertheless, inspired by Boyd et al. (2004), we notice that the multiplication itself is quasiconcave if the activation function is ReLU. The quasiconvexity (quasiconcavity) is a generalization of convexity (concavity), which shares similar properties, and hence, is a desired property in the neural network. To preserve quasiconcavity in the network structure when each building block is integrated, we design special layers (e.g., minimization pooling layer), as shown in the middle of Figure 1. In doing so, we arrive at a quasiconvex optimization problem of training the network, which can be equivalently solved by tackling convex feasibility problems. Unlike non-convex deep neural networks, the quasi-convexity in QCNN enables us to learn the optimal network weights efficiently with guaranteed performance.\n\n2 RELATED WORK Failure of training non-convex neural networks. In training a non-convex neural network, the commonly used method, such as gradient descent in the backpropagation procedure, can get stuck in bad local optima and experience arbitrarily slow convergence (Janzamin et al. (2015)). Explicit examples of the failure of network training and the presence of bad local optima have been discussed in (Brady et al. (1989); Frasconi et al. (1993)). For instance, Brady et al. (1989) constructed simple cases of linearly separable classes that backpropagation fails. Under non-linear separability setting, Gori & Tesi (1992) also showed failure of backpropagation. These studies indicate that deep neural\n\n2\n\nProblem: Extract/Utilize convexity in general neural networksChallenge: Multiplication of weights and non-linear activationsData:MotivationQuasiconvexityLoss Function in Parameter SpaceNonconvexityQCNNMinpool to preserve quasiconcavity1Quasiconcavity in product of ReLU23Solve quasiconvex as convex problemminvector input scalar labelUnder review as a conference paper at ICLR 2023\n\nnetworks are not suitable for all tasks. Therefore, it motivates us to think: can simple networks with convex structure beat deep networks with non-convex structure in some tasks?\n\nConvexity in neural network. The lack of convexity has been seen as one of the major issues of deep neural networks (Bengio et al. (2005)), drawing much research in the machine learning community. Many people studied convex structures and convex problems in neural networks. For instance, (Bengio et al. (2005); Andoni et al. (2014); Choromanska et al. (2015); Milne (2019); Rister & Rubin (2017)) showed that training a neural network under some strong conditions can be viewed as a convex optimization problem. (Farnia & Tse (2018); Ergen & Pilanci (2020); Wang et al. (2021); Pilanci & Ergen (2020)) studied convex dual problems of the neural network optimization and derived strong duality under specific assumptions.\n\nAside from directly studying convexity in neural networks, people also discussed conditions where the local optima become global. For example, Haeffele & Vidal (2015) presented that if the network is over-parameterized (i.e., has sufficient neurons) such that there exist local optima where some of the neurons have zero contribution, then such local optima is global optima (Janzamin et al. (2015)). Similarly, Haeffele & Vidal (2017) also showed that all critical points are either global minimizers or saddle points if the network size is large enough. However, such studies only provide theoretical possibilities while efficient algorithms to solve (infinitely) large networks are missing. Based on the existing literature, we restrict our research to finding a simple but practical neural network with some convexity to provide performance guarantees.\n\nQuasiconvex optimization problem. The study of quasiconvex functions, as well as quasiconvex optimization problems, started from (Fenchel & Blackett (1953); Luenberger (1968)) and has become popular nowadays since the real-world function is not always convex. For instance, quasiconvex functions have been of particular interest in economics (Agrawal & Boyd (2020)), modeling the utility functions in an equilibrium study (Arrow & Debreu (1954); Guerraggio & Molho (2004)). The quasiconvex optimization has been applied to many applications recently, including engineering (Bullo & Liberzon (2006)), model order reduction (Sou), computer vision (Ke & Kanade (2007)) and machine learning (Hazan et al. (2015)). Among many solutions to quasiconvex optimization problems, a simple algorithm is bisection (Boyd et al. (2004)), which solves equivalent convex feasibility problems iteratively until converging.\n\n3 PRELIMINARY\n\nTo model for the general case, we consider a L-layer network with layer weights Wl\n\nml , [L], where m0 = d and mL = 1 are the input and output dimensions, respectively. As the l\n∀ dimensions suggest, the input data is a vector, and the output is a scalar. Given a labeled dataset\n\n∈\n\n∈\n\n×\n\nRml−1\n\ni=1 with n samples, we consider a neural network with the following architecture.\n\n=\n\n{\n\nD\n\n(xi, yi)\n\n}\n\nn\n\nfθ(X) = hL, hl = g(hl\n\n1Wl),\n\nwhere hl denotes the layer activation and h0 = X are the network weights which need to be optimized via training, and g( ·\nfunction. The network is trained with L2 loss as follows:\n\n∈\n\n×\n\n− Rn\n\n[L]\n\nl ∀\n\n∈\n\nd is the data matrix. Here, θ =\n\n(1) L\nl=1 ) is the non-linear activation\n\nWl\n\n{\n\n}\n\nˆθ = arg min\n\nθ\n\n1 2 ∥\n\nfθ(X)\n\n2\n\n2,\n\ny\n\n∥\n\n−\n\n(2)\n\n∈\n\nRn is the data label vector. The loss function in Equation 2 is generally non-convex where y because of the multiplication of weights as well as non-linear activation functions. As discussed previously, non-convexity will likely cause the network to be trapped in a bad local optima with large errors. Therefore, we still want to extract some convexity in this loss function to help with the training process. In this paper, we will show that quasiconvexity and quasiconcavity are hidden in the network. These properties can be utilized to construct a convex optimization problem to train the optimal network weights. Here, we introduce the definition of quasiconvexity and quasiconcavity. Definition 1. A function f : Rd dom f ,\nα f (x) }\nevery superlevel set\n\nα are convex. Similarly, a function is quasiconcave if x\n\nR is quasiconvex if its domain and all its sublevel sets\n\n∈ f is quasiconvex, i.e.,\n\nis convex.\n\nf (x)\n\n→\n\n−\n\n≤\n\nα\n\nx\n\n{\n\n|\n\n∀ {\n\n|\n\n≥\n\n}\n\nWe also note, since convex functions always have convex sublevel sets, they are naturally quasiconvex, while the converse is not true. Therefore, the quasiconvexity can be regarded as a generalization of convexity, which is exactly what we seek in the non-convex deep neural networks.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n4 QUASI-CONCAVE STRUCTURE\n\nFor designing a quasiconvex structure of neural networks, we start by considering the simplest and most representative building block in a network and analyze its characteristics. Specifically, we consider the network\n\nf (w1; w2) = g(g(x⊤w1)w2),\n\n(3)\n\n∈\n\nRd is the input data, w1 ∈\n\nR are the weights for two hidden layers. where x We analyze such a two-layer structure because it is the simplest case in neural networks yet can be generalized to deep neural networks. In Equation 3, the network is not convex in weights (w1; w2) because (1) the network function contains the multiplication of the weights and (2) the activation function g( ·\n\nRd and w2 ∈\n\n) is usually non-linear.\n\nNevertheless, we still want to explore the potential possibilities of the network becoming convex or being related to convex. Inspired by Boyd et al. (2004), we notice that although the multiplication of two weights in the forward propagation makes the network non-convex, the multiplication itself is quasiconcave in specific circumstances. For example, the product of two variables forms the shape of a saddle, which is not convex in those variables. However, if we restrict these two variables to be positive, the saddle shape will reduce to a quasiconcave surface, as shown in Figure 2. Lemma 1. The function f (w1, w2) = w1w2 with dom f = R2\n\n+ is quasiconcave.\n\nFigure 2: (Left) The function f (w1, w2) = w1w2 has a saddle shape. (Middle) Constrained on positive domain, i.e., dom f = R2 +, the function becomes quasiconcave. (Right) The quasiconcave function always has convex superlevel sets.\n\nMotivated by Lemma 1 and Figure 2, to preserve the property of quasiconcavity of the network in Equation 3, a straightforward approach is to assume the network weights (w1; w2) to be nonnegative, like in (Amos et al. (2017)). However, this assumption will significantly reduce the neural network’s representation power. In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1/2m representation power. To bypass this impractical assumption, we notice that some activation functions naturally restrict the output to be non-negative. For example, the ReLU activation function g(x) = max forces the negative input to be zero. Therefore, we can demonstrate that the network in Equation 3 with ReLU activation function is quasiconcave in the network weights, as shown in Theorem 1.\n\n0, x\n\n}\n\n{\n\nTheorem 1. The neural network in Equation 3 with ReLU activation function g( ) is quasiconcave ·\nin the network weights (w1; w2).\n\nProof. To prove quasiconcavity, we need to show that all superlevel sets\n\nSα =\n\n(w1; w2) |\n\n{\n\nf (w1; w2)\n\nα\n\n, }\n\n≥\n\nR\n\nα\n\n∀\n\n∈\n\nare convex sets. When α 0, the superlevel set is the complete set, i.e., Sα = dom f due to the ReLU activation function. Hence, Sα is evidently convex. When α > 0, the superlevel set is neither the empty set nor the complete set. For any two elements ( ˆw1; ˆw2), ( ̃w1; ̃w2) Sα, we aim to show (0, 1). From the condition α > 0, we that (λ ˆw1 + (1\n\nλ) ̃w1; λ ˆw2 + (1\n\nSα for λ\n\nλ) ̃w2)\n\n≤\n\n∈\n\n−\n\n−\n\n∈\n\n∈\n\n4\n\nSaddle ShapeQuasiconcavityConvex superlevesetconstrainedUnder review as a conference paper at ICLR 2023\n\nknow that x⊤ ˆw1 > 0 and ˆw2 > 0, as well as x⊤ ̃w1 > 0 and ̃w2 > 0. Therefore, we would have\n\nf (λ ˆw1 + (1 = λ2x⊤ ˆw1 ˆw2 + (1\n\n−\n\nλ) ̃w1; λ ˆw2 + (1\n\n− λ)2x⊤ ̃w1 ̃w2 + λ(1\n\nλ) ̃w2) = (cid:2)λx⊤ ˆw1 + (1\n\nλ)x⊤ ̃w1 −\nλ) (cid:2)x⊤ ˆw1 ̃w2 + x⊤ ̃w1 ˆw2\n\n(cid:3) [λ ˆw2 + (1 (cid:3)\n\nλ) ̃w2]\n\n−\n\nλ2α + (1\n\nλ2α + (1\n\nλ2α + (1\n\n−\n\n−\n\n−\n\n≥\n\n≥\n\n≥\n\n− λ)2α + λ(1\n\n− λ) (cid:2)x⊤ ˆw1 ̃w2 + x⊤ ̃w1 ˆw2\n\n(cid:3)\n\nλ)2α + λ(1\n\nλ)2α + λ(1\n\n−\n\n−\n\n−\n\nλ)\n\nλ)\n\n(cid:21)\n\n ̃w2 +\n\n(cid:20) α ˆw2 2α = α[λ2 + (1\n\nα ̃w2\n\nˆw2\n\n×\n\nλ)2 + 2λ(1\n\n−\n\nλ)] = α.\n\n−\n\nIn Theorem 1, we show that the simple two-layer network in Equation 3 is quasiconcave in network weights, given that the activation function is ReLU. Then, a natural question arises: does this property of quasiconcavity remain in deeper networks? Unfortunately, the quasiconcavity does not hold in more complex neural networks due to one fact: the summation of quasiconvex (quasiconcave) functions is not necessarily quasiconvex (quasiconcave). The deeper networks can be regarded as weighted summations of many networks in Equation 3, hence, not quasiconcave anymore. Therefore, we aim to design new network structures to preserve the property of quasiconcavity to more general neural networks.\n\nTo achieve this goal, we focus on the operations that preserve quasiconcavity, including (1) the composition of a non-decreasing convex function, (2) the non-negative weighted minimization, and (3) the supremum over some variables. Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient. Specifically, we can apply a minimization pooling layer to integrate the simple networks in Equation 3, as shown in Figure 3. In doing so, we manage to extend the network in the simplest building block to more general structures, where quasiconcavity is ensured by Lemma 2 and visually explained in Figure 4. Meanwhile, we note that the proposed network is still a shallow network. Although infinitely stacking layers with appropriate minimization pooling layers can also keep the entire network convex, too many minimization pooling layers will damage the representation power of the neural network.\n\nFigure 3: The structure of quasiconvex shallow neural network (QCNN).\n\nLemma 2. Provided that f1, · · · non-negative weighted minimum\n\n, fn are quasiconcave functions defined on the same domain, the\n\nis quasiconcave given a1,\n\n, an\n\n∈\n\n· · ·\n\nR+.\n\nf := min\n\na1f1, a2f2,\n\n, anfn\n\n}\n\n· · ·\n\n{\n\nProof. The superlevel set Sα =\n\nx\n\n{\n\n∈\n\ndom f\n\nf (x) |\n\nα\n\n}\n\n≥\n\nof f can be regarded as:\n\nSα =\n\nx\n\n{\n\n∈\n\ndom f\n\nmin\n\n|\n\na1f1(x), a2f2(x),\n\n{\n\n, anfn(x)\n\n} ≥\n\n=\n\nα\n\n}\n\n· · ·\n\nn\n\ni=1{\n\n∩\n\nx\n\ndom f\n\nfi |\n\n≥\n\n∈\n\nα ,\nai }\n\nwhich is the intersection of (convex) superlevel sets of fi(i = 1,\n\n, n).\n\n· · ·\n\n5\n\ndataoutput<latexit sha1_base64=\"6BwYjUKZ9Za63neeFZ+kfEr0/oQ=\">AAACNHicbVDLSgMxFM3UV62vUZdugkVoN2WmFBVEKLoR3FSwD2jrkEkz09DMgySjlqEf5cYPcSOCC0Xc+g1m2kH68ELg3HPOJfceO2RUSMN40zJLyyura9n13Mbm1vaOvrvXEEHEManjgAW8ZSNBGPVJXVLJSCvkBHk2I017cJnozXvCBQ38WzkMSddDrk8dipFUlKVfO4WOh2TfduKHkWWejRvuJU25eA7dgvunP47uOjIIp+3FGbul542SMS64CMwU5EFaNUt/6fQCHHnEl5ghIdqmEcpujLikmJFRrhMJEiI8QC5pK+gjj4huPD56BI8U04NOwNXzJRyz0xMx8oQYerZyJkuKeS0h/9PakXROuzH1w0gSH08+ciIGZQCTBGGPcoIlGyqAMKdqV4j7iCMsVc45FYI5f/IiaJRL5nGpclPJVy/SOLLgAByCAjDBCaiCK1ADdYDBE3gFH+BTe9betS/te2LNaOnMPpgp7ecXoASsEA==</latexit>f(w1;w2)=g(g(x>w1)w2)Simple NNGeneral NNQuasiconcavityminimization pooling<latexit sha1_base64=\"hpnySaJh+/CAhHti5mNvHw101yA=\">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI9FLx4r2A9oQ9lsN83S3U3Y3Qgl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5QcKZNq777ZQ2Nre2d8q7lb39g8Oj6vFJV8epIrRDYh6rfoA15UzSjmGG036iKBYBp71gepf7vSeqNIvlo5kl1Bd4IlnICDa5NEwiNqrW3Lq7AFonXkFqUKA9qn4NxzFJBZWGcKz1wHMT42dYGUY4nVeGqaYJJlM8oQNLJRZU+9ni1jm6sMoYhbGyJQ1aqL8nMiy0nonAdgpsIr3q5eJ/3iA14Y2fMZmkhkqyXBSmHJkY5Y+jMVOUGD6zBBPF7K2IRFhhYmw8FRuCt/ryOule1b1mvfHQqLVuizjKcAbncAkeXEML7qENHSAQwTO8wpsjnBfn3flYtpacYuYU/sD5/AEWdo5I</latexit><latexit sha1_base64=\"BTnZDRVd2omL9I9zBg/ZFxhgY2U=\">AAACOnicbVDLSgMxFM34rPVVdekmWIR2U2ZKUUGEohuXLdgHdMYhk2ba0MyDJKOWYb7LjV/hzoUbF4q49QNM2yn04QmBk3PvIfceJ2RUSF1/01ZW19Y3NjNb2e2d3b393MFhUwQRx6SBAxbwtoMEYdQnDUklI+2QE+Q5jLScwc2o3nogXNDAv5PDkFge6vnUpRhJJdm5ulswPST7jhu3Etu4nD4eE7tcvIJm2KeFnjpT/Sm5N2UQznqKc56incvrJX0MuEyMlORBipqdezW7AY484kvMkBAdQw+lFSMuKWYkyZqRICHCA9QjHUV95BFhxePVE3iqlC50A66uL+FYnXXEyBNi6DmqczSlWKyNxP9qnUi6F1ZM/TCSxMeTj9yIQRnAUY6wSznBkg0VQZhTNSvEfcQRlirtrArBWFx5mTTLJeOsVKlX8tXrNI4MOAYnoAAMcA6q4BbUQANg8AzewSf40l60D+1b+5m0rmip5wjMQfv9A1+srcw=</latexit>f(W1;w2)=(g(g(x>W1)w2))<latexit sha1_base64=\"mMKJOLXPCPvBneM50MTKqFL5HUw=\">AAACWnicbVFZSwMxEM6u2tOjHm++BIvQfSm7taggQtEXHyvYA7p1yabZNjR7mGTVsuyf9EUE/4pgeii1dULCN/PNTCZf3IhRIU3zQ9M3Nrcy2Vy+UNze2d0r7R+0RRhzTFo4ZCHvukgQRgPSklQy0o04Qb7LSMcd3075zjPhgobBg5xEpO+jYUA9ipFUIaf05FVsH8mR6yWd1LGulpzar/OSOmfGNbSjEa3MjqFaP+Rr+mjLMFruYix3MYw/bQynVDar5szgOrAWoAwW1nRKb/YgxLFPAokZEqJnmZHsJ4hLihlJC3YsSITwGA1JT8EA+UT0k5k0KTxVkQH0Qq52IOEsulyRIF+Iie+qzOmUYpWbBv/jerH0LvsJDaJYkgDPL/JiBmUIpzrDAeUESzZRAGFO1awQjxBHWKrfKCgRrNUnr4N2rWqdV+v39XLjZiFHDhyDE1ABFrgADXAHmqAFMHgHX1pGy2qfuq7n9eI8VdcWNYfgj+lH341ZtMI=</latexit>f(W1;W2;w3)=((g(g(x>W1)W2))w3))Under review as a conference paper at ICLR 2023\n\nFigure 4: The minimization of quasiconcave functions on the same domain is still quasiconcave.\n\n5 QUASICONVEX OPTIMIZATION OF NEURAL NETWORK\n\nIn Section 4, we design a neural network structure where output f (θ) is a quasiconcave function over the network weights θ. To further utilize the property of quasiconcavity, in this section, we propose to train the neural network as a quasiconvex optimization problem. Even though function f (θ) is quasiconcave, the optimization problem in Equation 2 is not quasiconvex, since the L2 loss is not monotonic. However, if we restrict the network output to be smaller than the network labels, y, the L2 loss is non-increasing in this range. Therefore, the resulting loss function in i.e., f (θ) Equation 2, as a composition of a convex non-increasing function over a quasiconcave function, is quasiconvex. That is, the training of QCNN is an unconstrained quasiconvex optimization problem\n\n≤\n\nP ∗ = min\n\nθ\n\nl(θ) = min\n\nθ\n\n1 2 ∥\n\nf (θ)\n\ny\n\n2.\n\n2 ∥\n\n−\n\n(4)\n\nTo solve the quasiconvex optimization problem in Equation 4, we can transform it into an equivalent R be a family of convex functions convex feasibility problem. Let φt(θ) := y t\n0. Then, the quasiconvex optimization problem in Equation 4 can satisfying l(θ) φt(θ) be equivalently considered as\n\nf (θ), t\n\n⇐⇒\n\n−\n\n−\n\n≤\n\n≤\n\n∈\n\nt\n\nmin θ\n\nl(θ)\n\n⇐⇒\n\nmin θ,t\n\nt\n\ns.t.\n\nl(θ)\n\n=\n\n⇒\n\nt\n\n≤\n\nfind\n\nθ\n\n(5)\n\ns.t. φt(θ)\n\n0.\n\n≤\n\nThe problem in Equation 5 is a convex feasibility problem since the inequality constraint function is convex. For every given value t, we can solve the convex feasibility problem. If the convex 0, this point θ is also feasible for the quasiconvex feasibility problem is feasible, i.e., θ, φt(θ) problem by satisfying l(θ) t. In this circumstance, we can reduce the value t and conduct the above procedure again to approach the optimal value P ∗. On the other hand, if the convex feasibility problem is infeasible, we know that P ∗ t. In this case, we should increase the value of t. Through this procedure, the quasiconvex optimization problem in Equation 4 can be solved using bisection, i.e., solving a convex feasibility problem at each step (Boyd et al. (2004)). The procedure is summarized as Algorithm 1.\n\nt. It indicates that the optimal value P ∗ is smaller than t, i.e., P ∗\n\n≤\n\n≤\n\n≤\n\n≥\n\n∃\n\nAlgorithm 1 QCNN Training Process\n\n≤ −\n\nP ∗, tolerance ε > 0\n\nP ∗, u ≥\nl > ε do t := (l + u)/2 Solve the convex feasibility problem in Equation 5 if problem Equation 5 is feasible then\n\n1: given l 2: while u 3: 4: 5: 6: 7: 8: 9:\n\nelse\n\nend if 10: end while\n\nu := t\n\nl := t\n\n▷ lower/upper bounds of optimal value ▷ convergence criterion\n\n▷ record the feasible point θ\n\n▷ return the current feasible point θ\n\nRemark 1. The quasiconvex optimization problem in Equation 4 has zero duality gap. The proof can be derived by verifying that our unconstrained quasiconvex optimization problem in Equation 4 satisfies the condition in Fang et al. (2014). Therefore, the quasiconvex optimization problem could also be solved via exploring its dual problem.\n\n6\n\nf(w1,w2)=w1w2f(w1,w2)=w1f(w1,w2)=w2f(w1,w2)=w1+w2minimizationUnder review as a conference paper at ICLR 2023\n\n6\n\nEXPERIMENTS\n\nWe use the proposed framework in Section 5 to conduct several machine learning tasks, comparing QCNN to deep neural networks. Our experiments aim to validate the core benefits of QCNN: (1) the convexity, even in shallow networks, makes learning more accurate than non-convex deep networks in some tasks, and (2) the convexity enables the network to be more robust and converge faster.\n\n6.1 FUNCTION APPROXIMATION\n\nSince the purpose of neural networks can be generally seen as learning a mapping from input x to label y, in this section, we evaluate the performance of using QCNN to approximate some function.\n\nSynthetic dataset. For synthetic scenario, the dataset is generated by randomly sampling x from a 1, 1) and calculating the corresponding label y = f (x) given function uniform distribution Unif( f . We generate 1,000 samples for training and 200 samples for testing, where the mean square error (MSE) of the testing set is used to evaluate the model performance.\n\n−\n\nThe results of approximating various functions are summarized in Figure 5. As we see, the performance of deep neural networks depends on the choice of initial guess of network weights. In the first two experiments (first two rows in Figure 5), the deep network seems to be trapped in a bad local optima, which corresponds to a relatively large MSE. In the third experiment, the deep network arrives at a good local optima. However, it still exhibits certain flaws at the non-differentiable points (turning points) in the function f . It matches the finding of Brady et al. (1989) where deep neural networks fail in simple cases of linearly separable classifying tasks. On the contrary, although QCNN uses a shallow structure, its quasiconvexity nature enables it to learn piecewise linear functions to approximate function f . In many replications of experiments, we find that learning procedure of QCNN is more robust to initial guess of network weights since it is quasiconvex. Moreover, QCNN demonstrates a quicker convergence when learning the function f .\n\nFigure 5: The performance of approximating functions using deep neural networks and QCNN: QCNN tends to behave better because the quasiconvex structure enables it to learn piecewise linear mappings more efficiently.\n\nContour detection dataset. For the real-world application of function approximation, we consider the task of detecting the contour of an object, which is usually the first step for many applications in computer vision, such as image-foreground extraction (Banerjee et al. (2016)), simple-image segmentation (Saini & Arora (2014)), detection, and recognition (Kulkarni et al. (2013)). The experiment is conducted on the Berkeley Segmentation Dataset (Arbel ́aez et al. (2013)) composed of 200 training, 200 testing, and 100 validation images. Each image has between five and six manually annotated labels representing the ground truth (Yang et al. (2019)). These labels are also used to calculate two metrics: optimal dataset scale (ODS) and optimal image scale (OIS) to evaluate the model performance. The comparison was performed against DeepNet (Kivinen et al. (2014)).\n\n7\n\nMSE: 0.00Converge:1.9sMSE: 0.001Converge:2.2sMSE: 0.005Converge:2.3sFunction. fPiecewise-linear Piecewise-linear+nonlinear Piecewise-nonlinearDeep neural network. QCNNMSE: 0.03Converge:3.4sMSE: 0.04Converge:3.8sMSE: 0.007Converge:4.1sUnder review as a conference paper at ICLR 2023\n\nIn the experiment, we find that the performance of QCNN and DeepNet depend on the objects in the image. For some objects with clear and angular contours, e.g., a phone in Figure 6 (a), detecting such a contour can be seen as learning a closed polygon with piecewise linear functions defining its edges. For such a class of images, the ODS of QCNN achieves 0.824 compared to 0.784 of DeepNet, while the OIS of QCNN achieves 0.831 compared to 0.798 of DeepNet. On the contrary, DeepNet has better accuracy in recognizing complex (e.g., highly non-linear) contours. In this class of images, the ODS of QCNN is 0.717 compared to 0.743 of DeepNet, while and the OIS of QCNN is 0.729 compared to 0.760 of DeepNet. To conclude, QCNN with the quasiconvex structure still outperforms the deep networks when the task involves some characteristics related to convexity.\n\nDRD⊤M −\n\nMass-damper system dataset. Aside from synthetic functions and irregular functions, we also learn functions that have physical meanings. Specifically, we consider the mass-damper system, 1q. In the system, ̇q is a vector of momenta, D is the which can be depicted as: ̇q = incidence matrix of the system, R is the diagonal matrix of the damping coefficients for each line of the system, and M is the diagonal matrix of each node mass of the system. Thus, we can set y = ̇q 1. We simulate the dataset for and x = q with the goal to learn the parameter matrix a 10-node system and obtain 6,000 samples for 1-min simulations with a step size to be 0.01s (Li et al. (2022)). Figure 6 (b) shows the prediction error (MSE) during training epochs, where QCNN converges faster to a smaller error, compared to deep neural networks. This is perhaps because that the target parameter matrix in the system constructs a linear bridge between input x and label y.\n\nDRD⊤M −\n\n−\n\n−\n\nFigure 6: (a) QCNN works better in detecting angular contours while deep networks are better for detecting complex contours. (b) MSE against training epochs in learning the mass-damper function.\n\n6.2 CLASSIFICATION TASK\n\nThe experiments in Section 6.1 represent the regression tasks. In this section, we further consider the classification task, covering two major categories of machine learning applications.\n\nChange point detection of distributions. Finding the transition of the underlying distribution of a sequence has various applications in engineering fields, such as video surveillance (Sultani et al. (2018)), sensor networks Xie & Siegmund (2013) and infrastructure health monitoring Liao et al. (2019). Aside from engineering tasks, it is also important in many machine learning tasks, including speech recognition Chowdhury et al. (2012), sequence classification Ahad & Davenport (2020), and dataset shift diagnosis Lu et al. (2016). To simulate a sequence of measurements, we randomly generate the pre-change sequence from normal distribution (0, 0.2) and generate the post-change sequence from (1, 0.1) where the change time is λ = 50. The time-series sequence is shown in the left part of Figure 7.\n\nN\n\nN\n\nUsing the neural network to detect the change point λ can be seen as classifying pre-change data and post-change data using five samples in a shifted window. The classifying threshold is chosen as α, i.e., the maximum false alarm rate (Liao et al. (2016)). The results are shown in Figure 7 using 1,000 Monte Carlo experiments. As we see, QCNN shows a smaller average detection delay than deep neural networks. Meanwhile, it seems that QCNN is less likely to falsely report a fake change, since the empirical false alarm rate of QCNN is below that of deep networks, and is mostly below the theoretical upper bound α (especially when α 0). QCNN outperforms the deep neural network in this task because the transition of the distribution is abrupt, as shown in Figure 7 (Left). The abrupt change results in a non-differentiable/non-smooth point in the mapping to be learned, which is more efficiently represented by QCNN via piecewise linear functions.\n\n→\n\n8\n\n(a) contour detectionobjects with angular contoursobjects with complex contoursDeep neural network. QCNN(b) training of mass-damper systemEpochMSEQCNN conveyers faster12QCNN has smaller errorUnder review as a conference paper at ICLR 2023\n\nFigure 7: (Left) The change of underlying distribution of the sequence. (Middle) The average detection delay of detecting the distribution change using QCNN and deep neural networks. (Right) The false alarm rate of detecting the distribution change using QCNN and deep neural networks.\n\nSolar meters classification. The UMass Smart dataset (Laboratory for Advanced System Software (Accessed Sep. 2022.)) contains approximately 600,000 meters from a U.S. city with a one-hour interval between each meter reading (Cook et al. (2021)). Among these meters, around 1,973 have installed solar panels, and are labeled as solar in the classification task. The remainder of the meters are labeled as non-solar. The average smart meter readings, including the household electricity consumption and the PV generation, as well as the household’s address, are considered as input data to classify whether a meter has solar panels. We randomly select 20,000 samples from this dataset to train and select 1,000 samples to test the performance. Figure 8 shows the location of all the meters and solar meters. As we see, the meters that have solar installed are concentrated in a roughly convex area instead of being spread over in the entire area. Therefore, learning the classifier for solar meters using the feature of address is equivalent to learning a convex domain. It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%).\n\nFigure 8: The locations of (solar) meters and the classification accuracy of using QCNN and deep neural network to classify the solar meters.\n\n7 CONCLUSION\n\nIn this work, we analyze the problem of convex neural networks. First, we observe that deep neural networks are not suitable for all tasks since the network is highly non-convex. The non-convex network could fail, i.e., being trapped in bad local optima with large errors, especially when the task involves convexity (e.g., linearly separable classification). Therefore, it motivates us to design a convex structure of neural networks to ensure efficient training with performance guarantees. While convexity is damaged due to the multiplication of weights as well as non-linear activation functions, we manage to decompose the neural network into building blocks, where the quasiconvexity is thoroughly studied. In the building block, we find that the multiplication of ReLU output is quasiconcave over network weights. To preserve the property of quasiconcavity when such building blocks are integrated into a general network, we design minimization pooling layers. The proposed QuasiConvex shallow Neural Network (QCNN), can be equivalently trained via solving convex feasibility problems iteratively. With the quasiconvex structure, QCNN allows for efficient training with theoretical guarantees. We verify the proposed QCNN using several common machine learning tasks. The quasiconvex structure in QCNN demonstrates even better learning ability than non-convex deep networks in some tasks.\n\n9\n\npre-changetimex[N]NSequencepost-changeN(0,0.2)N(1,0.1)Average detection delay. False alarm rate1QCNN has smaller delay2QCNN has smaller false rateLocations of all the metersLocations of solar net metersSolar meters locate together in a convex areazoom inClassification accuracyQCNN (94.2%) > Deep NN (92.7%)Under review as a conference paper at ICLR 2023\n\nREFERENCES\n\nAkshay Agrawal and Stephen Boyd. Disciplined quasiconvex programming. Optimization Letters,\n\n14(7):1643–1657, 2020.\n\nNauman Ahad and Mark A Davenport. Semi-supervised sequence classification through change\n\npoint detection. arXiv preprint arXiv:2009.11829, 2020.\n\nBrandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In International Confer-\n\nence on Machine Learning, pp. 146–155. PMLR, 2017.\n\nAlexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International conference on machine learning, pp. 1908–1916. PMLR, 2014.\n\nP Arbel ́aez, M Maire, C Fowlkes, and J Malik. Contour detection and image segmentation resources.\n\nBerkeley Segmentation Data Set and Benchmarks, 500, 2013.\n\nKenneth J Arrow and Gerard Debreu. Existence of an equilibrium for a competitive economy.\n\nEconometrica: Journal of the Econometric Society, pp. 265–290, 1954.\n\nBiplab Banerjee, Sudipan Saha, and Shabbir N Merchant.\n\nImage foreground extraction—a supervised framework based on region transfer. In 2016 International Conference on Signal and Information Processing (IConSIP), pp. 1–5. IEEE, 2016.\n\nYoshua Bengio, Nicolas Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex\n\nneural networks. Advances in neural information processing systems, 18, 2005.\n\nStephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-\n\nversity press, 2004.\n\nMartin L Brady, Raghu Raghavan, and Joseph Slawny. Back propagation fails to separate where\n\nperceptrons succeed. IEEE Transactions on Circuits and Systems, 36(5):665–674, 1989.\n\nFrancesco Bullo and Daniel Liberzon. Quantized control via locational optimization. IEEE Trans-\n\nactions on Automatic Control, 51(1):2–13, 2006.\n\nAnna Choromanska, Mikael Henaff, Michael Mathieu, G ́erard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In Artificial intelligence and statistics, pp. 192–204. PMLR, 2015.\n\nMd Foezur Rahman Chowdhury, S-A Selouani, and D O’Shaughnessy. Bayesian on-line spectral change point detection: a soft computing approach for on-line asr. International Journal of Speech Technology, 15(1):5–23, 2012.\n\nDan Ciregan, Ueli Meier, and J ̈urgen Schmidhuber. Multi-column deep neural networks for image classification. In 2012 IEEE conference on computer vision and pattern recognition, pp. 3642– 3649. IEEE, 2012.\n\nElizabeth Cook, Shuman Luo, and Yang Weng. Solar panel identification via deep semi-supervised learning and deep one-class classification. IEEE Transactions on Power Systems, 37(4):2516– 2526, 2021.\n\nTolga Ergen and Mert Pilanci. arXiv:2002.09773, 22, 2020.\n\nConvex duality of deep neural networks.\n\narXiv preprint\n\nDonghui Fang, XianFa Luo, and Xianyun Wang. Strong and total lagrange dualities for quasiconvex\n\nprogramming. Journal of Applied Mathematics, 2014, 2014.\n\nFarzan Farnia and David Tse. A convex duality framework for gans. Advances in neural information\n\nprocessing systems, 31, 2018.\n\nWerner Fenchel and Donald W Blackett. Convex cones, sets, and functions. Princeton University,\n\nDepartment of Mathematics, Logistics Research Project, 1953.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nPaolo Frasconi, Marco Gori, Alberto Tesi, et al. Successes and failures of backpropagation: A\n\ntheoretical investigation. Progress in Neural Networks, 5(0), 1993.\n\nMarco Gori and Alberto Tesi. On the problem of local minima in backpropagation. IEEE Transac-\n\ntions on Pattern Analysis and Machine Intelligence, 14(1):76–86, 1992.\n\nAngelo Guerraggio and Elena Molho. The origins of quasi-concavity: a development between\n\nmathematics and economics. Historia Mathematica, 31(1):62–75, 2004.\n\nBenjamin D Haeffele and Ren ́e Vidal. Global optimality in tensor factorization, deep learning, and\n\nbeyond. arXiv preprint arXiv:1506.07540, 2015.\n\nBenjamin D Haeffele and Ren ́e Vidal. Global optimality in neural network training. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7331–7339, 2017.\n\nElad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex opti-\n\nmization. Advances in neural information processing systems, 28, 2015.\n\nGeoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal processing magazine, 29(6):82–97, 2012.\n\nGeoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural\n\nnetworks. science, 313(5786):504–507, 2006.\n\nMajid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.\n\nQifa Ke and Takeo Kanade. Quasiconvex optimization for robust geometric reconstruction. IEEE\n\nTransactions on Pattern Analysis and Machine Intelligence, 29(10):1834–1847, 2007.\n\nDurk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. Advances in neural information processing systems, 27, 2014.\n\nJyri Kivinen, Chris Williams, and Nicolas Heess. Visual boundary prediction: A deep neural prediction network and quality dissection. In Artificial Intelligence and Statistics, pp. 512–521. PMLR, 2014.\n\nGirish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. Babytalk: Understanding and generating simple image descriptions. IEEE transactions on pattern analysis and machine intelligence, 35(12):2891–2903, 2013.\n\nLaboratory for Advanced System Software. Umass dataset. edu/index.php/Smart/Smart, Accessed Sep. 2022.\n\nhttp://traces.cs.umass.\n\nYann LeCun, Leon Bottou, Genevieve B Orr, Klaus-Robert M ̈uller, et al. Neural networks: Tricks\n\nof the trade. Springer Lecture Notes in Computer Sciences, 1524(5-50):6, 1998.\n\nHaoran Li, Yang Weng, and Hanghang Tong. Console: Convex neural symbolic learning. arXiv\n\npreprint arXiv:2206.00257, 2022.\n\nYizheng Liao, Yang Weng, Chin-Woo Tan, and Ram Rajagopal. Urban distribution grid line outage identification. In 2016 International Conference on Probabilistic Methods Applied to Power Systems (PMAPS), pp. 1–8. IEEE, 2016.\n\nYizheng Liao, Anne S Kiremidjian, Ram Rajagopal, and Chin-Hsuing Loh. Structural damage detection and localization with unknown postdamage feature distribution using sequential changepoint detection method. Journal of Aerospace Engineering, 32(2):04018149, 2019.\n\nKang-Ping Lu, Shao-Tung Chang, and Miin-Shen Yang. Change-point detection for shifts in control charts using fuzzy shift change-point algorithms. Computers & Industrial Engineering, 93:12–27, 2016.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nDavid G Luenberger. Quasi-convex programming. SIAM Journal on Applied Mathematics, 16(5):\n\n1090–1095, 1968.\n\nTristan Milne. Piecewise strong convexity of neural networks. Advances in Neural Information\n\nProcessing Systems, 32, 2019.\n\nMert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time convex optimization formulations for two-layer networks. In International Conference on Machine Learning, pp. 7695–7705. PMLR, 2020.\n\nBlaine Rister and Daniel L Rubin. Piecewise convexity of artificial neural networks. Neural Net-\n\nworks, 94:34–45, 2017.\n\nDavid Saad. Online algorithms and stochastic approximations. Online Learning, 5(3):6, 1998.\n\nSujata Saini and Komal Arora. A study analysis on the different image segmentation techniques.\n\nInternational Journal of Information & Computation Technology, 4(14):1445–1452, 2014.\n\nKin Cheong Sou. Parameterized model order reduction via quasi-convex optimization.\n\nWaqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6479–6488, 2018.\n\nShizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural networks: A theoretical view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.\n\nYifei Wang, Tolga Ergen, and Mert Pilanci. Parallel deep neural networks have zero duality gap.\n\narXiv preprint arXiv:2110.06482, 2021.\n\nYao Xie and David Siegmund. Sequential multi-sensor change-point detection. In 2013 Information\n\nTheory and Applications Workshop (ITA), pp. 1–20. IEEE, 2013.\n\nHongju Yang, Yao Li, Xuefeng Yan, and Fuyuan Cao. Contourgan: Image contour detection with\n\ngenerative adversarial network. Knowledge-Based Systems, 164:21–28, 2019.\n\n12",
  "translations": [
    "# Summary Of The Paper\n\nThis submission develops neural network architectures for which the squared-error training objective is quasi-c.\nThe proposed models are based on the observation that composition of single ReLU neurons preserves quasi-concavity, as does\nthe minimum-pooling operation.\nBy combining these operators, the authors obtain quasi-concave neural networks capable of representing \nnon-negative piece-wise linear functions.\nThey propose a training algorithm based on bisection search that solves convex feasibility problems as a sub-routine. \nThe submission concludes with empirical evaluations on both synthetic and real datasets.\n\n# Strength And Weaknesses\n\nThe main strength of this submission is the simplicity and elegance of the construction for quasi-concave neural networks. \nEndowing neural networks with additional structure that makes them possible to globally optimize while preserving some of their\nrepresentation power is a great approach to principled deep learning and valuable for the ICLR community. \nThe manuscript text is also polished and contains many nice figures which contribute to an intuitive understanding of the quasi-concave architecture.\n\nHowever, I feel the strengths of this submission are outweighed by its weakness, which are summarized in the following points:\n\n- Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader. \n\n- The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, $t \\in \\mathbb{R}$ while $y, f(\\theta) \\in \\mathbb{R}^n$, where $n$ is the size of the training set, so that the operation $y - t - f(\\theta)$ is not well-defined. Moreover, even if $y, f(\\theta) \\in \\mathbb{R}$, the inequality $\\psi_t(\\theta) \\leq 0$ implies $l(\\theta) \\leq t^2 / 2$, rather than $\\(\\theta) \\leq t$. Since, in general, the training problem will be defined for $y \\in \\mathbb{R}^n$, the derivations in the text should handle this general case.\n\n- The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are \"reasonable\". The experiments as given have several notable flaws:\n\n    - Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.\n    - Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair _overall_ comparison between the two methods.\n    - Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well. \n    - Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.\n\n#### Minor Comments\n\n**Introduction**: \n    - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. \n    - The correct reference for backpropagation is Rumelhart et al. [2]  \n    - \"Issue 1: Is non-convex deep neural networks always better?\": \"is\" should be \"are\".\n    - \"While some experiments show that certain local optima are equivalent and yield similar learning performance\" -- this should be supported by a reference.\n    - \"However, the derivation of strong duality in the literature requires the planted model assumption\" --- what do you mean by \"planted model assumption\"? The only necessary assumption for these works is that the shallow network is sufficiently wide.\n\n**Section 4**:\n    - \"In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only $1/2^m$ representation power.\" -- A statement like this only makes sense under some definition of \"representation power\". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. \n    - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. \n    - \"Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient.\" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex.\n    - \"... too many minimization pooling layers will damage the representation power of the neural network\" --- why? Can the authors expand on this observation? \n\n**Section 5**:\n    - \"... if we restrict the network output to be smaller than the network labels, i.e., $f(\\theta) ≤ y$\" --- note that this observation requires $y \\geq 0$, which does not appear to be explicitly mentioned. \n    - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere. \n\n**Figure 6**:\n    - Panel (b): \"conveyers\" -> \"converges\".\n\n**Figure 7**:\n    - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text.\n    - \"It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)\" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. \n### References\n\n[1] Robbins, Herbert, and Sutton Monro. \"A stochastic approximation method.\" The annals of mathematical statistics (1951): 400-407.\n\n[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. \"Learning representations by back-propagating errors.\" nature 323.6088 (1986): 533-536.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe writing is generally of good quality, although (as mentioned above) some of the mathematical developments are imprecise or unclear. The quality of referencing could be improved, as sometimes the wrong citation is used for classical works. See \"Minor Comments\" above. \n\nI have not seen any previous works which attempt to derive quasi-concave neural networks, so the submission is novel as far as I am aware. \nHowever, it is not reproducible. Many details required to reproduce the experiments results are omitted (step-size, batch-size, optimizer, etc). For instance, it is not even stated what procedure is used to solve the convex feasibility problems necessary for training the quasi-concave models.\n\n# Summary Of The Review\n\nThis is an interesting attempt to develop a model class somewhere between the representation power of non-convex neural networks\nand the polynomial-time optimization guarantees of convex models. While the quasi-concave neural networks\nare elegant and mathematically appealing, their utility is not effectively demonstrated by the experimental evaluation and a more rigorous presentation of the training algorithm is necessary. \nI think this could be a strong submission if sufficient baseline experiments are added and I encourage the authors to continue the work.\nHowever, I feel that it is not ready for publication at this time.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Shallow Neural Network (QCNN) architecture, which aims to address the limitations of deep neural networks (DNNs) caused by their non-convex nature. By leveraging quasiconvexity, the authors propose a new training methodology that reformulates the optimization problem as a convex feasibility problem, thus enhancing training efficiency and providing theoretical guarantees. Experimental results demonstrate that QCNN outperforms DNNs in function approximation and classification tasks, particularly in scenarios where convexity plays a crucial role.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its novel approach to neural network architecture by introducing quasiconvexity as a foundational principle, which is a significant departure from traditional DNNs. The proposed training methodology is well-justified and theoretically sound, providing a clear pathway for practical implementation. However, the paper’s analysis could benefit from a deeper exploration of the limitations of QCNN, especially in more complex tasks or datasets that may not align with the quasiconvexity assumption. Additionally, while experimental results are promising, more extensive testing across diverse applications would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and findings. The definitions and theoretical discussions around quasiconvexity are presented in a manner that is accessible, although some technical sections may require careful reading to fully grasp the implications of the proposed architecture. The quality of writing is high, with clear figures and tables that complement the text. Regarding reproducibility, while the methodology is described in detail, the paper would benefit from providing code or further experimental details to facilitate independent verification of the results.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the QCNN architecture as a viable alternative to DNNs for specific tasks where convexity is beneficial. The theoretical contributions are substantial, and empirical results support the effectiveness of the proposed approach. However, further exploration of potential limitations and broader applications is necessary for a comprehensive understanding of QCNN's applicability.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the QuasiConvex shallow Neural Network (QCNN), which aims to address the challenges posed by non-convexity in deep neural networks. The methodology involves decomposing neural networks into building blocks that preserve quasiconvexity, allowing for efficient training with theoretical guarantees. The findings indicate that QCNN outperforms traditional deep networks in several tasks, such as function approximation, contour detection, and classification, particularly in terms of robustness against local optima and convergence speed.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its solid theoretical foundation for quasiconvexity, which underpins the QCNN architecture and supports its training efficiency. Additionally, the empirical results demonstrate superior performance in specific tasks, suggesting a tangible advantage in scenarios where convexity is beneficial. However, the limitations include a narrow scope of application, as the efficacy of QCNN in more complex tasks and deeper architectures remains untested. The potential computational overhead associated with solving convex feasibility problems may also hinder its applicability compared to conventional deep learning methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation, methodology, and results. The theoretical and empirical components are presented with sufficient detail to facilitate reproducibility. The novelty stems from the introduction of quasiconvexity in neural network design, although the exploration of its application is somewhat limited. Overall, the quality of the writing and the clarity of the presented ideas are commendable.\n\n# Summary Of The Review\nThe QCNN presents an innovative approach to mitigating the challenges of non-convexity in neural networks, showcasing promising empirical results across various tasks. While the theoretical contributions and performance advantages are noteworthy, further investigation is needed to assess its generalization capabilities and applicability to deeper architectures.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents the QuasiConvex shallow Neural Network (QCNN) architecture, which addresses the challenges associated with non-convex deep neural networks. It argues that the non-convex nature of deep networks can lead to suboptimal training outcomes and proposes QCNN as a solution that maintains quasiconvexity under mild assumptions. The authors provide theoretical guarantees for the training process, demonstrating that QCNN can achieve superior performance in tasks such as function approximation and classification compared to traditional deep networks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its novel approach to addressing the non-convexity of neural networks by introducing a quasiconvex framework. This provides a theoretically grounded way to optimize neural network training, which may lead to enhanced performance and convergence properties. Additionally, the empirical results demonstrate the practical advantages of QCNN over deep networks in specific applications. However, the paper could benefit from a more extensive comparison with other recent methods aimed at addressing non-convexity, as well as a better explanation of the assumptions required for quasiconvexity to hold.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly presents the concepts of quasiconvexity and its application to neural networks. The methodology is articulated in a structured manner, making it accessible to readers with varying levels of expertise in the field. The reproducibility of the results could be improved with more detailed descriptions of the experimental setups and datasets used, as well as the specific configurations of the QCNN models employed in the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of neural network optimization by introducing the QCNN architecture, which leverages quasiconvexity for improved training efficiency and performance. While the theoretical insights and empirical results are promising, the paper would benefit from deeper comparisons with existing approaches and enhanced details to support reproducibility.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the QuasiConvex shallow Neural Network (QCNN) architecture, which aims to address the limitations of non-convex deep neural networks. The methodology leverages quasiconvex optimization principles to provide theoretical guarantees for efficient training and performance. The findings demonstrate that QCNN exhibits improved learning capabilities on tasks with convex characteristics and shows advantages in terms of convergence speed and robustness compared to traditional deep networks.\n\n# Strength And Weaknesses\nThe QCNN architecture is a significant contribution that provides a fresh perspective on neural network design, particularly in its use of quasiconvex properties to enhance training efficiency. However, the reliance on mild assumptions regarding activation and loss functions raises questions about the generalizability of the approach. Although the theoretical framework offers insights into training dynamics, the paper lacks extensive empirical validation across diverse tasks, which may limit the understanding of QCNN's practical performance. Additionally, while the introduction of minimization pooling layers is innovative, it may inadvertently reduce the representation power of the network. The discussion surrounding local optima is insightful but remains largely theoretical without practical solutions for current deep network architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, providing clear definitions and context for the concepts discussed, particularly quasiconvexity. The quality of the writing is high, facilitating reader comprehension. However, the novelty, while present, could be further emphasized by exploring alternative convexity-related methods in neural networks. The reproducibility of the results could benefit from more extensive benchmarking against various neural network architectures and tasks to validate the findings more robustly.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to neural network architecture through the introduction of QCNN, supported by a solid theoretical framework and promising empirical results. However, the reliance on specific assumptions and the need for broader empirical validation limit its immediate applicability and impact in the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel architecture called the **QuasiConvex Shallow Neural Network (QCSNN)**, which addresses the challenges of non-convexity in deep neural network training. The QCSNN leverages the properties of quasiconvex functions to enhance training efficiency and model robustness, particularly for tasks that exhibit convex structures. Key contributions include a new neural network architecture that utilizes quasiconvex building blocks, specialized minimization pooling layers, and a training algorithm framed as a quasiconvex optimization problem. Empirical results demonstrate that QCSNN outperforms traditional deep learning architectures in tasks requiring efficient learning of convex mappings, showcasing improved convergence rates and reduced susceptibility to local optima.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the non-convexity problem in neural networks, introducing the concept of quasiconvexity in a practical architecture. The empirical validation across various tasks solidifies the claims regarding the performance advantages of the QCSNN. However, a notable weakness is the potential limitation of the architecture's applicability; while the results are promising, they are primarily demonstrated in specific tasks, and further work is needed to explore its generalizability across a wider range of applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear explanations of the methodology and contributions. The theoretical foundations are well-articulated, making the concepts accessible to the reader. The novelty of the approach is substantial, as it challenges the prevailing belief that deeper architectures are always necessary for high performance. The reproducibility of results is supported by detailed descriptions of the training algorithm and empirical setups, although the code and data availability should be explicitly mentioned to enhance reproducibility further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of neural network design by introducing the QCSNN architecture, which utilizes quasiconvex properties for improved training efficiency and robustness. The empirical results support the theoretical claims, making a compelling case for further exploration of quasiconvexity in neural networks.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Shallow Neural Network (QCNN), a novel architecture aimed at enhancing adversarial training in neural networks. The authors propose that traditional deep neural networks often fall prey to local optima due to their non-convex structures, which compromises their robustness against adversarial attacks. By employing a quasiconvex structure, QCNN facilitates more stable optimization and provides theoretical guarantees for improved performance in adversarial contexts. Empirical results demonstrate that QCNN significantly outperforms standard deep networks in terms of robustness and accuracy when exposed to adversarial attacks.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its strong theoretical foundation regarding quasiconvexity and its empirical validation, which shows that QCNN can achieve superior robustness in adversarial settings. The innovative approach of framing adversarial training as a quasiconvex optimization problem is a notable contribution that distinguishes this work from existing methods. However, the paper lacks a thorough discussion on the limitations associated with quasiconvexity, particularly in terms of model complexity and representational power. Additionally, further investigation into the generalizability of QCNN across various adversarial attacks and datasets would enhance the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its contributions, methodology, and findings. The quality of the writing is high, making complex concepts accessible. The novelty of framing adversarial training as a quasiconvex problem is significant and could inspire further research in this direction. However, the reproducibility of the results may be questioned due to the lack of detailed discussions on limitations and the potential trade-offs associated with the proposed architecture.\n\n# Summary Of The Review\nThe QCNN framework represents a meaningful advancement in adversarial training, effectively addressing limitations in traditional deep networks. By leveraging the concept of quasiconvexity, the authors offer a novel and theoretically sound solution that demonstrates improved robustness against adversarial attacks. The paper merits consideration for acceptance at ICLR due to its innovative contributions and compelling empirical results.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Shallow Neural Network (QCNN), a novel architecture that aims to address the inherent issues associated with non-convex deep neural networks. The authors propose that QCNN ensures convexity without strong assumptions, thereby improving training efficiency and accuracy across various machine learning tasks. The paper presents a new optimization paradigm, extensive theoretical guarantees, and experimental validation that claims QCNN outperforms existing models universally.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its ambitious approach to redefining neural network architecture through the QCNN framework, which is claimed to enhance performance and training efficiency significantly. Additionally, the theoretical guarantees provided may offer a compelling basis for future work. However, the paper exhibits weaknesses in its overly broad claims, suggesting that QCNN could render all prior models obsolete. The critique of existing architectures appears overly harsh and lacks nuanced consideration of their contributions. The empirical validation, while extensive, raises concerns about the reproducibility and generalizability of the reported results, as the claims of superiority over deep networks are not substantiated with sufficient breadth or detail.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written in a clear and structured manner, making it accessible to readers. However, the novelty of the claims is overshadowed by the lack of rigorous comparative analysis with existing models and the sweeping assertions regarding the obsolescence of deep networks. The reproducibility of the results is questionable, given the extraordinary performance claims without detailed methodologies or datasets that allow for replication of experiments.\n\n# Summary Of The Review\nThe QCNN paper presents a bold approach to neural network design, claiming significant advantages over existing architectures. While it provides interesting theoretical insights, the paper's sweeping assertions and lack of detailed empirical substantiation diminish its overall impact and credibility.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Neural Network (QCNN), a shallow neural network architecture designed to enhance training efficiency and provide theoretical guarantees through the use of quasiconvexity. The authors argue that QCNN outperforms traditional deep non-convex networks in various tasks, showcasing its capabilities in function approximation, contour detection, mass-damper system modeling, and classification tasks. Experimental results demonstrate significant improvements in mean square error and convergence speed, indicating that QCNN achieves better performance with fewer epochs compared to deep networks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to addressing the non-convexity challenges in deep networks by leveraging quasiconvex structures, which allows for improved training efficiency. The empirical results are compelling, showing that QCNN consistently outperforms deep networks across multiple tasks. However, the paper does have notable weaknesses; it lacks a comprehensive discussion on the scalability of QCNN for more complex tasks and deeper architectures. Additionally, while the paper presents strong empirical evidence, theoretical underpinning for the advantages claimed could be further elaborated.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, making its contributions accessible to readers. The quality of figures and data presentation is satisfactory, and the methodology is described in sufficient detail to allow for reproducibility. However, additional information regarding the implementation details, such as hyperparameter choices and optimization techniques, would enhance reproducibility. The novelty of the proposed QCNN architecture is significant, as it presents a fresh perspective on neural network design by integrating quasiconvexity.\n\n# Summary Of The Review\nOverall, the paper presents a promising new approach to neural network design that effectively addresses the limitations of deep networks through the use of quasiconvex structures. While the empirical results are strong, the paper would benefit from a deeper theoretical exploration and a more detailed discussion of implementation aspects to strengthen its contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a new framework called Quasiconvex Convolutional Neural Networks (QCNN) as an alternative to traditional deep networks, which the authors argue suffer from non-convexity and local optima issues. The methodology centers on establishing quasiconvexity as a property of the optimization landscape, with the goal of improving convergence and generalization across various tasks. The findings suggest that QCNNs can outperform deep networks in certain scenarios, primarily those that benefit from quasiconvex optimization.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its theoretical exploration of quasiconvexity and its potential advantages over non-convex structures. However, it has several weaknesses. The assumption that quasiconvexity will universally yield better performance may not hold true across all practical applications, as the results are limited to specific activation functions and tasks. Additionally, the experiments lack robustness, failing to address performance under adversarial conditions or noise. The exploration of computational overhead introduced by the proposed structure is also insufficient.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, though some arguments could benefit from more rigorous justification and empirical validation. The novelty of introducing quasiconvexity to the context of neural networks is noteworthy; however, the claims regarding its superiority need further empirical support. Reproducibility may be hindered by the reliance on specific activation functions and the limited scope of experimental validation.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing approach to addressing the challenges posed by non-convexity in deep learning through the lens of quasiconvexity. While the theoretical framework is compelling, the empirical evidence is limited and may not generalize well across diverse tasks. Further exploration is necessary to validate the claims made and assess the practicality of the proposed QCNN framework.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex shallow Neural Network (QCNN) architecture, designed to tackle the challenges posed by non-convexity in traditional deep neural networks. By leveraging the properties of quasiconvexity, the authors propose a novel training framework that enhances efficiency and provides theoretical guarantees. The findings demonstrate that QCNN outperforms conventional non-convex networks in various tasks, particularly those where convexity is beneficial, such as function approximation and classification challenges.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to neural network architecture and the rigorous mathematical framework it establishes around quasiconvexity. The experiments provide strong empirical support for the proposed QCNN, showing improved convergence rates and robustness in learning compared to deep networks. However, the paper could benefit from a more extensive discussion on the limitations of QCNN and potential challenges in scaling the architecture for larger, more complex datasets. Additionally, while the theoretical contributions are compelling, the practical implications in real-world scenarios require further exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the QCNN architecture. The methodology is presented in a logical sequence, making it accessible to readers with a background in neural networks and optimization. The quality of writing is high, with appropriate use of mathematical definitions and proofs to support claims. The novelty of the approach is significant, as it introduces a new perspective on the use of convex structures in neural networks. Reproducibility is facilitated through detailed descriptions of the experiments and results, although sharing the code and datasets used would enhance this aspect further.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the design of neural networks by introducing the QCNN architecture, which effectively addresses non-convexity issues. The experimental results validate the theoretical claims, making a strong case for the relevance of quasiconvexity in machine learning applications. However, the paper could strengthen its discussion on limitations and scalability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents a novel framework for tackling a specific machine learning problem, focusing on the limitations of existing methodologies. The authors propose an innovative algorithm that integrates both theoretical insights and practical applications, aiming to enhance performance on various tasks. Key findings indicate that the proposed method outperforms several baseline techniques in terms of accuracy and convergence speed, supported by comprehensive empirical evaluations across multiple datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper include a solid theoretical foundation, clearly articulated assumptions, and logical structure, which contribute to its accessibility. The proposed framework appears to have broad applicability, suggesting potential impacts across multiple subfields within machine learning. However, the empirical validation does not fully cover all relevant scenarios, raising concerns about the generalizability of the results. Additionally, some performance claims lack robust comparisons to established baselines, and the paper could benefit from a more thorough discussion of limitations and edge cases.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with well-organized sections and clear explanations of complex concepts. The mathematical formulations and algorithms are well-defined, though additional details on implementation would enhance reproducibility. The novelty of the approach is significant, as it introduces a new perspective on the problem, but a deeper engagement with related work could strengthen its position within the existing literature.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of machine learning by addressing specific challenges with a novel approach. While the theoretical aspects are strong, the empirical validation and engagement with related work could be improved to bolster the paper's impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper presents a novel architecture known as the QuasiConvex shallow Neural Network (QCNN), designed to overcome the challenges associated with the non-convex nature of traditional deep neural networks (DNNs). The authors propose that incorporating quasiconvexity can enhance learning performance, particularly for tasks that inherently possess convex characteristics. The QCNN architecture is built using specialized layers, known as minimization pooling layers, which maintain quasiconvexity while decomposing the network into manageable building blocks. Theoretical guarantees are provided for efficient training, and experimental results indicate that QCNNs outperform conventional non-convex neural networks in specific applications.\n\n# Strength And Weaknesses\nThe key strengths of this paper lie in its innovative approach to leveraging quasiconvexity within neural networks, which is a relatively unexplored area in the literature. The theoretical framework supporting the QCNN design is solid, offering insights into efficient training methodologies. The experimental findings bolster the claims made regarding the performance improvements of QCNNs over traditional DNNs. However, a notable weakness is the limited scope of the experiments, which may not comprehensively demonstrate the QCNN's advantages across various tasks. Additionally, the assumptions made about the activation and loss functions, while not overly restrictive, could be further clarified to enhance the generalizability of the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers familiar with neural networks. The quality of the writing is high, with a logical flow from problem statement to conclusions. The novelty of the proposed QCNN architecture is significant, as it introduces a new perspective on neural network design that leverages quasiconvexity. However, reproducibility could be a concern, as the paper does not provide extensive details on experimental setups or datasets used, which might hinder other researchers from replicating the results.\n\n# Summary Of The Review\nOverall, the paper makes a substantial contribution to the field of neural network research by introducing the QCNN architecture, which capitalizes on quasiconvexity to improve learning outcomes. While the findings are promising, the limited experimental evaluation and reproducibility aspects could be addressed to enhance the paper’s impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Shallow Neural Network (QCNN), a novel architecture that aims to address the challenges posed by non-convexity in deep neural networks. By structuring the network with quasiconvex building blocks and specialized pooling layers, the authors provide a framework that maintains quasiconvex properties throughout the training process. The paper presents theoretical guarantees for the efficiency of the QCNN architecture, supported by empirical results demonstrating its superior performance in various tasks, including function approximation, contour detection, and classification tasks.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to overcoming the limitations of non-convex architectures, potentially leading to improved training efficiency and performance. The theoretical framework established for QCNN is a valuable contribution that adds rigor to the methodology. However, the paper could benefit from a more comprehensive evaluation of QCNN across a wider range of complex tasks, as its current empirical validation primarily focuses on relatively simpler scenarios. Additionally, the lack of a thorough comparison with other established methods in the domain may limit the perceived impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations, methodologies, and results of the research. The theoretical underpinnings are presented in a coherent manner, making it accessible to the reader. The novelty of the approach is significant, as it provides a fresh perspective on the challenges of neural network training. However, reproducibility could be strengthened by including detailed descriptions of the experimental setups and hyperparameter choices in the evaluation section.\n\n# Summary Of The Review\nOverall, the QCNN paper presents a compelling solution to the challenges posed by non-convexity in deep learning, backed by both theoretical and empirical evidence. While it demonstrates promising results in specific tasks, further validation across a broader range of applications and improved reproducibility would enhance its impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper titled \"QuasiConvex Shallow Neural Network\" introduces a novel architecture aimed at addressing the non-convexity challenges prevalent in traditional neural networks. The authors propose the QuasiConvex Neural Network (QCNN), which is constructed by decomposing networks into quasiconvex building blocks. This design choice is supported by a theoretical framework that underlines the benefits of quasiconvexity for optimization. The empirical results demonstrate that QCNN outperforms conventional deep neural networks across various tasks, showcasing improved training stability and performance metrics.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to addressing the limitations of non-convex neural networks through the introduction of quasiconvexity. The methodology is well-articulated, providing a solid theoretical foundation for the proposed QCNN architecture, which is a notable contribution to the literature. However, a potential weakness is the limited exploration of the scalability of QCNN in more complex datasets and tasks, which may affect the generalizability of the findings. Additionally, while the paper presents comprehensive experimental results, comparisons with a broader range of existing methods could strengthen the validation of QCNN's effectiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written, with a clear and logical flow of ideas. Technical terms are adequately defined, making the content accessible to a broad audience. The methodology is detailed enough to allow for reproducibility, and the authors provide sufficient information about the experimental setup and datasets used. The novelty of the approach is evident, as it addresses a significant gap in the understanding and application of convexity in neural networks.\n\n# Summary Of The Review\nOverall, \"QuasiConvex Shallow Neural Network\" presents a compelling and original approach to tackling the challenges posed by non-convexity in neural networks. The well-founded methodology and promising empirical results suggest that QCNN could significantly advance the field of machine learning. However, further exploration of scalability and broader comparisons with existing methods would enhance the paper's contributions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThis paper introduces the QuasiConvex shallow Neural Network (QCNN), which aims to address the non-convexity issues inherent in traditional deep neural networks (DNNs). The authors propose an architecture that maintains quasiconvexity, allowing for efficient training and improved convergence properties. The methodology involves breaking down the network into subsystems, analyzing quasiconvexity, and formulating the training process as a quasiconvex optimization problem. Empirical results demonstrate that QCNN outperforms conventional DNNs in tasks requiring convexity, such as contour detection and piecewise linear mapping.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its novel approach to addressing the limitations of DNNs through the introduction of the QCNN architecture, which is theoretically grounded and supported by empirical evidence. The formulation of a quasiconvex optimization problem is a significant contribution, providing formal guarantees for efficient training. However, the paper could benefit from a more detailed exploration of the limitations of QCNN, including its performance in complex tasks compared to deeper networks that may leverage non-convexity for better feature extraction. Additionally, the empirical validation, while promising, could be strengthened by more extensive experimentation across diverse datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, effectively communicating the key concepts and methodologies. The novelty of the QCNN architecture and its theoretical foundations are clearly articulated, contributing to the field of neural network optimization. The reproducibility of the results could be enhanced by providing additional details regarding the experimental setup, including hyperparameters and dataset characteristics. Overall, the clarity of the presentation allows for a good understanding of the proposed methods and findings, although deeper insights into the experimental design would improve reproducibility.\n\n# Summary Of The Review\nThe paper presents a compelling and innovative approach to mitigating the challenges posed by non-convexity in deep neural networks through the QuasiConvex shallow Neural Network architecture. While the contributions are significant and well-supported by empirical evidence, the paper could improve in detailing the limitations of the approach and enhancing the reproducibility of the experiments.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel architecture known as the QuasiConvex shallow Neural Network (QCNN) aimed at addressing the challenges posed by non-convexity in deep neural networks. It presents a framework that leverages quasiconvexity principles to improve optimization and learning outcomes. However, the paper's methodology lacks a thorough justification for its design choices and presents limited experimental validation, raising questions about the efficacy and robustness of QCNN in comparison to existing deeper architectures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to tackle the well-known issue of non-convexity in neural networks, which is a relevant and significant topic in the machine learning community. However, the paper exhibits several weaknesses, including a weak justification for the QCNN architecture, limited scope of experimental validation, and vague theoretical guarantees regarding performance. Additionally, it fails to adequately explore the model's limitations and potential failure modes, which diminishes the overall impact of the contributions presented.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from convoluted presentation and dense writing, which may hinder reader comprehension and engagement. While the exploration of quasiconvexity is interesting, the novelty of the approach is questionable due to its similarities with existing literature. The reproducibility of the results is also uncertain, as the experiments conducted are limited and do not comprehensively demonstrate QCNN's advantages under varied conditions.\n\n# Summary Of The Review\nOverall, the paper presents an intriguing concept with the QCNN architecture; however, it falls short in providing adequate justification, rigorous experimental validation, and a comprehensive discussion of its limitations. The lack of novelty and clarity further detracts from the paper's contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Shallow Neural Network (QCNN), an innovative architecture designed to enhance training efficiency and model performance by leveraging a quasiconvex structure. The methodology involves framing the training process as a quasiconvex optimization problem, which theoretically guarantees improved convergence properties compared to traditional deep neural networks. Experimental findings demonstrate that QCNN outperforms standard deep learning models in various tasks, including function approximation, contour detection, and classification, with notable performance metrics like a 94.2% accuracy in classifying solar meters.\n\n# Strength And Weaknesses\nThe primary strength of the QCNN lies in its ability to overcome the non-convexity issues commonly faced by deep neural networks, enabling more reliable and faster convergence to optimal solutions. The theoretical guarantees provided strengthen the architectural claims, adding rigor to the methodology. However, the paper may lack a comprehensive exploration of potential limitations or scenarios where QCNN might underperform compared to existing models, which could be an area for further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting complex ideas in a clear and accessible manner. The quality of the experimental validation is commendable, demonstrating the robustness of the QCNN across different tasks. The novelty of the approach is significant, as it introduces a new paradigm in neural network design. Reproducibility appears feasible given the clear presentation of methodology and results, though additional details on the implementation could further enhance this aspect.\n\n# Summary Of The Review\nOverall, the QCNN represents a significant advancement in neural network architecture, combining theoretical rigor with practical applicability. Its promising results across various tasks suggest it could become a preferred choice for future machine learning applications. The paper effectively articulates the potential impact of QCNN on the field, though a deeper exploration of its limitations would strengthen the discussion.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper introduces the QuasiConvex Shallow Neural Network (QCNN), a novel neural network architecture designed to address the challenges posed by the non-convex structures of deep neural networks (DNNs). The authors propose a theoretical framework that emphasizes the advantages of quasiconvexity, allowing for the efficient training of shallow networks with guarantees of improved convergence properties. Through a decomposition approach, the QCNN architecture utilizes specific activation functions and specially designed minimization pooling layers to maintain quasiconvexity, ultimately framing the training process as a quasiconvex optimization problem.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, which provide a fresh perspective on the role of convexity in neural network design. The introduction of quasiconvexity offers a compelling framework that addresses the limitations of traditional DNNs, potentially leading to superior performance in certain tasks. However, a notable weakness is the lack of empirical validation of the proposed architecture. While the theoretical insights are robust, the absence of experimental results that demonstrate the practical advantages of QCNNs limits the paper's impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making complex theoretical concepts accessible to the reader. The quality of the theoretical analysis is high, with detailed explanations of quasiconvexity and its implications for neural network training. However, the reproducibility of the findings could be improved by providing concrete experimental results and implementation details. The novelty of the proposed approach is significant, as it challenges existing paradigms in neural network architecture and optimization.\n\n# Summary Of The Review\nOverall, the paper presents a strong theoretical framework for quasiconvex shallow neural networks, offering valuable insights into the optimization landscape of neural networks. While the theoretical contributions are commendable, the lack of empirical validation is a critical gap that needs to be addressed for the findings to have a broader impact on the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex Shallow Neural Network (QCNN), a novel neural network architecture that exploits the properties of quasiconvex functions to enhance training efficiency and theoretical guarantees. The methodology involves decomposing the QCNN into building blocks, each analyzed for quasiconvexity. The architecture incorporates special layers, such as minimization pooling layers, that maintain quasiconvex properties throughout the network. The training process utilizes a quasiconvex optimization framework, transformed into a convex feasibility problem, with an iterative bisection algorithm for solving it. Experimental results validate the theoretical claims using both synthetic and real-world datasets, such as the Berkeley Segmentation Dataset.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, particularly the rigorous analysis of quasiconvex functions in the context of neural networks. The proposed architecture and training methodology are innovative and provide a fresh perspective on optimizing neural networks. However, the paper has notable weaknesses, including a lack of practical implementation details, absence of publicly available code, and insufficient discussion on the scalability and computational efficiency of the proposed method. Additionally, there is a lack of detailed experimental setup information, which may hinder reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, although some sections could benefit from more detailed explanations, particularly regarding the experimental methodology and the implementation of the proposed algorithm. The novelty of the approach is significant, as it introduces quasiconvexity to the domain of neural networks. However, the reproducibility of the findings is compromised due to the lack of code availability and insufficient details about the datasets and preprocessing steps used in experiments.\n\n# Summary Of The Review\nOverall, the QCNN paper presents a theoretically robust framework for training shallow neural networks using quasiconvex optimization. While the theoretical insights and proposed methodology are commendable, the lack of practical implementation details and reproducibility issues detract from the paper's overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex shallow Neural Network (QCNN) as an alternative to traditional non-convex deep neural networks, arguing that QCNNs can mitigate the issues associated with local optima in training. The authors present theoretical guarantees for the QCNN architecture and claim superior learning capabilities in specific tasks. However, the methodology lacks a comprehensive comparison with existing deep learning techniques, and the findings suggest a bias towards the advantages of QCNN without adequately addressing the strengths and successes of deep networks across various applications.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, providing a new perspective on neural network architecture that could potentially address non-convexity issues. However, the paper suffers from several weaknesses, including an unfair comparison with deep networks, overstating the advantages of QCNN without acknowledging the practical successes of deep architectures. The experimental design appears biased, favoring QCNN results while neglecting a balanced evaluation against contemporary deep learning methods. Additionally, the paper lacks nuance in its discussions, oversimplifying the capabilities of deep networks and ignoring hybrid approaches that have shown promise in recent research.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but its clarity is compromised by a lack of depth in discussing the broader context of deep learning advancements. While it presents novel theoretical insights, the empirical validation is insufficient and does not convincingly demonstrate the claimed superiority of QCNN over deep networks. The reproducibility of results is also questionable due to the selective presentation of experimental outcomes and a lack of comprehensive comparative analysis.\n\n# Summary Of The Review\nOverall, the paper introduces an intriguing approach to neural network design but falls short in providing a balanced and nuanced evaluation of the existing literature on deep learning. The claims made regarding the superiority of QCNN are not sufficiently substantiated by the experimental evidence presented, leading to a potentially misleading narrative.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to deep learning through the introduction of QuasiConvex Shallow Neural Networks (QCNNs). The authors claim that QCNNs can achieve effective learning capabilities without strong assumptions on the activation or loss functions, thus overcoming limitations posed by traditional non-convex structures in neural networks. The methodology involves formulating a quasiconvex optimization problem, demonstrating its effectiveness through theoretical proofs and empirical results, which show that QCNNs outperform non-convex deep networks on specific tasks.\n\n# Strength And Weaknesses\nOne of the primary strengths of the paper is its innovative approach to utilizing quasiconvexity, which could potentially lead to improved performance in neural network training. The theoretical framework is well developed, providing a solid foundation for the proposed QCNNs. However, the paper suffers from several clarity and consistency issues, such as inconsistent terminology, notation errors, and formatting inconsistencies that may hinder reader understanding. Additionally, the empirical results, while promising, could be strengthened by a more comprehensive comparison with state-of-the-art models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by several instances of confusing notation and inconsistent terminology. For example, terms like \"quasiconvex\" and \"quasi-convex\" are used interchangeably without clear definitions, which can lead to misunderstandings. While the theoretical novelty is significant, the empirical results need to be presented more rigorously to improve reproducibility. Furthermore, the paper requires a thorough proofreading to address grammatical errors and awkward phrasing that detract from the overall quality.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling concept in the form of QCNNs, offering a new perspective on neural network optimization. However, the paper's clarity and consistency issues significantly impact its readability and could potentially limit its adoption by the research community. Addressing these concerns would enhance its contribution to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces a novel architecture known as the Quasiconvex Neural Network (QCNN), which leverages quasiconvexity to improve learning efficiency and model performance. The authors present a series of experiments on traditional benchmarks demonstrating the QCNN's capabilities in comparison to standard neural networks. The findings suggest that QCNNs outperform conventional architectures in specific scenarios, highlighting their potential in certain machine learning tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of quasiconvexity as a framework for neural network design, which is a fresh perspective in the field. The experimental results are compelling and indicate that QCNNs can achieve better performance on certain tasks. However, the paper has notable weaknesses, including a lack of exploration into the scalability of the QCNN architecture and insufficient discussion on its limitations. Additionally, the focus on traditional benchmarks restricts the assessment of the model's robustness in more complex or dynamic environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers with varying levels of expertise. The quality of the methodology is sound, although the lack of discussion on scalability and limitations detracts from its overall completeness. While the concept of quasiconvexity is novel, the paper could benefit from a comparative analysis with existing models to enhance reproducibility and provide a more comprehensive understanding of the QCNN's advantages and disadvantages.\n\n# Summary Of The Review\nOverall, the paper presents an interesting and novel approach in the form of QCNNs, with promising initial results. However, it falls short in several key areas, including scalability, application to diverse datasets, and a balanced discussion of potential limitations. Addressing these gaps would significantly enhance the paper's contribution to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces a novel framework called Quasiconvex Convolutional Neural Network (QCNN) designed to address the challenges of non-convexity in traditional deep neural networks. It provides theoretical guarantees regarding efficient training through quasiconvex optimization, presenting a robust methodology that includes extensive experimental validation across common machine learning tasks. The findings indicate that QCNN outperforms conventional deep networks in several benchmarks, demonstrating improved robustness and lower error rates.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its theoretical foundation, which successfully establishes quasiconvexity as a means to overcome the limitations of non-convex networks. The extensive empirical validation across various tasks, including function approximation, contour detection, and classification, reinforces the claims made. However, a notable weakness is the reliance on strong assumptions regarding network design and real-world data applicability, which may limit the generalizability of the findings. Additionally, while the statistical analysis supports the results, the paper could benefit from a broader discussion on the implications of these assumptions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the problem of non-convexity and the proposed solution through QCNN. The methodology is presented in a coherent manner, making it accessible to readers familiar with deep learning concepts. The novelty of the approach is significant, as it introduces a new perspective on optimizing neural networks. The empirical results are reproducible, given that the experiments are well-documented and utilize standard performance metrics, although additional implementation details would further enhance transparency.\n\n# Summary Of The Review\nOverall, the paper presents a compelling case for the QCNN framework, combining theoretical contributions with solid empirical validation. While the work exhibits significant contributions to the field of deep learning optimization, some assumptions may limit its practical applicability in diverse real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex shallow Neural Network (QCNN) architecture, which aims to leverage quasiconvex optimization principles to improve learning efficiency and performance in specific tasks. The authors present a theoretical framework underpinning QCNN, along with preliminary empirical evaluations on a limited set of datasets. The findings suggest that QCNN can achieve competitive performance compared to traditional deep neural networks, though the scope of the experiments remains narrow.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its theoretical contributions, particularly the introduction of quasiconvexity as a guiding principle for shallow neural network design. However, the paper exhibits several weaknesses. It does not investigate the scalability of QCNN to deeper architectures, nor does it explore its adaptability to diverse real-world datasets. The empirical comparisons are limited to deep networks without evaluating a broader range of architectures, raising concerns about the robustness and generalizability of the results. Furthermore, the paper fails to address potential overfitting issues and lacks discussions on the implications of various activation functions and the performance of QCNN in adversarial settings or with noisy data.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its concepts clearly. However, the novelty is somewhat undermined by the narrow focus on shallow architectures and the limited empirical validation. The reproducibility of the results may also be questionable due to the lack of comprehensive performance comparisons and the absence of detailed evaluations of computational efficiency and resource requirements.\n\n# Summary Of The Review\nOverall, while the paper presents an interesting theoretical perspective on quasiconvex optimization in shallow neural networks, it falls short in empirical validation and broader applicability. The limitations in scalability, adaptability, and performance assessment warrant further investigation and development before the QCNN architecture can be deemed robust for practical applications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"Quasiconvex Shallow Neural Network\" (QCNN) proposes a new architecture aimed at addressing the challenges posed by non-convexity in deep neural networks. The authors claim that their QCNN framework is superior for tasks involving convex functions, while outlining a methodology that decomposes the network into \"building blocks\" based on the principle of quasiconvexity. The results presented suggest that QCNN outperforms traditional methods in synthetic function approximation and contour detection tasks, although the findings appear to favor simpler scenarios over more complex ones.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of the QCNN architecture, which attempts to tackle well-known issues associated with non-convex optimization in neural networks. However, the weaknesses are significant; the contributions feel derivative, as they largely reiterate established ideas about local minima and the limitations of deep networks. The experimental validation is limited, potentially leading to biased conclusions that do not generalize well beyond the chosen tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity suffers from convoluted explanations, particularly in the theoretical sections where the authors attempt to justify their definitions of quasiconvexity. While the quality of the writing is competent, it does not compensate for the lack of novel insights. The reproducibility of the results is questionable, as the experiments lack rigorous benchmarking against a broader range of tasks and existing techniques, making it difficult for other researchers to validate the findings independently.\n\n# Summary Of The Review\nOverall, the paper presents a novel architecture in the QCNN that attempts to address familiar challenges in neural network training. However, it falls short of providing substantial new insights or empirical evidence to support its claims, making it feel more like a rehash of established concepts than a true advancement in the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper introduces the Quasiconvex Convolutional Neural Network (QCNN) architecture, which aims to leverage quasiconvex properties to enhance learning efficiency and model robustness. The authors provide a detailed methodology outlining the design of QCNNs, emphasizing their shallow network characteristics while demonstrating the potential for improved convergence speed and generalization across various tasks. Empirical results indicate that QCNNs outperform traditional architectures in terms of robustness, although the focus remains primarily on ReLU activation functions.\n\n# Strength And Weaknesses\nThe contributions of this paper are significant as they present a novel approach to integrating quasiconvexity into neural network design, which may lead to increased efficiency and robustness. However, the exploration of quasiconvex properties is limited to shallow networks, which could restrict the applicability of the findings. The paper could benefit from a deeper investigation into maintaining quasiconvexity in deeper architectures and incorporating diverse activation functions. Furthermore, while the theoretical guarantees for training are commendable, extending these guarantees to generalization would strengthen the overall impact of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to a broad audience. The quality of the experiments is good, although more comprehensive benchmarking against state-of-the-art models is needed for validation. The novelty lies primarily in the introduction of QCNNs, though further exploration of hybrid models and diverse activation functions would enhance the contribution. Reproducibility is supported, but additional details on experimental setups and datasets would further aid future researchers in replicating the results.\n\n# Summary Of The Review\nOverall, the paper introduces an interesting approach to neural network design through the QCNN architecture, demonstrating promising results in terms of robustness and efficiency. However, the work could be strengthened by exploring deeper architectures, incorporating diverse activation functions, and providing more extensive experimental validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces the QuasiConvex shallow Neural Network (QCNN), highlighting its performance in various machine learning tasks compared to traditional deep neural networks. The methodology involves evaluating QCNN's ability to approximate functions, detect contours, and perform classification tasks. Key findings indicate that QCNN excels in approximating piecewise linear functions and demonstrates improved performance in specific benchmark tasks, such as change point detection and solar meter classification, while also noting its limitations in handling more complex structures.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its empirical evaluation of QCNN across diverse tasks, showcasing its superior performance in scenarios that involve convex characteristics or simpler function structures. The results consistently indicate faster convergence and lower mean square error (MSE) values compared to deep networks. However, a notable weakness is QCNN's comparatively poorer performance on more complex functions and contours, which suggests that its applicability may be limited to specific problem domains rather than a general-purpose architecture.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings. The experimental results are articulated effectively, making it easy for the reader to follow the comparisons made with deep networks. Regarding novelty, QCNN offers a unique approach by focusing on quasi-convexity, which is not widely explored in existing literature. However, the reproducibility of results may be challenged if the specific implementation details or hyperparameter settings are not thoroughly documented.\n\n# Summary Of The Review\nOverall, the paper presents a compelling argument for the QCNN's efficacy in specific machine learning tasks, particularly those involving convex characteristics. While the findings are promising, the limitations in handling complex structures need to be addressed to establish QCNN as a more versatile approach.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"QUASICONVEX SHALLOW NEURAL NETWORK\" investigates the concept of quasiconvexity in the design of shallow neural networks (QCNNs). The authors propose a novel architecture that leverages quasiconvex properties to improve training efficiency and generalization performance. The methodology includes a theoretical analysis of the quasiconvex landscape and empirical evaluations on benchmark datasets, demonstrating that QCNNs outperform traditional shallow architectures in specific contexts. The findings suggest that integrating quasiconvex principles can lead to more robust neural network designs.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to neural network architecture by introducing quasiconvexity, which is a relatively unexplored area in the context of neural networks. This contribution is significant as it may pave the way for new methodologies in network design. However, the paper is hampered by a lack of clarity and coherence in presentation. The abstract is dense and complex, which may hinder understanding for readers unfamiliar with the topic. Additionally, the introduction could benefit from a more structured flow, and the repetitive explanations of certain concepts detract from the overall quality.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is a notable concern, as many technical terms and concepts are introduced without sufficient context or explanation. This may alienate readers who are not specialists in the field. The quality of the writing suffers from grammatical issues, lengthy sentence structures, and inconsistent terminology. In terms of novelty, the introduction of quasiconvexity in neural networks is a fresh perspective that adds significant value to the literature. However, reproducibility could be an issue due to the lack of detailed methodology and insufficient discussion of figures, which may hinder a reader's ability to replicate the experiments.\n\n# Summary Of The Review\nOverall, the paper presents a novel and potentially impactful approach to neural network architecture through the lens of quasiconvexity. However, significant improvements in clarity, structure, and reproducibility are necessary to make the findings more accessible and comprehensible to a broader audience.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.1249005351666725,
    -1.482729953372352,
    -1.674421534135782,
    -1.424987569456538,
    -1.8803742344953882,
    -1.5595001555314303,
    -1.5992647049008952,
    -1.9215057854317181,
    -1.6192085128844937,
    -1.632286984005169,
    -1.5431080284396819,
    -1.7034126403697072,
    -1.5655440679140766,
    -1.606906329257651,
    -1.4310268191279194,
    -1.4853030267837801,
    -1.7412008208318772,
    -1.7042039257155523,
    -1.4540780623213125,
    -1.594206756230822,
    -1.8127527936056662,
    -1.5944583465609188,
    -1.490533006411061,
    -1.7185570576619886,
    -1.7124169273791743,
    -1.8514997235763901,
    -1.703682119956533,
    -1.9239963791429457,
    -1.5419228198750783
  ],
  "logp_cond": [
    [
      0.0,
      -2.0662047140453037,
      -2.0622844200142225,
      -2.0597265564270635,
      -2.067500651031489,
      -2.0662165107881907,
      -2.066926039967977,
      -2.0705972123797727,
      -2.0477872381765385,
      -2.0683485057076494,
      -2.0669205651925258,
      -2.089591473527327,
      -2.0667957683099867,
      -2.059001577082908,
      -2.062210093828637,
      -2.0622005050310888,
      -2.067088715816092,
      -2.073182503460953,
      -2.07405180844238,
      -2.0582552642406644,
      -2.0660774914757383,
      -2.0722357119714627,
      -2.0821447998071054,
      -2.07540027926863,
      -2.076107861608035,
      -2.071589279760812,
      -2.0714207000013065,
      -2.069626652055272,
      -2.0750183344739663
    ],
    [
      -1.2339679436657918,
      0.0,
      -1.0937379449941418,
      -1.0839779079164125,
      -1.1214695154906087,
      -1.1209690048204428,
      -1.1454428789262026,
      -1.1279812830332692,
      -1.1246381544191124,
      -1.194391495027651,
      -0.9899179932053808,
      -1.2201157766133344,
      -1.0903843014862065,
      -1.1140106555845621,
      -1.1552409165922535,
      -1.0595920799938787,
      -1.1069818063781338,
      -1.1084233926536122,
      -1.1605497133173055,
      -1.1135373237235502,
      -1.1266988447958741,
      -1.1680464096467014,
      -1.181412123333434,
      -1.1450701601756759,
      -1.1676055412063273,
      -1.05662931478221,
      -1.1985531415432982,
      -1.1112738631018124,
      -1.1470273180374568
    ],
    [
      -1.3345874277456702,
      -1.288809719446769,
      0.0,
      -1.2536642055063827,
      -1.2087197256217037,
      -1.2616545747319488,
      -1.28190630532814,
      -1.3098508747152953,
      -1.2369413824473097,
      -1.277247869699549,
      -1.2042693083729639,
      -1.333498302440031,
      -1.288614363455334,
      -1.1621660483762812,
      -1.1846003852923177,
      -1.239834819661567,
      -1.243525600752381,
      -1.258854005359295,
      -1.2669478003247447,
      -1.2819171310246702,
      -1.2855085598910383,
      -1.304234983933096,
      -1.327628538933324,
      -1.2423276147360829,
      -1.2998996236892852,
      -1.2138145160058373,
      -1.3077465196099791,
      -1.2229918834187008,
      -1.3414933558862117
    ],
    [
      -1.160981670227339,
      -1.0101992183651005,
      -0.9628060457066036,
      0.0,
      -1.0200446852691973,
      -1.0877538671294462,
      -1.0778555151017541,
      -1.048008096909705,
      -1.038843427760494,
      -1.0937200033933503,
      -1.0123562512258366,
      -1.1504457334894962,
      -1.0195788830197867,
      -0.997963414036621,
      -1.0460187827380676,
      -0.9685037500561754,
      -1.0332957574569273,
      -1.0402092805493213,
      -1.0451130656753178,
      -1.0623151962158583,
      -1.0348817624817686,
      -1.0501970225650727,
      -1.0807545938043808,
      -1.0518807232277616,
      -1.0384476880591662,
      -1.0578621126560994,
      -1.063338026720545,
      -1.0226295294604395,
      -1.062107510086967
    ],
    [
      -1.5837085608242853,
      -1.5158787809932597,
      -1.406375647501913,
      -1.4569645330283323,
      0.0,
      -1.433286457419853,
      -1.5015985993927863,
      -1.5066368921512014,
      -1.558036722925084,
      -1.4749534682198762,
      -1.387612550422486,
      -1.6122510803039254,
      -1.3977166207208747,
      -1.4540458969008445,
      -1.4785699303958673,
      -1.47352358651339,
      -1.4574101923030625,
      -1.5567197662678178,
      -1.448721646384807,
      -1.4626389563575117,
      -1.4562881926696039,
      -1.493962279956102,
      -1.5409317259559863,
      -1.5025549822772362,
      -1.4615810627465244,
      -1.4864362085011835,
      -1.4883185459511004,
      -1.508476174250292,
      -1.521017215480168
    ],
    [
      -1.300864561239457,
      -1.2084223154934899,
      -1.1721000570898246,
      -1.2389327699547752,
      -1.1668029190054736,
      0.0,
      -1.2084439921498467,
      -1.249531157550094,
      -1.2180847641877925,
      -1.196623925725926,
      -1.1547775217487004,
      -1.2680064429852678,
      -1.1609407219311718,
      -1.1461325316911914,
      -1.1960858648693051,
      -1.193132047009367,
      -1.2224122083771372,
      -1.2179883322487424,
      -1.1236347967467348,
      -1.1823764381458481,
      -1.2351137484971368,
      -1.1968457270655941,
      -1.2556423262370477,
      -1.228004068905948,
      -1.2545116999354542,
      -1.2160502458892726,
      -1.2239972046522085,
      -1.2308536903252179,
      -1.2306021158313485
    ],
    [
      -1.324756540324837,
      -1.2559132397759587,
      -1.2428201276103306,
      -1.2277881708818128,
      -1.25095072039028,
      -1.282636640344858,
      0.0,
      -1.2347738853610635,
      -1.2557949971393738,
      -1.2670811616704645,
      -1.2484870873875584,
      -1.3394637989855702,
      -1.2714170766345778,
      -1.2222500822964038,
      -1.2434623886240566,
      -1.221317821745273,
      -1.2501900263505494,
      -1.2201512022836327,
      -1.2495361467747066,
      -1.258500127886914,
      -1.2644891456454457,
      -1.2675057904252187,
      -1.2818127591859032,
      -1.2627538994821692,
      -1.2672860113935747,
      -1.2397636470867661,
      -1.3052193805745023,
      -1.274598658360572,
      -1.2821884983302418
    ],
    [
      -1.7023497169364161,
      -1.555329345506814,
      -1.5788838133071859,
      -1.5147814446629149,
      -1.5725250234036585,
      -1.5920256906400871,
      -1.4966925083076017,
      0.0,
      -1.5639663629005778,
      -1.6236565519176804,
      -1.5688105228075728,
      -1.6707269083017091,
      -1.5253767034120518,
      -1.529963352428626,
      -1.5622473183166663,
      -1.5111354749429966,
      -1.4753812279895449,
      -1.525164881215364,
      -1.554924497129269,
      -1.5516606116158926,
      -1.497849458320478,
      -1.5916835478580318,
      -1.5746190376410643,
      -1.53614770272017,
      -1.536968921369332,
      -1.5273813325788224,
      -1.579369520461417,
      -1.5520362661569378,
      -1.5343643363709383
    ],
    [
      -1.2616780910482026,
      -1.1664140045980107,
      -1.1941568056488778,
      -1.212926506947081,
      -1.2698306618399224,
      -1.2742773029015073,
      -1.1957905438522447,
      -1.1987729771643372,
      0.0,
      -1.2848461068357622,
      -1.1485141830572763,
      -1.3153556444358758,
      -1.229450048216059,
      -1.1900753354224387,
      -1.2399266578595867,
      -1.2316490038667811,
      -1.2199735925084358,
      -1.16391889257625,
      -1.2488760763681848,
      -1.2352193158396956,
      -1.193914606520515,
      -1.2673529401451575,
      -1.2764869799801914,
      -1.2261129147788883,
      -1.2826641309742866,
      -1.2242250555175505,
      -1.2514299140108491,
      -1.1840718801421484,
      -1.2619340170978948
    ],
    [
      -1.3403860428021852,
      -1.3283664763393406,
      -1.243624922959161,
      -1.297615120269503,
      -1.246065187084209,
      -1.3128480149283168,
      -1.282004256452331,
      -1.3046598180182152,
      -1.3377641198628127,
      0.0,
      -1.2614419512225024,
      -1.33055580711827,
      -1.3058407754622674,
      -1.2849232034143783,
      -1.278552455338552,
      -1.318004533259548,
      -1.263304698667572,
      -1.3291028607628206,
      -1.2870667172116619,
      -1.3484558553478383,
      -1.274254769524816,
      -1.2585928102149564,
      -1.3018457391980283,
      -1.2706195780351521,
      -1.276971418116439,
      -1.270112191517442,
      -1.2710259752306754,
      -1.3047407267847173,
      -1.3183819915204364
    ],
    [
      -1.3028800763285775,
      -1.0989284535135813,
      -1.0973717122481903,
      -1.135478129210839,
      -1.101944923662282,
      -1.1066129676843361,
      -1.1725518192359334,
      -1.183856105671214,
      -1.129452051728535,
      -1.181206162229561,
      0.0,
      -1.2287755103665083,
      -1.1300892505496332,
      -1.127497181662505,
      -1.1346722312722763,
      -1.116863834042521,
      -1.1400407349990627,
      -1.1876267517078591,
      -1.1463516146591353,
      -1.1647805656958539,
      -1.1547318608630313,
      -1.1645287083472629,
      -1.2156640592754158,
      -1.1501140516256403,
      -1.1753412974745479,
      -1.1756630898528952,
      -1.2230262690117184,
      -1.174619217435824,
      -1.2088625436580034
    ],
    [
      -1.3967350461356283,
      -1.3885024108855035,
      -1.3607723394949438,
      -1.3919644575118058,
      -1.306689258532665,
      -1.3290798570473588,
      -1.3971960672978052,
      -1.323907016143816,
      -1.361524977150605,
      -1.3501021732401934,
      -1.3277518411625375,
      0.0,
      -1.387296597817012,
      -1.3175182521480053,
      -1.3455894428269228,
      -1.3772940432513125,
      -1.3741549571411764,
      -1.3873209996409088,
      -1.334562988384678,
      -1.3693887125960933,
      -1.3765553110313937,
      -1.3242598097511462,
      -1.3859788562550106,
      -1.3943214682224352,
      -1.3741069521341012,
      -1.3912201464105973,
      -1.3671600366988077,
      -1.3687110830743783,
      -1.3721267704272002
    ],
    [
      -1.2772981910065904,
      -1.1992903378348623,
      -1.1952382719648753,
      -1.1851518991110364,
      -1.1133018597862019,
      -1.2052499697596493,
      -1.2533691291271871,
      -1.2317162485268407,
      -1.2810085022669326,
      -1.2767488199787678,
      -1.179005199982023,
      -1.3479188502441595,
      0.0,
      -1.1946455918315444,
      -1.238655423603465,
      -1.160998690584264,
      -1.179966365183083,
      -1.2735791088444,
      -1.1905923278602528,
      -1.200446000193727,
      -1.2138156677445178,
      -1.209089337302898,
      -1.2686666992379318,
      -1.2794616844923277,
      -1.2577978437098536,
      -1.2006576451904458,
      -1.2596989395740448,
      -1.2510270074680665,
      -1.2435403940777616
    ],
    [
      -1.336639830970702,
      -1.253467047342629,
      -1.097373141651756,
      -1.1921654113671936,
      -1.2017015061216139,
      -1.223875946743423,
      -1.2061807936718374,
      -1.2291778550287855,
      -1.2126780855706654,
      -1.273461715638287,
      -1.1957407610376414,
      -1.3278833306882933,
      -1.2114905181737785,
      0.0,
      -1.1711599713669196,
      -1.1667333630391388,
      -1.2174530012720206,
      -1.2322825650638858,
      -1.2032211057742928,
      -1.2408866389593378,
      -1.266758263366726,
      -1.2463160003554283,
      -1.312966719469707,
      -1.2700750347294938,
      -1.2643774483055985,
      -1.177746973053135,
      -1.288162927440981,
      -1.2378372461160847,
      -1.2701764154624795
    ],
    [
      -1.1959097445527178,
      -1.0734637964384597,
      -1.026411794254196,
      -1.0888075313036671,
      -1.0857635789465192,
      -1.0610375278182091,
      -1.0742707039064192,
      -1.1010419877434066,
      -1.1066057577189918,
      -1.1170505605103027,
      -1.012157425257148,
      -1.1249763271258586,
      -1.0931521535692854,
      -1.0474529848033418,
      0.0,
      -1.0586664011056905,
      -1.0614265962104086,
      -1.0994199894566774,
      -1.0731211723885026,
      -1.1171660549914886,
      -1.124941581642245,
      -1.1134580279183826,
      -1.160150996763138,
      -1.1189140458635862,
      -1.1464532590365208,
      -1.0591087112048538,
      -1.1368572610376266,
      -1.1151847525606942,
      -1.0956359524311463
    ],
    [
      -1.2483177743037315,
      -1.1583466302334922,
      -1.089236069761246,
      -1.0819787758701167,
      -1.1333368670308348,
      -1.1558791083294835,
      -1.171820823986614,
      -1.1424373569599,
      -1.1636372836415383,
      -1.2229515809691183,
      -1.1081116449100148,
      -1.269871035939249,
      -1.1077149835115727,
      -1.1020795813638986,
      -1.1629857237068446,
      0.0,
      -1.104373304128259,
      -1.142768193005205,
      -1.1497123794866024,
      -1.1691000277861998,
      -1.1400688037884468,
      -1.1925946482114804,
      -1.2235748645256515,
      -1.129486070687088,
      -1.2172286673923556,
      -1.127900725341641,
      -1.177067176233523,
      -1.1335025437219075,
      -1.1844917598336733
    ],
    [
      -1.4659912204674246,
      -1.3325355044860023,
      -1.3221517271405785,
      -1.2819735625869753,
      -1.3347197781895583,
      -1.4032100316025284,
      -1.3246589839124177,
      -1.3354568236572908,
      -1.360518095074812,
      -1.40832648393609,
      -1.3177722982479716,
      -1.469644045570266,
      -1.2925461219675738,
      -1.3569115412354757,
      -1.3261718791217052,
      -1.315224090954755,
      0.0,
      -1.3357620010317666,
      -1.4021154814049417,
      -1.3931079206405312,
      -1.3334522760361633,
      -1.403168562865706,
      -1.3460154435658243,
      -1.3266972917429176,
      -1.3488001527057223,
      -1.2735223859190072,
      -1.3909803025941818,
      -1.339951182359518,
      -1.3744204664144195
    ],
    [
      -1.434810835822973,
      -1.3346133872030441,
      -1.29878051464371,
      -1.3080222676154205,
      -1.3648730137901077,
      -1.3877460259073284,
      -1.2771637105519944,
      -1.2939879925064943,
      -1.281539295048935,
      -1.4273345731242617,
      -1.33595345322815,
      -1.447542405322834,
      -1.3514895165299667,
      -1.276870078429689,
      -1.3562938586876887,
      -1.316782527709027,
      -1.292009757910676,
      0.0,
      -1.3361717547820948,
      -1.3577259572528009,
      -1.3226002932219396,
      -1.3695213946170326,
      -1.3576596370074778,
      -1.2888729126866132,
      -1.3891719793505253,
      -1.3018793099002282,
      -1.367706495081637,
      -1.2208265601163069,
      -1.3730546822982248
    ],
    [
      -1.197580693033879,
      -1.1226203323824808,
      -1.094772886763595,
      -1.1064497339404586,
      -1.0390804482733949,
      -1.0138562173633154,
      -1.1014811392168036,
      -1.1003576251108238,
      -1.1264493094849088,
      -1.0989470073027232,
      -1.0658320948110773,
      -1.155045888941507,
      -1.0504432056859188,
      -1.056410766537767,
      -1.0486909142264564,
      -1.0452344122239687,
      -1.1376060430691195,
      -1.067087233859279,
      0.0,
      -1.0221196926079272,
      -1.119821804016406,
      -1.0811454086181562,
      -1.1472067280922955,
      -1.1310725807116788,
      -1.1391935117787977,
      -1.128340198495377,
      -1.1330960578266602,
      -1.1395132028141406,
      -1.1093425587138164
    ],
    [
      -1.2662482127221693,
      -1.2076427794809192,
      -1.191407345017447,
      -1.2519583255531237,
      -1.208156481589485,
      -1.2244265938623582,
      -1.2094770349477224,
      -1.1818656316363547,
      -1.259488632572077,
      -1.2641773677105865,
      -1.218542638658855,
      -1.3336295145161114,
      -1.1974676349393596,
      -1.2053341275032807,
      -1.2447893610430794,
      -1.2110848002505983,
      -1.2156477868378834,
      -1.2048203578898085,
      -1.1451314387133262,
      0.0,
      -1.2556653595424556,
      -1.2510697091023728,
      -1.244445732713239,
      -1.247556894345087,
      -1.2431830994264164,
      -1.2221427080002756,
      -1.2555897715303224,
      -1.2655733787616745,
      -1.2063673647660171
    ],
    [
      -1.5123092991992295,
      -1.4576240643457379,
      -1.4470163432244445,
      -1.386216729500789,
      -1.4570511702288587,
      -1.5110980437366492,
      -1.4552976420488122,
      -1.3400877292994229,
      -1.41369033754592,
      -1.501713881883795,
      -1.4361668614811238,
      -1.5429078951110469,
      -1.4101756876006821,
      -1.4444807604353485,
      -1.4882201606529086,
      -1.4169611615601432,
      -1.3984450572375557,
      -1.4591379071660968,
      -1.4751133051820968,
      -1.4111636556740037,
      0.0,
      -1.4494594773799907,
      -1.4692161126230194,
      -1.4306541157852513,
      -1.4166378139170814,
      -1.4920459271203446,
      -1.4543206671547604,
      -1.3983827115043277,
      -1.4487330748028746
    ],
    [
      -1.3464885130725903,
      -1.2478926739749439,
      -1.2044011391797569,
      -1.210345936192692,
      -1.1877560835761998,
      -1.213670536718082,
      -1.2141926747706318,
      -1.2219984111745148,
      -1.2388498294100807,
      -1.2014938615951105,
      -1.190248997946852,
      -1.2672140172474586,
      -1.217913678439153,
      -1.1634285283351773,
      -1.1965076731069988,
      -1.2290297292519183,
      -1.2345444726020565,
      -1.2566818289262094,
      -1.1843180233003303,
      -1.2221134722317435,
      -1.2402725150748868,
      0.0,
      -1.2448696749885884,
      -1.2551772006058064,
      -1.2486467325356452,
      -1.2586212264103716,
      -1.2437741994545057,
      -1.234927939024341,
      -1.1934459492628413
    ],
    [
      -1.18279276679057,
      -1.1052154326171726,
      -1.1294874625968918,
      -1.1192633382517692,
      -1.1330489509370298,
      -1.1430425646648852,
      -1.1026729102913189,
      -1.124767503612447,
      -1.145246848584967,
      -1.115059381195756,
      -1.1244441956713644,
      -1.1615010405131512,
      -1.1384979600404475,
      -1.125334153677846,
      -1.1383420985243315,
      -1.1229789422869785,
      -1.0695630531918627,
      -1.1237001377635605,
      -1.1422002747359847,
      -1.117434941855444,
      -1.1102883109333672,
      -1.1464401886568825,
      0.0,
      -1.1252320016482846,
      -1.0782134697235854,
      -1.098449164458689,
      -1.064304413777479,
      -1.1158620785165811,
      -1.111113237151225
    ],
    [
      -1.4098156836429778,
      -1.3410674257916426,
      -1.284207089916614,
      -1.2712427731854519,
      -1.3442318186995887,
      -1.3826784087747692,
      -1.3693592982544167,
      -1.343048334633854,
      -1.2727201291725885,
      -1.3423839059462628,
      -1.2887646316657226,
      -1.4521090110662367,
      -1.327922758203953,
      -1.284267629235759,
      -1.3744267315220973,
      -1.2620907392958962,
      -1.3259273751855742,
      -1.2973169709208796,
      -1.3967085663264716,
      -1.395240542238923,
      -1.3232200970078272,
      -1.3687991402897497,
      -1.3822617082368711,
      0.0,
      -1.4043719836592246,
      -1.2863302730839488,
      -1.355872153836975,
      -1.2998917488156527,
      -1.3732598488855945
    ],
    [
      -1.3811785937046146,
      -1.321910823201714,
      -1.2704767011503073,
      -1.2876730645955927,
      -1.2374372703946808,
      -1.3527094549685117,
      -1.306928603065078,
      -1.3317996469411308,
      -1.3740726436111843,
      -1.303748712552842,
      -1.2940945406908804,
      -1.4005448292405032,
      -1.3045769873807997,
      -1.3160621328888666,
      -1.316976902941746,
      -1.3155786485099916,
      -1.3038426552999671,
      -1.3471123931140017,
      -1.314146812058042,
      -1.333775661110083,
      -1.2791529714269994,
      -1.3649863016113597,
      -1.322784711660062,
      -1.3214909451918233,
      0.0,
      -1.3137408528683547,
      -1.3126821392298844,
      -1.328578540146437,
      -1.3439007477285563
    ],
    [
      -1.57947312303881,
      -1.4315016695169425,
      -1.4155638948520775,
      -1.4490274007187618,
      -1.4910000871076965,
      -1.4890628904375165,
      -1.4538103501673607,
      -1.4547723890886939,
      -1.4622457570932297,
      -1.4998092789075386,
      -1.4623253617036942,
      -1.5940053318301934,
      -1.4329947802730925,
      -1.4264432374092588,
      -1.4710324882606916,
      -1.3928713206262093,
      -1.3906022228658086,
      -1.440859138932262,
      -1.4762704030867964,
      -1.4573228189676952,
      -1.4615857842652722,
      -1.5427954421773606,
      -1.4874903338669676,
      -1.4288217110950119,
      -1.4860570901217052,
      0.0,
      -1.4669287975538137,
      -1.4571684805945913,
      -1.4589553210263917
    ],
    [
      -1.4585991627744526,
      -1.399059933731049,
      -1.381529545078233,
      -1.3365145358779105,
      -1.3060696107615937,
      -1.3962588848621555,
      -1.3690154351659085,
      -1.3407949814633,
      -1.3793003765605272,
      -1.3869610724435084,
      -1.408175068148701,
      -1.4379778256746474,
      -1.3715716389125931,
      -1.3755774806929209,
      -1.4182908334294493,
      -1.365041363051792,
      -1.342103137881222,
      -1.4169237291511934,
      -1.420673859808462,
      -1.4110637386517129,
      -1.3559831588169073,
      -1.4310002729204494,
      -1.33237313217568,
      -1.346474715072496,
      -1.3665606399750865,
      -1.3660674060450497,
      0.0,
      -1.420305159432471,
      -1.3718593175883982
    ],
    [
      -1.604605992054517,
      -1.5247624237820356,
      -1.4778871904012267,
      -1.5212231433502257,
      -1.5181711090364198,
      -1.5527739454635732,
      -1.5535813987815983,
      -1.5126526924894435,
      -1.4769881803155387,
      -1.6089562892791043,
      -1.5220955172139723,
      -1.6358731827889024,
      -1.4933327387609003,
      -1.4963046303632874,
      -1.5531100898179722,
      -1.5315527228522388,
      -1.480510961295169,
      -1.4274156223336238,
      -1.5768612975357248,
      -1.5351643643694806,
      -1.4989797102858908,
      -1.6277391188812347,
      -1.5674328811232872,
      -1.5290120369640545,
      -1.5413570827890133,
      -1.4868810096783738,
      -1.5718104113812095,
      0.0,
      -1.5680769353207387
    ],
    [
      -1.273789952528628,
      -1.2091439233817622,
      -1.2046263638749122,
      -1.1869911116343774,
      -1.1638604937040797,
      -1.2071660613392057,
      -1.1709127143573699,
      -1.1978428718862772,
      -1.218705211464142,
      -1.2065729429161345,
      -1.1954532442092376,
      -1.2723066585078988,
      -1.1820124503177476,
      -1.1916963062113204,
      -1.1893946684353467,
      -1.1915397657148983,
      -1.162119505001081,
      -1.2035807726026515,
      -1.192814436191913,
      -1.1960168223271948,
      -1.1940599423042046,
      -1.1832457606346167,
      -1.193000060266992,
      -1.221333261132608,
      -1.1958523942135175,
      -1.1722971289964867,
      -1.2041804349897134,
      -1.1822653914526304,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.05869582112136884,
      0.06261611515245002,
      0.06517397873960906,
      0.05739988413518349,
      0.05868402437848186,
      0.057974495198695575,
      0.054303322786899866,
      0.07711329699013403,
      0.056552029459023156,
      0.05797996997414678,
      0.03530906163934544,
      0.05810476685668586,
      0.06589895808376456,
      0.06269044133803536,
      0.06270003013558378,
      0.05781181935058033,
      0.051718031705719536,
      0.050848726724292526,
      0.06664527092600814,
      0.058823043690934274,
      0.05266482319520982,
      0.042755735359567115,
      0.04950025589804241,
      0.04879267355863748,
      0.05331125540586035,
      0.05347983516536603,
      0.05527388311140058,
      0.04988220069270621
    ],
    [
      0.24876200970656015,
      0.0,
      0.38899200837821013,
      0.39875204545593945,
      0.3612604378817432,
      0.3617609485519091,
      0.33728707444614936,
      0.35474867033908275,
      0.3580917989532395,
      0.28833845834470084,
      0.49281196016697115,
      0.26261417675901755,
      0.39234565188614545,
      0.3687192977877898,
      0.32748903678009844,
      0.4231378733784732,
      0.3757481469942181,
      0.3743065607187397,
      0.3221802400550464,
      0.36919262964880173,
      0.3560311085764778,
      0.31468354372565055,
      0.3013178300389179,
      0.33765979319667605,
      0.3151244121660246,
      0.426100638590142,
      0.28417681182905374,
      0.3714560902705395,
      0.3357026353348951
    ],
    [
      0.3398341063901118,
      0.38561181468901307,
      0.0,
      0.4207573286293993,
      0.4657018085140783,
      0.41276695940383323,
      0.3925152288076419,
      0.3645706594204867,
      0.4374801516884723,
      0.3971736644362329,
      0.4701522257628181,
      0.3409232316957509,
      0.385807170680448,
      0.5122554857595008,
      0.4898211488434643,
      0.43458671447421504,
      0.43089593338340104,
      0.41556752877648706,
      0.40747373381103724,
      0.3925044031111118,
      0.3889129742447437,
      0.370186550202686,
      0.346792995202458,
      0.4320939193996991,
      0.37452191044649674,
      0.46060701812994465,
      0.36667501452580287,
      0.4514296507170812,
      0.33292817824957033
    ],
    [
      0.2640058992291989,
      0.4147883510914374,
      0.4621815237499344,
      0.0,
      0.4049428841873406,
      0.3372337023270917,
      0.3471320543547838,
      0.37697947254683295,
      0.38614414169604383,
      0.33126756606318764,
      0.4126313182307013,
      0.27454183596704174,
      0.4054086864367512,
      0.4270241554199169,
      0.37896878671847034,
      0.45648381940036253,
      0.3916918119996107,
      0.38477828890721666,
      0.3798745037812201,
      0.3626723732406796,
      0.3901058069747694,
      0.3747905468914652,
      0.34423297565215716,
      0.3731068462287763,
      0.3865398813973717,
      0.3671254568004385,
      0.3616495427359929,
      0.4023580399960984,
      0.3628800593695709
    ],
    [
      0.29666567367110286,
      0.36449545350212853,
      0.4739985869934753,
      0.4234097014670559,
      0.0,
      0.44708777707553526,
      0.3787756351026019,
      0.37373734234418676,
      0.3223375115703042,
      0.40542076627551205,
      0.4927616840729021,
      0.2681231541914628,
      0.48265761377451355,
      0.4263283375945437,
      0.40180430409952095,
      0.4068506479819982,
      0.42296404219232575,
      0.3236544682275704,
      0.43165258811058127,
      0.4177352781378765,
      0.42408604182578435,
      0.3864119545392861,
      0.33944250853940194,
      0.37781925221815205,
      0.41879317174886377,
      0.3939380259942047,
      0.39205568854428785,
      0.3718980602450963,
      0.3593570190152202
    ],
    [
      0.25863559429197336,
      0.3510778400379404,
      0.3874000984416057,
      0.3205673855766551,
      0.39269723652595667,
      0.0,
      0.35105616338158363,
      0.30996899798133626,
      0.34141539134363774,
      0.3628762298055044,
      0.4047226337827299,
      0.2914937125461625,
      0.3985594336002585,
      0.41336762384023884,
      0.36341429066212516,
      0.36636810852206336,
      0.33708794715429313,
      0.3415118232826879,
      0.43586535878469546,
      0.37712371738558215,
      0.32438640703429344,
      0.3626544284658362,
      0.30385782929438254,
      0.33149608662548236,
      0.3049884555959761,
      0.3434499096421577,
      0.3355029508792218,
      0.32864646520621243,
      0.3288980397000818
    ],
    [
      0.2745081645760583,
      0.34335146512493653,
      0.35644457729056467,
      0.3714765340190824,
      0.3483139845106151,
      0.3166280645560373,
      0.0,
      0.36449081953983176,
      0.3434697077615214,
      0.33218354323043076,
      0.3507776175133368,
      0.259800905915325,
      0.3278476282663174,
      0.3770146226044915,
      0.35580231627683867,
      0.3779468831556223,
      0.3490746785503458,
      0.3791135026172625,
      0.34972855812618864,
      0.34076457701398133,
      0.33477555925544955,
      0.3317589144756765,
      0.317451945714992,
      0.336510805418726,
      0.33197869350732057,
      0.3595010578141291,
      0.29404532432639297,
      0.32466604654032327,
      0.3170762065706534
    ],
    [
      0.219156068495302,
      0.3661764399249041,
      0.34262197212453227,
      0.40672434076880326,
      0.3489807620280596,
      0.329480094791631,
      0.42481327712411643,
      0.0,
      0.35753942253114035,
      0.2978492335140377,
      0.3526952626241453,
      0.250778877130009,
      0.39612908201966635,
      0.3915424330030921,
      0.3592584671150518,
      0.4103703104887215,
      0.4461245574421733,
      0.39634090421635415,
      0.36658128830244907,
      0.3698451738158255,
      0.4236563271112401,
      0.32982223757368634,
      0.34688674779065387,
      0.38535808271154814,
      0.3845368640623861,
      0.39412445285289577,
      0.34213626497030103,
      0.36946951927478033,
      0.38714144906077985
    ],
    [
      0.35753042183629113,
      0.45279450828648304,
      0.4250517072356159,
      0.4062820059374128,
      0.3493778510445713,
      0.34493120998298643,
      0.42341796903224904,
      0.4204355357201566,
      0.0,
      0.3343624060487316,
      0.4706943298272175,
      0.3038528684486179,
      0.38975846466843467,
      0.42913317746205504,
      0.3792818550249071,
      0.38755950901771263,
      0.3992349203760579,
      0.4552896203082437,
      0.370332436516309,
      0.38398919704479817,
      0.4252939063639787,
      0.3518555727393362,
      0.3427215329043023,
      0.39309559810560546,
      0.3365443819102072,
      0.3949834573669433,
      0.3677785988736446,
      0.4351366327423454,
      0.357274495786599
    ],
    [
      0.2919009412029838,
      0.30392050766582845,
      0.3886620610460081,
      0.33467186373566604,
      0.38622179692096004,
      0.3194389690768522,
      0.35028272755283796,
      0.32762716598695385,
      0.2945228641423563,
      0.0,
      0.3708450327826667,
      0.3017311768868991,
      0.32644620854290163,
      0.34736378059079076,
      0.3537345286666169,
      0.314282450745621,
      0.368982285337597,
      0.30318412324234845,
      0.34522026679350715,
      0.28383112865733073,
      0.35803221448035294,
      0.37369417379021264,
      0.33044124480714077,
      0.3616674059700169,
      0.3553155658887299,
      0.362174792487727,
      0.36126100877449363,
      0.3275462572204517,
      0.3139049924847326
    ],
    [
      0.24022795211110437,
      0.4441795749261006,
      0.44573631619149157,
      0.40762989922884296,
      0.44116310477739984,
      0.4364950607553457,
      0.37055620920374843,
      0.35925192276846785,
      0.41365597671114696,
      0.36190186621012077,
      0.0,
      0.31433251807317353,
      0.4130187778900487,
      0.41561084677717686,
      0.4084357971674055,
      0.4262441943971609,
      0.4030672934406192,
      0.3554812767318227,
      0.3967564137805466,
      0.378327462743828,
      0.3883761675766506,
      0.378579320092419,
      0.3274439691642661,
      0.3929939768140416,
      0.367766730965134,
      0.3674449385867866,
      0.32008175942796346,
      0.3684888110038578,
      0.3342454847816785
    ],
    [
      0.3066775942340789,
      0.31491022948420366,
      0.3426403008747634,
      0.31144818285790143,
      0.39672338183704214,
      0.37433278332234843,
      0.306216573071902,
      0.3795056242258912,
      0.34188766321910213,
      0.35331046712951375,
      0.37566079920716966,
      0.0,
      0.3161160425526952,
      0.38589438822170186,
      0.35782319754278435,
      0.32611859711839464,
      0.3292576832285308,
      0.31609164072879836,
      0.3688496519850293,
      0.3340239277736139,
      0.32685732933831346,
      0.379152830618561,
      0.3174337841146966,
      0.309091172147272,
      0.329305688235606,
      0.31219249395910986,
      0.3362526036708995,
      0.3347015572953289,
      0.33128586994250697
    ],
    [
      0.28824587690748626,
      0.36625373007921436,
      0.3703057959492013,
      0.3803921688030403,
      0.45224220812787475,
      0.36029409815442737,
      0.3121749387868895,
      0.33382781938723594,
      0.284535565647144,
      0.28879524793530886,
      0.3865388679320536,
      0.21762521766991716,
      0.0,
      0.37089847608253224,
      0.3268886443106116,
      0.40454537732981266,
      0.3855777027309937,
      0.2919649590696767,
      0.3749517400538238,
      0.3650980677203497,
      0.35172840016955886,
      0.3564547306111787,
      0.2968773686761448,
      0.28608238342174896,
      0.30774622420422304,
      0.3648864227236308,
      0.30584512834003186,
      0.31451706044601013,
      0.322003673836315
    ],
    [
      0.27026649828694915,
      0.35343928191502205,
      0.509533187605895,
      0.41474091789045753,
      0.40520482313603723,
      0.38303038251422805,
      0.4007255355858137,
      0.3777284742288656,
      0.3942282436869857,
      0.33344461361936406,
      0.4111655682200097,
      0.27902299856935775,
      0.3954158110838726,
      0.0,
      0.43574635789073146,
      0.4401729662185123,
      0.3894533279856305,
      0.3746237641937653,
      0.40368522348335834,
      0.36601969029831327,
      0.34014806589092506,
      0.36059032890222276,
      0.2939396097879441,
      0.3368312945281573,
      0.3425288809520526,
      0.4291593562045162,
      0.31874340181667016,
      0.3690690831415664,
      0.3367299137951716
    ],
    [
      0.2351170745752016,
      0.3575630226894597,
      0.4046150248737235,
      0.34221928782425226,
      0.3452632401814002,
      0.36998929130971026,
      0.3567561152215002,
      0.32998483138451284,
      0.32442106140892757,
      0.3139762586176167,
      0.4188693938707715,
      0.3060504920020608,
      0.337874665558634,
      0.3835738343245776,
      0.0,
      0.37236041802222886,
      0.3696002229175108,
      0.331606829671242,
      0.3579056467394168,
      0.3138607641364308,
      0.3060852374856744,
      0.31756879120953685,
      0.27087582236478136,
      0.3121127732643332,
      0.28457356009139856,
      0.37191810792306557,
      0.29416955809029277,
      0.31584206656722524,
      0.3353908666967731
    ],
    [
      0.23698525248004865,
      0.32695639655028796,
      0.39606695702253414,
      0.4033242509136634,
      0.3519661597529453,
      0.32942391845429664,
      0.3134822027971662,
      0.3428656698238801,
      0.32166574314224183,
      0.26235144581466185,
      0.37719138187376533,
      0.21543199084453102,
      0.3775880432722074,
      0.3832234454198815,
      0.3223173030769355,
      0.0,
      0.38092972265552105,
      0.3425348337785752,
      0.3355906472971777,
      0.3162029989975803,
      0.34523422299533335,
      0.2927083785722997,
      0.26172816225812867,
      0.35581695609669217,
      0.26807435939142454,
      0.357402301442139,
      0.30823585055025715,
      0.3518004830618726,
      0.30081126695010685
    ],
    [
      0.27520960036445263,
      0.4086653163458749,
      0.4190490936912987,
      0.45922725824490196,
      0.4064810426423189,
      0.3379907892293488,
      0.41654183691945956,
      0.4057439971745864,
      0.3806827257570653,
      0.33287433689578716,
      0.4234285225839056,
      0.27155677526161126,
      0.4486546988643034,
      0.38428927959640147,
      0.415028941710172,
      0.4259767298771222,
      0.0,
      0.40543881980011065,
      0.33908533942693553,
      0.348092900191346,
      0.4077485447957139,
      0.3380322579661712,
      0.39518537726605296,
      0.4145035290889596,
      0.39240066812615493,
      0.46767843491287,
      0.3502205182376954,
      0.40124963847235917,
      0.3667803544174577
    ],
    [
      0.2693930898925794,
      0.3695905385125082,
      0.40542341107184243,
      0.39618165810013184,
      0.3393309119254446,
      0.3164578998082239,
      0.42704021516355795,
      0.410215933209058,
      0.4226646306666173,
      0.27686935259129064,
      0.3682504724874023,
      0.2566615203927183,
      0.3527144091855856,
      0.42733384728586343,
      0.34791006702786365,
      0.3874213980065253,
      0.4121941678048764,
      0.0,
      0.3680321709334575,
      0.3464779684627515,
      0.38160363249361273,
      0.33468253109851975,
      0.3465442887080745,
      0.41533101302893916,
      0.31503194636502707,
      0.4023246158153242,
      0.33649743063391524,
      0.4833773655992455,
      0.33114924341732754
    ],
    [
      0.2564973692874335,
      0.3314577299388317,
      0.35930517555771746,
      0.3476283283808539,
      0.4149976140479177,
      0.4402218449579971,
      0.35259692310450896,
      0.35372043721048874,
      0.3276287528364037,
      0.3551310550185893,
      0.3882459675102352,
      0.2990321733798056,
      0.4036348566353938,
      0.3976672957835454,
      0.4053871480948561,
      0.4088436500973438,
      0.31647201925219304,
      0.3869908284620336,
      0.0,
      0.4319583697133853,
      0.33425625830490646,
      0.3729326537031563,
      0.306871334229017,
      0.3230054816096337,
      0.31488455054251485,
      0.3257378638259356,
      0.3209820044946523,
      0.31456485950717195,
      0.34473550360749616
    ],
    [
      0.3279585435086527,
      0.3865639767499027,
      0.40279941121337504,
      0.3422484306776983,
      0.386050274641337,
      0.3697801623684638,
      0.38472972128309957,
      0.41234112459446726,
      0.33471812365874487,
      0.33002938852023544,
      0.37566411757196705,
      0.26057724171471053,
      0.3967391212914624,
      0.38887262872754125,
      0.3494173951877426,
      0.3831219559802237,
      0.37855896939293854,
      0.3893863983410135,
      0.44907531751749574,
      0.0,
      0.33854139668836636,
      0.34313704712844917,
      0.349761023517583,
      0.346649861885735,
      0.3510236568044056,
      0.3720640482305464,
      0.3386169847004996,
      0.32863337746914745,
      0.38783939146480484
    ],
    [
      0.3004434944064367,
      0.35512872925992833,
      0.3657364503812217,
      0.4265360641048772,
      0.35570162337680755,
      0.30165474986901697,
      0.35745515155685403,
      0.47266506430624333,
      0.3990624560597462,
      0.31103891172187126,
      0.3765859321245424,
      0.26984489849461935,
      0.4025771060049841,
      0.36827203317031776,
      0.3245326329527576,
      0.39579163204552303,
      0.41430773636811047,
      0.3536148864395694,
      0.3376394884235694,
      0.4015891379316625,
      0.0,
      0.3632933162256755,
      0.3435366809826468,
      0.38209867782041496,
      0.3961149796885848,
      0.32070686648532165,
      0.35843212645090583,
      0.4143700821013385,
      0.3640197188027916
    ],
    [
      0.24796983348832846,
      0.3465656725859749,
      0.3900572073811619,
      0.38411241036822674,
      0.406702262984719,
      0.3807878098428368,
      0.38026567179028703,
      0.372459935386404,
      0.3556085171508381,
      0.39296448496580827,
      0.40420934861406677,
      0.32724432931346015,
      0.3765446681217657,
      0.4310298182257415,
      0.39795067345392,
      0.36542861730900045,
      0.3599138739588623,
      0.3377765176347094,
      0.41014032326058847,
      0.3723448743291753,
      0.354185831486032,
      0.0,
      0.3495886715723304,
      0.3392811459551124,
      0.3458116140252736,
      0.33583712015054723,
      0.35068414710641305,
      0.3595304075365777,
      0.40101239729807747
    ],
    [
      0.3077402396204909,
      0.3853175737938883,
      0.3610455438141691,
      0.37126966815929174,
      0.35748405547403106,
      0.3474904417461757,
      0.38786009611974204,
      0.365765502798614,
      0.34528615782609395,
      0.3754736252153048,
      0.36608881073969646,
      0.32903196589790973,
      0.35203504637061345,
      0.36519885273321484,
      0.35219090788672935,
      0.3675540641240824,
      0.42096995321919817,
      0.3668328686475004,
      0.34833273167507617,
      0.373098064555617,
      0.3802446954776937,
      0.34409281775417844,
      0.0,
      0.36530100476277627,
      0.41231953668747545,
      0.392083841952372,
      0.4262285926335818,
      0.37467092789447975,
      0.379419769259836
    ],
    [
      0.3087413740190108,
      0.377489631870346,
      0.4343499677453746,
      0.44731428447653676,
      0.3743252389623999,
      0.3358786488872194,
      0.3491977594075719,
      0.37550872302813465,
      0.44583692848940015,
      0.37617315171572585,
      0.429792425996266,
      0.26644804659575194,
      0.3906342994580356,
      0.43428942842622953,
      0.34413032613989136,
      0.4564663183660924,
      0.3926296824764144,
      0.421240086741109,
      0.3218484913355171,
      0.3233165154230657,
      0.39533696065416146,
      0.34975791737223894,
      0.3362953494251175,
      0.0,
      0.3141850740027641,
      0.43222678457803987,
      0.3626849038250137,
      0.4186653088463359,
      0.34529720877639414
    ],
    [
      0.3312383336745597,
      0.39050610417746023,
      0.441940226228867,
      0.42474386278358156,
      0.4749796569844935,
      0.3597074724106626,
      0.4054883243140963,
      0.38061728043804344,
      0.33834428376799,
      0.4086682148263323,
      0.4183223866882939,
      0.3118720981386711,
      0.40783993999837453,
      0.39635479449030764,
      0.3954400244374283,
      0.39683827886918266,
      0.40857427207920716,
      0.3653045342651726,
      0.3982701153211323,
      0.3786412662690912,
      0.43326395595217493,
      0.3474306257678146,
      0.38963221571911233,
      0.390925982187351,
      0.0,
      0.39867607451081954,
      0.39973478814928987,
      0.38383838723273733,
      0.368516179650618
    ],
    [
      0.2720266005375802,
      0.4199980540594477,
      0.4359358287243127,
      0.40247232285762835,
      0.36049963646869365,
      0.36243683313887365,
      0.39768937340902943,
      0.39672733448769626,
      0.38925396648316046,
      0.3516904446688516,
      0.38917436187269594,
      0.2574943917461967,
      0.4185049433032977,
      0.4250564861671313,
      0.3804672353156986,
      0.4586284029501808,
      0.46089750071058155,
      0.4106405846441281,
      0.3752293204895938,
      0.3941769046086949,
      0.38991393931111795,
      0.3087042813990295,
      0.36400938970942254,
      0.42267801248137826,
      0.365442633454685,
      0.0,
      0.3845709260225765,
      0.3943312429817989,
      0.39254440254999845
    ],
    [
      0.2450829571820805,
      0.3046221862254841,
      0.32215257487830007,
      0.3671675840786226,
      0.3976125091949394,
      0.3074232350943775,
      0.33466668479062456,
      0.36288713849323306,
      0.3243817433960059,
      0.31672104751302466,
      0.295507051807832,
      0.2657042942818857,
      0.3321104810439399,
      0.3281046392636122,
      0.2853912865270838,
      0.33864075690474116,
      0.36157898207531103,
      0.2867583908053397,
      0.283008260148071,
      0.2926183813048202,
      0.34769896113962573,
      0.27268184703608367,
      0.3713089877808531,
      0.357207404884037,
      0.3371214799814466,
      0.33761471391148334,
      0.0,
      0.28337696052406214,
      0.3318228023681349
    ],
    [
      0.3193903870884287,
      0.3992339553609101,
      0.446109188741719,
      0.40277323579272006,
      0.4058252701065259,
      0.3712224336793726,
      0.37041498036134746,
      0.4113436866535023,
      0.44700819882740706,
      0.31504008986384147,
      0.40190086192897345,
      0.2881231963540434,
      0.4306636403820454,
      0.4276917487796583,
      0.3708862893249736,
      0.392443656290707,
      0.44348541784777673,
      0.4965807568093219,
      0.34713508160722095,
      0.3888320147734652,
      0.4250166688570549,
      0.29625726026171106,
      0.3565634980196586,
      0.3949843421788912,
      0.38263929635393246,
      0.43711536946457197,
      0.3521859677617363,
      0.0,
      0.3559194438222071
    ],
    [
      0.26813286734645025,
      0.33277889649331605,
      0.3372964560001661,
      0.35493170824070086,
      0.37806232617099855,
      0.3347567585358726,
      0.3710101055177084,
      0.34407994798880104,
      0.3232176084109364,
      0.3353498769589438,
      0.3464695756658407,
      0.2696161613671795,
      0.3599103695573307,
      0.3502265136637579,
      0.3525281514397316,
      0.35038305416018,
      0.3798033148739972,
      0.3383420472724268,
      0.3491083836831652,
      0.34590599754788354,
      0.34786287757087364,
      0.35867705924046156,
      0.3489227596080864,
      0.3205895587424703,
      0.34607042566156077,
      0.36962569087859154,
      0.3377423848853649,
      0.35965742842244786,
      0.0
    ]
  ],
  "row_avgs": [
    0.05652513395620473,
    0.3517425674986148,
    0.4078766967641424,
    0.37719786897837376,
    0.39372365318055336,
    0.3488960771210955,
    0.33808938229544466,
    0.36414785403101024,
    0.3888569346647078,
    0.33774669769573507,
    0.38133905793922673,
    0.33977721635492,
    0.338117781968087,
    0.37376384290829984,
    0.33500515210793774,
    0.32778251233164835,
    0.38706490456644416,
    0.3659537760602959,
    0.35483528746764353,
    0.36446068181537894,
    0.3654553795555836,
    0.36700029233200854,
    0.3685866913157083,
    0.3771450298942914,
    0.3909182028333166,
    0.3850426912340528,
    0.32110619080839486,
    0.38845664061763296,
    0.3432520823537587
  ],
  "col_avgs": [
    0.2806551363718191,
    0.35757617151650717,
    0.38850238458426906,
    0.3762205967183411,
    0.37733971380506554,
    0.3481209415793985,
    0.3592186801216274,
    0.3619322297947819,
    0.35115009406510034,
    0.32849384917784114,
    0.38675492433689723,
    0.2712442611170441,
    0.37413005811809696,
    0.386508436760192,
    0.35874062696473014,
    0.38347380054883523,
    0.37810314234995984,
    0.3570241027156766,
    0.35801435886308935,
    0.35124603770582385,
    0.35993594770520765,
    0.3344020264485269,
    0.3229435585074247,
    0.3515640220239525,
    0.336792046993417,
    0.3696432539514326,
    0.334309647050797,
    0.3574487747652819,
    0.3383774559893752
  ],
  "combined_avgs": [
    0.16859013516401192,
    0.354659369507561,
    0.39818954067420576,
    0.37670923284835744,
    0.3855316834928094,
    0.348508509350247,
    0.348654031208536,
    0.36304004191289607,
    0.3700035143649041,
    0.3331202734367881,
    0.38404699113806195,
    0.305510738735982,
    0.356123920043092,
    0.3801361398342459,
    0.34687288953633394,
    0.3556281564402418,
    0.38258402345820197,
    0.3614889393879862,
    0.35642482316536644,
    0.3578533597606014,
    0.3626956636303956,
    0.3507011593902677,
    0.3457651249115665,
    0.36435452595912193,
    0.3638551249133668,
    0.37734297259274274,
    0.32770791892959594,
    0.37295270769145744,
    0.34081476917156694
  ],
  "gppm": [
    583.1204535683077,
    661.919128592409,
    647.7282202654036,
    655.8714074212585,
    648.6601782450729,
    663.9189835540226,
    659.6151223879773,
    656.9166709539023,
    663.3294938772043,
    673.9677008478272,
    648.7289700037162,
    701.6337360077141,
    652.0938880686311,
    648.6449380651478,
    662.4734098641181,
    650.1209663237245,
    652.8726695918349,
    661.7284785706568,
    663.0262630514318,
    662.9821313460277,
    658.0729807974756,
    671.3646231470201,
    680.2909906132111,
    663.957505871068,
    670.7278436628437,
    653.8891539147455,
    670.5887861174798,
    659.4554972668294,
    669.6092876570938
  ],
  "gppm_normalized": [
    1.3760066380163105,
    1.687336196044774,
    1.6567863618453396,
    1.6779657127833423,
    1.6554117515415616,
    1.6964963476654873,
    1.6847604874539674,
    1.685344983079736,
    1.6924873672489693,
    1.720772622358318,
    1.6572081432467223,
    1.801630208612859,
    1.6645625474909045,
    1.65669575140326,
    1.696233558286448,
    1.6623775576587039,
    1.670109776805374,
    1.692495064107095,
    1.6937789715550762,
    1.6921897605532998,
    1.68406150204829,
    1.7152294813898952,
    1.7349701536051865,
    1.7002635680568023,
    1.7137810024788191,
    1.6720782326174164,
    1.7103486329120474,
    1.6866576477140411,
    1.7105794572323878
  ],
  "token_counts": [
    301,
    388,
    419,
    442,
    388,
    427,
    412,
    466,
    407,
    425,
    397,
    630,
    398,
    392,
    463,
    417,
    423,
    430,
    413,
    414,
    437,
    427,
    409,
    455,
    430,
    420,
    403,
    426,
    422,
    742,
    457,
    402,
    407,
    384,
    425,
    586,
    447,
    476,
    395,
    412,
    440,
    450,
    425,
    400,
    433,
    422,
    379,
    389,
    427,
    392,
    371,
    364,
    413,
    432,
    393,
    358,
    382,
    420,
    606,
    434,
    448,
    474,
    401,
    495,
    477,
    426,
    580,
    412,
    415,
    465,
    498,
    455,
    421,
    458,
    403,
    420,
    442,
    452,
    435,
    381,
    344,
    473,
    422,
    437,
    351,
    428,
    360,
    852,
    466,
    462,
    474,
    415,
    413,
    436,
    422,
    465,
    448,
    470,
    377,
    462,
    415,
    455,
    415,
    425,
    413,
    422,
    411,
    459,
    427,
    407,
    447,
    412,
    368,
    423,
    422,
    431,
    885,
    465,
    444,
    471,
    435,
    421,
    397,
    467,
    393,
    448,
    471,
    447,
    418,
    426,
    423,
    395,
    392,
    431,
    460,
    459,
    395,
    389,
    413,
    412,
    402,
    405,
    364,
    428,
    394,
    389,
    442,
    502,
    431,
    485,
    414,
    543,
    421,
    441,
    441,
    462,
    364,
    465,
    477,
    435,
    486,
    420,
    424,
    410,
    443,
    468,
    375,
    436,
    428,
    405,
    503,
    435,
    423,
    372,
    943,
    435,
    392,
    442,
    409,
    423,
    534,
    391,
    430,
    413,
    411,
    412,
    435,
    471,
    353,
    468,
    380,
    432,
    392,
    446,
    345,
    381,
    360,
    430,
    386,
    343,
    392,
    417,
    386,
    850,
    464,
    481,
    546,
    442,
    472,
    423,
    428,
    515,
    456,
    456,
    499,
    481,
    460,
    443,
    480,
    437,
    444,
    432,
    484,
    476,
    442,
    440,
    579,
    477,
    380,
    401,
    480,
    377,
    333,
    535,
    441,
    415,
    440,
    532,
    467,
    476,
    390,
    400,
    461,
    506,
    501,
    483,
    418,
    403,
    393,
    415,
    434,
    464,
    456,
    391,
    393,
    426,
    444,
    420,
    400,
    420,
    534,
    448,
    422,
    431,
    410,
    445,
    437,
    448,
    356,
    418,
    400,
    408,
    567,
    423,
    428,
    389,
    424,
    419,
    417,
    416,
    470,
    368,
    384,
    403,
    423,
    394,
    366,
    382,
    442,
    412,
    532,
    444,
    478,
    502,
    430,
    533,
    483,
    440,
    480,
    379,
    427,
    503,
    458,
    484,
    371,
    423,
    429,
    400,
    409,
    416,
    406,
    403,
    340,
    427,
    412,
    405,
    403,
    478,
    337,
    881,
    516,
    435,
    467,
    432,
    482,
    427,
    429,
    438,
    480,
    526,
    521,
    442,
    442,
    405,
    427,
    443,
    406,
    409,
    416,
    406,
    404,
    382,
    465,
    409,
    391,
    417,
    484,
    418,
    1833,
    463,
    409,
    431,
    428,
    476,
    461,
    399,
    432,
    424,
    452,
    363,
    494,
    429,
    456,
    471,
    385,
    410,
    444,
    447,
    419,
    436,
    403,
    412,
    406,
    410,
    428,
    387,
    466
  ],
  "response_lengths": [
    8729,
    2531,
    2239,
    2309,
    2437,
    2535,
    2515,
    2284,
    2402,
    2203,
    2486,
    2100,
    2631,
    2339,
    2428,
    2594,
    2087,
    2311,
    2381,
    2489,
    2402,
    2320,
    2156,
    2325,
    2298,
    2176,
    2397,
    2197,
    2449
  ]
}