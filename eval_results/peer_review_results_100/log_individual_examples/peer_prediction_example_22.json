{
  "example_idx": 22,
  "reference": "Under review as a conference paper at ICLR 2023\n\nRED-GCN: REVISIT THE DEPTH OF GRAPH CONVOLUTIONAL NETWORK\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nFinding the proper depth d of a GNN that provides strong representation power has drawn significant attention, yet nonetheless largely remains an open problem for the graph learning community. Although noteworthy progress has been made, the depth or the number of layers of a corresponding GCN is realized by a series of graph convolution operations, which naturally makes d a positive integer (d ∈ N+). An interesting question is whether breaking the constraint of N+ by making d a real number (d ∈ R) can bring new insights into graph learning mechanisms. In this work, by redefining GCN’s depth d as a trainable parameter continuously adjustable within (−∞, +∞), we open a new door of controlling its expressiveness on graph signal processing to model graph homophily/heterophily (nodes with similar/dissimilar labels/attributes tend to inter-connect). A simple and powerful GCN model RED-GCN, is proposed to retain the simplicity of GCN and meanwhile automatically search for the optimal d without the prior knowledge regarding whether the input graph is homophilic or heterophilic. Negative-valued d intrinsically enables high-pass frequency filtering functionality for graph heterophily. Variants extending the model flexibility/scalability are also developed. The theoretical feasibility of having a real-valued depth with explainable physical meanings is ensured via eigen-decomposition of the graph Laplacian and a properly designed transformation function from the perspective of functional calculus. Extensive experiments demonstrate the superiority of RED-GCN on node classification tasks for a variety of graphs. Furthermore, by introducing the concept of eigengraph, a novel graph augmentation method is obtained: the optimal d effectively generates a new topology through a properly weighted combination of eigengraphs, which dramatically boosts the performance even for a vanilla GCN.\n\n1\n\nINTRODUCTION\n\nGraph convolutional network (GCN) (Kipf & Welling, 2016; Veliˇckovi ́c et al., 2017; Hamilton et al., 2017) has exhibited great power in a variety of graph learning tasks, such as node classification (Kipf & Welling, 2016; Luan et al., 2019; 2022a), link prediction (Zhang & Chen, 2018), community detection (Chen et al., 2020), and many more. Since the representation power of GCN is largely determined by its depth, i.e., the number of graph convolution layers, tremendous research efforts have been made on finding the optimal depth that strengthens the model’s ability for downstream tasks. Upon increasing the depth, the over-smoothing issue arises: a GCN’s performance is deteriorated if its depth exceeds a uncertain threshold (Kipf & Welling, 2016). It is unveiled in (Li et al., 2018) that a graph convolution operation is a special form of Laplacian smoothing (Taubin, 1995). Thus, the similarity between the graph node embeddings grows with the depth so that these embeddings eventually become indistinguishable. Various techniques are developed to alleviate this issue, e.g., applying pairwise normalization can make distant nodes dissimilar (Zhao & Akoglu, 2019), and dropping sampled edges during training slows down the growth of embedding smoothness as depth increases (Rong et al., 2019).\n\nOther than the over-smoothing issue due to large GCN depth, another fundamental phenomenon widely existing in real-world graphs is homophily and heterophily. In a homophilic graph, nodes with similar labels or attributes tend to inter-connect, while in a heterophily graph, connected nodes usually have distinct labels or dissimilar attributes. Most graph neural networks (GNNs) are developed based on homophilic assumption (Yang et al., 2016), while models able to perform well on heterophilic\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\ngraphs often need special treatment and complex designs (Bianchi et al., 2021; Zhu et al., 2020). Despite the achievements made by these methodologies, little correlation has been found between the adopted GNN model’s depth and its capability of characterizing graph heterophily.\n\nFor most GNNs, if not all, the depth needs to be manually set as a hyper-parameter before training, and finding the proper depth usually requires a considerable amount of trials or good prior knowledge of the graph dataset. Since the depth represents the number of graph convolution operations and naturally takes only positive integer values, little attention has been paid to the question whether a non-integer depth is realizable, and if yes, whether it is practically meaningful, and whether it can bring unique advantages to current graph learning mechanisms.\n\nThis work revisits the GCN depth from spectral and spatial perspectives and explains the interdependencies between the following key ingredients in graph learning: the depth of a GCN, the spectrum of the graph signal, and the homophily/heterophily of the underlying graph. Firstly, through eigen-decomposition of the symmetrically normalized graph Laplacian, we present the correlation between graph homophily/heterophily and the eigenvector frequencies. Secondly, by introducing the concept of eigengraph, we show the graph topology is equivalent to a weighted linear combination of eigengraphs, and the weight values determine the GCN’s capability of capturing homophilic/heterophilic graph signals. Thirdly, we reveal that the eigengraph weights can be controlled by GCN’s depth, so that an automatically tunable depth parameter is needed to adjust the eigengraph weights into the designated distribution in match of the underlying graph homophily/heterophily.\n\nTo realize the adaptive GCN depth, we extend its definition from a positive integer to an arbitrary real number with theoretical feasibility guarantees from functional calculus (Shah & Okutmu ̧stur, 2020). With a trainable depth parameter, we propose a simple and powerful model, Redefined Depth-GCN (ReD-GCN), with two variants. Extensive experiments demonstrate the automatically optimal depth searching ability, and it is found that negative-valued depth plays the key role in handling heterophilic graphs. Systematical investigation on the optimal depth is conducted in both spectral and spatial domains. It in turn inspires the development of a novel graph augmentation methodology. With clear geometric explanability, the augmented graph structure possesses supreme advantages over the raw input topology, especially for graphs with heterophily. The main contributions of this paper are summarized as following:\n\n• The interdependence between negative GCN depth and graph heterophily is discovered;\n\nIn-depth geometric and spectral explanations are presented.\n\n• A novel problem of automatic GCN depth tuning for graph homophily/heterophily detection is formulated. To our best knowledge, this work presents the first trial to make GCN’s depth trainable by redefining it on the real number domain.\n\n• A simple and powerful model RED-GCN with two variants (RED-GCN-S and RED-GCN-\n\nD) is proposed. A novel graph augmentation method is discussed.\n\n• Our model achieves superior performance on semi-supervised node classification tasks on\n\n11 graph datasets.\n\n2 PRELIMINARIES\n\nNotations. We utilize bold uppercase letters for matrices (e.g., A), bold lowercase letters for column vectors (e.g., u) and lowercase letters for scalars (e.g., α). We use the superscript ⊤ for transpose of matrices and vectors (e.g., A⊤ and u⊤). An attributed undirected graph G = {A, X} contains an adjacency matrix A ∈ Rn×n and an attribute matrix X ∈ Rn×q with the number of nodes n and the dimension of node attributes q. D denotes the diagonal degree matrix of A. The adjacency matrix with self-loops is given by ̃A = A + I (I is the identity matrix), and all variables derived from ̃A are decorated with symbol ̃, e.g., ̃D represents the diagonal degree matrix of ̃A. Md stands for the d-th power of matrix M, while the parameter and node embedding matrices in the d-th layer of a GCN are denoted by W(d) and H(d).\n\nGraph convolutional network (GCN) and simplified graph convolutional network (SGC). The layer-wise message-passing and aggregation of GCN (Kipf & Welling, 2016) is given by\n\nH(d+1) = σ( ̃D− 1\n\n2 ̃A ̃D− 1\n\n2 H(d)W(d)),\n\n(1)\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 1: Decompose the symmetrically normalized adjacency matrix into three eigengraphs.\n\nwhere H(d)/H(d+1) stands for the embedding matrix (H(0) = X) in the d-th/(d + 1)-th layer; W(d) is the trainable parameter matrix; and σ(·) is the non-linear activation function. With σ(·) removed in each layer, SGC (Wu et al., 2019) is obtained as below:\n\nH(d) = ̃SdXW,\n\n(2)\n\nwhere ̃S = ̃D− 1 W = (cid:81)d−1\n\ni=0 W(i).\n\n2 ̃A ̃D− 1\n\n2 , and the parameter of each layer W(i) are compressed into one trainable\n\nGraph Laplacian and spectrum. In graph theory, graph Laplacian L = D−A and its symmetrically 2 AD− 1 normalized correspondence Lsym = I − D− 1 2 possess critical properties of the underlying graph G. Lsym has eigenvalues [λ1, λ2, . . . , λn], where λi ∈ [0, 2), ∀i ∈ {1, 2, . . . , n} (Chung & Graham, 1997). 1 Here they are put in ascending order: 0 = λ1 ≤ λ2 ≤ · · · ≤ λn < 2. It can be eigen-decomposed as: Lsym = UΛU⊤, where U = [u1, u2, . . . , un] is the eigenvector matrix (ui ⊥ uj, ∀i ̸= j), and Λ is the diagonal eigenvalue matrix: \n\n\n\nΛ =\n\nλ1 ...\n\n0\n\n \n\n0\n\n· · · ... . . . · · · λn\n\n  .\n\ni ∈ Rn×n. As we will show in Section 3, this n × n matrix For each eigenvector ui, we have uiu⊤ can be viewed as a weighted adjacency matrix of a graph with possible negative edges, which we name uiu⊤ i as the i-th eigengraph of G. Accordingly, Lsym can be written as the linear combination of all eigengraphs weighted by the corresponding eigenvalues (Chung & Graham, 1997):\n\nLsym = λ1u1u⊤\n\ni + . . . + λnunu⊤ n , (3) 1 has an identical value 1 where the first eigenvalue λ1 = 0, and the corresponding eigengraph u1u⊤ for all entries (Shuman et al., 2013). Thus, for SGC, we have n\n(cid:88)\n\n1 + . . . + λiuiu⊤\n\nn\n\n ̃S = I − ̃Lsym = ̃U(I − ̃Λ) ̃U⊤ =\n\n(1 − ̃λi) ̃ui ̃u⊤ i .\n\n(4)\n\nA SGC with d layers requires d consecutive graph convolution operations, which involves the multiplication of ̃S by d times. Due to the orthogonality of ̃U, namely, ̃U⊤ ̃U = I, we obtain\n\ni=0\n\n ̃Sd = ̃U(I − ̃Λ) ̃U⊤ ̃U(I − ̃Λ) ̃U⊤ . . . ̃U(I − ̃Λ) ̃U⊤ = ̃U(I − ̃Λ)d ̃U⊤ =\n\nn (cid:88)\n\n(1 − ̃λi)d ̃ui ̃u⊤\n\ni , (5)\n\nwhere 1 − ̃λi ∈ (−1, 1], and the depth d of SGC serves as the power of ̃S’s eigenvalues. ̃Sd can be viewed as the sum of eigengraphs ̃ui ̃u⊤\n\ni weighted by coefficients (1 − ̃λi)d.\n\ni=1\n\nGraph homophily and heterophily. Graph homophily describes to what extent edges tend to link nodes with the same labels and similar features. In this work, we focus on edge homophily (Zhu\n\net al., 2020): h(G) =\n\n∈ [0, 1], where ⟨x⟩ = 1 if x is true and 0 otherwise. A\n\n(cid:80)\n\ni,j,A[i,j]=1⟨y[i]=y[j]⟩ i,j A[i,j]\n\n(cid:80)\n\ngraph is more homophilic for h(G) closer to 1 or more heterophilic for h(G) closer to 0.\n\n3 MODEL\n\nFirstly, we establish the intrinsic connections between eigengraphs with small/large weights, graph signals with high/low frequencies, and graphs with homophilic/heterophilic properties. Secondly, we\n\n1This work focuses on connected graph without bipartite components (i.e., a connected component which is\n\na bipartite graph).\n\n3\n\n111v!v\"v#v!v\"v#normalization121212=v!v\"v#1−λ!=1,(λ!=0)131313131313v!v\"v#−1316−13231616v!v\"v#1−λ\"=−12,(λ\"=32)−121212++AS=D!\"#AD!\"#eigengraph1eigengraph2eigengraph31−λ#=−12,(λ#=32)Under review as a conference paper at ICLR 2023\n\nshow how the positive/negative depth d of a GCN affects the eigengraph weights and in turn determines the algorithm’s expressive power to process homophilic/heterophilic graph signals. Thirdly, with the help of functional calculus (Shah & Okutmu ̧stur, 2020), we present the theoretical feasibility of extending the domain of d from N+ to R. Finally, by making d a trainable parameter, we present our model RED-GCN and its variants, which are capable of automatically detecting the homophily/heterophily of the input graph and finding the corresponding optimal depth.\n\nThe eigenvectors of a graph Laplacian form a complete set of basis vectors in the n-dimensional space capable of expressing the original node attribute X as a linear combination. From the perspective of graph spectrum analysis (Shuman et al., 2013), the frequency of eigenvector ui reflects how much the j-th entry ui[j] deviates from the k-th entry ui[k] for each connected node pair vj and vk in G. This deviation is measured by the set of zero crossings of ui: Z(ui) := {e = (vj, vk) ∈ E : ui[j]ui[k] < 0}, where E is the set of edges in graph G. Larger/smaller |Z(ui)| indicates higher/lower eigenvector frequency. A zero-crossing also corresponds a negative weighted edge in an eigengraph. Due to the widely existing positive correlation between λi and |Z(ui)| (Shuman et al., 2013), large/small eigenvalues mostly correspond to the high/low frequencies of the related eigenvectors. As illustrated by the toy example of n = 3 in Figure 1, for λ1 = 0, we have |Z(u1)| = 0, and eigengraph u1u⊤ is well-connected with identical edge weight 1 n ; negative edge weights exist in the 2nd and 3rd eigengraphs, indicating more zero crossings (|Z(u2)| = 1 and |Z(u3)| = 2) and higher eigenvector frequencies.\n\n1\n\nSince node labels correlate with their attributes (Zheng et al., 2022a), and node attribute similarities indicate the extent of smoothness/homophily (Luan et al., 2020; 2021), plus node attributes can be expressed by eigenvectors, the deviation between eigenvector entry pairs naturally implies the extent of heterophily. Apparently, high frequency eigenvectors and their corresponding eigengraphs have advantage on capturing graph heterophily. High frequency eigengraphs should accordingly take larger weights when modeling heterophilic graphs, while low frequency ones should carry larger weights when dealing with homophilic graphs. In turn, eigengraph weights are controlled by GCN/SGC’s depth d, e.g., for a SGC of depth d, the weight of the i-th eigengraph is (1 − ̃λi)d, and changing the layer d of SGC adjusts the weights of different eigengraphs. Therefore, depth d controls the model’s expressive power to effectively filter low/high-frequency signals for graph homophily/heterophily.\n\nA question is naturally raised: instead of manually setting the depth d, can d be built into the model as a trainable parameter so that a proper set of the eigengraph weights matching the graph homophily/heterophily can be automatically reached by finding the optimal d in an end-to-end fashion during training? Differentiable variables need continuity, which requires the extension of depth d from the discrete positive integer domain (N+) to the continuous real number domain R. According to functional calculus (Shah & Okutmu ̧stur, 2020), applying an arbitrary function f on a graph Laplacian Lsym is equivalent to applying the same function only on the eigenvalue matrix Λ:\n\nf (Lsym) = Uf (Λ)U⊤ = U\n\n\n\n \n\nf (λ1) ...\n\n0\n\n· · · . . . · · ·\n\n\n\n0\n\n... f (λn)\n\n\n\n U⊤,\n\n(6)\n\ni (d ∈ R).\n\ni=1(1 − ̃λi)d ̃ui ̃u⊤\n\nwhich also applies to ̃Lsym and ̃S. Armed with this, we seek to realize an arbitrary depth SGC via a power function as f ( ̃S) = ̃Sd = ̃U(I − ̃Λ)d ̃U⊤ = (cid:80)n However, since ̃λi ∈ [0, 2), we have (1 − ̃λi) ≤ 0 when 1 ≤ ̃λi < 2, and for (1 − ̃λi) taking zero or negative values, (1 − ̃λi)d is not well-defined or involving complex-number-based calculations for a real-valued d (e.g.,(−0.5) 3 (Shah & Okutmu ̧stur, 2020). Moreover, even for integer-valued ds under which (1 − ̃λi)d is easy to compute, the behavior of (1 − ̃λi)d is complicated versus ̃λi and diverges when ̃λi = 1 for negative ds, as shown in Figure 2a. Thus, the favored weight distribution may be hard to obtain by tuning d.\n\nFigure 2: Eigengraph weight versus eigenvalue for (a) SGC and (b) RED-GCN-S under different depth ds.\n\n(a) (1 − ̃λ)d.\n\n(b) (1 − 1\n\n2 λ)d\n\n8 )\n\n4\n\n0.00.51.01.5432101234(1)dd=2d=1d=0d=1d=20.00.51.01.50.00.51.01.52.02.53.03.54.0(10.5)dd=2d=1d=0d=1d=2Under review as a conference paper at ICLR 2023\n\nTo avoid such complications and alleviate the difficulties for manipulating the eigengraph weights, a transformation function g(·) operating on the graph Laplacian Lsym or ̃Lsym is in need to shift g(λi) or g( ̃λi) into a proper value range so that its power of a real-valued d is easy to obtain and well-behaved versus λi or ̃λi. Without the loss of generality, our following analysis focuses on Lsym and λi. There may exist multiple choices for g(·) satisfying the requirements. In this work, we focus on the following form:\n\nˆS = g(Lsym) =\n\n1 2\n\n(2I − Lsym).\n\n(7)\n\nThis choice of g(·) holds three properties: (1) Positive eigenvalues. Since we have Lsym’s i-th eigenvalue λi ∈ [0, 2), the corresponding eigenvalue of ˆS is g(λi) = 1 2 (2 − λi) ∈ (0, 1]. Thus, the d-th power of g(λi) is computable for any d ∈ R. (2) Monotonicity versus eigenvalues λ. As shown in Figure 2b, g(λi)d = (1 − 1 2 λ)d is monotonically increasing/decreasing when λ varies between 0 and 2 under negative/positive depth. (3) Geometric interpretability. Filter ˆS can be expressed as:\n\nˆS = U ˆΛU⊤ = U(I −\n\n1 2\n\nΛ)U⊤ =\n\n1 2\n\nI +\n\n1 2\n\n(I − Lsym) =\n\n1 2\n\n(I + D− 1\n\n2 AD− 1\n\n2 ).\n\n(8)\n\nAs shown in Figure 3, in spatial domain, ˆS is obtained via 3 operations on adjacency matrix A: normalization, adding self-loops, and scaling all edge weights by 1 2 (a type of lazy random walk (Luan et al., 2020)), while ̃S in vanilla GCN/SGC contains 2 operations: adding self-loops and normalization.\n\nWith the help of transformation g, the depth d is redefined on real number domain, and the message propagation process of depth d can be realized via the following steps: (1) Eigen-decompose Lsym; (2) Calculate ˆSd via weight g(λi)d and the weighted sum of all eigengraphs: ˆSd = U ˆΛdU⊤ = (cid:80)n\n\ni (3) Multiply ˆSd with original node attributes X.\n\ni=1 g(λi)duiu⊤\n\nNegative depth explained. An intuitive explanation of negative d can be obtained from the perspective of matrix inverse and message diffusion process when d takes integer values. Since ˆS−1 ˆS = U ˆΛ−1U⊤U ˆΛ1U⊤ = I, ˆS−1 is the inverse matrix of ˆS. In diffusion dynamics, X can be viewed as an intermediate state generated in a series of message propagation steps. ˆSX effectively propagates the message one-step forward, while ˆS−1 can cancel the effect of ˆS on X and recover the original message by moving backward: ˆS−1 ˆSX = X. Accordingly, ˆS−1X traces back to the message’s previous state in the series. However, neither A or L has inverse due to their non-positive eigenvalues. More discussions on the impact of negative depth in spatial domain are presented in Section 4.4. Non-integer d indicates the back- or forward propagation can be a continuous process.\n\nFigure 3: The difference between ̃S (left) for GCN/SGC and ˆS (right) for RED-GCN.\n\nRED-GCN-S. By further making d a trainable parameter, we present our model, Redefined DepthGCN-Single (RED-GCN-S), whose final node embedding matrix is given by\n\nH = σ(ˆSdXW), where σ(·) is the nonlinear activation function; W is a trainable parameter matrix; and d is the trainable depth parameter. As observed from Figure 2b, weight distribution of different frequencies/eigengraphs is tuned via d: (1) for d = 0, the weight is uniformed distributed among all frequency components (g(λi)d = 1), which implies that no particular frequency is preferred by the graph signal; (2) for d > 0, weight g(λi)d decreases with the corresponding frequency, which indicates the low frequency components are favored so that RED-GCN-S effectively functions as a low-pass filter and therefore captures graph homophily; (3) for d < 0, high frequency components gains amplified weights so that RED-GCN-S serves as a high-pass filter capturing graph heterophily. During training, RED-GCN-S tunes its frequency filtering functionality to suit the underlying graph signal by automatically finding the optimal d.\n\n(9)\n\nRED-GCN-D. During optimization, RED-GCN-S embraces a single depth d unified for all eigengraphs and selects its preferences for either homophily or heterophily. However, RED-GCN-S\n\n5\n\nv!v\"v#v!v\"v#141414121212v!v\"v#11112D!\"#AD!\"#12I!Sv!v\"v#131313131313#S111Under review as a conference paper at ICLR 2023\n\nrequires a full eigen-decomposition of Lsym, which can be expensive for large graphs. Additionally, the high and low frequency components in a graph signal may not be mutually exclusive, namely, there exists the possibility for a graph to simultaneously possess homophilic and heterophilic counterparts. Therefore, we propose the second variant of RED-GCN: RED-GCN-D (Dual), which introduces two separate trainable depths, dh and dl, to gain more flexible weighting of the high and low frequency related eigengraphs respectively. Arnoldi method (Lehoucq et al., 1998) is adopted to conduct EVD on Lsym and obtain the top-K largest and smallest eigen-pairs (λi, ui)s. By denoting Ul = U[:, 0 : K] and Uh = U[:, n − K : n] (Ul and Uh ∈ Rn×K), we define a new diffusion matrix ˆSdual(dl, dh, K) as\n\nˆSdual(dl, dh, K) = Ul ˆΛdl\n\nl U⊤\n\nl + Uh ˆΛdh\n\nh U⊤ h ,\n\n(10)\n\nwhere ˆΛl ∈ RK×K and ˆΛh ∈ RK×K are diagonal matrices of the top-K smallest and largest eigenvalues. 2 The final node embedding of RED-GCN-D is presented as\n\nH = σ(ˆSdual(dl, dh, K)XW),\n\n(11)\n\nwhere depths dl and dh are trainable; and W is a trainable parameter matrix. We make RED-GCN-D scalable on large graphs by choosing K ≪ n, so that ˆSdual(dl, dh, K) approximates the full diffusion matrix by covering only a small subset of all eigengraphs. For small graphs, we use K = ⌊ n 2 ⌋ to include all eigengraphs, and ˆSdual(dl, dh, K) thus gains higher flexibility than ˆS with the help of the two separate depth parameters instead of a unified one.\n\nDifferences with ODE-based GNNs. Previous attempts on GNNs with continuous diffusion are mostly inspired by graph diffusion equation, an Ordinary Difference Equation (ODE) characterizing the dynamical message propagation process versus time. In contrast, our framework starts from discrete graph convolution operations without explicitly involving ODE. CGNN (Xhonneux et al., 2020) aims to build a deep GNN immune to over-smoothing by adopting the neural ODE framework (Chen et al., 2018). But its time parameter t is a non-trainable hyper-parameter predefined within the positive domain, which is the key difference with RED-GCN. A critical CGNN component for preventing over-smoothing, restart distribution (the skip connection from the first layer), is not needed in our framework. Moreover, CGNN applies the same depth to all frequency components, while RED-GCN-D has the flexibility to adopt two different depths respectively to be adaptive to high and low frequency components. GRAND (Chamberlain et al., 2021) introduces non-Eular multi-step schemes with adaptive step size to obtain more precise solutions of the diffusion equation. Its depth (total integration time) is continuous but still predefined/non-trainable and takes only positive values. DGC (Wang et al., 2021) decouples the SGC depth into two prefinded non-trainable hyper-parameters: a positive real-valued T controlling the total time and a positive integer-valued Kdgc corresponding to the number of diffusion steps. However, realizing negative depth in DGC is non-applicable since the implementation is through propagation by Kdgc times, rather than through an arbitrary real-valued exponent d on eigengraph weights in RED-GCN.\n\n4 EXPERIMENT\n\nIn this section, we evaluate the proposed RED-GCN on the semi-supervised node classification task on both homophilic graphs and heterophilic graphs.\n\n4.1 EXPERIMENT SETUP\n\nDatasets. We use 11 datasets for evaluation, including 4 homophilic graphs: Cora (Kipf & Welling, 2016), Citeseer (Kipf & Welling, 2016), Pubmed (Kipf & Welling, 2016) and DBLP (Bojchevski & Günnemann, 2017), and 7 heterophilic graphs: Cornell (Pei et al., 2020), Texas (Pei et al., 2020), Wisconsin (Pei et al., 2020), Actor (Pei et al., 2020), Chameleon (Rozemberczki et al., 2021), Squirrel (Rozemberczki et al., 2021), and cornell5 (Fey & Lenssen, 2019). We collect all datasets from the public GCN platform Pytorch-Geometric (Fey & Lenssen, 2019). For Cora, Citeseer, Pubmed with data splits in Pytorch-Geometric, we keep the training/validation/testing set split as in GCN (Kipf & Welling, 2016). For the remaining 8 datasets, we randomly split every dataset into 20/20/60% for training, validation, and testing. The statistics of all datasets are presented in Appendix.\n\n2Case λi = 2, namely g(λi) = 0, is excluded since it corresponds to the existence of bipartite components.\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nBaselines and Metrics. We compare our model with 7 baseline methods,including 4 classic GNNs: GCN (Kipf & Welling, 2016), SGC (Wu et al., 2019), APPNP (Klicpera et al., 2018) and ChebNet (Defferrard et al., 2016), and 3 GNNs tailored for heterophilic graphs: FAGCN (Bo et al., 2021), GPRGNN (Chien et al., 2020) and H2GCN (Zhu et al., 2020). Accuracy (ACC) is used as the evaluation metric. We report the average ACCs with the standard deviation (std) for all methods, each obtained by 5 runs with different initializations.\n\nImplementation Details. See Appendix due to the page limit.\n\n4.2 NODE CLASSIFICATION\n\nThe semi-supervised node classification performances on homophilic graphs and heterophilic graphs are shown in Table 1 and Table 2 respectively.\n\nHomophilic graphs. From Table 1, it is observed that different methods have similar performance on homophilic graphs. RED-GCN-S achieves the best accuracies on two datasets: Cora and DBLP. On the remaining two datasets, RED-GCN-S is only 1.1% and 0.4% below the best baselines (APPNP on Citeseer and SGC on Pubmed). For RED-GCN-D, it obtains similar performance as the other methods, even though it only uses the top-K largest/smallest eigen-pairs.\n\nTable 1: Performance comparison (mean±std accuracy) on homophilic graphs.\n\nDatasets GCN SGC APPNP GPRGNN FAGCN H2GCN ChebNet RED-GCN-S RED-GCN-D\n\nCora 80.8±0.8 80.9±0.4 81.0±1.0 82.0±0.7 80.3±0.4 78.8±1.0 78.8±0.5 82.5±1.1 82.4 ±0.7\n\nCiteseer 70.5±0.6 70.8±0.8 71.9±0.4 69.3±0.9 71.7±0.8 70.5±1.0 71.1±0.4 70.8±0.7 70.6 ±0.6\n\nPubmed 78.8±0.6 79.6±0.4 79.3±0.2 78.6±0.7 78.5±0.9 77.9±0.3 78.1±0.8 79.2±0.2 77.9 ±0.3\n\nDBLP 84.1±0.2 84.1±0.2 83.0±0.5 84.5±0.3 82.4±0.7 82.4±0.3 83.1±0.1 84.7±0.3 84.2±0.2\n\nTable 2: Performance comparison (mean±std accuracy) on heterphilic graphs.\n\nDatasets GCN SGC APPNP GPRGNN FAGCN H2GCN ChebNet RED-GCN-S RED-GCN-D\n\nTexas 55.9±3.4 58.7±3.1 55.1±3.7 61.3 ±5.8 60.2±7.8 68.8±6.5 76.2±2.9 77.6±5.9 77.1 ±2.5\n\nCornell 44.3±4.4 43.8±4.4 51.5±2.4 53.3±4.6 54.8±7.4 61.4±4.4 66.7±3.9 72.0±5.8 72.0 ±2.8\n\nWisconsin 51.4±2.2 47.3±2.1 58.0±3.1 71.0±4.8 60.1±5.2 69.9±5.3 75.4±3.5 82.0±2.6 81.5 ±2.4\n\nActor 27.5±0.5 28.0±0.8 32.8±0.8 33.6±0.4 32.3±0.5 33.9±0.3 34.3±0.5 35.3±0.7 27.6 ±0.8\n\nSquirrel 35.8±1.3 37.2±1.8 29.5±0.9 34.1±1.0 31.2±1.6 30.4±0.9 31.8±0.5 38.2±1.2 44.2±0.9\n\nChameleon 55.2±1.8 55.3±1.0 46.7±0.8 55.0±3.9 50.4±1.9 48.8±1.9 49.6±1.8 55.7±1.3 56.9±0.9\n\ncornell5 67.9±0.2 67.4±0.5 68.3±0.5 67.3±0.3 68.3±0.7 68.4±0.2 OOM 68.5±0.4 70.0±0.2\n\nHeterophilic graphs. RED-GCN-S/RED-GCN-D outperforms every baseline on all heterophilic graphs, as shown in Table 2. These results demonstrate that without manually setting the model depth and without the prior knowledge of the input graph, RED-GCN has the capability of automatically detecting the underlying graph heterophily. We have an interesting observation: on 3 large datasets, Squirrel, Chameleon, and cornell5, even with only a small portion of the eigengraphs, RED-GCN-D is able to achieve better performance than RED-GCN-S with the complete set of eigengraphs. This suggests that the graph signal in some real-world graphs might be dominated by a few low and high frequency components, and allowing two independent depth parameters in RED-GCN-D brings the flexibility to capture the low and high frequencies at the same time.\n\n4.3\n\nTRAINABLE DEPTH\n\nA systematic study is conducted on the node classification performance w.r.t the trainable depth d.\n\nOptimal depth. In Figure 4, the optimal depths and their corresponding classification accuracies are annotated. For two homophilic graphs, Cora and Citeseer, the optimal depths are positive (5.029 and 3.735) in terms of the best ACCs, while for two heterophilic graphs, Actor and Squirrel, the optimal depths are negative (−0.027 and −3.751). These results demonstrate our model indeed automatically capture graph heterophily/homophily by finding the suitable depth to suppress or amplify the relative weights of the corresponding frequency components. Namely, high/low frequency components are suppressed for homophilic/heterophilic graphs respectively.\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(a) Cora\n\n(b) Citeseer\n\n(c) Actor\n\n(d) Squirrel\n\nFigure 4: Node classification accuracy w.r.t. the trainable depth d on four datasets: Cora, Citeseer, Actor and Squirrel. (the optimal d, accuracy) is annotated (e.g., (-0.027, 36.9%) for Actor).\n\nClose to zero depth. For the two homophilic graphs in Figures 4a and 4b, sharp performance drop is observed when depth d approaches 0, since the eigengraphs gain close-to-uniform weights. For the heterophilic Actor dataset, its optimal depth −0.027 is close to 0, as shown in Figure 4c. In addition, the performance of RED-GCN-D (27.6%) is similar to that of GCN (27.5%), both of which are much worse than RED-GCN-S (35.3%). This result indicates that Actor is a special graph where all frequency components have similar importance. Due to the absence of the intermediate frequency components between the high- and low-end ones, the performance of RED-GCN-D is severely impacted. For vanilla GCN, the suppressed weights of the high frequency components deviate from the near-uniform spectrum and thus lead to low ACC on this dataset.\n\n4.4 GRAPH AUGMENTATION AND GEOMETRIC INSIGHTS\n\nTable 3: The performance of one-layer vanilla GCN over the augmented ˆSd.\n\nIt is especially interesting to analyze what change a negative depth brings to the spatial domain and how such change impacts the subsequent model performance. Graph augmentation. By picking the optimal depth d according to the best performance on the validation set, a new diffusion matrix ˆSd is obtained. With the optimal d fixed, 2 ̃A ̃D− 1 substituting the normalized adjacency matrix ̃D− 1 in Eq. 1 by ˆSd is equivalent to applying the vanilla GCN to a new topology. This topology effectively plays the role of a structural augmentation for the original graph. The impact of such augmentation on performance is tested on 3 heterophilic graphs: Texas, Cornell and Wisconsin, as shown in Table 3. Apparently, for the vanilla GCN, the performance obtained with this new topology is superior over that with the raw input graph: it dramatically brings 20%-30% lifts in ACC. Moreover, the augmented topologies also make vanilla GCN outperform RED-GCN-S and RED-GCN-D on 2 out of the 3 datasets. By nature, the augmented graph is a re-weighted linear combination of the eigengraphs, and its topological structure intrinsically assigns higher weights to eigengraphs corresponding to higher frequencies, as shown in Figures 5.\n\nDatasets GCN RED-GCN-S RED-GCN-D GCN (ˆSd)\n\nTexas 55.9 77.6 77.1 75.9\n\nCornell Wisconsin\n\n51.4 82.0 81.5 83.4\n\n44.3 72.0 72.0 72.7\n\n2\n\nGeometric properties. To further understand how the topology of ˆSd with a negative optimal d differs from that of ˆS and why the performance is significantly boosted, a heat map of (ˆSd − ˆS) is presented in Figure 6 for Cornell. 3 First, the dark red diagonal line in the heat map indicates the weights of self-loops are significantly strengthened in the augmented graph, and as a result, in consistency with the previous findings (Zheng et al., 2022a), the raw node attributes make more contributions in determining their labels. These strengthened self-weights also play the similar role as restart distribution or skip connections (Xhonneux et al., 2020) preventing the node embeddings becoming over-smoothed. In addition, there is a horizontal line and a vertical line (light yellow line marked by dashed ovals) in the heat map in Figure 6, correspond to the hub node in the graph, namely the node with the largest degree. Interestingly, the connections between this node and most other nodes in the graph experience a negative weight change. Therefore, the influence of the\n\nFigure 5: The weights of eigengraphs w.r.t. eigenvalues on the augmented diffusion matrix ˆSd and original ˆS for Cornell (d = −0.362).\n\n3Heat maps for Texas and Wisconsin are in Appendix with similar observations.\n\n8\n\n012345d505560657075808590Accuracy(%)the optimal (5.029, 82.8%)Performance on Cora012345d50556065707580Accuracy(%)the optimal (3.735, 71.4%)Performance on Citeseer101d202530354045Accuracy(%)the optimal (-0.027, 36.9%)Performance on Actor3.93.83.73.63.53.43.3d20253035404550Accuracy(%)the optimal (-3.751,41.5%)Performance on Squirrel0.00.20.40.60.81.01.21.41.61.80.00.51.01.52.02.53.0WeightsSdSUnder review as a conference paper at ICLR 2023\n\nhub node on most other nodes are systematically reduced. Consequently, the augmentation amplifies the deviations between node embeddings and facilitates the characterization of graph heterophily.\n\n5 RELATED WORKS\n\nGraph Convolutional Network (GCN). GCN models can be mainly divided into two categories: (1) spectral graph convolutional networks and (2) spatial convolutional networks. In (1), Spectral CNN (Bruna et al., 2013) borrows the idea from convolutional neural network (Goodfellow et al., 2016) to construct a diagonal matrix as the convolution kernel. ChebNet (Defferrard et al., 2016) adopts a polynomial approximation of the convolution kernel. GCN (Kipf & Welling, 2016) further simplifies the ChebNet via the first order approximation. Recently, (He et al., 2021; Bianchi et al., 2021; Wang & Zhang, 2022) propose more advanced filters as the convolution kernel. Most works in (2) follow the message-passing mechanism. GraphSAGE (Hamilton et al., 2017) iteratively aggregates features from local neighborhood. GAT (Veliˇckovi ́c et al., 2017) applies self-attention to the neighbors. APPNP (Klicpera et al., 2018) deploys personalized pagerank (Tong et al., 2006) to sample nodes for aggregration. MoNet (Monti et al., 2017) unifies GCNs in the spatial domain.\n\nFigure 6: The difference between the augmented diffusion matrix and the original one ˆSd − ˆS for Cornell in heat map. Best viewed in color.\n\nThe Depth of GCN and Over-smoothing. A large amount of works focus on the over-smoothing issue. Its intrinsic cause is demystified: a linear GCN layer is a Laplacian smoothing operator (Li et al., 2018; Wu et al., 2019). PairNorm (Zhao & Akoglu, 2019) forces distant nodes to be distinctive by adding an intermediate normalization layer. Dropedge (Rong et al., 2019), DeepGCN (Li et al., 2019), AS-GCN (Huang et al., 2018), and JK-net (Xu et al., 2018) borrow the idea of ResNet (He et al., 2016) to dis-intensify smoothing. DeeperGXX (Zheng et al., 2021) adopts a topology-guided graph contrastive loss for connected node pairs to obtain discriminative representations. Most works aim to build deep GCNs (i.e., d is a large positive integer) by reducing over-smoothing, while RED-GCN extends the depth from N+ to R and explores the negative depth.\n\nNode Classification on Homophilic and Heterophilic Graphs. GCN/GNN models mostly follow the homophily assumption that connected nodes tend to share similar labels (Kipf & Welling, 2016; Veliˇckovi ́c et al., 2017; Hamilton et al., 2017). Recently, heterophilic graphs, in which neighbors often have disparate labels, attract lots of attention. Geom-GCN (Pei et al., 2020) and H2GCN (Zhu et al., 2020) extend the neighborhood for aggregation. FAGCN (Bo et al., 2021) and GPRGNN (Chien et al., 2020) adaptively integrate the high/low frequency signals with trainable parameters. Alternative message-passing mechanisms have been proposed in HOG-GCN (Wang & Zhang, 2022) and CPGNN (Zhu et al., 2021). The latest related works include ACM-GCN (Luan et al., 2021; 2022b), LINKX (Lim et al., 2021), BernNet (He et al., 2021), GloGNN (Li et al., 2022) and GBKGNN (Du et al., 2022). Other works can be found in a recent survey (Zheng et al., 2022b).\n\n6 CONCLUSION AND FUTURE WORK\n\nTo our best knowledge, this work presents the first effort to make GCN’s depth trainable by redefining it on the real number domain. We unveil the interdependence between negative GCN depth and graph heterophily. A novel problem of automatic GCN depth tuning for graph homophily/heterophily detection is formulated, and we propose a simple and powerful solution named RED-GCN with two variants (RED-GCN-S and RED-GCN-D). An effective graph augmentation method is also discussed via the new understanding on the message propagation mechanism generated by the negative depth. Superior performance of our method is demonstrated via extensive experiments with semi-supervised node classification on 11 graph datasets. The new insights on GCN’s depth obtained by our work may open a new direction for future research on spectral and spatial GNNs. Since RED-GCN requires to conduct eigen-decomposition of the graph Laplacian, it is not directly applicable to inductive and dynamic graph learning problems, which we leave for future exploration.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nFilippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks with convolutional arma filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\n\nDeyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutional networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 3950–3957, 2021.\n\nAleksandar Bojchevski and Stephan Günnemann. Deep gaussian embedding of graphs: Unsupervised\n\ninductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.\n\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\n\nconnected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.\n\nBen Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and In International Conference on Machine\n\nEmanuele Rossi. Grand: Graph neural diffusion. Learning, pp. 1407–1418. PMLR, 2021.\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\n\ndifferential equations. Advances in neural information processing systems, 31, 2018.\n\nZhengdao Chen, Lisha Li, and Joan Bruna. Supervised community detection with line graph neural\n\nnetworks. In International conference on learning representations, 2020.\n\nEli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank\n\ngraph neural network. arXiv preprint arXiv:2006.07988, 2020.\n\nFan RK Chung and Fan Chung Graham. Spectral graph theory, volume 92. American Mathematical\n\nSoc., 1997.\n\nMichaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems, 29, 2016.\n\nLun Du, Xiaozhou Shi, Qiang Fu, Xiaojun Ma, Hengyu Liu, Shi Han, and Dongmei Zhang. Gbkgnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily. In Proceedings of the ACM Web Conference 2022, pp. 1550–1558, 2022.\n\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. IEEE\n\nSignal Processing Magazine, 30:83–98, 2019.\n\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\n\nAdvances in neural information processing systems, 30, 2017.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n\nMingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via bernstein approximation. Advances in Neural Information Processing Systems, 34:14239–14251, 2021.\n\nWenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph\n\nrepresentation learning. Advances in neural information processing systems, 31, 2018.\n\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.\n\narXiv preprint arXiv:1609.02907, 2016.\n\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate: Graph\n\nneural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nRichard B Lehoucq, Danny C Sorensen, and Chao Yang. ARPACK users’ guide: solution of large-\n\nscale eigenvalue problems with implicitly restarted Arnoldi methods. SIAM, 1998.\n\nGuohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep In Proceedings of the IEEE/CVF international conference on computer vision, pp.\n\nas cnns? 9267–9276, 2019.\n\nQimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelligence, 2018.\n\nXiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian. Finding global homophily in graph neural networks when meeting heterophily. arXiv preprint arXiv:2205.07308, 2022.\n\nDerek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems, 34:20887–20902, 2021.\n\nSitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multiscale deep graph convolutional networks. Advances in neural information processing systems, 32, 2019.\n\nSitao Luan, Mingde Zhao, Chenqing Hua, Xiao-Wen Chang, and Doina Precup. Complete the missing half: Augmenting aggregation filtering with diversification for graph convolutional networks. arXiv preprint arXiv:2008.08844, 2020.\n\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Is heterophily a real nightmare for graph neural networks to do node classification? arXiv preprint arXiv:2109.05641, 2021.\n\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang, and Doina Precup. When do\n\nwe need gnn for node classification? arXiv preprint arXiv:2210.16979, 2022a.\n\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang, and Doina Precup. Revisiting heterophily for graph neural networks. arXiv preprint arXiv:2210.07606, 2022b.\n\nFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5115–5124, 2017.\n\nHongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric\n\ngraph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.\n\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph\n\nconvolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.\n\nBenedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal\n\nof Complex Networks, 9(2):cnab014, 2021.\n\nKamal Shah and Baver Okutmu ̧stur. Functional Calculus. BoD–Books on Demand, 2020.\n\nDavid I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE signal processing magazine, 30(3):83–98, 2013.\n\nGabriel Taubin. A signal processing approach to fair surface design. In Proceedings of the 22nd\n\nannual conference on Computer graphics and interactive techniques, pp. 351–358, 1995.\n\nHanghang Tong, Christos Faloutsos, and Jia-Yu Pan. Fast random walk with restart and its applications.\n\nIn Sixth international conference on data mining (ICDM’06), pp. 613–622. IEEE, 2006.\n\nPetar Veliˇckovi ́c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\n\nBengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nXiyuan Wang and Muhan Zhang. How powerful are spectral graph neural networks. arXiv preprint\n\narXiv:2205.11172, 2022.\n\nYifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process in linear graph convolutional networks. Advances in Neural Information Processing Systems, 34: 5758–5769, 2021.\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861–6871. PMLR, 2019.\n\nLouis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In International\n\nConference on Machine Learning, pp. 10432–10441. PMLR, 2020.\n\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In International conference on machine learning, pp. 5453–5462. PMLR, 2018.\n\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pp. 40–48. PMLR, 2016.\n\nMuhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural\n\ninformation processing systems, 31, 2018.\n\nLingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International\n\nConference on Learning Representations, 2019.\n\nLecheng Zheng, Dongqi Fu, and Jingrui He. Tackling oversmoothing of gnns with contrastive\n\nlearning. arXiv preprint arXiv:2110.13798, 2021.\n\nWenqing Zheng, W Edward Huang, Nikhil Rao, Sumeet Katariya, Zhangyang Wang, and Karthik Subbian. Cold brew: Distilling graph node represen- tations with incomplete or missing neighborhoods. In International Conference on Learning Representations, 2022a.\n\nXin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks for\n\ngraphs with heterophily: A survey. arXiv preprint arXiv:2202.07082, 2022b.\n\nJiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793–7804, 2020.\n\nJiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra. Graph neural networks with heterophily. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11168–11176, 2021.\n\n12",
  "translations": [
    "# Summary Of The Paper\n\nIn this work, by redefining GCN’s depth d as a trainable parameter continuously adjustable within positive infinity and negative infinity, a simple and powerful GCN model RED-GCN is proposed to retain the simplicity of GCN and meanwhile automatically search for the optimal d without the prior knowledge regarding whether the input graph is homophilic or heterophilic.\n\n# Strength And Weaknesses\n\nStrength: The problem is interesting and usefull for GCN designing.\nWeaknesses: The motivation and the physical meaning of the negative depth value are not clear. When the depth of GCN is negative, what dose it mean?\n\n# Clarity, Quality, Novelty And Reproducibility\n\nThe problem is interesting and the proposed method is novel. But the physical meaning of negative results is not clear.\n\n# Summary Of The Review\n\nThe problem is interesting and the proposed method is novel. But the physical meaning of negative results is not clear.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper titled \"RED-GCN: Revisit the Depth of Graph Convolutional Network\" addresses the challenge of optimizing the depth of Graph Convolutional Networks (GCNs) by introducing a novel approach that treats depth as a continuous real-valued parameter rather than a fixed positive integer. The proposed model, RED-GCN, allows for automatic depth tuning, which enhances the representation of both homophilic (similar label) and heterophilic (dissimilar label) graphs. The methodology is supported by a theoretical analysis using eigen-decomposition of the graph Laplacian and is validated through extensive experiments across 11 datasets, showing that RED-GCN significantly outperforms traditional GCNs and other state-of-the-art methods in various node classification tasks.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to depth optimization in GCNs, addressing an important limitation of traditional models regarding over-smoothing and depth selection. The introduction of a trainable depth parameter that adapts to graph characteristics is particularly noteworthy, as it opens avenues for improved performance in both homophilic and heterophilic scenarios. The extensive experimentation on diverse datasets and the development of a novel graph augmentation method further solidify the paper's contributions. However, a potential weakness lies in the complexity of the model and the computational demands associated with training and tuning the proposed depth parameter, which may limit its practical applicability in real-world scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulately presents its ideas, with clear definitions and explanations of concepts such as eigengraphs and the implications of depth in GCNs. The quality of writing is high, and the theoretical underpinnings are solid, enhancing the reader's understanding of the proposed methodology. The novelty of treating depth as a continuous parameter is significant, contributing to the field's understanding of GCN limitations. Reproducibility is facilitated by the thorough description of the methodology and experiments, although availability of code and data would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of GCNs by proposing a continuous depth parameter that enhances model flexibility and performance. With robust theoretical backing and empirical validation, RED-GCN is a valuable contribution that addresses a key challenge within graph neural networks. However, considerations regarding computational complexity may affect its practical deployment.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"RED-GCN: Revisit the Depth of Graph Convolutional Network\" introduces a novel approach for determining the depth of Graph Neural Networks (GNNs) by treating depth \\(d\\) as a continuous, trainable parameter rather than a fixed positive integer. This flexibility allows for improved representation power, particularly in distinguishing between homophilic and heterophilic graph structures. The authors propose two variants of the model: RED-GCN-S, which maintains a unified depth across eigengraphs, and RED-GCN-D, which uses separate depths for high and low-frequency eigengraphs. The experimental results demonstrate that RED-GCN outperforms baseline models on heterophilic datasets and shows competitive performance on homophilic datasets. Furthermore, the paper provides a theoretical framework that connects graph properties with model performance, highlighting the impact of optimal depth on accuracy.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative perspective on GCN depth, which allows for adaptive tuning based on data characteristics, and its comprehensive theoretical framework that relates graph properties to model effectiveness. The model shows impressive performance on heterophilic datasets, significantly improving accuracy compared to traditional GNNs. However, the requirement for eigen-decomposition may limit scalability, especially for large or dynamic graphs. Additionally, while the exploration of negative depth is a novel contribution, it introduces additional complexity that could complicate practical implementations. Finally, the results, though promising, would benefit from further validation across a broader range of datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The theoretical underpinnings are presented logically, making the complex concepts more accessible. The empirical results are thorough, with detailed comparisons against baseline methods. However, the implementation details, while included, could be expanded for better reproducibility, especially regarding hyperparameter tuning and computational resources required for eigen-decomposition.\n\n# Summary Of The Review\nOverall, \"RED-GCN\" presents a significant advancement in the field of Graph Neural Networks by innovatively redefining the concept of depth as a trainable parameter. While the model demonstrates superior performance on heterophilic graphs and provides a strong theoretical foundation, its scalability and complexity in implementation warrant further investigation.\n\n# Correctness\n4/5 - The methodology appears sound, and the results are consistent with the claims made in the paper. However, some aspects, like the scalability of eigen-decomposition, could affect practical correctness.\n\n# Technical Novelty And Significance\n5/5 - The introduction of a trainable depth parameter and the concept of eigengraphs represent a notable advancement in GNNs, adding significant value to the theoretical and practical understanding of graph representation.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are compelling, particularly the model's performance on heterophilic datasets. Nonetheless, additional validation across diverse real-world datasets would strengthen the claims of novelty and significance.",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach to Graph Convolutional Networks (GCNs) that allows the depth \\(d\\) to be a trainable real-valued parameter. This flexibility aims to enhance the model's expressiveness in handling graph homophily and heterophily. The authors leverage the eigen-decomposition of the graph Laplacian and functional calculus to introduce a high-pass filtering mechanism for heterophilic graphs, thus enabling performance improvements in node classification tasks across various datasets. Experimental results demonstrate that RED-GCN outperforms traditional GCNs and models specifically designed for heterophilic graphs, suggesting its effectiveness in both homophilic and heterophilic scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to redefining the depth of GCNs, which is traditionally constrained to positive integers. By introducing trainable real-valued depths, the authors provide a significant contribution to the expressiveness of GCNs, particularly in distinguishing between homophilic and heterophilic graph structures. The extensive empirical validation across 11 datasets supports the claims of improved performance. However, a potential weakness is that the focus is primarily on node classification tasks, which may limit the generalizability of the findings to other graph learning problems. Additionally, the theoretical implications of negative depths could be elaborated further for clarity.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The theoretical foundations are laid out coherently, and the experiments are well-defined with appropriate metrics and baselines. However, while the novelty of allowing for a trainable depth parameter is significant, the paper could improve its reproducibility by providing more details on hyperparameter tuning and the implementation of the model. Overall, the clarity and quality of the writing are commendable, but some sections would benefit from additional detail.\n\n# Summary Of The Review\nThe paper introduces a pioneering approach to GCNs by making depth a trainable real-valued parameter, enhancing the model's ability to handle diverse graph structures. Its extensive empirical results demonstrate clear advantages over existing methods, although further exploration beyond node classification and additional implementation details could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach to Graph Convolutional Networks (GCNs) that redefines the concept of depth from a positive integer to a continuous real number. This innovative framework allows for automatic tuning of GCN depth without prior knowledge of the graph's characteristics, particularly enhancing performance on heterophilic graphs. The authors introduce a graph augmentation method that leverages the optimal depth to create new topologies, and they provide extensive empirical validation across 11 datasets, demonstrating the robustness of the RED-GCN model. The theoretical foundations are grounded in eigen-decomposition and functional calculus, supporting the proposed methodologies.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to GCN depth, allowing for greater model flexibility and expressiveness. The automatic depth tuning mechanism is a significant advancement, especially for heterophilic graphs, where RED-GCN outperforms existing models in node classification tasks. The introduction of a graph augmentation method also contributes positively to GCN performance. However, the paper has notable weaknesses, including the lack of comprehensive validation on dynamic graphs and the potential complexity of its theoretical constructs, which could hinder practical implementation. Additionally, the sensitivity of the depth tuning process to initial conditions raises concerns about reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally strong, though some concepts, such as negative depth, could be better explained to enhance understanding for practitioners. The quality of the experimental validation is commendable, but the generalizability of results across diverse datasets remains untested. The novelty of the approach is high, particularly in redefining GCN depth; however, reproducibility may be challenged by computational complexities and the need for eigen-decomposition. The methodology could benefit from clearer visualizations and tools to assist users in interpreting model adjustments and performance outcomes.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of graph neural networks by proposing a novel and flexible approach to GCN depth and demonstrating promising results on heterophilic graphs. However, certain limitations in theoretical clarity, computational scalability, and empirical validation need to be addressed to strengthen the paper's impact and applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces RED-GCN, a novel framework for Graph Convolutional Networks (GCNs) that redefines depth as a continuous, trainable parameter. This innovative approach enhances the expressiveness of GCNs by enabling them to model both homophilic and heterophilic graph structures effectively. Key contributions include the introduction of a continuous depth parameterization, the development of eigengraphs for graph augmentation, and a solid theoretical foundation supporting the feasibility of real-valued depths. Extensive experimental validation across 11 graph datasets demonstrates RED-GCN's superior performance in semi-supervised node classification tasks compared to traditional GCNs and other state-of-the-art methods.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its novel methodological contributions and robust empirical validation. The continuous depth parameterization is a significant advancement that allows for greater flexibility in GCNs, and the theoretical insights regarding eigen-decomposition enhance the understanding of the model's capabilities. However, the paper does have limitations, including potential computational complexity due to eigen-decomposition, which may hinder scalability for large or dynamic graphs. Additionally, while performance is demonstrated on selected datasets, the generalizability of the RED-GCN framework across different graph types and structures requires further investigation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its contributions clearly, making it accessible to readers. The quality of the writing is high, and the theoretical and empirical results are well-articulated. The novelty of the continuous depth redefinition is a significant advancement in the field, and the reproducibility is supported by extensive experimental results and comparisons with baseline methods. However, the computational burden associated with eigen-decomposition may pose challenges for practitioners seeking to implement the model in real-world scenarios.\n\n# Summary Of The Review\nOverall, this paper presents a substantial contribution to the field of graph neural networks by redefining depth in a continuous manner, enabling improved modeling of graph structures. The theoretical foundations and empirical results provide a strong basis for the proposed methodology, making RED-GCN a promising direction for future research. However, considerations regarding computational complexity and generalizability warrant further exploration.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "## Review of ICLR Paper: \"RED-GCN: Revisit the Depth of Graph Convolutional Network\"\n\n### Summary Of The Paper\nThe paper introduces RED-GCN, a novel approach to enhancing the robustness of Graph Convolutional Networks (GCNs) against adversarial attacks by treating the depth of the network as a trainable real-valued parameter. This adaptive depth mechanism allows the model to tune its architecture dynamically based on input data characteristics, particularly in adversarial contexts. The authors provide a theoretical foundation through spectral graph theory, demonstrating how different eigenvalues can help filter out adversarial noise. Empirical results show that RED-GCN outperforms existing models in node classification tasks on both homophilic and heterophilic graphs, validating the proposed framework's effectiveness.\n\n### Strength And Weaknesses\n**Strengths:**\n- The introduction of a trainable depth parameter is a novel contribution that significantly enhances the flexibility of GCNs, particularly in adversarial settings.\n- The theoretical underpinnings provided through eigen-decomposition lend credibility to the model's approach and findings.\n- Comprehensive empirical validation across multiple datasets showcases the robustness of the method against various adversarial strategies, reinforcing the practical applicability of the research.\n\n**Weaknesses:**\n- The complexity introduced by a continuous depth parameter may complicate the model's training and implementation, possibly increasing computational costs.\n- The focus on transductive settings limits the exploration of the method's applicability to inductive learning scenarios, which could restrict its broader use.\n- There is a potential risk of overfitting to adversarial examples due to the adaptive depth tuning, which might impair generalization on unseen data.\n\n### Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible for readers. The quality of writing is high, with a coherent flow of ideas. The novelty is significant, as it proposes a fresh perspective on GCN depth management. However, the reproducibility could be impacted by the complexity of the model's implementation, which may require additional resources and expertise to effectively replicate the results.\n\n### Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of adversarial training in graph neural networks by proposing a flexible and adaptive depth tuning mechanism. The strong theoretical foundation and extensive empirical validation enhance the work's significance. Future research could benefit from exploring inductive learning implications and refining the method for practical implementation.\n\n### Correctness\n4/5\n\n### Technical Novelty And Significance\n5/5\n\n### Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach to Graph Convolutional Networks (GCNs) that redefines the concept of depth from a discrete integer to a continuous real number, claiming to enhance model expressiveness significantly. The methodology includes the introduction of eigengraphs and a framework for automatic depth tuning based on graph properties, purportedly allowing the model to adapt dynamically without prior knowledge of these properties. The authors conduct experiments on 11 datasets, asserting that RED-GCN outperforms traditional models in node classification tasks, although the performance gains reported are marginal.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to redefining GCN depth and its proposed automatic depth tuning mechanism, which could simplify model training. However, the claims of superior performance are not strongly supported by significant empirical evidence, as the improvements over baseline models appear to be minimal. Additionally, the complexity of the proposed mathematical framework may limit its practical applicability, and the discussion lacks substantial backing for some of its more ambitious claims regarding the benefits of negative depth.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by the complexity of the mathematical concepts introduced, which may hinder reproducibility for practitioners in the field. While the novelty of redefining GCN depth is noteworthy, the practical implications and real-world applicability of the proposed methods remain unclear. The experiments, while extensive, do not convincingly demonstrate the promised advancements, raising questions about the overall quality and impact of the research.\n\n# Summary Of The Review\nOverall, the paper offers an intriguing take on GCN depth management and proposes a framework that could have implications for future research. However, the actual contributions appear to be limited in scope, and the performance improvements claimed may not justify the complexity introduced by the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper introduces RED-GCN, a graph convolutional network (GCN) model featuring a trainable depth parameter \\(d\\) that allows for values beyond the traditional positive integers, including negative values. The authors argue that such flexibility enhances the model's performance on heterophilic graphs, which are often challenging for conventional GCNs that assume homophily. The methodology includes a novel graph augmentation technique using eigengraphs, and the findings indicate RED-GCN's superior performance in node classification tasks across a diverse set of datasets, outperforming baseline models, particularly in heterophilic scenarios.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the limitations of traditional GCNs by allowing depth to be adjusted dynamically, which is particularly beneficial for heterogeneous graph structures. Furthermore, the empirical results demonstrate the effectiveness of RED-GCN across multiple datasets, showcasing its versatility in both homophilic and heterophilic contexts. However, a potential weakness is the limited exploration of the implications of negative depths in dynamic and inductive graph learning, which could be further elaborated upon to strengthen the contribution of this aspect.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivations behind the proposed model. The experimental setup is straightforward, with comprehensive results that are presented in an accessible manner. The novelty of RED-GCN is evident in its approach to depth tuning and graph augmentation. However, while the results appear robust, the paper does not provide extensive details on the reproducibility of experiments, such as data preprocessing and hyperparameter tuning, which could enhance the accessibility of the work for future researchers.\n\n# Summary Of The Review\nOverall, RED-GCN presents a significant advancement in the field of graph neural networks by enabling flexible depth tuning, which is crucial for effectively dealing with both homophilic and heterophilic graphs. The paper is clear, well-executed, and demonstrates substantial empirical validation. However, further exploration of the implications of the model's innovative features could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to Graph Convolutional Networks (GCNs) by redefining the depth of the network as a continuous real number rather than a positive integer. This new formulation, referred to as RED-GCN (Real-valued Depth Graph Convolutional Network), aims to address the over-smoothing issue commonly encountered in GCNs, particularly when applied to heterophilic graphs. The authors propose making the depth a trainable parameter, thereby allowing the model to adaptively tune itself based on the structure of the input graph. They also introduce a graph augmentation technique based on the optimal depth found during training, claiming significant performance improvements across various datasets.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to depth representation in GCNs and its potential to address the over-smoothing problem. The introduction of a trainable depth parameter could enhance model adaptability and performance. However, there are notable weaknesses, including the reliance on eigen-decomposition of the graph Laplacian, which may limit scalability to larger graphs, and the need for empirical validation of the proposed augmentation method across diverse real-world scenarios. Additionally, while the distinctions between homophilic and heterophilic graphs are discussed, there may be limitations in this binary classification.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly, although some sections could benefit from more detailed explanations, particularly regarding the implications of negative depth. The novelty of redefining depth in GCNs is significant, but the reproducibility of the results may be questioned due to the complexity introduced by trainable parameters and the reliance on specific graph structures. The experiments provided do demonstrate improvements, yet further validation is necessary to establish generalizability.\n\n# Summary Of The Review\nOverall, the paper introduces a compelling and novel approach to GCNs by redefining depth and addressing over-smoothing concerns. While the proposed model shows promise, its practical applicability may be hindered by scalability issues and the need for more robust empirical validation across varied graph types.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach to Graph Convolutional Networks (GCNs) that redefines the depth of the network as a continuous trainable parameter. This methodology seeks to enhance the representational power of GCNs, particularly in the context of both homophilic and heterophilic graph structures. The findings demonstrate that RED-GCN outperforms traditional GCNs and baseline methods in node classification tasks across multiple datasets, with a specific emphasis on its ability to adaptively tune depth based on the underlying graph properties.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to treating depth as a trainable parameter, which addresses the common issues of over-smoothing and manual depth selection in GCNs. The systematic analysis of the optimal depth and its correlation with graph properties adds significant value to the methodology. However, the paper could benefit from further exploration of the implications of negative depth and additional ablation studies to solidify claims regarding performance improvements. Additionally, while the model shows promise, its effectiveness in real-world applications remains underexplored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its motivations, methodology, and findings. The quality of the writing is high, and the use of mathematical foundations is appropriate and informative. While the methodology is novel, the reproducibility of results could be improved by providing more detailed experimental setups and hyperparameter settings. Overall, the clarity is strong, but additional details could enhance reproducibility for future researchers.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of graph neural networks by introducing a flexible depth parameter that enhances performance across different graph types. While the methodology is well-executed and clearly presented, further exploration of certain aspects could strengthen the overall impact of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to few-shot learning, offering insights into the challenge of effectively generalizing from limited labeled data. The authors propose a new framework based on meta-learning that aims to enhance model performance in scenarios with scarce training examples. Through extensive experiments on benchmark datasets, the proposed method demonstrates improved accuracy and robustness compared to existing state-of-the-art techniques.\n\n# Strength And Weaknesses\n## Strengths\n- **Innovative Methodology**: The paper introduces a meta-learning technique that integrates episodic training, which is a fresh perspective on few-shot learning and addresses existing limitations in model generalization.\n- **Robust Empirical Validation**: The authors perform comprehensive experiments across multiple datasets, showcasing the effectiveness of their approach in various settings, which bolsters the credibility of their claims.\n- **Clear Presentation**: The structure of the paper is logical and coherent, making the complex concepts accessible to readers with varying levels of expertise in the field.\n\n## Weaknesses\n- **Limited Baseline Comparisons**: While the paper presents competitive results, it could benefit from a broader comparison with more recent methods, particularly those published in the last year, to better illustrate its novelty and effectiveness.\n- **Scalability Issues**: The authors do not sufficiently address the scalability of their method when applied to larger datasets, which could limit its practical applicability in real-world scenarios.\n- **Ambiguity in Theoretical Analysis**: Some theoretical claims made in the paper lack rigorous justification, which could leave readers questioning the underlying assumptions of the proposed approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and logically structured, allowing for clear understanding. The novelty of the proposed method is evident, though the limited comparison with recent advancements somewhat diminishes its impact. Reproducibility is addressed through detailed descriptions of the methodology and datasets used, but providing code or detailed implementation guidelines would further enhance this aspect.\n\n# Summary Of The Review\nOverall, the paper makes a valuable contribution to the field of few-shot learning through its innovative meta-learning approach. While it has notable strengths in empirical validation and clarity, addressing the weaknesses related to baseline comparisons and scalability would significantly strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces RED-GCN, a novel Graph Convolutional Network (GCN) architecture that allows for the depth (d) of the network to be a trainable parameter, extending it from the traditional integer values to real numbers. This flexibility enhances the expressiveness of GCNs in modeling graph structures characterized by homophily (similar labels) and heterophily (dissimilar labels). The authors demonstrate that negative values of d serve as high-pass filters, which are particularly advantageous for heterophilic graphs. The methodology involves a comprehensive examination of the relationship between depth, graph spectrum, and the nature of graph connections. The findings show that RED-GCN outperforms traditional GCNs in node classification tasks and introduces a novel graph augmentation technique based on eigengraphs.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to GCN depth, providing a more nuanced and flexible way to tune this critical hyperparameter without requiring prior knowledge of the graph type. This could significantly alleviate the over-smoothing problem commonly faced by deep GCNs. The empirical results across various datasets convincingly support the effectiveness of RED-GCN. However, a potential weakness is the lack of rigorous theoretical justification for why allowing depth to take on real values improves performance, which might leave some readers questioning the foundational principles behind the approach.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates its contributions, making it accessible to readers familiar with GNNs. The quality of the writing is high, and the methodology is detailed enough to allow for reproducibility. However, while the empirical results are compelling, further details on the implementation and experimental setup could enhance reproducibility, especially regarding the graph augmentation method. The novelty of allowing a continuous depth in GCNs is significant, providing a fresh perspective in the ongoing research on GNN architectures.\n\n# Summary Of The Review\nOverall, the paper presents a novel and impactful contribution to the field of graph neural networks by introducing RED-GCN, which allows for a trainable depth parameter. While the empirical results are strong, further theoretical insights could enhance the paper's robustness. The innovative approach and promising results suggest that this work could pave the way for new research directions in GNNs.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel Graph Convolutional Network (GCN) model that allows the depth \\(d\\) to be a real-valued trainable parameter, addressing the challenges of over-smoothing in GNNs. By introducing the concept of eigengraphs for graph augmentation and exploring the relationship between negative GCN depth and graph heterophily, the authors demonstrate that their approach effectively handles both homophilic and heterophilic graphs. Extensive experiments across 11 datasets reveal that RED-GCN achieves state-of-the-art performance in node classification tasks, establishing the importance of adaptive depth tuning in GNNs.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to depth representation in GCNs and its solid theoretical foundation through eigen-decomposition. The introduction of eigengraphs is a noteworthy contribution that enhances graph representation. However, the paper does face weaknesses, such as limited discussion on practical implementation challenges of training with a real-valued depth and potential computational overhead associated with eigen-decomposition in larger graphs. Additionally, while the performance improvements are significant, the paper could benefit from more detailed analyses of the scenarios where RED-GCN outperforms existing models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The writing quality is high, making it accessible to readers familiar with GNNs. The novelty is substantial, particularly in the introduction of a trainable depth parameter and the concept of eigengraphs. The reproducibility of results appears strong, with extensive experimentation and clear metrics provided. However, more details regarding the implementation specifics and hyperparameter settings would enhance reproducibility.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the understanding and application of GCNs through the introduction of a trainable depth parameter and eigengraphs. While the contributions are impactful and well-supported by empirical results, further exploration of practical implementation challenges and additional analyses could strengthen the paper.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces RED-GCN, a novel Graph Convolutional Network (GCN) that addresses the challenge of determining optimal depth in GCNs by allowing the depth parameter to be a trainable real-valued variable. It proposes two variants of the model, RED-GCN-S and RED-GCN-D, and presents a new graph augmentation method based on eigengraphs. The authors demonstrate that RED-GCN enhances representational power for both homophilic and heterophilic graphs, achieving superior performance across 11 datasets compared to conventional GCN approaches.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative approach to redefining the depth parameter in GCNs, which could significantly impact the performance of these models across various types of graphs. The introduction of the eigengraph-based augmentation method is also a noteworthy contribution, offering a fresh perspective on enhancing graph representations. However, the paper could benefit from more extensive comparisons with state-of-the-art models and a deeper theoretical analysis of the implications of continuous depth. Additionally, some concepts, particularly regarding eigengraphs, could be articulated more clearly to enhance understanding.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but clarity is compromised in certain sections, particularly in explaining the eigengraph concept and its implications for the model's performance. The quality of the writing is adequate, yet there are areas where additional detail could improve the reader's comprehension. In terms of novelty, the approach to continuous depth is a significant step forward, establishing a new direction in GCN research. Reproducibility is supported through the use of multiple datasets and baselines, although further experimental validation against more recent models could strengthen this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a promising and innovative approach to the depth problem in GCNs with the RED-GCN model. While the experimental results are compelling, clearer explanations of key concepts and more comprehensive validation with contemporary methods would enhance the paper's contribution to the field.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach to Graph Convolutional Networks (GCNs) that redefines the depth of GCNs as a continuous, trainable parameter rather than a fixed positive integer. This extension allows the model to adapt its depth to both homophilic and heterophilic graphs, addressing the common over-smoothing issue encountered in traditional GCNs. The authors substantiate their approach through theoretical analysis involving eigen-decomposition of the graph Laplacian and demonstrate the efficacy of RED-GCN across diverse datasets, achieving improved performance in node classification tasks.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to depth in GCNs, which enhances the model's flexibility and adaptability. The theoretical grounding is robust, providing a solid framework for understanding the implications of continuous depth. The empirical results are compelling and indicate significant improvements over conventional methods. However, the paper could benefit from a more detailed discussion on the computational complexity introduced by the new depth mechanism and its implications on scalability in practical applications, as well as potential limitations regarding interpretability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the proposed changes to GCN depth. The methodology is presented in a logical manner, supported by sufficient theoretical insights. The novelty of redefining depth as a continuous parameter is significant, contributing to a better understanding of graph signal processing. The reproducibility of the results is facilitated by the detailed descriptions of experimental setups and the provision of empirical evaluations, although sharing code and datasets would further enhance reproducibility.\n\n# Summary Of The Review\nOverall, RED-GCN offers a significant advancement in the design of GCNs by allowing for continuous depth tuning, which addresses both over-smoothing and varying graph characteristics. The theoretical and empirical contributions are substantial, establishing a strong basis for further exploration in graph learning frameworks.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper proposes a model called RED-GCN that redefines the depth of Graph Convolutional Networks (GCNs) as a trainable real-valued parameter instead of a fixed integer. It aims to provide a flexible mechanism to adapt depth based on the characteristics of the input graph, claiming the ability to automatically detect graph homophily and heterophily. However, the paper lacks a convincing demonstration of the practical advantages of this approach compared to traditional fixed-depth models. Extensive experiments reveal only marginal improvements over baseline performance, and concerns about the computational scalability due to reliance on eigen-decomposition of the graph Laplacian are raised.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to redefining depth as a trainable parameter, which could potentially offer a new perspective on GCN architecture. However, the paper suffers from several weaknesses, including a lack of clear theoretical grounding for the introduction of negative depth values, insufficient empirical validation of the model's claims, and limited engagement with existing literature on GNNs. Additionally, the proposed graph augmentation method does not convincingly demonstrate significant performance improvements over simpler baseline models.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by vague explanations and a lack of intuition regarding the implications of negative depth parameters. The quality of the methodology is questionable, particularly in light of the computational limitations that arise from using eigen-decomposition. Novelty is present in the proposed concepts, but their significance is diminished by inadequate empirical support. Reproducibility may be hindered due to the complexity of the model and its reliance on specific graph structures that are not well-explained.\n\n# Summary Of The Review\nOverall, the paper presents an incremental advancement in GNN research through the introduction of RED-GCN, but it fails to convincingly address existing challenges and lacks sufficient empirical validation. The claims made regarding the practical advantages of the model are not well-supported, leading to an overall perception that the contributions may not be as groundbreaking as suggested.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach to Graph Convolutional Networks (GCNs) that redefines the concept of depth from a discrete integer to a continuous real number. This innovative methodology allows the model to adaptively adjust its depth based on the characteristics of the input graph, whether it is homophilic or heterophilic, significantly enhancing its representation power. The findings indicate that RED-GCN outperforms existing models in semi-supervised node classification tasks across various datasets, demonstrating superior accuracy and expressiveness. Additionally, the introduction of eigengraphs for graph augmentation and two variants, RED-GCN-S and RED-GCN-D, further contribute to the model's flexibility and scalability.\n\n# Strength And Weaknesses\nThe strengths of RED-GCN lie in its novel approach to depth selection, which allows for greater flexibility and adaptability in graph learning tasks. The ability to perform high-pass filtering through negative depth enhances its capability in handling heterophilic graphs, a notable advancement in the field. The incorporation of eigengraphs as a novel augmentation method significantly boosts performance, further validating the model's effectiveness. However, potential weaknesses include the complexity introduced by the continuous depth parameter, which may complicate implementation and understanding for practitioners. Additionally, the paper could benefit from a more detailed exploration of the theoretical underpinnings of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its concepts clearly, making it accessible to readers familiar with GCNs. The novelty of redefining depth as a continuous parameter is a significant contribution to the field, and the empirical results substantiate the claims made. However, reproducibility might be a concern due to the complexity of the model and the potentially challenging nature of implementing the eigengraph augmentation method without detailed guidelines.\n\n# Summary Of The Review\nOverall, RED-GCN represents a substantial advancement in graph convolutional networks by introducing a flexible and adaptive depth mechanism, enhancing the model's performance across various graph types. While the contributions are significant and the results promising, the complexity of the proposed methods may pose challenges for practitioners seeking to apply this model in real-world scenarios.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper proposes a novel approach to understanding the depth of Graph Convolutional Networks (GCNs) by redefining depth from a discrete integer domain to a continuous real number domain. The authors introduce the RED-GCN model, which treats the depth as a trainable parameter, allowing for enhanced expressiveness in processing graph signals, particularly in homophilic and heterophilic contexts. By leveraging the eigen-decomposition of the graph Laplacian, the paper establishes a theoretical foundation that links depth to the frequency components of graph signals, positing that negative depth can serve as a high-pass filter, thereby expanding GCN capabilities.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its theoretical innovation, providing a fresh perspective on GCN depth that challenges conventional views. The introduction of continuous depth as a parameter has significant implications for understanding the expressiveness of GCNs in various graph contexts. However, the paper could be criticized for its heavy theoretical focus, which may limit practical applicability without empirical validation. Furthermore, while the theoretical framework is robust, the lack of extensive experimental results to substantiate claims regarding the performance of RED-GCN is a notable weakness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical contributions, making complex concepts accessible. The quality of the writing is high, with a logical flow from introduction to conclusion. The novelty of redefined depth in GCNs is significant, pushing the boundaries of existing research. However, the reproducibility of the findings could be a concern due to the absence of detailed experimental protocols and quantitative evaluations that would allow other researchers to replicate the proposed model effectively.\n\n# Summary Of The Review\nThis paper presents a compelling theoretical advancement in the understanding of GCN depth by introducing a continuous depth parameter, which has the potential to enhance the expressiveness of GCNs. While the theoretical contributions are strong, the lack of empirical validation raises questions about practical implications.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper proposes RED-GCN, a novel approach to graph convolutional networks (GCNs) that redefines the concept of depth in GCNs by treating it as a trainable real-valued parameter rather than a fixed positive integer. The methodology includes two variants: RED-GCN-S, which utilizes a single depth parameter, and RED-GCN-D, which employs dual depths for distinct graph structures. The model leverages eigen-decomposition of the graph Laplacian to construct a diffusion matrix and focuses on optimizing the depth parameter during training to adapt to graph homophily and heterophily. The experiments demonstrate that RED-GCN outperforms several existing GCN baselines on a variety of datasets, showcasing its effectiveness in node classification tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to depth in GCNs, allowing for more flexibility and potential improvements in model performance. The systematic analysis of trainable depth and its impact on model accuracy provides valuable insights into the functioning of GCNs in various graph structures. However, a notable weakness is the reliance on eigen-decomposition, which may limit the model's applicability to large and dynamic graphs due to computational intensity. Additionally, the exploration of broader implications for graph learning methodologies is somewhat lacking, as the focus remains primarily on depth optimization.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its methodology clearly, making it accessible to readers familiar with GCNs. The quality of the experiments is solid, utilizing a diverse set of datasets and baseline comparisons. However, while the novelty of redefining depth in GCNs is commendable, the paper could benefit from a more thorough discussion on the limitations imposed by eigen-decomposition and the potential for future scalability. The code availability in PyTorch Geometric enhances reproducibility, although more detailed implementation specifics in the main text would further support this aspect.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of graph convolutional networks by introducing a flexible depth parameter that adapts to the graph's structure. While the methodology and experimental results are promising, the reliance on eigen-decomposition presents a notable limitation for practical applications in larger and dynamic graphs. Further exploration of scalability for these scenarios would strengthen the paper's contributions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a novel approach to Graph Convolutional Networks (GCNs) by redefining the concept of GCN depth from positive integers to real numbers. It aims to address the over-smoothing problem associated with GCNs and proposes a new model termed RED-GCN, which incorporates negative depth parameters and utilizes eigengraphs. The authors claim that their method outperforms existing GNNs on heterophilic graphs while providing a new augmentation technique for graph data. However, the empirical results suggest only marginal improvements over established models.\n\n# Strength And Weaknesses\nThe strengths of the paper include its innovative attempt to redefine GCN depth and the introduction of a negative depth parameter, which could theoretically offer new perspectives in GNN design. However, the weaknesses are significant; the paper lacks substantial novelty, as many of its contributions echo prior works, such as CGNN and techniques like PairNorm and DropEdge, which have already addressed similar challenges. Furthermore, the claimed improvements in performance over existing models appear marginal and do not convincingly demonstrate the proposed model's competitiveness.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, but the clarity of its contributions is undermined by a lack of clear differentiation from prior works. While the methodology is described adequately, the novelty claims are overstated and not sufficiently supported by experimental evidence. The reproducibility of the results is questionable due to the lack of detailed explanations about the proposed eigengraphs and their practical implications. Overall, the quality of the presentation is solid, yet the novelty and clarity of contributions need improvement.\n\n# Summary Of The Review\nThe paper attempts to innovate on GCNs by redefining depth and introducing a new model, RED-GCN. However, it falls short of providing substantial advancements over existing methods, and its claims of novelty and performance improvements are not convincingly substantiated.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents \"RED-GCN: Revisit the Depth of Graph Convolutional Networks,\" which proposes a novel approach to address the depth-related challenges in Graph Convolutional Networks (GCNs). The authors introduce a new model that utilizes negative depths to enhance high-pass frequency filtering capabilities, thereby mitigating the over-smoothing issue commonly associated with increased depth in GCNs. The findings demonstrate that their model achieves superior performance on semi-supervised node classification tasks across 11 benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to addressing the depth of GCNs, which is a well-known challenge in the field. The methodology is sound, utilizing both theoretical insights and empirical validation to support the proposed model. However, the paper suffers from several clarity issues, particularly in the mathematical notations and terminology used, which could hinder comprehension for readers unfamiliar with the subject. Additionally, the empirical results could be more informative if accompanied by clearer definitions of \"superior performance.\"\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is undermined by inconsistent notation and terminology, as seen in various mathematical expressions and definitions. While the quality of the research is high, the presentation could benefit from more rigorous proofreading to eliminate grammatical and formatting inconsistencies. The novelty of the approach is commendable, as it revisits a well-studied problem with fresh insights. However, reproducibility may be compromised due to the lack of comprehensive details on the experimental setup and dataset descriptions.\n\n# Summary Of The Review\nOverall, the paper offers a significant contribution to the field of graph neural networks by addressing the depth-related challenges of GCNs through innovative methodology. However, clarity issues and inconsistent notation detract from the overall presentation, potentially impacting the paper's accessibility and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a novel approach to Graph Convolutional Networks (GCNs) by introducing a model termed RED-GCN, which allows for the adjustment of graph depth through a trainable parameter. The authors explore the implications of negative depths and discuss the significance of homophily and heterophily in graph structures. The methodology revolves around theoretical guarantees for real-valued depth and a series of experiments on selected datasets. However, the paper falls short in addressing practical considerations and broader applicability.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to defining depth in GCNs, particularly through the exploration of negative depths, which could open new avenues in graph neural network architectures. However, several weaknesses are evident: the limited applicability of the model to dynamic graphs, the absence of comprehensive benchmarking against state-of-the-art methods, and a lack of exploration into how depth adjustments interact with other essential graph properties. Furthermore, the focus on a narrow set of datasets raises concerns about the generalizability of the findings. The potential implications regarding ethical considerations and model interpretability are also inadequately addressed.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper’s clarity is somewhat hindered by the abstract nature of its theoretical discussions, particularly concerning the practical applications of real-valued depth. While the novelty of the approach is commendable, the lack of a robust experimental framework and detailed explanations on the proposed methodologies limits reproducibility. The discussion regarding graph augmentations is too cursory and could benefit from systematic exploration to enhance the model's practical utility.\n\n# Summary Of The Review\nOverall, the paper introduces a promising concept of adjustable depth in GCNs but lacks depth in its practical implications and experimental validation. While the theoretical contributions are intriguing, the limitations in empirical evaluation and broader applicability significantly weaken the findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"RED-GCN: Revisit the Depth of Graph Convolutional Network\" introduces a novel approach to defining the depth \\( d \\) of Graph Convolutional Networks (GCNs) as a real-valued parameter rather than a positive integer. This redefinition is backed by a rigorous statistical framework that evaluates the correlation between depth and the representation power of GCNs across various datasets, focusing on both homophilic and heterophilic graphs. Through extensive experiments involving 11 datasets and comparisons with 7 baseline models, the authors demonstrate statistically significant performance improvements attributed to their method, particularly highlighting the optimal depth for different graph types.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative approach to redefining depth in GCNs and the robust statistical methodology employed to validate its claims. The comprehensive dataset selection and the statistical significance testing add credibility to the findings. However, the paper could benefit from a deeper exploration of the implications of continuous depth and its practical applicability in real-world scenarios. Additionally, while the statistical analyses are thorough, further clarification on the experimental setup would enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally high, with well-structured sections that guide the reader through the methodological framework and findings. The quality of the experiments is commendable, as extensive statistical analyses reinforce the proposed contributions. The novelty of redefining depth in GCNs is significant, marking a departure from traditional discrete approaches. However, aspects of reproducibility could be improved by providing more details on the experimental setup and the exact implementation of the proposed methods.\n\n# Summary Of The Review\nOverall, the paper presents a novel and statistically robust approach to redefining the depth of GCNs, demonstrating significant improvements in performance across various datasets. While the methodology and findings are compelling, additional details regarding implementation and real-world applicability could enhance the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces RED-GCN, a novel graph neural network (GNN) architecture that employs a unique approach to graph representation by utilizing real-valued depth. The methodology focuses on enhancing the model's capacity to capture complex graph structures while aiming to optimize computational efficiency. The findings demonstrate RED-GCN's potential for improved performance on benchmark datasets; however, the authors do not thoroughly address the limitations related to inductive and dynamic graph learning, as well as the computational complexities associated with eigen-decomposition.\n\n# Strength And Weaknesses\nThe main strength of RED-GCN lies in its innovative approach to graph representation that seeks to balance depth and efficiency. However, several weaknesses significantly hinder its overall impact. The paper lacks a comprehensive analysis of the model's scalability due to the reliance on eigen-decomposition, which may restrict its application in real-time scenarios or large graphs. Furthermore, the absence of robustness testing against noisy or incomplete data raises concerns about its practical applicability. Additionally, the experiments conducted do not adequately cover a diverse array of graph types, which limits the generalizability of the findings. The paper also fails to explore the sensitivity of the model to hyperparameter choices beyond depth and lacks a comparative analysis against more complex GNN architectures.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is satisfactory, though certain sections could benefit from a more in-depth discussion of the implications of using real-valued depth on interpretability and explainability. The novelty of the approach is noteworthy; however, the paper's lack of thorough exploration in critical areas such as robustness and scalability affects its overall quality. Reproducibility is not sufficiently addressed, with limited details provided on experimental setups and parameter choices, leaving questions about the reproducibility of results.\n\n# Summary Of The Review\nOverall, while RED-GCN presents a novel approach to graph representation, its contributions are undermined by significant limitations in scalability, robustness, and empirical validation. The paper requires further exploration of key issues to enhance its applicability and impact within the graph learning community.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel approach that redefines the depth of Graph Convolutional Networks (GCNs) as a real-valued parameter rather than a positive integer. The authors assert that this trainable depth can enhance representation power, particularly in the context of homophilic and heterophilic graphs. They introduce two model variants, RED-GCN-S and RED-GCN-D, and conduct experiments to demonstrate their proposed method's superiority in node classification tasks, suggesting that optimal depth can improve performance through topology alterations.\n\n# Strength And Weaknesses\nWhile the paper attempts to address the depth selection issue in GCNs, it largely revisits well-understood concepts without offering substantial new insights. The notion of trainable depth is presented as a significant advancement, but this is essentially a rebranding of existing hyperparameter tuning practices. The exploration of negative depth as a high-pass filter is a notable idea, yet it does not provide groundbreaking principles. The experimental results, while extensive, may be biased towards the selected datasets, raising concerns about overfitting. Overall, the work lacks depth in terms of novel contributions to the already established knowledge in the field.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and clear, making it accessible to readers. However, the novelty is questionable, as much of the content revisits established concepts without significant advancement. Reproducibility may be hindered by the potential overfitting observed in the experiments, and the reliance on specific datasets might limit the generalizability of the findings.\n\n# Summary Of The Review\nIn summary, RED-GCN presents a rehash of existing ideas in GCNs under the guise of novelty, lacking substantial contributions to the field. While the methodology is clear and the experiments are well-executed, the paper does not provide the innovative insights it claims.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents RED-GCN, a novel framework that explores the continuous depth of Graph Convolutional Networks (GCNs), allowing for a trainable depth parameter that can take real values rather than being restricted to positive integers. The authors address the over-smoothing issue commonly faced in deeper GCNs and introduce eigengraphs to assess the impacts of graph homophily and heterophily. Their findings indicate that negative depth can enhance performance specifically on heterophilic graphs, suggesting a new direction for research into the implications of depth in various graph structures. The authors provide empirical results demonstrating that RED-GCN outperforms traditional GNNs in these contexts, particularly in semi-supervised learning scenarios.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to continuous depth tuning, which aligns well with current trends in adaptive hyperparameter optimization. The introduction of eigengraphs adds a unique perspective to the understanding of graph structures. However, the paper could benefit from a deeper exploration of the interpretability of the trainable depth parameter, as well as a broader evaluation of its scalability across a variety of datasets and applications. Limitations concerning eigen-decomposition for large graphs are acknowledged, but more detail on potential solutions or approximations would be valuable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings, making it accessible to the reader. The methodology is sound, though the reproducibility could be enhanced by providing more detailed descriptions of the experimental setup and datasets. The novelty is significant, particularly in the context of GCNs, as it pushes the boundaries of hyperparameter tuning and its implications for graph learning.\n\n# Summary Of The Review\nOverall, the RED-GCN framework offers a compelling approach to addressing long-standing challenges in GCNs, particularly the over-smoothing issue and the implications of depth on performance. While the contributions are notable, further exploration of interpretability and scalability would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the RED-GCN model, which demonstrates superior performance in semi-supervised node classification tasks across 11 datasets, effectively addressing both homophilic and heterophilic graph scenarios. The methodology includes a systematic study of optimal depth adjustments, allowing the model to dynamically adapt to the characteristics of the graph. Key findings indicate that RED-GCN-S achieves high accuracy on homophilic datasets such as Cora and DBLP, while RED-GCN-D shows competitive performance on heterophilic datasets like Squirrel and Cornell. The model's innovative depth adjustment mechanism and graph augmentation methods lead to significant accuracy improvements, establishing RED-GCN as a robust solution in the graph neural network domain.\n\n# Strength And Weaknesses\nStrengths of this paper include its comprehensive evaluation across various datasets and the model's ability to adapt to different graph characteristics, which is a notable advancement in the field. The systematic approach to depth adjustment based on graph type is particularly commendable, providing insights into how graph neural networks can be optimized for varying contexts. However, a potential weakness lies in the reliance on empirical results, which may require further theoretical backing to fully understand the underlying mechanisms driving the observed performance gains. Additionally, more clarity on the implementation details could enhance reproducibility.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, presenting its methodology and findings in a clear and logical manner. The quality of the experiments is high, with extensive benchmarking against existing models. The novelty of the approach lies in the dynamic adjustment of depth and the graph augmentation method, which are significant contributions to the field. However, the reproducibility of results could be improved by providing more details on experimental setups, hyperparameter configurations, and the specific datasets used.\n\n# Summary Of The Review\nOverall, the paper presents a novel and effective approach to semi-supervised node classification through the RED-GCN model, demonstrating strong performance across various graph types. The depth adjustment mechanism and graph augmentation techniques are particularly noteworthy, although further clarity on implementation details would enhance reproducibility and understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving Graph Convolutional Networks (GCNs) by introducing a new methodology for depth enhancement and graph representation. The authors propose a framework that leverages eigengraph properties and Laplacian smoothing, aiming to address limitations in existing GCN architectures. The findings demonstrate that the proposed method not only enhances the performance of GCNs on benchmark datasets but also provides insights into the underlying graph structures, thereby contributing to a deeper understanding of GNNs.\n\n# Strength And Weaknesses\nThe paper’s strengths lie in its clear contributions to the field of graph neural networks, particularly in enhancing GCNs through theoretical insights and empirical validation. The methodology is innovative, addressing critical challenges in the domain. However, the paper suffers from a lack of clarity in some sections, primarily due to complex sentence structures and inconsistent terminology. Additionally, while the experimental results are promising, the significance of these findings could be better articulated to emphasize their impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nClarity is a significant issue in the paper, with overly complex sentences and dense paragraphs making it challenging to follow the main ideas. The quality of the writing can be improved by simplifying sentence structures and ensuring consistency in terminology. The novelty of the approach is noteworthy, yet it could be better highlighted throughout the paper. Reproducibility is hampered by the insufficient explanation of equations and lack of clear references to figures and tables that support the findings.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of GNNs with its innovative approach to enhancing GCNs; however, issues related to clarity and presentation detract from its impact. Improved organization and clearer articulation of results and methodologies would significantly enhance the paper's readability and effectiveness.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.341505471246828,
    -1.570854642466478,
    -1.5990933343264628,
    -1.6825519777301996,
    -1.8996376187739197,
    -1.614682902159974,
    -1.7043124415181643,
    -1.930403158008817,
    -1.7297128667979602,
    -1.6403604571751365,
    -1.6856404983192799,
    -1.4183333593529295,
    -1.652958817062251,
    -1.6265097397475785,
    -1.6434182311975416,
    -1.6749355319938806,
    -1.8459539286385784,
    -1.5589178603213918,
    -1.6309934752991087,
    -1.6468452405413954,
    -1.850921462312562,
    -1.6872451739026617,
    -1.9710464490087054,
    -1.6474573748879544,
    -1.8613099666229513,
    -1.9003068830627228,
    -1.7997392793717149,
    -1.6795681025823532,
    -1.7725435486878316
  ],
  "logp_cond": [
    [
      0.0,
      -1.7539110438310348,
      -1.7229306818925751,
      -1.752269297711383,
      -1.7584785975258337,
      -1.7651322042798416,
      -1.8004271373098177,
      -1.7798083838923702,
      -1.7144431926974768,
      -1.7576754520567528,
      -1.7501658816181376,
      -1.9601198798223491,
      -1.7275702592859454,
      -1.732180820975872,
      -1.7775029791731174,
      -1.7384076129438706,
      -1.7135703229619095,
      -1.7503624824309698,
      -1.7231488142019935,
      -1.7535586714593545,
      -1.7945642793054224,
      -1.8120596810254117,
      -1.750012057635474,
      -1.7439111947388126,
      -1.8412085135570302,
      -1.7473482208922064,
      -1.7480636927047264,
      -1.8351860804953968,
      -1.904209313018355
    ],
    [
      -1.262033136094154,
      0.0,
      -1.2133223614801902,
      -1.153632261227379,
      -1.1334570633199605,
      -1.1469879386855297,
      -1.2376988489417642,
      -1.162926144451178,
      -1.1405582936582495,
      -1.1297053624471332,
      -1.1757672460717346,
      -1.348079692923984,
      -1.1172236136516562,
      -1.1260392367237655,
      -1.1404624000451593,
      -1.1553636386360773,
      -1.1840891423535589,
      -1.1909105512382412,
      -1.203927874814004,
      -1.1728223027914955,
      -1.1586729104907443,
      -1.2203223205728793,
      -1.19563072678253,
      -1.1798381369363122,
      -1.2774273385372263,
      -1.1771585470353894,
      -1.1427616058429726,
      -1.2302574146553742,
      -1.2467558488275645
    ],
    [
      -1.3100448399859537,
      -1.2629096693812736,
      0.0,
      -1.246559290243292,
      -1.2498535909374417,
      -1.2469824978778927,
      -1.2942270352684206,
      -1.2793204942959606,
      -1.232951615460554,
      -1.2535367874865309,
      -1.2308513957490008,
      -1.3906712810962059,
      -1.2472320283286695,
      -1.2075311127843567,
      -1.2584882386050131,
      -1.3064317264317613,
      -1.2787897261516434,
      -1.247840997629826,
      -1.2823782517071367,
      -1.2361854118748918,
      -1.2797331452245058,
      -1.3104493926698721,
      -1.269539243717574,
      -1.2714738968286736,
      -1.3279025668159687,
      -1.2414217097044908,
      -1.2256570673877816,
      -1.3216730278953046,
      -1.3628414470631751
    ],
    [
      -1.336913727495414,
      -1.2516310802171022,
      -1.2793500308539336,
      0.0,
      -1.2539300342410455,
      -1.2709627626317914,
      -1.3095449924744307,
      -1.2700924149363009,
      -1.2247668590828715,
      -1.2824448326432754,
      -1.2215852541206647,
      -1.424725334274268,
      -1.225560511185337,
      -1.204828074688208,
      -1.2993865806380707,
      -1.2935652794363757,
      -1.2998824961964528,
      -1.2870939271782902,
      -1.2225171848635885,
      -1.2742044915782618,
      -1.296701683797914,
      -1.2753995684876054,
      -1.2710086080177967,
      -1.2722841794291655,
      -1.370980559903637,
      -1.243555127155673,
      -1.2534155838968892,
      -1.3580038876601583,
      -1.3613116222673627
    ],
    [
      -1.5475150370725859,
      -1.4805289369675183,
      -1.47879479697952,
      -1.5010818852091103,
      0.0,
      -1.4966176349319815,
      -1.5241655045332199,
      -1.4305770921320469,
      -1.5071773704754066,
      -1.4475606342568001,
      -1.5376045263798612,
      -1.6663606357240475,
      -1.5221626320765544,
      -1.527914749407994,
      -1.5164185668302854,
      -1.5521831846069918,
      -1.503898487447679,
      -1.5177084772851708,
      -1.5519826681764848,
      -1.5403608443026002,
      -1.5251807954627152,
      -1.5562256775761907,
      -1.484966751586291,
      -1.5658977396999323,
      -1.5660894967377976,
      -1.52961257967921,
      -1.5450516607900577,
      -1.5502780193254562,
      -1.5987145683032282
    ],
    [
      -1.2647760370951031,
      -1.1817700441317032,
      -1.2229246153361684,
      -1.2091182225299824,
      -1.1799276038905135,
      0.0,
      -1.2766405261712035,
      -1.1782481771356268,
      -1.181333765795248,
      -1.203063834464275,
      -1.2347396140992901,
      -1.353114044377613,
      -1.219148366700611,
      -1.1655756741803758,
      -1.1895520036024823,
      -1.2664652701550672,
      -1.1892932140137662,
      -1.202211747224163,
      -1.1995703056935059,
      -1.1966139484691343,
      -1.2005766002950204,
      -1.269491644051519,
      -1.1986053041988491,
      -1.2077082015347333,
      -1.2104601339921075,
      -1.2081322137366801,
      -1.1862663515559972,
      -1.2541170971099254,
      -1.2974790527930267
    ],
    [
      -1.4499483919876994,
      -1.3169535909499,
      -1.3700540326172888,
      -1.3503050703138069,
      -1.3133917899456982,
      -1.3410237004784942,
      0.0,
      -1.3901858226632386,
      -1.357675609747145,
      -1.372116757221074,
      -1.3890019649418448,
      -1.481839616092893,
      -1.376134699545048,
      -1.3511222417668112,
      -1.366072514939465,
      -1.402693034618421,
      -1.3862435685549865,
      -1.3649910906893399,
      -1.3246468802036244,
      -1.4052743959296332,
      -1.4247653852909878,
      -1.4185813164300674,
      -1.410235809064044,
      -1.3593326614996801,
      -1.4381362429409918,
      -1.3539104368461186,
      -1.3757407558900012,
      -1.4517649743384797,
      -1.4855047185774226
    ],
    [
      -1.4841841401207165,
      -1.4203590740443977,
      -1.5139660188522521,
      -1.4576355907276817,
      -1.2915304812621362,
      -1.4140681316985728,
      -1.4932644671826298,
      0.0,
      -1.4309641089913203,
      -1.4531748369898645,
      -1.4365493283579995,
      -1.625589073480986,
      -1.442068489188077,
      -1.389249280037894,
      -1.4392925227913325,
      -1.4826407390277465,
      -1.4608084269568176,
      -1.3555752015085583,
      -1.4512215716741814,
      -1.4721529992912952,
      -1.3847653076636064,
      -1.453209138991361,
      -1.4663410937635981,
      -1.4584894272705833,
      -1.5395884901549752,
      -1.4532605573316477,
      -1.4180611765824727,
      -1.4661480137346254,
      -1.531946685791749
    ],
    [
      -1.381424059526917,
      -1.315742431237184,
      -1.3434225089487846,
      -1.294447248845808,
      -1.3296792717037205,
      -1.33730011081464,
      -1.4027738359233728,
      -1.3396058642538355,
      0.0,
      -1.3528285036545358,
      -1.3045540714684167,
      -1.4637304610458894,
      -1.244752388624278,
      -1.2536243664017357,
      -1.3414728470064672,
      -1.3559694271388822,
      -1.356187016552335,
      -1.32664759232411,
      -1.3994471545886735,
      -1.390725476693065,
      -1.343750740743662,
      -1.4093262936883473,
      -1.3637828853627658,
      -1.3648467320646713,
      -1.3965789844044862,
      -1.3387964703978614,
      -1.3214864555805943,
      -1.3507274001358331,
      -1.4139941203856172
    ],
    [
      -1.2654642775424187,
      -1.1869282137386425,
      -1.223978476268403,
      -1.2162595708412618,
      -1.1253263383533552,
      -1.226591004493662,
      -1.3047168026257767,
      -1.217320969431275,
      -1.2476911076598336,
      0.0,
      -1.2276755349858925,
      -1.4086162156511706,
      -1.240745914538221,
      -1.1726649922972112,
      -1.2672590429576862,
      -1.1891257057490445,
      -1.1723324734424247,
      -1.2510891329657376,
      -1.2746009153464273,
      -1.1972569672345206,
      -1.1829236352222252,
      -1.2862001043834077,
      -1.2259355926064335,
      -1.2591464242206365,
      -1.251874885337762,
      -1.2111057650912982,
      -1.2073689717459002,
      -1.301041139658028,
      -1.333715441470085
    ],
    [
      -1.252868350882577,
      -1.1671444319091613,
      -1.2232217718546583,
      -1.1900655686558994,
      -1.2314363755745048,
      -1.2216715703514445,
      -1.2886122229627448,
      -1.2064056128185268,
      -1.1927903482875406,
      -1.215917774848636,
      0.0,
      -1.3826819060193505,
      -1.1936955274139756,
      -1.1756038549749168,
      -1.2921229535721073,
      -1.2167327716555927,
      -1.2152516251533252,
      -1.2579021227944052,
      -1.2167131633705774,
      -1.209369588214787,
      -1.2490270822386886,
      -1.2550005486517184,
      -1.2285447425190286,
      -1.2433306136799127,
      -1.2859822973650943,
      -1.1764076272552189,
      -1.18283776511147,
      -1.2689995244202992,
      -1.308499087032321
    ],
    [
      -1.1870113054060836,
      -1.15428503520958,
      -1.1922679647988124,
      -1.1697292960619254,
      -1.1624889017011493,
      -1.1673808251974098,
      -1.1440742644374575,
      -1.1393316639777935,
      -1.1425275973572353,
      -1.137214771698603,
      -1.1731960296745847,
      0.0,
      -1.1781881504837937,
      -1.1497356323656858,
      -1.1444738580415244,
      -1.1504468662520342,
      -1.1609101330556633,
      -1.1981000555358927,
      -1.1609172736722988,
      -1.1900076496330534,
      -1.1573456683705863,
      -1.1218643113335909,
      -1.1219753875415919,
      -1.1596445654195384,
      -1.129321561749551,
      -1.13347865429527,
      -1.1398569818609174,
      -1.2075678098160278,
      -1.0992367028140972
    ],
    [
      -1.2918119718231726,
      -1.2079149684169854,
      -1.3032735727388949,
      -1.2093697515669903,
      -1.2874947410766295,
      -1.236230767316778,
      -1.3558138151997137,
      -1.3102304209846107,
      -1.1586525841821838,
      -1.3010519109448535,
      -1.2459004729343868,
      -1.4370415755512098,
      0.0,
      -1.2001386159242808,
      -1.2753883042535121,
      -1.2991587377959037,
      -1.2679334233226685,
      -1.2307321597874763,
      -1.2835289205743885,
      -1.2985857954987188,
      -1.259017593073427,
      -1.325936580429092,
      -1.304384196725947,
      -1.3201690924318528,
      -1.3497927244768706,
      -1.286057663322574,
      -1.2359711136956362,
      -1.3015972216020173,
      -1.3433931214479446
    ],
    [
      -1.2389001540130893,
      -1.1531129908016768,
      -1.2224852761234029,
      -1.1273933036138624,
      -1.1894706167097637,
      -1.1526124264225273,
      -1.2706985017199641,
      -1.1692336908756373,
      -1.126528282108927,
      -1.172956991624424,
      -1.1822710130242673,
      -1.3883994161139235,
      -1.1295207961144496,
      0.0,
      -1.2051745596881587,
      -1.2198283952705624,
      -1.1610979647709816,
      -1.1849739081726607,
      -1.183084725164817,
      -1.2200903204448128,
      -1.155119292923703,
      -1.2430723819911067,
      -1.2200083252465705,
      -1.1921285248241364,
      -1.25933149696992,
      -1.1962574199601586,
      -1.1134564927604622,
      -1.2493976494648529,
      -1.2826981510694708
    ],
    [
      -1.277591522735132,
      -1.1628643075090022,
      -1.190800137991443,
      -1.2176897352719829,
      -1.1600720542737923,
      -1.19804244162454,
      -1.2740324221206072,
      -1.218432712429799,
      -1.2076566177665229,
      -1.2088299037080428,
      -1.2592705132282138,
      -1.3939391365489584,
      -1.1922173852667939,
      -1.2093424819885519,
      0.0,
      -1.2707128750368144,
      -1.189635233198966,
      -1.198826448039117,
      -1.2573200248539738,
      -1.199129712297571,
      -1.2089346519187645,
      -1.30881179421181,
      -1.2279171107665978,
      -1.2529315607944362,
      -1.3044315773348412,
      -1.1678378050949945,
      -1.2249295712091326,
      -1.2848371878864766,
      -1.323709879298465
    ],
    [
      -1.2755049542227488,
      -1.1236252409823897,
      -1.2802433717587642,
      -1.1979964611759473,
      -1.1777570164338151,
      -1.2069414169062638,
      -1.2541133332092234,
      -1.2562753367249184,
      -1.1988082188854088,
      -1.1882704634612014,
      -1.1984166549847561,
      -1.3968336959938774,
      -1.2083186709024158,
      -1.156635600094367,
      -1.2727194957834607,
      0.0,
      -1.2311514145446336,
      -1.2416428784848115,
      -1.21766549081147,
      -1.2324148728074662,
      -1.2385937763620893,
      -1.2696745329349932,
      -1.2348071609528295,
      -1.2554589062943655,
      -1.2944411589855436,
      -1.2247066975838365,
      -1.1700650596786524,
      -1.3138955792772709,
      -1.339974532495291
    ],
    [
      -1.4582643569736424,
      -1.4305190069641158,
      -1.4805217681968506,
      -1.4425780355700826,
      -1.4392924862413972,
      -1.445347384354897,
      -1.5374932525086586,
      -1.4484377905294952,
      -1.4289418547803312,
      -1.3421817731332268,
      -1.483138205031778,
      -1.6189320543568924,
      -1.410346480564404,
      -1.3982220159860292,
      -1.4650342553966413,
      -1.4713227685974586,
      0.0,
      -1.429554919859582,
      -1.4763049885955442,
      -1.4033860344415778,
      -1.4217997338136776,
      -1.4851947808902044,
      -1.437515919990831,
      -1.4937517005468903,
      -1.5026483622745523,
      -1.4389998144626817,
      -1.4001530035094558,
      -1.5134260365257997,
      -1.5401983742524379
    ],
    [
      -1.1961706779140933,
      -1.155830661908564,
      -1.1621199491823735,
      -1.1499191345442912,
      -1.1437455697555152,
      -1.1224617650661275,
      -1.2242024427817941,
      -1.1045521949374857,
      -1.1253589255202452,
      -1.13235777737856,
      -1.1772526498778304,
      -1.2998871576099091,
      -1.1366979760680067,
      -1.1439698460528251,
      -1.1225725357330845,
      -1.1971663386032072,
      -1.160493439603275,
      0.0,
      -1.1188551884910083,
      -1.1544885631554644,
      -1.1232235923550715,
      -1.1676674893116787,
      -1.1639776426119781,
      -1.1547375391806156,
      -1.2210767769093864,
      -1.1207734724801521,
      -1.1585197612572646,
      -1.1805521543694604,
      -1.2283886356583813
    ],
    [
      -1.2369240577204532,
      -1.2233394623004608,
      -1.2974463898464417,
      -1.242784582441051,
      -1.2498598979788693,
      -1.2676743987018113,
      -1.2591102076253726,
      -1.2299542710121791,
      -1.2365971113180796,
      -1.2004908614593304,
      -1.242169079616355,
      -1.4123574953601605,
      -1.2236403201153798,
      -1.25795979274611,
      -1.2950033880127954,
      -1.2416344783732591,
      -1.217843677326364,
      -1.2548307919580204,
      0.0,
      -1.2896989799462149,
      -1.188269251365023,
      -1.260720145964958,
      -1.2344984128176735,
      -1.2357610344785281,
      -1.2940528744139148,
      -1.1888188642792912,
      -1.2416665093529435,
      -1.3059628575814193,
      -1.3186541942056478
    ],
    [
      -1.3169238497787379,
      -1.2807058159671354,
      -1.2720539433167881,
      -1.280162635424773,
      -1.2836753126812637,
      -1.3258728954854673,
      -1.357371777809281,
      -1.3089634779273147,
      -1.287328951237324,
      -1.3056967905169166,
      -1.2823178698553195,
      -1.4468767525191806,
      -1.3270172048104623,
      -1.3297028184080444,
      -1.3192173138217886,
      -1.3162742521088302,
      -1.2871440433936048,
      -1.3181893643020821,
      -1.3391727334854895,
      0.0,
      -1.3193360275821948,
      -1.348196728287134,
      -1.2740365380945091,
      -1.3117709631543983,
      -1.3253510506685389,
      -1.2428903203804313,
      -1.3168295565034251,
      -1.3427113837544022,
      -1.3948878926569668
    ],
    [
      -1.4566738577452212,
      -1.401519973328904,
      -1.4536805736275664,
      -1.3936515761328623,
      -1.4058286649943212,
      -1.4404950775658116,
      -1.50926911579467,
      -1.423948097684702,
      -1.402536375201574,
      -1.3778731830365343,
      -1.4207816294381097,
      -1.6058125840339963,
      -1.397819117858535,
      -1.3813638676256172,
      -1.4357473015065787,
      -1.4417123807152026,
      -1.4458239423590744,
      -1.4008131759739177,
      -1.4463281303629432,
      -1.4641114092555054,
      0.0,
      -1.468342715580567,
      -1.4494045935710902,
      -1.432578766460927,
      -1.533869747015554,
      -1.4539019857018425,
      -1.3696130874930754,
      -1.4974880180133703,
      -1.4796535752972078
    ],
    [
      -1.3243936183673544,
      -1.243915272184431,
      -1.3284223774988957,
      -1.2954397466317586,
      -1.287689256438851,
      -1.2849192714750068,
      -1.3076390644606173,
      -1.269026006998726,
      -1.2789484338218602,
      -1.2860713382588416,
      -1.252888417859253,
      -1.409315133893244,
      -1.2668050296549755,
      -1.287483831078238,
      -1.2833505375932115,
      -1.2773803015758727,
      -1.2772292540829495,
      -1.2722458808037858,
      -1.289300545282166,
      -1.3003324161469663,
      -1.2874328359716989,
      0.0,
      -1.2969061333183354,
      -1.2853144661236278,
      -1.3208955269132203,
      -1.2751705596745067,
      -1.2321161898494042,
      -1.2731935832017192,
      -1.2840808182303545
    ],
    [
      -1.5679913185610563,
      -1.5425623313426882,
      -1.5681638606385648,
      -1.5205238556741258,
      -1.497982101940097,
      -1.5350059302088137,
      -1.6061779172556523,
      -1.5064631642040904,
      -1.4873781549975746,
      -1.4796656590305513,
      -1.535870985430934,
      -1.6764110683693407,
      -1.5071312145362223,
      -1.5529095531075292,
      -1.5694273210577399,
      -1.55313116462143,
      -1.4978929022295138,
      -1.5712242979644127,
      -1.5348015539728597,
      -1.5324642745633896,
      -1.496298596781686,
      -1.5691995904426244,
      0.0,
      -1.5519074811677116,
      -1.5201964121848273,
      -1.5493144039829287,
      -1.4957846997832829,
      -1.5851316126897905,
      -1.6403542022084554
    ],
    [
      -1.288893226101477,
      -1.1737753644864537,
      -1.2346192325545362,
      -1.1960353512422275,
      -1.2178056043026204,
      -1.232227570189812,
      -1.249019003219273,
      -1.1945229846457104,
      -1.226572539840669,
      -1.2202394804516712,
      -1.2353812219311238,
      -1.3899919618202636,
      -1.2343185481470813,
      -1.2151893149288735,
      -1.2144714619604666,
      -1.2729855941807025,
      -1.2682485554987541,
      -1.231609963721522,
      -1.2406770616910892,
      -1.2470774201322135,
      -1.2354862158279125,
      -1.260418328872566,
      -1.250790296829834,
      0.0,
      -1.2640370857708516,
      -1.2038820244300663,
      -1.231611028587474,
      -1.2338819223072048,
      -1.297859590339823
    ],
    [
      -1.4957267448519533,
      -1.4352870884992588,
      -1.4939779087124492,
      -1.4338646386803138,
      -1.4270853401490868,
      -1.4164614197235736,
      -1.4881031877869382,
      -1.404366909650245,
      -1.4064681519585187,
      -1.3970478758988665,
      -1.4332357280749668,
      -1.5730604932967869,
      -1.4352295538445723,
      -1.4364802426554688,
      -1.4729476342658698,
      -1.4487587229740146,
      -1.3961475292150174,
      -1.4404956527981319,
      -1.4589269512338638,
      -1.3905556625780167,
      -1.4538802869630885,
      -1.4963961403997161,
      -1.393861400323505,
      -1.439222479418341,
      0.0,
      -1.427572052290778,
      -1.4254209342576865,
      -1.526484039146628,
      -1.5260912040595278
    ],
    [
      -1.4638103323926313,
      -1.4196097681687803,
      -1.44205085725,
      -1.436108565621647,
      -1.4483037871375484,
      -1.4869957896497914,
      -1.5117308701660366,
      -1.4444102154355745,
      -1.4852921434500508,
      -1.4320169324453615,
      -1.4220844390209217,
      -1.60688870575023,
      -1.43108402658527,
      -1.439926718065234,
      -1.4803210685580113,
      -1.4715156364853135,
      -1.4551229458404107,
      -1.3840923023846872,
      -1.4156081148566462,
      -1.4273856999953582,
      -1.440095900309754,
      -1.4662633329827002,
      -1.4550365697573187,
      -1.4194697945613195,
      -1.5202043442332003,
      0.0,
      -1.4384035954635508,
      -1.5313080726321642,
      -1.557798528959741
    ],
    [
      -1.4112336378811572,
      -1.3649450605944642,
      -1.3332224571305187,
      -1.391484809448252,
      -1.4002501583055316,
      -1.375659217339695,
      -1.4456019352332452,
      -1.375384465884431,
      -1.3426706170261582,
      -1.3421507525625678,
      -1.4043172172593084,
      -1.5233273226102282,
      -1.3125721528293395,
      -1.3023337322204174,
      -1.4041945874462656,
      -1.3875084299978637,
      -1.364828496815288,
      -1.4090400854207348,
      -1.414706606554494,
      -1.3634738355566838,
      -1.32579709220573,
      -1.4274789982190248,
      -1.3716336865648773,
      -1.382805063285259,
      -1.4298839005318587,
      -1.3509168504562197,
      0.0,
      -1.4689220766470794,
      -1.45440234146009
    ],
    [
      -1.3343408527635585,
      -1.2647237307600059,
      -1.3135981101178462,
      -1.288086924563563,
      -1.2819892833121989,
      -1.284734692972521,
      -1.3705625872964062,
      -1.3047078171127782,
      -1.2533500277255019,
      -1.2665908642966623,
      -1.287231042284744,
      -1.4260840938510524,
      -1.2815340245174154,
      -1.286703655858988,
      -1.287439719745209,
      -1.3100668678499099,
      -1.3074547471838138,
      -1.251298220875841,
      -1.2801372830112008,
      -1.2701066163181631,
      -1.2929613855108424,
      -1.3077082591568918,
      -1.3037422682405355,
      -1.253995693991337,
      -1.3105620860116955,
      -1.2495233180810725,
      -1.2892705463927556,
      0.0,
      -1.3422611706644947
    ],
    [
      -1.4622351592215963,
      -1.3633243663199905,
      -1.4530062172232552,
      -1.4085957585141415,
      -1.396900161318993,
      -1.3893505046321355,
      -1.4164510279458016,
      -1.399895485566186,
      -1.4069643723845304,
      -1.4193540983084965,
      -1.411433672355136,
      -1.4004152200797881,
      -1.3864361727584362,
      -1.3964582647384611,
      -1.4202970153322485,
      -1.4106360286448985,
      -1.4052479643405535,
      -1.4260057448314696,
      -1.4054406135502837,
      -1.4234336928457487,
      -1.4063793772812694,
      -1.3222739614156203,
      -1.381106104669657,
      -1.403362382584598,
      -1.406632817932117,
      -1.442279394917215,
      -1.3882088080714998,
      -1.405610863388453,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.5875944274157932,
      0.6185747893542528,
      0.589236173535445,
      0.5830268737209943,
      0.5763732669669863,
      0.5410783339370102,
      0.5616970873544578,
      0.6270622785493511,
      0.5838300191900752,
      0.5913395896286904,
      0.3813855914244788,
      0.6139352119608825,
      0.609324650270956,
      0.5640024920737106,
      0.6030978583029574,
      0.6279351482849185,
      0.5911429888158581,
      0.6183566570448344,
      0.5879467997874734,
      0.5469411919414056,
      0.5294457902214162,
      0.5914934136113539,
      0.5975942765080153,
      0.5002969576897978,
      0.5941572503546215,
      0.5934417785421016,
      0.5063193907514312,
      0.4372961582284729
    ],
    [
      0.3088215063723241,
      0.0,
      0.35753228098628775,
      0.41722238123909894,
      0.43739757914651745,
      0.42386670378094826,
      0.3331557935247138,
      0.40792849801529996,
      0.43029634880822853,
      0.44114928001934484,
      0.39508739639474344,
      0.222774949542494,
      0.45363102881482176,
      0.4448154057427125,
      0.4303922424213187,
      0.4154910038304007,
      0.3867655001129191,
      0.3799440912282368,
      0.3669267676524739,
      0.3980323396749825,
      0.4121817319757337,
      0.35053232189359873,
      0.3752239156839481,
      0.39101650553016576,
      0.29342730392925165,
      0.39369609543108863,
      0.4280930366235054,
      0.3405972278111038,
      0.3240987936389135
    ],
    [
      0.28904849434050917,
      0.33618366494518925,
      0.0,
      0.3525340440831708,
      0.3492397433890211,
      0.3521108364485701,
      0.3048662990580422,
      0.3197728400305022,
      0.36614171886590885,
      0.345556546839932,
      0.368241938577462,
      0.20842205323025698,
      0.3518613059977933,
      0.3915622215421062,
      0.3406050957214497,
      0.2926616078947015,
      0.3203036081748194,
      0.3512523366966369,
      0.3167150826193261,
      0.362907922451571,
      0.31936018910195707,
      0.2886439416565907,
      0.32955409060888874,
      0.3276194374977892,
      0.2711907675104941,
      0.357671624621972,
      0.37343626693868126,
      0.2774203064311582,
      0.23625188726328772
    ],
    [
      0.34563825023478567,
      0.43092089751309737,
      0.403201946876266,
      0.0,
      0.42862194348915406,
      0.4115892150984082,
      0.3730069852557689,
      0.4124595627938987,
      0.45778511864732807,
      0.40010714508692424,
      0.46096672360953495,
      0.25782664345593154,
      0.45699146654486267,
      0.4777239030419915,
      0.38316539709212893,
      0.38898669829382393,
      0.3826694815337468,
      0.3954580505519094,
      0.4600347928666111,
      0.40834748615193783,
      0.38585029393228565,
      0.4071524092425942,
      0.4115433697124029,
      0.4102677983010341,
      0.3115714178265625,
      0.43899685057452653,
      0.42913639383331037,
      0.32454809007004126,
      0.3212403554628369
    ],
    [
      0.3521225817013338,
      0.4191086818064014,
      0.4208428217943996,
      0.39855573356480933,
      0.0,
      0.40301998384193816,
      0.3754721142406998,
      0.4690605266418728,
      0.3924602482985131,
      0.45207698451711953,
      0.36203309239405845,
      0.2332769830498722,
      0.37747498669736523,
      0.3717228693659256,
      0.38321905194363426,
      0.3474544341669279,
      0.3957391313262406,
      0.3819291414887489,
      0.3476549505974349,
      0.35927677447131945,
      0.37445682331120445,
      0.343411941197729,
      0.4146708671876287,
      0.3337398790739874,
      0.33354812203612205,
      0.37002503909470974,
      0.35458595798386194,
      0.3493595994484635,
      0.30092305047069146
    ],
    [
      0.3499068650648709,
      0.43291285802827084,
      0.39175828682380565,
      0.40556467962999165,
      0.43475529826946047,
      0.0,
      0.3380423759887705,
      0.4364347250243472,
      0.43334913636472594,
      0.411619067695699,
      0.3799432880606839,
      0.26156885778236094,
      0.3955345354593629,
      0.44910722797959823,
      0.42513089855749175,
      0.34821763200490685,
      0.4253896881462078,
      0.4124711549358111,
      0.41511259646646814,
      0.41806895369083974,
      0.41410630186495356,
      0.3451912581084551,
      0.41607759796112487,
      0.40697470062524066,
      0.4042227681678665,
      0.4065506884232939,
      0.42841655060397676,
      0.36056580505004865,
      0.31720384936694734
    ],
    [
      0.2543640495304649,
      0.38735885056826436,
      0.33425840890087555,
      0.35400737120435743,
      0.39092065157246614,
      0.3632887410396701,
      0.0,
      0.31412661885492565,
      0.34663683177101934,
      0.33219568429709034,
      0.3153104765763195,
      0.22247282542527125,
      0.3281777419731162,
      0.35319019975135313,
      0.3382399265786993,
      0.3016194068997433,
      0.31806887296317776,
      0.3393213508288244,
      0.3796655613145399,
      0.2990380455885311,
      0.27954705622717646,
      0.28573112508809695,
      0.2940766324541202,
      0.3449797800184842,
      0.26617619857717245,
      0.35040200467204574,
      0.3285716856281631,
      0.25254746717968457,
      0.2188077229407417
    ],
    [
      0.4462190178881005,
      0.5100440839644194,
      0.4164371391565649,
      0.47276756728113534,
      0.6388726767466808,
      0.5163350263102442,
      0.43713869082618717,
      0.0,
      0.4994390490174967,
      0.4772283210189525,
      0.4938538296508175,
      0.3048140845278311,
      0.48833466882073995,
      0.5411538779709231,
      0.49111063521748455,
      0.4477624189810705,
      0.46959473105199945,
      0.5748279565002588,
      0.4791815863346356,
      0.45825015871752184,
      0.5456378503452106,
      0.47719401901745595,
      0.4640620642452189,
      0.4719137307382337,
      0.3908146678538418,
      0.4771426006771693,
      0.5123419814263444,
      0.46425514427419157,
      0.39845647221706804
    ],
    [
      0.3482888072710433,
      0.4139704355607763,
      0.38629035784917565,
      0.43526561795215213,
      0.40003359509423975,
      0.39241275598332015,
      0.3269390308745874,
      0.39010700254412467,
      0.0,
      0.37688436314342444,
      0.4251587953295435,
      0.26598240575207077,
      0.48496047817368226,
      0.4760885003962245,
      0.38824001979149303,
      0.37374343965907797,
      0.3735258502456251,
      0.4030652744738503,
      0.3302657122092867,
      0.3389873901048952,
      0.38596212605429825,
      0.32038657310961294,
      0.3659299814351944,
      0.3648661347332889,
      0.33313388239347397,
      0.3909163964000988,
      0.4082264112173659,
      0.37898546666212707,
      0.315718746412343
    ],
    [
      0.3748961796327177,
      0.4534322434364939,
      0.4163819809067335,
      0.42410088633387466,
      0.5150341188217813,
      0.41376945268147436,
      0.33564365454935974,
      0.42303948774386146,
      0.39266934951530286,
      0.0,
      0.4126849221892439,
      0.2317442415239659,
      0.39961454263691554,
      0.46769546487792524,
      0.37310141421745024,
      0.45123475142609193,
      0.4680279837327117,
      0.3892713242093988,
      0.3657595418287092,
      0.4431034899406159,
      0.45743682195291124,
      0.35416035279172875,
      0.414424864568703,
      0.38121403295449996,
      0.38848557183737453,
      0.42925469208383826,
      0.43299148542923627,
      0.33931931751710853,
      0.30664501570505154
    ],
    [
      0.4327721474367028,
      0.5184960664101186,
      0.46241872646462157,
      0.49557492966338046,
      0.45420412274477506,
      0.4639689279678354,
      0.39702827535653507,
      0.4792348855007531,
      0.49285015003173926,
      0.4697227234706438,
      0.0,
      0.3029585922999294,
      0.4919449709053043,
      0.510036643344363,
      0.3935175447471726,
      0.4689077266636872,
      0.4703888731659547,
      0.4277383755248747,
      0.4689273349487024,
      0.47627091010449285,
      0.43661341608059123,
      0.4306399496675615,
      0.4570957558002513,
      0.44230988463936716,
      0.3996582009541856,
      0.509232871064061,
      0.5028027332078098,
      0.4166409738989807,
      0.3771414112869589
    ],
    [
      0.2313220539468459,
      0.26404832414334956,
      0.22606539455411712,
      0.24860406329100404,
      0.2558444576517802,
      0.25095253415551966,
      0.27425909491547196,
      0.27900169537513597,
      0.2758057619956942,
      0.2811185876543265,
      0.24513732967834478,
      0.0,
      0.24014520886913582,
      0.2685977269872437,
      0.27385950131140513,
      0.2678864931008953,
      0.2574232262972662,
      0.22023330381703676,
      0.2574160856806307,
      0.2283257097198761,
      0.2609876909823432,
      0.2964690480193386,
      0.2963579718113376,
      0.25868879393339106,
      0.2890117976033786,
      0.28485470505765953,
      0.27847637749201204,
      0.21076554953690163,
      0.31909665653883224
    ],
    [
      0.36114684523907825,
      0.44504384864526547,
      0.349685244323356,
      0.44358906549526056,
      0.36546407598562136,
      0.41672804974547284,
      0.29714500186253723,
      0.34272839607764016,
      0.49430623288006714,
      0.3519069061173974,
      0.4070583441278641,
      0.21591724151104108,
      0.0,
      0.45282020113797006,
      0.37757051280873877,
      0.35380007926634716,
      0.38502539373958244,
      0.4222266572747746,
      0.3694298964878624,
      0.3543730215635321,
      0.3939412239888238,
      0.327022236633159,
      0.3485746203363038,
      0.33278972463039813,
      0.3031660925853803,
      0.3669011537396769,
      0.4169877033666147,
      0.3513615954602336,
      0.30956569561430625
    ],
    [
      0.3876095857344892,
      0.47339674894590167,
      0.40402446362417566,
      0.49911643613371615,
      0.4370391230378148,
      0.47389731332505125,
      0.3558112380276144,
      0.4572760488719412,
      0.4999814576386514,
      0.4535527481231545,
      0.4442387267233112,
      0.23811032363365503,
      0.49698894363312895,
      0.0,
      0.42133518005941983,
      0.4066813444770161,
      0.4654117749765969,
      0.4415358315749178,
      0.4434250145827614,
      0.4064194193027657,
      0.47139044682387543,
      0.3834373577564718,
      0.40650141450100796,
      0.43438121492344206,
      0.3671782427776584,
      0.43025231978741996,
      0.5130532469871163,
      0.37711209028272563,
      0.3438115886781077
    ],
    [
      0.3658267084624096,
      0.48055392368853944,
      0.4526180932060986,
      0.4257284959255587,
      0.48334617692374926,
      0.4453757895730015,
      0.3693858090769344,
      0.4249855187677427,
      0.4357616134310187,
      0.4345883274894988,
      0.3841477179693278,
      0.2494790946485832,
      0.4512008459307477,
      0.4340757492089897,
      0.0,
      0.3727053561607272,
      0.4537829979985757,
      0.4445917831584245,
      0.38609820634356784,
      0.44428851889997056,
      0.4344835792787771,
      0.33460643698573156,
      0.4155011204309438,
      0.39048667040310536,
      0.33898665386270044,
      0.47558042610254714,
      0.418488659988409,
      0.358581043311065,
      0.31970835189907665
    ],
    [
      0.3994305777711318,
      0.5513102910114909,
      0.39469216023511633,
      0.4769390708179333,
      0.4971785155600654,
      0.4679941150876168,
      0.4208221987846572,
      0.41866019526896214,
      0.4761273131084718,
      0.4866650685326792,
      0.47651887700912443,
      0.27810183600000316,
      0.4666168610914647,
      0.5182999318995136,
      0.4022160362104199,
      0.0,
      0.44378411744924695,
      0.4332926535090691,
      0.45727004118241066,
      0.44252065918641437,
      0.43634175563179123,
      0.40526099905888735,
      0.44012837104105107,
      0.41947662569951505,
      0.380494373008337,
      0.450228834410044,
      0.5048704723152282,
      0.3610399527166097,
      0.3349609994985896
    ],
    [
      0.387689571664936,
      0.4154349216744626,
      0.3654321604417279,
      0.40337589306849586,
      0.40666144239718127,
      0.4006065442836815,
      0.3084606761299198,
      0.39751613810908326,
      0.41701207385824723,
      0.5037721555053516,
      0.36281572360680037,
      0.22702187428168608,
      0.4356074480741745,
      0.4477319126525492,
      0.3809196732419371,
      0.37463116004111985,
      0.0,
      0.4163990087789964,
      0.3696489400430343,
      0.4425678941970006,
      0.4241541948249008,
      0.36075914774837403,
      0.4084380086477475,
      0.3522022280916881,
      0.3433055663640261,
      0.4069541141758968,
      0.4458009251291226,
      0.3325278921127788,
      0.30575555438614055
    ],
    [
      0.36274718240729853,
      0.4030871984128277,
      0.3967979111390183,
      0.4089987257771006,
      0.41517229056587657,
      0.4364560952552643,
      0.33471541753959766,
      0.4543656653839061,
      0.4335589348011466,
      0.42656008294283176,
      0.3816652104435614,
      0.25903070271148265,
      0.4222198842533851,
      0.41494801426856665,
      0.4363453245883073,
      0.3617515217181846,
      0.3984244207181169,
      0.0,
      0.4400626718303835,
      0.40442929716592735,
      0.43569426796632027,
      0.39125037100971305,
      0.3949402177094137,
      0.4041803211407762,
      0.33784108341200536,
      0.43814438784123966,
      0.40039809906412716,
      0.3783657059519314,
      0.3305292246630105
    ],
    [
      0.39406941757865543,
      0.4076540129986479,
      0.333547085452667,
      0.3882088928580576,
      0.38113357732023934,
      0.36331907659729734,
      0.37188326767373603,
      0.4010392042869295,
      0.39439636398102906,
      0.4305026138397783,
      0.38882439568275373,
      0.21863597993894812,
      0.40735315518372883,
      0.37303368255299874,
      0.3359900872863133,
      0.3893589969258495,
      0.41314979797274476,
      0.37616268334108827,
      0.0,
      0.3412944953528938,
      0.4427242239340856,
      0.3702733293341507,
      0.3964950624814352,
      0.3952324408205805,
      0.33694060088519384,
      0.4421746110198175,
      0.3893269659461651,
      0.3250306177176894,
      0.3123392810934609
    ],
    [
      0.3299213907626575,
      0.36613942457425996,
      0.37479129722460724,
      0.3666826051166223,
      0.3631699278601317,
      0.3209723450559281,
      0.2894734627321143,
      0.33788176261408065,
      0.3595162893040713,
      0.3411484500244788,
      0.3645273706860759,
      0.19996848802221479,
      0.3198280357309331,
      0.317142422133351,
      0.3276279267196067,
      0.33057098843256516,
      0.3597011971477906,
      0.32865587623931325,
      0.3076725070559059,
      0.0,
      0.32750921295920055,
      0.29864851225426126,
      0.37280870244688624,
      0.335074277386997,
      0.3214941898728565,
      0.403954920160964,
      0.3300156840379702,
      0.3041338567869931,
      0.2519573478844286
    ],
    [
      0.39424760456734087,
      0.44940148898365795,
      0.39724088868499563,
      0.45726988617969977,
      0.4450927973182408,
      0.4104263847467504,
      0.341652346517892,
      0.42697336462785995,
      0.448385087110988,
      0.47304827927602777,
      0.43013983287445234,
      0.24510887827856576,
      0.453102344454027,
      0.4695575946869448,
      0.41517416080598335,
      0.40920908159735947,
      0.40509751995348764,
      0.4501082863386443,
      0.4045933319496189,
      0.3868100530570566,
      0.0,
      0.382578746731995,
      0.40151686874147186,
      0.41834269585163497,
      0.31705171529700804,
      0.39701947661071957,
      0.48130837481948663,
      0.35343344429919177,
      0.3712678870153543
    ],
    [
      0.36285155553530735,
      0.4433299017182306,
      0.358822796403766,
      0.39180542727090306,
      0.3995559174638108,
      0.40232590242765487,
      0.3796061094420444,
      0.4182191669039357,
      0.4082967400808015,
      0.40117383564382014,
      0.43435675604340873,
      0.2779300400094178,
      0.42044014424768617,
      0.39976134282442377,
      0.4038946363094502,
      0.40986487232678903,
      0.4100159198197122,
      0.41499929309887595,
      0.39794462862049573,
      0.38691275775569545,
      0.3998123379309628,
      0.0,
      0.39033904058432634,
      0.40193070777903395,
      0.3663496469894414,
      0.41207461422815506,
      0.4551289840532575,
      0.41405159070094255,
      0.40316435567230724
    ],
    [
      0.40305513044764907,
      0.42848411766601724,
      0.40288258837014057,
      0.45052259333457956,
      0.47306434706860845,
      0.4360405187998917,
      0.3648685317530531,
      0.46458328480461497,
      0.48366829401113076,
      0.49138078997815415,
      0.43517546357777137,
      0.2946353806393647,
      0.46391523447248306,
      0.4181368959011762,
      0.40161912795096555,
      0.4179152843872753,
      0.47315354677919164,
      0.39982215104429275,
      0.43624489503584574,
      0.4385821744453158,
      0.47474785222701943,
      0.40184685856608104,
      0.0,
      0.4191389678409938,
      0.45085003682387814,
      0.42173204502577666,
      0.4752617492254225,
      0.3859148363189149,
      0.33069224680025
    ],
    [
      0.35856414878647747,
      0.4736820104015007,
      0.4128381423334182,
      0.4514220236457269,
      0.429651770585334,
      0.4152298046981424,
      0.39843837166868146,
      0.452934390242244,
      0.42088483504728536,
      0.4272178944362832,
      0.41207615295683064,
      0.2574654130676908,
      0.4131388267408731,
      0.4322680599590809,
      0.4329859129274878,
      0.37447178070725196,
      0.37920881938920026,
      0.4158474111664323,
      0.40678031319686525,
      0.40037995475574095,
      0.4119711590600419,
      0.3870390460153883,
      0.3966670780581205,
      0.0,
      0.3834202891171028,
      0.4435753504578881,
      0.41584634630048045,
      0.41357545258074957,
      0.3495977845481315
    ],
    [
      0.365583221770998,
      0.42602287812369255,
      0.36733205791050216,
      0.42744532794263757,
      0.4342246264738645,
      0.4448485468993777,
      0.37320677883601316,
      0.45694305697270643,
      0.45484181466443263,
      0.46426209072408486,
      0.42807423854798454,
      0.28824947332616446,
      0.42608041277837905,
      0.42482972396748253,
      0.3883623323570815,
      0.4125512436489367,
      0.46516243740793395,
      0.4208143138248195,
      0.4023830153890875,
      0.4707543040449347,
      0.4074296796598629,
      0.3649138262232352,
      0.4674485662994463,
      0.4220874872046103,
      0.0,
      0.43373791433217335,
      0.4358890323652649,
      0.33482592747632345,
      0.33521876256342353
    ],
    [
      0.4364965506700915,
      0.4806971148939425,
      0.45825602581272284,
      0.46419831744107576,
      0.4520030959251744,
      0.4133110934129314,
      0.3885760128966862,
      0.4558966676271483,
      0.415014739612672,
      0.46828995061736123,
      0.47822244404180103,
      0.29341817731249287,
      0.46922285647745277,
      0.4603801649974888,
      0.4199858145047115,
      0.4287912465774093,
      0.4451839372223121,
      0.5162145806780356,
      0.4846987682060766,
      0.4729211830673645,
      0.4602109827529688,
      0.4340435500800226,
      0.4452703133054041,
      0.4808370885014033,
      0.3801025388295225,
      0.0,
      0.461903287599172,
      0.3689988104305586,
      0.34250835410298186
    ],
    [
      0.38850564149055766,
      0.43479421877725066,
      0.46651682224119617,
      0.4082544699234629,
      0.3994891210661833,
      0.42408006203201976,
      0.35413734413846965,
      0.4243548134872839,
      0.45706866234555665,
      0.4575885268091471,
      0.3954220621124065,
      0.27641195676148667,
      0.4871671265423754,
      0.4974055471512975,
      0.3955446919254493,
      0.4122308493738511,
      0.4349107825564269,
      0.3906991939509801,
      0.38503267281722087,
      0.4362654438150311,
      0.4739421871659848,
      0.37226028115269005,
      0.4281055928068376,
      0.41693421608645576,
      0.3698553788398562,
      0.4488224289154952,
      0.0,
      0.33081720272463544,
      0.3453369379116249
    ],
    [
      0.3452272498187947,
      0.41484437182234735,
      0.365969992464507,
      0.3914811780187901,
      0.39757881927015437,
      0.39483340960983226,
      0.309005515285947,
      0.37486028546957506,
      0.42621807485685137,
      0.4129772382856909,
      0.3923370602976093,
      0.25348400873130084,
      0.39803407806493785,
      0.3928644467233653,
      0.39212838283714424,
      0.36950123473244334,
      0.3721133553985394,
      0.42826988170651226,
      0.39943081957115245,
      0.4094614862641901,
      0.38660671707151084,
      0.3718598434254614,
      0.3758258343418177,
      0.4255724085910162,
      0.3690060165706577,
      0.4300447845012807,
      0.3902975561895976,
      0.0,
      0.33730693191785854
    ],
    [
      0.3103083894662353,
      0.40921918236784105,
      0.3195373314645764,
      0.36394779017369006,
      0.3756433873688385,
      0.3831930440556961,
      0.35609252074203,
      0.37264806312164556,
      0.36557917630330117,
      0.35318945037933513,
      0.36110987633269565,
      0.37212832860804346,
      0.38610737592939537,
      0.37608528394937046,
      0.3522465333555831,
      0.36190752004293314,
      0.36729558434727805,
      0.346537803856362,
      0.3671029351375479,
      0.3491098558420829,
      0.3661641714065622,
      0.45026958727221134,
      0.3914374440181745,
      0.3691811661032336,
      0.36591073075571456,
      0.3302641537706166,
      0.38433474061633177,
      0.3669326852993786,
      0.0
    ]
  ],
  "row_avgs": [
    0.5697830873381335,
    0.38428921535089916,
    0.3250405668763496,
    0.39913602453906083,
    0.3720436561326076,
    0.39514991593377075,
    0.31761076030087126,
    0.4773280018134928,
    0.37836912681522844,
    0.4019691851801814,
    0.4517534686911197,
    0.2621698265757241,
    0.36986696645158235,
    0.42617748731942534,
    0.4080344167544937,
    0.43718724653913726,
    0.3872572444118236,
    0.3965242939529043,
    0.37571764000203345,
    0.3303924454009738,
    0.4083985150491591,
    0.3987485364958806,
    0.42635481940342357,
    0.40597066224465894,
    0.41226868184769483,
    0.438416202414178,
    0.4111412226757583,
    0.3831121779228174,
    0.3669101468602395
  ],
  "col_avgs": [
    0.3602385973426359,
    0.4377559353035018,
    0.3948123998214176,
    0.42187213024649034,
    0.4301221454584915,
    0.41490448356716164,
    0.3589254018441098,
    0.41338317687558856,
    0.43232548910360824,
    0.42676118327352175,
    0.40808812982582937,
    0.26208301519630733,
    0.4285582043021127,
    0.43536998804592464,
    0.39530466262721564,
    0.38903592970144085,
    0.41311620349686873,
    0.41117259852189225,
    0.4022798331076246,
    0.40591594639714196,
    0.41307876737331284,
    0.3737510307236433,
    0.4039467421725199,
    0.3981797569860136,
    0.3540532432989593,
    0.41908436976909985,
    0.42819401739036206,
    0.35635810867149864,
    0.3284500508493285
  ],
  "combined_avgs": [
    0.4650108423403847,
    0.4110225753272005,
    0.35992648334888355,
    0.4105040773927756,
    0.40108290079554954,
    0.4050271997504662,
    0.3382680810724905,
    0.44535558934454067,
    0.4053473079594183,
    0.41436518422685154,
    0.42992079925847454,
    0.2621264208860157,
    0.39921258537684756,
    0.430773737682675,
    0.40166953969085467,
    0.41311158812028903,
    0.4001867239543462,
    0.40384844623739824,
    0.388998736554829,
    0.36815419589905785,
    0.41073864121123593,
    0.3862497836097619,
    0.41515078078797174,
    0.40207520961533627,
    0.3831609625733271,
    0.4287502860916389,
    0.4196676200330602,
    0.369735143297158,
    0.347680098854784
  ],
  "gppm": [
    611.205598812614,
    574.9620553222982,
    588.9056676588053,
    581.3577209832273,
    573.8451124693266,
    585.9573560702813,
    606.6379800394768,
    585.0984856553869,
    577.2839896320106,
    581.3236449432118,
    590.9517264021443,
    657.1412899918835,
    578.6080363169497,
    577.8614756361383,
    594.9604035481163,
    599.5758383918917,
    584.9202154088999,
    588.7724738717163,
    593.6442949479716,
    588.1581441521608,
    586.1416345070658,
    606.8797633443197,
    588.8168511263066,
    595.3626468667355,
    610.4397069947239,
    582.5272837778272,
    579.0166650363398,
    612.2734665701631,
    626.0282638392836
  ],
  "gppm_normalized": [
    1.3514133427241681,
    1.3014192486483147,
    1.3433542594211136,
    1.3133138631601928,
    1.2973107893394877,
    1.322705405395601,
    1.3754834321415748,
    1.3129509886640012,
    1.3011761868363696,
    1.3099316501529852,
    1.3269875592741014,
    1.4825393574381356,
    1.3076846876998545,
    1.301817513860138,
    1.3412789645842051,
    1.3464220796243969,
    1.3162340059488007,
    1.3284288990779627,
    1.3347475353858833,
    1.3305452876234274,
    1.3160940301554291,
    1.3612673485340232,
    1.320935877259931,
    1.338259615093607,
    1.374748536218697,
    1.3083246627328506,
    1.3028067152434097,
    1.378934451704751,
    1.4035219578802967
  ],
  "token_counts": [
    277,
    515,
    611,
    479,
    483,
    468,
    529,
    403,
    449,
    451,
    404,
    473,
    494,
    442,
    447,
    417,
    443,
    473,
    430,
    500,
    413,
    401,
    405,
    424,
    448,
    408,
    436,
    450,
    391
  ],
  "response_lengths": [
    1350,
    2810,
    3414,
    2627,
    2679,
    2674,
    2885,
    2233,
    2464,
    2455,
    2216,
    2706,
    2661,
    2412,
    2360,
    2348,
    2453,
    2596,
    2379,
    2708,
    2205,
    2235,
    2293,
    2380,
    2521,
    2117,
    2375,
    2574,
    2181
  ]
}