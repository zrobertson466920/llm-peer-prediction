{
  "example_idx": 84,
  "reference": "Under review as a conference paper at ICLR 2023\n\nON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe abstract the features of multi-modal data into 1) uni-modal features, which can be learned from uni-modal training, and 2) paired features, which can only be learned from cross-modal interaction. Multi-modal joint training is expected to benefit from cross-modal interaction on the basis of ensuring uni-modal feature learning. However, recent late-fusion training approaches still suffer from insufficient learning of uni-modal features on each modality, and we prove that this phenomenon does hurt the model’s generalization ability. Given a multi-modal task, we propose to choose targeted late-fusion learning method from Uni-Modal Ensemble (UME) and the proposed Uni-Modal Teacher (UMT), according to the distribution of uni-modal and paired features. We demonstrate that, under a simple guiding strategy, we can achieve comparable results to other complex late-fusion or intermediate-fusion methods on multi-modal datasets, including VGG-Sound, Kinetics-400, UCF101, and ModelNet40.\n\nFigure 1: Overview of Modality Laziness. Although multi-modal joint training provides the opportunity for cross-modal interaction to learn paired features, the model easily saturates and ignores the uni-modal features that are hard to learn but also important to generalization.\n\n1\n\nINTRODUCTION\n\nMulti-modal signals, e.g., vision, sound, text, are ubiquitous in our daily life, allowing us to perceive the world through multiple sensory systems. Inspired by the crucial role that multi-modal interactions play in human perception and decision (Smith & Gasser, 2005), substantial efforts have been made to build effective and reliable computational multi-modal systems in fields like multimedia computing (Wang et al., 2020; Xiao et al., 2020), representation learning (Arandjelovic & Zisserman, 2017; Radford et al., 2021) and robotics (Chen et al., 2020a).\n\nAccording to how the features of multi-modal data can be learned, we abstract them into two categories: (1) uni-modal features, which can be learned from uni-modal training, and (2) paired features, which can only be learned from cross-modal interaction. In this paper, we focus on multimodal tasks where uni-modal priors are meaningful 1 (Kay et al., 2017; Chen et al., 2020b). Ideally, we hope that multi-modal joint training can learn paired features through cross-modal interactions on the basis of ensuring that enough uni-modal features are learned.\n\n1Uni-modal prior here means that we get predictions only according to one modality in multi-modal tasks.\n\n1\n\nUni-Modal TrainingMulti-Modal Traininguni-modalfeaturespairedfeaturesuni-modalfeaturespairedfeatureslearned featuresunlearned featuresModality #1Modality #2Modality #1Modality #2Under review as a conference paper at ICLR 2023\n\nHowever, recent late-fusion methods still suffer from learning insufficient uni-modal representations of each modality (Peng et al., 2022). We term this phenomenon as Modality Laziness and illustrate that in Figure 1. We theoretically characterize Modality Laziness and prove that it does hurt the generalization ability of the model, especially when uni-modal features are dominant in the given task. Besides the laziness problem, another shortcoming of recent late-fusion approaches is that they are complex to implement. For example, G-Blending (Wang et al., 2020) needs an extra split of data to estimate the overfitting-to-generalization ratio to re-weight the losses and then re-train the model again and again. Peng et al. (2022) proposes OGM-GE, which dynamically adjusts the gradients of different modalities during training. However, it needs to tune too many hyper-parameters 2, including the start and end epoch of the gradient modulation, an “alpha” used to calculate the coefficients for the modulation and whether adaptive Gaussian noise Enhancement (GE) is needed. The more complicated thing is that these hyper-parameters need to be re-tuned on new datasets.\n\nTo this end, more simple and effective methods are urgently needed. We pay attention to the learning of uni-modal features and propose to choose targeted late-fusion training method from Uni-Modal Ensemble (UME) and proposed Uni-Modal Teacher (UMT) according to the distribution of unimodal and paired features. If both uni-modal and paired features are essential, UMT is effective, which helps multi-modal models better learn uni-modal features via uni-modal distillation; if both modalities have strong uni-modal features and paired features are not important enough, UME is properer, which combines predictions of uni-modal models and completely avoids insufficient learning of uni-modal features. We also provide an empirical trick to decide which one to use. Under this guidance, we achieve comparable results to other complex late-fusion or intermediate-fusion methods on multiple multi-modal datasets, including VGG-Sound (Chen et al., 2020b), Kinetics400 (Kay et al., 2017), UCF101 (Soomro et al., 2012) and ModelNet40 (Wu et al., 2022).\n\n2 RELATED WORK\n\nMulti-modal training approaches aim to train a multi-modal model by using all modalities simultaneously (Liang et al., 2021), including audio-visual classification (Peng et al., 2022; Xiao et al., 2020; Panda et al., 2021), action recognition (Wang et al., 2020; Panda et al., 2021), visual question answering (Agrawal et al., 2018) and RGB-D segmentation (Park et al., 2017; Hu et al., 2019; Seichter et al., 2020). There are several different fusion methods, including early/middle fusion (Seichter et al., 2020; Nagrani et al., 2021; Wu et al., 2022) and late fusion (Wang et al., 2020; Peng et al., 2022; Fayek & Kumar, 2020). In this paper, we mainly consider the late-fusion methods following Wang et al. (2020), which is convenient and straightforward to evaluate the learning of uni-modal features. We demonstrate that simple late-fusion approaches can outperform approaches with more complex model architecture (Wu et al., 2022; Xiao et al., 2020).\n\nMulti-modal learning theory. The research on multi-modal learning theory is still at an early age. A line of work focuses on understanding multi-view tasks (Amini et al., 2009; Xu et al., 2013; Arora et al., 2016; Allen-Zhu & Li, 2020), and our assumption on the data structure partially stems from Allen-Zhu & Li (2020). Huang et al. (2021) explains multi-modal learning is potentially better than uni-modal learning and Huang et al. (2022) explains why failure exists in multi-modal learning. Our paper investigates the different types of features in multi-modal data and provides solutions for the weakness of multi-modal learning.\n\nKnowledge distillation was introduced to compress the knowledge from an ensemble into a smaller and faster model but still preserve competitive generalization power (Buciluˇa et al., 2006; Hinton et al., 2015; Tian et al., 2019; Gou et al., 2021; Allen-Zhu & Li, 2020). In this paper, we propose Uni-Modal Teacher to leverage uni-modal distillation for joint training to help the learning of unimodal features, without involving cross-modal knowledge distillation (Pham et al., 2019; Gupta et al., 2016; Tan & Bansal, 2020; Garcia et al., 2018; Luo et al., 2018).\n\n3 ANALYSIS, LEARNING GUIDANCE AND THEORY\n\nIn this section, we show the drawbacks and advantages of joint training. On one hand, joint training results in insufficient learning of uni-modal features (Modality Laziness). On the other hand, it\n\n2https://github.com/GeWu-Lab/OGM-GE_CVPR2022\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\nTable 1: Top 1 test accuracy (in %) of linear evaluation on encoders from various multi-modal latefusion training methods and uni-modal training on VGG-Sound and UCF101.\n\nMethod\n\nLinear-Fusion MLP-Fusion Attention-Fusion G-Blending OGM-GE\n\nUni-Modal Training\n\nVGG-Sound\n\nUCF101\n\nRGB Encoder Audio Encoder RGB Encoder Opt-Flow Encoder\n\n15.56 14.52 13.31 17.69 15.60\n\n23.17\n\n43.44 40.01 43.97 43.90 41.95\n\n45.15\n\n75.66 75.65 74.84 74.91 73.54\n\n77.08\n\n48.08 51.89 7.72 44.49 65.03\n\n74.99\n\n(a) RGB encoder evaluation on VGG-Sound.\n\n(b) Optical flow encoder evaluation on UCF101.\n\nFigure 2: By building a linear classifier on encoders and checking the top-1 accuracy, we evaluate the RGB encoder in VGG-Sound and the optical flow encoder in UCF101 from different multi-modal late-fusion methods.\n\nallows interactions between modalities to learn representations beyond uni-modal features, namely paired features. Based on this, we offer guidance on multi-modal late-fusion learning. Finally, we provide a theoretical analysis of Modality Laziness and justification for our solution.\n\nDiscussion. The importance of uni-modal prior varies across different multi-modal tasks. In tasks like video classification (Chen et al., 2020b) and action recognition (Feichtenhofer et al., 2016; Wang et al., 2020), uni-modal models can achieve good performance alone, suggesting that unimodal priors in these settings are essential. Visual question and answering (VQA) (Agrawal et al., 2018) is a counter example. Specifically, the same image with different text questions may have totally different labels, making it pointless to check its uni-modal accuracy. In this paper, we focus on the tasks where uni-modal priors are essential, following Wu et al. (2022); Peng et al. (2022).\n\n3.1\n\nINSUFFICIENT LEARNING OF UNI-MODAL FEATURES IN MULTI-MODAL TRAINING\n\nThis subsection illustrates that existing multi-modal late-fusion training methods suffer from Modality Laziness. Even recent methods, G-Blending (Wang et al., 2020) and OGM-GE (Peng et al., 2022), are no exception.\n\nIn multi-modal late-fusion learning, each modality is encoded by its corresponding encoder and then a fusion module is applied on top of them to produce outputs. By building a classifier on frozen encoder (Chen et al., 2020c), we assess the learned representations of the encoder:\n\n• As Table 1 shows, all encoders from multi-modal training are worse than those from unimodal training, especially the RGB encoder in VGG-Sound and optical flow encoder in UCF101. No matter which optimizer is used (Appendix A.3).\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\nTable 2: Top-1 test accuracy of multi-modal models and uni-modal models on certain classes of VGG-Sound. Avg Pred: average the two uni-modal models’ predictions directly. Linear Clf: train a multi-modal classifier on top of uni-modal trained encoders. Naive Fusion: train a multi-modal late-fusion model from scratch.\n\nClass ID\n\n164\n\n303\n\nUni-RGB Uni-Audio\n\nAvg Pred Linear Clf Naive Fusion\n\n3 30\n\n37 35 43\n\n2 7\n\n10 15 18\n\n33\n\n4 34\n\n37 33 48\n\n255\n\n3 10\n\n7 27 22\n\n91\n\n4 43\n\n28 60 55\n\n4\n\n12 50\n\n63 65 67\n\n152\n\n127\n\n2 18\n\n21 26 26\n\n0 0\n\n0 2\n4\n\n68\n\n15 53\n\n51 53 72\n\n155 mean acc\n\n5 32\n\n30 49 40\n\n5 27.7\n\n28.4 36.5 39.5\n\nAlgorithm 1 Uni-Modal Teacher (UMT) for late-fusion learning\n\nInput: Uni-modal supervised pre-trained models F m1 fusion multi-modal model F mm, iteration number N , loss weight λtask, λdistill. for 0 to N do\n\npretrain, F m2\n\npretrain, random initialized late-\n\nSample multi-modal data {X m1 , X m2, Y } ∼ D. Compute uni-modal pre-trained features f m1 Compute the prediction and features ˆY , f m1, f m2 from multi-modal model. Compute the losses between ˆY , f m1, f m2 λtask, λdistill, λdistill, respectively. Update the multi-modal model by SGD or its variant.\n\npre of the data by F m1\n\nand Y, f m1\n\npre, f m2\n\npre, f m2\n\npre\n\npretrain, F m2\n\npretrain.\n\nand multiply by the\n\nend for Return: A multi-modal model trained by UMT.\n\n• As Figure 2 shows, throughout the training process, the two encoders mentioned above not only cannot achieve comparable performance to their uni-modal counterparts but are far worse than them.\n\n3.2 HOW DOES A MODEL BENEFIT FROM MULTI-MODAL TRAINING?\n\nIn Sec 3.1, we empirically show that recent late-fusion methods suffer from insufficient learning of uni-modal features. Combining predictions from uni-modal trained models avoids laziness by nature, which raises another question: How does a multi-modal model benefit from multi-modal training? We answer this question by investigating different models on VGG-Sound and find that the model learns some representations beyond uni-modal features.\n\nAs Table 2 shows, in certain classes of VGG-Sound, the accuracy of naive fusion (navie fusion or naive multi-modal late-fusion learning means no carefully designed tricks are used) exceeds the sum of the accuracy of the two uni-modal models. Besides, we evaluate two other methods. One is to directly average the uni-modal models’ predictions, which has little cross-modal interaction. The other one is to train a multi-modal linear classifier on top of uni-modal pre-trained encoders, where modalities can interact with each other through the linear layer. We find that naive fusion training, which owns maximum freedom of cross-modal interaction among these models, gets the best mean accuracy across these classes, suggesting that joint training enables the model to learn representations beyond uni-modal features, which we term as paired features. They are a type of feature that uni-modal training cannot learn.\n\nWe offer more explanations on paired features in Appendix A.10. We also analyze more datasets and find that different datasets have different characteristics, we put more experimental analysis and interpretation in Appendix A.7.\n\n3.3 GUIDANCE ON MULTI-MODAL LEARNING\n\nGiven a multi-modal task, if both uni-modal features and paired features are essential, Uni-Modal Teacher (UMT) is effective; if both modalities have strong uni-modal features and paired features\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nare not important enough, simply combining the predictions of uni-modal models works well, which is named as Uni-Modal Ensemble (UME).\n\nUMT. Uni-Modal Teacher (UMT) is proposed for late-fusion joint training. It distills the pre-trained uni-modal features to the corresponding parts of multi-modal late-fusion models. Distilling knowledge from uni-modal models can help multi-modal models learn uni-modal features better, which happens in feature-level. The framework of UMT is shown in Algorithm 1 and Figure 4. Noting that we use the same backbone in uni-modal and multi-modal model for a specified modality. The backbones and loss function used can be found in Sec 4. More details can be found in Appendix A.4.\n\nUME. If both modalities have strong uni-modal features, joint training does more harm than good. Combining predictions of uni-modal models avoids insufficient learning of uni-modal features by nature. Firstly, we can train uni-modal models independently. Then, we can give final output by weighting the predictions of uni-modal models. The simple ensemble method is named as UniModal Ensemble (UME). We demonstrate that UME can show competitive performance on certain multi-modal datasets.\n\nAn empirical trick to decide which method to use. We can train a multi-modal linear classifier on uni-modal pre-trained encoders and compare that with averaging predictions of uni-modal models. If the performance of the classifier is better, it means we can benefit from cross-modal interaction in this task and we can choose UMT, where cross-modal interactions are preserved while guaranteeing improved learning of uni-modal features; otherwise, the simple cross-modal interaction does more harm than good because of the strong uni-modal features of each modality, and we can choose UME, which avoids Modality Laziness completely.\n\n3.4 THEORETICAL CHARACTERIZATION AND JUSTIFICATION\n\nIn this subsection, we characterize Modality Laziness of Sec 3.1 from a feature learning perspective and prove it does hurt the generalization of the model. And then, we give justification for the learning guidance proposed in Sec 3.3.\n\nBefore diving into the technical details, we first provide some intuition behind the proof. Our goal is to show that how Modality Laziness happens in multi-modal joint training, and we refer to Figure 3 as an illustration. Here we omit the effect of paired features for easier to understand the intuition. During the naive multi-modal training process, learning those easy-to-learn features suffices to reach zero training error (point A in Figure 3). However, the model is under-trained at point A, and the zero-training-error region stops us from further training. As a comparison, uni-modal models can learn more features and achieve point B, outperforming point A.\n\nWe next give Modality Laziness a theoretical explanation under a simple but effective regime. We mainly consider cases with two modalities xm1 and xm2, and similar techniques can be directly generalized to the cases with more modalities.\n\nData distribution. We formalize the distribution of the multi-modal features. Specifically, we abstract the features into uni-modal features (Definition 3.1) and paired features (Definition 3.2) to describe the core differences between uni-modal training and multi-modal joint training. We consider the binary classification regime where the label y has a uniform distribution over {−1, 1} without loss of generality. Such simplification is self-contained to describe the differences between uni-modal features and paired features.\n\nDefinition 3.1 (Uni-modal features, which can be learned from uni-modal training). The i-th unimodal feature (fi(xm1 )) in modality xm1 is generated as3:\n\nw.p. p(fi), yfi(xm1 ) > 0; w.p. 1 − p(fi) − ε(fi), yfi(xm1 ) = 0; w.p. ε(fi), yfi(xm1 ) < 0.\n\nThe i-th uni-modal feature (gi(xm2)) in modality xm2 is similarly generated with parameters p(gi) and ε(gi).\n\n3We simplify “with probability” as “w.p.”\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 3: An illustration of the feature learning results of uni-modal training and multi-modal training without considering paired features. In uni-modal training, modality xmi learns feature set Fi. However, naive joint training learns less features of each modality than uni-modal training when getting zero training error (namely F i ). Uncontroversially, combining predictions of individual trained uni-modal models (B) outperforms naive joint training (A).\n\n′\n\nDefinition 3.2 (Paired features, which can only be learned from cross-modal interaction). The j-th paired feature4 hj is generated as:\n\nw.p. p(hj), yhj(xm1 )hj(xm2) > 0; w.p. 1 − p(hj) − ε(hj), yhj(xm1 )hj(xm2) = 0; w.p. ε(hj), yhj(xm1 )hj(xm2) < 0.\n\nWhen the context is clear, we abuse the notation ri to represent either fi (uni-modal feature in modality xm1), gi (uni-modal feature in modality xm2 ), or hi (paired feature). We name p(ri) as the predicting probability of feature ri. When ri is present (meaning that ri ̸= 0), we use I(ri > 0) − I(ri < 0) to predict y. Otherwise (ri = 0), we random guess y uniformly over {−1, 1}. To simplify the discussion, we always assume ε(fi) = p(fi)/c, where c > 1 is a fixed constant. For the ease of notations, we define the empty feature in Definition 3.3. Definition 3.3 (Empty Feature). Empty feature ei is a kind of uni-modal feature (or paired feature) with p(ei) = ε(ei) = 0.\n\nEvaluation procedure. When the context is clear, we abuse ri to denote the learned features. For each data point, we random guess ˆy on {−1, 1} uniformly when (cid:80) I(ri < 0). Otherwise, we predict the label by ˆy = 2I((cid:80) I(ri < 0)) − 1. We define the error as (cid:80)\n\nI(ri > 0) = (cid:80)\n\nI(ri > 0) > (cid:80)\n\nI(yri < 0) − (cid:80)\n\nI(yri > 0).\n\ni\n\ni\n\ni\n\ni\n\ni\n\ni\n\nTraining procedure. (a.) multi-modal joint training, which directly train the model using both modality xm1 and modality xm2 ; (b.) uni-modal ensemble, which firstly train the features via independent training (xm1 and xm2 separately), and then combine the xm1-learned features and xm2-learned features.\n\nDuring the training process, we first initialize all the features with empty features ei to imitate random initialization. The models then learn the features in descending order of predicting probability, meaning that the powerful features (with large predicting probability) are learned first5. Our goal is to minimize the training error to zero6.\n\nWe now state our main theorem in Theorem 3.4, demonstrating naive joint training learn fewer uni-modal features compared to uni-modal training, which hurts the model’s generalization. Theorem 3.4. In uni-modal ensemble, assume that the training procedure learns bm1 features in modality xm1 and learns bm2 features in modality xm2. We order the probability of uni-modal fea-\n\n4We abuse the notation h to simplify the notations where h(xm1 ) and h(xm2 ) can have different forms. 5Recent works have demonstrated that neural networks indeed prefer easy-to-learn features (Shah et al.,\n\n2020; Pezeshki et al., 2020).\n\n6We always assume that the training error can be minimized to zero.\n\n6\n\nF!′F!x\"!F#′F#x\"\"F#x\"\"F!x\"!AABBTraining ProcedureTesting ProcedureTest errordecreases:Region where training error ≠0:Region where training error =0Fi/Fi′: The feature set learnedfrom modalityx!!in uni/multi-modallearningA: Naïve multi-modal training reaches Point AB: Combining uni-modal trained models reaches Bmulti-modal training curveUnder review as a conference paper at ICLR 2023\n\ntures (both xm1 and xm2) in decreasing order of predicting probability p, namely, p[1], p[2], . . . . In multi-modal training approaches, assume that the training procedure learns km1 uni-modal features in modality xm1, learns km2 uni-modal features in modality xm2, and learns kpa paired features with predicting probability p(h1), . . . , p(hkpa ). We provide three types of laziness:\n\n(a. ) Quantity Laziness: km1 + km2 + kpa ≤ min{bm1, bm2}.\n\n(b. ) Uni-modal Laziness: Each modality in multi-modal training approaches performs worse\n\nthan uni-modal training.\n\n(c. ) Performance Laziness: Consider a new testing point, then for every δ > 0, if the following\n\ninequality holds:\n\n(cid:88)\n\np(hi) ≤\n\n(cid:88)\n\np[i] + ∆(δ),\n\ni∈[kpa]\n\ni∈[bm1+1,bm1+bm2]\n\nwhere ∆(δ) = (cid:112)8(kpa + bm1 − km1 + bm2 − km2) log(1/δ), then with probability7 at least 1 − δ, uni-modal ensemble outperform multi-modal training approaches concerning the loss on the testing point with probability.\n\nIn theorem 3.4, we describe three notations of laziness problem: Quantity Laziness indicates that the number of features learned in naive multi-modal training is less than uni-modal training. Unimodal Laziness shows encoders from multi-modal training perform worse than from uni-modal training because of Quantity Laziness, which fits the experimental results in sec3.1. Performance Laziness compares the performance of multi-modal joint training approaches with Uni-Modal Ensemble, demonstrating that when uni-modal features dominate, combining uni-modal predictions is more effective. We defer the complete proof to Appendix B.1 and generalize that to more modalities (Appendix B.2). We give a concrete example in Appendix B.3 to better illustrate Theorem 3.4.\n\nWe next prove that UMT proposed in Sec 3.3 indeed helps uni-modal feature learning and can also learn some easy-to-learn paired features in Theorem 3.5 and Appendix B.3. Theorem 3.5. Denote the paired features by h1, . . . hL with corresponding predicting probability p(h1), . . . , p(hL). Assume that distillation can boost the training priority by p0 > 0. If there exists paired features whose predicting probability exceeds the boosting probability p0, namely, the set S is not empty:\n\nS = {hi : p(hi) > p0} ̸= φ.\n\nThen UMT helps uni-modal feature learning and can also learn easy-to-learn paired features.\n\n4 EXPERIMENTS\n\nIn Sec 3.4, we justify our method theoretically. In this section, we firstly introduce the experimental setup and then demonstrate that choosing a suitable learning method from UMT and UME can outperforming other complex late-fusion or intermediate-fusion methods in various multi-modal tasks.\n\n4.1 EXPERIMENTAL SETUP\n\nDataset. We run experiments on four datasets. Kinetics-400 (Kay et al., 2017) is a video recognition dataset with 240k videos for training and 19k for validation. We treat the two modalities, RGB and audio, as the inputs. VGG-Sound (Chen et al., 2020b) is an audio-visual classification dataset which contains over 200k video clips for 309 different sound classes. UCF101 (Soomro et al., 2012) is an action recognition dataset with 101 action categories, including 7k videos for training and 3k for testing. ModelNet40 is a 3D object classification task with 9,483 training samples and 2,468 test samples. Following Wu et al. (2022), we treat the front and rear view as two modalities.\n\nTraining Settings. In VGG-Sound, UCF101 and ModelNet40, we use ResNet as our backbone, all with 18 layers. As for Kinetics-400, we use 50 or 101 layers’ ResNet to encode the inputs. Noting that 3D CNN is used for visual data of VGG-Sound and Kinetics-400. The data preprocessing, hyper-parameters, optimizer can be found in the Appendix A.1 and A.2.\n\n7The probability is taken over the randomness of the testing point\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\nTable 3: Comparison between averaging unimodal predictions and multi-modal classifier trained on uni-modal pre-trained encoders.\n\nDataset\n\nMM Clf Avg Preds\n\nVGG-Sound Kinetics-400 UCF101 ModelNet40\n\n51.0 76.4 84.4 91.7\n\n46.1 74.8 86.8 91.9\n\nTable 4: Evaluation of uni-modal classifiers from the multi-modal linear classifier trained on uni-modal pre-trained encoders in UCF101. This evaluation method borrows from Peng et al. (2022).\n\nModel\n\nRGB Opt-flow\n\nUni-Clf from MM Clf Uni-Modal Model\n\n68.2 77.1\n\n52.9 75.0\n\nTable 5: Results of different late-fusion methods. * means the result comes from its original paper\n\nMethod\n\nVGG-Sound Kinetics-400\n\nLinear-Head MLP-Head Atten-Head Aux-CELoss G-Blending OGM-GE\n\nUMT (ours)\n\n49.5 44.8 49.8 49.9 50.4 50.6*\n\n53.5\n\n74.3 74.8 74.1 73.2 75.8* 74.5\n\n76.8\n\nTable 6: Comparison between UMT and Audio-Visual SlowFast (Xiao et al., 2020) on Kinetics-400. AVSlowFast is an representative intermediate fusion method.\n\nMethod\n\nRGB Encoder Acc\n\nAVSlowFast UMT (ours)\n\nAVSlowFast UMT (ours)\n\nSlowFast-50 SlowFast-50\n\nSlowFast-101 SlowFast-101\n\n77.0 78.1\n\n78.8 79.4\n\n4.2 AN EMPIRICAL TRICK TO DECIDE WHICH LEARNING METHOD TO USE.\n\nWe train a multi-modal linear classifier on frozen uni-modal pre-trained encoders and compare that with averaging uni-modal predictions. As Table 3 shows, in VGG-Sound and Kinetics-400, the classifier is better, meaning cross-modal interaction can benefit the classifier in the two datasets. However, in UCF101 and ModelNet40, averaging uni-modal predictions performs well. To explore why the classifier fails in UCF101, we check the uni-modal classifiers of the newly trained multimodal linear layer (details in Appendix A.7.1). As Table 4 shows, they are far worse than the unimodal models. The result shows the simple linear classifier suffers from serious Modality Laziness in UCF101, which negatively impact the performance. Both modalities in ModelNet40 also have strong uni-modal features and can achieve 89% accuracy individually. Averaging uni-modal predictions avoids laziness problem and achieves competitive performance. Based on the above analysis, we perform UMT on VGG-Sound and Kinetics-400, and UME on UCF101 and ModelNet40.\n\n4.3 UMT IS AN EFFECTIVE REGULARIZER FOR UNI-MODAL FEATURE LEARNING\n\nIn this subsection, we demonstrate that Uni-Modal Teacher outperforms other multi-modal training methods in VGG-Sound and Kinetics-400. We use MSELoss as the distillation loss and set the weight of that as 50. Cross Entropy is used as classification loss and its weight is set as 1.\n\n4.3.1 UMT IS EFFECTIVE ON VGG-SOUND AND KINETICS-400.\n\nUMT vs Other Late-Fusion Methods. The late-fusion architecture is commonly used for multimodal classification tasks (Wang et al., 2020; Peng et al., 2022). In late-fusion architecture, the features are extracted from different modalities by the corresponding encoders, and then the head layer is applied to output predictions. We compare different heads, including linear layer, MLP, and attention layer. In UMT, we use a simple linear layer as the multi-modal head. We also conduct another experiment, which adds extra uni-modal linear heads to receive the uni-modal features and generating additional losses to joint optimize the model, namely Auxiliary-CEloss. Auxiliary-CEloss gives all losses equal weights, while G-Blending reweights the losses according to the overfitting-togeneralization-ratio (OGR) (Wang et al., 2020). OGM-GE (Peng et al., 2022) controls the optimization of each modality by online gradient modulation. However, both OGM-GE and G-Blending are complex to implement. As shown in Table 5, UMT outperforms other methods.\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nTable 7: Evaluation on the encoders trained by naive multi-modal training and UMT.\n\nTable 8: Self-Distillation vs UMT on VGGSound.\n\nMethods.\n\nKinetics-400 VGG-Sound RGB Audio RGB Audio\n\nUni-Train MM Baseline UMT\n\n23.2 15.9 24.4\n\n45.2 18.3 45.9\n\n74.1 72.9 74.6\n\n23.5 18.3 21.6\n\nMethod\n\nTest Acc\n\nBaseline Self-Distill (label) Self-Distill (feature)\n\nUMT\n\n49.5 49.7 49.9\n\n53.5\n\nTable 9: Comparison Uni-Modal Ensemble with other joint training methods on UCF101.\n\nMethod\n\nTest Acc\n\nLinear-Head MLP-Head Atten-Head Aux-CELoss G-Blending OGM-GE\n\nUME (ours)\n\n82.3 80.0 74.2 81.3 83.0 84.0\n\n86.8\n\nTable 10: Comparison between Uni-Modal Ensemble and balanced multi-modal learning algorithm (Wu et al., 2022) on ModelNet40. * means the result comes from Wu et al. (2022).\n\nMethod\n\nmulti-modal (vanilla) +RUBi +random +guided\n\nUME (ours)\n\nTest Acc\n\n90.09 ± 0.58* 90.45 ± 0.58* 91.36 ± 0.10* 91.37 ± 0.28*\n\n91.92 ± 0.14\n\nUMT vs AVSlowFast. Audio-Visual SlowFast is an representative intermediate fusion method. We compare UMT with AVSlowFast in Kinetics-400. As Table 6 shows, under different RGB encoders, UMT consistently exceeds AVSlowFast, although we cannot reproduce their results due to the dynamics of Kinetics-400 (Appendix A.11).\n\nAblation Study of UMT. We first evaluate the encoders of UMT by training linear classifiers on them to verify that UMT does improve the uni-modal feature learning. As Table 7 shows, UMT makes its encoders stand out. Benefiting from uni-modal distillation, some encoders even outperform their uni-modal counterparts. We then compare UMT with classic self-distillation methods (distillation on soft label (Hinton et al., 2015) and feature (Romero et al., 2014)). As Table 8 shows, naive self-distillation can only bring limited improvement, showing that UMT improves overall performance by improving the uni-modal feature learning instead of knowledge distillation.\n\n4.3.2 UNI-MODAL ENSEMBLE IN MULTI-MODAL LEARNING\n\nIn this subsection, we demonstrate that Uni-Modal Ensemble is effective on multi-modal datasets where modalities have strong uni-modal features, outperforming other complex methods. Even though we don’t combine these uni-modal predictions in any special way, but simply average.\n\nIn UCF101, we compare Uni-Modal Ensemble with various multi-modal late-fusion methods. As Table 9 shows, although Gradient Blending (Wang et al., 2020) and OGM-GE (Peng et al., 2022) outperforms baseline methods, they are far worse than Uni-Modal Ensemble.\n\nIn ModelNet40, the main comparing methods come from Wu et al. (2022), which uses a multimodal DNN with intermediate fusion. It proposes a balanced multi-modal algorithm which balances conditional utilization of each modality by re-balancing the optimization step. UME surpass their balanced multi-modal algorithm, as Table 10 shows.\n\n5 CONCLUSION\n\nThis paper analyzes the phenomenon of insufficient uni-modal feature learning in multi-modal training and proves that it does hurt the overall performance. We propose to choose proper learning method from Uni-Modal Ensemble and proposed Uni-Modal Teacher according to the distribution of uni-modal and paired features and demonstrate the effectiveness of the guiding principle.\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\n6 REPRODUCIBILITY STATEMENT\n\nIn our paper, we detail the specific implementation steps of the methods in Sec 3.3 and also give the values of the hyper-parameters we use in detail in Appendix A.2. We also provide the codes in supplementary material, which can reproduce the results of the experiments.\n\nREFERENCES\n\nAishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don’t just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4971–4980, 2018.\n\nZeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and\n\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.\n\nMassih R Amini, Nicolas Usunier, and Cyril Goutte. Learning from multiple partially observed views-an application to multilingual text categorization. Advances in neural information processing systems, 22:28–36, 2009.\n\nRelja Arandjelovic and Andrew Zisserman. Look, listen and learn.\n\nIn Proceedings of the IEEE\n\nInternational Conference on Computer Vision, 2017.\n\nRaman Arora, Poorya Mianjy, and Teodor Marinov. Stochastic optimization for multiview representation learning using partial least squares. In International Conference on Machine Learning, pp. 1786–1794. PMLR, 2016.\n\nCristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression.\n\nIn Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535–541, 2006.\n\nChangan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi Krishna Ithapu, Philip Robinson, and Kristen Grauman. Soundspaces: Audio-visual navIn Proceedings of the European Conference on Computer Vision igation in 3d environments. (ECCV), 2020a.\n\nHonglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scale audiovisual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 721–725. IEEE, 2020b.\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020c.\n\nKin Wai Cheuk, Hans Anderson, Kat Agres, and Dorien Herremans. nnaudio: An on-the-fly gpu audio to spectrogram conversion toolbox using 1d convolutional neural networks. IEEE Access, 8:161981–162003, 2020.\n\nHaytham M Fayek and Anurag Kumar. Large scale audiovisual learning of sounds with weakly\n\nlabeled data. arXiv preprint arXiv:2006.01595, 2020.\n\nChristoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1933–1941, 2016.\n\nNuno C Garcia, Pietro Morerio, and Vittorio Murino. Modality distillation with multiple stream networks for action recognition. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 103–118, 2018.\n\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\n\nsurvey. International Journal of Computer Vision, 129(6):1789–1819, 2021.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nSaurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross modal distillation for supervision transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2827– 2836, 2016.\n\nJack Hessel and Lillian Lee. Does my multimodal model learn cross-modal interactions? it’s harder\n\nto tell than you might think! arXiv preprint arXiv:2010.06572, 2020.\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\n\npreprint arXiv:1503.02531, 2015.\n\nXinxin Hu, Kailun Yang, Lei Fei, and Kaiwei Wang. Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 1440–1444. IEEE, 2019.\n\nYu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multimodal learning better than single (provably). arXiv preprint arXiv:2106.04538, 2021.\n\nYu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, and Longbo Huang. Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably). arXiv preprint arXiv:2203.12221, 2022.\n\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.\n\nPaul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning. arXiv preprint arXiv:2107.07502, 2021.\n\nZelun Luo, Jun-Ting Hsieh, Lu Jiang, Juan Carlos Niebles, and Li Fei-Fei. Graph distillation for action detection with privileged modalities. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 166–183, 2018.\n\nArsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. Advances in Neural Information Processing Systems, 34, 2021.\n\nNatalia Neverova, Christian Wolf, Graham Taylor, and Florian Nebout. Moddrop: adaptive multimodal gesture recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38 (8):1692–1706, 2015.\n\nJiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multi-\n\nmodal deep learning. In ICML, 2011.\n\nRameswar Panda, Chun-Fu Chen, Quanfu Fan, Ximeng Sun, Kate Saenko, Aude Oliva, and Rogerio Feris. Adamml: Adaptive multi-modal learning for efficient video recognition. arXiv preprint arXiv:2105.05165, 2021.\n\nSeong-Jin Park, Ki-Sang Hong, and Seungyong Lee. Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation. In Proceedings of the IEEE international conference on computer vision, pp. 4980–4989, 2017.\n\nXiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and Di Hu. Balanced multimodal learning\n\nvia on-the-fly gradient modulation. arXiv preprint arXiv:2203.15332, 2022.\n\nMohammad Pezeshki, S ́ekou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. arXiv preprint arXiv:2011.09468, 2020.\n\nHai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency, and Barnab ́as P ́oczos. Found in translation: Learning robust joint representations by cyclic translations between modalities. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6892–6899, 2019.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and\n\nYoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\nDaniel Seichter, Mona K ̈ohler, Benjamin Lewandowski, Tim Wengefeld, and Horst-Michael arXiv preprint\n\nEfficient rgb-d semantic segmentation for indoor scene analysis.\n\nGross. arXiv:2011.06961, 2020.\n\nHarshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The\n\npitfalls of simplicity bias in neural networks. arXiv preprint arXiv:2006.07710, 2020.\n\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and supIn European conference on computer vision, pp. 746–760.\n\nport inference from rgbd images. Springer, 2012.\n\nLinda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies.\n\nArtificial life, 11(1-2):13–29, 2005.\n\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\n\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.\n\nHao Tan and Mohit Bansal. Vokenization: Improving language understanding with contextualized,\n\nvisual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020.\n\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv\n\npreprint arXiv:1910.10699, 2019.\n\nWeiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12695–12705, 2020.\n\nNan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and Krzysztof J Geras. Characterizing and overIn International\n\ncoming the greedy nature of learning in multi-modal deep neural networks. Conference on Machine Learning, pp. 24043–24055. PMLR, 2022.\n\nFanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer. Audio-\n\nvisual slowfast networks for video recognition. arXiv preprint arXiv:2001.08740, 2020.\n\nChang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning.\n\narXiv preprint\n\narXiv:1304.5634, 2013.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA EXPERIMENTAL DETAILS AND ADDITIONAL EXPERIMENTS\n\nA.1 DATASETS\n\nHere, we describe the preprocessing of Kinetics-400, VGG-Sound, UCF101 and ModelNet40 in detail.\n\nKinetics-400 dataset (Kay et al., 2017) contains over 240k videos for training and 19k for validation, which we download from cvdfoundation 8. Kinetics-400 is a commonly used dataset with 400 classes, and we mainly follow the open source preprocessing methods to process that. For RGB modality, we follow the procedure of PySlowFast 9, which resizes the video to the short edge size of 256. and for audio modality, we follow mmaction2 10 to extract specgram features. When performing joint training, we take consecutive 64 frames from a video with fps of 30 and random crop the video to 224*224, and for audio inputs, we take the specgram that can be aligned in time with the clip extracted from the video. When testing, we ensemble the predictions from uniformly sampled clips with RGB and audio from a video and give the final outputs, following PySlowfast.\n\nVGG-Sound dataset (Chen et al., 2020b), which contains over 200k video clips for 309 different sound classes, is also used for evaluating our method. It is an audio-visual dataset in the wild where each object that emits sound is also visible in the corresponding video clip, making it suitable for scene classification tasks. Please note that some clips in the dataset are no longer available on YouTube, and we actually use about 175k videos for training and 15k for testing, but the number of classes remains the same. We design a preprocessing paradigm to improve training efficiency as follows: (1) each video is interpolated to 256×256 and saved as stacked images; (2) each audio is first converted to 16 kHz and 32-bit precision in the floating-point PCM format, then randomly cropped or tiled to a fixed duration of 10s. For video input, 32 frames are uniformly sampled from each clip before feeding to the video encoder. While for the audio input, a 1024-point discrete Fourier transform is performed using nnAudio (Cheuk et al., 2020), with 64 ms frame length and 32 ms frame-shift. And we only feed the magnitude spectrogram to the audio encoder.\n\nUCF101 dataset (Soomro et al., 2012) is an action recognition dataset with 101 action categories, including 7k videos for training and 3k for testing. And we use the rgb and flow provided by (Feichtenhofer et al., 2016). For RGB, we use one image of (3 ∗ 224 ∗ 224) as the input; while for flow, we use a stack of optical flow images which contained 10 x-channel and 10 y-channel images, So its input shape is (20 ∗ 224 ∗ 224). During training, we perform random crop and random horizontal flip as the data augmentation; while testing, we resize the image to 224 and do not perform data augmentation operations.\n\nModalNet40 is a 3D object classification dataset with 9,483 training samples and 2,468 test samples. We base on the front view and the rear view of the 3D object to classify that, following Wu et al. (2022).\n\nA.2 TRAINING HYPERPARAMETERS\n\nIn this subsection, we show the hyperparameters of our experiments in UCF101 and VGG-Sound in Table 11.\n\nAs for Kinetics-400’s RGB modality, we totally follow the hyperparameters and settings of PySlowFast 11. As for audio modality, we modify the hyperparameters 12 to be as consistent as possible with the RGB training for further joint training. Specifically, we use the same learning rate and batch size as RGB training used.\n\nAs for ModelNet40, we totally follow the experimental settings of Wu et al. (2022) 13.\n\n8https://github.com/cvdfoundation/kinetics-dataset 9https://github.com/facebookresearch/SlowFast/ 10https://github.com/open-mmlab/mmaction2/blob/master/tools/data/build audio features.py/ 11https://github.com/facebookresearch/SlowFast/configs/Kinetics/SLOWFAST 8x8 R50.yaml 12openmmlab/mmaction2/blob/master/configs/recognition audio/resnet/tsn r18 64x1x1 100e kinetics400 audio feature.py 13https://github.com/nyukat/greedy multimodal learning\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nTable 11: The Hyperparameters used in our experiments for VGG-Sound and UCF101.\n\nHyperparameter\n\nValue (VGG-Sound)\n\nValue (UCF101)\n\nEncoder Linear Head MLP Head\n\nAttension Head Training Epoches LR Batch Size Optimizer Scheduler Loss Fusion\n\nResNet3D (Video), 2D (Audio) ResNet2D(Both Modalities) (1024, 309) (1024, 1024) ReLU (1024, 309)\n\n(1024, 101) (1024, 1024) ReLU (1024, 101)\n\nAttension Layer (without new parameters) + a linear layer\n\n20 1e-3 24 Adam StepLR (step=10, gamma=0.1)\n\n20 1e-2 64 SGD ReduceLROnPlateau (patience=1)\n\nCross Entropy for task, MSE for distillation\n\nA.3 CAN EXISTING OPTIMIZERS SOLVE MODALITY LAZINESS?\n\nTable 12: Top-1 test accuracy (in %) of linear classifiers trained on frozen encoders from multimodal late-fusion training under different optimizers and uni-modal training on VGG-Sound.\n\nOptimizer\n\nMulti-modal Performance Audio Encoder RGB Encoder\n\nSGD RMSprop Adagrad Adadelta Adamw Adam\n\nUni-Training\n\n47.13 47.90 42.19 23.18 49.39 49.47\n\n/\n\n40.02 42.77 35.68 17.70 42.41 43.44\n\n45.15\n\n15.53 13.64 19.65 17.37 15.11 15.56\n\n23.17\n\nWhile the results in Table 1 show that different multi-modal methods suffer from learning insufficient uni-modal features, how about changing the optimizer? To answer this question, we try different optimizers for multi-modal late-fusion training (with a linear multi-modal head), including SGD, RMSprop, Adagrad, Adadelta, Adamw and Adam. As Table 12 shows, Modality Laziness exists no matter which optimizer is used.\n\nA.4 DETAILS ON UNI-MODAL TEACHER (UMT)\n\nIn this subsection, we describe how Uni-Molda Teacher (UMT) applies on multi-modal late-fusion tasks. The overall architecture can be found in Figure 4.\n\nUMT in late-fusion classification. In multi-modal late-fusion architecture, modalities are first encoded by the corresponding encoders and then mapped to the output space by a multi-modal fusion head (Figure 4 left). Uni-Modal Teacher distills the pre-trained uni-modal features to the corresponding parts in multi-modal networks in multi-modal training (Figure 4 right). Uni-modal distillation happens before fusion, so it’s suitable for late-fusion multi-modal architecture. The pre-trained unimodal features are generated by inputting the data to the pre-trained uni-modal models.\n\nUMT’s weights. For VGG-Sound and Kinetics, we use 50 (both audio feature distillation and RGB feature distillation) as the distillation loss’s weight. We test different distillation weights on VGGSound and Kinetics-400. As shown in Table 13, on both datasets, UMT performs well with an distillation weight of 50.\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\n′ Figure 4: Model architecture of naive late fusion (left) and Uni-Modal Teacher (UMT) (right). φ mi is the encoder which is supervised pre-trained on uni-modal data. φmi is a random initialed encoder without pre-training. Lmulti is the loss between multi-modal predictions and labels. Ldistill is the uni-modal distillation loss.\n\nTable 13: Different distillation weights of UMT on VGG-Sound and Kinetics-400\n\nDataset VGG-Sound Kinetics-400\n\n0 49.46 74.25\n\n1 49.51 74.99\n\n10 51.31 75.57\n\n20 51.51 76.11\n\n50 53.46 76.77\n\n100 53.11 76.55\n\nA.5 DROPOUT IN MULTI-MODAL TRAINING.\n\nHere we consider the common regularizer, dropout (Srivastava et al., 2014), and a variant of it, namely modality-wise dropout, which randomly drops (with probability 1/3) the feature from one modality in each iteration. Modality dropout is akin to the ModDrop in Neverova et al. (2015). As Table 14 shows, modality-wise dropout is significantly better than dropout, which implies that modality-wise laziness is serious and modality-wise dropout is also effective.\n\nA.6 FINETUNING THE UNI-MODAL PRE-TRAINED ENCODERS\n\nIn this subsection, we use the uni-modal pre-trained encoders’ parameters as the initialized weights in multi-modal training and randomly initialize a multi-modal linear classifier on the encoders. We set the classifier’s learning rate as 1e − 3 and try different learning rates on the encoders.\n\nAs Table 15 shows, using the uni-modal supervised pre-trained encoder’s weights in multi-modal training and then fine-tuning the whole multi-modal model can bring some improvement compared to naive fusion (49.46) but is worse than UMT, which gets 53.46 accuracy. When the learning rate of encoders is large, the encoders forget some abilities to extract uni-modal features.\n\nNgiam et al. (2011) proposes to use Bimodal Deep Autoencoder to pre-train the encoders with multiple modalities. It is a direction worth exploring to address Modality Laziness of deep multimodal models.\n\nA.7 THE ROLE OF CROSS-MODAL INTERACTION ON DIFFERENT DATASETS\n\nIn this subsection, we conduct various experiments to further investigate the effect of cross-modal interaction and explore the benefits and harms that cross-modal interaction brings in different multi-\n\nTable 14: Dropout in multi-modal training on VGG-Sound.\n\nMethod\n\nPerformance\n\nBaseline Dropout Modal-Drop\n\nUMT\n\n49.46 49.83 51.37\n\n53.46\n\n15\n\nφm1φm2φm2φm1''LdistillLdistillLmultiφm1φm2LmultiUnder review as a conference paper at ICLR 2023\n\nTable 15: The top-1 test accuracy of finetuning the uni-modal pre-trained encoders and linear evaluation on finetuned encoders on VGG-Sound.\n\nEncoder LR Top-1 Acc\n\n1e-3 1e-4 1e-5 1e-6\n\n0\n\n50.98 49.37 50.45 50.86\n\n50.95\n\nEncoder Eval Audio RGB\n\n43.98 44.71 45.28 45.29\n\n21.86 21.97 23.13 23.27\n\n45.15\n\n23.17\n\nmodal tasks/datasets. We find that in different datasets, cross-modal interaction has a different effect on the performance.\n\nA.7.1 AVERAGING THE UNI-MODAL PREDICTIONS vs THE LINEAR CLASSIFIER TRAINED ON\n\nUNI-MODAL PRE-TRAINED ENCODERS.\n\nIn Sec 4.2, we train a multi-modal linear classifier on frozen uni-modal pre-trained encoders and compare this classifier with directly averaging uni-modal models’ predictions. As Table 3 shows, this classifier does not consistently outperform simply averaging the uni-modal predictions on all datasets. It shows better performance on VGG-Sound and Kinetics-400, but worse performance on UCF101 and ModelNet40.\n\nTo further explain this phenomenon, we check and disassemble this new trained multi-modal classifier on UCF101. In the late-fusion multi-modal training, the features of different modalities are concatenated first and then the multi-modal classifier receives them and output predictions. Different modalities in the classifier do not share the parameters. So we split the new trained multi-modal linear classifier into uni-modal classifiers. We use the uni-modal pre-trained encoders to extract features and then the uni-modal classifiers receive the corresponding features and output predictions. Noting that OGM-GE (Peng et al., 2022) uses similar technique to check how well different modalities are trained. As Table 4 shows, the uni-modal classifiers from new trained multi-modal classifiers are significantly worse than uni-modal models, implying that the multi-modal classifier trained on uni-modal pre-trained encoders suffers from serious Modality Laziness on UCF101, although it is just a simple linear layer, resulting in worse performance than directly averaging the uni-modal predictions.\n\nA.7.2 CLASS-LEVEL EVALUATION ON DIFFERENT MULTI-MODAL DATASETS\n\nIn this subsection, we compare naive late-fusion learning with averaging predictions of uni-modal models in class level. It’s obvious that there are more cross-modal interactions in naive fusion.\n\nAlthough naive fusion suffers from learning insufficient uni-modal features, we find in some classes in Kinetics and VGG-Sound, the accuracy of naive fusion model outperforms averaging the unimodal models’ predictions, and even outperforms the sum of the accuracy of the two uni-modal models in VGG-Sound and Kinetics-400, as shown in Table 2 and 16.\n\nHowever, We cannot find any class that naive fusion can exceed the sum of the accuracy of the uniRGB model and uni-flow model in UCF101. We select classes in UCF101 by sorting the differences of accuracy between naive fusion and the best uni-modal model in class level and the top ten with the largest difference are selected. In these classes where naive fusion has advantages, averaging the predictions can outperform naive fusion in some classes (ID:29, 67, 71), and this phenomenon is not found in VGG-Sound and Kinetics. And as Tabla 4 and Table 17 show, both RGB and optical flow in UCF101 can get strong performance individually. All the evidence shows that in UCF101, the uni-modal features are totally dominate and any joint training can lead to serious Modality Laziness.\n\nThe mapping betwenn class ID and class name in different datasets The correspondence between id and name of the selected class in VGG-Sound is: 164: People Sniggering, 303: Wood Thrush Calling, 33: Cat Meowing, 255: Sea Waves, 91: Footsteps On Snow, 4: Alligators\n\n16\n\nUnder review as a conference paper at ICLR 2023\n\nTable 16: Top-1 test accuracy of different models on some classes of Kinetics. The accuracy of naive fusion model outperforms averaging the uni-modal models’ predictions, and even outperforms the sum of the accuracy of the uni-audio model and uni-video model.\n\nClass ID\n\nUni-Audio Uni-RGB\n\nAvg Pred Naive Fusion\n\n53\n\n0 42\n\n42 56\n\n90\n\n0 50\n\n50 62\n\n184\n\n0 22\n\n22 32\n\n2\n\n4 28\n\n28 40\n\n368\n\n158\n\n113\n\n263\n\n287\n\n4 mean accuracy\n\n0 39\n\n39 45\n\n0 43\n\n43 49\n\n0 29\n\n29 35\n\n0 82\n\n82 84\n\n4 76\n\n78 86\n\n2 50\n\n50 58\n\n1 46.1\n\n46.3 54.7\n\nTable 17: Top-1 test accuracy of different models on selected classes of UCF101. We select the top-10 classes according to the gap of accuracy between the multi-modal and uni-modal models. As we can see, uni-modal model’s performance is high, meaning paired features in UCF101 are rare.\n\nClass ID\n\nUni-RGB Uni-Flow\n\nAvg Pred Naive Fusion\n\n6\n\n74 60\n\n70 86\n\n10\n\n84 82\n\n95 95\n\n12\n\n76 58\n\n79 87\n\n22\n\n61 47\n\n64 72\n\n29\n\n67 61\n\n86 83\n\n31\n\n32 41\n\n46 59\n\n48\n\n78 64\n\n89 92\n\n57\n\n21 24\n\n36 42\n\n67\n\n75 78\n\n98 88\n\n71 mean accuracy\n\n57 63\n\n83 73\n\n62.5 57.8\n\n74.6 77.7\n\nCrocodiles Hissing, 152: People Gargling, 127: Mynah Bird Singing, 68: Door Slamming, 155: People Humming.\n\nFor Kinetics-400, we sort the classes alphabetically from smallest to largest according to the class name, and then we can get the mapping between class name and id.\n\nIn UCF101, the mapping can be found in classInd.txt, a given file of UCF101.\n\nA.8 CROSS-MODAL INTERACTION IN UNI-MODAL TEACHER (UMT)\n\nIn order to verify whether the multimodal loss in UMT makes sense, we train uni-modal models by knowledge distillation to get better performance than encoders trained by UMT and then combine them by introducing a new multi-modal classifier on these encoders. As Table 18 shows, UMT works better in multi-modal performance, although the encoders of UMT in unimodal evaluation are worse, showing that UMT indeed benefits from cross-modal interaction.\n\nA.9 EXPLORING UMT FOR MULTI-MODAL SEGMENTATION\n\nTable 18: Comparison of UMT with combining uni-modal models trained by distillation on VGG-Sound.\n\nMethod\n\nRGB Audio\n\nR+A\n\nLinear Clf UMT\n\n25.99 24.43\n\n46.00 45.89\n\n52.98 53.46\n\nNYU Depth V2 dataset (Silberman et al., 2012) contains 1449 indoor RGB-Depth data totally and we use 40-class label setting. The number of training set and testing set is 795 and 654 respectively. All perprocessing operations are following (Seichter et al., 2020).\n\nIn contrast to the late fusion classification task, the RGB-Depth semantic segmentation belongs to middle fusion. The main encoder receives RGB inputs, and the depth inputs are fed into the depth encoder. At each intermediate layer, the main encoder fuses its own intermediate outputs and the depth features obtained from the depth encoder, which makes it a mid-fusion task (Seichter et al., 2020). Since features generated by each layer matter, we distill multi-scale depth feature maps using the MSE loss. For feature maps from the RGB encoder, however, since they are generated by fusing RGB and depth modalities, we cannot distill RGB feature maps directly like depth feature maps. To mitigate this effect, we curate predictors, namely 2 layers CNNs, aiming to facilitate the fused feature maps to predict the RGB feature maps trained by the RGB modality before distillation. The\n\n17\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 5: Distillation details of UMT for RGB (left) and depth (right) modalities in multi-modal semantic segmentation (based on ESANet).\n\nTable 19: Model performance comparison under UMT and ESANet on NYU-DepthV2 RGB-Depth semantic segmentation task.\n\nInitialization\n\nTraining Setting UMT\n\nESANet\n\nFrom Scratch ImageNet Pre-train\n\n38.59 48.48\n\n40.45 (+1.86) 49.39 (+0.91)\n\nfull schematic diagram is presented in Figure 5. As shown in Table 19, UMT can also improve multi-modal segmentation whether the encoder is pre-trained on ImageNet or not.\n\nA.10 EXPLANATIONS ON PAIRED FEATURES\n\nWe revisit the definitions of uni-modal features and paired features: uni-modal features, which can be learned by uni-modal training; paired features, which can only be learned by cross-modal interaction in joint training. Different datasets contain different proportions of these features.\n\nIn this subsection, we use synthetic datasets to explain the uni-modal features and paired features in multi-modal tasks.\n\nUnderstanding different types of features in multi-modal tasks by synthetic datasets. Three different multi-modal datasets are generated to help us understand the uni-modal features and paired features in multi-modal tasks. The process of data generation mainly refers to Hessel & Lee (2020).\n\nFirstly, we generate a dataset where each modality can extract the features to give correct predictions. We name this dataset as Dataset α. The data generation process is as follows:\n\n1. Sample random projection P1 ∈ Rd1×d and P2 ∈ Rd2×d from U (−0.5, 0.5). 2. Sample z ∈ Rd ∼ N (0, 1). Normalize z to unit length 3. Sample x ∈ Rd ∼ N (0, 1). Normalize x to unit length\n\n4. if |x · z| ≤ 0.1, return to the Step 3.\n\n5. If x · z > 0.1, then y = 1; else y = 0.\n\n6. Get the data point (P1x, P2x, y).\n\n7. If the amount of data generated is less than N , return to the Step 3; else break\n\nThe P1x, P2x represents two modalities of the multi-modal dataset and we set d1, d2, d, N as 200, 100, 50, 5000, respectively. And we randomly split 80% of the generated data as train set, and the rest serves as a test set. In this dataset, uni-modal models can extract useful features to give correct predictions and multi-modal joint training is not necessary, as Table 20 shows (Dataset α). We name the features, that uni-modal models can learns to give correct predictions in the given task, as uni-modal features.\n\nSecondly, we generate another dataset where the model must rely on both the two modalities to make correct predictions. We name this dataset as Dataset β. The data generation process is as follows:\n\n1. Sample random projection P1 ∈ Rd1×d and P2 ∈ Rd2×d from U (−0.5, 0.5).\n\n18\n\nPredictorFused RGBFeature MapsPredicted RGBFeature MapsPre-trained RGBFeature MapsLdistillDepthFeature MapsPre-trained DepthFeature MapsLdistillUnder review as a conference paper at ICLR 2023\n\nTable 20: Test Accuracy of uni-modal models and multi-modal model on different synthetic datasets. Synthetic Dataset α mainly contains uni-modal features which can be learned in uni-modal training; Synthetic Dataset β mainly contains paired features which need joint training to learn; Synthetic Dataset γ contains uni-modal features and paired features.\n\nDataset\n\nSynthetic Dataset α Synthetic Datset β\n\nSynthetic Dataset γ\n\nUni-modal (Modality 1) Uni-modal (Modality 2)\n\nMulti-modal\n\n100 100\n\n100\n\n51.4 51.8\n\n92\n\n70.9 70.1\n\n94.4\n\nTable 21: The confusion matrix of uni-modal model in Dataset γ. In the data labeled 0, each modality contains features that can give a correct prediction, while in the data labeled 1 or 2, we need both modalities together to make the right predictions.\n\nPredicted 1\n0 2\n0 100% 0\n43% 57% 0\n45.4% 54.6% 0\n\na u\n\nl 0 1\n2\n\nt c\nA\n\n2. Sample x1, x2 ∈ Rd ∼ N (0, 1). Normalize x1, x2 to unit length 3. if |x1 · x2| ≤ 0.25, return to the Step 2.\n\n4. If x1 · x2 > 0.25, then y = 1; else y = 0.\n\n5. Get the data point (P1x1, P2x2, y).\n\n6. If the amount of data generated is less than N , return to the Step 2; else break\n\nThis multi-modal dataset is different from the first dataset, because the labels in this dataset are highly dependent on the relationship between the two modalities. As we can see in Table 20 (Dataset β), the uni-modal models can only gives about 50 percent accuracy, while the multi-modal models can give about 90 percent accuracy. In binary classification tasks, 50 percent accuracy is no different from guessing. In this dataset, because labels are heavily relied on the relationships of the two modalities, we must train both modalities simultaneously to extract the joint representations to learn the relationship of the two modalities, which are beyond uni-modal features. In order to better carry out theoretical analysis, we abstract these representations into paired features, which can only be learned from multi-modal joint training in multi-modal tasks.\n\nFinally, we generate a dataset that contains both uni-modal features and paired features. We name this dataset as Dataset γ. The data generation process is as follows:\n\n1. Sample random projection P1 ∈ Rd1×d and P2 ∈ Rd2×d from U (−0.5, 0.5). 2. Sample z ∈ Rd ∼ N (0, 1). Normalize z to unit length 3. Sample x ∈ Rd ∼ N (0, 1). Normalize x to unit length\n\n4. if x · z ≥ 0.1, get the data point (P1x, P2x, y = 0). else, return to the step 3 until collecting\n\n2500 data points.\n\n5. Sample x1, x2 ∈ Rd ∼ N (0, 1). Normalize x1, x2 to unit length. If x1 · z > −0.1 or\n\nx2 · z > −0.1, resample.\n\n6. if |x1 · x2| ≤ 0.25, return to the Step 5.\n\n7. If x1 · x2 > 0.25, then y = 2; else y = 1.\n\n8. Get the data point (P1x1, P2x2, y).\n\n9. If the total amount of data generated is less than 7500, return to the Step 5; else break\n\nIn the data labeled 0, each modality contains features that can give a correct prediction, while in the data labeled 1 or 2, we need both modalities together to make the right predictions. To further\n\n19\n\nUnder review as a conference paper at ICLR 2023\n\nunderstand how uni-modal models give predictions in dataset that containing both uni-modal and paired features, we give the confusion matrix of the uni-modal model. As Table 21 shows, the unimodal model can give correct predictions for data labeled 0, while for data labeled 1 or 2, it fails and gives a random predictions. Because for data labeled 1 or 2, we need to learn the relationship of the two modalities to give the correct predictions.\n\nIn this subsection, we mainly discuss the synthetic multi-modal datasets and in Appendix A.7, we conduct various experiments on real-world multi-modal datasets to help us understand the uni-modal features, paired features and cross-modal interaction in multi-modal training better.\n\nTraining settings on synthetics datasets. We use a two layer MLP with ReLU as activation function. As for hidden layer, we use 200 dimensions for multi-modal training and 100 dimensions for uni-modal training. We use SGD as the optimizer and the learning rate is 0.2. In each iteration, we use the whole training set to compute the gradients. And we provide the code in supplement materials.\n\nA.11 UNI-MODAL PERFORMANCE IN KINETICS-400\n\nKinetics-400 is a dynamic dataset, because videos may be removed from YouTube. In this subsection, we report the uni-modal performance of ours and Xiao et al. (2020)’s on Kinetics-400. As Table 22 shows, we cannot reproduce their uni-modal performance and ours are lower than theirs. But we demonstrate that UMT outperforms AVSlowFast in Sec 4.3.1, which shows UMT’s effectiveness.\n\nTable 22: Uni-Modal Performance of ours and Xiao et al. (2020)’s on Kinetics-400\n\nours Xiao et al. (2020)\n\nUni-Audio Uni-RGB (SlowFast-50) Uni-RGB (SlowFast-101)\n\n23.5 74.9 77.2\n\n24.8 75.6 77.9\n\n20\n\nUnder review as a conference paper at ICLR 2023\n\nB PROOF\n\nB.1 PROOF OF THEOREM 3.4\n\nTheorem 3.4. In uni-modal ensemble, assume that the training procedure learns bm1 features in modality xm1 and learns bm2 features in modality xm2. We order the probability of uni-modal features (both xm1 and xm2) in decreasing order of predicting probability p, namely, p[1], p[2], . . . . In multi-modal training approaches, assume that the training procedure learns km1 uni-modal features in modality xm1, learns km2 uni-modal features in modality xm2 , and learns kpa paired features with predicting probability p(h1), . . . , p(hkpa ). We provide three types of laziness:\n\n(a. ) Quantity Laziness: km1 + km2 + kpa ≤ min{bm1, bm2}.\n\n(b. ) Uni-modal Laziness: Each modality in multi-modal training approaches performs worse\n\nthan uni-modal training.\n\n(c. ) Performance Laziness: Consider a new testing point, then for every δ > 0, if the following\n\ninequality holds:\n\n(cid:88)\n\np(hi) ≤\n\n(cid:88)\n\np[i] + ∆(δ),\n\ni∈[kpa]\n\ni∈[bm1+1,bm1+bm2]\n\nwhere ∆(δ) = (cid:112)8(kpa + bm1 − km1 + bm2 − km2) log(1/δ), then with probability14 at least 1 − δ, uni-modal ensemble outperform multi-modal training approaches concerning the loss on the testing point with probability.\n\nWe prove the theorem, which shows that naive joint training indeed suffers from overfitting issues, meaning that it learns less features compared to uni-modal ensemble.\n\nProof. We first introduce some additional notations used in the proof. We define the features trained in xm1 -uni-modal training as f1(xm1), . . . , fbm1(xm1), define the features trained in xm1uni-modal training as g1(xm2), . . . , gbm2(xm2). Therefore, there are in total bm1 + bm2 features learned in uni-modal ensemble, namely, f1(xm1), . . . , fbm1(xm1 ), g1(xm2), . . . , gbm2(xm2 ). Besides, We define the features trained in multi-modal training approaches as f1(xm1), . . . , fkm1 (xm1 ), g1(xm2), . . . , gkm2(xm2 ), h1(xm1, xm2 ), . . . , hkpa (xm1, xm2). When the context is clear, we omit the dependency of xm1, xm2 and denote them as fi, gi, hi for simplicity. When the context is clear, we abuse the notation r to represent arbitrary f , g or h. The corresponding predicting probability of feature ri is denoted as p(ri). To summary, there are bm1 + bm2 features in uni-modal ensemble, km1 + km2 + kpa features in multi-modal training approaches.\n\nWe first prove statement (a.), which claims that the number of features learned in multi-modal training approaches are provably less than any of the number of features learned in uni-modal training. The proof depends on the following Lemma B.1.\n\nLemma B.1. Assume there exists T features ri, i = 1, . . . , T . If we replace one of the T features (without loss of generality, rT ) with a more powerful feature r′, where p(r′) > p(rT ), then the predicting probability for each data point increases (where the probability is taken over the randomness of the training data).\n\nWe next provide the proof of statements (a.): based on Lemma B.1. We shall prove km1 + km2 + kpa < bm1 without loss of generality. Start from the features f1(xm1), . . . , fkm1 (xm1 ) which are common features in both multi-modal training approaches and Uni-modal training. Next step, we add feature fkm1+1 in uni-modal approachesand g1 in multi-modal training approaches. Obviously, p(g1) > p(fkm1+1) due to the training priority (or multi-modal training approaches should learn fkm1+1 instead of g1). Therefore, the predicting probability of multi-modal training approaches is larger than uni-modal approaches.\n\nRepeating the procedure by comparing gi with fkm1+i and comparing hj with fkm1+km2+j, the predicting probability of multi-modal training approaches is always larger than uni-modal approaches. Note that bm1 should be always larger than km1 + km2, or the predicting probability of uni-modal\n\n14The probability is taken over the randomness of the testing point\n\n21\n\nUnder review as a conference paper at ICLR 2023\n\napproaches would be smaller than multi-modal training approaches. At the end of the comparison, the predicting probability of multi-modal training approaches is still larger than uni-modal approaches. This requires that uni-modal approaches should learn more features, which can be regarded as uni-modal approacheslearns a features while multi-modal training approaches learns an empty feature. In conclusion, uni-modal approaches learns more features compared to multi-modal training approaches, leading to bm1 > km1 + km2 + kpa.\n\nWe next prove the statement (b.). The proof of (b.) is based on (a.). We next only consider modality xm1, the proof for modality xm2 is similar. Note that the since the number of features learned in multi-modal training approaches is less than bm1, the number of features learned in xm1 must be less than bm1 (Note that those features can be either paired feature or uni-modal feature, namely, f1, . . . , fkm1 and h1, . . . , hkpa ). Therefore, multi-modal training approaches learns less features compared to uni-modal approaches in modality xm1. On the other hand, the predicting probability of features learned in multi-modal training approaches (f1, . . . , fkm1 and h1, . . . , hkpa , considering only modality xm1 for the paired feature) is less than that learned in uni-modal approaches (f1, . . . , fbm1), because otherwise, uni-modal approaches will learn the features in h instead of f . In conclusion, when considering only modality xm1, the number of features learned in multi-modal training approaches is less and its corresponding predicting probability is small. Therefore, each modality in multi-modal training approaches performs worse than uni-modal approaches. We finally prove the statement (c.). Recall that the loss is − (cid:80) 0) − I(yri < 0). Note that E(u(ri)) = 1\n\ni u(ri) where u(ri) = I(yri >\n\n2 p(ri) and |u(ri)| ≤ 1. We derive that:\n\n\n\nP\n\n−\n\n(cid:88)\n\nu(fi) −\n\n(cid:88)\n\nu(gi) −\n\n(cid:88)\n\nu(hi) ≤ −\n\ni∈[km1]\n\ni∈[km2]\n\ni∈[kpa]\n\n\n\nu(gi)\n\n\n\nu(fi) −\n\n(cid:88)\n\ni∈[bm2]\n\n(cid:88)\n\ni∈[bm1] \n\n\n\n\n\n\n\n\n\n=P\n\n=P\n\n(cid:88)\n\nu(fi) +\n\n(cid:88)\n\nu(gi) −\n\nkm1<i≤bm1\n\nkm2<i≤bm2\n\n(cid:88)\n\nu(fi) +\n\n(cid:88)\n\nu(gi) −\n\nkm1<i≤bm1\n\nkm2<i≤bm2\n\n(cid:88)\n\ni∈[kpa]\n\n(cid:88)\n\ni∈[kpa]\n\nu(hi) ≤ 0\n\n\n\nu(hi) +\n\n1 2\n\nE ≤\n\n\n\nE\n\n ,\n\n1 2\n\nwhere E = −E((cid:80) (cid:80)\n\np(fi) − (cid:80)\n\nkm1<i≤bm1\n\nkm1<i≤bm1\n\nu(fi) + (cid:80)\n\nkm2<i≤bm2\n\nu(gi) − (cid:80)\n\ni∈[kpa] u(hi)) = (cid:80)\n\ni∈[kpa] p(hi) −\n\nkm2<i≤bm2\n\np(gi). Due to the training priority and the conclusion in (a.),\n\n(cid:88)\n\np[i] ≤\n\n(cid:88)\n\np(fi) +\n\n(cid:88)\n\np(gi).\n\ni∈[bm1+1,bm1+bm2]\n\nkm1<i≤bm1\n\nkm2<i≤bm2\n\nE\n\ni∈[kpa] p(hi) ≤\nTherefore, (cid:112)8(kpa + bm1 − km1 + bm2 − km2) log(1/δ). We next apply Hoeffding inequality on Equation B.1 and derive that\n\ni∈[bm1+1,bm1+bm2] p[i]\n\n≤\n\n(cid:80)\n\n− (cid:80)\n\n\n\nP\n\n−\n\n(cid:88)\n\nu(fi) −\n\n(cid:88)\n\nu(gi) −\n\n(cid:88)\n\nu(hi) < −\n\n(cid:88)\n\nu(fi) −\n\n(cid:88)\n\ni∈[km1]\n\ni∈[km2]\n\ni∈[kpa]\n\ni∈[bm1]\n\ni∈[bm2]\n\n\n\nu(gi)\n\n\n\n≤ exp(−E2/8(kpa + bm1 − km1 + bm2 − km2)) ≤δ\n\nTo conclude, multi-modal training approaches outperform uni-modal ensemble concerning the testing loss with probability at least 1 − δ.\n\n22\n\nUnder review as a conference paper at ICLR 2023\n\nCompared to uni-modal ensemble, denote the additional paired feature are indexed by c, and the additional uni-modal feature in uni-modal ensemble are indexed by v. We have that:\n\n(cid:88)\n\nP(\n\n(I(fi(x) > 0) − I(fi(x) < 0)) −\n\n(cid:88)\n\n(I(fj(x) > 0) − I(fj(x) < 0)) > 0)\n\ni∈[c]\n\n(cid:88)\n\n=P(\n\ni∈[c]\n\nI(fi(x) > 0) −\n\nI(fj(x) > 0) −\n\nj∈[v] 1\n2\n\n(cid:88)\n\nj∈[v]\n\n≤ exp(−(\n\n(cid:88)\n\n(cid:88)\n\npj −\n\npi)2/8|c + v|)\n\nj∈[v]\n\ni∈[c]\n\n(cid:88)\n\n[\n\ni∈[c]\n\npi −\n\n(cid:88)\n\nj∈[v]\n\npj] >\n\n1 2\n\n[\n\n(cid:88)\n\npj −\n\n(cid:88)\n\npi])\n\nj∈[v]\n\ni∈[c]\n\n(1)\n\nTherefore, if (cid:80) a new data point, uni-modal ensemble can outperforms multi-modal training approaches with high probability.\n\ni∈[c] pi ≥ (cid:112)8(c + v) log(1/δ), the probability is done. Therefore, for\n\nj∈[v] pj − (cid:80)\n\nProof of Lemma B.1. We define r[−T ] as the features r1, . . . , rT −1. The proof is divided into two parts, depending on whether (cid:80) I(ri ̸= 0) is even or odd. We regard the term (cid:80) I(ri ̸= 0) as the number of effective features in r[−T ]. To simplify the discussion, we\n\ni∈[T −1]\n\ni∈[T −1]\n\nrescale r such that |yr| = 1 (when r ̸= 0) or |yr| = 0 (when r = 0). Case 1: When the number of effective features in r[−T ] is even. (a. ) If | (cid:80) rT or r′ does not alter the predicting probability, namely\n\ni∈[T −1] yri| ≥ 2, adding\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n\n\nP\n\ny\n\nrT +\n\n\n\nyri\n\n > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n\n\n=P\n\ny\n\n r′ +\n\n\n\nyri\n\n > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i∈[T −1] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\ni∈[T −1]\n\n(cid:88)\n\nyri\n\n\n\n≥ 2\n\n +\n\nP\n\n1 2\n\n\n\n\n\ny\n\nrT +\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri\n\ni∈[T −1]\n\nyri\n\n≥ 2\n\n\n\n +\n\n1 2\n\n\n\nP\n\ny\n\n r′ +\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i∈[T −1]\n\nyri\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n≥ 2\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n≥ 2\n\n .\n\n(b. ) When the number of effective features in r[−T ] is even, | (cid:80) (c. ) When | (cid:80) adding r′ helps increase the predicting probability compared to rT , namely\n\ni∈[T −1] yri| = 0, due to the assumption that p(r′) > p(rT ) and ε(r) = p(r)/c,\n\ni∈[T −1] yri| ̸= 1.\n\n\n\n\n\nP\n\ny\n\nrT +\n\n\n\nyri\n\n > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n\n\n>P\n\ny\n\n r′ +\n\n\n\nyri\n\n > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\nyri\n\n(cid:88)\n\nyri\n\n= 0\n\ni∈[T −1]\n\n\n\n= 0\n\n +\n\nP\n\n1 2\n\n\n\n\n\ny\n\nrT +\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri\n\ni∈[T −1]\n\n\n\n +\n\n1 2\n\n\n\nP\n\ny\n\n r′ +\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i∈[T −1]\n\nyri\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n= 0\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n= 0\n\n .\n\n\n\n\n\nThe above inequality is derived based on the following equation: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) \n\nrT +\n\n > 0\n\n +\n\ni∈[T −1]\n\ni∈[T −1]\n\ny\n\ny\n\n= 0\n\n(cid:88)\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\nyri\n\nyri\n\n1 2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n\n\n\n\n\n\n\n\n\n\nP\n\nP\n\nrT +\n\n=P\n\nyrT > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri\n\ni∈[T −1]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n= 0\n\n +\n\nP\n\nyrT = 0\n\n1 2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri\n\ni∈[T −1]\n\n= 0\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri\n\ni∈[T −1]\n\n\n\n= 0\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n\n\n=p(rT ) +\n\n1 2\n\n[1 − p(rT ) − ε(rT )]\n\n=\n\n1 2\n\n[1 + (1 − 1/c)p(rT )] .\n\nSince we assume c > 1, the probability increases with probability p(rT ).\n\nTherefore, under the three conditions, adding r′ increase the predicting probability more compared to rT . In summary, under case 1 (a-c), adding r′ increase the predicting probability compared to rT .\n\n23\n\nUnder review as a conference paper at ICLR 2023\n\nCase 2: When the number of features in r[−T ] is odd. The discussion in (b.) can be a little bit more complex compared to case 1. (a. ) If | (cid:80) ity, namely\n\ni∈[T −1] yri| ≥ 2, similar to case 1, adding rT or r′ does not alter the predicting probabil-\n\n\n\n\n\nP\n\ny\n\nrT +\n\n\n\nyri\n\n > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n\n\n=P\n\ny\n\n r′ +\n\n\n\nyri\n\n > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\nyri\n\n(cid:88)\n\nyri\n\n≥ 2\n\ni∈[T −1]\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n≥ 2\n\n +\n\nP\n\n1 2\n\n\n\n\n\ny\n\nrT +\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri\n\ni∈[T −1]\n\n\n\n +\n\n1 2\n\n\n\nP\n\ny\n\n r′ +\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) i∈[T −1]\n\nyri\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n≥ 2\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)\n\n\n\n≥ 2\n\n .\n\ni∈[T −1] yri = −1: \n\n\n\n\nyri = −1\n\n +\n\nP\n\ny\n\nrT +\n\n1 2\n\n\n\nyri\n\n = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n(cid:88)\n\nyri = −1\n\ni∈[T −1]\n\n\n\n\n\n\n\nyrT − 1 = 0\n\nP\n\n1 2\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri = −1\n\ni∈[T −1]\n\n\n\n\n\n(b. ) If | (cid:80)\n\ni∈[T −1] yri| = 1: (b.1 ) If (cid:80)\n\n\n\n\n\nP\n\ny\n\nrT +\n\n\n\nyri\n\n > 0\n\n(cid:88)\n\ni∈[T −1]\n\n\n\n=P\n\nyrT − 1 > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri = −1\n\n +\n\n\n\nP\n\nyrT − 1 = 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\nyri = −1\n\ni∈[T −1]\n\n\n\n\n\np(rT ).\n\n=\n\n=\n\n1 2\n\n1 2\n\n(b.2 ) If (cid:80)\n\ni∈[T −1] yri = +1:\n\n\n\n\n\nP\n\ny\n\nrT +\n\n\n\nyri\n\n > 0\n\n(cid:88)\n\ni∈[T −1]\n\n\n\n=P\n\nyrT + 1 > 0\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n1 2\n\nε(rT )\n\n=(1 − ε(rT )) +\n\n=1 −\n\n1 2c\n\np(rT ).\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1] \n\nyri = 1\n\n +\n\n\n\nyri = 1\n\n +\n\n\n\n\n\ny\n\nrT +\n\nP\n\n1 2\n\n\n\nyri\n\n = 0\n\n\n\nyri = 1\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyrT + 1 = 0\n\nP\n\n1 2\n\n\n\nyri = 1\n\n\n\n(cid:12) (cid:12) (cid:12) (cid:12)\n\n(cid:88)\n\ni∈[T −1]\n\nNote that the probability of event (b.1) and the probability of event (b.2) satisfy the following equation by Lemma B.2:\n\n\n\n\n\nP\n\n(cid:88)\n\ni∈[T −1]\n\n  = cP\n\nyri = 1\n\n\n\n\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri = −1\n\n .\n\n(2)\n\nTherefore, the total probability under case (b) is\n\n1 2\n\np(rT )P\n\n\n\n\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri = −1\n\n + (1 −\n\n1 2c\n\np(rT ))P\n\n\n\n\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri = 1\n\n\n\n\n\n\n\n=P\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri = 1\n\n\n\nwhich is independent of p(rT ). Therefore, adding rT or r′ share the same predicting probability. (c. ) When the number of effective features in r[−T ] is odd, | (cid:80)\n\ni∈[T −1] yri| ̸= 0.\n\n24\n\nUnder review as a conference paper at ICLR 2023\n\nIn summary, under case 2 (a-c), adding r′ do not decrease the predicting probability compared to rT .\n\nThe following lemmas are used during the proof.\n\nLemma B.2. Consider T − 1 features r1, . . . , rT −1, the following equation holds:\n\n\n\n\n\nP\n\n(cid:88)\n\ni∈[T −1]\n\n  = cP\n\nyri = 1\n\n\n\n\n\n(cid:88)\n\ni∈[T −1]\n\n\n\nyri = −1\n\n .\n\n(3)\n\nProof. It can be proved to compare the events A = {(cid:80) −1}. Every event in A has a complementary event in B, namely,\n\ni∈[T −1] yri = 1} and B = {(cid:80)\n\ni∈[T −1] yri =\n\nyri = 1 in B if yri = −1 in A\n\nyri = −1 in B if yri = 1 in A yri = 0 in B if yri = 0 in A\n\nComparing each event in A with its complementary event in B leads to the conclusion.\n\nCombining case 1 and case 2 together leads to the final conclusion.\n\nB.2 GENERALIZE THEOREM 3.4 TO MORE MODALITIES\n\nWe next show that the results in Theorem 3.4 can be generalized to the regime of more modals.\n\nSpecifically, we assume a T -modal regimes, and denote the modals as xmi, i ∈ [T ]. In uni-modal pre-training approaches, let bmi denote the number of returned features in modal i. In multi-modal joint training, let kmi denote the number of uni-modal features for modal i, and kpa denote the number of returned paired features. We derive the following Theorem B.3 for the multi-modal regimes.\n\nTheorem B.3. Based on the above notations, we provide three types of laziness from three perspectives:\n\n(a. ) Quantity Laziness: (cid:80)\n\ni kmi + kpa ≤ mini{bmi}.\n\n(b. ) Uni-modal Laziness: Each modality in multi-modal training approaches performs worse\n\nthan uni-modal training.\n\n(c. ) Performance Laziness: Consider a new testing point, then for every δ > 0, if the following\n\ninequality holds:\n\n(cid:88)\n\ni∈[kpa]\n\np(hi) ≤\n\n(cid:88)\n\np[i] + ∆(δ),\n\ni∈[mini{bmi }+1,(cid:80)\n\ni bmi ]\n\n(cid:113)\n\n8(kpa + (cid:80)\n\nwhere ∆(δ) =\n\nj[bmj − kmj ]) log(1/δ), then with probability15 at least 1 − δ, uni-modal ensemble outperform multi-modal training approaches concerning the loss on the testing point with probability.\n\nB.3 PROOF OF THEOREM 3.5\n\nTheorem 3.5. Denote the paired features by h1, . . . hL with corresponding predicting probability p(h1), . . . , p(hL). Assume that distillation can boost the training priority by p0 > 0. If there exists paired features whose predicting probability exceeds the boosting probability p0, namely, the set S is not empty:\n\nS = {hi : p(hi) > p0} ̸= φ.\n\nThen UMT helps uni-modal feature learning and can also learn easy-to-learn paired features.\n\n15The probability is taken over the randomness of the testing point\n\n25\n\nUnder review as a conference paper at ICLR 2023\n\nTable 23: Dataset used in Example B.4. + means the feature is larger than zero and − means the feature is less than zero. We denote the predicting probability by p and the rectified probability (due to pushing force) by p′. f1 0.20 0.35(↑) +\n0 +\n+\n\nf2 0.10 0.25(↑) +\n+ +\n-\n\ng2 0.08 0.23(↑) -\n+ +\n+\n\ng1 0.15 0.32(↑) +\n+ -\n+\n\ng3 0.02 0.17(↑) +\n- +\n0\n\nf3 0.05 0.20(↑) +\n0 0\n+\n\np p′ data a data b data c data d\n\nh 0.28 0.28 +\n+ 0\n+\n\ny /\n/ +1 +1 -1 -1\n\nProof. The core of Theorem 3.5 is to clarify the training priority. We revisit the notations of Theorem 3.4 as follows without further clarification. At the end of the training, uni-modal ensemble learn bm1 + bm2 useful features, namely, f1, . . . , fbm1 , g1, . . . , gbm2 . And multi-modal training approaches learn km1 + km2 + kpa features: f1, . . . , fkm1, g1, . . . , gkm2 , h1, . . . , hkpa. We note that there are still many empty features ei in the model due to the initialization.\n\nBy distillation, the model learns the features according to the new priority. Since the set S is not empty, there exists paired features that is learned before the empty features. By distillation, the model would learn all the useful features that appear in uni-modal approaches, as well as those features in set S. Therefore, UMT outperforms uni-modal ensemblewhen there exists useful paired features.\n\nB.4 A CONCRETE EXAMPLE TO ILLUSTRATE THEOREM 3.4\n\nWe next provide a concrete example to better illustrate the Modality Laziness issues. For Example B.4, we aim to show the Modality Laziness issues. For Example B.5, we aim to show the role of the pushing force. Example B.4. Consider modality xm1 with features f1, f2, f3 (corresponding prediction probability p = 0.2, 0.1, 0.05), and modality xm2 with features g1, g2, g3 (corresponding prediction probability p = 0.15, 0.08, 0.02). We show the dataset in Table 23 and aim to minimize the training loss to zero.\n\nIn uni-modal approaches, we learn features f1, f2 and f3 on modality xm1 (similarly, g1, g2, and g3 on modality xm2). Therefore, we learn features f1, f2, f3, g1, g2, g3 in uni-modal ensemble. In multi-modal training approaches without paired feature, we can only learn three features f1, f2, g2 due to the training priority f1 > g1 > f2 > g2 > f3 > g3 (decreasing order in p). This phenomenon is caused by modality laziness.\n\nWe next consider another paired feature h with probability p = 0.28. Under the case, multi-modal training approaches only learn two features h and f1. Therefore, when h is not powerful enough, uni-modal ensemble outperforms multi-modal training approaches. Example B.5. We follow the notations and dataset in Example B.4. By applying the pushing force, assume that each probability of uni-modal feature boosts 0.15, which changes the training priority to f1 > g1 > h > f2 > g2 > f3 > g3 (decreasing order in p′). Therefore, multi-modal training approaches (with pushing force) learns f1, f2, h. As a comparison, multi-modal training approaches (without pushing force) can only learn f1, h. Therefore, pushing force helps learn more features. We additionally remark that we only consider the training error in this example, and there might be other penalties in practice (e.g., distillation loss).\n\n26",
  "translations": [
    "# Summary Of The Paper\n\nThis paper deals with multi-modal joint training methods on deep neural networks.\nRelated to this topic, several works have reported that the best uni-modal networks outperform the multi-modal networks even though the multi-modal networks receive more information.\nThis work is focused on this topic, and the main contributions of this paper are two-fold: (1) to propose an improved methodology with experimental supports on audio-video data for sound classification (VGG-Sound), audio-video data for action recognition (UCF-101, Kinetics-400), front-rear view of object data (ModelNet40), (2) to investigate theoretical results on characteristics among uni-modal features (learned from uni-modal training), and paired features (only learned from cross-modal interaction).\n\n# Strength And Weaknesses\n\n*Strength\n- To identify interesting conceptualization on uni-modal features and paired features\n- To provide experimental evidences (mostly shown in Appendices) to justify their arguments and choices on this work\n- To explain theoretical aspects to characterize modality laziness\n\n*Weaknesses\n- It seems that some terms in the paper are NOT so clear: what is the exact definition of modality laziness? Why should we utilize new term? What are the discriminating points for it? \n- This paper is hard to follow due to lack of explanation for core parts such as model architectures of uni-modal teacher, loss functions with λ_task, λ_distill, λ_distill in Algorithm 1 or the citation of reference works, which also needs for reproducibility.\n- The organization of this manuscript makes readers confusing. It seems that theoretical part (Section 3.4) and the others are totally separated. Uni-modal features and paired features also are redefined in Section 3.4. On the other hand, there is no experimental result related to Section 3.4 in main body of the manuscript (but in Appendices). \n- Even though related work Section is not bad, it would be better to compare one of the recent works [1], which deeply related to this work.\n\n[1] Modality Competition: What Makes Joint Training of Multi-modal Network Fail in Deep Learning? (Provably), ICML 2022\n\n# Clarity, Quality, Novelty And Reproducibility\n\nIf all of details has no problems, and so all of claims are validated, I think that the idea is novel and the contributions are explicit, and this work is worth publishing only considering the achievements and results. \n\nHowever, some parts of details are missing as described Weaknesses above.\nI think that it needs to re-organize and re-write the manuscript with respect to clarity and reproducibility.\n\n# Summary Of The Review\n\nThe main contribution, a method of combination of Uni-modal Ensemble and Uni-modal Teacher, seems practically utilizable and showing better performance for multi-modal joint training on deep neural networks. \nAlso, I think it is interesting idea to leverage distillation for generalized setting of multi-modal joint training. \nHowever, the writing is confusing and not enough clear to be reproducible.\n\n# Correctness\n\n3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.\n\n# Technical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n# Empirical Novelty And Significance\n\n3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
    "# Summary Of The Paper\nThe paper titled \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" addresses the challenges in learning uni-modal features within the context of multi-modal learning, particularly under late-fusion frameworks. The authors introduce two methodologies: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), aimed at enhancing uni-modal feature learning while leveraging cross-modal interactions. Through empirical evaluations across multiple datasets, the study demonstrates that these proposed methods can significantly improve performance compared to traditional late-fusion techniques, particularly emphasizing the importance of uni-modal features for better generalization.\n\n# Strength And Weaknesses\nThe main strengths of the paper include its clear identification of the issue of Modality Laziness and its empirical validation across various datasets, which effectively supports the proposed methods. Additionally, the theoretical underpinning regarding the necessity of uni-modal feature learning adds depth to the contribution. However, the paper could benefit from a more comprehensive exploration of the limitations of the proposed methods and a discussion on their applicability in real-world scenarios. Furthermore, while the experiments cover multiple datasets, a broader range of tasks could strengthen the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe writing is generally clear and well-structured, allowing readers to follow the authors' arguments and methodologies. The presentation of the proposed methods, along with the accompanying algorithms, is straightforward. The novelty lies in the focus on uni-modal feature learning within multi-modal contexts, a relatively underexplored area. The authors have provided sufficient implementation details, hyper-parameter settings, and code availability to ensure the reproducibility of their results, which is commendable.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the field of multi-modal learning by addressing the critical issue of uni-modal feature learning. The proposed methods, UME and UMT, demonstrate effective strategies to mitigate Modality Laziness, supported by empirical evidence across diverse datasets. While the paper is well-executed, a more extensive discussion of potential limitations would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper investigates feature learning in multi-modal data, introducing the concepts of uni-modal features and paired features while critiquing existing late-fusion methods for their inadequacy in capturing uni-modal features—termed \"Modality Laziness.\" The authors propose two innovative methods for enhancing late-fusion learning: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), which are designed to optimize the learning of uni-modal features based on their distribution. Empirical results demonstrate that UMT outperforms complex late-fusion approaches in accuracy across several datasets, while UME shows competitive performance, emphasizing the practical advantages of these methods.\n\n# Strength And Weaknesses\nStrengths of the paper include its novel insights into the concept of Modality Laziness, providing a fresh perspective on multi-modal feature learning. The proposed methodologies (UME and UMT) are practical and straightforward, making them accessible for implementation compared to existing complex methods. Additionally, the comprehensive evaluation across diverse datasets establishes the robustness of the findings. However, limitations include the lack of exploration into the generalizability of the approaches beyond the tested datasets, and while the methods are simpler, they still require a solid understanding of the underlying models, which may not be trivial for all practitioners. Furthermore, the potential risk of overfitting due to reliance on pre-trained models and hyperparameter tuning is a concern.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents the problem, methodology, and results. The novelty of the concepts introduced, particularly Modality Laziness, is a significant contribution to the field. The empirical validation is thorough, and the results are presented in a comprehensible manner. However, the reproducibility could be improved by providing more details on the implementation specifics of UME and UMT, including parameter settings and training protocols.\n\n# Summary Of The Review\nOverall, the paper presents valuable insights into multi-modal feature learning and introduces practical methods that address critical challenges in the field. While the findings are promising, the work would benefit from broader validation across diverse datasets and clearer implementation guidance to enhance reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"On Uni-Modal Feature Learning in Supervised Multi-Modal Learning\" examines the challenges posed by insufficient learning of uni-modal features in multi-modal training, coining the term **Modality Laziness**. It proposes two novel methods—**Uni-Modal Ensemble (UME)** and **Uni-Modal Teacher (UMT)**—to enhance the learning of uni-modal features while still benefiting from paired features. The authors demonstrate that these methods perform comparably to more complex late-fusion approaches across several datasets, including VGG-Sound, Kinetics-400, UCF101, and ModelNet40.\n\n# Strength And Weaknesses\nThe strength of the paper lies in its clear identification of the issue of Modality Laziness and its theoretical underpinning, which is backed by empirical validation. The proposed methods (UME and UMT) are well-justified and show strong performance in various multi-modal tasks. However, a potential weakness is the lack of extensive comparison with a wider range of existing methods beyond late-fusion techniques, which could further substantiate the claimed advantages of UME and UMT.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The methodologies are described comprehensively, and the theoretical aspects are neatly laid out. The reproducibility statement is thorough, detailing necessary implementation steps and providing supplementary materials, which enhances the paper's credibility. The novelty of the concepts introduced, particularly Modality Laziness and the proposed methods, is significant.\n\n# Summary Of The Review\nOverall, this paper provides a valuable contribution to the field of multi-modal learning by addressing an important issue and proposing effective solutions. Its theoretical insights and empirical results are compelling, although broader comparisons with existing methods could strengthen its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" introduces the novel concept of \"Modality Laziness,\" which describes the phenomenon where multi-modal training inadequately captures uni-modal features. The authors propose two frameworks, the Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), aimed at mitigating this issue. Through empirical experiments across several datasets, the proposed methods demonstrate comparable performance to more complex late-fusion techniques, highlighting their potential for simplifying multi-modal training. A theoretical analysis of Modality Laziness grounds the proposed solutions, although the paper calls for broader empirical validation.\n\n# Strength And Weaknesses\nThe paper's strengths include the innovative introduction of Modality Laziness and the practical solutions of UME and UMT, which provide valuable insights into improving uni-modal feature learning. However, the paper lacks extensive empirical validation across diverse datasets, limiting the generalizability of its findings. The theoretical framework, while solid, may be too complex for practitioners, and the absence of detailed analyses on potential scenarios of underperformance is a notable weakness. Additionally, while the reproducibility statement is thorough, clearer access to necessary resources is crucial for practical implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured and presents its ideas clearly. However, the complexity of the theoretical analysis could hinder comprehension for some readers, particularly practitioners. The novelty of the concept of Modality Laziness is compelling, though the empirical validation could be strengthened to enhance the significance of the findings. The reproducibility statement is a positive aspect, although the lack of accessible code and datasets diminishes practical reproducibility.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of multi-modal learning by addressing the challenges of uni-modal feature learning through the introduction of Modality Laziness and the proposed frameworks. However, the findings would benefit from broader empirical validation and clearer practical guidance for implementation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"On Uni-Modal Feature Extraction in Supervised Multi-Modal Learning\" introduces a novel approach to multi-modal learning that emphasizes the extraction of uni-modal features while leveraging paired features through innovative training strategies. The authors propose two key concepts: Uni-Modal Fusion (UMF), which utilizes an adaptive late-fusion strategy based on the importance of uni-modal versus paired features, and Uni-Modal Mentor (UMM), which distills knowledge from pre-trained uni-modal models to enhance feature learning. The empirical findings demonstrate that their methodologies not only improve performance on various datasets (VGG-Sound, Kinetics-400, UCF101, ModelNet40) but also present a more straightforward implementation compared to traditional late-fusion methods.\n\n# Strength And Weaknesses\nStrengths of the paper include the clear identification of the \"Modality Saturation\" phenomenon, which provides a compelling rationale for the proposed methods. The introduction of UMF and UMM offers practical solutions that are empirically validated across multiple datasets, suggesting a meaningful contribution to the field. However, a potential weakness lies in the limited exploration of parameter tuning for the proposed methods, which may hinder reproducibility and broader applicability in diverse real-world situations. Additionally, while the theoretical insights are valuable, the paper could benefit from a more comprehensive discussion of the implications for specific applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presented, making the main ideas accessible to readers. The methodology is detailed, and the empirical validation supports the claims effectively, enhancing the overall quality of the work. In terms of novelty, the focus on uni-modal feature extraction within the multi-modal context represents a significant shift from traditional approaches, marking a noteworthy contribution. However, the reproducibility of the findings could be improved through more extensive parameter exploration and additional validation across a wider range of datasets.\n\n# Summary Of The Review\nOverall, the paper provides a meaningful contribution to multi-modal learning by emphasizing the importance of uni-modal feature extraction and presenting innovative methodologies that improve performance. While the clarity and structure are commendable, further attention to parameter tuning and real-world applicability would enhance the study's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper investigates the role of adversarial training in enhancing the robustness of multi-modal learning systems. The authors propose a novel framework that integrates adversarial examples from various modalities to improve the generalization capabilities of models trained on multi-modal datasets. They introduce the concept of \"Modality Vulnerability,\" providing a theoretical analysis that explains how traditional training methods inadequately address the distinct noise characteristics of different modalities. Empirical results demonstrate that models utilizing the proposed adversarial training techniques outperform those trained with standard methods across several benchmark datasets, including Kinetics-400 and VGG-Sound.\n\n# Strength And Weaknesses\nThe paper has several strengths, including its relevance to contemporary challenges in multi-modal systems, the provision of theoretical insights that clarify existing limitations, and a robust empirical evaluation across diverse datasets. However, it also has weaknesses, such as the complexity of implementing adversarial training, which may hinder practical adoption, especially in resource-limited settings. Additionally, the exploration of hyperparameter sensitivity lacks depth, which could help in understanding the robustness of the proposed training framework.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to readers. The theoretical contributions are of high quality and add to the understanding of multi-modal systems. The novelty lies in the integration of adversarial training specifically tailored for multi-modal data, which is an underexplored area. However, reproducibility could be improved by providing additional details on the implementation and the impact of hyperparameters on performance.\n\n# Summary Of The Review\nOverall, this paper provides a significant contribution to the field of multi-modal learning by demonstrating the effectiveness of adversarial training. The theoretical insights and comprehensive empirical validation strengthen the case for its practical application, although attention to implementation complexity and hyperparameter sensitivity would enhance its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"On Uni-Modal Feature Learning in Supervised Multi-Modal Learning\" introduces a novel approach to transforming multi-modal data features into uni-modal and paired features. The authors highlight the concept of \"Modality Laziness,\" which they argue limits the effectiveness of existing methods. They propose two methods, Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), which reportedly lead to significant improvements in model generalization via a late-fusion training method. Empirical results demonstrate that these methods outperform complex models on several datasets, suggesting that simplicity in model design can yield superior performance in multi-modal learning.\n\n# Strength And Weaknesses\nThe paper makes several significant contributions, notably the introduction of \"Modality Laziness\" and the development of UME and UMT, which may reshape the understanding of feature learning in multi-modal contexts. The empirical results are compelling, showing marked improvements over traditional methods, which supports the authors' claims of the effectiveness of their approach. However, the paper could benefit from a more thorough exploration of the theoretical underpinnings of their claims, particularly regarding the implications of Modality Laziness. Additionally, while the empirical results are promising, further validation across a wider range of datasets and tasks would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its ideas clearly, making it accessible to readers. The quality of the empirical results is commendable, but the novelty of the theoretical contributions could be better substantiated with a more detailed discussion of related work. The reproducibility of the findings is somewhat limited by the lack of detailed methodology descriptions regarding the implementation of UME and UMT, which would be necessary for other researchers to replicate the results reliably.\n\n# Summary Of The Review\nOverall, the paper proposes innovative methods for uni-modal feature learning in multi-modal scenarios, claiming to address critical shortcomings in existing approaches. While the findings are promising and could have significant implications for the field, the paper would benefit from deeper theoretical analysis and clearer methodological details to enhance reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n5/5",
    "# Summary Of The Paper\nThe paper titled \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" addresses the challenges of insufficient uni-modal feature learning in multi-modal systems, attributing this to a phenomenon termed Modality Laziness. The authors propose two novel methods, Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), which aim to enhance the retention of uni-modal features while improving overall model performance. Through empirical evaluations across several datasets, the paper demonstrates that these methods can achieve competitive results compared to more complex fusion strategies, suggesting a potential paradigm shift in multi-modal learning methodologies.\n\n# Strength And Weaknesses\nThe main strength of this paper lies in its identification of Modality Laziness as a critical factor affecting model generalization in multi-modal learning. The proposed methods, UME and UMT, offer practical solutions that are both effective and simpler than existing approaches. The empirical results presented indicate that these methods outperform traditional late-fusion techniques, providing robust evidence for their efficacy. However, a potential weakness is the lack of detailed theoretical underpinnings for the observed benefits of UMT and UME, which may leave some readers questioning the generalizability of the findings beyond the tested scenarios.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its arguments in a clear and structured manner. The methodology is adequately explained, and the empirical results are clearly documented, allowing for easy understanding of the contributions. The novelty of the paper is notable, as it challenges existing paradigms in multi-modal learning and introduces innovative methods that may inspire further research. Additionally, the authors have committed to providing implementation details and code in the supplementary materials, enhancing the reproducibility of their findings.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of multi-modal learning by addressing the issue of uni-modal feature learning through novel methods that demonstrate improved performance. While the empirical results are compelling, further theoretical justification could strengthen the paper's impact. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to multi-modal learning by categorizing features into uni-modal and paired features, positing that the latter can only be effectively learned through multi-modal interactions. It introduces two methods, UME (Uni-modal to Multi-modal Enhancement) and UMT (Uni-modal to Multi-modal Transfer), which aim to address the challenges posed by \"Modality Laziness,\" where models favor simpler features over complex ones. Empirical results suggest that naive fusion methods outperform uni-modal training, indicating potential benefits of joint training, although the applicability of these findings across different datasets remains a concern.\n\n# Strength And Weaknesses\nThe paper makes several valuable contributions, particularly in its exploration of feature categorization and the introduction of UME and UMT methods. However, it suffers from certain weaknesses. The rigid distinction between uni-modal and paired features may overlook overlaps that could enhance learning. The concept of Modality Laziness is intriguing but lacks empirical support to validate its universality. Additionally, the interpretation of empirical results could be biased by dataset characteristics, and the paper does not sufficiently address the role of hyperparameter tuning or the dynamics of training across different modalities.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but some assumptions, such as the effectiveness of uni-modal distillation and the benefits of cross-modal interactions, require clearer justification. The theoretical foundations are based on potentially simplistic assumptions about feature distributions, which could affect the reproducibility of results. While the contributions are novel, the paper could benefit from a more comprehensive evaluation of its methods, including additional metrics beyond accuracy to provide a deeper understanding of performance nuances.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to multi-modal learning with promising methods; however, it is hampered by questionable assumptions and a lack of thorough empirical validation. Addressing the noted weaknesses could strengthen the paper's contributions and enhance its impact in the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper tackles the challenge of insufficient uni-modal feature learning in multi-modal tasks, proposing two innovative late-fusion methods: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT). These methods focus on optimizing model performance by leveraging both uni-modal and paired feature distributions. Through rigorous experiments across various datasets, the study demonstrates that UME and UMT outperform more complex late-fusion techniques, emphasizing the critical role of robust uni-modal feature learning in enhancing model generalization.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear identification of the problem of \"Modality Laziness,\" which is a significant barrier to effective multi-modal learning. The proposed methods are well-motivated and grounded in theoretical insights, offering a fresh perspective to the existing literature on late-fusion strategies. However, the paper could benefit from a more detailed exploration of the limitations of the proposed methods and their applicability to diverse multi-modal scenarios. Additionally, while the results are promising, further benchmarking against more state-of-the-art methods would strengthen the claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making the complex ideas of uni-modal feature learning accessible to readers. The quality of the writing is high, with thorough explanations of concepts and methodologies. The novelty of the proposed methods is noteworthy, as they address a pertinent issue in multi-modal learning. Importantly, the authors provide a comprehensive reproducibility statement that includes detailed implementation instructions and hyper-parameter settings, ensuring that other researchers can replicate the results effectively.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of multi-modal learning by addressing the challenge of uni-modal feature learning and proposing effective late-fusion strategies. While the methodologies and findings are robust, further exploration of the limitations and broader applicability of the approaches would enhance the paper's impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper addresses the challenge of feature learning in multi-modal contexts by proposing a novel framework that enhances the interaction between different modalities while preserving the integrity of uni-modal feature learning. The authors introduce innovative concepts related to the dynamics of uni-modal and paired features, supported by a theoretical analysis that lays a solid foundation for their proposed methods. The findings indicate that their approach outperforms existing techniques on various tasks, suggesting significant potential for advancing research in multi-modal learning.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Relevance:** The topic is timely and aligns well with ongoing advancements in machine learning, particularly in the area of multi-modal learning.\n2. **Novelty:** The introduction of new concepts around feature interaction is a commendable innovation that offers fresh insights into the complexities of multi-modal data.\n3. **Clarity:** The paper is organized effectively, enabling readers to comprehend the proposed methodology and its theoretical underpinnings with ease.\n4. **Theoretical Contributions:** The theoretical framework presented strengthens the credibility of the proposed methods and supports their potential effectiveness.\n5. **Potential Impact:** Should the findings be validated, they have the capacity to significantly influence future research trajectories in the field of multi-modal learning.\n\n**Weaknesses:**\n1. **Empirical Validation:** While the experiments are extensive, they may lack sufficient detail in comparing the new methods against baseline approaches, potentially undermining the perceived efficacy of the proposed framework.\n2. **Generalizability:** The applicability of the results may be limited to specific datasets or tasks, raising concerns about the broader usefulness of the methods in diverse scenarios.\n3. **Complexity:** The methodologies introduced appear complex, which could pose challenges for practitioners lacking a strong background in this domain.\n4. **Hyperparameter Sensitivity:** There are indications that the proposed methods may be sensitive to hyperparameter settings, which could hinder reproducibility and practical application.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly written, making the proposed methodologies easy to follow. The novelty of the concepts introduced is noteworthy, presenting valuable contributions to the field. However, concerns regarding the reproducibility of the results due to potential hyperparameter sensitivity and the need for clearer empirical validation diminish the overall quality of the presented findings.\n\n# Summary Of The Review\nThis paper provides a compelling exploration of feature interaction in multi-modal learning, showcasing innovative methodologies supported by a solid theoretical foundation. While the results are promising, additional empirical validation and more explicit comparisons with existing methods would enhance the paper's impact and applicability.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper \"On Uni-Modal Feature Learning in Supervised Multi-Modal Learning\" addresses the challenge of effectively learning uni-modal features in the context of multi-modal data, where different modalities (like vision, sound, and text) are integrated for improved decision-making. The authors introduce two novel late-fusion methods: the Uni-Modal Ensemble (UME) for tasks requiring both uni-modal and paired features, and the Uni-Modal Teacher (UMT) for tasks where uni-modal features dominate. The findings indicate that these proposed methods enhance the learning of uni-modal representations, thereby mitigating the issues associated with \"Modality Laziness\" and yielding performance comparable to more complex fusion techniques across various datasets.\n\n# Strength And Weaknesses\nThe paper provides significant contributions by identifying the shortcomings of existing late-fusion methods in learning uni-modal features and proposing targeted solutions that are both effective and simpler. The introduction of UME and UMT is innovative and addresses a critical gap in multi-modal learning. However, a potential limitation is the lack of extensive empirical validation across diverse tasks and real-world scenarios, which could strengthen the claims made regarding the generalization of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates the motivation behind the proposed methods, as well as their implementation. The methodology is described in sufficient detail to allow for reproducibility, although additional empirical results and comparisons with state-of-the-art methods would enhance clarity regarding the relative performance. The novelty of the approach is commendable, as it directly tackles the issue of uni-modal feature learning, which has been somewhat overlooked in previous studies.\n\n# Summary Of The Review\nOverall, the paper presents a compelling argument for the necessity of improved uni-modal feature learning in multi-modal systems. The proposed methods, UME and UMT, demonstrate promise in addressing the identified challenges, though further empirical validation would bolster the findings. The clarity of presentation and the innovative nature of the contributions make this work a valuable addition to the field.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" presents novel methodologies for improving the performance of multi-modal models by focusing on uni-modal feature learning. The authors introduce two targeted late-fusion methods: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), which are designed to enhance the generalization capabilities of multi-modal systems. The findings indicate that UMT outperforms more complex models in specific scenarios while UME demonstrates effectiveness when robust uni-modal features are present. The study is supported by theoretical insights into the challenges of modality laziness and extensive empirical evaluations across multiple datasets.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its clear identification of the challenges faced in multi-modal feature learning and the introduction of effective solutions backed by both theoretical and empirical evidence. The analysis of modality laziness provides a compelling rationale for the proposed methods. However, one potential weakness is the reliance on specific datasets, which may limit the generalizability of the findings. Additionally, while the proposed methods show promise, comparisons with state-of-the-art techniques could be more comprehensive to enhance the evaluation of their significance.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, making it accessible to readers. The quality of writing is high, and the theoretical underpinnings are well-explained. The novelty of the proposed methods is significant, as they address a gap in the current understanding of multi-modal learning. The authors have provided sufficient implementation details and hyperparameters, ensuring that the results can be reproduced effectively, which is crucial for validating their claims.\n\n# Summary Of The Review\nOverall, the paper offers a valuable contribution to the field of multi-modal learning by highlighting the importance of uni-modal features and proposing innovative methods to improve model performance. The theoretical insights and empirical results presented are compelling, although further comparisons with existing approaches would strengthen the paper's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents a novel approach to improving uni-modal feature learning within multi-modal tasks, specifically addressing the challenges associated with Modality Laziness. The authors propose two key methodologies: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT), which are designed to optimize feature learning by leveraging paired features from different modalities. Comprehensive experiments on datasets such as VGG-Sound, Kinetics-400, UCF101, and ModelNet40 demonstrate significant improvements in performance metrics, validating the effectiveness of the proposed methods.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear presentation of the methodologies and the thorough experimental validation, including ablation studies that reinforce the findings. The theoretical analysis of Modality Laziness provides a solid foundation for understanding the implications of the proposed methods. However, the paper could benefit from a more detailed discussion of its limitations and the potential impact of hyperparameter choices on the results. Additionally, exploring further datasets could enhance the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and written in clear, accessible language, making it easy to follow the arguments and methodologies presented. The use of tables and figures is effective in conveying results, and the comprehensive list of references situates the work within the broader field of multi-modal learning. Reproducibility is adequately addressed through detailed descriptions of experimental setups and the mention of code availability, although a deeper exploration of the impact of hyperparameters would strengthen this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of multi-modal learning by introducing effective strategies for uni-modal feature enhancement. The methodologies are well-validated through rigorous experimentation, although further discussions on limitations and robustness testing would improve the paper's overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" investigates the shortcomings of existing late-fusion techniques in capturing uni-modal features, which are vital for improving model generalization in multi-modal tasks. The authors propose two innovative methodologies: the Uni-Modal Ensemble (UME), which aggregates predictions from independently trained uni-modal models, and the Uni-Modal Teacher (UMT), which utilizes uni-modal distillation to enhance learning. Empirical results demonstrate that both approaches achieve competitive performance compared to more complex late-fusion methods across multiple datasets, indicating their effectiveness in preserving uni-modal feature integrity while enabling paired feature learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear identification of the limitations of late-fusion methods and the introduction of novel training methodologies that address these issues. The theoretical grounding of the concept of modality laziness provides a strong basis for understanding the observed phenomena. The empirical results support the claims effectively, showing that both UME and UMT can outperform traditional approaches under specific conditions. However, a potential weakness is the reliance on empirical validation across a limited set of datasets, which may not fully capture the generalizability of the proposed methodologies across diverse real-world applications.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and articulates its contributions clearly, with a logical flow from problem identification to proposed solutions and empirical validation. The quality of writing is high, and the methodologies are presented with sufficient detail, allowing for reproducibility. The novel approaches introduced are theoretically sound, but the practical implications may require further exploration in broader contexts beyond the datasets used in the experiments.\n\n# Summary Of The Review\nOverall, the paper makes a significant contribution to the field of multi-modal learning by addressing the critical issue of uni-modal feature acquisition and proposing effective solutions. The methodologies are innovative, empirically validated, and theoretically justified, although their applicability may need further exploration in varied settings. \n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces the concept of \"Modality Laziness\" in multi-modal learning, proposing two methods: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT). The authors claim these methods enhance the learning of uni-modal features in multi-modal tasks. However, the theoretical framework of modality laziness is superficial, and the proposed methodologies appear to be minor modifications of existing techniques. The experiments conducted are limited to a narrow range of datasets, raising questions about the generalizability and practical applicability of the findings.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to address a relevant problem in multi-modal learning. However, the contributions lack depth, with UME and UMT being only slight variations on established practices. The experimental validation is limited in scope, and there is a notable absence of comprehensive ablation studies to isolate the contributions of the proposed methods. Furthermore, the theoretical aspects are overly complex and do not add significant value to the practical application of the findings. The paper also fails to critically evaluate the implications of its claims in real-world settings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper's clarity is undermined by the complexity of its theoretical explanations, which may hinder understanding for practitioners seeking actionable insights. The quality of the experiments is questionable due to the limited dataset selection and lack of thorough analysis. In terms of novelty, the introduction of modality laziness does not significantly advance the field, as proposed methods do not demonstrate substantial improvements over simpler alternatives. Reproducibility may be an issue due to the lack of detailed ablation studies and the dependence on specific datasets.\n\n# Summary Of The Review\nOverall, the paper attempts to tackle an important issue in multi-modal learning but falls short in providing novel contributions and robust empirical evidence. The methodologies proposed are not sufficiently distinct from existing techniques, and the experimental validation lacks depth. The theoretical insights presented do not effectively translate into practical applications, limiting the paper's overall impact.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThis paper presents a novel approach to multi-modal learning by emphasizing the role of uni-modal feature learning, which has been largely overlooked in the field. The authors introduce two innovative methods: the Uni-Modal Ensemble (UME) and the Uni-Modal Teacher (UMT), both aimed at optimizing feature learning for multi-modal tasks. The findings indicate that these methods not only achieve comparable performance to complex late-fusion techniques but do so with greater simplicity and efficiency across various datasets, including VGG-Sound, Kinetics-400, UCF101, and ModelNet40.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its clear distinction between uni-modal and paired features, which enhances understanding of their respective roles in multi-modal systems. The proposed late-fusion methods are innovative and demonstrate significant performance improvements, making them practical for real-world applications. However, one potential weakness is the lack of extensive comparison with other state-of-the-art techniques, which may limit the perceived robustness of the claims regarding their superiority.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and provides a clear explanation of the methodologies and results. The novelty of focusing on uni-modal feature learning in the context of multi-modal tasks is significant, and the practical guidance on selecting between UME and UMT based on feature distribution is particularly insightful. The reproducibility of the results appears to be supported by the thorough empirical evaluation, although more detailed descriptions of the experiments and hyperparameter settings could enhance this aspect.\n\n# Summary Of The Review\nOverall, this paper makes a substantial contribution to the multi-modal learning field by highlighting the importance of uni-modal feature learning and introducing effective methodologies. The results are impressive, showcasing the potential for simplified approaches to achieve strong performance in multi-modal tasks. This work is poised to inspire future research and practical applications.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThis paper explores the theoretical aspects of uni-modal feature learning within the context of multi-modal learning. The authors introduce the concept of Modality Laziness, which describes how excessive focus on paired features can hinder the learning of essential uni-modal features, ultimately impacting model generalization. They propose a framework that includes the Uni-Modal Teacher (UMT) and Uni-Modal Ensemble (UME) to enhance the learning of uni-modal features while still accommodating paired features. The paper provides rigorous theoretical proofs to support its claims and generalizes the findings to a broader multi-modal context, emphasizing the need for targeted learning strategies.\n\n# Strength And Weaknesses\nStrengths of the paper include its strong theoretical foundation, which is supported by clear definitions and rigorous proofs regarding the significance of uni-modal versus paired features in multi-modal learning. The introduction of Modality Laziness as a concept is a novel contribution that sheds light on a common pitfall in multi-modal training. However, the paper's reliance on theoretical constructs over empirical validation may be viewed as a limitation, as it presents empirical results only as supplementary evidence rather than primary validation of the proposed methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its theoretical framework, making it accessible to readers familiar with multi-modal learning. The quality of the theoretical arguments is high, and the novelty lies in the introduction of concepts like Modality Laziness and the proposed learning strategies (UMT and UME). However, the reproducibility of the findings may be challenged due to the limited emphasis on empirical validation, which could leave readers questioning the practical applicability of the theoretical insights.\n\n# Summary Of The Review\nOverall, this paper presents a significant theoretical advancement in the understanding of uni-modal feature learning within multi-modal contexts. While the theoretical contributions are robust and novel, the lack of strong empirical validation may limit the immediate applicability of the proposed methods.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" introduces two key techniques: Uni-Modal Ensemble (UME) and Uni-Modal Teacher (UMT) to enhance uni-modal feature learning within multi-modal contexts. The UME approach combines predictions from uni-modal models to mitigate lazy learning, while UMT distills uni-modal features into multi-modal models for improved learning efficacy. The authors conduct experiments on several datasets, including VGG-Sound and Kinetics-400, and present a comparative analysis against existing methods, demonstrating the effectiveness of their approaches in optimizing uni-modal features.\n\n# Strength And Weaknesses\nThe strengths of this paper lie in its practical contributions to improving uni-modal feature learning through the introduction of UME and UMT, both of which exhibit clear empirical advantages over complex alternatives. The methodology is well-defined, and the experimental setup appears robust, utilizing multiple datasets and comprehensive performance metrics. However, the paper's weaknesses include a lack of theoretical insights or broader conceptual frameworks that could contextualize the findings. Additionally, it could benefit from a more in-depth discussion on the limitations of the proposed methods and potential future work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is commendable, with a well-structured presentation of the methodology and results. The quality of the experiments is high, with sufficient details provided for reproducibility, including code availability and hyperparameter configurations. In terms of novelty, while the techniques proposed are effective, they do not introduce fundamentally new concepts in multi-modal learning but rather refine existing methodologies. The empirical validation is strong, showcasing significant improvements over competing methods.\n\n# Summary Of The Review\nOverall, the paper presents a solid contribution to the field of multi-modal learning by enhancing uni-modal feature extraction techniques. While the empirical results are promising and the methodologies are straightforward, the lack of theoretical depth and broader implications limits the overall impact of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n3/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper addresses the issue of \"Modality Laziness\" in multi-modal training by introducing a method called Uni-Modal Teacher (UMT). The authors claim that UMT offers comparable performance to more complex models while simplifying the training process. They present experimental results on datasets such as VGG-Sound and Kinetics-400, asserting that their approach enhances uni-modal feature learning and outperforms existing methods like AVSlowFast. However, the paper lacks thorough evidence to support these claims and does not provide a robust comparative analysis against the latest advancements in multi-modal learning.\n\n# Strength And Weaknesses\nThe strengths of the paper include a clear focus on a relevant issue in multi-modal training and the proposal of a potentially simpler method (UMT) that could appeal to practitioners interested in reducing complexity. However, the weaknesses are significant: the novelty of the proposed solutions is questionable, as previous works have already attempted to tackle similar problems. The paper also fails to convincingly demonstrate the advantages of UMT over established methods, often relying on limited comparisons and neglecting scenarios where traditional methods excel. The theoretical contributions do not advance the understanding of multi-modal dynamics, and the experimental results appear selectively reported.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear in its presentation, but the novelty of the proposed method is undermined by a lack of rigorous validation against existing approaches. The quality of the experiments is insufficient, as they do not convincingly showcase the claimed benefits of UMT. Reproducibility may be hindered by the absence of comprehensive details regarding the experimental setup and hyperparameter tuning, particularly in relation to the methods being compared.\n\n# Summary Of The Review\nOverall, while the paper presents a commendable intention to simplify multi-modal training, it falls short in providing solid evidence of the advantages of the proposed Uni-Modal Teacher method over existing techniques. The lack of depth in validation and comparative analysis significantly undermines the paper's contributions to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents a novel approach to uni-modal feature learning within the context of supervised multi-modal learning. It introduces a new framework that addresses the challenges associated with modality laziness, where certain modalities contribute less effectively to the learning process. The methodology involves a comprehensive analysis of paired features and their interactions across different modalities, leading to improved accuracy in multi-modal tasks. The findings suggest that the proposed framework enhances feature extraction and representation, leading to significant performance gains on benchmark datasets.\n\n# Strength And Weaknesses\nThe primary strength of this paper is its innovative approach to handling modality laziness, which is a pertinent issue in multi-modal learning. The thorough exploration of paired features offers valuable insights into their roles, and the empirical results demonstrate clear advancements over existing methods. However, the paper has weaknesses related to clarity and consistency in terminology, which could hinder reader comprehension. Additionally, some sections are overly verbose, leading to potential redundancy that detracts from the overall impact of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is compromised by inconsistent terminology, particularly regarding the definitions of \"modality\" and \"paired features.\" The quality of the writing would benefit from a thorough grammar and syntax review to eliminate awkward phrasing. While the proposed methodology is novel and significant, the reproducibility of the results could be enhanced by providing clearer details on experimental setups and datasets. Supplementary materials should succinctly outline what is included to facilitate replication.\n\n# Summary Of The Review\nOverall, the paper makes a meaningful contribution to the field of multi-modal learning by addressing modality laziness and offering a novel framework. However, clarity issues and inconsistent terminology need to be addressed for the paper to reach its full potential. The findings are promising, but improvements in presentation and reproducibility are necessary.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper investigates the phenomenon of Modality Laziness in multi-modal learning, proposing a targeted late-fusion approach to address this issue. It details the methodology employed, which includes experiments on datasets such as VGG-Sound, Kinetics-400, UCF101, and ModelNet40, to demonstrate the effectiveness of the proposed methods. The findings indicate that the targeted late-fusion approach can improve performance in multi-modal tasks, although the authors acknowledge limitations in their dataset choices and the need for broader experimentation.\n\n# Strength And Weaknesses\nThe paper's primary strength lies in its focus on Modality Laziness, a relevant and underexplored aspect of multi-modal learning. The proposed late-fusion approach provides a novel angle on mitigating this issue. However, the weaknesses include a narrow scope of datasets, a lack of discussion on noise effects in multi-modal data, and insufficient exploration of computational efficiency and scalability. The absence of real-world application implications and ethical considerations also detracts from the paper's overall impact.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clear, presenting its contributions in a logical manner. However, the novelty could be enhanced by exploring alternative fusion strategies and architectures. Reproducibility might be a concern given the limited datasets used; broader testing across varied datasets would increase confidence in the findings. The paper would benefit from a deeper discussion of the implications of its findings, including ethical considerations and biases.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to the study of Modality Laziness in multi-modal learning with a focused methodology. However, it suffers from limitations in dataset diversity, lack of discussion on noise and computational efficiency, and minimal engagement with real-world applications and ethical implications. Expanding these areas could significantly enhance the research's relevance and impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents a comprehensive analysis of feature categorization in multi-modal data, distinguishing between uni-modal and paired features. It emphasizes the importance of statistical methodologies in validating claims about model generalization, particularly addressing the concept of \"Modality Laziness.\" The authors provide theoretical foundations supported by statistical proofs, experimental designs using various datasets, and a rigorous approach to performance evaluation through statistical significance testing. Findings indicate that the choice between Unimodal Training (UMT) and Unimodal Ensemble (UME) should be statistically informed, with evidence supporting UMT's superiority in certain contexts.\n\n# Strength And Weaknesses\nStrengths of the paper include its strong theoretical foundation, which is underpinned by well-defined statistical proofs and concepts that contribute to the understanding of multi-modal feature learning. The use of various datasets and robust statistical methods for performance evaluation enhances the credibility of the findings. However, a notable weakness is the reliance on statistical significance tests without a thorough exploration of practical implications in real-world applications. Additionally, the complexity of the theoretical constructs may pose challenges for readers less familiar with advanced statistical methodologies.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear definitions and a logical flow from theoretical concepts to empirical validation. The quality of writing is high, enabling readers to follow the arguments effectively. However, the novelty is somewhat limited by the existing body of literature on statistical approaches in multi-modal learning, although the specific focus on Modality Laziness provides a fresh perspective. Reproducibility is facilitated by the detailed description of methodologies and datasets used, although the paper could benefit from providing code or supplementary materials to further assist in replicating the experiments.\n\n# Summary Of The Review\nOverall, the paper delivers a valuable contribution to the field of multi-modal learning by rigorously applying statistical methodologies to validate claims about model performance. While the theoretical insights are robust, the practical implications could be better articulated, and the novelty could be further enhanced by exploring broader contexts.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces two novel approaches for multi-modal learning: Uni-Modal Teacher (UMT) and Uni-Modal Ensemble (UME), aimed at addressing challenges in late-fusion methods. The authors explore the concept of \"Modality Laziness,\" suggesting that leveraging single modalities can enhance performance when paired modalities are not optimally represented. However, the paper primarily focuses on late-fusion techniques, lacking a thorough examination of early or intermediate fusion methods. While the proposed solutions show promise, the empirical validation across diverse datasets is limited, raising questions about their generalizability.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its introduction of UMT and UME as potential solutions to existing multi-modal challenges, as well as the interesting concept of \"Modality Laziness.\" However, the weaknesses are significant. Crucially, the paper does not provide extensive empirical evidence to validate its claims, particularly regarding the scalability of its methods to more complex datasets. It also fails to address the limitations of UMT and UME in scenarios where paired features are vital but insufficiently represented in training data. Furthermore, the lack of discussion on hyperparameter tuning and computational efficiency may hinder practical applications, and the robustness of the models against variations in data quality is not explored.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with clear explanations of the proposed methods. However, the novelty is somewhat diminished due to the lack of comprehensive discussions on scalability, real-world applicability, and interaction effects between modalities. The reproducibility of the findings could be improved with a more detailed exploration of hyperparameter tuning and empirical validation across a wider range of datasets.\n\n# Summary Of The Review\nOverall, the paper presents interesting ideas in multi-modal learning through UMT and UME, but it falls short in empirical validation and practical considerations. The lack of depth in exploring potential limitations and the need for further experimentation limit its impact and applicability in real-world scenarios.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper titled \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" discusses the challenges associated with learning uni-modal features within the context of multi-modal learning frameworks. The authors introduce concepts such as \"modality laziness\" and propose a strategy to enhance the learning of uni-modal features during multi-modal training. They claim that their approach yields comparable results to more complex methods. However, the foundational ideas presented are largely elementary, emphasizing the well-established understanding that effective multi-modal learning relies on robust uni-modal feature extraction.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its attempt to highlight the importance of uni-modal feature learning in multi-modal systems, a concept that is indeed critical. However, the weaknesses far outweigh this strength. The paper presents no new methodology or significant insights and largely reiterates well-known principles in multi-modal learning. The discussions around \"modality laziness\" and the proposed guidance for feature selection are simplistic and lack depth, reducing the overall impact of the paper.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is clear in its presentation, but the quality of the contributions is lacking. The novelty is minimal, as the paper fails to provide fresh insights or methodologies that advance the field. Reproducibility is not addressed in detail, and the reliance on basic optimization techniques may lead to questions about the generalizability of the findings.\n\n# Summary Of The Review\nOverall, this paper does not contribute significantly to the field of multi-modal learning, as it primarily restates established concepts without offering new methodologies or insights. The findings, while valid, do not present a compelling case for their significance in advancing the understanding of uni-modal feature learning.\n\n# Correctness\n4/5 - The paper's claims are fundamentally correct, but they lack depth and novelty.\n\n# Technical Novelty And Significance\n2/5 - The technical contributions are minimal and do not provide significant advancements over existing knowledge.\n\n# Empirical Novelty And Significance\n2/5 - The empirical findings are largely expected and do not introduce new paradigms or insights into multi-modal learning practices.",
    "# Summary Of The Paper\nThe paper \"ON UNI-MODAL FEATURE LEARNING IN SUPERVISED MULTI-MODAL LEARNING\" introduces the concept of \"Modality Laziness,\" which highlights the inadequacies of existing approaches in leveraging uni-modal feature representations within multi-modal learning contexts. The authors propose two novel methodologies: the Uni-Modal Ensemble (UME) and the Uni-Modal Teacher (UMT), aimed at enhancing the robustness of uni-modal feature learning before engaging in complex multi-modal interactions. Empirical findings indicate that these approaches yield improved performance across various datasets, demonstrating the potential for better generalization when focusing on efficient uni-modal representations.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its insightful critique of the current state of multi-modal learning, particularly the emphasis on the importance of uni-modal feature robustness. The proposed methods (UME and UMT) present a promising alternative to traditional late-fusion techniques, advocating for simplicity and interpretability in model design. However, one weakness is the lack of extensive exploration of knowledge distillation techniques within UMT, which could enhance the model's performance further by facilitating cross-modal interactions. Additionally, while the empirical findings are compelling, a more formalized framework for model selection based on performance metrics would strengthen the overall contribution.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its ideas clearly, making it accessible to a broad audience. The quality of the methodology is high, with rigorous ablation studies that support the findings. The novelty of the concepts introduced, particularly Modality Laziness and the proposed methodologies, is substantial, providing a fresh perspective on multi-modal learning. However, the reproducibility of results could be improved by providing more detailed descriptions of the experimental setups and datasets used.\n\n# Summary Of The Review\nOverall, the paper makes significant contributions to the field of multi-modal learning by emphasizing the importance of uni-modal feature learning and proposing novel methodologies. While it demonstrates strong potential for improved model performance and generalization, there are areas for further exploration, particularly regarding knowledge distillation and model selection frameworks.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces two novel methods, Uni-Modal Teacher (UMT) and Uni-Modal Ensemble (UME), aimed at improving performance in multi-modal learning tasks. Through extensive experiments on several benchmark datasets (VGG-Sound, Kinetics-400, UCF101, ModelNet40), the authors demonstrate that these methods outperform existing late-fusion strategies and naive multi-modal training approaches. Key findings reveal that UMT achieves a top-1 accuracy of 53.5% on VGG-Sound and 76.8% on Kinetics-400, while UME reaches 86.8% on UCF101 and 91.92% on ModelNet40, underscoring the importance of effective feature learning.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its comprehensive empirical evaluation across multiple datasets, clearly demonstrating the effectiveness of the proposed methods compared to existing techniques. The results are compelling, showing consistent improvements in accuracy that establish the superiority of UMT and UME in multi-modal scenarios. However, one weakness could be a lack of in-depth exploration into the underlying mechanisms that enable these improvements, which may leave questions about the generalizability of the methods in other contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and findings. The methodology is described in sufficient detail for replication, and the empirical results are presented in an accessible manner. The novelty of the proposed approaches is noteworthy; however, the reproducibility could be enhanced by providing more detailed descriptions of experimental configurations and hyperparameter settings.\n\n# Summary Of The Review\nOverall, the paper presents a significant contribution to the field of multi-modal learning by introducing effective methods that leverage uni-modal features for improved performance. The results are robust and suggest potential for broad applicability, although further exploration of the methods' underlying mechanisms would strengthen the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThe paper presents a novel approach to integrating multi-modal features in machine learning, specifically focusing on the concept of \"Modality Laziness.\" It proposes a framework that utilizes both uni-modal and paired features to enhance the performance of models in tasks that require understanding multiple modalities. The methodology involves a series of experiments to evaluate the efficacy of the proposed framework against existing benchmarks. The findings indicate significant improvements in model accuracy and robustness, particularly in scenarios where data is sparse or noisy.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative approach to multi-modal integration, which addresses existing gaps in the literature regarding modality interactions. Additionally, the empirical results are promising and suggest practical applicability in real-world scenarios. However, the paper suffers from several weaknesses, including a lack of clarity in its presentation, particularly in defining key terms and concepts. The complex sentence structures and inconsistent formatting detract from the overall readability, potentially hindering understanding among readers unfamiliar with the subject matter.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper introduces a novel concept, the clarity of the writing is notably compromised by complex language and insufficient explanations of technical terms. The quality of the visuals and their integration into the narrative could be improved, as some figures lack clear relevance to the surrounding text. The reproducibility of the results is affirmed through a statement in the paper; however, it would benefit from explicit directions on accessing supplementary materials and code to facilitate independent verification of the results.\n\n# Summary Of The Review\nThis paper contributes valuable insights into multi-modal feature integration in machine learning, but its impact is diminished by clarity and presentation issues. Improvements in the articulation of key concepts and more structured formatting could greatly enhance its accessibility and understanding.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.62832028043311,
    -1.6295221511920106,
    -1.7652242929980373,
    -1.763411331234986,
    -1.816612556192586,
    -1.7637182059228942,
    -1.6679501001056107,
    -1.6235366312533677,
    -1.6695089149472322,
    -1.9907235867896842,
    -1.6821318928468965,
    -1.6506866683639785,
    -1.6939502387548142,
    -1.6436794149876794,
    -1.684832512842529,
    -1.795036828780453,
    -1.6724669208749399,
    -1.765927694199464,
    -1.6135592052918029,
    -1.7328909907583852,
    -1.8990918784555022,
    -1.751009422956445,
    -1.7293948783024375,
    -1.8727325205622456,
    -1.7686411843337113,
    -1.7222244433492315,
    -1.678971778502854,
    -1.6235128765580096,
    -1.8566352571374485
  ],
  "logp_cond": [
    [
      0.0,
      -2.390247662898395,
      -2.384206755818687,
      -2.333422199638018,
      -2.4013217781115297,
      -2.3839181972717154,
      -2.4660642071004086,
      -2.402150466465383,
      -2.4067442817346056,
      -2.381586096916638,
      -2.389470942141115,
      -2.432057097339096,
      -2.3929588597205407,
      -2.3960771143042767,
      -2.3603012434605675,
      -2.389076335739429,
      -2.3984278172293205,
      -2.3768642297496765,
      -2.39331237840919,
      -2.412567166601358,
      -2.4083728028122016,
      -2.455783449078918,
      -2.4098897839506375,
      -2.4086017240743316,
      -2.4128205532011817,
      -2.4465652224411003,
      -2.411846276174697,
      -2.4056680813256928,
      -2.4401712196636427
    ],
    [
      -1.2614109944475296,
      0.0,
      -1.1683655031082492,
      -1.071458044516122,
      -1.1051103820879318,
      -1.222312938007207,
      -1.3174235771192664,
      -1.1734168478719167,
      -1.044580929388531,
      -1.2165994038601644,
      -1.1078120565969327,
      -1.3001600663681059,
      -1.1008335976521197,
      -1.0356054982702287,
      -1.1931704701923362,
      -1.0575386067374029,
      -1.1948388435265682,
      -1.2309745768182565,
      -1.1894245238246806,
      -1.1951720503281018,
      -1.2386493385462005,
      -1.2856249349542435,
      -1.268391896066316,
      -1.283103677697236,
      -1.2140147859869768,
      -1.2552623544871613,
      -1.1214007557186936,
      -1.271551287641214,
      -1.248508816916075
    ],
    [
      -1.440287549358874,
      -1.3366455497140368,
      0.0,
      -1.258576713897007,
      -1.2416708018405078,
      -1.3406575135023273,
      -1.4700432925320577,
      -1.2710032742288435,
      -1.2897008548956566,
      -1.332535775036295,
      -1.2488642544568296,
      -1.3974328868060968,
      -1.275929258057486,
      -1.3305412031151347,
      -1.3758412628728645,
      -1.2763014686522198,
      -1.3487066716086498,
      -1.2953289294729764,
      -1.3506786042564398,
      -1.4192402480102178,
      -1.4038405605550355,
      -1.4469608411509567,
      -1.411075938288304,
      -1.343524655907049,
      -1.3310282025168503,
      -1.4946516690739406,
      -1.3206660120570557,
      -1.4020492782289398,
      -1.4259093048209095
    ],
    [
      -1.3234740583266753,
      -1.2305770151154296,
      -1.2117239226088976,
      0.0,
      -1.112117279692881,
      -1.1898041922556113,
      -1.37262015772491,
      -1.1926831948213228,
      -1.1555030784404448,
      -1.2961248872317563,
      -1.1462833957445158,
      -1.3608992417028898,
      -1.178650578859131,
      -1.208496104186036,
      -1.2114880923005564,
      -1.1778792017551558,
      -1.325195065292882,
      -1.1768363383789677,
      -1.2144654260469032,
      -1.279865988662627,
      -1.3169936470793413,
      -1.3749315366434094,
      -1.2933041028215755,
      -1.2982273602648937,
      -1.2906491612889863,
      -1.3678632393416648,
      -1.2341664980118707,
      -1.2427171783509514,
      -1.3303549786447462
    ],
    [
      -1.4541298669155835,
      -1.2464460706272942,
      -1.328874183291545,
      -1.1993634990570368,
      0.0,
      -1.3674434284645063,
      -1.4616519380455866,
      -1.3131464850924885,
      -1.2227309159451971,
      -1.3881679782736558,
      -1.3209975579695532,
      -1.4252711340312791,
      -1.2869788096820023,
      -1.2951028077285336,
      -1.3658465925551166,
      -1.2710172421960424,
      -1.3378896567863208,
      -1.4054246881705093,
      -1.3206378899057383,
      -1.3695267359137677,
      -1.4025829801357617,
      -1.4426679794105473,
      -1.4310358861238306,
      -1.4141616511022708,
      -1.343732776911112,
      -1.3787478852932824,
      -1.294702181828659,
      -1.4394220395010902,
      -1.4422018031353807
    ],
    [
      -1.4215037363667984,
      -1.338086000493494,
      -1.3905390180023403,
      -1.2716299771768615,
      -1.3747483024432652,
      0.0,
      -1.4823119691711075,
      -1.3995306891274248,
      -1.3538818325918984,
      -1.410492280257351,
      -1.4138763067151217,
      -1.4506781500747739,
      -1.319064857963538,
      -1.3716758971952894,
      -1.4107489305101566,
      -1.3147097291349197,
      -1.421636961383861,
      -1.3155204615880742,
      -1.4341051781407734,
      -1.3913206811948269,
      -1.4578647372091782,
      -1.4772069808536756,
      -1.3841649244047634,
      -1.4210110846723996,
      -1.4170688092253714,
      -1.473649256317587,
      -1.3660002543152614,
      -1.3885099091984456,
      -1.4713511826635248
    ],
    [
      -1.3447153105520167,
      -1.3105448746149222,
      -1.262283781365677,
      -1.2474488081641688,
      -1.281157791936086,
      -1.2300654960304385,
      0.0,
      -1.2955324641553956,
      -1.2874660889351808,
      -1.239702874573324,
      -1.3194789478194788,
      -1.279439078903334,
      -1.2865631582521253,
      -1.284923524432048,
      -1.2693542285348964,
      -1.3100494634491275,
      -1.2598035468122868,
      -1.265836042099388,
      -1.258379581285647,
      -1.3029835554396172,
      -1.2248758346635866,
      -1.3369235132349666,
      -1.2712335420630374,
      -1.2593068718314055,
      -1.2410660036385137,
      -1.3524744671737814,
      -1.2759332557950194,
      -1.253496490128406,
      -1.2958460519523625
    ],
    [
      -1.3148773920779295,
      -1.201386551952721,
      -1.1513250351678121,
      -1.1772804600125841,
      -1.189167976817609,
      -1.2121346419697894,
      -1.3584907027194901,
      0.0,
      -1.1625696976802888,
      -1.2370821379176786,
      -1.2246098993025758,
      -1.3000526031429824,
      -1.1903616168665527,
      -1.202424094870087,
      -1.2511507697361772,
      -1.181024646556172,
      -1.2898234026070061,
      -1.2674482374460938,
      -1.219567681054547,
      -1.2254611999861937,
      -1.3022481108292405,
      -1.2505843349545287,
      -1.3486062269987098,
      -1.2609651164456945,
      -1.2633622548243497,
      -1.2616145231716278,
      -1.1661126651902303,
      -1.291844352694347,
      -1.3024660154011245
    ],
    [
      -1.2922277549133732,
      -1.0788525889285228,
      -1.1384405370236335,
      -1.0881261603137566,
      -1.1002224695214915,
      -1.2252791635088516,
      -1.3294631252032438,
      -1.1015099204571157,
      0.0,
      -1.2526098174554294,
      -1.1250876684123854,
      -1.3088318004013881,
      -1.1103761461992547,
      -1.0748314630833757,
      -1.2409376878119467,
      -1.0719955100017895,
      -1.2324433551001412,
      -1.2283654325822198,
      -1.1858724394742493,
      -1.212298680766243,
      -1.2641508512482766,
      -1.2919740364133165,
      -1.3014613484643205,
      -1.257130191485882,
      -1.2418072794394754,
      -1.250589025231263,
      -1.1180492163195963,
      -1.2281404007879244,
      -1.2589706355952504
    ],
    [
      -1.5953369803933846,
      -1.5790432252491735,
      -1.5602606719178236,
      -1.5710487132430446,
      -1.567033673457502,
      -1.6045529368217462,
      -1.6821900447594447,
      -1.5812734732597624,
      -1.5889147862112833,
      0.0,
      -1.5816975837643146,
      -1.6150553410430004,
      -1.581887274681411,
      -1.6126217981465794,
      -1.6221707403148835,
      -1.5790950177385243,
      -1.5878263032326831,
      -1.6378506304011362,
      -1.553843012041083,
      -1.6891923071612438,
      -1.6267799768051165,
      -1.6123970595277795,
      -1.6354619212476698,
      -1.5570947942606004,
      -1.5655697103191146,
      -1.6837644650152113,
      -1.5838398564108613,
      -1.6655252047157676,
      -1.587851460459406
    ],
    [
      -1.268716269504672,
      -1.146960848413594,
      -1.108007513834303,
      -1.118281913305077,
      -1.1457767667938032,
      -1.23899677524345,
      -1.3537957604315973,
      -1.1367764741439688,
      -1.1424692248217834,
      -1.2236985174549617,
      0.0,
      -1.3351214858044693,
      -1.1516810145752332,
      -1.1098651947285392,
      -1.2194249927622058,
      -1.147721891152917,
      -1.2254741576435757,
      -1.2120240518802128,
      -1.176700100328743,
      -1.2823420509000332,
      -1.2735036133658704,
      -1.298695479579397,
      -1.2809360609369842,
      -1.2131641462047587,
      -1.2310296790407738,
      -1.3234882595926418,
      -1.1695893584894792,
      -1.2813819229261165,
      -1.2698967735331594
    ],
    [
      -1.4263205682681732,
      -1.3800920184630092,
      -1.3657029675638097,
      -1.366769993277022,
      -1.3596896696172496,
      -1.3889828199909093,
      -1.4283296628682653,
      -1.3710364758695628,
      -1.3872587765844633,
      -1.3118469830644424,
      -1.3695413415654185,
      0.0,
      -1.3514579500656803,
      -1.3918427829331685,
      -1.3733027027079894,
      -1.3707932056390393,
      -1.3870919424989248,
      -1.3800198187305284,
      -1.3243494811513232,
      -1.356283445396913,
      -1.3764245339995134,
      -1.3808710953836492,
      -1.416830927077079,
      -1.3828126076284082,
      -1.347837877419784,
      -1.3922071210458866,
      -1.386900348047216,
      -1.3840093728939304,
      -1.4203120042504618
    ],
    [
      -1.3274947717407277,
      -1.2397296445285753,
      -1.1737218436327828,
      -1.180625774094035,
      -1.1768808851866754,
      -1.2427555356846134,
      -1.3593705840655352,
      -1.1871096031591755,
      -1.1686677183529366,
      -1.2486842826051643,
      -1.1903804169840428,
      -1.3469803849924835,
      0.0,
      -1.209389358671313,
      -1.29710935337568,
      -1.172017115692421,
      -1.2969480788211925,
      -1.22104200144091,
      -1.211040035832789,
      -1.2992278117222436,
      -1.2747953096108326,
      -1.3234766987582693,
      -1.3128443637250327,
      -1.276867567492494,
      -1.2266838459519536,
      -1.3213968415216357,
      -1.2402059387260647,
      -1.3551504004052337,
      -1.3104926797981686
    ],
    [
      -1.25047809300434,
      -1.0188311415247218,
      -1.1673239398682973,
      -1.052136966774589,
      -1.0805074973008413,
      -1.1892666441146835,
      -1.3017788744749004,
      -1.1157168954118184,
      -1.0096210240579748,
      -1.1939930216920989,
      -1.0338370652614606,
      -1.2871114054773425,
      -1.0671601177395444,
      0.0,
      -1.2185463964123309,
      -1.033687184922281,
      -1.1838941078888952,
      -1.1947282453509045,
      -1.1642054089391976,
      -1.1624765273076894,
      -1.1890524025627478,
      -1.253779799160941,
      -1.2448494825678533,
      -1.210360502756497,
      -1.181246992386421,
      -1.19470760067259,
      -1.1064937433523752,
      -1.240845174709464,
      -1.2531335034748434
    ],
    [
      -1.2028815894059421,
      -1.171678136741375,
      -1.1959423171590327,
      -1.0725134505413942,
      -1.2209686339976167,
      -1.1520027361876706,
      -1.3204490235839323,
      -1.1813738803846343,
      -1.1995410158000077,
      -1.211257384982117,
      -1.1750848332084964,
      -1.2507651361693364,
      -1.2146333917478769,
      -1.1975765485326213,
      0.0,
      -1.201500887625336,
      -1.2218856566616747,
      -1.1800891572810464,
      -1.2068230965403446,
      -1.1175468285336196,
      -1.2259731358950179,
      -1.2225947223794273,
      -1.1941764982804788,
      -1.2264886679862275,
      -1.2174937709464868,
      -1.2837077852954504,
      -1.2306553452017,
      -1.1819867816580343,
      -1.248860943036079
    ],
    [
      -1.3886370560061052,
      -1.2401853989861233,
      -1.246032534554371,
      -1.1939537668062836,
      -1.2363917751116251,
      -1.302345473559846,
      -1.453579639848245,
      -1.2569988103716816,
      -1.1992127521668985,
      -1.311056677012383,
      -1.2378330175735952,
      -1.3838795019987302,
      -1.2156790844211642,
      -1.2015505175908663,
      -1.3586732602084532,
      0.0,
      -1.326738791629564,
      -1.340677694121321,
      -1.2819668639647384,
      -1.3309223276574078,
      -1.3876877731877284,
      -1.402320167196818,
      -1.4180109824846203,
      -1.3346290243210843,
      -1.3188733838018378,
      -1.403103366921127,
      -1.2659398156160302,
      -1.3503210225323756,
      -1.3828182156079967
    ],
    [
      -1.294154145937154,
      -1.219129789558559,
      -1.2519335223820238,
      -1.197177105464728,
      -1.1861358841656375,
      -1.2598205294564577,
      -1.3814961489445539,
      -1.302090444540676,
      -1.2243704347668438,
      -1.2827611873084883,
      -1.2209089134195614,
      -1.3731122858340006,
      -1.2128370543416265,
      -1.241049604773611,
      -1.289766844299828,
      -1.2073017290702852,
      0.0,
      -1.3133234288243998,
      -1.2191408055387307,
      -1.3173103599950113,
      -1.270667355087957,
      -1.3896008712623895,
      -1.29584829729824,
      -1.2718624104763154,
      -1.2423783085899367,
      -1.3570670585220266,
      -1.2596947917497445,
      -1.319808234309035,
      -1.3482460749000518
    ],
    [
      -1.3439532736197788,
      -1.344636787958473,
      -1.2562734565959386,
      -1.1876034284912957,
      -1.279687152892586,
      -1.1769996031533967,
      -1.39591903268426,
      -1.2883959392075117,
      -1.2900854942660518,
      -1.337276947385759,
      -1.246750240139732,
      -1.3283156632542668,
      -1.258593063381064,
      -1.2400268976229325,
      -1.2956341698866476,
      -1.2831282370709887,
      -1.3632980632766651,
      0.0,
      -1.304816996161378,
      -1.3405915363144612,
      -1.354835893172736,
      -1.4368169307358065,
      -1.3633105481940244,
      -1.3576139150628896,
      -1.316889025964475,
      -1.4342968411701211,
      -1.3430313506533846,
      -1.2286909387190283,
      -1.4085807868816487
    ],
    [
      -1.2881919614874762,
      -1.190477017107291,
      -1.1677818853699204,
      -1.1445488331821647,
      -1.213790395617055,
      -1.205422748823236,
      -1.2997281736669892,
      -1.172671664757421,
      -1.1485594820850629,
      -1.214175216516661,
      -1.1686497500916164,
      -1.2697361306813038,
      -1.1981625480097235,
      -1.1985774732433854,
      -1.2411644950804066,
      -1.140923431980009,
      -1.2249452004843424,
      -1.2211756581777926,
      0.0,
      -1.2679870478209663,
      -1.2356127866040998,
      -1.2926258656146052,
      -1.3146668350874098,
      -1.1778694092697766,
      -1.1955412264506446,
      -1.296669152754809,
      -1.2144085653114898,
      -1.2278128873811505,
      -1.2842323126199564
    ],
    [
      -1.344063463074282,
      -1.2181871370180288,
      -1.3286188786158226,
      -1.2458013930979512,
      -1.2386004963915531,
      -1.2654313750435668,
      -1.3710795080083045,
      -1.2301479140030445,
      -1.2802675196952922,
      -1.3466231790241083,
      -1.289986019838385,
      -1.3504808850196786,
      -1.2939410858566929,
      -1.245523700552449,
      -1.2418053870817656,
      -1.2604029297725736,
      -1.2817861787221705,
      -1.3326820525060226,
      -1.3280484963185557,
      0.0,
      -1.2756449060838209,
      -1.3313877494694373,
      -1.3449831912729227,
      -1.3559876610892672,
      -1.337536751245741,
      -1.2700851390071108,
      -1.2678387074649649,
      -1.340197635733037,
      -1.3828887127511105
    ],
    [
      -1.518087045120098,
      -1.450328494903843,
      -1.4257607667900607,
      -1.3811852688968673,
      -1.4036113861109119,
      -1.4650557488123437,
      -1.495689040371863,
      -1.5166883706929397,
      -1.444499190003329,
      -1.4605780316466288,
      -1.4319358971714713,
      -1.5532448328961317,
      -1.4258909252472973,
      -1.4431445909367138,
      -1.4570705884797968,
      -1.465207320243129,
      -1.3960294957828456,
      -1.4754670939240375,
      -1.4476791716953485,
      -1.4769669477630198,
      0.0,
      -1.5542800371068148,
      -1.4623939892272264,
      -1.5149386224889634,
      -1.433377108048947,
      -1.5061320851277387,
      -1.453687798834967,
      -1.4772146913595685,
      -1.513668523694198
    ],
    [
      -1.3560504764096502,
      -1.3294098609367924,
      -1.3945592927004948,
      -1.2960066528131853,
      -1.3305384497067192,
      -1.3640405756680989,
      -1.4352249611886467,
      -1.3094369843328186,
      -1.3226998942467731,
      -1.317233876275728,
      -1.3751085805717256,
      -1.3443696805797472,
      -1.3221755839881169,
      -1.3393838253844286,
      -1.2907321847598359,
      -1.355314798423393,
      -1.383959547701162,
      -1.3927551052215101,
      -1.343506882017004,
      -1.3276167875100255,
      -1.3533197415283755,
      0.0,
      -1.3847781306521765,
      -1.368724307861004,
      -1.360193359309195,
      -1.3156642979825799,
      -1.3437033240863154,
      -1.4018350315215318,
      -1.2421316189397327
    ],
    [
      -1.3872654229561037,
      -1.3719300145024178,
      -1.391767507412909,
      -1.3065564194507557,
      -1.3814910061496366,
      -1.357018233412411,
      -1.3984413602703512,
      -1.4524336986942878,
      -1.3820512668353655,
      -1.3446786401896145,
      -1.3898273911921162,
      -1.4553316865048591,
      -1.4039031582987076,
      -1.3773185610313998,
      -1.3169399404059847,
      -1.3794682114143517,
      -1.3504376740330182,
      -1.3942740864841752,
      -1.3998689917363603,
      -1.441242150851592,
      -1.362650823224711,
      -1.4195597433149862,
      0.0,
      -1.361024056052607,
      -1.344589587853815,
      -1.4254662037167372,
      -1.3983813087869328,
      -1.347012546088267,
      -1.423219355706643
    ],
    [
      -1.4959863624964689,
      -1.4820018018819385,
      -1.3981283083053286,
      -1.443234348308241,
      -1.428757895136397,
      -1.4609845860749895,
      -1.5013574260266849,
      -1.4324295273097107,
      -1.4502401693056028,
      -1.3786144954265562,
      -1.4375762862492978,
      -1.5411857900229227,
      -1.4705848755710462,
      -1.4559863913712296,
      -1.4873641694137125,
      -1.4402366750949898,
      -1.4306109854464149,
      -1.483750197995071,
      -1.3823222571090426,
      -1.5237044585217607,
      -1.4877887287807692,
      -1.5244448786124873,
      -1.4904801033354622,
      0.0,
      -1.4080542921455692,
      -1.5601546576871488,
      -1.4809681062694158,
      -1.5107855721802557,
      -1.464975382925557
    ],
    [
      -1.3846259198291504,
      -1.318822769937044,
      -1.268370963497543,
      -1.3126512591880939,
      -1.2901228382062138,
      -1.3252705428615408,
      -1.4280088132930742,
      -1.371547357896051,
      -1.350072073340828,
      -1.2934037612464746,
      -1.2672668503535232,
      -1.4475376489515812,
      -1.2800099168306684,
      -1.3396730561192651,
      -1.3829364765664247,
      -1.2881175255630626,
      -1.347462658420765,
      -1.363286761022492,
      -1.3303857631314244,
      -1.466413530043535,
      -1.3934992580739407,
      -1.4563934861381422,
      -1.3709493388780052,
      -1.3546180439997662,
      0.0,
      -1.434676153623982,
      -1.3412750107191842,
      -1.3571505390565648,
      -1.404537128313719
    ],
    [
      -1.3729699928255845,
      -1.2874113214774685,
      -1.3373145951538628,
      -1.2740511561614853,
      -1.2641893473564387,
      -1.3395517666547772,
      -1.3929472574091986,
      -1.2831448019495812,
      -1.2525898804385716,
      -1.333263680899092,
      -1.3250192288669111,
      -1.3455612560307262,
      -1.2972656738202133,
      -1.2464183509581326,
      -1.3393100888802707,
      -1.2840332535698213,
      -1.335550870632017,
      -1.3472074086897046,
      -1.3333400786118175,
      -1.2789218916011216,
      -1.3362337901727752,
      -1.3272875980818963,
      -1.383902552168542,
      -1.3545146512100077,
      -1.3537281309452571,
      0.0,
      -1.2882229155412628,
      -1.3980239553318137,
      -1.3548515568780368
    ],
    [
      -1.3273869067973838,
      -1.1835770598044493,
      -1.2072954692385143,
      -1.1876796805176766,
      -1.1499804176784176,
      -1.2535201435796575,
      -1.3565630049498087,
      -1.1630561436837525,
      -1.1704359013704129,
      -1.285678698385376,
      -1.2025975571242706,
      -1.3521376522472082,
      -1.2147891334193879,
      -1.1617915550417484,
      -1.3002604821248744,
      -1.1678614722699627,
      -1.2691824337420752,
      -1.2805365027778808,
      -1.250260096509101,
      -1.2785692571820841,
      -1.2950861941328353,
      -1.334915670130447,
      -1.3167352627308302,
      -1.2859569568839804,
      -1.2437623940544191,
      -1.3078459594292875,
      0.0,
      -1.3146235323519697,
      -1.308424855743476
    ],
    [
      -1.223234231206252,
      -1.2103745097395,
      -1.1868724571163944,
      -1.116398152843584,
      -1.1901667857004015,
      -1.1058014346279157,
      -1.2805829074036996,
      -1.2313708404475627,
      -1.1674530050478873,
      -1.2316689744416822,
      -1.1895489198677895,
      -1.278409012784891,
      -1.2109086762446326,
      -1.2099238328461444,
      -1.1858559437966443,
      -1.2013215245354998,
      -1.2264630059040658,
      -1.0861364898856685,
      -1.2194952017341048,
      -1.2141055047818041,
      -1.2184480901912496,
      -1.2881716580833322,
      -1.1997245635063405,
      -1.2572506409640292,
      -1.1881968603833832,
      -1.3342597278848365,
      -1.1902708910589965,
      0.0,
      -1.286091319604674
    ],
    [
      -1.4706862527324565,
      -1.4380333668153669,
      -1.3957728897732216,
      -1.3846710951848953,
      -1.401089065789795,
      -1.4531441839372536,
      -1.463082587932592,
      -1.397045238319729,
      -1.4196995213351893,
      -1.3923659506618449,
      -1.3647774668287158,
      -1.4443599996767362,
      -1.424964036182556,
      -1.449326364965241,
      -1.4231326644847375,
      -1.4392016444474782,
      -1.4755190231064437,
      -1.4558103544337189,
      -1.404020131703517,
      -1.4906381235955342,
      -1.452029110009583,
      -1.3260784964926926,
      -1.4793106591718796,
      -1.3491002318848506,
      -1.387765688635179,
      -1.5174859082729202,
      -1.43279230334568,
      -1.4874426553672795,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.2380726175347152,
      0.24411352461442304,
      0.2948980807950923,
      0.22699850232158036,
      0.24440208316139467,
      0.16225607333270142,
      0.22616981396772706,
      0.22157599869850442,
      0.24673418351647225,
      0.23884933829199495,
      0.19626318309401425,
      0.23536142071256938,
      0.23224316612883333,
      0.2680190369725426,
      0.23924394469368115,
      0.22989246320378953,
      0.2514560506834336,
      0.23500790202391997,
      0.21575311383175189,
      0.2199474776209085,
      0.17253683135419218,
      0.2184304964824726,
      0.21971855635877846,
      0.21549972723192834,
      0.18175505799200975,
      0.216474004258413,
      0.2226521991074173,
      0.18814906076946736
    ],
    [
      0.368111156744481,
      0.0,
      0.46115664808376144,
      0.5580641066758887,
      0.5244117691040788,
      0.4072092131848035,
      0.3120985740727442,
      0.4561053033200939,
      0.5849412218034795,
      0.4129227473318462,
      0.5217100945950779,
      0.3293620848239047,
      0.5286885535398909,
      0.5939166529217819,
      0.43635168099967436,
      0.5719835444546078,
      0.4346833076654424,
      0.3985475743737541,
      0.44009762736733005,
      0.4343501008639088,
      0.3908728126458101,
      0.34389721623776714,
      0.3611302551256945,
      0.3464184734947746,
      0.41550736520503384,
      0.3742597967048493,
      0.508121395473317,
      0.35797086355079655,
      0.38101333427593564
    ],
    [
      0.32493674363916325,
      0.4285787432840005,
      0.0,
      0.5066475791010303,
      0.5235534911575295,
      0.42456677949571,
      0.2951810004659796,
      0.4942210187691938,
      0.47552343810238074,
      0.43268851796174235,
      0.5163600385412077,
      0.3677914061919405,
      0.4892950349405514,
      0.4346830898829026,
      0.38938303012517284,
      0.48892282434581746,
      0.4165176213893875,
      0.4698953635250609,
      0.4145456887415975,
      0.34598404498781954,
      0.3613837324430018,
      0.3182634518470806,
      0.35414835470973327,
      0.42169963709098823,
      0.434196090481187,
      0.27057262392409664,
      0.4445582809409816,
      0.3631750147690975,
      0.3393149881771278
    ],
    [
      0.43993727290831064,
      0.5328343161195563,
      0.5516874086260883,
      0.0,
      0.6512940515421048,
      0.5736071389793747,
      0.390791173510076,
      0.5707281364136632,
      0.6079082527945412,
      0.4672864440032296,
      0.6171279354904702,
      0.40251208953209616,
      0.5847607523758549,
      0.5549152270489499,
      0.5519232389344295,
      0.5855321294798301,
      0.43821626594210406,
      0.5865749928560182,
      0.5489459051880827,
      0.48354534257235904,
      0.44641768415564465,
      0.38847979459157655,
      0.47010722841341046,
      0.46518397097009223,
      0.4727621699459996,
      0.3955480918933212,
      0.5292448332231152,
      0.5206941528840345,
      0.4330563525902398
    ],
    [
      0.36248268927700256,
      0.5701664855652919,
      0.48773837290104116,
      0.6172490571355493,
      0.0,
      0.4491691277280798,
      0.35496061814699953,
      0.5034660711000976,
      0.593881640247389,
      0.4284445779189303,
      0.4956149982230329,
      0.39134142216130696,
      0.5296337465105838,
      0.5215097484640525,
      0.45076596363746946,
      0.5455953139965437,
      0.47872289940626533,
      0.41118786802207685,
      0.49597466628684783,
      0.4470858202788184,
      0.41402957605682444,
      0.37394457678203885,
      0.3855766700687555,
      0.40245090509031534,
      0.47287977928147407,
      0.4378646708993037,
      0.5219103743639271,
      0.37719051669149595,
      0.3744107530572054
    ],
    [
      0.3422144695560958,
      0.42563220542940017,
      0.37317918792055393,
      0.49208822874603264,
      0.388969903479629,
      0.0,
      0.28140623675178666,
      0.36418751679546935,
      0.40983637333099576,
      0.35322592566554323,
      0.34984189920777253,
      0.31304005584812034,
      0.4446533479593562,
      0.3920423087276048,
      0.3529692754127376,
      0.4490084767879745,
      0.3420812445390331,
      0.44819774433481996,
      0.32961302778212076,
      0.3723975247280673,
      0.305853468713716,
      0.28651122506921856,
      0.37955328151813084,
      0.34270712125049463,
      0.3466493966975228,
      0.29006894960530727,
      0.3977179516076328,
      0.37520829672444855,
      0.29236702325936936
    ],
    [
      0.3232347895535941,
      0.35740522549068854,
      0.40566631873993364,
      0.4205012919414419,
      0.3867923081695248,
      0.43788460407517227,
      0.0,
      0.37241763595021515,
      0.38048401117042996,
      0.42824722553228667,
      0.3484711522861319,
      0.38851102120227665,
      0.38138694185348543,
      0.3830265756735627,
      0.3985958715707143,
      0.3579006366564832,
      0.40814655329332394,
      0.4021140580062228,
      0.40957051881996365,
      0.36496654466599354,
      0.4430742654420241,
      0.3310265868706441,
      0.39671655804257333,
      0.4086432282742052,
      0.426884096467097,
      0.31547563293182934,
      0.39201684431059136,
      0.4144536099772047,
      0.37210404815324827
    ],
    [
      0.30865923917543814,
      0.42215007930064674,
      0.4722115960855555,
      0.44625617124078354,
      0.43436865443575856,
      0.4114019892835783,
      0.26504592853387754,
      0.0,
      0.46096693357307883,
      0.38645449333568904,
      0.39892673195079187,
      0.32348402811038524,
      0.43317501438681494,
      0.4211125363832806,
      0.37238586151719044,
      0.44251198469719566,
      0.33371322864636155,
      0.35608839380727386,
      0.4039689501988206,
      0.39807543126717393,
      0.32128852042412714,
      0.37295229629883897,
      0.27493040425465787,
      0.36257151480767313,
      0.36017437642901795,
      0.36192210808173986,
      0.4574239660631374,
      0.3316922785590206,
      0.32107061585224317
    ],
    [
      0.37728116003385903,
      0.5906563260187094,
      0.5310683779235987,
      0.5813827546334756,
      0.5692864454257407,
      0.4442297514383806,
      0.34004578974398836,
      0.5679989944901165,
      0.0,
      0.41689909749180276,
      0.5444212465348468,
      0.36067711454584406,
      0.5591327687479775,
      0.5946774518638565,
      0.4285712271352855,
      0.5975134049454427,
      0.43706555984709095,
      0.44114348236501244,
      0.48363647547298294,
      0.45721023418098916,
      0.40535806369895555,
      0.37753487853391565,
      0.3680475664829117,
      0.4123787234613503,
      0.42770163550775675,
      0.41891988971596916,
      0.5514596986276359,
      0.4413685141593078,
      0.41053827935198184
    ],
    [
      0.39538660639629963,
      0.4116803615405107,
      0.4304629148718606,
      0.41967487354663957,
      0.4236899133321821,
      0.38617064996793804,
      0.3085335420302395,
      0.4094501135299218,
      0.40180880057840085,
      0.0,
      0.4090260030253696,
      0.3756682457466838,
      0.4088363121082732,
      0.37810178864310484,
      0.3685528464748007,
      0.41162856905115985,
      0.40289728355700105,
      0.352872956388548,
      0.4368805747486011,
      0.3015312796284404,
      0.36394360998456765,
      0.37832652726190474,
      0.35526166554201444,
      0.4336287925290838,
      0.42515387647056957,
      0.30695912177447293,
      0.40688373037882286,
      0.3251983820739166,
      0.40287212633027814
    ],
    [
      0.41341562334222437,
      0.5351710444333024,
      0.5741243790125934,
      0.5638499795418195,
      0.5363551260530932,
      0.44313511760344637,
      0.32833613241529913,
      0.5453554187029277,
      0.5396626680251131,
      0.45843337539193474,
      0.0,
      0.34701040704242714,
      0.5304508782716633,
      0.5722666981183573,
      0.4627069000846906,
      0.5344100016939795,
      0.45665773520332076,
      0.4701078409666837,
      0.5054317925181535,
      0.3997898419468633,
      0.4086282794810261,
      0.38343641326749944,
      0.40119583190991226,
      0.4689677466421378,
      0.4511022138061227,
      0.35864363325425463,
      0.5125425343574173,
      0.40074996992077994,
      0.412235119313737
    ],
    [
      0.22436610009580527,
      0.27059464990096926,
      0.28498370080016877,
      0.2839166750869564,
      0.2909969987467289,
      0.26170384837306915,
      0.22235700549571313,
      0.2796501924944157,
      0.2634278917795152,
      0.3388396852995361,
      0.28114532679855997,
      0.0,
      0.2992287182982982,
      0.25884388543080994,
      0.2773839656559891,
      0.2798934627249392,
      0.2635947258650537,
      0.2706668496334501,
      0.32633718721265526,
      0.29440322296706545,
      0.2742621343644651,
      0.2698155729803293,
      0.23385574128689957,
      0.2678740607355703,
      0.3028487909441946,
      0.25847954731809186,
      0.2637863203167625,
      0.26667729547004804,
      0.2303746641135167
    ],
    [
      0.3664554670140865,
      0.45422059422623895,
      0.5202283951220315,
      0.5133244646607793,
      0.5170693535681388,
      0.4511947030702008,
      0.33457965468927897,
      0.5068406355956387,
      0.5252825204018776,
      0.4452659561496499,
      0.5035698217707714,
      0.3469698537623307,
      0.0,
      0.4845608800835013,
      0.3968408853791343,
      0.5219331230623931,
      0.39700215993362176,
      0.4729082373139042,
      0.48291020292202513,
      0.39472242703257066,
      0.41915492914398156,
      0.37047353999654487,
      0.38110587502978155,
      0.4170826712623201,
      0.46726639280286064,
      0.3725533972331785,
      0.45374430002874955,
      0.3387998383495805,
      0.38345755895664557
    ],
    [
      0.39320132198333946,
      0.6248482734629577,
      0.47635547511938214,
      0.5915424482130904,
      0.5631719176868382,
      0.4544127708729959,
      0.34190054051277907,
      0.527962519575861,
      0.6340583909297046,
      0.4496863932955806,
      0.6098423497262189,
      0.35656800951033696,
      0.576519297248135,
      0.0,
      0.4251330185753486,
      0.6099922300653984,
      0.4597853070987843,
      0.44895116963677495,
      0.4794740060484819,
      0.4812028876799901,
      0.45462701242493164,
      0.3898996158267385,
      0.3988299324198261,
      0.43331891223118246,
      0.4624324226012584,
      0.44897181431508937,
      0.5371856716353043,
      0.40283424027821546,
      0.390545911512836
    ],
    [
      0.48195092343658685,
      0.5131543761011539,
      0.48889019568349634,
      0.6123190623011348,
      0.4638638788449123,
      0.5328297766548584,
      0.36438348925859665,
      0.5034586324578947,
      0.4852914970425213,
      0.4735751278604119,
      0.5097476796340326,
      0.4340673766731926,
      0.4701991210946521,
      0.4872559643099077,
      0.0,
      0.4833316252171931,
      0.4629468561808543,
      0.5047433555614826,
      0.4780094163021844,
      0.5672856843089094,
      0.4588593769475111,
      0.46223779046310165,
      0.4906560145620502,
      0.45834384485630153,
      0.4673387418960422,
      0.4011247275470786,
      0.45417716764082905,
      0.5028457311844947,
      0.4359715698064499
    ],
    [
      0.4063997727743478,
      0.5548514297943297,
      0.549004294226082,
      0.6010830619741694,
      0.5586450536688279,
      0.4926913552206069,
      0.341457188932208,
      0.5380380184087714,
      0.5958240766135545,
      0.48398015176806997,
      0.5572038112068578,
      0.4111573267817228,
      0.5793577443592888,
      0.5934863111895867,
      0.43636356857199976,
      0.0,
      0.4682980371508889,
      0.454359134659132,
      0.5130699648157147,
      0.4641145011230452,
      0.4073490555927246,
      0.39271666158363505,
      0.3770258462958327,
      0.4604078044593687,
      0.4761634449786152,
      0.391933461859326,
      0.5290970131644228,
      0.4447158062480774,
      0.41221861317245634
    ],
    [
      0.3783127749377859,
      0.45333713131638076,
      0.42053339849291604,
      0.47528981541021187,
      0.48633103670930233,
      0.41264639141848214,
      0.290970771930386,
      0.37037647633426385,
      0.44809648610809605,
      0.38970573356645155,
      0.45155800745537844,
      0.2993546350409393,
      0.4596298665333134,
      0.43141731610132883,
      0.38270007657511185,
      0.46516519180465465,
      0.0,
      0.3591434920505401,
      0.45332611533620915,
      0.35515656087992853,
      0.40179956578698284,
      0.28286604961255035,
      0.37661862357669995,
      0.40060451039862444,
      0.43008861228500317,
      0.3153998623529133,
      0.41277212912519534,
      0.35265868656590493,
      0.3242208459748881
    ],
    [
      0.42197442057968515,
      0.42129090624099086,
      0.5096542376035254,
      0.5783242657081682,
      0.486240541306878,
      0.5889280910460672,
      0.37000866151520384,
      0.4775317549919522,
      0.47584219993341215,
      0.42865074681370485,
      0.519177454059732,
      0.4376120309451972,
      0.5073346308183999,
      0.5259007965765314,
      0.47029352431281635,
      0.4827994571284753,
      0.4026296309227988,
      0.0,
      0.4611106980380859,
      0.4253361578850028,
      0.41109180102672793,
      0.3291107634636574,
      0.4026171460054395,
      0.4083137791365743,
      0.44903866823498895,
      0.3316308530293428,
      0.4228963435460793,
      0.5372367554804356,
      0.35734690731781527
    ],
    [
      0.3253672438043267,
      0.42308218818451193,
      0.4457773199218824,
      0.46901037210963814,
      0.3997688096747478,
      0.4081364564685668,
      0.31383103162481363,
      0.44088754053438195,
      0.46499972320674,
      0.3993839887751418,
      0.44490945520018643,
      0.3438230746104991,
      0.4153966572820793,
      0.4149817320484175,
      0.37239471021139625,
      0.47263577331179385,
      0.38861400480746044,
      0.39238354711401024,
      0.0,
      0.3455721574708366,
      0.37794641868770307,
      0.3209333396771976,
      0.2988923702043931,
      0.4356897960220263,
      0.4180179788411582,
      0.31689005253699376,
      0.3991506399803131,
      0.3857463179106524,
      0.32932689267184645
    ],
    [
      0.3888275276841031,
      0.5147038537403563,
      0.40427211214256253,
      0.4870895976604339,
      0.49429049436683203,
      0.4674596157148183,
      0.3618114827500807,
      0.5027430767553407,
      0.452623471063093,
      0.3862678117342768,
      0.44290497092000014,
      0.3824101057387066,
      0.4389499049016923,
      0.48736729020593605,
      0.4910856036766196,
      0.47248806098581153,
      0.45110481203621466,
      0.4002089382523626,
      0.40484249443982945,
      0.0,
      0.4572460846745643,
      0.4015032412889479,
      0.3879077994854625,
      0.376903329669118,
      0.39535423951264415,
      0.4628058517512743,
      0.4650522832934203,
      0.3926933550253482,
      0.3500022780072747
    ],
    [
      0.38100483333540414,
      0.4487633835516591,
      0.4733311116654415,
      0.5179066095586349,
      0.49548049234459035,
      0.4340361296431585,
      0.4034028380836392,
      0.3824035077625625,
      0.45459268845217315,
      0.4385138468088734,
      0.4671559812840309,
      0.3458470455593705,
      0.4732009532082049,
      0.45594728751878844,
      0.44202128997570544,
      0.43388455821237315,
      0.5030623826726566,
      0.4236247845314647,
      0.45141270676015366,
      0.42212493069248236,
      0.0,
      0.34481184134868736,
      0.4366978892282758,
      0.3841532559665388,
      0.4657147704065552,
      0.39295979332776354,
      0.4454040796205352,
      0.4218771870959337,
      0.3854233547613042
    ],
    [
      0.39495894654679486,
      0.42159956201965265,
      0.3564501302559502,
      0.45500277014325974,
      0.4204709732497258,
      0.38696884728834613,
      0.3157844617677983,
      0.4415724386236264,
      0.42830952870967187,
      0.43377554668071694,
      0.37590084238471944,
      0.40663974237669787,
      0.42883383896832816,
      0.41162559757201644,
      0.46027723819660915,
      0.39569462453305193,
      0.3670498752552831,
      0.3582543177349349,
      0.40750254093944105,
      0.4233926354464195,
      0.3976896814280695,
      0.0,
      0.36623129230426854,
      0.38228511509544094,
      0.39081606364724997,
      0.4353451249738651,
      0.4073060988701296,
      0.34917439143491324,
      0.5088778040167123
    ],
    [
      0.3421294553463339,
      0.35746486380001974,
      0.33762737088952854,
      0.4228384588516818,
      0.34790387215280094,
      0.37237664489002653,
      0.3309535180320864,
      0.2769611796081497,
      0.34734361146707204,
      0.3847162381128231,
      0.33956748711032136,
      0.27406319179757843,
      0.32549172000372995,
      0.3520763172710377,
      0.4124549378964528,
      0.3499266668880858,
      0.37895720426941937,
      0.3351207918182624,
      0.3295258865660773,
      0.2881527274508455,
      0.36674405507772656,
      0.3098351349874513,
      0.0,
      0.36837082224983053,
      0.38480529044862255,
      0.3039286745857004,
      0.33101356951550476,
      0.3823823322141706,
      0.3061755225957945
    ],
    [
      0.3767461580657767,
      0.39073071868030707,
      0.47460421225691696,
      0.42949817225400455,
      0.44397462542584853,
      0.4117479344872561,
      0.3713750945355607,
      0.44030299325253486,
      0.42249235125664275,
      0.49411802513568936,
      0.4351562343129478,
      0.33154673053932293,
      0.40214764499119937,
      0.41674612919101595,
      0.3853683511485331,
      0.43249584546725584,
      0.44212153511583074,
      0.3889823225671747,
      0.49041026345320304,
      0.34902806204048487,
      0.3849437917814764,
      0.34828764194975825,
      0.3822524172267834,
      0.0,
      0.4646782284166764,
      0.3125778628750968,
      0.3917644142928298,
      0.36194694838198993,
      0.40775713763668864
    ],
    [
      0.3840152645045609,
      0.44981841439666725,
      0.5002702208361682,
      0.4559899251456174,
      0.47851834612749755,
      0.44337064147217053,
      0.34063237104063715,
      0.3970938264376602,
      0.4185691109928833,
      0.4752374230872367,
      0.5013743339801882,
      0.3211035353821301,
      0.48863126750304287,
      0.4289681282144462,
      0.3857047077672866,
      0.48052365877064873,
      0.42117852591294636,
      0.4053544233112194,
      0.4382554212022869,
      0.3022276542901763,
      0.3751419262597706,
      0.3122476981955691,
      0.39769184545570613,
      0.41402314033394516,
      0.0,
      0.33396503070972927,
      0.4273661736145271,
      0.4114906452771465,
      0.3641040560199924
    ],
    [
      0.349254450523647,
      0.434813121871763,
      0.3849098481953688,
      0.4481732871877462,
      0.4580350959927928,
      0.3826726766944544,
      0.32927718594003297,
      0.4390796413996503,
      0.46963456291065997,
      0.38896076245013944,
      0.3972052144823204,
      0.37666318731850534,
      0.42495876952901823,
      0.47580609239109894,
      0.3829143544689608,
      0.4381911897794102,
      0.3866735727172146,
      0.37501703465952696,
      0.38888436473741406,
      0.44330255174810995,
      0.3859906531764563,
      0.3949368452673352,
      0.33832189118068956,
      0.36770979213922383,
      0.3684963124039744,
      0.0,
      0.43400152780796875,
      0.32420048801741785,
      0.3673728864711947
    ],
    [
      0.35158487170547015,
      0.4953947186984047,
      0.4716763092643397,
      0.49129209798517737,
      0.5289913608244363,
      0.4254516349231965,
      0.3224087735530452,
      0.5159156348191014,
      0.5085358771324411,
      0.3932930801174779,
      0.4763742213785833,
      0.3268341262556458,
      0.46418264508346607,
      0.5171802234611056,
      0.3787112963779795,
      0.5111103062328912,
      0.4097893447607788,
      0.39843527572497317,
      0.42871168199375287,
      0.4004025213207698,
      0.38388558437001863,
      0.34405610837240697,
      0.36223651577202376,
      0.3930148216188736,
      0.4352093844484348,
      0.37112581907356645,
      0.0,
      0.36434824615088424,
      0.37054692275937806
    ],
    [
      0.4002786453517575,
      0.4131383668185096,
      0.43664041944161514,
      0.5071147237144256,
      0.4333460908576081,
      0.5177114419300939,
      0.34292996915430995,
      0.39214203611044685,
      0.4560598715101223,
      0.39184390211632736,
      0.43396395669022003,
      0.3451038637731185,
      0.41260420031337697,
      0.4135890437118652,
      0.4376569327613653,
      0.4221913520225098,
      0.3970498706539438,
      0.5373763866723411,
      0.4040176748239048,
      0.40940737177620545,
      0.40506478636676,
      0.33534121847467735,
      0.4237883130516691,
      0.3662622355939804,
      0.43531601617462634,
      0.2892531486731731,
      0.43324198549901305,
      0.0,
      0.33742155695333564
    ],
    [
      0.385949004404992,
      0.4186018903220816,
      0.4608623673642269,
      0.4719641619525532,
      0.4555461913476535,
      0.40349107320019484,
      0.39355266920485654,
      0.45959001881771955,
      0.4369357358022592,
      0.4642693064756036,
      0.49185779030873267,
      0.41227525746071225,
      0.4316712209548925,
      0.40730889217220745,
      0.43350259265271096,
      0.41743361268997026,
      0.3811162340310048,
      0.4008249027037296,
      0.45261512543393145,
      0.3659971335419143,
      0.40460614712786547,
      0.5305567606447559,
      0.37732459796556883,
      0.5075350252525979,
      0.46886956850226946,
      0.3391493488645283,
      0.4238429537917685,
      0.369192601770169,
      0.0
    ]
  ],
  "row_avgs": [
    0.22508835388409748,
    0.4376394098085904,
    0.4123781296082672,
    0.5093436554637348,
    0.4605446039035256,
    0.3657579166946055,
    0.38770421982574504,
    0.38053511881071955,
    0.46915017544209947,
    0.3868243381254145,
    0.46479188222574214,
    0.2728681507210563,
    0.4371256370914934,
    0.479401923588835,
    0.48031639299385126,
    0.48182187537834503,
    0.395717148702873,
    0.4514258294167031,
    0.39155534260334696,
    0.4328899888384687,
    0.4330271261920344,
    0.40456392965941773,
    0.34496098378204054,
    0.406564351669243,
    0.41260241843720913,
    0.3984091914807891,
    0.42288212157780786,
    0.4117805493211179,
    0.42737293517005254
  ],
  "col_avgs": [
    0.3717298904543312,
    0.44902699456584916,
    0.44740999457360753,
    0.48972471761697983,
    0.4638701892113351,
    0.43105737458165866,
    0.32608113489373997,
    0.44295179096497855,
    0.46301817620131264,
    0.4175507255141853,
    0.45282015631608924,
    0.35527500542232165,
    0.45441832044636216,
    0.45148418326092454,
    0.4089654281095974,
    0.46049791320355615,
    0.40569886578849557,
    0.41123361747407816,
    0.4319317457204919,
    0.39116137380739074,
    0.38740001767515503,
    0.35237641508028644,
    0.371326872271487,
    0.3973664852496932,
    0.4189630590738887,
    0.34964585527870956,
    0.4346484387624408,
    0.38354195233117505,
    0.36386700656710586
  ],
  "combined_avgs": [
    0.29840912216921434,
    0.4433332021872198,
    0.4298940620909374,
    0.4995341865403573,
    0.46220739655743037,
    0.3984076456381321,
    0.3568926773597425,
    0.411743454887849,
    0.466084175821706,
    0.40218753181979994,
    0.4588060192709157,
    0.31407157807168895,
    0.44577197876892777,
    0.4654430534248798,
    0.44464091055172433,
    0.4711598942909506,
    0.4007080072456843,
    0.43132972344539067,
    0.41174354416191944,
    0.4120256813229297,
    0.4102135719335947,
    0.3784701723698521,
    0.35814392802676376,
    0.4019654184594681,
    0.41578273875554894,
    0.3740275233797493,
    0.42876528017012433,
    0.39766125082614645,
    0.3956199708685792
  ],
  "gppm": [
    592.9460478706267,
    597.8668362050148,
    595.9009577241552,
    579.9503596454218,
    589.1223202784104,
    602.8373811242217,
    653.9492064490213,
    600.4083515573124,
    591.7141504605042,
    607.1075278449532,
    597.032166717942,
    633.464352810203,
    594.803645884711,
    597.2963281818448,
    616.8219638212947,
    590.7886565326938,
    617.0052332153795,
    614.4596966949272,
    607.03410164121,
    622.3097292837912,
    621.9573422100062,
    641.1494479425975,
    633.0081135969428,
    617.9729424760759,
    609.9311193476309,
    640.4114076262794,
    603.2985220747171,
    628.6005506072686,
    635.5644683483237
  ],
  "gppm_normalized": [
    1.367420625183173,
    1.3443215260258083,
    1.340131353775725,
    1.2989702942444006,
    1.3188089859502197,
    1.3555955875902568,
    1.474940309812905,
    1.3476941263556845,
    1.324983329768879,
    1.3651669875225199,
    1.340231802546321,
    1.4289230330184297,
    1.335377849745619,
    1.3381303152160466,
    1.3849437542147085,
    1.3277830755034512,
    1.384334946275451,
    1.374195463424576,
    1.3604561652255696,
    1.4023203423900608,
    1.3876178409792608,
    1.4399932651351304,
    1.4158660701321477,
    1.3885096027116568,
    1.3662098166148093,
    1.4390005189382706,
    1.3538730838285564,
    1.407689568018144,
    1.4281261702072634
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398,
    556,
    436,
    455,
    520,
    439,
    403,
    443,
    430,
    421,
    442,
    402,
    469,
    440,
    412,
    457,
    437,
    417,
    397,
    430,
    478,
    440,
    351,
    389,
    390,
    417,
    413,
    412,
    418,
    373,
    589,
    405,
    404,
    436,
    416,
    426,
    434,
    447,
    434,
    406,
    406,
    524,
    449,
    391,
    456,
    425,
    488,
    400,
    372,
    431,
    460,
    423,
    389,
    472,
    413,
    395,
    393,
    447,
    357,
    756,
    505,
    437,
    451,
    455,
    446,
    567,
    465,
    461,
    428,
    451,
    350,
    461,
    430,
    486,
    497,
    420,
    414,
    408,
    431,
    401,
    373,
    409,
    440,
    446,
    377,
    441,
    425,
    432,
    719,
    452,
    465,
    403,
    440,
    476,
    406,
    461,
    445,
    427,
    419,
    561,
    443,
    439,
    402,
    436,
    435,
    404,
    421,
    436,
    431,
    405,
    404,
    434,
    436,
    444,
    457,
    417,
    387
  ],
  "response_lengths": [
    3494,
    2509,
    2540,
    2081,
    2427,
    2666,
    2380,
    2524,
    2458,
    2406,
    2343,
    3189,
    2439,
    2418,
    2235,
    2487,
    2423,
    2238,
    2350,
    2381,
    2376,
    2320,
    2209,
    2574,
    2375,
    2400,
    2564,
    2148,
    2278
  ]
}