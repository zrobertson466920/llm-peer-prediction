{
  "example_idx": 89,
  "reference": "Under review as a conference paper at ICLR 2023\n\nGEOVEX: GEOSPATIAL VECTORS WITH HEXAGONAL CONVOLUTIONAL AUTOENCODERS\n\nAnonymous authors Paper under double-blind review\n\nABSTRACT\n\nWe introduce a new geospatial representation model called GeoVeX to learn global vectors for all geographical locations on Earth land cover. GeoVeX is built on a novel model architecture named Hexagonal Convolutional Autoencoders (HCAE) combined with a Zero-Inflated Poisson (ZIP) reconstruction layer, applied to a grid of Uber’s H3 hexagons, each one described by the histogram of OpenStreetMap (OSM) geographical tags occurrences. GeoVeX is novel on two aspects: first, it produces pre-trained task-agnostic geospatial vectors with H3 and OSM that are, for the first time, contextualized on the neighboring hexagons features, by leveraging an hexagonal convolutional autoencoder applied on an H3/OSM grid centered on the location to embed; secondly, it introduces a zeroinflated Poisson autoencoder reconstruction layer, to adapt a standard autoencoder network to train on sparse geographical count data distributed on an hexagonal grid. Experiments demonstrate that GeoVeX embeddings improve upon two stateof-the-art geospatial location representations models, Hex2Vec and Space2Vec, on two different downstream tasks: worldwide listings price prediction in the travel industry, and hyperlocal interpolation of climate data from weather stations. The qualitative analysis of the latent representation structures learnt by GeoVeX showcases the higher quality of the geographical structures learnt by the geographically contextualized embeddings learnt by GeoVeX.\n\n1\n\nINTRODUCTION\n\nEntity embedding is ubiquitous in a variety of Machine Learning tasks thanks to its many advantages: it captures the semantics of each entity in the context of a given domain; it enables transfer learning to different related tasks; it reduces the sparsity of the entity representation and compresses the feature space. In NLP domain, global word embedding models, such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019) have been successful at capturing the word semantics of big open-source vocabularies (e.g. Wikipedia, Gigaword) and are used to transfer learning to multiple downstream tasks, such as sentiment analysis (Tang et al., 2014; Deho et al., 2018; Alamoudi & Alghamdi, 2021), question retrieval (Zhou et al., 2015), and medical semantics (Wang et al., 2018). Similar approaches inspired by NLP have been since then proved to be useful in many industrial domains, where multiple models have been proposed for learning the latent representations of entities specific to an industry, such as Product2Vec (Biswas et al., 2017) and User2Vec (Hallac et al., 2019) in e-commerce, or Wave2Vec (Baevski et al., 2020) in speech representation, just to name a few.\n\nIn comparison, in the field of Geographic Information Science (GIS), a global set of task-agnostic embeddings for geographical space representation can benefit multiple domains and use cases, such as: price prediction for houses (Wang et al., 2021), hotel rooms (Kisilevich et al., 2013), and vacation homes (Islam et al., 2022; Pradip & Suthar, 2022); interpolation of climate variables such as temperature and pressure (Wu & Li, 2013); computer vision tasks with geo-located images (Berg et al., 2014). These tasks, just to name a few, have in common the application of some transformations to the spatial coordinates, but they do not leverage the spatial distribution of geo entities (such as parks, water, beach, buildings, streets, bars, etc.), which convey a more rich information of the geographical context. Besides, in terms of modelling, previous approaches to learn geospatial embeddings have a set of limitations, such as being non-contextual, task-specific and/or region-specific (Sec. 2) that we address with a novel model architecture and loss function formulation (Sec. 3.6).\n\n1\n\nUnder review as a conference paper at ICLR 2023\n\n[Without GeoVeX]\n\nTask Features\n\nTask Features\n\n[With GeoVeX]\n\nlat/lng\n\nLook-up Pre-Trained GeoVeX Embeddings\n\nTask Features + GeoVeX Embedding\n\nTask Model\n\nTask Model\n\nFigure 1: Workflow to use GeoVeX embeddings in downstream tasks: the task features are expanded by simply concatenating the GeoVeX embedding associated to the H3 hexagon corresponding to the latitude (lat) and longitude (lng) coordinates of each task item, without retraining the embeddings.\n\nOur new approach to learn the geospatial embedding of each location on Earth, based on nearby geographical entities, aims to pre-train a finite set of embeddings that can cover the whole Earth, so that they can be stored and used with ease as extra features in any Machine Learning task where entities have latitude and longitude coordinates. The general workflow is summarized in Fig. 1. To achieve the goal of wide adoption to multiple downstream tasks, we leverage Uber Hexagonal Hierarchical Spatial Index grid system named H31 to spatially index the data coordinates into small regions of approximately the same size, since H3 minimizes map distortion. A pair of coordinates (i, j) is thus represented by a unique H3 hexagonal id for which we learn a GeoVeX embedding.\n\nTo learn GeoVeX embeddings that have a geographical semantic, we associate each H3 hexagon to the geographical tags of the entities obtained from OpenStreetMap (OSM)2. OSM is a project that creates and distributes free worldwide geographic data, and, as of January 2022, has ≈7 billion nodes and ≈4 million map changes per day. This makes it the equivalent of Wikipedia for word embeddings: a massive, scalable, and information-rich global open dataset for creating and updating global embeddings. In particular, OSM contains nodes, ways and relations, which together can be transformed to points, lines and polygons, each one characterized by a set of semantic tags, such as amenity:bar, highway:motorway, natural:forest. By intersecting these OSM entities with the H3 hexagons, and by using a Bag-Of-Words (BOW) model on their tags, each hexagon can be sparsely described by a K-dimensional histogram vector, where K is the size of a subset of the vocabulary of OSM geographical tags, where each element represents the number of times an entity with the respective tag is contained or intersects the hexagon itself. This information needs then to be properly aggregated to produce an embedding while, at the same time, taking into account the information from neighboring hexagons, which provide the geographical context. This concept follows the first law of geography: everything is related to everything else, but near things are more related than distant things (Tobler, 1970). The convolution operation, borrowed from Computer Vision, presents in this domain challenges to address: 1) not square, but hexagonal grids, and 2) different distribution of each ”channel”: not dense pixel values, but highly sparse counts. GeoVeX model aims to bridge the gap of convolutional neural networks usage on these hexagonal grids described by sparse counts.\n\nIn summary, the contributions of our work are:\n\n1. the GeoVeX architecture design to learn task-agnostic pre-trained location embeddings with H3 and OSM that are for the first time contextualized on the neighboring hexagons. We demonstrate their expressive power qualitatively, by using an analysis of the cosine similarities (Sec. 4.1), and quantitatively, by adding the embeddings to the feature set of two downstream tasks: price prediction in the worldwide travel industry (Sec. 4.2) and temperature interpolation of climate data from weather stations (Sec. A.6);\n\n2. the novel Zero-Inflated Poisson (ZIP) autencoder’s probabilistic decoder block, which is trained with a spatial contextual loss function, to adapt the standard reconstruction layer of autoencoders network to the case of zero-inflated spatial contextual count data produced by the H3 grid and the OSM entity tags counts (Sec. 3.6).\n\n1https://eng.uber.com/h3 2https://www.openstreetmap.org/\n\n2\n\nUnder review as a conference paper at ICLR 2023\n\n2 RELATED WORK\n\nGlobal geospatial embeddings Hex2Vec (Wo ́zniak & Szyma ́nski, 2021) and Space2Vec (Mai et al., 2020) are the closest approaches to our work, since they create pre-trained location-specific embeddings, which can add geospatial information to any downstream task without any task-specific retraining nor any manual location imputation logic.\n\nHex2Vec (Wo ́zniak & Szyma ́nski, 2021) is a state-of-the-art work that uses H3 hexagons described by OSM entities, and develops an approach to learn geospatial embeddings. GeoVeX follows a similar approach to create the training setup (H3 as grid methodology, and OSM as provider of geographical entities), but it is different in terms of both model design and loss function formulation. In fact, Hex2Vec embeddings are not contextualized, since the embedding of an H3 hexagon does not consider any information of the surrounding hexagons (e.g. an hexagon described with just the tag ”street” is always embedded equally worldwide, without any distinction if this hexagon is surrounded by others with tags of ”offices” or ”ski-lifts” or ”beaches”). Regarding the loss, the authors proposed a skip-gram model with negative sampling, paired with a triplet loss function: for each hexagon to embed (the triplet anchor), the positive hexagon is taken from the first adjacent ring of hexagons, and the negative hexagon is sampled from at least 3 rings away from it. It is easy to see that imposition of complete similarity between the hexagon to embed (the anchor) and its adjacent ones (the positives) in the loss formulation is not always appropriate since it imposes perfect similarity of a beach-like hexagon’s embedding with a town-like one in case they are adjacent. Finally, the authors applied the approach only to 54 cities, and without a quantitative analysis of the improvements on down-stream tasks.\n\nSpace2Vec (Mai et al., 2020) is one of the state-of-the-art methods proposed to encode a pairs of coordinates in a high dimensional embedding which can be used in downstream tasks. In particular it uses trigonometric functions with different frequencies to encode a given position in space instead of using the simple pairs of coordinates in a modeling task. The approach can actually be considered as a worldwide embedding model in the sense that any coordinate on Earth can be transposed to a latent representation. However, the embeddings are not learnt based on the geographical entities (e.g. buildings, streets, bars, etc.), so Space2Vec vectors cannot be really considered as pre-trained embeddings aiming at learning geographic characteristics of a location, in order to draw similarities of different regions on Earth.\n\nLocal or task-specific geospatial embeddings Other approaches to learn geospatial embeddings in GIS domain are trained for specific tasks and/or specific regions, so they cannot be used as pretrained geospatial embeddings without task-specific retraining and/or without manual imputation logic. Region2Vec (Xiang, 2020) combines Point Of Interest (POI) and mobile sensors data obtained in a specific region in China; Zone2Vec (Du et al., 2018) uses trajectories generated by taxis in Beijing. Urban2Vec (Wang et al., 2020b) uses Street View images and focuses only on the urban use cases, where such images are available. RegionEncoder (Jenkins et al., 2019) combines mobility from taxis, categories of POI and satellite images to create a model specific to two urban regions. Tile2Vec (Jean et al., 2019) applies an unsupervised representation learning on satellite images, while using the triplet loss formulation, which leads to the same issues described for Hex2Vec. In (Islam et al., 2022), the authors apply Moran Eigenvector Spatial Filtering (MESF) for the specific task of predicting AirBnB listing price in San Jose County in US. In (Mac Aodha et al., 2019), the location encoder is specifically learnt from photos to improve specific image-classification tasks, and learnt embeddings cover only the locations in the task-specific training dataset. In (Sheehan et al., 2019), the authors learn task-specific location embeddings from geolocated Wikipedia articles, which cover a limited set of coordinates, thus requiring manual imputation logic for unknown locations.\n\nConvolutional Autoencoders In terms of network architecture, our design is inspired by recent works on Convolutional Autoencoders (CAE), among which the deep CAE architecture of Guo et al. (2017) represents an example of the state of the art. In addition, (Hahner & Garcke, 2022) recently introduced the use of hexagonal convolutional autoencoders. However, these architectures are designed to work with images, and not with tensors that represent zero-inflated count variables, so we propose some modifications to account for this.\n\n3\n\nUnder review as a conference paper at ICLR 2023\n\n3 PROPOSED APPROACH: GEOVEX\n\nIn this section, first, we formalize the problem of training geospatial embeddings of a pair of coordinates on Earth, then we present the proposed GeoVeX model architecture, its input format and its novel specific components: hexagonal convolutions, the zero-inflated Poisson reconstruction layer for the autoencoder, and the loss function used to train the network.\n\n3.1 PROBLEM FORMALIZATION\n\nGiven a pair of latitude and longitude coordinates on Earth p = (x, y) ∈ R2, define a function Fθ(p) : R2 → RD parameterized by θ which maps the point p to a D-dimensional embedding e = Fθ(p), which represents the geographical characteristics of the space nearby the given point proportionally to the distance to it, in accordance to Tobler (1970).\n\nIn this work, we propose to formulate the F function as Fθ(p) = fθ(t(p), Nn′(t(p))), where fθ : RK × Rn′×K → RD is the GeoVeX encoder, and t : R2 → RK is the function which assigns the vector of counts z = t(p) of K geographical tags of the entities that intersect the hexagon where the point p resides, and Nn′(t(p)) is the set of count vectors of n′ neighbor hexagons of p. The GeoVeX pre-trained embedding e can later be concatenated to existing features of any downstream task involving entities described by latitude and longitude coordinates. This effectively serves to expand the features set used by the model addressing the machine learning task to solve.\n\n3.2 GEOVEX MODEL 3D TENSOR INPUT FROM 2D HEXAGONAL CELLS\n\nIn this section we briefly describe the GeoVeX 3D input tensor required by the network architecture. GeoVeX uses an hexagonal tiling system, which is the state-of-the-art representation of a geographical grid, because it guarantees the isotropy of local neighbourhoods (Wang et al., 2020a). By analogy with image domain, in GeoVeX the first 2 dimensions represent the 2D coordinates of the hexagon in a spatial coordinate system and the 3rd dimension describes the count of K OSM tags of the entities intersecting a given H3 cell, following the Bag-Of-Words (BOW) model. After cleaning and preprocessing, we cut the number of tags to the top K = 1024 most frequent ones with worldwide minimum coverage, since the vocabulary presents a long tail distribution. More details can be found in Section A.3 of Appendix.\n\nEach hexagon’s neighborhood is defined by the r-rings neighboring cells as described in Fig. 2 for r = 7 rings around the point (0, 0) (r = 7 has been experimentally defined to fit our GPU memory constraints). The importance of each ring decays with distance to center, in alignment with the first law of geography (Tobler, 1970). This is implemented using a distance weighting kernel in the model loss function formulation in Sec. 3.6, so that the representation of an hexagon is influenced by neighboring hexagons proportionally to the hexagonal distance to it, making them contextualized. This is not the case for Hex2Vec model design, instead.\n\nTransposition of the 2D hexagonal grid to 2D matricial form, for the first 2 dimensions of the 3Dtensor, is done via axial coordinate system representation (see Luo et al. (2019) for more details on different transposition methods), and is depicted in Fig. 3. The final 3D representation is (2r + 1) × (2r +1)×K. To enable correct convolutions within this network design, the dimensions of the input matrix are required to be a power of 2, which can be achieved through padding. Since we trained the network with r = 7 the input of the GeoVeX model is a 16 × 16 × 1024 3D tensor.\n\n3.3 MODEL ARCHITECTURE\n\nThe architecture of GeoVeX network is summarized in Fig. 4. The base design is the one of a Convolutional AutoEncoders (CAE) network (Guo et al., 2017), which is composed of two main components: the encoder h = f (z) and the decoder z′ = g(h), where z is the input tensor, h is its latent representation learnt by applying the encoder block, and z′ is the reconstructed tensor created by the decoder block. The parameters of the CAE network are updated by minimizing the reconstruction error described in Sec. 3.6.\n\nThe encoder is a set of convolutional blocks stacked on the 3D input tensor to extract hierarchical features. The output units of the convolutions are flattened to form a vector, and are passed to a\n\n4\n\nUnder review as a conference paper at ICLR 2023\n\nFigure 2: The hexagonal grid of the r-rings centered on the hexagon to embed, with the axial coordinates (i, j) assigned to each hexagon based on its position in the grid.\n\nFigure 3: The transposed hexagonal grid represented as a 3D matrix, using the axial coordinates transposition and assigning the OSM counts as 1024 tensor channels. In grey, the padding cells that are not valid.\n\nfully connected layer with D units, the final dimension of GeoVeX embedding. In practice, we have found that D = 32 is a good trade-off between expressiveness and cost to store 200+ millions vectors worldwide. The input tensor is thus transformed by the encoder into a 32-unit latent embedding representation h, forcing the autoencoder to capture the most salient features that minimize the loss function. The decoder is defined as a set of convolutional transposed layers to transform embedded feature back to original input, followed by a novel Zero-Inflated Poisson layer of our contribution to reconstruct the count tensor in input (Sec. 3.5). Batch Normalization (Ioffe & Szegedy, 2015) is used to re-center and re-scale the counts tensor in input and output of every convolution operation. The usage of Batch Normalization and ReLU is aligned to (Ioffe & Szegedy, 2015): we apply the normalization before the non-linearity and we remove the redundant bias term in the convolution operation. In terms of up/sub sampling, GeoVeX uses convolutional layers with stride of 2, instead of convolutional layers followed by pooling layers (as in (Hahner & Garcke, 2022)), in both the encoder (standard convolution) and the decoder (transposed convolution, sometimes referred as ”deconvolution”), since Springenberg et al. (2014) demonstrated that replacing all the pooling operations in a network with strided-convolutions improves overall performance.\n\nFigure 4: Illustration of the GeoVeX network architecture, showing the different layers of the Hexagonal Convolutional Autoencoders (HCAE) neural network with a Zero-Inflated Poisson (ZIP) probabilistic head layer to reconstruct the hexagonal matrix with geographical entity tags count. Each hexagonal convolution block and hexagonal transposed convolution block is composed by the respective convolution operator followed by Batch Normalization and ReLU operations.\n\n5\n\nUnder review as a conference paper at ICLR 2023\n\n3.4 HEXAGONAL CONVOLUTIONS\n\nThe main building block of both the encoder and the decoder is the hexagonal convolution operator, which aims at extracting relevant geographical features at different granularity. The network assumes in input an hexagonal grid transposed to a two dimensional matrix using the the axial coordinate system representation as described in Sec. 3.2. In this representation, unused matrix cells are put in the corners, avoiding empty cells in the middle of the matrix and supporting the hexagonal convolutional autoencoder network described below. In this context, it is shown that hexagonal convolutions give better results than standard convolutions in case of hexagonal shaped data (Steppa & Holch, 2019; Hoogeboom et al., 2018). In particular, the hexagonal convolution is achieved by multiplying the 3x3 convolution kernel with a 3x3 mask to set to zero the kernel at top-left and bottom-right cells (the mathematical formula of the 3x3 masked hexagonal convolution is given in Section A.1 of Appendix).\n\n3.5 ZERO-INFLATED POISSON RECONSTRUCTION LAYER\n\nSince the network is intended to be used with geographical count data assigned to each hexagon, the Poisson head layer is needed since the variable to reconstruct is coming from a counting process. Moreover, it expects a large number of zeros, since it is evident that for any given hexagon on Earth, only few geographical tags from the full vocabulary of tags are present: this hypothesis shall be verified in the data using the respective tests (Yang et al., 2010; Van den Broek, 1995). Under this scenario, multiple studies have demonstrated that the best fit to such data with excess of zeros is provided by zero-inflated distributions (Ridout et al., 1998; Beckett et al., 2014; Lambert, 1992; Unhapipat et al., 2018).\n\nThe Zero-Inflated Poisson (ZIP), which is denoted by ZIP (π, λ), has two parameters π and λ, and has the following probability mass function, where 0 ≤ π ≤ 1 and λ ≥ 0:\n\n∀i ∈ {1, . . . , K}, P(z.,.,i = c) =\n\n(cid:26)π + (1 − π) · e−λ (1 − π) · e−λ · λc\n\nc!\n\nif c = 0 if c ∈ {1, 2, 3, . . . }\n\n(1)\n\nIn the GeoVeX model, the parameters π and λ are estimated in the last layer of the decoder, and are used by the above ZIP mixture model to reconstruct, from the H3 embedding, the OSM count of each geographical tag k of each hexagon at axial coordinates i, j. In fact, the final decoder layer produces two output variables for each coordinate i, j, k: gπ and gλ. The first one is activated with a sigmoid function to produce π (Lambert, 1992), and the second one is activated with the exponential function to ensure that λ > 0, as is obviously required:\n\n(cid:101)πi,j,k =\n\negπ|i,j,k(h) 1 + egπ|i,j,k(h)\n\n(cid:101)λi,j,k = egλ|i,j,k(h)\n\n(2)\n\n(3)\n\nwhere gπ|i,j,k(h), gλ|i,j,k(h) are the two output layers of the decoder network which are conditioned on embedding h and produce the estimated (cid:101)π and (cid:101)λ for each coordinate i, j, k (Fig. 4).\n\n3.6 GEOVEX LOSS FUNCTION\n\nGeoVeX adapts the Negative Log-Likelihood (NLL) loss function in the context of a hexagonal grid to align with the design of the input data. In fact, the loss formulation encourages first the learning (and, thus, the reconstruction) of the geographical tags count describing the hexagon to embed, and then decay the importance of learning (and, thus, reconstructing) the tags count in each external ring of hexagons, by weighting the loss of each hexagon based on its position in the grid, thus justified simply by grid construction.\n\nIn details, the formulation of the GeoVeX loss function is an adaptation of the NLL loss with ZeroInflated Poisson distribution, in the context of a spatial relationship among the neighbouring regions. Based on the ZIP mass function (Eq. 1), and given that we have N samples in each learning batch, the log-likelihood function of each element of the 3D reconstructed tensor can be derived as:\n\n6\n\nUnder review as a conference paper at ICLR 2023\n\nlog L((cid:101)π, (cid:101)λ; y)i,j,k =\n\nN (cid:88)\n\nI(yn|i,j,k = 0) ln[(cid:101)πn|i,j,k + (1 − (cid:101)πn|i,j,k)e(cid:101)λn|i,j,k ]\n\nn=1\n\nN (cid:88)\n\n+\n\nI(yn|i,j,k > 0)[ln(1 − (cid:101)πn|i,j,k) − (cid:101)λn|i,j,k + yn|i,j,k ln (cid:101)λn|i,j,k − ln(yn|i,j,k!)]\n\nn=1\n\n(4) where n is the n-th sample, i and j are the two axial coordinates, k is the k-th geographical tag count, and I(yn|i,j,k) denotes an indicator variable that takes value 1 if the condition is verified, and zero otherwise. See derivation in (Hossain & Howlader, 2015) and (Lambert, 1992). To make the loss of each grid hexagon weighting proportional to the ring where it is located, we apply two positional weights to the ZIP NLL loss of each hexagon of the grid, based on its i, j axial coordinates:\n\n• the distance weighting kernel Wdist decays the importance of each subsequent neighbouring r-ith ring of hexagons, based on the inverse of the distance from the central hexagon to the ring number r (i.e. ring r = 1 applies a weight of 1/2, while ring r = 7 applies a weight of 1/8);\n\n• the numerosity weighting kernel Wnum equalizes the importance of each hexagonal r-ith ring, by subdividing the loss among the hexagons that compose it, so that each ring has same total importance in the final loss. By construction, each ring is composed of 6 times the number of the ring number r (i.e. ring r = 1 has 6 hexagons, while ring r = 7 has 42 hexagons, so each hexagon has to weight 6 · r to make each ring equal in importance)\n\nThe mathematical formulations in Eq. 5 and 6 are justified by construction (based on the semantic imposed by using an hexagonal grid centered on the hexagon to embed), where r is the hexagonal ring number, and |i − j| ≤ R is a simple condition to explicitly put to zero the loss at the invalid top-left and bottom-right values in the axial coordinates matrix representation.\n\nwdisti,j =\n\n \n\n\n\n1 1/(1 + ri,j) 0\n\nif i = j = 0 if |i − j| ≤ R otherwise\n\nwnumi,j =\n\n \n\n\n\n1 1/(6 · ri,j) 0\n\nif i = j = 0 if |i − j| ≤ R otherwise\n\n(5)\n\n(6)\n\nThe two loss weighting kernels are depicted in Fig. 6 and 7 in Sec. A.2 of Appendix, to help visualization of the kernel effect. By combining the two weighting kernels, the loss to minimize is the sum of the ZIP negative log-likelihood loss of each k-th geographical tag count of each i, j element of the hexagonal grid (Eq. 4), weighted based on its grid position using the kernels Wdist and Wnum:\n\nL = −\n\nK (cid:88)\n\nk=1\n\nlog L((cid:101)π, (cid:101)λ)k · Wdist · Wnum\n\nK (cid:88)\n\nM (cid:88)\n\nM (cid:88)\n\n= −\n\nk=1\n\ni=1\n\nj=1\n\nlog L((cid:101)π, (cid:101)λ)i,j,k ·\n\nwdisti,j\n\n(cid:80)M\n\ni,j=1 wdisti,j\n\n·\n\nwnumi,j\n\n(cid:80)M\n\ni,j=1 wnumi,j\n\n(7)\n\n4 EXPERIMENTS\n\nWe evaluate the task-agnostic GeoVeX vectors with a qualitative analysis of the latent structures using cosine similarities, and with a quantitative analysis of the improvement of performances on down-stream task.\n\n4.1 QUALITATIVE EXPERIMENTS\n\nIn this analysis, we perform a qualitative comparison between GeoVeX and Hex2Vec embedding vectors, where both models are trained on the same exact worldwide input data from H3 and OSM\n\n7\n\nUnder review as a conference paper at ICLR 2023\n\n(200+ millions vectors worldwide), and with the same size of embeddings space (32), to not provide any advantage based on a different representation space. Space2Vec embeddings do not represent any geographical characteristic so they cannot be used for unsupervised geographical explorations.\n\nFirst, we selected a sample of different cities all around the world, from different continents, each characterized by a mix of urban, suburban, green and water regions: Los Angeles, London, Cape Town, Tokyo and Mexico City. Then, we calculated the cosine similarity between an anchor point located in downtown (the exact center of the image) and each other point located in the region, to evaluate the potential discovery of clusters of locations with similar characteristics. The cosine similarities are shown in Fig. 5: the blue color indicates higher similarity to the center point location, and the red color indicates lower similarity.\n\nThe cosine similarity calculated using the GeoVex embeddings is shown in upper images: we can see that the geographic semantic captured by the model is aligned to human expectations, to some degree: adjacent hexagons tend to have similar embeddings until they are very different in terms of nearby geographical entities, and clusters of locations can be defined to delineate similar regions.\n\nThe cosine similarity calculated using Hex2Vec is shown in bottom images from Fig. 5: in this case, there is no evident human-readable semantic that can be extracted by this plot. We can conclude that the Hex2Vec model formulation does not perform well in terms of assigning similar embeddings to adjacent hexagons, as there are very few smooth clusters of hexagons that can be seen inside these more noisy patterns.\n\nFigure 5: The cosine similarity calculated using embedding vectors from GeoVeX (top) and Hex2Vec (bottom), using the yellow hexagon of the center point location as anchor item, for Los Angeles, London, Cape Town, Tokyo and Mexico City.\n\n4.2 QUANTITATIVE EXPERIMENTS\n\nWe conducted quantitative experiments on the impact of using GeoVeX vectors as additional features in two downstream tasks: 1) prediction of worldwide vacation rental listings price in the travel industry (Sec. 4.2) and 2) hyperlocal interpolation of temperatures from weather stations (Sec. A.6 in Appendix). In both cases, the embeddings are pre-trained, agnostic of downstream tasks, frozen and then used as-is as additional features, to demonstrate that the task-agnostic learnt representations are meaningful.\n\nExpressiveness of GeoVeX, Hex2Vec and Space2Vec vectors is evaluated with a feature ablation study, by comparing models using: 1) Common task features (Baseline); Additional features: 2) GeoVeX vectors; 3) Hex2Vec vectors; 4) Space2Vec encoding vectors; 5) both GeoVeX and Space2Vec; 6) both Hex2Vec and Space2Vec.\n\nAll competing models are trained with the same Gradient Boosting Machines (GBM) regression model architecture, which enables the discovery of interactions among features. For each task, the model hyperparameters are optimized through random search (see Sec. A.4 of Appendix for details on how to reproduce the experiments setup). For fair comparison, Space2Vec hyper-parameters\n\n8\n\nUnder review as a conference paper at ICLR 2023\n\nhave been optimized for best task performance, since this model requires manual parameters to be configured, and Hex2Vec pre-trained embeddings are learnt with same data and same embedding size of GeoVeX.\n\nVacation Rentals Price Prediction Task Open-source Inside Airbnb dataset3 contains 1M+ listings located worldwide and describes each vacation rental listing with lodging characteristics, geographical coordinates, neighborhood name and daily price, which is the value to predict, after a conversion to a common currency. The hypothesis is that the geographical characteristics of the location influence its price, and thus GeoVeX vectors improve model performance when added as contextual geographical features. See Section A.5 of Appendix for the list of cities and regions obtained, so to replicate the experiment setup.\n\nIn terms of features, the baseline model is designed with the set of state-of-the-art features form (Islam et al., 2022), where base geographical information is composed of latitude, longitude and neighborhood name. Its intent is not be used as a performance benchmark, but to underline how much information the additional geospatial embeddings bring on top of a state-of-the-art set of task-specific features. The loss function is the Mean Squared Error (MSE) of the logarithm of the price, and the metrics monitored are both the loss and the Mean Absolute Error (MAE) of the price. The listings are split into training, validation and test with ratios 80%;10%;10%, with two splitting methods: a common random split, and a split by city names, to evaluate how well the model is able to generalize to new unseen geographical areas (i.e. out of training space). Table 1 shows the average performance of all models trained 7 times, together with the respective standard deviation.\n\nWe can see that the GeoVeX vectors are useful in both test strategies. For unseen cities, the GBM model with GeoVeX vectors confidently outperforms every other tested model, suggesting that GeoVeX improves both upon Hex2Vec, to compress geographical information useful to predict out-of-space, and upon Space2Vec, which encodes the location coordinates without considering the geographical entities. For unseen coordinates (random split), the GBM model with GeoVeX vectors paired with Space2Vec is the only model able to confidently improve the baseline performance.\n\nTable 1: Experiment results for the price prediction task. The lower the better.\n\nModel\n\nSplit Randomly\n\nSplit By City\n\nMSE ln(Price) MAE Price MSE ln(Price) MAE Price\n\nBaseline w/ GeoVeX w/ Hex2Vec w/ Space2Vec w/ GeoVeX & Space2Vec w/ Hex2Vec & Space2Vec\n\n0.281 ± 0.005 0.285 ± 0.008 0.286 ± 0.010 0.290 ± 0.013 0.271 ± 0.009 0.280 ± 0.004\n\n51.61 ± 0.46 52.29 ± 0.61 52.20 ± 0.91 52.51 ± 1.17 50.65 ± 0.89 51.59 ± 0.35\n\n0.492 ± 0.017 0.465 ± 0.007 0.489 ± 0.008 0.508 ± 0.010 0.492 ± 0.010 0.487 ± 0.010\n\n72.69 ± 1.15 70.30 ± 0.26 71.33 ± 0.42 72.56 ± 0.61 71.99 ± 0.66 72.53 ± 0.44\n\n5 CONCLUSION\n\nWe presented GeoVeX, a representation model which learns geospatial vectors as latent representations of the location’s geographical characteristics, contextualized on the neighboring locations. GeoVeX is based on a novel architecture named Hexagonal Convolutional Autoencoder (HCAE) with a Zero-Inflated Poisson (ZIP) reconstruction layer to learn from hexagonal grids described with zero-inflated counts of the geographical tags of OSM entities. Qualitative analysis showcases the improvement of representation structures compared to state-of-the-art Hex2Vec model. Then, quantitative experiments on real open data support the fact that the pre-trained GeoVeX embeddings can improve models in multiple domains without task-specific re-training. In future, we aim to analyze the impact of changing the H3 grid resolution, test different zero-inflated probabilistic distributions, compare the task-agnostic vectors against some task-specialized versions, and tackle a broader set of downstream tasks that involve geolocated entities, such as: climate-related tasks (e.g. effect of urbanization on temperatures), classifications of geolocated images, or ranking of geolocated items.\n\n3http://insideairbnb.com\n\n9\n\nUnder review as a conference paper at ICLR 2023\n\nREFERENCES\n\nEman Saeed Alamoudi and Norah Saleh Alghamdi. Sentiment classification and aspect-based sentiment analysis on yelp reviews using deep learning and word embeddings. Journal of Decision Systems, 30(2-3):259–281, 2021.\n\nAlexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rylwJxrYDS.\n\nSadie Beckett, Joshua Jee, Thapelo Ncube, Sophia Pompilus, Quintel Washington, Anshuman Singh, and Nabendu Pal. Zero-inflated poisson (zip) distribution: Parameter estimation and applications to model data from natural calamities. Involve, a Journal of Mathematics, 7(6):751–767, 2014.\n\nThomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2019–2026, 2014.\n\nJames Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb):281–305, 2012. ISSN ISSN 1533-7928. URL http: //www.jmlr.org/papers/v13/bergstra12a.html.\n\nArijit Biswas, Mukul Bhutani, and Subhajit Sanyal. Mrnet-product2vec: A multi-task recurrent neural network for product embeddings. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 153–165. Springer, 2017.\n\nB. Oscar Deho, A. William Agangiba, L. Felix Aryeh, and A. Jeffery Ansah. Sentiment analysis with word embedding. 2018 IEEE 7th International Conference on Adaptive Science & Technology (ICAST), pp. 1–4, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\n\nbidirectional transformers for language understanding. ArXiv, abs/1810.04805, 2019.\n\nJiahong Du, Yujun Chen, Yue Wang, and Juhua Pu. Zone2vec: Distributed representation learning of urban zones. In 2018 24th International Conference on Pattern Recognition (ICPR), pp. 880–885. IEEE, 2018.\n\nXifeng Guo, Xinwang Liu, En Zhu, and Jianping Yin. Deep clustering with convolutional autoenIn International conference on neural information processing, pp. 373–382. Springer,\n\ncoders. 2017.\n\nSara Hahner and Jochen Garcke. Mesh convolutional autoencoder for semi-regular meshes of different sizes. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 885–894, 2022.\n\nIbrahim Riza Hallac, Semiha Makinist, Betul Ay, and Galip Aydin. user2vec: Social media user representation based on distributed document embeddings. 2019 International Artificial Intelligence and Data Processing Symposium (IDAP), pp. 1–5, 2019.\n\nEmiel Hoogeboom, Jorn WT Peters, Taco S Cohen, and Max Welling. Hexaconv. arXiv preprint\n\narXiv:1803.02108, 2018.\n\nShakhawat Hossain and Hatem Howlader. Estimation techniques for regression model with zero-\n\ninflated poisson data. International Journal of Statistics and Probability, 4:64, 2015.\n\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. PMLR, 2015.\n\nMd Didarul Islam, Bin Li, Kazi Saiful Islam, Rakibul Ahasan, Md Rimu Mia, and Md Emdadul Haque. Airbnb rental price modeling based on latent dirichlet allocation and mesf-xgboost composite model. Machine Learning with Applications, 7:100208, 2022.\n\n10\n\nUnder review as a conference paper at ICLR 2023\n\nNeal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon. In Proceedings\n\nTile2vec: Unsupervised representation learning for spatially distributed data. of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3967–3974, 2019.\n\nPorter Jenkins, Ahmad Farag, Suhang Wang, and Zhenhui Li. Unsupervised representation learnIn Proceedings of the 28th ACM international\n\ning of spatial data via multimodal embedding. conference on information and knowledge management, pp. 1993–2002, 2019.\n\nSlava Kisilevich, Daniel A. Keim, and Lior Rokach. A gis-based decision support system for hotel room rate estimation and temporal price prediction: The hotel brokers’ context. Decis. Support Syst., 54:1119–1133, 2013.\n\nDiane Lambert. Zero-inflated poisson regression, with an application to defects in manufacturing.\n\nTechnometrics, 34(1):1–14, 1992.\n\nJunren Luo, Wanpeng Zhang, Jiongming Su, and Fengtao Xiang. Hexagonal convolutional neural\n\nnetworks for hexagonal grids. IEEE Access, 7:142738–142749, 2019.\n\nOisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only geographical priors for finegrained image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9596–9606, 2019.\n\nGengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, and N. Lao. Multi-scale representation learning for spatial feature distributions using grid cells. ArXiv, abs/2003.00824, 2020.\n\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.\n\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014.\n\nPrajapati Pradip and Prof. Monali Suthar. A survey on price prediction model for airbnb listing using machine learning. International Journal of Scientific Research in Science, Engineering and Technology, 2022.\n\nMartin Ridout, Clarice GB Dem ́etrio, and John Hinde. Models for count data with many zeros. In Proceedings of the XIXth international biometric conference, volume 19, pp. 179–192. International Biometric Society Invited Papers Cape Town, South Africa, 1998.\n\nEvan Sheehan, Chenlin Meng, Matthew Tan, Burak Uzkent, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Predicting economic development using geolocated wikipedia articles. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery data mining, pp. 2698–2706, 2019.\n\nJost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for\n\nsimplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.\n\nConstantin Steppa and Tim L Holch. Hexagdly—processing hexagonally sampled data with cnns in\n\npytorch. SoftwareX, 9:193–198, 2019.\n\nDuyu Tang, Furu Wei, Nan Yang, M. Zhou, Ting Liu, and Bing Qin. Learning sentiment-specific\n\nword embedding for twitter sentiment classification. In ACL, 2014.\n\nWaldo R Tobler. A computer movie simulating urban growth in the detroit region. Economic\n\ngeography, 46(sup1):234–240, 1970.\n\nSuntaree Unhapipat, Montip Tiensuwan, and Nabendu Pal. Bayesian predictive inference for zeroinflated poisson (zip) distribution with applications. American Journal of Mathematical and Management Sciences, 37(1):66–79, 2018.\n\nJan Van den Broek. A score test for zero inflation in a poisson distribution. Biometrics, pp. 738–743,\n\n1995.\n\n11\n\nUnder review as a conference paper at ICLR 2023\n\nLu Wang, Tinghua Ai, Yilang Shen, and Jingzhong Li. The isotropic organization of dem structure and extraction of valley lines using hexagonal grid. Transactions in GIS, 24(2):483–507, 2020a.\n\nPei-Ying Wang, Chiao-Ting Chen, Jain-Wun Su, Tingwei Wang, and Szu-Hao Huang. Deep learning model for house price prediction using heterogeneous data analysis along with joint self-attention mechanism. IEEE Access, 9:55244–55259, 2021.\n\nYanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul Kingsbury, and Hongfang Liu. A comparison of word embeddings for the biomedical natural language processing. Journal of biomedical informatics, 87:12–20, 2018.\n\nZhecheng Wang, Haoyuan Li, and Ram Rajagopal. Urban2vec: Incorporating street view imagery and pois for multi-modal urban neighborhood embedding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 1013–1020, 2020b.\n\nSzymon Wo ́zniak and Piotr Szyma ́nski. hex2vec: Context-aware embedding h3 hexagons with openstreetmap tags. In Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, pp. 61–71, 2021.\n\nTingting Wu and Yingru Li. Spatial interpolation of temperature in the united states using residual\n\nkriging. Applied Geography, 44:112–120, 2013.\n\nMingjun Xiang. Region2vec: An approach for urban land use detection by fusing multiple features. In Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence, pp. 13–18, 2020.\n\nZhao Yang, James W Hardin, and Cheryl L Addy. Score tests for zero-inflation in overdispersed\n\ncount data. Communications in Statistics—Theory and Methods, 39(11):2008–2030, 2010.\n\nGuangyou Zhou, Tingting He, Jun Zhao, and Po Hu. Learning continuous word embedding with\n\nmetadata for question retrieval in community question answering. In ACL, 2015.\n\n12\n\nUnder review as a conference paper at ICLR 2023\n\nA APPENDIX\n\nA.1 MATHEMATICAL FORMULA OF 3X3 MASKED HEXAGONAL CONVOLUTION\n\nThe mathematical formulation of the 3x3 masked hexagonal convolution is the following:\n\nYk = Hk ∗ Xk = (Wk ⊙ Mk) ∗ Xk = (\n\n(cid:34) w−1,1 w1,1 w0,1 w1,0 w0,0 w−1,0 w−1,−1 w0,−1 w1,−1\n\n(cid:35)\n\n⊙\n\n(cid:34) 0 1\n1\n\n1 1\n1\n\n1 1\n0\n\n(cid:35)\n\n) ∗ Xk (8)\n\nwhere H is the 3x3 masked hexagonal convolution kernel; W is the standard 3x3 convolution kernel; M is the 3x3 hexagonal mask; X is the input tensor; ∗ is the convolution operation; ⊙ is the elementwise multiplication operation; i and j are the two axial coordinates; k is the 3rd channel of the tensor, representing the normalized count of the k-th OSM tag in the first convolution, and the k-th normalized filter in the subsequent convolutions.\n\nA.2 VISUAL REPRESENTATION OF THE LOSS KERNELS\n\nIn this section, we show the plots of the weighting kernels used in the loss formulation: the distance weighting kernel and numerosity weighting kernel. The kernels are plotted in the axial coordinates system, projected to a 2D matrix form. For each i, j pair of coordinates, the weight is displayed.\n\nFigure 6: The distance weighting kernel.\n\nFigure 7: The numerosity weighting kernel.\n\nA.3 H3 AND OPENSTREETMAP DATA PREPARATION\n\nThe construction of the database of hexagons described by respective geographical entities is done as follows. First, we calculate the set of hexagons that lie within the boundary of each country shape, using resolution 8 (best trade-off between data granularity, computational power and memory needed to compute the expensive geospatial operations). The result is a set of 200+ millions hexagons to cover the Earth’s lands. Then, from the worldwide OSM database of geographical entities, we extract: 150 millions of points, identified by their coordinates; 220 millions of lines, identified by a set of coordinates; 500 millions of multi-polygons, consisting of one or more set of coordinates describing the polygons, and one or more set of coordinates describing the holes in the polygons. These three types of entities are geographically intersected with the hexagons, and a Bag-Of-Words (BOW) model is used to count the OSM geographical tags attributed to the entities intersecting each hexagon.\n\nThe extraction of the set of points, lines and polygons covering all the meaningful entities that can be used to describe the world is done with GDAL ogr2ogr 4. The initial selection of OSM keys is done based on both the outcome of the analysis done in Wo ́zniak & Szyma ́nski (2021), with some\n\n4https://gdal.org/programs/ogr2ogr.html\n\n13\n\nUnder review as a conference paper at ICLR 2023\n\nother tags kept and cleaned since relevant to our extended use case (e.g. highways). In addition, we filtered out the tags with the value containing the following post-fixes: :yes and :no, since we found them to be not consistent and misleading. We also filtered out tags that are only available in few countries, so not relevant worldwide.\n\nThe geographical operations to clean, manipulate and intersect OSM entities with H3 hexagons are achieved using Apache Sedona 5 on an Apache Spark 6 cluster with a total of 10 Tb of memory and 1280 cores. Once the geospatial joins of hexagons and OSM entities are computed, a second level of sort and filtering is performed, in order to select the most meaningful features used to describe the hexagons of the world. In fact, based on the analysis of aggregated statistics, it is clear that some key=value pairs are only valid in few parts of the world (e.g. administrative tags), so we keep only pairs that are assigned to hexagons in at least 20 countries. Finally, we sort the pairs by descending distinct count of hexagons with that pair assigned, and we select the top 1024 tags. This cut-off is chosen using a trade-off between the quality of additional descriptors and the GPU memory constraint during model training.\n\nA.4 HYPERPARAMETER SELECTION FOR TASKS MODELS\n\nFor each task and for each model to train, we optimize the following LightGBM hyper-parameters, by performing a random search with 60 trials, since it can be proven that if the close-to-optimal region of hyperparameters occupies at least 5% of the grid surface, then random search with 60 trials will find that region with high probability (Bergstra & Bengio, 2012):\n\n• max depth: random int from 0 to 32;\n\n• num leaves: random int from 4 to 2048;\n\n• min data in leaf : random int from 20 to 10000;\n\n• min gain to split: random int from 0 to 15;\n\n• max cat threshold: random int from 32 to 1024;\n\n• bagging fraction: random float from 0.2 to 0.95, with step of 0.1;\n\n• feature fraction: random float from 0.3 to 0.95, with step of 0.1;\n\n• learning rate: a range of float from 0.001 to 0.2;\n\n• lambda l1: random int from 0 to 100, with step of 5;\n\n• lambda l2: random int from 0 to 100, with step of 5;\n\n• extra trees: random true or false.\n\nFor each model using Space2Vec coordinates encoding, the following additional hyper-parameters are optimized, using a range of values suggested by the authors:\n\n• lambda min: random value in: 0.1, 1, 10, 100, 500;\n\n• lambda min: random value in: 1000, 10000, 40000;\n\n• num frequencies: random value in: 8, 16, 32, 64.\n\nA.5 VACATION RENTALS PRICE PREDICTION TASK - DETAILS\n\nThe cities and regions for which we obtained listing prices from by inside-airbnb open data are the following: Antwerp, Asheville, Athens, Austin, Bangkok, Barcelona, Beijing, Belize, Bergamo, Berlin, Bologna, Bordeaux, Boston, Bristol, Broward County, Brussels, Buenos Aires, Cambridge, Cape Town, Chicago, Clark County (NV), Columbus, Copenhagen, Crete, Denver, Dublin, Edinburgh, Euskadi, Florence, Gauteng, Geneva, Girona, Greater Manchester, Hawaii, Hong Kong, Istanbul, Jersey City, Lisbon, London, Los Angeles, Lyon, Madrid, Malaga, Mallorca, Menorca, Mexico City, Milan, Montreal, Munich, Naples, Nashville, New Brunswick, New Orleans, New York City, Oakland, Oslo, Ottawa, Pacific Grove, Paris, Portland, Porto, Prague, Puglia, Quebec City,\n\n5https://sedona.apache.org/ 6https://spark.apache.org/\n\n14\n\nUnder review as a conference paper at ICLR 2023\n\nRhode Island, Rio De Janeiro, Rome, Salem (OR), San Diego, San Francisco, San Mateo County, Santa Clara County, Santa Cruz County, Santiago, Seattle, Sevilla, Shanghai, Sicily, Singapore, South Aegean, Stockholm, Taipei, Thessaloniki, Tokyo, Toronto, Trentino, Twin Cities (MSA), Valencia, Vancouver, Vaud, Venice, Victoria, Vienna, Washington DC, Zurich.\n\nA.6 HYPERLOCAL TEMPERATURES INTERPOLATION TASK\n\nARPA Lombardia is a regional agency for environment protection which provides open weather sensors data7 in the area of Milan (Italy). The data contains 1.4 millions of hourly temperature samples, recorded by 13 weather stations from January 2010 to August 2022 in Milan and nearby. Each sample is described by: the temperature, the date, the hour, and the coordinates of the weather station. The task is the interpolation of the temperature between weather stations, which is modeled as a prediction of the temperature in an unknown location. This is achieved by removing a subset of stations from the training data and by comparing the model predictions with the actual temperatures observed by the test stations. The hypothesis is that the geographical characteristics of the location help to explain the differences in recorded temperatures from nearby sensors.\n\nThe experimental setup is the same as in Sec. 4.2. Baseline model denotes the model relying on all the available features when geospatial data are only latitude and longitude which are the date components (year, month and week of year) and the hour of the day. The loss function is the MSE of the temperature, and the metrics monitored are both the loss and the MAE. The temperature records are split into training, validation and test with ratios 80%;10%;10% by randomly sampling the sensors, to analyze the performance of estimating the temperatures at an unknown location.\n\nTable 2 shows the performance of the different models on the test set. We can see that the GBM models with GeoVeX emebddings (both alone and combined to Space2Vec) confidently outperform every other model. This shows that GeoVeX vectors help in improving the estimation of temperatures in this region with a mix of urban and green areas, by providing the geographical context of each location.\n\nTable 2: Experiment results for the hyperlocal temperature interpolation task. The lower the better.\n\nModel\n\nMSE Temperature MAE Temperature\n\nBaseline w/ GeoVeX w/ Hex2Vec w/ Space2Vec w/ GeoVeX & Space2Vec w/ Hex2Vec & Space2Vec\n\n11.144 ± 0.094 10.656 ± 0.135 10.862 ± 0.110 10.880 ± 0.105 10.651 ± 0.090 10.870 ± 0.172\n\n2.651 ± 0.012 2.615 ± 0.016 2.628 ± 0.010 2.639 ± 0.012 2.613 ± 0.012 2.629 ± 0.019\n\n7https://www.arpalombardia.it/Pages/Meteorologia/Richiesta-dati-misurati.aspx\n\n15",
  "translations": [
    "# Summary Of The Paper\n\nThe paper proposes to learn spatial embedding for coordinates based on H3 hexagons and convolutional autoencoders using the OSM’s tag data.\n\n# Strength And Weaknesses\n\nS1. The problem is important and using convolutional autoencoders in building global spatial embeddings seems to be novel.\n\nW1. The method is very similar to Hex2Vec (both based on hexagons and OSM data). The performance improvement in the experiments is also very marginal.\n\nW2. The scale effect is very important for spatial big data analysis. However, the current method only learns representation for a fixed scale of the spatial region (H3 hexagon), which may not be useful for practical spatial data analysis.\n\nW3. The evaluation is only conducted for US and Italy, which cannot well support the motivation that the learned embedding is for the whole earth.\n\n# Clarity, Quality, Novelty And Reproducibility\n\nclarity: the paper's basic idea is clear\n\nnovelty: using a hexagon convolution encoder seems to be novel, but existing work has already used the same data for learning spatial embeddings and achieved similar performance (Hex2Vec).\n\nquality: more experiments on multi-scale/global spatial data analysis tasks are preferred.\n\n# Summary Of The Review\n\nIn brief, learning global spatial embedding is a valuable task. However, the currently proposed method and the used data lack significant contribution beyond existing work.\n\n# Correctness\n\n2: Several of the paper’s claims are incorrect or not well-supported.\n\n# Technical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.\n\n# Empirical Novelty And Significance\n\n2: The contributions are only marginally significant or novel.",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a geospatial representation model designed for learning global vector embeddings across all geographical locations on Earth. The model is built on Hexagonal Convolutional Autoencoders (HCAE) and incorporates a Zero-Inflated Poisson (ZIP) reconstruction layer to effectively handle sparse geographical count data represented in a hexagonal grid, specifically using Uber's H3 hexagons and OpenStreetMap (OSM) geographical tags. The key contributions of the paper include the development of task-agnostic geospatial vector embeddings that leverage neighboring hexagon features and the novel ZIP decoder for training on sparse data. Experimental results indicate that GeoVeX outperforms existing models, Hex2Vec and Space2Vec, in tasks such as worldwide rental price prediction and hyperlocal temperature interpolation.\n\n# Strength And Weaknesses\nThe strengths of GeoVeX lie in its innovative approach to contextualizing geospatial embeddings through neighboring hexagons, which enhances its performance in predictive tasks compared to previous models. The introduction of a ZIP layer addresses the challenge of sparse data in geographical contexts effectively. However, the paper could benefit from a more extensive exploration of the model's limitations, such as potential computational overhead associated with hexagonal convolutions and the scalability of the approach across larger datasets or different geographical regions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology clearly, making it accessible to readers with varying levels of familiarity with geospatial modeling. The quality of the experimental design is commendable, with both qualitative and quantitative analyses provided, showcasing the model's capabilities. The novelty of the ZIP decoder and the unique application of hexagonal convolutions are significant contributions to the field. However, details on hyperparameter tuning and specific dataset preparation methods could enhance reproducibility.\n\n# Summary Of The Review\nOverall, GeoVeX presents a compelling advancement in geospatial vector embedding by integrating hexagonal convolutional structures and a probabilistic decoder. The results demonstrate significant improvements over existing models, although future work should address potential limitations and enhance reproducibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents GeoVeX, a geospatial representation model utilizing a Hexagonal Convolutional Autoencoder (HCAE) to learn embeddings for global geographical locations. Key contributions include the implementation of the H3 hexagonal grid system to reduce map distortion and the introduction of a Zero-Inflated Poisson (ZIP) autoencoder layer, which is tailored for sparse geographical count data, particularly from OpenStreetMap. The methodology involves hexagonal convolutions and a customized loss function that accounts for spatial relationships, leading to superior representation quality. Experimental results demonstrate that GeoVeX outperforms existing models (Hex2Vec, Space2Vec) in tasks such as vacation rental price prediction and hyperlocal temperature interpolation, showcasing its potential across various geospatial applications.\n\n# Strength And Weaknesses\nThe paper's strengths lie in its innovative architecture and contextual awareness, which effectively address challenges in geospatial data representation. The use of hexagonal convolutions allows for more meaningful embeddings, while the ZIP layer caters specifically to the nature of geographical data. Additionally, the versatility of the model across different tasks indicates its broad applicability. However, the paper does face limitations, including dependence on the quality of OSM data, which may affect performance, and the computational complexity associated with processing large datasets. Furthermore, the fixed resolution of the H3 grid may not be optimal for all applications, suggesting a potential area for future exploration.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and structured, providing a clear explanation of the methodology and results. The quality of the experiments is high, with both qualitative and quantitative analyses supporting the claims made. The novelty of the approach is significant, particularly in its architectural design and the contextual embedding strategy. However, reproducibility may be somewhat hindered by the reliance on OSM data quality and the computational demands of the model, which could pose challenges for broader adoption.\n\n# Summary Of The Review\nGeoVeX represents a notable advancement in geospatial embedding techniques, effectively leveraging hexagonal convolutions and a specialized ZIP layer to enhance representation quality. While the model shows promise across diverse applications, its reliance on data quality and computational complexity may limit its accessibility.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel geospatial representation model that employs Hexagonal Convolutional Autoencoders (HCAE) and a Zero-Inflated Poisson (ZIP) reconstruction layer to address the limitations of existing geospatial embedding approaches. GeoVeX generates task-agnostic geospatial vectors contextualized by neighboring hexagons and demonstrates superior performance in two key applications: worldwide travel listings price prediction and hyperlocal climate data interpolation. The authors validate their approach through qualitative and quantitative experiments, showing improvements over previous models like Hex2Vec and Space2Vec.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative approach to geospatial embeddings that incorporate contextualization and sparsity handling through the ZIP layer. This addresses significant limitations of prior models that were either task-specific or lacked contextual awareness. The methodology is well-structured, with a clear explanation of the model architecture and loss function. However, the paper could benefit from further discussion on the computational efficiency of the model and its scalability across larger datasets, as well as a more detailed exploration of potential limitations in the model's application.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly structured, making it easy to follow the proposed methodology and findings. The quality of the experiments is high, with both qualitative and quantitative analyses supporting the claims made. The novelty of the approach is significant, particularly in the integration of hexagonal convolutions and the ZIP layer for sparse data. Reproducibility is supported by detailed descriptions of the model architecture and loss function, although the authors could enhance reproducibility by providing access to code and datasets used in the experiments.\n\n# Summary Of The Review\nOverall, GeoVeX presents a compelling advancement in the field of geospatial embeddings, offering both methodological innovation and empirical validation. The model's ability to generate contextualized geospatial vectors shows promise for various applications in Geographic Information Science. However, further insights into the model's scalability and computational efficiency would strengthen the paper.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces Hexagonal Convolutional Autoencoders (HCAE), a novel architecture designed to enhance the processing of geospatial data by utilizing hexagonal grids for improved representation. The methodology includes the development of GeoVeX, which generates contextualized embeddings by considering the features of neighboring hexagons, leading to semantically richer representations. The findings indicate that GeoVeX outperforms existing models like Hex2Vec and Space2Vec in practical applications, such as price prediction and temperature interpolation, while effectively handling sparse count data through a Zero-Inflated Poisson (ZIP) reconstruction layer.\n\n# Strength And Weaknesses\nThe paper presents several significant strengths, including the introduction of a novel architecture that addresses the unique challenges of geospatial data and the creation of meaningful embeddings through contextualization. However, there are notable weaknesses, such as the complexity of the architecture potentially hindering interpretability and requiring substantial computational resources. Additionally, while the performance improvements are promising, they are based on specific datasets, raising concerns about the model's generalizability across different geographical regions and tasks. The reliance on ZIP could also limit flexibility in handling diverse data distributions.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, with a quality presentation of both qualitative and quantitative validations. However, the novelty lies primarily in its architectural design and the contextualized embedding approach. The reproducibility could be strengthened by providing more detailed information on training procedures and hyperparameter tuning, as well as by evaluating the model on a broader range of datasets to showcase its robustness.\n\n# Summary Of The Review\nOverall, the paper presents a valuable contribution to geospatial data processing through its novel architecture and methods. While it demonstrates promising results, the limitations regarding generalizability, complexity, and potential biases from data sources warrant further investigation and validation.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel model for generating contextualized geospatial embeddings using Hexagonal Convolutional Autoencoders (HCAE) and a Zero-Inflated Poisson (ZIP) reconstruction mechanism. The authors argue that GeoVeX enhances the representation of geographical locations by taking into account adjacent spatial features, outperforming existing models like Hex2Vec and Space2Vec in specific downstream tasks. The methodology includes a unique architecture that operates on hexagonal grids, a ZIP layer to manage sparsity in geographical count data, and a contextual learning mechanism that emphasizes the relationships between neighboring hexagons.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to geospatial data representation through hexagonal grids and its effective handling of zero-inflated data. The contextual learning mechanism and comprehensive loss function significantly improve embedding quality and performance in downstream tasks. However, the paper could benefit from a more extensive discussion on the limitations of the proposed model and its applicability to broader geolocated tasks beyond those tested. Furthermore, while the qualitative and quantitative results are promising, additional experimental validation on diverse datasets would strengthen the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers. The quality of the writing and figures is high, aiding in the understanding of complex concepts. In terms of novelty, the introduction of HCAE for geospatial data is a significant advancement; however, the reproducibility of results could be improved by providing more details on the experimental setup and datasets used. The authors do outline future work, which suggests an openness to refining the model and enhancing its applicability.\n\n# Summary Of The Review\nGeoVeX presents a novel and significant advancement in geospatial embeddings, demonstrating improved performance through its innovative use of hexagonal convolutional architecture and contextual learning. While the contributions are notable, further validation across diverse datasets and a discussion on limitations would enhance the paper's impact and applicability in real-world scenarios.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces GEOVEX, a novel framework that utilizes hexagonal convolutional autoencoders to process geospatial vectors for improved representation learning. The authors present an innovative architecture that leverages hexagonal grids to capture spatial relationships more effectively than traditional square grids. Their methodology includes the development of a new loss function tailored for geospatial data, and the findings demonstrate that GEOVEX achieves superior performance in various downstream tasks, including land cover classification and urban feature extraction, when compared to existing methods.\n\n# Strength And Weaknesses\n**Strengths:**\n1. **Innovative Use of Hexagonal Structures:** The application of hexagonal grids facilitates a more isotropic representation of spatial data, which is a significant improvement over conventional square grid approaches.\n2. **Robust Experimental Validation:** The paper includes thorough empirical evaluations across multiple datasets, showcasing GEOVEX's effectiveness in various scenarios.\n3. **Practical Applications:** Demonstrated applicability in real-world geospatial tasks enhances the relevance of the proposed method.\n\n**Weaknesses:**\n1. **Complexity of Implementation:** The hexagonal convolutional architecture may introduce additional complexity in implementation, which could hinder adoption in practice.\n2. **Limited Exploration of Hyperparameters:** While the results are promising, the exploration of hyperparameter settings appears somewhat limited, which may impact the reproducibility of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-written and clearly articulates its contributions, methodology, and findings. The quality of figures and visual aids enhances the clarity of the presented concepts. However, the novelty of the approach could be further contextualized with a more detailed discussion of related works. The reproducibility of the results might be a concern due to the potential complexity of the implementation and the limited hyperparameter exploration.\n\n# Summary Of The Review\nOverall, the paper presents a compelling approach to processing geospatial data using hexagonal convolutional autoencoders, demonstrating significant advancements in representation learning. While the methodology is innovative and the empirical results are strong, some concerns regarding implementation complexity and hyperparameter tuning may limit its immediate applicability.\n\n# Correctness\n4/5 - The methodology appears sound, and the experimental results support the claims made. Minor concerns regarding hyperparameter tuning may affect the robustness of findings.\n\n# Technical Novelty And Significance\n5/5 - The introduction of hexagonal convolutional structures represents a significant advancement in the field, particularly for geospatial applications, and offers a fresh perspective on representation learning.\n\n# Empirical Novelty And Significance\n4/5 - The empirical results are strong and provide valuable insights; however, the limited exploration of hyperparameters might affect the perceived novelty and applicability in diverse contexts.",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel model aimed at enhancing global geospatial representation through innovative methodologies. Key contributions include the GeoVeX architecture, which redefines geographical relationships; the introduction of a Zero-Inflated Poisson (ZIP) layer for better handling of sparse data; and the provision of contextualized embeddings based on neighboring hexagons. The authors claim that GeoVeX significantly outperforms existing models like Hex2Vec and Space2Vec, asserting that it leads to unprecedented accuracy and efficiency in geospatial data processing. The paper emphasizes the potential applications of GeoVeX across diverse fields, including climate science and urban planning, while framing it as a transformative advancement in geospatial technology.\n\n# Strength And Weaknesses\nThe paper presents several strengths, including a compelling architecture designed specifically for geospatial data and innovative approaches to addressing data sparsity. The introduction of the ZIP layer is a notable contribution that could have practical implications in various applications. However, the claims regarding the model's superiority appear overstated, as the improvements may represent an incremental advancement rather than a revolutionary breakthrough. The paper lacks comprehensive comparisons to other methods and does not sufficiently investigate the limitations of the GeoVeX model, which diminishes the robustness of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written, with a clear structure that outlines the contributions and findings. While the novelty of GeoVeX is stated, the experimental results and methodologies could be better detailed to enhance reproducibility. The authors make bold claims regarding the model's performance but do not provide enough empirical evidence or thorough comparisons to substantiate these claims fully. The reproducibility of the results would benefit from clearer descriptions of the experimental setup and parameters used in training the model.\n\n# Summary Of The Review\nOverall, the paper presents an interesting approach to geospatial representation with the GeoVeX model, highlighting several innovative contributions. However, the claims regarding its revolutionary impact are overstated, and the paper would benefit from a more rigorous empirical evaluation and clearer discussions on limitations and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper \"GEOVEX: GEOSPATIAL VECTORS WITH HEXAGONAL CONVOLUTIONAL AUTOENCODERS\" presents a novel geospatial representation model that utilizes Hexagonal Convolutional Autoencoders (HCAE) to learn global vectors for geographic locations on Earth. The approach incorporates a Zero-Inflated Poisson (ZIP) reconstruction layer and operates on an H3 hexagonal grid. The authors claim improvements in tasks such as worldwide listings price prediction and hyperlocal interpolation of climate data when compared to existing models like Hex2Vec and Space2Vec, demonstrating better performance metrics and clearer semantic clustering in the learned embeddings.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative use of hexagonal grids and ZIP autoencoders, addressing the limitations of prior models by incorporating contextual information from nearby geographical entities. The experiments showcase robust qualitative and quantitative results, indicating a significant enhancement in performance metrics for both price prediction and temperature interpolation tasks. However, weaknesses include a lack of thorough exploration of hyperparameter optimization and potential overfitting concerns due to the model's complexity. Additionally, while the comparison with existing methods is valuable, further evaluation on more diverse datasets could strengthen the claims made.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and presents its methodology and findings clearly, making it accessible to readers. The quality of the experiments appears high, with a clear distinction made between results for GeoVeX and its competitors. The novelty of the approach is significant, particularly in the application of hexagonal grids and the ZIP layer. However, reproducibility may be a concern as the paper does not provide extensive details on the implementation specifics or datasets used, which could hinder independent verification of the results.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in the field of geospatial embeddings through the introduction of GeoVeX, demonstrating strong empirical results and a novel methodological approach. While the contributions are valuable, the reproducibility of results could be improved through more detailed methodological descriptions.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel framework for contextualizing geographical embeddings using hexagonal grids. It aims to improve upon existing models like Hex2Vec and Space2Vec by incorporating neighborhood influence through a distance-weighted kernel and employing a zero-inflated Poisson (ZIP) model for reconstruction. The authors claim that GeoVeX produces task-agnostic embeddings that can generalize across various downstream applications, demonstrating improved performance on several evaluation metrics.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to embedding geographical data by leveraging hexagonal grids and contextual information. However, several weaknesses are evident. The assumption that proximity implies semantic similarity may not hold universally, and the reliance on the ZIP model requires further validation against actual data distributions. Additionally, the use of a bag-of-words model for aggregating geographical tags risks oversimplifying complex relationships, which may undermine the quality of the embeddings. The evaluation metrics do not sufficiently address potential overfitting, raising concerns about the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally clear and well-structured, presenting the methodology and findings in a coherent manner. However, the underlying assumptions, such as the appropriateness of hexagonal grids and the fixed number of top K tags, require more thorough justification. The novelty of the approach is commendable, but the reproducibility is hindered by the lack of detailed statistical validation and potential biases in data representation.\n\n# Summary Of The Review\nOverall, GeoVeX presents a promising approach to geographical embeddings by contextualizing data based on hexagonal grids. However, several critical assumptions and potential oversights in its methodology may limit its robustness and generalizability. Further validation and a deeper analysis of its implications are necessary.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a geospatial representation model designed to learn global vectors for geographical locations. The model employs a novel architecture known as Hexagonal Convolutional Autoencoders (HCAE) alongside a Zero-Inflated Poisson (ZIP) reconstruction layer to effectively contextualize embeddings based on neighboring hexagonal regions. The experimental results demonstrate that GeoVeX outperforms existing models, such as Hex2Vec and Space2Vec, in downstream tasks including vacation rental price prediction and hyperlocal temperature interpolation.\n\n# Strength And Weaknesses\nThe primary strengths of the paper include its innovative use of hexagonal convolutions, which are well-suited for sparse geographic data, and the incorporation of a ZIP reconstruction layer that addresses zero-inflated count distributions. The experimental evaluation is robust, showing significant improvements in performance metrics over previous models. However, the paper could benefit from a more comprehensive analysis of the limitations of GeoVeX, particularly in terms of its generalizability across diverse geospatial contexts and tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its objectives, methodology, and findings. The novelty of the approach is evident in the introduction of HCAE and the ZIP layer, which collectively enhance the model's capability to learn context-aware geospatial embeddings. The reproducibility is supported by sufficient detail in the methodology, although the code and datasets used in the experiments are not explicitly mentioned, which could hinder replication efforts by other researchers.\n\n# Summary Of The Review\nGeoVeX represents a significant advancement in geospatial representation learning, demonstrating improved performance on relevant tasks through innovative architectural choices. While the model's strengths are apparent, additional clarity regarding its limitations and the availability of reproducible resources would enhance the overall contribution of the work.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces a new neural network architecture aimed at improving the efficiency and accuracy of image classification tasks. The authors propose a hybrid model that combines convolutional layers with attention mechanisms, thereby enhancing feature extraction and context-awareness in image data. Through extensive experiments on standard benchmark datasets, the authors demonstrate that their method outperforms existing state-of-the-art techniques in terms of accuracy while maintaining computational efficiency.\n\n# Strengths And Weaknesses\n**Strengths:**\n1. **Innovative Approach:** The combination of convolutional layers with attention mechanisms is a novel contribution that addresses limitations in traditional architectures regarding context understanding.\n2. **Strong Empirical Evidence:** The extensive experiments conducted on multiple datasets provide compelling evidence of the proposed method's effectiveness, showcasing significant performance improvements.\n3. **Clear Problem Formulation:** The paper clearly articulates the problem being addressed and provides a well-defined research question, which enhances the reader's understanding of the contributions.\n4. **Well-Structured Presentation:** The organization of the paper is logical, with clear sections that guide the reader through the motivation, methodology, results, and discussions.\n\n**Weaknesses:**\n1. **Limited Comparison with State-of-the-Art:** Although the proposed method shows improved performance, the comparison with existing methods could be more detailed to establish a clearer context for its advantages.\n2. **Insufficient Theoretical Insights:** The theoretical underpinnings of the proposed architecture could be elaborated further to help readers appreciate the foundational principles driving its design.\n3. **Lack of Robustness Testing:** The paper could benefit from additional experiments that assess the model’s robustness under various conditions, such as noise or adversarial attacks.\n4. **Generalizability Concerns:** The findings are primarily based on specific datasets; therefore, the generalizability of the model to other domains or datasets remains unclear.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is written clearly and is of high quality, with well-defined sections and a logical flow of information. The novelty of the proposed method is significant, as it integrates established techniques in a unique way. However, the reproducibility could be improved by providing more detailed descriptions of the experimental setups, including hyperparameters and training procedures.\n\n# Summary Of The Review\nThis paper presents a novel hybrid architecture that effectively enhances image classification tasks by integrating convolutional and attention mechanisms. While the experimental results are promising, further comparison with state-of-the-art methods and a deeper theoretical exploration are necessary to strengthen the contributions and generalizability of the work.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel geospatial representation model designed for learning global vectors that represent all geographical locations on Earth's land cover. It employs a Hexagonal Convolutional Autoencoder (HCAE) with a Zero-Inflated Poisson (ZIP) reconstruction layer, which allows for context-aware embeddings by utilizing features from neighboring hexagons. The model demonstrates improved performance over existing approaches such as Hex2Vec and Space2Vec, particularly in applications like price prediction for travel and climate data interpolation. The qualitative analysis indicates that GeoVeX embeddings capture geographical structures with higher fidelity compared to previous models.\n\n# Strength And Weaknesses\nThe primary strength of this paper lies in its innovative use of hexagonal convolutional techniques combined with a ZIP reconstruction layer, specifically tailored for sparse geographical count data. This addresses the limitations of prior models, which often lacked contextual awareness or were overly task-specific. However, the paper could benefit from a more detailed discussion on the limitations of the proposed model, including potential scalability issues and the impact of different grid resolutions on performance. Additionally, while the qualitative results are promising, further quantitative comparisons and benchmarks against a wider array of existing models would strengthen the evaluation of GeoVeX.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions, methodology, and findings. The novelty of combining hexagonal convolutions with a ZIP layer is a significant advancement in the field of geospatial embeddings. The experiments are adequately described, but reproducibility could be enhanced by providing more specific details about the dataset used and the training process. Overall, the clarity and quality of the writing are commendable, but additional details on experimental setups would be beneficial.\n\n# Summary Of The Review\nGeoVeX presents a significant advancement in geospatial representation through the integration of hexagonal convolutional techniques with ZIP reconstruction, demonstrating improved performance in various applications. While the paper is clear and well-structured, it could be enhanced by addressing potential limitations and providing more thorough experimental details for better reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel model for creating geospatial representations using Hexagonal Convolutional Autoencoders (HCAE) paired with a Zero-Inflated Poisson (ZIP) layer. GeoVeX aims to generate global, context-aware embeddings for geographical locations by leveraging OpenStreetMap (OSM) data. The authors demonstrate that GeoVeX outperforms existing models, such as Hex2Vec and Space2Vec, across various tasks, including price prediction and temperature interpolation, indicating its improved capability in handling geospatial data.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative architecture that combines hexagonal convolutions with a ZIP autoencoder, addressing the limitations of previous geospatial models that were often task-specific and non-contextual. The comprehensive evaluation against established benchmarks highlights the model's effectiveness and broad applicability. However, a notable weakness is the lack of detailed ablation studies to dissect the contributions of individual components of the GeoVeX architecture. Additionally, while the paper outlines its findings, more extensive empirical validation across diverse datasets and tasks could strengthen its claims.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly communicates the methodology and results. The introduction effectively contextualizes the work within the existing literature, and the detailed descriptions of the proposed approach and experimental setup facilitate understanding. The novelty of integrating HCAE with a ZIP layer for geospatial data representation is significant. However, the reproducibility of the results could be enhanced by providing access to the code and detailed dataset descriptions.\n\n# Summary Of The Review\nOverall, the paper presents a compelling contribution to the field of geospatial representation learning through the development of the GeoVeX model. Its innovative approach and empirical results indicate a promising advancement over existing models, though further validation and resource availability would bolster the work's impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper titled \"GEOVEX: GEOSPATIAL VECTORS WITH HEXAGONAL CONVOLUTIONAL AUTOENCODERS\" presents a novel approach to generating geospatial embeddings through the GeoVeX model, which employs a Hexagonal Convolutional Autoencoder architecture. The authors introduce a Zero-Inflated Poisson (ZIP) reconstruction layer, providing a comprehensive framework for encoding geospatial data. The findings demonstrate that GeoVeX outperforms existing models, such as Hex2Vec and Space2Vec, in both qualitative and quantitative evaluations, showcasing significant improvements in performance metrics like Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n\n# Strength And Weaknesses\nThe paper's strengths lie in its thorough exploration of the limitations of existing geospatial embedding methods and the introduction of a novel architecture that effectively addresses these gaps. The detailed methodology and experimental setup lend credibility to the findings, making a strong case for the model's efficacy. However, the paper could benefit from a more explicit discussion of the specific applications and implications of the results, which may enhance its significance in practical contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and presents its ideas clearly, with a logical flow from introduction to conclusion. Technical jargon is adequately explained, making the content accessible to a broad audience. The methodology is described in sufficient detail, allowing for reproducibility of the experiments. Overall, the quality of presentation is high, and the novelty of the approach is evident in the context of existing literature.\n\n# Summary Of The Review\nThis paper makes a significant contribution to the field of geospatial representation learning by introducing a novel model that effectively addresses existing limitations. The methodology is sound, and the results are compelling, warranting acceptance with minor revisions for improved clarity in application implications.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n5/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel geospatial representation model designed to generate global vectors that capture terrestrial land cover across various geographical locations. The methodology features a Hexagonal Convolutional Autoencoder (HCAE) architecture coupled with a Zero-Inflated Poisson (ZIP) reconstruction layer to account for sparse geographical count data. The model utilizes Uber's H3 hexagonal grid framework, whereby each hexagon is represented by a histogram of OpenStreetMap (OSM) geographical tag occurrences. The findings demonstrate that GeoVeX embeddings outperform existing models, such as Hex2Vec and Space2Vec, in tasks such as travel industry price prediction and hyperlocal climate data interpolation.\n\n# Strength And Weaknesses\nStrengths of the paper include its innovative use of hexagonal convolutions, which enhance the isotropic processing of spatial data, and the incorporation of a ZIP layer that effectively addresses the prevalence of zero counts in geographical data. The empirical validation is robust, showcasing significant performance improvements over baseline models in real-world applications. However, weaknesses include a limited exploration of the model's hyperparameters and the potential scalability issues associated with processing large datasets on hexagonal grids. Additionally, the paper could benefit from a more detailed discussion on the interpretability of the learned embeddings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured, with clear descriptions of the methodology and contributions. The quality of the writing is high, making complex concepts accessible to readers. The novelty of the approach is significant, particularly in the application of hexagonal convolutions to geospatial data representation. However, while the methodology is described in detail, the reproducibility of the results may be hindered due to the lack of available code or datasets, which are essential for other researchers to validate the findings.\n\n# Summary Of The Review\nGeoVeX presents a compelling advancement in geospatial vector representation through its innovative architecture and empirical validation. While the approach demonstrates substantial improvements over prior models, the paper could enhance its reproducibility and interpretability aspects.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel geospatial representation model that utilizes hexagonal convolutional autoencoders. While the authors claim that GeoVeX enhances geospatial data representation, the methodology primarily builds upon existing frameworks, such as H3 and OpenStreetMap (OSM), without introducing substantial innovation. The findings suggest some improvements over established methods like Hex2Vec and Space2Vec; however, these results are not robustly validated across diverse scenarios.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its attempt to leverage hexagonal convolutions, which may provide a new perspective on geospatial data representation. However, the novelty is undermined by the model's reliance on existing frameworks and a questionable justification for the use of hexagonal convolutions over traditional methods. The Zero-Inflated Poisson (ZIP) reconstruction layer adds complexity without clear benefits, and the qualitative analysis lacks convincing evidence of its advantages. Additionally, concerns about overfitting, limited generalizability due to fixed geographical tags, and the choice of datasets weaken the overall contributions of the work.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper suffers from verbosity and convoluted presentation, making it challenging for readers to extract key insights. While the authors are transparent about their future work and the need for further analysis, this may reflect a lack of confidence in the current model’s robustness. The clarity of the results is diminished by subjective qualitative assessments and simplistic metrics, leading to questions about the reproducibility of the findings.\n\n# Summary Of The Review\nOverall, while GeoVeX presents itself as an advancement in geospatial representation, it ultimately appears to be an incremental improvement rather than a significant breakthrough. The contributions are not sufficiently distinctive or compelling, raising doubts about the claims of novelty and effectiveness.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n3\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents GeoVeX, an innovative geospatial representation model designed to generate global vectors for all geographical locations, emphasizing land cover. It employs a Hexagonal Convolutional Autoencoder (HCAE) architecture that captures geographical relationships through contextualized embeddings informed by neighboring hexagons. The introduction of a Zero-Inflated Poisson (ZIP) reconstruction layer addresses the challenges of sparse geographical count data, leading to significant performance improvements over existing models like Hex2Vec and Space2Vec in various downstream applications, including price prediction in the travel industry and hyperlocal climate data interpolation.\n\n# Strength And Weaknesses\nStrengths of GeoVeX include its novel approach to generating task-agnostic geospatial vectors that leverage the first law of geography, enhancing the quality of spatial representations. The ZIP reconstruction layer specifically caters to the unique nature of geographical data, providing a tailored solution that results in superior embedding performance. However, the paper could benefit from a more detailed discussion on the computational complexity and scalability of the HCAE architecture, as well as limitations regarding the generalizability of the embeddings across diverse geographical contexts.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its contributions and methodology. The quality of the experiments and results is high, showcasing the efficacy of GeoVeX through rigorous evaluations in multiple tasks. The novelty of the approach, particularly the integration of hexagonal embeddings and the ZIP layer, is significant and represents a meaningful advancement in geospatial representation learning. However, the reproducibility of the results would be enhanced by providing more detailed implementation specifics, including hyperparameter settings and dataset descriptions.\n\n# Summary Of The Review\nGeoVeX introduces a compelling approach to geospatial representation learning, showcasing significant advancements in both methodology and empirical performance. While the paper is clear and well-founded, some areas would benefit from additional detail regarding computational aspects and reproducibility.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel framework for geospatial representation utilizing Hexagonal Convolutional Autoencoders (HCAE). The methodology centers on the innovative use of hexagonal convolutions to better capture isotropic spatial relationships, thereby enhancing the contextualization of geographical embeddings. Key findings indicate that GeoVeX effectively models geospatial data by addressing issues related to spatial relationships and sparsity through a Zero-Inflated Poisson (ZIP) reconstruction layer, ultimately leading to superior geospatial embeddings compared to traditional approaches.\n\n# Strength And Weaknesses\nThe strength of this paper lies in its theoretical advancements, particularly the integration of hexagonal convolutions and the ZIP layer, which are well-grounded in statistical theories and spatial context. The ability to model complex geographical interactions through contextual embeddings is a significant contribution to Geographic Information Science (GIS). However, the paper may benefit from a more comprehensive empirical evaluation to substantiate its claims, as the theoretical underpinnings, while robust, require further validation through real-world applications and datasets.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and clearly articulates its theoretical contributions and methodology. The quality of the theoretical discussion is high, with a coherent structure that guides the reader through the concepts. However, the novelty, while present in the theoretical innovations, could be further emphasized through a comparative analysis with existing models. Reproducibility could be enhanced by providing more detailed descriptions of the implementation and datasets used in the experiments, as well as clearer instructions for future research.\n\n# Summary Of The Review\nGeoVeX presents a compelling advancement in the modeling of geospatial data through innovative theoretical frameworks and methodologies. While the paper's contributions are significant, further empirical validation and clarity regarding implementation details would enhance its overall impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n5\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper introduces GeoVeX, a novel model designed for analyzing spatial data using Hexagonal Convolutional Autoencoders (HCAE) combined with a Zero-Inflated Poisson (ZIP) reconstruction layer. The methodology involves processing a 3D tensor derived from a hexagonal grid, specifically utilizing Uber’s H3 system for spatial indexing, which allows for the efficient handling of over 200 million hexagons and filtering down to the most relevant OpenStreetMap (OSM) tags. The authors detail the architecture, including a masked hexagonal convolution, ZIP layer for sparse data handling, and a tailored loss function that considers geographical context. The paper reports successful applications of GeoVeX in tasks such as price prediction and temperature interpolation, demonstrating its effectiveness through various evaluation metrics.\n\n# Strength And Weaknesses\nThe main strengths of the paper lie in its innovative approach to leveraging hexagonal convolution for spatial data, which addresses the unique challenges of geographical contexts such as sparsity and irregular data distributions. The detailed mathematical formulations for both the hexagonal convolution and the ZIP layer provide clarity and rigor to the methodology. However, a notable weakness is the lack of code availability, which raises concerns regarding reproducibility. Additionally, while the future work indicates potential directions, it does not sufficiently address practical applications or broader implications of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nOverall, the paper is well-structured and clearly articulates its contributions. The mathematical details enhance the quality of the methodology, making it accessible for readers with a technical background. The novelty of applying hexagonal convolution and ZIP for spatial data is significant, though the absence of code limits reproducibility and may hinder further research. The authors could improve clarity by including more discussions on the practical implications of their work.\n\n# Summary Of The Review\nGeoVeX presents a promising approach to spatial data analysis through innovative use of hexagonal convolution and ZIP layers, demonstrating strong methodological rigor and empirical validation. However, concerns about reproducibility due to the lack of code availability and limited discussion on practical applications should be addressed in future revisions.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n4/5",
    "# Summary Of The Paper\nThe paper presents GeoVeX, a novel geospatial embedding model that claims to outperform existing models such as Hex2Vec and Space2Vec in various tasks, including price prediction and temperature interpolation. The authors propose a new loss function, the Zero-Inflated Poisson loss, and emphasize the contextualization of embeddings through the incorporation of neighboring hexagons. The findings suggest that GeoVeX yields superior performance metrics, although the methodology for these comparisons is occasionally opaque and may not fully account for the strengths of the competing models.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its introduction of a new model that aims to improve upon existing geospatial embedding techniques through innovative data processing and a novel loss function. However, the paper exhibits weaknesses in its comparative analysis, often presenting GeoVeX in an overly favorable light while inadequately addressing the merits of Hex2Vec and Space2Vec. The qualitative and quantitative evaluations are not only selective but also lack robustness, as they fail to provide a comprehensive overview of how these models perform under varying conditions and tasks.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper is generally well-written, the clarity suffers due to its biased presentation of the results, which may confuse readers regarding the actual contributions of GeoVeX versus existing models. The novelty of the model is evident in its approach to contextualization and the proposed loss function; however, the claim of innovation in data processing techniques is undermined by a lack of acknowledgment of similar methodologies in prior work. Reproducibility is a concern, as the selective nature of the results and the absence of detailed methodology raise questions about the reliability of the conclusions drawn.\n\n# Summary Of The Review\nThe paper introduces GeoVeX, a geospatial embedding model that demonstrates potential improvements over existing frameworks. However, the comparative analysis lacks depth and fairness, leading to a biased portrayal of GeoVeX’s advantages. A more balanced discussion of existing models would enhance the paper's credibility and overall contribution to the field.\n\n# Correctness\n3\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper titled \"GEOVEX: GEOSPATIAL VECTORS WITH HEXAGONAL CONVOLUTIONAL AUTOENCODERS\" introduces a novel model for generating geospatial location representations using Hexagonal Convolutional Autoencoders (HCAE). The authors propose a methodology that intersects OpenStreetMap (OSM) entities with H3 hexagons, aiming to leverage the spatial relationships inherent in geospatial data. The findings demonstrate that the GeoVeX model effectively bridges the gap in utilizing convolutional neural networks for hexagonal grids, resulting in improved contextualized embeddings and enhanced performance in geospatial tasks.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative application of hexagonal grids to geospatial data, which is a relatively underexplored area. The methodology is sound, utilizing a clear framework for model development and evaluation. However, a notable weakness is the lack of clarity in some sections, particularly regarding the loss function and the significance of certain parameters, which could confuse readers. Additionally, while the empirical results are promising, the experiments could benefit from further comparative analysis with existing state-of-the-art methods.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is somewhat compromised by inconsistent terminology and formatting issues that detract from the overall quality. While the novelty of the approach is commendable, the reproducibility of the results could be enhanced by providing clearer explanations of the experimental setup and parameter choices. Some technical definitions are not adequately introduced, making it challenging for readers who may not be familiar with specific concepts.\n\n# Summary Of The Review\nOverall, the paper presents a novel approach to geospatial data representation using hexagonal convolutional autoencoders. While the contributions are significant, particularly in terms of methodology, issues with clarity and reproducibility need to be addressed to improve the paper's overall impact.\n\n# Correctness\n4/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n3/5",
    "# Summary Of The Paper\nThis paper introduces GeoVeX, a novel model focusing on geospatial vectors for Earth land cover. The methodology involves a Zero-Inflated Poisson (ZIP) reconstruction layer to address the sparsity of spatial data and improve predictive capabilities. The findings demonstrate that GeoVeX outperforms existing approaches in specific tasks, such as price prediction and climate data interpolation, although its broader applicability remains underexplored.\n\n# Strength And Weaknesses\nThe main strength of the paper lies in its innovative approach to modeling geospatial data using ZIP reconstruction, which is a significant contribution to the field. However, the paper's weaknesses include a limited exploration of potential applications in urban planning and disaster response, as well as a lack of evaluation across a wider range of downstream tasks. Furthermore, the absence of a comparative analysis across different geographic regions and the reliance on OpenStreetMap data without discussing its limitations are notable shortcomings. The paper also misses opportunities to address computational efficiency, ethical implications, and the interpretability of the embeddings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-written and presents its contributions clearly, although some technical aspects could benefit from additional detail. The novelty of the ZIP reconstruction layer is evident, yet the lack of discussion regarding alternative methods and future research directions limits the overall impact. Reproducibility could be enhanced by providing a more comprehensive framework for evaluating model performance across various contexts and tasks, particularly in real-world applications.\n\n# Summary Of The Review\nOverall, the paper presents a promising approach to geospatial modeling with GeoVeX, demonstrating technical innovation and potential improvements over existing methods. However, it falls short in exploring broader applications, providing comparative analyses, and addressing critical limitations and ethical considerations.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents GeoVeX, a new embedding framework designed to handle geographical data by utilizing a Zero-Inflated Poisson (ZIP) distribution to model count data from OpenStreetMap (OSM). The authors introduce a tailored Negative Log-Likelihood (NLL) loss function which incorporates distance and numerosity weighting kernels to enhance the learning of geospatial embeddings. Through extensive quantitative experiments, the paper demonstrates the effectiveness of GeoVeX embeddings in two downstream tasks—price prediction for vacation rentals and hyperlocal temperature interpolation—showing significant performance improvements over baseline models and other embedding techniques. \n\n# Strength And Weaknesses\nThe strengths of the paper lie in its rigorous statistical methodology and thorough evaluation metrics. The use of ZIP distribution addresses the specific challenges posed by excess zeros in geographical data, thereby enhancing the robustness of the model. The feature ablation study and generalization tests provide valuable insights into the significance of GeoVeX embeddings compared to existing methods. However, a notable weakness is the lack of formal statistical testing for the qualitative evaluation of embeddings, which may undermine the credibility of the qualitative insights presented. Additionally, while the paper offers a comprehensive statistical framework, the complexity of the methods may pose challenges for reproducibility without clear guidance on implementation.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly articulates its methodology and findings. The statistical rigor is commendable, with detailed mathematical formulations provided in the appendices, which enhances transparency and reproducibility. However, while the approach is novel in the context of geospatial embeddings, some sections may benefit from clearer explanations, particularly for readers unfamiliar with advanced statistical concepts. Overall, the quality of writing and presentation is high, though the nuances of the statistical methods could be more accessible.\n\n# Summary Of The Review\nOverall, this paper makes a significant contribution to the field of geospatial embeddings through its innovative use of statistical methodologies and rigorous empirical validation. While it excels in its technical execution and clarity, there are areas for improvement regarding the formalization of qualitative insights and accessibility for a broader audience.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4",
    "# Summary Of The Paper\nThe paper presents the GeoVeX model, a geospatial embedding framework designed to create vector representations of geographical entities utilizing OpenStreetMap (OSM) data. The authors claim that GeoVeX effectively captures spatial relationships and latent structures, demonstrating its utility through evaluations on two specific downstream tasks. However, the paper lacks comprehensive analysis regarding the model's scalability, generalizability, and robustness in diverse geographical contexts.\n\n# Strength And Weaknesses\nThe primary strength of the paper lies in its innovative approach to geospatial embeddings, leveraging OSM data to enhance representation learning. However, several weaknesses limit the impact of the findings: the scalability of the model is not assessed for high-resolution grids or larger geographical areas, raising concerns about its applicability. Furthermore, the evaluation is restricted to only two downstream tasks, failing to showcase the versatility of GeoVeX. The qualitative analysis lacks statistical rigor, and the absence of comparisons with other geospatial frameworks diminishes the contextual understanding of the model's performance. Additionally, the paper does not address potential biases introduced by OSM data quality, ethical implications of geospatial data usage, or the need for real-time adaptability in rapidly changing environments.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe clarity of the paper is generally good, with a well-structured presentation of the methodology; however, the lack of detailed discussions on hyperparameter tuning and future work creates ambiguities. The novelty of GeoVeX is commendable, but the paper does not sufficiently articulate the significance of its contributions in the broader landscape of geospatial representation learning. Reproducibility is hampered by the absence of detailed experimental setups and comparisons with alternative methods, along with a lack of exploration of varied data distributions.\n\n# Summary Of The Review\nWhile the GeoVeX model introduces a novel approach to geospatial embeddings, its limitations in scalability, generalizability, and comprehensive evaluation hinder its overall impact. The paper effectively presents a new framework but leaves significant gaps in robustness and contextual performance analysis.\n\n# Correctness\n3/5\n\n# Technical Novelty And Significance\n4/5\n\n# Empirical Novelty And Significance\n2/5",
    "# Summary Of The Paper\nThe paper titled \"GEOVEX: GEOSPATIAL VECTORS WITH HEXAGONAL CONVOLUTIONAL AUTOENCODERS\" proposes a novel approach for geospatial data representation using convolutional autoencoders structured around hexagonal grids. The authors claim that their model improves upon existing frameworks like Hex2Vec and Space2Vec by employing a Zero-Inflated Poisson reconstruction layer and embedding contextual features from neighboring hexagons. The findings suggest that the GeoVeX embeddings outperform two state-of-the-art geospatial location representations, with reported improvements in metrics such as Mean Absolute Error (MAE) and Mean Squared Error (MSE) in downstream tasks.\n\n# Strength And Weaknesses\nWhile the paper attempts to build on existing methodologies in geospatial data representation, many of its contributions appear to lack novelty. The use of convolutional autoencoders and hexagonal tiling is not particularly innovative, and the claims of improvement over previous models rely heavily on conventional metrics that do not convincingly demonstrate substantial advancements. Additionally, the discussion surrounding the implications of geographical characteristics on price prediction is an expected insight rather than a groundbreaking revelation. However, the technical implementation of the model is thorough, and the experimental results are presented in a clear manner, though their significance remains questionable.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is generally well-structured, but the clarity is hampered by convoluted explanations and an overemphasis on technical jargon. While the methodology is described in sufficient detail for reproducibility, the overall novelty is minimal, as the proposed approach appears to reiterate established concepts without offering significant advancements. Furthermore, the authors could have benefited from a more straightforward presentation of their findings, which would enhance the paper's overall quality and accessibility.\n\n# Summary Of The Review\nOverall, the paper presents an approach to geospatial data representation that lacks significant novelty and relies on established methodologies. While the results indicate some improvements over existing models, the contributions do not justify the complexity introduced by the authors. A more straightforward approach with a focus on practical applications could have yielded more impactful findings.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n2\n\n# Empirical Novelty And Significance\n2",
    "# Summary Of The Paper\nThe paper presents GeoVeX, a novel approach to geospatial embeddings utilizing hexagonal convolutional autoencoders. Its primary contributions include the introduction of a Zero-Inflated Poisson (ZIP) reconstruction layer, which aims to enhance the quality of embeddings in geospatial contexts. The methodology is evaluated through various downstream tasks in the travel and climate domains, demonstrating the effectiveness of the proposed embeddings. However, the paper identifies limitations in exploring hybrid models and the scalability of the approach.\n\n# Strength And Weaknesses\nThe strengths of the paper lie in its innovative use of hexagonal convolutional autoencoders and the introduction of the ZIP layer, which provides a fresh perspective on reconstructing geospatial data. The empirical evaluations indicate a solid performance across selected tasks. However, the weaknesses include a lack of exploration into alternative loss functions, insufficient attention to scalability in larger datasets, and a narrow focus on specific application domains. Moreover, the absence of temporal dynamics in the analysis could limit the generalizability of the findings.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-organized and clearly presents its contributions and findings. The quality of the methodology is high, although the novelty, while significant, could be enhanced by integrating additional embedding techniques and comparative analyses with other spatial indexing systems. Reproducibility might be improved by providing an open-source implementation of the GeoVeX model to facilitate community engagement and further research.\n\n# Summary Of The Review\nWhile the paper makes notable advancements in geospatial embeddings through the introduction of GeoVeX, it could benefit from addressing several key areas, including scalability, temporal dynamics, and broader application contexts. Overall, the research is compelling but requires further exploration to maximize its impact.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n3",
    "# Summary Of The Paper\nThe paper presents GeoVeX, a novel model that significantly enhances geographical embeddings for machine learning tasks. The authors demonstrate GeoVeX's effectiveness across two benchmark tasks: worldwide vacation rental price prediction and hyperlocal temperature interpolation. In both tasks, GeoVeX achieves superior performance metrics, with a Mean Squared Error (MSE) of 0.271 for vacation rental predictions and 10.656 for temperature interpolation, outpacing established models Hex2Vec and Space2Vec. The findings underscore the robustness and generalizability of GeoVeX, particularly in out-of-sample scenarios, and highlight the model's task-agnostic nature through an ablation study.\n\n# Strength And Weaknesses\nGeoVeX's primary strength lies in its empirical performance, exhibiting significant improvements over existing models, which is valuable for practical applications in geospatial machine learning. The comprehensive evaluation on benchmark tasks adds robustness to its claims. However, the paper could benefit from a deeper exploration of the underlying mechanisms that contribute to GeoVeX's superior performance, as well as additional experiments on a wider variety of tasks to further validate its generalizability.\n\n# Clarity, Quality, Novelty And Reproducibility\nThe paper is well-structured and clearly presents its methodology and findings, making it accessible to readers. The quality of the results is high, and the use of benchmark datasets enhances the credibility of the claims. The novelty of GeoVeX is evident in its approach to geographical embeddings; however, further discussion on the reproducibility of results in varied contexts would strengthen the paper.\n\n# Summary Of The Review\nOverall, GeoVeX is a compelling contribution to the field of geospatial machine learning, demonstrating significant improvements over existing models in key benchmark tasks. The paper effectively highlights the robustness and versatility of the proposed methodology, although further exploration of its underlying mechanisms and broader applicability would enhance its impact.\n\n# Correctness\n5\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n5",
    "# Summary Of The Paper\nThis paper introduces GeoVeX, a novel framework leveraging hexagonal convolutional autoencoders to address challenges in high-dimensional data representation. The authors propose two main innovations: the integration of Zero-Inflated Poisson models to handle sparse data and a new geometric convolution operation tailored for spatial data analysis. The extensive experiments demonstrate that GeoVeX outperforms existing models in various metrics, showcasing its effectiveness in both synthetic and real-world datasets.\n\n# Strength And Weaknesses\nThe primary strength of the paper is its contribution to the field of spatial data analysis through the introduction of a robust model that effectively manages sparsity. The methodology is sound and well-supported by empirical results, indicating a significant performance improvement over baseline models. However, the paper has several weaknesses, including a lack of clarity in the abstract and introduction, where complex sentences and jargon might hinder reader comprehension. Additionally, the absence of concrete examples to illustrate the proposed methods limits the paper’s accessibility to a broader audience.\n\n# Clarity, Quality, Novelty And Reproducibility\nWhile the paper's technical content is solid, clarity is compromised by dense language and lengthy paragraphs. The consistency of terminology and formatting could be improved, particularly in the use of abbreviations and reference styles. Although the novelty of GeoVeX is evident, the paper would benefit from clearer visual aids and examples to enhance understanding. The potential for reproducibility is present, but the extensive appendix may overwhelm readers, making it challenging to extract essential information quickly.\n\n# Summary Of The Review\nOverall, the paper presents a significant advancement in spatial data representation through the GeoVeX framework. However, improvements in clarity and accessibility are needed to make the findings more impactful. The paper's contributions are valuable, but the presentation requires refinement to engage a wider audience effectively.\n\n# Correctness\n4\n\n# Technical Novelty And Significance\n4\n\n# Empirical Novelty And Significance\n4"
  ],
  "condition_keys": [
    "Reference",
    "Faithful",
    "Objective Analysis",
    "Thorough Evaluation",
    "Balanced Critique",
    "Method Shift",
    "Question Shift",
    "Contribution Misrepresent",
    "Result Manipulation",
    "Assumption Attack",
    "Low Effort",
    "Generic",
    "Surface Skim",
    "Template Fill",
    "Checklist Review",
    "Overly Technical",
    "Harsh Critique",
    "Overly Positive",
    "Theory Focus",
    "Implementation Obsessed",
    "Comparison Fixated",
    "Pedantic Details",
    "Scope Creep",
    "Statistical Nitpick",
    "Future Work Focus",
    "Dismissive Expert",
    "Agenda Push",
    "Benchmark Obsessed",
    "Writing Critique"
  ],
  "logp_base": [
    -2.4981203818535285,
    -1.8673275067375579,
    -1.7815550817903498,
    -1.7339772042715293,
    -1.8937345301805315,
    -1.8071556953503611,
    -1.4138975993751473,
    -1.8233895316508675,
    -1.963342103213467,
    -1.9299338927495064,
    -1.7413021929187338,
    -1.3288145919574057,
    -1.7647552592159,
    -1.6894591295965178,
    -1.6441837899506895,
    -1.8543306909492465,
    -2.0682117604211316,
    -1.9043968919334124,
    -1.922305977396422,
    -1.9413604895286904,
    -1.7774546481958289,
    -1.734513008144343,
    -1.905034518990977,
    -1.9437790981233993,
    -1.705147155748554,
    -1.9441232402594517,
    -1.9030022214728979,
    -1.7304797325042836,
    -1.8443081044722633
  ],
  "logp_cond": [
    [
      0.0,
      -2.0352593414734015,
      -2.057675044373248,
      -2.088864386900943,
      -2.1010492860436627,
      -2.112219522016757,
      -2.1339245323705613,
      -2.0958003947341948,
      -2.045795745242038,
      -2.1022338022232714,
      -2.0923552510602765,
      -2.2526056655555595,
      -2.0841132923458523,
      -2.073215577820425,
      -2.1110449142364427,
      -2.0430089593738523,
      -2.0246207951550383,
      -2.08289801331995,
      -2.140423074153763,
      -2.119504195195557,
      -2.0942730505852567,
      -2.090399854037576,
      -2.1496652969479557,
      -2.139288146108006,
      -2.151174002936071,
      -2.075189242601828,
      -2.132072159016489,
      -2.1095487135188113,
      -2.1392706812828086
    ],
    [
      -1.5347346855970847,
      0.0,
      -1.2522673168031109,
      -1.2303679896131177,
      -1.3217433435504733,
      -1.2855670064408755,
      -1.5254505029160812,
      -1.3904566073613267,
      -1.2170596341001205,
      -1.3876329260455913,
      -1.275588126439596,
      -1.6408921969929735,
      -1.2402449993560898,
      -1.3587407075774751,
      -1.3784359316009016,
      -1.2177581969847229,
      -1.387049922813449,
      -1.2104241798317164,
      -1.415232335517912,
      -1.3824253994200046,
      -1.3976043502706386,
      -1.4716727809406474,
      -1.4730895292852302,
      -1.4789922898721144,
      -1.5278875270167758,
      -1.4003680825895075,
      -1.4468474588095146,
      -1.4297905632942451,
      -1.5358955398337455
    ],
    [
      -1.4470400267760546,
      -1.2052129148180548,
      0.0,
      -1.2669473000366915,
      -1.3186838943869212,
      -1.304805791667186,
      -1.452526784161704,
      -1.3768807556538298,
      -1.1957215595301647,
      -1.3553629339262434,
      -1.2464382107590688,
      -1.5661183811453943,
      -1.2329292037081214,
      -1.326083682210359,
      -1.3314738082839543,
      -1.2011360084545932,
      -1.3315001653720855,
      -1.2099096734576116,
      -1.3782908407732049,
      -1.3389860466651602,
      -1.3764790835731269,
      -1.403031744631985,
      -1.4482811357579373,
      -1.3638449379424629,
      -1.4382679772164406,
      -1.3345574707486623,
      -1.3849406988469812,
      -1.3567639303763104,
      -1.4591976121617465
    ],
    [
      -1.4033176861496874,
      -1.0953725072481038,
      -1.1466669728541208,
      0.0,
      -1.1704038505714145,
      -1.1759548424257535,
      -1.4098668317270806,
      -1.2027287205259811,
      -1.0594256237569235,
      -1.2808644835365777,
      -1.0945124351694597,
      -1.4898009342914242,
      -1.0628764977431837,
      -1.189513852602055,
      -1.2321065437710719,
      -1.1334392955334502,
      -1.246787111643668,
      -1.073573714175819,
      -1.2280726436064056,
      -1.2342435054484888,
      -1.2145564003045222,
      -1.3290322990477297,
      -1.2570844421889171,
      -1.2857006908396356,
      -1.4060515390433967,
      -1.1662997803343713,
      -1.2451283542859097,
      -1.284714072250157,
      -1.3280447442234082
    ],
    [
      -1.5201159081428364,
      -1.2664888939832368,
      -1.3587683051552224,
      -1.3256252289449995,
      0.0,
      -1.3444472164608146,
      -1.561823838733473,
      -1.4119504799808884,
      -1.3196394360858874,
      -1.4181552361450591,
      -1.359246257918693,
      -1.6393677855008029,
      -1.3095817607457767,
      -1.3986584637770396,
      -1.4481350683542684,
      -1.319620958088012,
      -1.342707424306526,
      -1.291423088870295,
      -1.437823073540182,
      -1.4225948085857696,
      -1.378577243823677,
      -1.544349428860564,
      -1.3860109986248463,
      -1.456781610192422,
      -1.509078846659514,
      -1.3433718630806386,
      -1.4269287448403623,
      -1.4372818014191266,
      -1.5240782886778557
    ],
    [
      -1.5033405632657768,
      -1.2954613054893256,
      -1.2886869812327209,
      -1.2971608181740069,
      -1.2754212753638774,
      0.0,
      -1.424410058891414,
      -1.3166031530237183,
      -1.283604242408397,
      -1.3193804331301224,
      -1.296877053279505,
      -1.5126604190680704,
      -1.2685352034368922,
      -1.3274032153777664,
      -1.2864787296376912,
      -1.283945929794969,
      -1.3114971767741488,
      -1.2764501823143704,
      -1.309402745835724,
      -1.3377728443461392,
      -1.3215287401569888,
      -1.3643109595840721,
      -1.4173894537102167,
      -1.3964991413475154,
      -1.4139587729835346,
      -1.3055768683517806,
      -1.3571418151647179,
      -1.422180630912713,
      -1.472146756887739
    ],
    [
      -1.2201607271693848,
      -1.1546182937314031,
      -1.1434624356897536,
      -1.1770187075361414,
      -1.1259599657950699,
      -1.1314949095246307,
      0.0,
      -1.165481181025318,
      -1.1160783767262938,
      -1.1551328128821157,
      -1.147865493694961,
      -1.1831511702141082,
      -1.146564363908737,
      -1.1601937083496823,
      -1.1282914771600843,
      -1.134244846341977,
      -1.1484953265842144,
      -1.145386835945757,
      -1.13183559635365,
      -1.1362286198058813,
      -1.1794980843555267,
      -1.115450555235667,
      -1.175463496830608,
      -1.1622784927390557,
      -1.171555590083486,
      -1.1414041047673724,
      -1.1554894787959382,
      -1.2041798341101129,
      -1.186261606580417
    ],
    [
      -1.5274327724387022,
      -1.4090796796864191,
      -1.4328433514744798,
      -1.3932085959283802,
      -1.3698159331549007,
      -1.4214856735477384,
      -1.5602576066896796,
      0.0,
      -1.4540331390787042,
      -1.47122300683112,
      -1.4230741201001167,
      -1.6001107381511346,
      -1.3811346097786494,
      -1.4439993384079777,
      -1.454502308995039,
      -1.4256237049562703,
      -1.4005348608370545,
      -1.3334468934613337,
      -1.450801557688546,
      -1.4949712881781372,
      -1.4275419963995692,
      -1.5225523675696238,
      -1.4905405964822829,
      -1.4836171974362824,
      -1.5445559472767778,
      -1.3557585555904772,
      -1.4750860600174596,
      -1.4610562968598315,
      -1.48046443943137
    ],
    [
      -1.5973264946070544,
      -1.2771356070324662,
      -1.3048648575132946,
      -1.2663170688589762,
      -1.3594975704906007,
      -1.3915727705849366,
      -1.6163185948616874,
      -1.4610820928359496,
      0.0,
      -1.4952735594039446,
      -1.3156399984221263,
      -1.7288325835151552,
      -1.288000547676396,
      -1.3475228035617741,
      -1.364294541470584,
      -1.2808836257036158,
      -1.4122189266648968,
      -1.2854573456111618,
      -1.4409352794277082,
      -1.4310261445869812,
      -1.4044495652843805,
      -1.489480815897909,
      -1.461487810156761,
      -1.4748083744718625,
      -1.5635013039818235,
      -1.3594275723008948,
      -1.436173489050776,
      -1.4441306472024418,
      -1.5635386254298933
    ],
    [
      -1.6120507364150845,
      -1.4243506903591399,
      -1.4733960104762513,
      -1.4726940071630779,
      -1.4534560611925271,
      -1.4415802783897576,
      -1.5733292948674067,
      -1.5026237520881713,
      -1.4949867686694798,
      0.0,
      -1.5130387852357352,
      -1.645846245441169,
      -1.4882367113428128,
      -1.5027827902077278,
      -1.5015324983153413,
      -1.4887508398781393,
      -1.4702653576023428,
      -1.4701874296753434,
      -1.5299305395177896,
      -1.4782110611587873,
      -1.4836304661130608,
      -1.5523456390670802,
      -1.5507029608195835,
      -1.5365097931693945,
      -1.510189754371257,
      -1.4792463947786567,
      -1.5414443628447259,
      -1.529243497760918,
      -1.5840534818022287
    ],
    [
      -1.387863050214983,
      -1.084112776011028,
      -1.1169150478855774,
      -1.0364866037391642,
      -1.1257421556977179,
      -1.1454866704208975,
      -1.3789751920382392,
      -1.169042497047352,
      -1.0552507558612205,
      -1.236909165077558,
      0.0,
      -1.466546605193485,
      -1.0682458556686838,
      -1.1113875161794817,
      -1.1878599842612976,
      -1.0812211066968973,
      -1.1760739968608394,
      -1.019340464086035,
      -1.1622391567402965,
      -1.1998417983520644,
      -1.1587773991144954,
      -1.312017482641907,
      -1.2290939604366116,
      -1.2349197029450796,
      -1.3652667745388058,
      -1.136512566879969,
      -1.1952915678596507,
      -1.1955150556185277,
      -1.300227522086577
    ],
    [
      -1.1682845982039067,
      -1.1471112795587415,
      -1.1364203279233622,
      -1.1653673679900414,
      -1.1219485945038872,
      -1.1141444196325534,
      -1.0970566625782459,
      -1.104086968682511,
      -1.1469843049341542,
      -1.1141636889877558,
      -1.1473900814780995,
      0.0,
      -1.1451763407168511,
      -1.1508853175596345,
      -1.1332829710653094,
      -1.1397824839388369,
      -1.1394233277338865,
      -1.1449428305015708,
      -1.127520069306558,
      -1.104777944017743,
      -1.1307736898160774,
      -1.0932281325529922,
      -1.1452587704953567,
      -1.1473303958046042,
      -1.1060952513670528,
      -1.1260523986086832,
      -1.1257130880746056,
      -1.1648668471079386,
      -1.1427967893520699
    ],
    [
      -1.4652289331731276,
      -1.1228502508026346,
      -1.1927076867169941,
      -1.1011930412804576,
      -1.218923910564568,
      -1.2011578101543623,
      -1.4982148997611278,
      -1.2596743186283013,
      -1.1117264485614786,
      -1.3118002956957953,
      -1.166923184517533,
      -1.5477203008054605,
      0.0,
      -1.1823472309013052,
      -1.2770759987158264,
      -1.0944330725308236,
      -1.2832838138268474,
      -1.0363051480454621,
      -1.2773770150096737,
      -1.317070832882693,
      -1.264807648739101,
      -1.418757579895182,
      -1.2822473005558328,
      -1.3523280322653484,
      -1.4481346494787064,
      -1.2212900847763408,
      -1.3059522881730776,
      -1.3694544931689003,
      -1.4109431776123713
    ],
    [
      -1.3106538536610344,
      -1.1197853741701214,
      -1.0591094496828461,
      -1.0854208294503758,
      -1.1375939570279152,
      -1.1483880856015303,
      -1.3466340855140257,
      -1.1653411231372535,
      -1.0231513405151247,
      -1.2123662267389161,
      -1.061839634514207,
      -1.4320826249415755,
      -1.053271994015562,
      0.0,
      -1.1701143280881392,
      -1.0615762841396188,
      -1.1519921277011793,
      -1.1020240880230778,
      -1.2098392147890151,
      -1.1752508668050505,
      -1.1444395232966322,
      -1.2646332143528034,
      -1.1949193103092446,
      -1.2065506545556732,
      -1.331203912357113,
      -1.1635092307337507,
      -1.2045651023291521,
      -1.2020936771188546,
      -1.2450166634872362
    ],
    [
      -1.2871761577242777,
      -1.04319400669913,
      -1.0824409240591952,
      -1.0607022925064007,
      -1.074161079818697,
      -1.0683287213222856,
      -1.1964906794874268,
      -1.1038501623938493,
      -1.0124268884775867,
      -1.115429610372204,
      -1.0711455398120386,
      -1.3671962479932376,
      -1.0693924492544002,
      -1.0937089145403356,
      0.0,
      -1.0623373511721388,
      -1.0800703432907763,
      -1.0438608473090392,
      -1.1178539884475474,
      -1.121182790577783,
      -1.1415285413744556,
      -1.0815159698536652,
      -1.1616674171568755,
      -1.2064025728687056,
      -1.1841963378322995,
      -0.9811066728937234,
      -1.1428249294970465,
      -1.1917770400344176,
      -1.1900081802893363
    ],
    [
      -1.4899114613660531,
      -1.169189718714223,
      -1.2035921765520217,
      -1.2310401986221124,
      -1.291137730329811,
      -1.2588935846466636,
      -1.4556786067268424,
      -1.3272901525163463,
      -1.1269407682443533,
      -1.3712564215127836,
      -1.1908449595577528,
      -1.5634973094930578,
      -1.1853488399815653,
      -1.289732855158797,
      -1.337312356178877,
      0.0,
      -1.317062678470237,
      -1.1269750882820115,
      -1.3097333487762786,
      -1.2876299523398875,
      -1.3525447075391712,
      -1.4267692530994638,
      -1.3526397315845682,
      -1.337068801761338,
      -1.4786101085348131,
      -1.3005372975794682,
      -1.341259048527489,
      -1.3832146317150176,
      -1.4599962292334594
    ],
    [
      -1.7191553579636165,
      -1.5177874159951867,
      -1.5677217592192783,
      -1.5203869083594554,
      -1.5377450507761163,
      -1.5601793365578345,
      -1.7135548494003132,
      -1.578695623374291,
      -1.5011036681414434,
      -1.5629907650222612,
      -1.5346974787657381,
      -1.8106466917917485,
      -1.53027285410038,
      -1.5684717932304555,
      -1.575725996661115,
      -1.456334585496737,
      0.0,
      -1.5112344976509906,
      -1.5543324541968697,
      -1.5922552318970755,
      -1.6031516524773244,
      -1.6711810413448223,
      -1.6123011254194393,
      -1.674965815892476,
      -1.6818603185645977,
      -1.4956128570728053,
      -1.5484576740658416,
      -1.6252633430091286,
      -1.6098673688520735
    ],
    [
      -1.550272125596349,
      -1.223471746612009,
      -1.2508814764808998,
      -1.2166055787038939,
      -1.2983306658101583,
      -1.3300621919767888,
      -1.5302122659089838,
      -1.3122413226436511,
      -1.2014947630565598,
      -1.3986113873818116,
      -1.2136792327531332,
      -1.6709435031645241,
      -1.169359284531642,
      -1.365108490672053,
      -1.356477673945495,
      -1.145454860703783,
      -1.3664882869913078,
      0.0,
      -1.3583706129027382,
      -1.4363334347710262,
      -1.340869094045042,
      -1.4795034592651342,
      -1.4028025460624414,
      -1.4055457479804645,
      -1.5362003086897986,
      -1.3137874407025625,
      -1.3831898333517312,
      -1.418720136638917,
      -1.4945617230576307
    ],
    [
      -1.5919046406030373,
      -1.4010986880036267,
      -1.4129074173580591,
      -1.3410643528335082,
      -1.407941758598097,
      -1.3703865576786922,
      -1.5627036497974138,
      -1.3826328557311724,
      -1.4230783290395816,
      -1.4513458178923364,
      -1.3908829702261187,
      -1.655199553237626,
      -1.3480502196449806,
      -1.4445044161456493,
      -1.4512695309458878,
      -1.4135640320392495,
      -1.4280261533445362,
      -1.354603128088876,
      0.0,
      -1.4532463593847378,
      -1.443978576181907,
      -1.525703278904618,
      -1.4384072648357247,
      -1.4607547571230177,
      -1.5644219609219343,
      -1.4098391816149136,
      -1.4146150470290597,
      -1.5238054722022585,
      -1.4869440614208551
    ],
    [
      -1.5873633539507421,
      -1.363068899420727,
      -1.4003442547801779,
      -1.43015828302654,
      -1.4390177637451647,
      -1.4368485479954785,
      -1.5597108325643514,
      -1.5213201277131547,
      -1.406291215780119,
      -1.5074655134743742,
      -1.4462147992410181,
      -1.6610146005183135,
      -1.4434428315892762,
      -1.4522623297212958,
      -1.4554962187698377,
      -1.378000512840107,
      -1.4875213541603922,
      -1.4204689685712202,
      -1.4937494944778549,
      0.0,
      -1.48715046968781,
      -1.492895329233677,
      -1.4976848817816577,
      -1.465660875152107,
      -1.5653005104211986,
      -1.4689015893744606,
      -1.4910233472653447,
      -1.5531821807676218,
      -1.5163337816142748
    ],
    [
      -1.4867369011827325,
      -1.3564415509763081,
      -1.3577964295785365,
      -1.3024193122809105,
      -1.2864426197135768,
      -1.3811901049464568,
      -1.5242690393529612,
      -1.3324774952902858,
      -1.3195145544428601,
      -1.4095972265173264,
      -1.30274941651878,
      -1.5684655607540914,
      -1.2989203993864509,
      -1.3092205779699977,
      -1.4135524068465979,
      -1.3338439720866708,
      -1.381932548603891,
      -1.2878655682032503,
      -1.432149979276106,
      -1.4068872640823462,
      0.0,
      -1.5232714691633276,
      -1.364267998626371,
      -1.3939629665684843,
      -1.4473935377218534,
      -1.3362686505575974,
      -1.3991387731666873,
      -1.3169049808898354,
      -1.4538634494109426
    ],
    [
      -1.3716972165251844,
      -1.2595327564406231,
      -1.2447584269601797,
      -1.2766618601248145,
      -1.2505616101676436,
      -1.1954406906058699,
      -1.3006676287351713,
      -1.2952079199140696,
      -1.2288965532630602,
      -1.2838664275100276,
      -1.2958271460281618,
      -1.4457356759407123,
      -1.294144476251576,
      -1.2894616640437346,
      -1.2267846286461237,
      -1.2722660082081474,
      -1.2822049654496952,
      -1.2736807241331656,
      -1.3084146372258327,
      -1.2423853962671914,
      -1.3179689725621133,
      0.0,
      -1.337231014439154,
      -1.3518193238757423,
      -1.3182796536724166,
      -1.1958847975225426,
      -1.3105237269786885,
      -1.372620292523326,
      -1.343044387738354
    ],
    [
      -1.5700597377151198,
      -1.391301613257859,
      -1.4196676170746445,
      -1.3902460075432548,
      -1.4184874241765502,
      -1.4537549487811356,
      -1.5741876201539502,
      -1.4407511981079002,
      -1.3674779007565228,
      -1.4552282703370538,
      -1.3737790090563873,
      -1.638126365423517,
      -1.3328369706686733,
      -1.4055480545959498,
      -1.455241041665656,
      -1.3332465388945884,
      -1.4383825742174374,
      -1.3201267808367658,
      -1.4157011253369116,
      -1.4241408819122214,
      -1.4413761353641672,
      -1.5160325482547279,
      0.0,
      -1.3923055733414302,
      -1.44202808240995,
      -1.4116611506455379,
      -1.3896762740766544,
      -1.4786742673257725,
      -1.5253491145856586
    ],
    [
      -1.6510768273201852,
      -1.540094053928402,
      -1.505575445020336,
      -1.5055260459718562,
      -1.5467561910596885,
      -1.591742305621723,
      -1.6553166670718522,
      -1.5684004586855496,
      -1.5165869777466698,
      -1.5834057513873632,
      -1.5174410458980416,
      -1.684978593846895,
      -1.5309253157680198,
      -1.5184698741519267,
      -1.6153102786386175,
      -1.4926118509233306,
      -1.6035792984716637,
      -1.5048831020125077,
      -1.5639040520513523,
      -1.5276795379847028,
      -1.5447175864929612,
      -1.6016161065956682,
      -1.5358268195821039,
      0.0,
      -1.5675530274680531,
      -1.544464925123747,
      -1.5495323830821535,
      -1.519021546518106,
      -1.6282572452546074
    ],
    [
      -1.4674869024551775,
      -1.4063726178598643,
      -1.387191371159672,
      -1.467862457733629,
      -1.4078598002976455,
      -1.3795343927334254,
      -1.42988453072889,
      -1.3978433037606113,
      -1.434643232073114,
      -1.3462683074445734,
      -1.4638425533771389,
      -1.4589610437325475,
      -1.4248602828819394,
      -1.4464697400516988,
      -1.400875709712417,
      -1.4411935765389319,
      -1.373583173632877,
      -1.441875790073742,
      -1.4227418632188482,
      -1.4034346230816805,
      -1.4159410838147746,
      -1.3701605185424244,
      -1.365883108576605,
      -1.380855559975564,
      0.0,
      -1.4452982503328111,
      -1.4243407351207797,
      -1.4202408155783515,
      -1.418997806231512
    ],
    [
      -1.6335694669874987,
      -1.4354259433073706,
      -1.4768390120326804,
      -1.412638559990007,
      -1.394772716736689,
      -1.4464715184839656,
      -1.5936339530970183,
      -1.4858957033837261,
      -1.3862365060655562,
      -1.4846907222190244,
      -1.4261017946045127,
      -1.726807699302446,
      -1.4006325530779196,
      -1.464723961688414,
      -1.4045949406493157,
      -1.4240346423807173,
      -1.4315296924841907,
      -1.4019274071845165,
      -1.502942015195756,
      -1.510606713990542,
      -1.4751372047501456,
      -1.504141037022863,
      -1.510988096733023,
      -1.5408925486760947,
      -1.590147344395566,
      0.0,
      -1.4692268119043612,
      -1.5326732336896263,
      -1.5356516796081732
    ],
    [
      -1.4899927866515956,
      -1.3819576572483103,
      -1.3936454523304729,
      -1.3212321898975365,
      -1.3495078567204186,
      -1.3697377862930837,
      -1.5279977381040752,
      -1.3919582489931792,
      -1.3042648732469013,
      -1.3968884737053395,
      -1.359168401850258,
      -1.5873091400323216,
      -1.3334744861099863,
      -1.3635190253981115,
      -1.3776893627641462,
      -1.301868908945573,
      -1.393883420234651,
      -1.2828705385147718,
      -1.3656911572366268,
      -1.389811820781601,
      -1.4110273633262842,
      -1.4277788599068566,
      -1.3431558283043423,
      -1.429596296342315,
      -1.4714894240115755,
      -1.3370102943506625,
      0.0,
      -1.4681832714215703,
      -1.4576220857587543
    ],
    [
      -1.4118797897415314,
      -1.249617646153113,
      -1.2512977619385626,
      -1.2475930856300248,
      -1.2926088188683595,
      -1.3157448771113107,
      -1.4248153980993945,
      -1.3190866033212851,
      -1.2429841298938742,
      -1.2961389779280654,
      -1.2829346571841036,
      -1.4806864166827876,
      -1.3191870937962178,
      -1.2937029318872264,
      -1.3319399815769692,
      -1.2986108303182333,
      -1.3235105717759779,
      -1.249358286873117,
      -1.3442028127609371,
      -1.3765995681136687,
      -1.2688385320194167,
      -1.3835319627793994,
      -1.3622593376363288,
      -1.2964553051421008,
      -1.3965658124633296,
      -1.2987417421798237,
      -1.384132870116022,
      0.0,
      -1.4128594703687853
    ],
    [
      -1.491131941529694,
      -1.4228586329179111,
      -1.4149786481783833,
      -1.3667984635160553,
      -1.3763543868044459,
      -1.3779146646448863,
      -1.455861801464396,
      -1.3403866946932634,
      -1.3901699253646562,
      -1.4333546144829894,
      -1.4037812807136891,
      -1.54019361657027,
      -1.3846996118368793,
      -1.364555633631746,
      -1.3568123675862658,
      -1.379430898460229,
      -1.3296822724271093,
      -1.376617742362575,
      -1.3550398058764996,
      -1.372060342490669,
      -1.3923496882592692,
      -1.3916438739454926,
      -1.4190024073650136,
      -1.3932424463949733,
      -1.4884011471621978,
      -1.3408708886605638,
      -1.3452826426620161,
      -1.4537516591936328,
      0.0
    ]
  ],
  "difference_matrix": [
    [
      0.0,
      0.462861040380127,
      0.4404453374802806,
      0.4092559949525856,
      0.39707109580986577,
      0.38590085983677147,
      0.3641958494829671,
      0.4023199871193337,
      0.4523246366114906,
      0.3958865796302571,
      0.40576513079325194,
      0.24551471629796895,
      0.4140070895076762,
      0.4249048040331034,
      0.3870754676170858,
      0.4551114224796762,
      0.4734995866984901,
      0.4152223685335783,
      0.3576973076997656,
      0.37861618665797137,
      0.40384733126827177,
      0.4077205278159526,
      0.34845508490557275,
      0.35883223574552225,
      0.3469463789174574,
      0.42293113925170056,
      0.36604822283703964,
      0.3885716683347171,
      0.35884970057071985
    ],
    [
      0.33259282114047317,
      0.0,
      0.615060189934447,
      0.6369595171244402,
      0.5455841631870846,
      0.5817605002966824,
      0.3418770038214767,
      0.47687089937623117,
      0.6502678726374373,
      0.4796945806919666,
      0.5917393802979618,
      0.22643530974458437,
      0.6270825073814681,
      0.5085867991600828,
      0.48889157513665626,
      0.649569309752835,
      0.4802775839241089,
      0.6569033269058415,
      0.45209517121964593,
      0.48490210731755323,
      0.4697231564669193,
      0.3956547257969105,
      0.3942379774523277,
      0.3883352168654435,
      0.3394399797207821,
      0.46695942414805036,
      0.4204800479280433,
      0.43753694344331273,
      0.3314319669038124
    ],
    [
      0.33451505501429524,
      0.576342166972295,
      0.0,
      0.5146077817536583,
      0.4628711874034286,
      0.4767492901231638,
      0.3290282976286458,
      0.40467432613652,
      0.5858335222601851,
      0.4261921478641064,
      0.535116871031281,
      0.21543670064495557,
      0.5486258780822284,
      0.45547139957999083,
      0.4500812735063955,
      0.5804190733357566,
      0.45005491641826434,
      0.5716454083327382,
      0.403264241017145,
      0.44256903512518964,
      0.40507599821722295,
      0.3785233371583647,
      0.33327394603241256,
      0.41771014384788696,
      0.3432871045739092,
      0.44699761104168756,
      0.3966143829433686,
      0.42479115141403945,
      0.32235746962860334
    ],
    [
      0.3306595181218419,
      0.6386046970234256,
      0.5873102314174086,
      0.0,
      0.5635733537001149,
      0.5580223618457758,
      0.32411037254444874,
      0.5312484837455482,
      0.6745515805146058,
      0.4531127207349517,
      0.6394647691020696,
      0.2441762699801051,
      0.6711007065283456,
      0.5444633516694743,
      0.5018706605004575,
      0.6005379087380791,
      0.4871900926278614,
      0.6604034900957103,
      0.5059045606651238,
      0.49973369882304053,
      0.5194208039670072,
      0.40494490522379967,
      0.4768927620826122,
      0.4482765134318938,
      0.3279256652281326,
      0.567677423937158,
      0.4888488499856196,
      0.44926313202137225,
      0.4059324600481211
    ],
    [
      0.37361862203769514,
      0.6272456361972947,
      0.5349662250253091,
      0.568109301235532,
      0.0,
      0.549287313719717,
      0.3319106914470584,
      0.4817840501996431,
      0.5740950940946441,
      0.4755792940354724,
      0.5344882722618385,
      0.25436674467972864,
      0.5841527694347548,
      0.4950760664034919,
      0.4455994618262631,
      0.5741135720925195,
      0.5510271058740055,
      0.6023114413102364,
      0.4559114566403495,
      0.4711397215947619,
      0.5151572863568545,
      0.3493851013199676,
      0.5077235315556852,
      0.4369529199881095,
      0.38465568352101753,
      0.5503626670998929,
      0.46680578534016925,
      0.45645272876140486,
      0.36965624150267584
    ],
    [
      0.3038151320845843,
      0.5116943898610355,
      0.5184687141176403,
      0.5099948771763543,
      0.5317344199864837,
      0.0,
      0.3827456364589472,
      0.4905525423266428,
      0.523551452941964,
      0.4877752622202387,
      0.510278642070856,
      0.2944952762822908,
      0.538620491913469,
      0.47975247997259474,
      0.52067696571267,
      0.5232097655553922,
      0.49565851857621235,
      0.5307055130359908,
      0.4977529495146371,
      0.46938285100422195,
      0.4856269551933723,
      0.442844735766289,
      0.3897662416401444,
      0.41065655400284573,
      0.3931969223668266,
      0.5015788269985806,
      0.45001388018564326,
      0.3849750644376482,
      0.33500893846262203
    ],
    [
      0.1937368722057624,
      0.25927930564374413,
      0.27043516368539366,
      0.2368788918390059,
      0.2879376335800774,
      0.28240268985051653,
      0.0,
      0.2484164183498292,
      0.2978192226488534,
      0.2587647864930316,
      0.2660321056801862,
      0.23074642916103905,
      0.2673332354664102,
      0.253703891025465,
      0.285606122215063,
      0.2796527530331703,
      0.26540227279093287,
      0.2685107634293902,
      0.2820620030214973,
      0.27766897956926595,
      0.23439951501962053,
      0.2984470441394802,
      0.23843410254453934,
      0.25161910663609155,
      0.24234200929166128,
      0.27249349460777483,
      0.25840812057920903,
      0.20971776526503438,
      0.22763599279473024
    ],
    [
      0.29595675921216524,
      0.41430985196444836,
      0.39054618017638765,
      0.4301809357224873,
      0.4535735984959668,
      0.4019038581031291,
      0.26313192496118787,
      0.0,
      0.36935639257216324,
      0.3521665248197474,
      0.4003154115507508,
      0.2232787934997329,
      0.44225492187221804,
      0.3793901932428898,
      0.3688872226558284,
      0.39776582669459715,
      0.422854670813813,
      0.48994263818953376,
      0.37258797396232146,
      0.3284182434727303,
      0.39584753525129823,
      0.3008371640812437,
      0.3328489351685846,
      0.3397723342145851,
      0.27883358437408967,
      0.4676309760603903,
      0.3483034716334079,
      0.36233323479103596,
      0.3429250922194975
    ],
    [
      0.3660156086064126,
      0.6862064961810008,
      0.6584772457001724,
      0.6970250343544908,
      0.6038445327228663,
      0.5717693326285305,
      0.34702350835177964,
      0.5022600103775174,
      0.0,
      0.46806854380952245,
      0.6477021047913407,
      0.2345095196983118,
      0.675341555537071,
      0.6158192996516929,
      0.599047561742883,
      0.6824584775098512,
      0.5511231765485702,
      0.6778847576023053,
      0.5224068237857589,
      0.5323159586264858,
      0.5588925379290866,
      0.4738612873155581,
      0.501854293056706,
      0.4885337287416045,
      0.3998407992316435,
      0.6039145309125722,
      0.527168614162691,
      0.5192114560110253,
      0.3998034777835737
    ],
    [
      0.317883156334422,
      0.5055832023903666,
      0.4565378822732551,
      0.4572398855864286,
      0.4764778315569793,
      0.4883536143597489,
      0.35660459788209975,
      0.4273101406613351,
      0.43494712408002667,
      0.0,
      0.41689510751377123,
      0.28408764730833735,
      0.4416971814066937,
      0.4271511025417787,
      0.42840139443416514,
      0.44118305287136717,
      0.45966853514716366,
      0.459746463074163,
      0.4000033532317169,
      0.45172283159071913,
      0.4463034266364456,
      0.3775882536824262,
      0.37923093192992297,
      0.39342409958011193,
      0.4197441383782494,
      0.4506874979708497,
      0.3884895299047806,
      0.4006903949885885,
      0.3458804109472777
    ],
    [
      0.3534391427037509,
      0.6571894169077057,
      0.6243871450331564,
      0.7048155891795695,
      0.6155600372210159,
      0.5958155224978363,
      0.36232700088049463,
      0.5722596958713817,
      0.6860514370575133,
      0.5043930278411757,
      0.0,
      0.2747555877252488,
      0.67305633725005,
      0.629914676739252,
      0.5534422086574362,
      0.6600810862218365,
      0.5652281960578944,
      0.7219617288326987,
      0.5790630361784372,
      0.5414603945666694,
      0.5825247938042384,
      0.4292847102768267,
      0.5122082324821222,
      0.5063824899736542,
      0.37603541837992793,
      0.6047896260387648,
      0.546010625059083,
      0.5457871373002061,
      0.4410746708321567
    ],
    [
      0.16052999375349897,
      0.18170331239866422,
      0.19239426403404347,
      0.16344722396736433,
      0.20686599745351852,
      0.2146701723248523,
      0.23175792937915984,
      0.2247276232748947,
      0.18183028702325155,
      0.21465090296964995,
      0.1814245104793062,
      0.0,
      0.18363825124055455,
      0.17792927439777118,
      0.1955316208920963,
      0.18903210801856885,
      0.18939126422351915,
      0.18387176145583495,
      0.2012945226508478,
      0.22403664793966271,
      0.1980409021413283,
      0.23558645940441347,
      0.18355582146204896,
      0.18148419615280154,
      0.22271934059035292,
      0.20276219334872247,
      0.20310150388280013,
      0.16394774484946706,
      0.18601780260533585
    ],
    [
      0.2995263260427723,
      0.6419050084132654,
      0.5720475724989058,
      0.6635622179354423,
      0.545831348651332,
      0.5635974490615376,
      0.26654035945477217,
      0.5050809405875987,
      0.6530288106544213,
      0.45295496352010467,
      0.597832074698367,
      0.2170349584104394,
      0.0,
      0.5824080283145947,
      0.4876792605000735,
      0.6703221866850764,
      0.4814714453890525,
      0.7284501111704378,
      0.4873782442062262,
      0.44768442633320693,
      0.49994761047679903,
      0.345997679320718,
      0.4825079586600671,
      0.41242722695055156,
      0.3166206097371935,
      0.5434651744395591,
      0.4588029710428223,
      0.3953007660469996,
      0.3538120816035286
    ],
    [
      0.3788052759354834,
      0.5696737554263964,
      0.6303496799136716,
      0.604038300146142,
      0.5518651725686026,
      0.5410710439949875,
      0.34282504408249204,
      0.5241180064592643,
      0.6663077890813931,
      0.47709290285760164,
      0.6276194950823109,
      0.25737650465494233,
      0.6361871355809559,
      0.0,
      0.5193448015083786,
      0.627882845456899,
      0.5374670018953385,
      0.58743504157344,
      0.47961991480750266,
      0.5142082627914673,
      0.5450196062998856,
      0.4248259152437144,
      0.4945398192872732,
      0.48290847504084455,
      0.3582552172394047,
      0.5259498988627671,
      0.48489402726736563,
      0.48736545247766316,
      0.4444424661092816
    ],
    [
      0.35700763222641174,
      0.6009897832515594,
      0.5617428658914942,
      0.5834814974442888,
      0.5700227101319926,
      0.5758550686284039,
      0.44769311046326266,
      0.5403336275568402,
      0.6317569014731028,
      0.5287541795784856,
      0.5730382501386508,
      0.2769875419574519,
      0.5747913406962892,
      0.5504748754103539,
      0.0,
      0.5818464387785507,
      0.5641134466599131,
      0.6003229426416503,
      0.5263298015031421,
      0.5230009993729066,
      0.5026552485762339,
      0.5626678200970243,
      0.482516372793814,
      0.4377812170819839,
      0.45998745211839,
      0.663077117056966,
      0.501358860453643,
      0.45240674991627183,
      0.4541756096613532
    ],
    [
      0.3644192295831934,
      0.6851409722350235,
      0.6507385143972249,
      0.6232904923271341,
      0.5631929606194355,
      0.595437106302583,
      0.39865208422240417,
      0.5270405384329002,
      0.7273899227048932,
      0.4830742694364629,
      0.6634857313914937,
      0.29083338145618876,
      0.6689818509676813,
      0.5645978357904495,
      0.5170183347703696,
      0.0,
      0.5372680124790095,
      0.727355602667235,
      0.544597342172968,
      0.566700738609359,
      0.5017859834100753,
      0.42756143784978273,
      0.5016909593646783,
      0.5172618891879086,
      0.3757205824144334,
      0.5537933933697783,
      0.5130716424217576,
      0.47111605923422895,
      0.3943344617157871
    ],
    [
      0.3490564024575151,
      0.5504243444259449,
      0.5004900012018534,
      0.5478248520616762,
      0.5304667096450153,
      0.5080324238632972,
      0.3546569110208184,
      0.4895161370468406,
      0.5671080922796883,
      0.5052209953988704,
      0.5335142816553935,
      0.2575650686293831,
      0.5379389063207516,
      0.49973996719067615,
      0.4924857637600166,
      0.6118771749243945,
      0.0,
      0.556977262770141,
      0.5138793062242619,
      0.4759565285240561,
      0.46506010794380725,
      0.3970307190763094,
      0.4559106350016924,
      0.3932459445286556,
      0.3863514418565339,
      0.5725989033483263,
      0.5197540863552901,
      0.4429484174120031,
      0.45834439156905815
    ],
    [
      0.35412476633706347,
      0.6809251453214034,
      0.6535154154525127,
      0.6877913132295186,
      0.6060662261232541,
      0.5743346999566237,
      0.3741846260244286,
      0.5921555692897613,
      0.7029021288768527,
      0.5057855045516009,
      0.6907176591802793,
      0.23345338876888833,
      0.7350376074017704,
      0.5392884012613595,
      0.5479192179879175,
      0.7589420312296293,
      0.5379086049421047,
      0.0,
      0.5460262790306742,
      0.4680634571623863,
      0.5635277978883704,
      0.4248934326682783,
      0.5015943458709711,
      0.49885114395294794,
      0.3681965832436138,
      0.59060945123085,
      0.5212070585816813,
      0.4856767552944954,
      0.40983516887578175
    ],
    [
      0.3304013367933847,
      0.5212072893927953,
      0.5093985600383628,
      0.5812416245629137,
      0.5143642187983251,
      0.5519194197177297,
      0.3596023275990081,
      0.5396731216652495,
      0.4992276483568403,
      0.47096015950408554,
      0.5314230071703032,
      0.26710642415879593,
      0.5742557577514413,
      0.47780156125077267,
      0.4710364464505341,
      0.5087419453571724,
      0.49427982405188575,
      0.567702849307546,
      0.0,
      0.46905961801168417,
      0.4783274012145149,
      0.39660269849180385,
      0.4838987125606973,
      0.46155122027340423,
      0.35788401647448764,
      0.5124667957815083,
      0.5076909303673622,
      0.39850050519416347,
      0.43536191597556684
    ],
    [
      0.3539971355779483,
      0.5782915901079635,
      0.5410162347485126,
      0.5112022065021504,
      0.5023427257835258,
      0.504511941533212,
      0.38164965696433906,
      0.4200403618155357,
      0.5350692737485714,
      0.43389497605431626,
      0.4951456902876723,
      0.28034588901037694,
      0.4979176579394142,
      0.4890981598073947,
      0.48586427075885275,
      0.5633599766885835,
      0.45383913536829823,
      0.5208915209574703,
      0.44761099505083557,
      0.0,
      0.4542100198408805,
      0.4484651602950134,
      0.44367560774703274,
      0.4756996143765835,
      0.3760599791074919,
      0.47245890015422987,
      0.45033714226334576,
      0.3881783087610686,
      0.42502670791441566
    ],
    [
      0.2907177470130964,
      0.42101309721952074,
      0.41965821861729236,
      0.4750353359149184,
      0.4910120284822521,
      0.3962645432493721,
      0.25318560884286767,
      0.44497715290554307,
      0.45794009375296874,
      0.36785742167850244,
      0.4747052316770488,
      0.2089890874417375,
      0.478534248809378,
      0.4682340702258312,
      0.363902241349231,
      0.44361067610915805,
      0.39552209959193796,
      0.4895890799925786,
      0.3453046689197228,
      0.37056738411348267,
      0.0,
      0.2541831790325013,
      0.41318664956945783,
      0.3834916816273446,
      0.33006111047397546,
      0.4411859976382315,
      0.37831587502914155,
      0.46054966730599345,
      0.3235911987848863
    ],
    [
      0.36281579161915856,
      0.4749802517037198,
      0.48975458118416326,
      0.4578511480195284,
      0.4839513979766994,
      0.5390723175384731,
      0.43384537940917167,
      0.43930508823027337,
      0.5056164548812827,
      0.4506465806343154,
      0.4386858621161811,
      0.28877733220363067,
      0.44036853189276703,
      0.4450513441006083,
      0.5077283794982193,
      0.46224699993619556,
      0.45230804269464775,
      0.4608322840111774,
      0.4260983709185102,
      0.4921276118771516,
      0.41654403558222963,
      0.0,
      0.397281993705189,
      0.3826936842686006,
      0.41623335447192633,
      0.5386282106218003,
      0.4239892811656545,
      0.361892715621017,
      0.39146862040598895
    ],
    [
      0.3349747812758572,
      0.5137329057331179,
      0.48536690191633247,
      0.5147885114477222,
      0.4865470948144268,
      0.45127957020984133,
      0.3308468988370268,
      0.4642833208830768,
      0.5375566182344542,
      0.44980624865392316,
      0.5312555099345897,
      0.26690815356746,
      0.5721975483223036,
      0.4994864643950272,
      0.44979347732532093,
      0.5717879800963885,
      0.46665194477353955,
      0.5849077381542112,
      0.48933339365406536,
      0.48089363707875554,
      0.4636583836268098,
      0.3890019707362491,
      0.0,
      0.5127289456495467,
      0.4630064365810269,
      0.4933733683454391,
      0.5153582449143226,
      0.4263602516652045,
      0.3796854044053184
    ],
    [
      0.2927022708032141,
      0.4036850441949973,
      0.4382036531030633,
      0.4382530521515431,
      0.3970229070637108,
      0.3520367925016763,
      0.2884624310515471,
      0.37537863943784977,
      0.4271921203767295,
      0.36037334673603616,
      0.4263380522253577,
      0.25880050427650425,
      0.4128537823553795,
      0.42530922397147264,
      0.3284688194847818,
      0.45116724720006873,
      0.34019979965173563,
      0.4388959961108916,
      0.37987504607204703,
      0.41609956013869653,
      0.3990615116304381,
      0.3421629915277311,
      0.40795227854129545,
      0.0,
      0.3762260706553462,
      0.39931417299965233,
      0.39424671504124587,
      0.42475755160529327,
      0.315521852868792
    ],
    [
      0.23766025329337648,
      0.2987745378886897,
      0.3179557845888821,
      0.23728469801492502,
      0.29728735545090856,
      0.32561276301512865,
      0.2752626250196639,
      0.30730385198794274,
      0.27050392367544007,
      0.35887884830398065,
      0.24130460237141516,
      0.2461861120160065,
      0.2802868728666146,
      0.2586774156968552,
      0.3042714460361371,
      0.26395357920962215,
      0.33156398211567706,
      0.263271365674812,
      0.2824052925297058,
      0.3017125326668735,
      0.28920607193377945,
      0.33498663720612964,
      0.339264047171949,
      0.32429159577298994,
      0.0,
      0.2598489054157429,
      0.2808064206277743,
      0.28490634017020255,
      0.28614934951704196
    ],
    [
      0.310553773271953,
      0.5086972969520811,
      0.4672842282267713,
      0.5314846802694446,
      0.5493505235227627,
      0.4976517217754861,
      0.35048928716243344,
      0.4582275368757256,
      0.5578867341938956,
      0.4594325180404273,
      0.518021445654939,
      0.21731554095700578,
      0.5434906871815322,
      0.4793992785710377,
      0.539528299610136,
      0.5200885978787344,
      0.512593547775261,
      0.5421958330749352,
      0.4411812250636957,
      0.4335165262689098,
      0.46898603550930607,
      0.43998220323658876,
      0.4331351435264288,
      0.403230691583357,
      0.35397589586388567,
      0.0,
      0.4748964283550905,
      0.41145000656982544,
      0.40847156065127854
    ],
    [
      0.41300943482130226,
      0.5210445642245876,
      0.509356769142425,
      0.5817700315753613,
      0.5534943647524793,
      0.5332644351798141,
      0.37500448336882264,
      0.5110439724797187,
      0.5987373482259966,
      0.5061137477675584,
      0.54383381962264,
      0.3156930814405763,
      0.5695277353629116,
      0.5394831960747863,
      0.5253128587087517,
      0.6011333125273248,
      0.5091188012382468,
      0.620131682958126,
      0.5373110642362711,
      0.5131904006912968,
      0.4919748581466137,
      0.4752233615660413,
      0.5598463931685556,
      0.4734059251305829,
      0.43151279746132243,
      0.5659919271222353,
      0.0,
      0.43481895005132754,
      0.4453801357141436
    ],
    [
      0.31859994276275216,
      0.48086208635117056,
      0.479181970565721,
      0.48288664687425875,
      0.437870913635924,
      0.41473485539297283,
      0.30566433440488905,
      0.4113931291829984,
      0.48749560261040936,
      0.4343407545762181,
      0.44754507532017995,
      0.249793315821496,
      0.41129263870806576,
      0.43677680061705715,
      0.3985397509273143,
      0.43186890218605023,
      0.4069691607283057,
      0.48112144563116654,
      0.38627691974334644,
      0.35388016439061487,
      0.4616412004848669,
      0.34694776972488417,
      0.36822039486795477,
      0.4340244273621827,
      0.333913920040954,
      0.4317379903244598,
      0.3463468623882615,
      0.0,
      0.3176202621354982
    ],
    [
      0.35317616294256937,
      0.4214494715543522,
      0.4293294562938801,
      0.47750964095620807,
      0.46795371766781746,
      0.466393439827377,
      0.3884463030078673,
      0.5039214097789999,
      0.4541381791076071,
      0.41095348998927395,
      0.4405268237585742,
      0.30411448790199325,
      0.45960849263538406,
      0.4797524708405174,
      0.48749573688599757,
      0.4648772060120343,
      0.514625832045154,
      0.4676903621096884,
      0.4892682985957637,
      0.47224776198159435,
      0.45195841621299415,
      0.45266423052677074,
      0.42530569710724975,
      0.45106565807729004,
      0.3559069573100655,
      0.5034372158116995,
      0.4990254618102472,
      0.39055644527863054,
      0.0
    ]
  ],
  "row_avgs": [
    0.3953527768310427,
    0.4811053592063064,
    0.4368617756102764,
    0.5037579051537179,
    0.4829262423412891,
    0.4612333571380785,
    0.25878166752024195,
    0.3702197946348654,
    0.539727866906108,
    0.41905474243797114,
    0.5506892489846464,
    0.1955694154397975,
    0.4883292075287809,
    0.5114817447016239,
    0.5244703379128726,
    0.5341268331976586,
    0.4812134205890097,
    0.5411976349905364,
    0.47398883343829784,
    0.4617928871130369,
    0.3943280498345704,
    0.4421712730817244,
    0.46877041804397707,
    0.38252005120632493,
    0.28927204322279526,
    0.4583041874151047,
    0.5091331947414222,
    0.40348382991999904,
    0.44583567235812854
  ],
  "col_avgs": [
    0.3237253908561412,
    0.5140648807255962,
    0.49801496380923443,
    0.5117075457973248,
    0.48906200810020956,
    0.4820608966905443,
    0.34149015299193153,
    0.4577220207876785,
    0.5253745094527387,
    0.4347294742282815,
    0.4965791042092147,
    0.25589584884625644,
    0.5164350614433421,
    0.4688479439977279,
    0.4518393085878226,
    0.5202443377349832,
    0.46168845003931935,
    0.531317170700126,
    0.4411621272255709,
    0.4428884380821683,
    0.45244373325104537,
    0.3913526949493133,
    0.4210360310448923,
    0.41666567428733303,
    0.36195998034371796,
    0.4866686726406924,
    0.4332283943759593,
    0.40928797729365146,
    0.3682069790073875
  ],
  "combined_avgs": [
    0.35953908384359196,
    0.49758511996595134,
    0.4674383697097554,
    0.5077327254755213,
    0.48599412522074936,
    0.47164712691431143,
    0.30013591025608677,
    0.41397090771127193,
    0.5325511881794234,
    0.4268921083331263,
    0.5236341765969306,
    0.22573263214302697,
    0.5023821344860615,
    0.4901648443496759,
    0.4881548232503476,
    0.527185585466321,
    0.4714509353141645,
    0.5362574028453312,
    0.4575754803319344,
    0.45234066259760264,
    0.4233858915428079,
    0.41676198401551884,
    0.44490322454443465,
    0.399592862746829,
    0.3256160117832566,
    0.4724864300278986,
    0.4711807945586907,
    0.40638590360682525,
    0.40702132568275806
  ],
  "gppm": [
    651.7336716135802,
    570.9529408448846,
    577.9903995650161,
    575.2562276268778,
    583.5787237348228,
    585.7716288121071,
    648.6290437771905,
    596.4965845479785,
    564.9161208164821,
    607.430011409693,
    583.0181209284644,
    687.3851498319482,
    572.1256510509179,
    595.6223002917214,
    603.3752696537987,
    569.245568904072,
    594.1996378843093,
    564.7034721078853,
    605.035360779435,
    600.262373293229,
    600.1567782542522,
    627.7401282783361,
    615.0335604205982,
    612.14514998261,
    639.8987634977257,
    581.8615250212401,
    609.5883457902479,
    620.9659027718955,
    638.4295014852405
  ],
  "gppm_normalized": [
    1.5031466832010512,
    1.3043307745322206,
    1.3204726016945982,
    1.3082087635147794,
    1.3269166415729787,
    1.338181310662637,
    1.4880858195117492,
    1.3582162891741605,
    1.2846259254621217,
    1.3864661729252947,
    1.3287187270841958,
    1.5737581179758284,
    1.3032730160531156,
    1.3552141515791682,
    1.3753824888753636,
    1.2992253176245916,
    1.3535527445971856,
    1.2801573837299185,
    1.3767233137598704,
    1.3745187893071773,
    1.3583312512793788,
    1.4322599028342144,
    1.3948090744512847,
    1.3955266082856526,
    1.4567973021836824,
    1.325402121909794,
    1.3890343723344092,
    1.4109495084628265,
    1.4561543086066866
  ],
  "token_counts": [
    592,
    468,
    467,
    422,
    415,
    464,
    502,
    432,
    416,
    451,
    450,
    475,
    446,
    415,
    432,
    462,
    427,
    385,
    414,
    484,
    370,
    426,
    385,
    441,
    414,
    437,
    435,
    402,
    438,
    900,
    449,
    438,
    451,
    629,
    382,
    443,
    461,
    435,
    409,
    435,
    495,
    472,
    420,
    449,
    474,
    395,
    386,
    412,
    477,
    383,
    361,
    437,
    433,
    378,
    393,
    417,
    384,
    389,
    740,
    463,
    467,
    478,
    461,
    468,
    475,
    424,
    581,
    430,
    387,
    456,
    472,
    459,
    354,
    472,
    425,
    461,
    402,
    393,
    396,
    429,
    371,
    422,
    405,
    465,
    409,
    350,
    388,
    536,
    459,
    459,
    406,
    437,
    447,
    406,
    427,
    392,
    413,
    380,
    518,
    437,
    441,
    395,
    410,
    398,
    387,
    409,
    434,
    399,
    372,
    389,
    407,
    400,
    391,
    415,
    433,
    358,
    540,
    392,
    429,
    465,
    427,
    412,
    424,
    454,
    458,
    449,
    425,
    493,
    453,
    403,
    390,
    398,
    403,
    431,
    446,
    448,
    445,
    336,
    419,
    447,
    420,
    363,
    416,
    440,
    360,
    291,
    469,
    439,
    398,
    434,
    444,
    418,
    397,
    434,
    460,
    385,
    550,
    495,
    392,
    392,
    425,
    406,
    446,
    408,
    417,
    413,
    456,
    377,
    432,
    375,
    407,
    451,
    421,
    383,
    1292,
    479,
    439,
    421,
    436,
    487,
    414,
    463,
    426,
    473,
    416,
    542,
    483,
    494,
    468,
    463,
    409,
    419,
    400,
    433,
    436,
    451,
    463,
    441,
    435,
    449,
    393,
    488,
    390,
    598,
    457,
    460,
    436,
    400,
    434,
    419,
    406,
    415,
    415,
    403,
    522,
    419,
    414,
    456,
    429,
    399,
    402,
    402,
    449,
    381,
    459,
    396,
    420,
    394,
    407,
    389,
    458,
    364,
    668,
    476,
    491,
    435,
    399,
    469,
    505,
    387,
    435,
    421,
    416,
    456,
    451,
    447,
    366,
    429,
    444,
    439,
    432,
    365,
    436,
    383,
    403,
    437,
    433,
    512,
    444,
    453,
    398,
    556,
    436,
    455,
    520,
    439,
    403,
    443,
    430,
    421,
    442,
    402,
    469,
    440,
    412,
    457,
    437,
    417,
    397,
    430,
    478,
    440,
    351,
    389,
    390,
    417,
    413,
    412,
    418,
    373,
    589,
    405,
    404,
    436,
    416,
    426,
    434,
    447,
    434,
    406,
    406,
    524,
    449,
    391,
    456,
    425,
    488,
    400,
    372,
    431,
    460,
    423,
    389,
    472,
    413,
    395,
    393,
    447,
    357,
    756,
    505,
    437,
    451,
    455,
    446,
    567,
    465,
    461,
    428,
    451,
    350,
    461,
    430,
    486,
    497,
    420,
    414,
    408,
    431,
    401,
    373,
    409,
    440,
    446,
    377,
    441,
    425,
    432,
    719,
    452,
    465,
    403,
    440,
    476,
    406,
    461,
    445,
    427,
    419,
    561,
    443,
    439,
    402,
    436,
    435,
    404,
    421,
    436,
    431,
    405,
    404,
    434,
    436,
    444,
    457,
    417,
    387,
    351,
    464,
    488,
    447,
    414,
    460,
    562,
    459,
    454,
    404,
    403,
    553,
    453,
    418,
    415,
    449,
    402,
    424,
    399,
    466,
    446,
    424,
    385,
    459,
    452,
    465,
    385,
    416,
    392
  ],
  "response_lengths": [
    1717,
    2525,
    2683,
    2482,
    2370,
    2491,
    3192,
    2571,
    2470,
    2196,
    2201,
    3127,
    2571,
    2266,
    2145,
    2458,
    2164,
    2422,
    2278,
    2556,
    2401,
    2194,
    2198,
    2630,
    2469,
    2557,
    2132,
    2218,
    2224
  ]
}